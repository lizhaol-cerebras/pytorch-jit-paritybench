import sys
_module = sys.modules[__name__]
del sys
example_1 = _module
example_2 = _module
print_world_size = _module
test_docs = _module
composer = _module
_version = _module
algorithms = _module
alibi = _module
alibi = _module
attention_surgery_functions = _module
_bert = _module
_gpt2 = _module
utils = _module
augmix = _module
augmix = _module
blurpool = _module
blurpool = _module
blurpool_layers = _module
channels_last = _module
channels_last = _module
colout = _module
colout = _module
cutmix = _module
cutmix = _module
cutout = _module
cutout = _module
ema = _module
ema = _module
factorize = _module
factorize = _module
factorize_core = _module
factorize_modules = _module
fused_layernorm = _module
fused_layernorm = _module
gated_linear_units = _module
gated_linear_unit_layers = _module
gated_linear_units = _module
ghost_batchnorm = _module
ghost_batchnorm = _module
gradient_clipping = _module
gradient_clipping = _module
gyro_dropout = _module
gyro_dropout = _module
label_smoothing = _module
label_smoothing = _module
layer_freezing = _module
layer_freezing = _module
low_precision_layernorm = _module
low_precision_layernorm = _module
mixup = _module
mixup = _module
no_op_model = _module
no_op_model = _module
progressive_resizing = _module
progressive_resizing = _module
randaugment = _module
randaugment = _module
sam = _module
sam = _module
selective_backprop = _module
selective_backprop = _module
seq_length_warmup = _module
seq_length_warmup = _module
squeeze_excite = _module
squeeze_excite = _module
stochastic_depth = _module
stochastic_depth = _module
stochastic_layers = _module
swa = _module
swa = _module
augmentation_common = _module
augmentation_primitives = _module
warnings = _module
weight_standardization = _module
weight_standardization = _module
callbacks = _module
checkpoint_saver = _module
early_stopper = _module
export_for_inference = _module
image_visualizer = _module
lr_monitor = _module
memory_monitor = _module
mlperf = _module
optimizer_monitor = _module
speed_monitor = _module
threshold_stopper = _module
cli = _module
launcher = _module
core = _module
algorithm = _module
callback = _module
data_spec = _module
engine = _module
evaluator = _module
event = _module
passes = _module
precision = _module
serializable = _module
state = _module
time = _module
types = _module
datasets = _module
ade20k = _module
brats = _module
c4 = _module
cifar = _module
ffcv_utils = _module
imagenet = _module
lm_dataset = _module
mnist = _module
synthetic = _module
synthetic_lm = _module
utils = _module
devices = _module
device = _module
device_cpu = _module
device_gpu = _module
device_mps = _module
device_tpu = _module
functional = _module
loggers = _module
cometml_logger = _module
console_logger = _module
file_logger = _module
in_memory_logger = _module
logger = _module
logger_destination = _module
mlflow_logger = _module
progress_bar_logger = _module
remote_uploader_downloader = _module
tensorboard_logger = _module
wandb_logger = _module
loss = _module
loss = _module
utils = _module
metrics = _module
map = _module
metrics = _module
nlp = _module
models = _module
base = _module
bert = _module
model = _module
classify_mnist = _module
model = _module
deeplabv3 = _module
model = _module
efficientnetb0 = _module
_layers = _module
efficientnets = _module
gpt2 = _module
huggingface = _module
initializers = _module
mmdetection = _module
resnet = _module
resnet_cifar = _module
resnets = _module
tasks = _module
classification = _module
timm = _module
unet = _module
_layers = _module
model = _module
unet = _module
vit_small_patch16 = _module
optim = _module
decoupled_weight_decay = _module
scheduler = _module
profiler = _module
json_trace_handler = _module
json_trace_merger = _module
marker = _module
profiler_action = _module
profiler_schedule = _module
system_profiler = _module
torch_profiler = _module
trace_handler = _module
trainer = _module
_deepspeed = _module
_scale_schedule = _module
_scaler = _module
dist_strategy = _module
trainer = _module
batch_helpers = _module
checkpoint = _module
collect_env = _module
device = _module
dist = _module
file_helpers = _module
fx_utils = _module
import_helpers = _module
inference = _module
iter_helpers = _module
misc = _module
module_surgery = _module
object_store = _module
libcloud_object_store = _module
oci_object_store = _module
s3_object_store = _module
sftp_object_store = _module
reproducibility = _module
retrying = _module
string_enum = _module
generate_build_matrix = _module
setup = _module
conf = _module
doctest_cleanup = _module
doctest_fixtures = _module
tables = _module
generate_cost_graphs = _module
update_alg_tables = _module
update_methods_overview = _module
update_model_tables = _module
checkpoint_with_wandb = _module
custom_models = _module
gyro_dropout_example = _module
train_resnet_imagenet1k = _module
profiler_demo = _module
train_deeplabv3_ade20k = _module
create_ffcv_datasets = _module
setup = _module
tests = _module
algorithm_settings = _module
algorithm_test_template = _module
test_algorithm_resumption = _module
test_algorithms_train = _module
test_alibi = _module
test_augmentations_functional = _module
test_blurpool = _module
test_blurpool_layers = _module
test_channels_last = _module
test_colout = _module
test_cutmix = _module
test_cutout = _module
test_ema = _module
test_factorize_algorithm = _module
test_factorize_core = _module
test_factorized_modules = _module
test_fused_layernorm = _module
test_gated_linear_units = _module
test_ghost_batchnorm = _module
test_gradient_clipping = _module
test_gyro_dropout = _module
test_label_smoothing = _module
test_layer_freezing = _module
test_low_precision_layernorm = _module
test_mixup = _module
test_progressive_resizing = _module
test_required_on_load = _module
test_selective_backprop = _module
test_seq_length_warmup = _module
test_squeeze_excite = _module
test_stochastic_depth = _module
test_torch_export = _module
test_weight_standardization = _module
callback_settings = _module
test_callbacks = _module
test_early_stopper = _module
test_image_visualizer = _module
test_inference = _module
test_loggers_across_callbacks = _module
test_memory_monitor = _module
test_mlperf_callback = _module
test_optimizer_monitor = _module
test_speed_monitor = _module
test_threshold_stopper = _module
test_cli = _module
common = _module
compare = _module
datasets = _module
events = _module
markers = _module
models = _module
conftest = _module
test_add_dataset_transform = _module
test_cifar = _module
test_dataset_utils = _module
test_ffcv_utils = _module
test_mnist = _module
test_segmentation_transforms = _module
test_synthetic_data = _module
test_synthetic_lm_data = _module
fixtures = _module
autouse_fixtures = _module
fixtures = _module
synthetic_hf_state = _module
test_cometml_logger = _module
test_console_logger = _module
test_file_logger = _module
test_in_memory_logger = _module
test_logger = _module
test_mlflow_logger = _module
test_progress_bar_logger = _module
test_remote_uploader_downloader = _module
test_wandb_logger = _module
metric_setter_callback = _module
test_current_metrics = _module
test_miou = _module
test_nlp_metrics = _module
test_bert = _module
test_composer_model = _module
test_efficientnet = _module
test_gpt2 = _module
test_hf_model = _module
test_mmdet_model = _module
test_scheduler = _module
test_json_trace_handler = _module
test_profiler = _module
test_device = _module
test_docker = _module
test_engine = _module
test_events = _module
test_full_nlp = _module
test_loss = _module
test_notebooks = _module
test_passes = _module
test_precision = _module
test_simple_nlp = _module
test_split_batch = _module
test_state = _module
test_synthetic_hf_state = _module
test_time = _module
test_checkpoint = _module
test_dataspec = _module
test_ddp = _module
test_ddp_sync_strategy = _module
test_predict = _module
test_scale_schedule = _module
test_trainer = _module
test_trainer_eval = _module
object_store_settings = _module
test_libcloud_object_store = _module
test_object_store = _module
test_oci_object_store = _module
test_s3_object_store = _module
test_batch_helpers = _module
test_dist = _module
test_dynamic_import = _module
test_file_helpers = _module
test_fx_utils = _module
test_inference = _module
test_iter_helpers = _module
test_module_surgery = _module
test_retrying = _module
test_string_enum = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from torch.utils.data import DataLoader


from torchvision import datasets


from torchvision import transforms


import logging


from typing import Optional


from typing import Sequence


from typing import Union


from torch.optim import Optimizer


import math


from types import MethodType


from typing import Tuple


from torch import nn


import inspect


from typing import Callable


from typing import Dict


from typing import Type


import functools


from typing import List


from typing import TypeVar


import numpy as np


import torch.utils.data


from torchvision.datasets import VisionDataset


import warnings


import torch.nn as nn


import torch.nn.functional as F


from torch.nn.common_types import _size_2_t


from typing import Any


from torch import Tensor


import copy


import itertools


from typing import cast


import abc


from typing import Iterable


from typing import TYPE_CHECKING


from functools import partial


import torchvision.transforms.functional


from torch.nn import functional as F


from math import ceil


from typing import Mapping


from torchvision.models.resnet import Bottleneck


from torch.fx import GraphModule


from torch.optim.swa_utils import SWALR


from torch.optim.swa_utils import AveragedModel


import torch.nn.utils.parametrize as parametrize


from torch.fx import symbolic_trace


from copy import deepcopy


import torch.cuda


from torch.utils.data import IterableDataset


import time


from abc import ABC


from abc import abstractmethod


import collections.abc


from torch.utils.data.distributed import DistributedSampler


from typing import Generator


import torch.nn.modules.utils


from torch.nn.parallel import DistributedDataParallel


from torch.utils.data import Dataset


import torchvision.transforms.functional as TF


import random


import torchvision


from torchvision.datasets import ImageFolder


from collections.abc import Mapping


from collections.abc import Sequence


import torch.nn


import torch.backends.cuda


import torch.backends.cudnn


import torch.cuda.amp


from torchvision.utils import draw_segmentation_masks


from functools import reduce


import re


from torch.nn.modules.loss import _Loss


from torchvision.ops import box_convert


import torch.distributed as torch_dist


from torchvision.models import _utils


from torchvision.models import resnet


from torch import nn as nn


from collections import UserDict


from torchvision.models.resnet import BasicBlock


from torch.optim import SGD


from torch.optim import AdamW


from torch.optim.optimizer import required


from torch.optim.lr_scheduler import LambdaLR


from typing import OrderedDict


import torch.profiler


from torch.profiler.profiler import ProfilerAction as TorchProfilerAction


from collections import Counter


from torch.optim.lr_scheduler import CosineAnnealingLR


from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts


from torch.optim.lr_scheduler import ExponentialLR


from torch.optim.lr_scheduler import MultiStepLR


from torch.optim.lr_scheduler import StepLR


from collections import defaultdict


from torch.cuda.amp.grad_scaler import GradScaler


from torch.cuda.amp.grad_scaler import OptState


from torch.cuda.amp.grad_scaler import _refresh_per_optimizer_state


from typing import ContextManager


from typing import TextIO


import torch.distributed


from torch.utils.data import DistributedSampler


from typing import NamedTuple


import torch.distributed as dist


from torch.fx import Node


from torch.fx.passes.split_utils import split_by_tags


import collections


import types


from typing import Callable as Callable


import torch.optim


from torchvision.datasets import MNIST


from torchvision.transforms import ToTensor


from torchvision.transforms.functional import InterpolationMode


from torch.utils.data import Subset


from torchvision.datasets import CIFAR10


from torch.nn import LayerNorm


from torch.nn.functional import gelu


from torch.nn.functional import relu


import torch.fx


import uuid


from torch.nn.functional import cross_entropy


from collections import ChainMap


from collections import OrderedDict


from collections import deque


from torch.fx.graph_module import GraphModule


from torchvision import models


def _symmetric_sample(level: float):
    """Helper function to sample from a symmetric distribution.

    The distribution over the domain [0.1, 10] with ``median == 1`` and uniform probability of ``x | 0.1 ≤ x ≤ 1``,
    and ``x | 1 ≤ x ≤ 10``.

    Used for sampling transforms that can range from intensity 0 to infinity and for which an intensity
    of 1 meaning no change.
    """
    if np.random.uniform() > 0.5:
        return np.random.uniform(1, level)
    else:
        return np.random.uniform(1 - 0.09 * level, 1)


def _float_parameter(level: float, maxval: float):
    """Helper function to scale a value between ``0`` and ``maxval`` and return as a float.

    Args:
        level (float): Level of the operation that will be between [0, 10].
        maxval (float): Maximum value that the operation can have. This will be scaled to
            ``level/10``.

    Returns:
        float: The result from scaling ``maxval`` according to ``level``.
    """
    return float(level) * maxval / 10.0


def _sample_level(n: float):
    """Helper function to sample from a uniform distribution between ``0.1`` and some value ``n``."""
    return np.random.uniform(low=0.1, high=n)


def _int_parameter(level: float, maxval: float):
    """Helper function to scale a value between ``0`` and ``maxval`` and return as an int.

    Args:
        level (float): Level of the operation that will be between ``[0, 10]``.
        maxval (float): Maximum value that the operation can have. This will be scaled to
            ``level/10``.

    Returns:
        int: The result from scaling ``maxval`` according to ``level``.
    """
    return int(level * maxval / 10)


def _default_2d_filter():
    default_filter = torch.tensor([[[[1, 2, 1], [2, 4, 2], [1, 2, 1]]]]) * 1 / 16.0
    return default_filter


def _padding_for_filt_2d_same(filt: torch.Tensor):
    _, _, h, w = filt.shape
    if h % 2 == 0:
        raise IndexError(f'Filter must have odd height; got {h}')
    if w % 2 == 0:
        raise IndexError(f'Filter must have odd width; got {w}')
    return int(torch.div(h, 2)), int(torch.div(w, 2))


def blur_2d(input: torch.Tensor, stride: _size_2_t=1, filter: Optional[torch.Tensor]=None) ->torch.Tensor:
    """Applies a spatial low-pass filter.

    Args:
        input (torch.Tensor): A 4d tensor of shape NCHW
        stride (int | tuple, optional): Stride(s) along H and W axes. If a single value is passed, this
            value is used for both dimensions.
        filter (torch.Tensor, optional): A 2d or 4d tensor to be cross-correlated with the input tensor
            at each spatial position, within each channel. If 4d, the structure
            is required to be ``(C, 1, kH, kW)`` where ``C`` is the number of
            channels in the input tensor and ``kH`` and ``kW`` are the spatial
            sizes of the filter.

    By default, the filter used is:

    .. code-block:: python

            [1 2 1]
            [2 4 2] * 1/16
            [1 2 1]

    Returns:
        The blurred input
    """
    _, c, h, w = input.shape
    n_in_channels = c
    if filter is None:
        filter = _default_2d_filter()
    if filter.shape[0] == 1 and n_in_channels > 1:
        filter = filter.repeat((n_in_channels, 1, 1, 1))
    _, _, filter_h, filter_w = filter.shape
    padding = _padding_for_filt_2d_same(filter)
    if h + 2 * padding[0] < filter_h:
        return input
    if w + 2 * padding[1] < filter_w:
        return input
    return F.conv2d(input, filter, stride=stride, padding=padding, groups=n_in_channels, bias=None)


def blurmax_pool2d(input: torch.Tensor, kernel_size: Optional[_size_2_t]=None, stride: _size_2_t=2, padding: _size_2_t=0, dilation: _size_2_t=1, ceil_mode: bool=False, filter: Optional[torch.Tensor]=None) ->torch.Tensor:
    """Max-pooling with anti-aliasing.

    This is a nearly drop-in replacement for PyTorch's :func:`torch.nn.functional.max_pool2d`.
    The only API difference is that the parameter ``return_indices`` is not
    available, because it is ill-defined when using anti-aliasing.

    See the associated `paper <http://proceedings.mlr.press/v97/zhang19a.html>`_
    for more details, experimental results, etc.

    This function can be understood as decoupling the max from the pooling, and
    inserting a low-pass filtering step between the two. Concretely, this
    function computes the max within spatial neighborhoods of shape
    ``kernel_size``, then applies an anti-aliasing filter to smooth the maxes,
    and only then pools according to ``stride``.

    See also: :func:`.blur_2d`.

    Args:
        input (torch.Tensor): A 4d tensor of shape NCHW
        kernel_size (int | tuple, optional): Size(s) of the spatial neighborhoods over which to pool.
            This is mostly commonly 2x2. If only a scalar ``s`` is provided, the
            neighborhood is of size ``(s, s)``. Default: ``(2, 2)``.
        stride (int | tuple, optional): Stride(s) along H and W axes. If a single value is passed, this
            value is used for both dimensions. Default: 2.
        padding (int | tuple, optional): implicit zero-padding to use. For the default 3x3 low-pass
            filter, ``padding=1`` (the default) returns output of the same size
            as the input. Default: 0.
        dilation (int | tuple, optional): Amount by which to "stretch" the pooling region for a given
            total size. See :class:`torch.nn.MaxPool2d`
            for our favorite explanation of how this works. Default: 1.
        ceil_mode (bool): When True, will use ceil instead of floor to compute the output shape. Default: ``False``.
        filter (torch.Tensor, optional): A 2d or 4d tensor to be cross-correlated with the input tensor
            at each spatial position, within each channel. If 4d, the structure
            is required to be ``(C, 1, kH, kW)`` where ``C`` is the number of
            channels in the input tensor and ``kH`` and ``kW`` are the spatial
            sizes of the filter.

    By default, the filter used is:

    .. code-block:: python

            [1 2 1]
            [2 4 2] * 1/16
            [1 2 1]

    Returns:
         The blurred and max-pooled input
    """
    if kernel_size is None:
        kernel_size = 2, 2
    maxs = F.max_pool2d(input, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, ceil_mode=ceil_mode)
    return blur_2d(maxs, stride=stride, filter=filter)


class BlurMaxPool2d(nn.Module):
    """This module is a (nearly) drop-in replacement for :class:`torch.nn.MaxPool2d`, but with an anti-aliasing filter.

    The only API difference is that the parameter ``return_indices`` is not
    available, because it is ill-defined when using anti-aliasing.

    See the associated `paper <http://proceedings.mlr.press/v97/zhang19a.html>`_
    for more details, experimental results, etc.

    See :func:`.blurmax_pool2d` for details.
    """

    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t]=None, padding: _size_2_t=0, dilation: _size_2_t=1, ceil_mode: bool=False):
        super(BlurMaxPool2d, self).__init__()
        self.kernel_size = kernel_size
        self.stride = stride if stride is not None else kernel_size
        self.padding = padding
        self.dilation = dilation
        self.ceil_mode = ceil_mode
        self.register_buffer('filt2d', _default_2d_filter())

    def extra_repr(self) ->str:
        return 'kernel_size={kernel_size}, stride={stride}, padding={padding}, dilation={dilation}, ceil_mode={ceil_mode}'.format(**self.__dict__)

    def forward(self, input: torch.Tensor):
        return blurmax_pool2d(input, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, ceil_mode=self.ceil_mode, filter=self.filt2d)

    @staticmethod
    def from_maxpool2d(module: torch.nn.MaxPool2d, module_index: int) ->'BlurMaxPool2d':
        return BlurMaxPool2d(kernel_size=module.kernel_size, stride=module.stride, padding=module.padding, dilation=module.dilation, ceil_mode=module.ceil_mode)


class BlurConv2d(nn.Module):
    """This module is a drop-in replacement for :class:`torch.nn.Conv2d`, but with an anti-aliasing filter.

    The one new parameter is ``blur_first``. When set to ``True``, the
    anti-aliasing filter is applied before the underlying convolution and
    vice-versa when set to ``False``. This mostly makes a difference when the
    stride is greater than one. In the former case, the only overhead is the
    cost of doing the anti-aliasing operation. In the latter case, the ``Conv2d``
    is applied with a stride of one to the input, and then the
    anti-aliasing is applied with the provided stride to the result. Setting
    the stride of the convolution to ``1`` can greatly increase the computational
    cost. E.g., replacing a stride of ``(2, 2)`` with a stride of ``1`` increases
    the number of operations by a factor of ``(2/1) * (2/1) = 4``. However,
    this approach most closely matches the behavior specified in the paper.

    This module should only be used to replace strided convolutions.

    See the associated `paper <http://proceedings.mlr.press/v97/zhang19a.html>`_
    for more details, experimental results, etc.

    See also: :func:`.blur_2d`.
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_2_t, stride: _size_2_t=None, padding: _size_2_t=0, dilation: _size_2_t=1, groups: int=1, bias: bool=True, blur_first: bool=True):
        super(BlurConv2d, self).__init__()
        self.blur_first = blur_first
        if self.blur_first:
            assert stride is not None
            conv_stride = stride
            self.blur_stride = 1
            blur_nchannels = in_channels
        else:
            conv_stride = 1
            self.blur_stride = kernel_size if stride is None else stride
            blur_nchannels = out_channels
        self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=conv_stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
        self.conv._already_blurpooled = True
        filt = _default_2d_filter().repeat(blur_nchannels, 1, 1, 1)
        self.register_buffer('blur_filter', filt)

    def forward(self, input: torch.Tensor):
        if self.blur_first:
            blurred = blur_2d(input, filter=self.blur_filter, stride=self.blur_stride)
            return self.conv.forward(blurred)
        else:
            activations = self.conv.forward(input)
            return blur_2d(activations, filter=self.blur_filter, stride=self.blur_stride)

    @staticmethod
    def from_conv2d(module: torch.nn.Conv2d, module_index: int=-1, blur_first: bool=True):
        has_bias = module.bias is not None and module.bias is not False
        blurconv = BlurConv2d(in_channels=module.in_channels, out_channels=module.out_channels, kernel_size=module.kernel_size, stride=module.stride, padding=module.padding, dilation=module.dilation, groups=module.groups, bias=has_bias, blur_first=blur_first)
        with torch.no_grad():
            blurconv.conv.weight.copy_(module.weight)
            if has_bias:
                blurconv.conv.bias.copy_(module.bias)
        return blurconv


class BlurPool2d(nn.Module):
    """This module just calls :func:`.blur_2d` in ``forward`` using the provided arguments."""

    def __init__(self, stride: _size_2_t=2, padding: _size_2_t=1) ->None:
        super(BlurPool2d, self).__init__()
        self.stride = stride
        self.padding = padding
        self.register_buffer('filt2d', _default_2d_filter())

    def forward(self, input: torch.Tensor):
        return blur_2d(input, stride=self.stride, filter=self.filt2d)


def _max_rank_with_possible_speedup(in_channels: int, out_channels: int, kernel_size: Optional[_size_2_t]=None) ->int:
    fan_in = in_channels
    if kernel_size is not None:
        fan_in *= np.prod(kernel_size)
    breakeven = fan_in * out_channels / (fan_in + out_channels)
    return int(math.ceil(breakeven - 1))


def _activations_conv2d_to_mat(activations, kernel_size, padding=0, padding_mode='zeros', stride=1, dilation=1, groups=1):
    if np.max(stride) > 1:
        raise NotImplementedError(f'Stride != 1 not implemented; got {stride}')
    if np.max(dilation) > 1:
        raise NotImplementedError(f'Dilation != 1 not implemented; got {dilation}')
    if groups != 1:
        raise NotImplementedError(f'Groups != 1 not implemented; got {groups}')
    if np.max(padding) > 0 and padding_mode.lower() != 'zeros':
        if not isinstance(padding, list):
            padding = [padding]
        activations = F.pad(activations, pad=padding, mode=padding_mode)
        padding = 0
    ret = F.unfold(activations, kernel_size=kernel_size, padding=padding)
    ret = ret.transpose(1, 2)
    return ret.reshape(-1, ret.shape[2])


def _mat_to_weights_conv2d(mat: Optional[torch.Tensor], kernel_size) ->Optional[torch.Tensor]:
    if mat is None:
        return None
    w = mat.T
    return w.reshape(w.shape[0], -1, *kernel_size)


def _weights_conv2d_to_mat(weights: torch.Tensor):
    return weights.reshape(weights.shape[0], -1).T


def _lstsq(A: torch.Tensor, B: torch.Tensor) ->torch.Tensor:
    if A.shape[0] != B.shape[0]:
        raise RuntimeError(f'A has different number of rows than B! A.shape = {A.shape}, B.shape = {B.shape}')
    if A.ndim != 2:
        raise RuntimeError('A is not a rank 2 tensor: has shape', A.shape)
    if B.ndim != 2:
        raise RuntimeError('B is not a rank 2 tensor: has shape', A.shape)
    return torch.linalg.lstsq(A, B).solution


def _nmse(Y: torch.Tensor, Y_hat: torch.Tensor) ->float:
    diffs = Y.detach() - Y_hat.detach()
    return float((diffs * diffs).mean() / Y.var())


def _svd_initialize(Wa: torch.Tensor, Wb: Optional[torch.Tensor], k: int) ->Tuple[torch.Tensor, torch.Tensor]:
    if Wb is None:
        W = Wa
    else:
        W = Wa @ Wb
    U, s, Vt = torch.linalg.svd(W, full_matrices=False)
    Wa = U[:, :k]
    Wb = Vt[:k]
    s_sqrt = torch.sqrt(s[:k])
    Wa *= s_sqrt
    Wb *= s_sqrt.reshape(-1, 1)
    return Wa, Wb


class BERTGatedFFOutput(torch.nn.Module):
    """
    Defines a single feed-forward block that uses `Gated Linear Units <https://arxiv.org/abs/2002.05202>`_.

    Args:
        d_embed (int): The input dimension for the feed-forward network.
        d_ff (int): The hidden dimension for the feed-forward network.
        dropout_rate (float): The dropout rate to use between the two projection matricies in the feed-forward block.
        act_fn (Callable[torch.Tensor, torch.Tensor]): The activation function to use in the feed-forward network.
        layernorm_eps (float): The epsilon term to use in the LayerNorm operator. Useful for when the variance is small.
        gated_layer_bias (bool): Whether to use a bias term in the gated projection matrix.
        non_gated_layer_bias (bool): Whether to use a bias term in teh non-gated projection matrix.
    """

    def __init__(self, d_embed: int, d_ff: int, dropout_rate: float, act_fn: Callable[[torch.Tensor], torch.Tensor], layernorm_eps: float, gated_layer_bias: bool=False, non_gated_layer_bias: bool=False):
        super().__init__()
        self.gated_layer = torch.nn.Linear(d_embed, d_ff, bias=gated_layer_bias)
        self.non_gated_layer = torch.nn.Linear(d_embed, d_ff, bias=non_gated_layer_bias)
        self.wo = torch.nn.Linear(d_ff, d_embed)
        self.dropout = torch.nn.Dropout(dropout_rate)
        self.act = act_fn
        self.layernorm = torch.nn.LayerNorm(d_embed, eps=layernorm_eps)

    def forward(self, hidden_states: torch.Tensor, residual_connection: torch.Tensor):
        """
        Args:
            hidden_states (torch.Tensor): The hidden states from the attention matrix.
            residual_connection (torch.Tensor): The residual connection to add before the LayerNorm operator.
        """
        hidden_states = self.act(self.gated_layer(hidden_states)) * self.non_gated_layer(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.wo(hidden_states)
        hidden_states = self.layernorm(hidden_states + residual_connection)
        return hidden_states


_TORCH_BATCHNORM_BASE_CLASS = torch.nn.modules.batchnorm._BatchNorm


def _corresponding_ghost_batchnorm_type(batchnorm: torch.nn.Module):
    if isinstance(batchnorm, torch.nn.BatchNorm1d):
        return GhostBatchNorm1d
    if isinstance(batchnorm, torch.nn.BatchNorm2d):
        return GhostBatchNorm2d
    if isinstance(batchnorm, torch.nn.BatchNorm3d):
        return GhostBatchNorm3d
    raise ValueError(f'Input was of type {type(batchnorm)}, not one of torch.nn.BatchNorm1d, torch.nn.BatchNorm2d, torch.nn.BatchNorm3d')


class _GhostBatchNorm(torch.nn.Module):
    """`Ghost batch normalization <https://arxiv.org/abs/1705.08741>`_ layer.

    Works by spliting input into chunks of ``ghost_batch_size`` samples and
    running batch normalization on each chunk separately. ``dim=0`` is assumed to
    be the sample axis.

    See also `torch.nn.BatchNorm1d <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html>`_,
    `torch.nn.BatchNorm2d <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html>`_, and
    `torch.nn.BatchNorm3d <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html>`_.

    Args:
        base_batchnorm (torch.nn.modules.batchnorm._BatchNorm): A batch normalization module to be applied to each chunk
        ghost_batch_size (int, optional): the size of the chunks passed into the underlying
            batch normalization. Default: ``32``.

    Raises:
        ValueError: If ``ghost_batch_size`` exceeds the number of samples in
            the batch provided to `forward`. This might happen when doing
            data-parallel training, because the per-worker batch size is usually
            much smaller than the overall batch size.
    """

    def __init__(self, base_batchnorm: _TORCH_BATCHNORM_BASE_CLASS, ghost_batch_size: int=32):
        super().__init__()
        self.ghost_batch_size = ghost_batch_size
        self.batchnorm = base_batchnorm
        self.batchnorm._already_ghost_batchnormed = True

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        batch_size = input.shape[0]
        if batch_size < self.ghost_batch_size:
            raise ValueError(f'Worker batch size {batch_size} < ghost_batch_size {self.ghost_batch_size}')
        nchunks: int = int(math.ceil(batch_size / self.ghost_batch_size))
        has_momentum = self.batchnorm.momentum is not None
        original_momentum: float = self.batchnorm.momentum
        if self.training and has_momentum:
            self._scale_momentum(nchunks)
        normalized_chunks = [self.batchnorm(chunk) for chunk in input.chunk(nchunks, 0)]
        if self.training and has_momentum:
            self._unscale_momentum(original_momentum)
        return torch.cat(normalized_chunks, dim=0)

    @staticmethod
    def from_batchnorm(module: torch.nn.Module, ghost_batch_size: int) ->'_GhostBatchNorm':
        assert isinstance(module, _TORCH_BATCHNORM_BASE_CLASS), 'Module is not a BatchNorm subclass!'
        bn_type = _corresponding_ghost_batchnorm_type(module)
        return bn_type(ghost_batch_size=ghost_batch_size, base_batchnorm=module)

    @torch.jit.unused
    def _scale_momentum(self, nchunks: int):
        self.batchnorm.momentum = float(self.batchnorm.momentum) / nchunks

    @torch.jit.unused
    def _unscale_momentum(self, original_momentum: float):
        self.batchnorm.momentum = original_momentum


class GyroDropoutLayer(torch.nn.Module):

    def __init__(self, iters_per_epoch: int, max_epoch: int, p: float, sigma: int, tau: int):
        super(GyroDropoutLayer, self).__init__()
        self.iters_per_epoch = iters_per_epoch
        self.max_epoch = max_epoch
        self.p = p
        self.sigma = sigma
        self.tau = tau
        self.preselect_masks = torch.empty(0, 0)
        self.dropout_mask = torch.empty(0, 0)
        self.selected_masks = torch.empty(0, 0)
        self.training_step = 0
        self.iter_num = 0

    def forward(self, x):
        if self.training:
            if self.training_step == 0:
                is_cuda_tensor = x.is_cuda
                if is_cuda_tensor:
                    self.preselect_masks = (torch.rand(self.sigma, x.shape[1]) > self.p).float()
                else:
                    self.preselect_masks = (torch.rand(self.sigma, x.shape[1]) > self.p).float()
                self.iter_num = int(self.iters_per_epoch * self.max_epoch / self.sigma) * self.tau
            if self.training_step % self.iter_num == 0:
                pick_idx = np.random.choice(self.sigma, self.tau)
                self.selected_masks = self.preselect_masks[pick_idx]
            self.dropout_mask = torch.repeat_interleave(self.selected_masks, x.shape[0] // self.tau, dim=0)
            self.training_step += 1
            return x * self.dropout_mask * (1 / (1 - self.p))
        else:
            return x


def _cast_if_autocast_enabled(hidden_states):
    if not torch.is_autocast_enabled():
        return hidden_states
    else:
        return torch.amp.autocast_mode._cast(hidden_states, torch.get_autocast_gpu_dtype())


class LPLayerNorm(torch.nn.LayerNorm):

    def __init__(self, layer):
        super().__init__(normalized_shape=layer.normalized_shape, eps=layer.eps, elementwise_affine=layer.elementwise_affine)
        with torch.no_grad():
            self.weight.copy_(layer.weight)
            self.bias.copy_(layer.bias)

    def forward(self, x):
        module_device = x.device
        downcast_x = _cast_if_autocast_enabled(x)
        downcast_weight = _cast_if_autocast_enabled(self.weight)
        downcast_bias = _cast_if_autocast_enabled(self.bias)
        with torch.autocast(enabled=False, device_type=module_device.type):
            return F.layer_norm(downcast_x, self.normalized_shape, downcast_weight, downcast_bias, self.eps)


class SqueezeExcite2d(torch.nn.Module):
    """Squeeze-and-Excitation block from (`Hu et al, 2019 <https://arxiv.org/abs/1709.01507>`_)

    This block applies global average pooling to the input, feeds the resulting
    vector to a single-hidden-layer fully-connected network (MLP), and uses the
    outputs of this MLP as attention coefficients to rescale the input. This
    allows the network to take into account global information about each input,
    as opposed to only local receptive fields like in a convolutional layer.

    Args:
        num_features (int): Number of features or channels in the input.
        latent_channels (float, optional): Dimensionality of the hidden layer within the added
            MLP. If less than 1, interpreted as a fraction of ``num_features``. Default: ``0.125``.
    """

    def __init__(self, num_features: int, latent_channels: float=0.125):
        super().__init__()
        self.latent_channels = int(latent_channels if latent_channels >= 1 else latent_channels * num_features)
        flattened_dims = num_features
        self.pool_and_mlp = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d(1), torch.nn.Flatten(), torch.nn.Linear(flattened_dims, self.latent_channels, bias=False), torch.nn.ReLU(), torch.nn.Linear(self.latent_channels, num_features, bias=False), torch.nn.Sigmoid())

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        n, c, _, _ = input.shape
        attention_coeffs = self.pool_and_mlp(input)
        return input * attention_coeffs.reshape(n, c, 1, 1)


class SqueezeExciteConv2d(torch.nn.Module):
    """Helper class used to add a :class:`.SqueezeExcite2d` module after a :class:`torch.nn.Conv2d`."""

    def __init__(self, *args, latent_channels: float=0.125, conv: Optional[torch.nn.Conv2d]=None, **kwargs):
        super().__init__()
        self.conv = torch.nn.Conv2d(*args, **kwargs) if conv is None else conv
        self.conv._already_squeeze_excited = True
        self.se = SqueezeExcite2d(num_features=self.conv.out_channels, latent_channels=latent_channels)

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        return self.se(self.conv(input))

    @staticmethod
    def from_conv2d(module: torch.nn.Conv2d, module_index: int, latent_channels: float):
        return SqueezeExciteConv2d(conv=module, latent_channels=latent_channels)


class BlockStochasticModule(nn.Module):
    """A convenience class that stochastically executes the provided main path of a residual block.

    Args:
        main (GraphModule): Operators in the main (non-residual) path of a residual block.
        residual (GraphModule | None): Operators, if any, in the residual path of a residual block.
        drop_rate: The base probability of dropping this layer. Must be between 0.0 (inclusive) and 1.0 (inclusive).

    Returns:
        BlockStochasticModule: An instance of :class:`.BlockStochasticModule`.
    """

    def __init__(self, main: GraphModule, residual: Optional[GraphModule]=None, drop_rate: float=0.2):
        super().__init__()
        self.drop_rate = torch.tensor(drop_rate)
        self.main = main
        self.residual = residual

    def forward(self, x):
        sample = not self.training or bool(torch.bernoulli(1 - self.drop_rate))
        residual_result = x
        if self.residual:
            residual_result = self.residual(x)
        if sample:
            main_result = self.main(x)
            if not self.training:
                main_result = main_result * (1 - self.drop_rate)
            residual_result = torch.add(main_result, residual_result)
        return residual_result


def _standardize_weights(W: torch.Tensor):
    """Function to standardize the input weight ``W``"""
    reduce_dims = list(range(1, W.dim()))
    W_var, W_mean = torch.var_mean(W, dim=reduce_dims, keepdim=True, unbiased=False)
    return (W - W_mean) / torch.sqrt(W_var + 1e-10)


class WeightStandardizer(nn.Module):
    """Class used to apply weight standardization with torch's parametrization package."""

    def forward(self, W):
        return _standardize_weights(W)


def check_for_index_targets(targets: torch.Tensor) ->bool:
    """Checks if a given set of targets are indices by looking at the type."""
    index_dtypes = [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64]
    return targets.dtype in index_dtypes


def _one_hot(tensor: torch.Tensor, num_classes: int=-1, dim: int=-1) ->torch.Tensor:
    """Converts a tensor of index class labels to a tensor of one-hot class labels.

    Implementation is based on MONAI one-hot conversion function:
    `<https://github.com/Project-MONAI/MONAI/blob/b390b0956334325edc0e5000afb58e2be7cbe550/monai/networks/utils.py#L49>`_.

    Args:
        tensor (torch.Tensor): Tensor containing index class labels.
        num_classes (int): Size of the class dimension for the output one-hot tensor. If set to -1,
            the number of classes will be inferred to be one greater than the largest value in ``tensor``.
        dim (int): Location of the new class dimension of size ``num_classes``.


    Returns:
        torch.Tensor: One-hot class labels i.e. the same shape as ``tensor`` except with an
            extra dimension of size ``num_classes`` inserted after the first dimension
    """
    if not check_for_index_targets(tensor):
        raise ValueError(f'tensor must be integer type, current type: {tensor.dtype}')
    max_index = tensor.max() + 1
    if num_classes == -1:
        num_classes = int(max_index)
    if num_classes < max_index:
        raise ValueError(f'num_classes must be greater than or equal to tensor.max() + 1: {num_classes} < {max_index}')
    neg_indices = tensor.min() < 0
    if neg_indices:
        warnings.warn('Negative label indices are being ignored in conversion to one-hot labels')
        tensor = tensor.clone().long()
        tensor[tensor < 0] = num_classes
        num_classes += 1
    tensor = tensor.unsqueeze(dim)
    tensor_shape = list(tensor.shape)
    tensor_shape[dim] = num_classes
    one_hot_tensor = torch.zeros(size=tensor_shape, dtype=tensor.dtype, device=tensor.device)
    one_hot_tensor.scatter_(dim=dim, index=tensor, value=1)
    if neg_indices:
        one_hot_tensor = one_hot_tensor[:, 0:-1]
    return one_hot_tensor


def infer_target_type(input: torch.Tensor, targets: torch.Tensor) ->str:
    """Infers whether the target is in indices format or one_hot format.

    Example indices format: [1, 4, 7] Example one_hot format [[0, 1, 0], [1, 0, 0], ...]
    """
    if input.shape == targets.shape:
        return 'one_hot'
    elif input.ndim == targets.ndim + 1:
        return 'indices'
    else:
        raise RuntimeError(f'Unable to infer indices or one_hot. Targets has shape {targets.shape} and the inputs to cross entropy has shape {input.shape}. For one_hot, expect targets.shape == inputs.shape. For indices, expect inputs.ndim == targets.ndim + 1')


def ensure_targets_one_hot(input: torch.Tensor, targets: torch.Tensor, num_classes: Optional[int]=None) ->torch.Tensor:
    """Ensures that the targets are in a one-hot format rather than an index format.

    Args:
        input (torch.Tensor): :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`
            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`
            in the case of K-dimensional loss. `input` is expected to contain unnormalized scores
            (often referred to as logits).
        targets (torch.Tensor) : If containing class indices, shape :math:`(N)` where each value is
            :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or :math:`(N, d_1, d_2, ..., d_K)` with
            :math:`K \\geq 1` in the case of K-dimensional loss. If containing class probabilities,
            same shape as the input.
        num_classes (int, optional): Number of classes. If not specified, this will be inferred
            from input. Default: ``None``
    """
    if infer_target_type(input, targets) == 'indices':
        if num_classes is None:
            num_classes = input.shape[1]
        targets = _one_hot(targets, num_classes=num_classes, dim=1)
    return targets.float()


class DiceLoss(_Loss):
    """Criterion that computes the dice loss between input and target.

    The implementation is derived from MONAI: `<https://docs.monai.io/en/stable/losses.html#diceloss>`_.
    For more information about the dice loss see the original paper on dice loss:
    `<https://arxiv.org/abs/1606.04797>`_.

    Args:
        sigmoid (bool): If true, apply a sigmoid function to the input. Default: ``False``
        softmax (bool): If true, apply a softmax function to the input. Default: ``False``
        squared_pred (bool): If true, square the inputs and targets when calculating the
            class unions. Default: ``False``
        jaccard (bool): If true, compute the jaccard index (soft IoU) instead of dice.
            Default: ``False``
        batch (bool): If true, sum the intersection and union areas over the batch
            dimension before dividing the two quantities. If false, a dice loss value is
            computed independently for each sample in the batch before the reduction.
        ignore_absent_classes (bool): If true, remove classes that are not present in
            the target from the loss calculation. Classes not present in the target do
            not contribute to the gradient, but can decrease the weight of present classes,
            slowing optimization. This should have no effect if all classes are present in
            each sample. Default: ``'False'``
        reduction (str): Specifies the reduction to apply to the output: ``'none'`` |
            ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be appied, ``'mean'``:
            the weighted mean of the output is taken, ``'sum'``: the output will be summed.
            Default: ``'mean'``

    """

    def __init__(self, sigmoid: bool=False, softmax: bool=False, squared_pred: bool=False, jaccard: bool=False, batch: bool=False, ignore_absent_classes: bool=False, reduction: str='mean'):
        super().__init__(reduction=reduction)
        if sigmoid and softmax:
            raise ValueError('Both sigmoid and softmax should not be true.')
        if not reduction in ['none', 'mean', 'sum']:
            raise ValueError(f'reduction was {reduction}, but must be one of ["none", "mean", "sum"]')
        self.sigmoid = sigmoid
        self.softmax = softmax
        self.squared_pred = squared_pred
        self.jaccard = jaccard
        self.reduction = reduction
        self.batch = batch
        self.ignore_absent_classes = ignore_absent_classes

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        target = ensure_targets_one_hot(input, target)
        target_mask = target.sum(dim=1, keepdim=True) != 0
        if input.shape != target.shape:
            raise AssertionError(f'ground truth has different shape ({target.shape}) from input ({input.shape})')
        if self.sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if self.softmax:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `softmax=True` ignored.')
            else:
                input = torch.softmax(input, 1)
        reduce_axis = torch.arange(2, len(input.shape)).tolist()
        if self.batch:
            reduce_axis = [0] + reduce_axis
        intersection = torch.sum(target * input, dim=reduce_axis)
        if self.squared_pred:
            target = torch.pow(target, 2)
            input = torch.pow(input, 2)
        input = target_mask * input
        ground_o = torch.sum(target, dim=reduce_axis)
        pred_o = torch.sum(input, dim=reduce_axis)
        union = ground_o + pred_o
        if self.jaccard:
            union = 2.0 * (union - intersection)
        epsilon = 1e-05
        ious = 1.0 - (2.0 * intersection + epsilon) / (union + epsilon)
        if self.ignore_absent_classes:
            if self.batch:
                ious = ious[ground_o > 0]
            else:
                ious = ious[:, ground_o.sum(dim=0) > 0]
        if self.reduction == 'mean':
            iou = torch.mean(ious)
        elif self.reduction == 'sum':
            iou = torch.sum(ious)
        elif self.reduction == 'none':
            iou = ious
        else:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return iou


Batch = Any


class Serializable:
    """Interface for serialization; used by checkpointing."""

    def state_dict(self) ->Dict[str, Any]:
        """Returns a dictionary representing the internal state.

        The returned dictionary must be pickale-able via :func:`torch.save`.

        Returns:
            Dict[str, Any]: The state of the object.
        """
        return {}

    def load_state_dict(self, state: Dict[str, Any]) ->None:
        """Restores the state of the object.

        Args:
            state (Dict[str, Any]): The state of the object, as previously returned by :meth:`.state_dict`.
        """
        pass


def _split_list(l, microbatch_size: int):
    if len(l) < microbatch_size:
        warnings.warn(f'Cannot split list of length {len(l)} into batches of size {microbatch_size}. As it is smaller, no splitting will be done.')
        microbatch_size = len(l)
    num_microbatches = math.ceil(len(l) / microbatch_size)
    chunked_microbatch_size = math.ceil(len(l) / num_microbatches)
    return [l[start:start + chunked_microbatch_size] for start in range(0, len(l), chunked_microbatch_size)]


def _split_tensor(t, microbatch_size: int):
    if len(t) < microbatch_size:
        warnings.warn(f'Cannot split tensor of length {len(t)} into batches of size {microbatch_size}. As it is smaller, no splitting will be done.')
        microbatch_size = len(t)
    num_microbatches = math.ceil(len(t) / microbatch_size)
    return t.chunk(num_microbatches)


def _split_mapping(m, microbatch_size: int):
    chunked = {}
    for k, v in m.items():
        if isinstance(v, torch.Tensor):
            chunked[k] = _split_tensor(v, microbatch_size)
        if isinstance(v, (List, Tuple)):
            chunked[k] = _split_list(v, microbatch_size)
    num_chunks = len(list(chunked.values())[0])
    return [{k: v[idx] for k, v in chunked.items()} for idx in range(num_chunks)]


def _default_split_batch(batch: Any, microbatch_size: int) ->Sequence:
    """Splits batch into chunks of size `microbatch_size` for gradient accumulation.

    Works with tensors, dictionaries of tensors, (x, y) tuples, and lists where ``batch`` is the 2nd dimension.

    Args:
        batch (Any): output from the dataloader.
        microbatch_size (int): Size of microbatches to batch into.
    """
    if isinstance(batch, torch.Tensor):
        return _split_tensor(batch, microbatch_size)
    elif isinstance(batch, Mapping):
        return _split_mapping(batch, microbatch_size)
    elif isinstance(batch, (Tuple, List)):
        result = []
        for item in batch:
            if isinstance(item, torch.Tensor):
                result.append(_split_tensor(item, microbatch_size))
            elif isinstance(item, (List, Tuple)):
                result.append(_split_list(item, microbatch_size))
            else:
                raise ValueError(f'Unsupported batch type: {type(item)}.')
        return list(zip(*result))
    raise NotImplementedError(textwrap.dedent("""            The default `split_fn` is unable to split the output of this dataloader. To enable microbatching,
             please and specify a `DataSpec` with `split_batch` for your dataset."""))


def _num_microbatches_split_list(l, num_microbatches: int):
    if len(l) < num_microbatches:
        raise ValueError(textwrap.dedent(f"""        Cannot split list of length {len(l)} into {num_microbatches} batches.
         make sure `grad_accum` is less than or equal to `train_batch_size // world_size`."""))
    chunked_microbatch_size = math.ceil(len(l) / num_microbatches)
    return [l[start:start + chunked_microbatch_size] for start in range(0, len(l), chunked_microbatch_size)]


def _num_microbatches_split_tensor(t, num_microbatches: int):
    if len(t) < num_microbatches:
        raise ValueError(textwrap.dedent(f"""        Cannot split tensor of length {len(t)} into {num_microbatches} batches.
         make sure `grad_accum` is less than or equal to `train_batch_size // world_size`."""))
    return t.chunk(num_microbatches)


def _num_microbatches_split_mapping(m, num_microbatches: int):
    chunked = {}
    for k, v in m.items():
        if isinstance(v, torch.Tensor):
            chunked[k] = _num_microbatches_split_tensor(v, num_microbatches)
        if isinstance(v, (List, Tuple)):
            chunked[k] = _num_microbatches_split_list(v, num_microbatches)
    num_chunks = len(list(chunked.values())[0])
    return [{k: v[idx] for k, v in chunked.items()} for idx in range(num_chunks)]


def _num_microbatches_split_batch(batch: Any, num_microbatches: int) ->Sequence:
    """Splits batch into `num_microbatches` chunks for gradient accumulation.

    Works with tensors, dictionaries of tensors, (x, y) tuples, and lists where ``batch`` is the 2nd dimension.

    Args:
        batch (Any): output from the dataloader.
        num_microbatches (int): number of microbatches to batch into. Will be set by `grad_accum`.
    """
    if num_microbatches < 1:
        raise ValueError('num_microbatches must be at least 1')
    if num_microbatches == 1:
        return [batch]
    if isinstance(batch, torch.Tensor):
        return _num_microbatches_split_tensor(batch, num_microbatches)
    if isinstance(batch, Mapping):
        return _num_microbatches_split_mapping(batch, num_microbatches)
    if isinstance(batch, (Tuple, List)):
        result = []
        for item in batch:
            if isinstance(item, torch.Tensor):
                result.append(_num_microbatches_split_tensor(item, num_microbatches))
            elif isinstance(item, (List, Tuple)):
                result.append(_num_microbatches_split_list(item, num_microbatches))
            else:
                raise ValueError(f'Unsupported batch type: {type(item)}.')
        return list(zip(*result))
    raise NotImplementedError(textwrap.dedent("""            The default `split_fn` is unable to split the output of this dataloader. To enable microbatching,
             please and specify a `DataSpec` with `split_batch` for your dataset and use `device_train_microbatch_size`."""))


def ensure_tuple(x):
    """Converts ``x`` into a tuple.

    * If ``x`` is ``None``, then ``tuple()`` is returned.
    * If ``x`` is a tuple, then ``x`` is returned as-is.
    * If ``x`` is a list, then ``tuple(x)`` is returned.
    * If ``x`` is a dict, then ``tuple(v for v in x.values())`` is returned.

    Otherwise, a single element tuple of ``(x,)`` is returned.

    Args:
        x (Any): The input to convert into a tuple.

    Returns:
        tuple: A tuple of ``x``.
    """
    if x is None:
        return ()
    if isinstance(x, (str, bytes, bytearray)):
        return x,
    if isinstance(x, collections.abc.Sequence):
        return tuple(x)
    if isinstance(x, dict):
        return tuple(x.values())
    return x,


class DataSpec:
    """Specifications for operating and training on data.

    An example of constructing a :class:`DataSpec` object with a ``device_transforms``
    callable (:class:`.NormalizationFn`) and then using it with :class:`~.Trainer`:

    .. doctest::

       >>> # In this case, we apply NormalizationFn
       >>> # Construct DataSpec as shown below to apply this transformation
       >>> from composer.datasets.utils import NormalizationFn
       >>> CHANNEL_MEAN = (0.485 * 255, 0.456 * 255, 0.406 * 255)
       >>> CHANNEL_STD = (0.229 * 255, 0.224 * 255, 0.225 * 255)
       >>> device_transform_fn = NormalizationFn(mean=CHANNEL_MEAN, std=CHANNEL_STD)
       >>> train_dspec = DataSpec(train_dataloader, device_transforms=device_transform_fn)
       >>> # The same function can be used for eval dataloader as well
       >>> eval_dspec = DataSpec(eval_dataloader, device_transforms=device_transform_fn)
       >>> # Use this DataSpec object to construct trainer
       >>> trainer = Trainer(
       ...     model=model,
       ...     train_dataloader=train_dspec,
       ...     eval_dataloader=eval_dspec,
       ...     optimizers=optimizer,
       ...     max_duration="1ep",
       ... )

    Args:
        dataloader (Iterable): The dataloader, which can be any iterable that yields batches.

        num_samples (int, optional): The total number of samples in an epoch, across all ranks. This field is used by
            the :class:`.Timestamp` (training progress tracker). If not specified, then ``len(dataloader.dataset)`` is
            used (if this property is available). Otherwise, the dataset is assumed to be unsized.

        num_tokens (int, optional): The total number of tokens in an epoch. This field is used by the
            :class:`.Timestamp` (training progress tracker).

        device_transforms ((Batch) -> Batch, optional): Function called by the :class:`.Trainer` to modify the
            batch once it has been moved onto the device. For example, this function can be used for GPU-based
            normalization. It can modify the batch in-place, and it should return the modified batch. If not specified,
            the batch is not modified.

        split_batch ((Batch, int) -> Sequence[Batch], optional): Function called by the :class:`.Trainer` to
            split a batch (the first parameter) into microbatches of a given size (the second parameter). If
            the ``dataloader`` yields batches not of type :class:`torch.Tensor`, Mapping, Tuple, or List, then
            this function must be specified.

        get_num_samples_in_batch ((Batch) -> int, optional): Function that is called by the :class:`.Trainer`
            to get the number of samples in the provided batch.

            By default, if the batch contains tensors that all have the same 0th dim, then the value of the 0th dim will
            be returned. If the batch contains tensors where the 0th dim differ, then this function must be specified.

        get_num_tokens_in_batch ((Batch) -> int, optional): Function that is called by the :class:`.Trainer` to
            get the number of tokens in the provided batch.

            By default, it returns 0, meaning that number of tokens processed will not be tracked as a part of the
            training progress tracking. This function must be specified to track the number of tokens processed during
            training.
    """

    def __init__(self, dataloader: Iterable, num_samples: Optional[int]=None, num_tokens: Optional[int]=None, device_transforms: Optional[Callable[[Batch], Batch]]=None, split_batch: Optional[Callable[[Batch, int], Sequence[Batch]]]=None, get_num_samples_in_batch: Optional[Callable[[Batch], int]]=None, get_num_tokens_in_batch: Optional[Callable[[Batch], int]]=None) ->None:
        self.dataloader = dataloader
        self.num_tokens = num_tokens
        self.device_transforms = self._default_device_transforms if device_transforms is None else device_transforms
        self.split_batch = _default_split_batch if split_batch is None else split_batch
        self._num_microbatches_split_batch = _num_microbatches_split_batch
        self.get_num_samples_in_batch = self._default_get_num_samples_in_batch if get_num_samples_in_batch is None else get_num_samples_in_batch
        self.get_num_tokens_in_batch = self._default_get_num_tokens_in_batch if get_num_tokens_in_batch is None else get_num_tokens_in_batch
        if num_samples is not None:
            self.num_samples = num_samples
        elif isinstance(dataloader, torch.utils.data.DataLoader) and isinstance(dataloader.dataset, collections.abc.Sized):
            try:
                self.num_samples = len(dataloader.dataset)
            except (TypeError, NotImplementedError):
                self.num_samples = None
        else:
            self.num_samples = None
        if isinstance(dataloader, torch.utils.data.DataLoader):
            if dataloader._iterator is not None:
                raise ValueError('The dataloader has an active iterator. This could occur if `persistent_workers=True` and the dataloader has already been iterated, or if the dataloader is mid-epoch. It is required that the training dataloader does not have an active iterator, so CPU dataset augmentations can be correctly inserted. To fix, please do not iterate over the dataloader before passing it into the Trainer.')
            world_size = dist.get_world_size()
            if world_size > 1 and not isinstance(dataloader.dataset, torch.utils.data.IterableDataset):
                is_sampler_distributed = dataloader.sampler is not None and isinstance(dataloader.sampler, DistributedSampler)
                is_batch_sampler_distributed = dataloader.batch_sampler is not None and isinstance(dataloader.batch_sampler, DistributedSampler)
                if not is_sampler_distributed and not is_batch_sampler_distributed:
                    raise ValueError(f'The world_size({world_size}) > 1 but dataloader does not use DistributedSampler. This will cause all ranks to train on the same data, removing any benefit from multi-GPU training. To resolve this, create a Dataloader with DistributedSampler. For example, DataLoader(..., sampler=composer.utils.dist.get_sampler(...)).Alternatively, the process group can be instantiated with composer.utils.dist.instantiate_dist(...) and DistributedSampler can directly be created with DataLoader(..., sampler=DistributedSampler(...)). For more information, see https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler.')

    def _default_device_transforms(self, batch: Batch):
        return batch

    def _default_get_num_samples_in_batch(self, batch: Batch) ->int:
        if isinstance(batch, torch.Tensor):
            return batch.shape[0]
        dim0_sizes = []
        if isinstance(batch, (list, tuple)):
            for tensors in batch:
                for t in ensure_tuple(tensors):
                    if not hasattr(t, 'shape'):
                        raise ValueError(f'Unable to determine the batch size, batch containsan element of type {type(t)}, which does not have ashape. Please use a DataSpec and provide a`get_num_samples_in_batch(your_batch) -> int` method.')
                    dim0_sizes.append(t.shape[0])
        elif isinstance(batch, dict):
            dim0_sizes = [t.shape[0] for t in batch.values()]
        if len(set(dim0_sizes)) == 1:
            return dim0_sizes[0]
        else:
            raise NotImplementedError(textwrap.dedent(f"""                    Cannot determine the batch size, as multiple Tensors of
                    different lengths were found in the batch: sizes in batch: {dim0_sizes}.
                    Please use a DataSpec and specify `get_num_samples_in_batch`."""))

    def _default_get_num_tokens_in_batch(self, batch: Batch) ->int:
        del batch
        return 0


T_Batch = TypeVar('T_Batch')


T_nnModule = TypeVar('T_nnModule', bound=torch.nn.Module)


def _map_batch(batch: Any, map_fn: Callable) ->Any:
    """Recursively maps a function to all items in a batch.

    Args:
        batch: Nested lists and dictionaries.
        map_fn: A function to invoke on each element.

    Returns:
        Collections: The result of applying ``map_fn`` on each element of the ``batch``.
        The type of ``batch`` is preserved.
    """
    if isinstance(batch, Mapping):
        return {k: _map_batch(v, map_fn) for k, v in batch.items()}
    if isinstance(batch, Sequence) and not isinstance(batch, (str, bytes)):
        try:
            return type(batch)(_map_batch(x, map_fn) for x in batch)
        except TypeError:
            return [_map_batch(x, map_fn) for x in batch]
    return map_fn(batch)


class Device(Serializable, ABC):
    """Abstract class for a device on which a model runs.

    Attributes:
        dist_backend (str): Distributed backend to use.
            Should be ``gloo``, ``mpi``, or ``nccl``.
            See `the pytorch docs <https://pytorch.org/docs/stable/distributed.html>`_
            for details.
    """
    dist_backend: str = ''

    @abstractmethod
    def module_to_device(self, module: T_nnModule) ->T_nnModule:
        """Invoked by the :class:`.Trainer` to move a ``module`` onto the device.

        Args:
            module (torch.nn.Module): The module to move to the device.

        Returns:
            torch.nn.Module: The module on the device.
        """
        pass

    @abstractmethod
    def tensor_to_device(self, tensor: torch.Tensor) ->torch.Tensor:
        """Invoked by the :class:`.Trainer` to move a tensor onto a device.

        Args:
            tensor (Tensor): The tensor to move to the device.

        Returns:
            Tensor: The tensor on the device.
        """
        pass

    def batch_to_device(self, batch: T_Batch) ->T_Batch:
        """Invoked by the :class:`.Trainer` move all tensors items in a batch to device.

        Supports nested sequences and mappings of tensors. Ignores non-tensor items. Preserves sequence and mapping types
        when possible; otherwise, sequences are converted to lists, and mappings are converted to dictionaries.

        Args:
            batch (Any): The batch to move to the device.

        Returns:
            Batch: The batch on the device.
        """

        def _to_device(x):
            if isinstance(x, torch.Tensor):
                return self.tensor_to_device(x)
            return x
        return _map_batch(batch, _to_device)

    def optimizer_to_device(self, optimizer: Optimizer) ->Optimizer:
        """Invoked by the :class:`.Trainer` to move the optimizer's state onto the device.

        Args:
            optimizer (Optimizer): The optimizer to move to the device

        Returns:
            Optimizer: The optimizer on the device
        """
        for state in optimizer.state.values():
            for k, v in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = self.tensor_to_device(v)
        return optimizer


TValue = TypeVar('TValue', int, float)


_NUM_REGEX = '-?[\\d.]+(?:e-?\\d+)?'


def ensure_data_spec(dataloader: Union[DataSpec, Iterable, dict]) ->DataSpec:
    """Ensures that the ``dataloader`` is a :class:`.DataSpec`.

    Args:
        dataloader (DataSpec | Iterable | dict): A DataSpec, DataLoader, or Dict of DataSpec kwargs.

    Returns:
        DataSpec: A DataSpec
    """
    if isinstance(dataloader, dict):
        dataloader = DataSpec(**dataloader)
    if not isinstance(dataloader, DataSpec):
        dataloader = DataSpec(dataloader)
    return dataloader


FORMAT_NAME_WITH_DIST_AND_TIME_TABLE = """
+----------------------------+------------------------------------------------------------+
| Variable                   | Description                                                |
+============================+============================================================+
| ``{run_name}``             | The name of the training run. See                          |
|                            | :attr:`.Logger.run_name`.                                  |
+----------------------------+------------------------------------------------------------+
| ``{rank}``                 | The global rank, as returned by                            |
|                            | :func:`~composer.utils.dist.get_global_rank`.              |
+----------------------------+------------------------------------------------------------+
| ``{local_rank}``           | The local rank of the process, as returned by              |
|                            | :func:`~composer.utils.dist.get_local_rank`.               |
+----------------------------+------------------------------------------------------------+
| ``{world_size}``           | The world size, as returned by                             |
|                            | :func:`~composer.utils.dist.get_world_size`.               |
+----------------------------+------------------------------------------------------------+
| ``{local_world_size}``     | The local world size, as returned by                       |
|                            | :func:`~composer.utils.dist.get_local_world_size`.         |
+----------------------------+------------------------------------------------------------+
| ``{node_rank}``            | The node rank, as returned by                              |
|                            | :func:`~composer.utils.dist.get_node_rank`.                |
+----------------------------+------------------------------------------------------------+
| ``{epoch}``                | The total epoch count, as returned by                      |
|                            | :meth:`~composer.core.time.Timestamp.epoch`.               |
+----------------------------+------------------------------------------------------------+
| ``{batch}``                | The total batch count, as returned by                      |
|                            | :meth:`~composer.core.time.Timestamp.batch`.               |
+----------------------------+------------------------------------------------------------+
| ``{batch_in_epoch}``       | The batch count in the current epoch, as returned by       |
|                            | :meth:`~composer.core.time.Timestamp.batch_in_epoch`.      |
+----------------------------+------------------------------------------------------------+
| ``{sample}``               | The total sample count, as returned by                     |
|                            | :meth:`~composer.core.time.Timestamp.sample`.              |
+----------------------------+------------------------------------------------------------+
| ``{sample_in_epoch}``      | The sample count in the current epoch, as returned by      |
|                            | :meth:`~composer.core.time.Timestamp.sample_in_epoch`.     |
+----------------------------+------------------------------------------------------------+
| ``{token}``                | The total token count, as returned by                      |
|                            | :meth:`~composer.core.time.Timestamp.token`.               |
+----------------------------+------------------------------------------------------------+
| ``{token_in_epoch}``       | The token count in the current epoch, as returned by       |
|                            | :meth:`~composer.core.time.Timestamp.token_in_epoch`.      |
+----------------------------+------------------------------------------------------------+
| ``{total_wct}``            | The total training duration in seconds, as returned by     |
|                            | :meth:`~composer.core.time.Timestamp.total_wct`.           |
+----------------------------+------------------------------------------------------------+
| ``{epoch_wct}``            | The epoch duration in seconds, as returned by              |
|                            | :meth:`~composer.core.time.Timestamp.epoch_wct`.           |
+----------------------------+------------------------------------------------------------+
| ``{batch_wct}``            | The batch duration in seconds, as returned by              |
|                            | :meth:`~composer.core.time.Timestamp.batch_wct`.           |
+----------------------------+------------------------------------------------------------+
"""


FORMAT_NAME_WITH_DIST_TABLE = """
+------------------------+-------------------------------------------------------+
| Variable               | Description                                           |
+========================+=======================================================+
| ``{run_name}``         | The name of the training run. See                     |
|                        | :attr:`.Logger.run_name`.                             |
+------------------------+-------------------------------------------------------+
| ``{rank}``             | The global rank, as returned by                       |
|                        | :func:`~composer.utils.dist.get_global_rank`.         |
+------------------------+-------------------------------------------------------+
| ``{local_rank}``       | The local rank of the process, as returned by         |
|                        | :func:`~composer.utils.dist.get_local_rank`.          |
+------------------------+-------------------------------------------------------+
| ``{world_size}``       | The world size, as returned by                        |
|                        | :func:`~composer.utils.dist.get_world_size`.          |
+------------------------+-------------------------------------------------------+
| ``{local_world_size}`` | The local world size, as returned by                  |
|                        | :func:`~composer.utils.dist.get_local_world_size`.    |
+------------------------+-------------------------------------------------------+
| ``{node_rank}``        | The node rank, as returned by                         |
|                        | :func:`~composer.utils.dist.get_node_rank`.           |
+------------------------+-------------------------------------------------------+
"""


def _get_dist_config(strict: bool=True) ->Dict[str, Any]:
    """Returns a dict of distributed settings (rank, world_size, etc.).

    If ``strict=True``, will error if a setting is not available (e.g. the
    environment variable is not set). Otherwise, will only return settings
    that are available.
    """
    settings = {'rank': dist.get_global_rank, 'local_rank': dist.get_local_rank, 'world_size': dist.get_world_size, 'local_world_size': dist.get_local_world_size, 'node_rank': dist.get_node_rank}
    dist_config = {}
    for name, func in settings.items():
        try:
            value = func()
        except dist.MissingEnvironmentError as e:
            if strict:
                raise e
        else:
            dist_config[name] = value
    return dist_config


def format_name_with_dist(format_str: str, run_name: str, **extra_format_kwargs: object):
    formatted_str = format_str.format(run_name=run_name, **_get_dist_config(strict=False), **extra_format_kwargs)
    return formatted_str


_STATE_DICT_SERIALIZED_ATTRIBUTES = ['model', 'optimizers', 'schedulers', 'algorithms', 'callbacks', 'scaler', 'timestamp']


def _ensure_backwards_compatible_checkpointing(state_dict: Dict[str, Any]):
    state = {}
    for k, v in state_dict.items():
        if k == '_is_model_ddp_wrapped':
            k = 'is_model_ddp'
        if k.startswith('_'):
            k = k[1:]
        state[k] = v
    return state


def _is_key_get_and_set_fn_pair(key):
    if all([callable(key_element) for key_element in key]):
        if len(key) == 2:
            return True
        else:
            raise ValueError(f"If key is a sequence of Callables, it should be of length 2' not {len(key)}")
    return False


def batch_get(batch: Any, key: Union[str, int, Callable, Any]):
    """Indexes into the batch given the key.

    >>> from composer.utils.batch_helpers import batch_get
    >>> batch_get([1,2,3], 1)
    2
    >>> batch_get({'a':1, 'b':7}, 'b')
    7
    >>> batch_get([{'a':1, 'b':7},{'c':5}], lambda x: x[1]['c'])
    5
    >>> batch_get([{'a':1, 'b':7},{'c':5}], (lambda x: x[1]['c'], lambda x: 10))
    5

    Args:
        batch (Any): An object that contains the input and label of the items in the batch.
            Can be any abritrary type that user creates, but we assume some sort of
            sequence (list, tuple, tensor, array), mapping (dictionary),
            or attribute store (object with data members, namedtuple).
        key (str | int | Tuple[Callable, Callable] | Any, optional): A key to index into the batch or a
                user-specified function to do the extracting. A pair of callables is also
                supported for cases where a get and set function pair are both passed
                (like in Algorithms). The getter is assumed to be the first of the pair.

    Returns:
        The part of the batch specified by the key. This could be any type
            depending on what the batch is composed of.
    """
    if isinstance(key, Sequence) and not isinstance(key, str) and _is_key_get_and_set_fn_pair(key):
        get_fn, _ = key
        return get_fn(batch)
    if isinstance(key, Callable):
        return key(batch)
    try:
        return itemgetter(key)(batch)
    except (IndexError, TypeError):
        try:
            return itemgetter(*key)(batch)
        except TypeError:
            try:
                return attrgetter(cast(str, key))(batch)
            except TypeError:
                return attrgetter(*key)(batch)


def _batch_set_tuple(batch: Any, key: Union[int, str], value: Any) ->Any:
    """Sets key value pairs in tuples and NamedTuples."""
    if hasattr(batch, '_fields'):
        if isinstance(key, str):
            batch = batch._replace(**{key: value})
        else:
            batch_list = list(batch)
            batch_list[key] = value
            batch = batch._make(batch_list)
    else:
        batch = list(batch)
        batch[key] = value
        batch = tuple(batch)
    return batch


def _batch_set(batch: Any, key: Any, value: Any) ->Any:
    """Sets a key value pair in a non-tuple batch."""
    if isinstance(batch, tuple):
        return _batch_set_tuple(batch, key, value)
    try:
        batch[key]
        batch[key] = value
    except TypeError as e:
        if 'object does not support item assignment' in str(e) or 'object is not subscriptable' in str(e):
            pass
        else:
            raise e
    else:
        return batch
    try:
        getattr(batch, key)
        setattr(batch, key, value)
    except (AttributeError, TypeError) as e:
        raise RuntimeError(f'Unable to set key {key} to value {value} on batch {batch}. Please specify a custom set_fn, if necessary.')
    else:
        return batch


def _batch_set_multiple(batch: Any, key: Any, value: Any) ->Any:
    """Sets multiple key value pairs in a non-tuple batch."""
    try:
        batch[key]
        batch[key] = value
        return batch
    except (IndexError, TypeError, KeyError):
        pass
    if not hasattr(value, '__len__') or isinstance(value, str):
        raise ValueError(f'value must be a sequence or array or tensor! and not {type(value)}')
    if len(key) != len(value):
        raise ValueError(f'value must be the same length as key ({len(key)}), but it is {len(value)} instead')
    for single_key, single_value in zip(key, value):
        batch = _batch_set(batch, single_key, single_value)
    return batch


def batch_set(batch: Any, key: Union[str, int, Callable, Any], value: Any) ->Any:
    """Indexes into the batch given the key and sets the element at that index to value.

    This is not an in-place operation for batches of type tuple as tuples are not mutable.

    >>> from composer.utils.batch_helpers import batch_set
    >>> batch_set([1,2,3], key=1, value=8)
    [1, 8, 3]
    >>> batch_set({'a':1, 'b':7}, key='b', value=11)
    {'a': 1, 'b': 11}
    >>> def setter(batch, value):
    ...     batch[1]['d'] = value
    ...     return batch
    ...
    >>> batch_set([{'a':1, 'b':7},{'d':3}], key=setter, value=20)
    [{'a': 1, 'b': 7}, {'d': 20}]
    >>> batch_set([{'a':1, 'b':7},{'d':3}], key=(lambda x: x[0]['b'], setter), value=20)
    [{'a': 1, 'b': 7}, {'d': 20}]


    Args:
        batch (Any): An object that contains the input and label of the items in the batch.
            Can be any abritrary type that user creates, but we assume some sort of
            sequence (list, tuple, tensor, array), mapping (dictionary),
            or attribute store (object with data members, namedtuple).
        key (str | int | Tuple[Callable, Callable] | Any, optional): A key to index into the batch or a user-specified function
            to do the setting. A pair of callables is also supported for cases where a get
            and set function pair are both passed (like in Algorithms). The setter is
            assumed to be the second of the pair.
        value (Any): The value that batch[key] or batch.key gets set to.

    Returns:
        batch (Any): updated batch with value set at key.

    """
    if isinstance(key, Sequence) and not isinstance(key, str) and _is_key_get_and_set_fn_pair(key):
        _, set_fn = key
        return set_fn(batch, value)
    if isinstance(key, Callable):
        return key(batch, value)
    if isinstance(key, Sequence) and not isinstance(key, str):
        return _batch_set_multiple(batch, key, value)
    else:
        return _batch_set(batch, key, value)


class ComposerEnv(NamedTuple):
    composer_version: str
    composer_commit_hash: Optional[str]
    node_world_size: int
    host_processor_model_name: str
    host_processor_core_count: int
    local_world_size: int
    accelerator_model_name: str
    cuda_device_count: int


def get_accel_model_name() ->str:
    """Query the accelerator name."""
    return accel_device_name(None) if cuda_available() else 'N/A'


def get_composer_commit_hash() ->Optional[str]:
    files = importlib_metadata.files('mosaicml')
    if files is None:
        return
    files = [f for f in files if str(f).endswith('direct_url.json')]
    if len(files) == 0:
        return
    f = files[0]
    direct_url = json.loads(f.read_text())
    vcs_info = direct_url.get('vcs_info', {})
    commit_id = vcs_info.get('commit_id')
    return commit_id


def get_composer_version() ->str:
    """Query the Composer version."""
    return str(composer.__version__)


def get_cuda_device_count() ->int:
    """Get the number of CUDA devices on the system."""
    return cuda_device_count() if TORCH_AVAILABLE else 0


def get_host_processor_cores() ->int:
    """Determines the number of physical host processor cores."""
    return psutil.cpu_count(logical=False)


@functools.lru_cache(maxsize=1)
def get_host_processor_name() ->str:
    """Query the host processor name."""
    cpu_info = cpuinfo.get_cpu_info()
    return str(cpu_info.get('brand_raw', 'CPU'))


class MissingEnvironmentError(Exception):
    pass


def _get_distributed_config_var(env_var: str, human_name: str, default: int, fetch_fn_name: Optional[str]=None) ->int:
    if not dist.is_available():
        return default
    if dist.is_initialized() and fetch_fn_name is not None:
        dist_value = int(getattr(dist, fetch_fn_name)())
        if env_var in os.environ:
            env_value = int(os.environ[env_var])
            if dist_value != env_value:
                raise RuntimeError(f'Torch distributed has been initialized with a value of {dist_value} for {human_name}, but environment variable {env_var} has value {env_value}.')
        return dist_value
    if env_var in os.environ:
        return int(os.environ[env_var])
    if dist.is_initialized():
        raise MissingEnvironmentError(f'Torch distributed is initialized but environment variable {env_var} is not set.')
    return default


def get_local_world_size() ->int:
    """Returns the local world size, which is the number of processes for the current node.

    Returns:
        int: The local world size.
    """
    return _get_distributed_config_var(env_var='LOCAL_WORLD_SIZE', default=1, human_name='local world size')


def get_node_world_size() ->int:
    """Query the number of nodes."""
    return int(dist.get_world_size() / dist.get_local_world_size())


def get_composer_env_dict() ->dict:
    """Query Composer pertinent system information as a dict."""
    mutable_dict = ComposerEnv(composer_version=get_composer_version(), composer_commit_hash=get_composer_commit_hash(), host_processor_model_name=get_host_processor_name(), host_processor_core_count=get_host_processor_cores(), node_world_size=get_node_world_size(), accelerator_model_name=get_accel_model_name(), local_world_size=get_local_world_size(), cuda_device_count=get_cuda_device_count())._asdict()
    return mutable_dict


def get_fsdp_full_optim_state_dict(model: torch.nn.Module, optim: torch.optim.Optimizer, rank0_only: bool=True):
    if version.parse(torch.__version__) < version.parse('1.13.0'):
        raise RuntimeError('To use FSDP with Composer, you must use torch>=1.13.0.')
    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
    return FSDP.full_optim_state_dict(model=model, optim=optim, rank0_only=rank0_only)


def get_fsdp_sharded_optim_state_dict(full_optim_state_dict: Dict[str, Any], model: torch.nn.Module):
    if version.parse(torch.__version__) < version.parse('1.13.0'):
        raise RuntimeError('To use FSDP with Composer, you must use torch>=1.13.0.')
    from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
    return FSDP.scatter_full_optim_state_dict(full_optim_state_dict=full_optim_state_dict, model=model)


log = logging.getLogger(__name__)


def soft_cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor]=None, size_average: Optional[bool]=None, ignore_index: int=-100, reduce: Optional[bool]=None, reduction: str='mean'):
    """Drop-in replacement for :class:`~.F.cross_entropy` that handles class indices or one-hot labels.

    .. note::

        This function will be obsolete with `this update <https://github.com/pytorch/pytorch/pull/61044>`_.

    Args:
        input (torch.Tensor) : :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`
            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`
            in the case of K-dimensional loss. `input` is expected to contain unnormalized scores
            (often referred to as logits).
        target (torch.Tensor) : If containing class indices, shape :math:`(N)` where each value is
            :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or :math:`(N, d_1, d_2, ..., d_K)` with
            :math:`K \\geq 1` in the case of K-dimensional loss. If containing class probabilities,
            same shape as the input.
        weight (torch.Tensor, optional): a manual rescaling weight given to each
            class. If given, has to be a Tensor of size `C`. Default: ``None``.
        size_average (bool, optional): Deprecated (see `reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there multiple elements per sample. If the field ``size_average``
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        ignore_index (int, optional): Specifies a target value that is ignored
            and does not contribute to the input gradient. When ``size_average`` is
            ``True``, the loss is averaged over non-ignored targets. Note that
            ``ignore_index`` is only applicable when the target contains class indices.
            Default: ``-100``
        reduce (bool, optional): Deprecated (see ``reduction``). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on `size_average`. When ``reduce`` is ``False``, returns a loss per
            batch element instead and ignores `size_average`. Default: ``True``
        reduction (str, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: ``size_average``
            and ``reduce`` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override ``reduction``. Default: ``'mean'``
    """
    target_type = infer_target_type(input, target)
    if target_type == 'indices':
        return F.cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
    elif target_type == 'one_hot':
        assert reduction in ['sum', 'mean', 'none'], f'{reduction} reduction not supported.'
        assert size_average is None, 'size_average is deprecated'
        assert reduce is None, 'reduce is deprecated'
        if ignore_index != -100:
            warnings.warn('ignore_index not supported when using dense labels. Ignoring targets with 0 probability.')
        xentropy = -(target * F.log_softmax(input, dim=1))
        if weight is not None:
            xentropy = torch.movedim(xentropy, 1, -1)
            xentropy *= weight
            xentropy = torch.movedim(xentropy, -1, 1)
        xentropy = xentropy.sum(dim=1)
        num_examples = torch.numel(xentropy)
        if reduction == 'sum':
            xentropy = xentropy.sum()
        elif reduction == 'mean':
            xentropy = xentropy.mean()
            total_prob = target.sum()
            if total_prob <= 0:
                raise ValueError('No targets have nonzero probability')
            if total_prob < num_examples:
                warnings.warn('Some targets have less than 1 total probability.')
            xentropy *= num_examples / total_prob
        return xentropy
    else:
        raise ValueError(f'Unrecognized target type {target_type}')


class SqueezeExcite(nn.Module):
    """Squeeze Excite Layer.

    Args:
        in_channels (int): Number of channels in the input tensor.
        latent_channels (int): Number of hidden channels.
        act_layer (torch.nn.Module): Activation layer to use in block.
    """

    def __init__(self, in_channels: int, latent_channels: int, act_layer: Callable[..., nn.Module]=nn.ReLU):
        super().__init__()
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv_reduce = nn.Conv2d(in_channels, latent_channels, kernel_size=1, bias=True)
        self.act1 = act_layer(inplace=True)
        self.conv_expand = nn.Conv2d(latent_channels, in_channels, kernel_size=1, bias=True)
        self.gate_fn = torch.nn.Sigmoid()

    def forward(self, x: torch.Tensor):
        out = self.global_avg_pool(x)
        out = self.conv_reduce(out)
        out = self.act1(out)
        out = self.conv_expand(out)
        out = x * self.gate_fn(out)
        return out


def calculate_same_padding(kernel_size, dilation, stride):
    """Calculates the amount of padding to use to get the "SAME" functionality in Tensorflow."""
    return (stride - 1 + dilation * (kernel_size - 1)) // 2


def drop_connect(inputs: torch.Tensor, drop_connect_rate: float, training: bool):
    """Randomly mask a set of samples. Provides similar regularization as stochastic depth.

    Args:
        input (torch.Tensor): Input tensor to mask.
        drop_connect_rate (float): Probability of droppping each sample.
        training (bool): Whether or not the model is training
    """
    if not training:
        return inputs
    keep_prob = 1 - drop_connect_rate
    rand_tensor = keep_prob + torch.rand([inputs.size()[0], 1, 1, 1], dtype=inputs.dtype, device=inputs.device)
    rand_tensor.floor_()
    output = inputs.div(keep_prob) * rand_tensor
    return output


class DepthwiseSeparableConv(nn.Module):
    """Depthwise Separable Convolution layer.

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels in the output tensor.
        kernel_size (int): Size of the convolving kernel.
        stride (int): Stride of the convolution.
        se_ratio (float): How much to scale `in_channels` for the hidden layer
            dimensionality of the squeeze-excite module.
        drop_connect_rate (float): Probability of dropping a sample before the
            identity connection, provides regularization similar to stochastic
            depth.
        act_layer (torch.nn.Module): Activation layer to use in block.
        norm_kwargs (dict): Normalization layer's keyword arguments.
        norm_layer (torch.nn.Module): Normalization layer to use in block.
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, se_ratio: float, drop_connect_rate: float, act_layer: Callable[..., nn.Module], norm_kwargs: dict, norm_layer: Callable[..., nn.Module]=nn.BatchNorm2d):
        super().__init__()
        self.drop_connect_rate = drop_connect_rate
        self.has_residual = in_channels == out_channels and stride == 1
        self.has_se = se_ratio is not None and se_ratio > 0.0
        padding = calculate_same_padding(kernel_size, dilation=1, stride=stride)
        self.conv_depthwise = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, groups=in_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
        self.bn1 = norm_layer(in_channels, **norm_kwargs)
        self.act1 = act_layer(inplace=True)
        if self.has_se:
            latent_channels = max(1, int(in_channels * se_ratio))
            self.se = SqueezeExcite(in_channels, latent_channels, act_layer)
        self.conv_pointwise = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False)
        self.bn2 = norm_layer(out_channels, **norm_kwargs)
        self.act2 = act_layer(inplace=True)

    def forward(self, input: torch.Tensor):
        residual = input
        out = self.conv_depthwise(input)
        out = self.bn1(out)
        out = self.act1(out)
        if self.has_se:
            out = self.se(out)
        out = self.conv_pointwise(out)
        out = self.bn2(out)
        out = self.act2(out)
        if self.has_residual:
            if self.drop_connect_rate > 0.0:
                out = drop_connect(out, self.drop_connect_rate, self.training)
            out += residual
        return out


def round_channels(channels: float, width_multiplier: float, divisor: int=8, min_value: Optional[int]=None) ->int:
    """Round number of channels after scaling with width multiplier.

    This function ensures that channel integers halfway in-between divisors is rounded up.

    Args:
        channels (float): Number to round.
        width_multiplier (float): Amount to scale `channels`.
        divisor (int): Number to make the output divisible by.
        min_value (int, optional): Minimum value the output can be. If not specified, defaults
            to the ``divisor``.
    """
    if not width_multiplier:
        return int(channels)
    channels *= width_multiplier
    min_value = min_value or divisor
    new_channels = max(min_value, int(channels + divisor / 2) // divisor * divisor)
    if new_channels < 0.9 * channels:
        new_channels += divisor
    return new_channels


class MBConvBlock(nn.Module):
    """Mobile Inverted Residual Bottleneck Block.

    This block is implemented as as defined in
    `MobileNetV2: Inverted Residuals and Linear Bottlenecks <https://arxiv.org/abs/1801.04381>`_ (Sandler et al, 2018).

    Args:
        in_channels (int): Number of channels in the input tensor.
        out_channels (int): Number of channels in the output tensor.
        kernel_size (int): Size of the convolving kernel.
        stride (int): Stride of the convolution.
        expand_ratio (int): How much to expand the input channels for the
            depthwise convolution.
        se_ratio (float): How much to scale `in_channels` for the hidden layer
            dimensionality of the squeeze-excite module.
        drop_connect_rate (float): Probability of dropping a sample before the
            identity connection, provides regularization similar to stochastic
            depth.
        act_layer (torch.nn.Module): Activation layer to use in block.
        norm_kwargs (dict): Normalization layer's keyword arguments.
        norm_layer (torch.nn.Module): Normalization layer to use in block.
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, expand_ratio: int, se_ratio: float, drop_connect_rate: float, act_layer: Callable[..., nn.Module], norm_kwargs: dict, norm_layer: Callable[..., nn.Module]=nn.BatchNorm2d):
        super().__init__()
        self.drop_connect_rate = drop_connect_rate
        self.has_residual = in_channels == out_channels and stride == 1
        self.has_se = se_ratio is not None and se_ratio > 0.0
        mid_channels = round_channels(in_channels, expand_ratio)
        self.conv1x1_expand = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)
        self.bn1 = norm_layer(mid_channels, **norm_kwargs)
        self.act1 = act_layer(inplace=True)
        padding = calculate_same_padding(kernel_size, dilation=1, stride=stride)
        self.conv_depthwise = nn.Conv2d(in_channels=mid_channels, out_channels=mid_channels, groups=mid_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
        self.bn2 = norm_layer(mid_channels, **norm_kwargs)
        self.act2 = act_layer(inplace=True)
        if self.has_se:
            latent_channels = max(1, int(in_channels * se_ratio))
            self.se = SqueezeExcite(mid_channels, latent_channels, act_layer)
        self.conv1x1_contract = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)
        self.bn3 = norm_layer(out_channels, **norm_kwargs)

    def forward(self, input: torch.Tensor):
        residual = input
        out = self.conv1x1_expand(input)
        out = self.bn1(out)
        out = self.act1(out)
        out = self.conv_depthwise(out)
        out = self.bn2(out)
        out = self.act2(out)
        if self.has_se:
            out = self.se(out)
        out = self.conv1x1_contract(out)
        out = self.bn3(out)
        if self.has_residual:
            if self.drop_connect_rate:
                out = drop_connect(out, self.drop_connect_rate, self.training)
            out += residual
        return out


class EfficientNet(nn.Module):
    """EfficientNet model based on (`Tan et al, 2019 <https://arxiv.org/abs/1905.11946>`_).

    Args:
        num_classes (int): Size of the EfficientNet output, typically viewed
             as the number of classes in a classification task.
        width_multiplier (float, optional): How much to scale the EfficientNet-B0 channel
             dimension throughout the model. Default: ``1.0``.
        depth_multiplier (float, optional): How much to scale the EFficientNet-B0 depth. Default: ``1.0``.
        drop_rate (float, optional): Dropout probability for the penultimate activations. Default: ``0.2``.
        drop_connect_rate (float, optional): Probability of dropping a sample before the
             identity connection, provides regularization similar to stochastic
             depth. Default: ``0.2``.
        act_layer (torch.nn.Module, optional): Activation layer to use in the model. Default: ``nn.SiLU``.
        norm_kwargs (dict, optional): Normalization layer's keyword arguments. Default: ``{"momentum": 0.1, "eps": 1e-5}``.
        norm_layer (torch.nn.Module, optional): Normalization layer to use in the model. Default: ``nn.BatchNorm2d``.
    """
    _blocks_strings = ['r1_k3_s1_e1_i32_o16_se0.25', 'r2_k3_s2_e6_i16_o24_se0.25', 'r2_k5_s2_e6_i24_o40_se0.25', 'r3_k3_s2_e6_i40_o80_se0.25', 'r3_k5_s1_e6_i80_o112_se0.25', 'r4_k5_s2_e6_i112_o192_se0.25', 'r1_k3_s1_e6_i192_o320_se0.25']

    def __init__(self, num_classes: int, width_multiplier: float=1.0, depth_multiplier: float=1.0, drop_rate: float=0.2, drop_connect_rate: float=0.2, act_layer: Callable[..., nn.Module]=nn.SiLU, norm_kwargs: Optional[dict]=None, norm_layer: Callable[..., nn.Module]=nn.BatchNorm2d):
        super(EfficientNet, self).__init__()
        self.num_classes = num_classes
        if norm_kwargs is None:
            norm_kwargs = {'momentum': 0.1, 'eps': 1e-05}
        in_channels = 3
        out_channels = round_channels(32, width_multiplier)
        padding = calculate_same_padding(kernel_size=3, dilation=1, stride=2)
        self.conv_stem = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=padding, bias=False)
        self.bn1 = norm_layer(num_features=out_channels, **norm_kwargs)
        self.act1 = act_layer(inplace=True)
        block_count = 0.0
        for block_string in self._blocks_strings:
            _, num_repeat = self._decode_block_string(block_string)
            block_count += num_repeat
        block_idx = 0.0
        blocks = []
        block_args = {}
        for block_string in self._blocks_strings:
            block_args, num_repeat = self._decode_block_string(block_string)
            block_args['in_channels'] = round_channels(block_args['in_channels'], width_multiplier)
            block_args['out_channels'] = round_channels(block_args['out_channels'], width_multiplier)
            num_repeat = int(math.ceil(depth_multiplier * num_repeat))
            block_args['act_layer'] = act_layer
            block_args['norm_kwargs'] = norm_kwargs
            block_args['norm_layer'] = norm_layer
            if block_args['expand_ratio'] == 1:
                del block_args['expand_ratio']
            for i in range(num_repeat):
                block_args['drop_connect_rate'] = drop_connect_rate * block_idx / block_count
                if 'expand_ratio' not in block_args:
                    blocks.append(DepthwiseSeparableConv(**block_args))
                else:
                    blocks.append(MBConvBlock(**block_args))
                block_idx += 1
                if i == 0:
                    block_args['stride'] = 1
                    block_args['in_channels'] = block_args['out_channels']
        self.blocks = nn.Sequential(*blocks)
        in_channels = block_args['out_channels']
        out_channels = round_channels(1280, width_multiplier)
        self.conv_head = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn2 = norm_layer(out_channels, **norm_kwargs)
        self.act2 = act_layer(inplace=True)
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(drop_rate)
        self.classifier = nn.Linear(out_channels, num_classes)
        for m in self.modules():
            if isinstance(m, torch.nn.Conv2d):
                fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels // m.groups
                m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, torch.nn.BatchNorm2d):
                m.weight.data.fill_(1.0)
                m.bias.data.zero_()
            elif isinstance(m, torch.nn.Linear):
                fan_out = m.weight.size(0)
                init_range = 1.0 / math.sqrt(fan_out)
                m.weight.data.uniform_(-init_range, init_range)
                m.bias.data.zero_()

    def extract_features(self, input: torch.Tensor):
        out = self.conv_stem(input)
        out = self.bn1(out)
        out = self.act1(out)
        out = self.blocks(out)
        out = self.conv_head(out)
        out = self.bn2(out)
        out = self.act2(out)
        out = self.global_avg_pool(out)
        return out.flatten(1)

    def forward(self, input: torch.Tensor):
        out = self.extract_features(input)
        out = self.dropout(out)
        return self.classifier(out)

    @staticmethod
    def get_model_from_name(model_name: str, num_classes, drop_connect_rate: float):
        """Instantiate an EfficientNet model family member based on the model_name string.

        Args:
            model_name: (str): One of ``'efficientnet-b0'`` through ``'efficientnet-b7'``.
            num_classes (int): Size of the EfficientNet output, typically viewed as the number of classes in a classification task.
            drop_connect_rate (float): Probability of dropping a sample before the identity connection,
                provides regularization similar to stochastic depth.
        """
        model_arch = {'efficientnet-b0': (1.0, 1.0, 224, 0.2), 'efficientnet-b1': (1.0, 1.1, 240, 0.2), 'efficientnet-b2': (1.1, 1.2, 260, 0.3), 'efficientnet-b3': (1.2, 1.4, 300, 0.3), 'efficientnet-b4': (1.4, 1.8, 380, 0.4), 'efficientnet-b5': (1.6, 2.2, 456, 0.4), 'efficientnet-b6': (1.8, 2.6, 528, 0.5), 'efficientnet-b7': (2.0, 3.1, 600, 0.5)}
        model_params = model_arch[model_name]
        width_multiplier = model_params[0]
        depth_multiplier = model_params[1]
        drop_rate = model_params[3]
        return EfficientNet(num_classes=num_classes, width_multiplier=width_multiplier, depth_multiplier=depth_multiplier, drop_rate=drop_rate, drop_connect_rate=drop_connect_rate)

    def _decode_block_string(self, block_string: str):
        """Decodes an EfficientNet block specification string into a dictionary of keyword arguments for a block in the
        architecture."""
        arg_strings = block_string.split('_')
        args = {}
        for arg_string in arg_strings:
            splits = re.split('(\\d.*)', arg_string)
            if len(splits) >= 2:
                key, value = splits[:2]
                args[key] = value
        num_repeat = int(args['r'])
        block_args = {'kernel_size': int(args['k']), 'stride': int(args['s']), 'expand_ratio': int(args['e']), 'in_channels': int(args['i']), 'out_channels': int(args['o']), 'se_ratio': float(args['se']) if 'se' in args else None}
        return block_args, num_repeat


class MissingConditionalImportError(ImportError):
    """Handles errors for external packages that might not be installed.

    Args:
        extra_deps_group (str): the pip package group, found in setup.py. For example, nlp for `mosaicml[nlp]`.
        conda_package (str, optional): The package(s) to install if using conda.
        conda_channel (str, optional): The conda channel to install packages from. Set to ``None`` if the
            package is not published on conda and must be installed via pip.
    """

    def __init__(self, extra_deps_group: str, conda_package: str, conda_channel: Optional[str]='conda-forge'):
        if conda_channel:
            conda_command = f'conda install -c {conda_channel} {conda_package}'
        else:
            conda_command = f'pip install {conda_package}'
        super().__init__(f"Composer was installed without {extra_deps_group} support. To use {extra_deps_group} related packages, with Composer, run `pip install 'mosaicml[{extra_deps_group}]'` if using pip or `{conda_command}` if using Anaconda.")


def _get_callback(description: str):
    if len(description) > 60:
        description = description[:42] + '...' + description[-15:]
    pbar = None

    def callback(num_bytes: int, total_size: int):
        nonlocal pbar
        if num_bytes == 0 or pbar is None:
            pbar = tqdm.tqdm(desc=description, total=total_size, unit='iB', unit_scale=True)
        n = num_bytes - pbar.n
        pbar.update(n)
        if num_bytes == total_size:
            pbar.close()
    return callback


def iterate_with_callback(iterator, total_len, callback=None):
    """Invoke ``callback`` after each chunk is yielded from ``iterator``.

    Args:
        iterator (Iterator): The iterator, which should yield chunks of data.
        total_len (int): The total length of the iterator.
        callback (Callable[[int, int], None], optional): The callback to invoke after
            each chunk of data is yielded back to the caller. Defaults to None, for no callback.

            It is called with the cumulative size of all chunks yielded thus far and the ``total_len``.
    """
    current_len = 0
    if callback is not None:
        callback(current_len, total_len)
    for chunk in iterator:
        current_len += len(chunk)
        yield chunk
        if callback is not None:
            callback(current_len, total_len)


class ObjectStoreTransientError(RuntimeError):
    """Custom exception class to signify transient errors.

    Implementations of the :class:`.ObjectStore` should re-raise any transient exceptions
    (e.g. too many requests, temporarily unavailable) with this class, so callers can easily
    detect whether they should attempt to retry any operation.

    For example, the :class:`.S3ObjectStore` does the following:

    .. testcode::

        from composer.utils import ObjectStore, ObjectStoreTransientError
        import botocore.exceptions

        class S3ObjectStore(ObjectStore):

            def upload_object(self, file_path: str, object_name: str):
                try:
                    ...
                except botocore.exceptions.ClientError as e:
                    if e.response['Error']['Code'] == 'LimitExceededException':
                        raise ObjectStoreTransientError(e.response['Error']['Code']) from e
                    raise e

    Then, callers can automatically handle exceptions:

    .. testcode::

        import time
        from composer.utils import ObjectStore, ObjectStoreTransientError

        def upload_file(object_store: ObjectStore, max_num_attempts: int = 3):
            for i in range(max_num_attempts):
                try:
                    object_store.upload_object(...)
                except ObjectStoreTransientError:
                    if i + 1 == max_num_attempts:
                        raise
                    else:
                        # Try again after exponential back-off
                        time.sleep(2**i)
                else:
                    # upload successful
                    return
    """
    pass


_NOT_FOUND_CODES = '403', '404', 'NoSuchKey'


def _ensure_not_found_errors_are_wrapped(uri: str, e: Exception):
    if isinstance(e, botocore.exceptions.ClientError):
        if e.response['Error']['Code'] in _NOT_FOUND_CODES:
            raise FileNotFoundError(f'Object {uri} not found') from e
    raise e


def parse_uri(uri: str) ->Tuple[str, str, str]:
    """Uses :py:func:`urllib.parse.urlparse` to parse the provided URI.

    Args:
        uri (str): The provided URI string

    Returns:
        Tuple[str, str, str]: A tuple containing the backend (e.g. s3), bucket name, and path.
                              Backend and bucket name will be empty string if the input is a local path
    """
    parse_result = urlparse(uri)
    backend, net_loc, path = parse_result.scheme, parse_result.netloc, parse_result.path
    bucket_name = net_loc if '@' not in net_loc else net_loc.split('@')[0]
    if backend == '' and bucket_name == '':
        return backend, bucket_name, path
    else:
        return backend, bucket_name, path.lstrip('/')


def import_object(name: str) ->Any:
    """Dynamically import a Python object (e.g. class, function, ...).

    .. note::

        To dynamically import a module, use :func:`importlib.import_module`.

    Args:
        name (str): The path to the Python object to import.

            Separate the module name and class name with a ``':'`` (e.g. ``'path.to.module:function_name'``).

            Example:

                >>> from composer.utils import import_object
                >>> import_object('functools:partial')
                <class 'functools.partial'>

            .. note::

                The module name must be discoverale with the Python path, as determined by :attr:`sys.path`.

    Returns:
        Any: The imported object.
    """
    module_name, object_name = name.split(':')
    module = importlib.import_module(module_name)
    return getattr(module, object_name)


class ResNet9(nn.Module):
    """A 9-layer residual network, excluding BatchNorms and activation functions.

    Based on the myrtle.ai `blog`_ and Deep Residual Learning for Image Recognition (`He et al, 2015`_).

    Args:
        num_classes (int, optional): The number of classes. Needed for classification tasks. Default: ``10``.

    .. _blog: https://myrtle.ai/learn/how-to-train-your-resnet-4-architecture/
    .. _He et al, 2015: https://arxiv.org/abs/1512.03385
    """

    def __init__(self, num_classes: int=10):
        super().__init__()
        self.body = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(num_features=64, momentum=0.9), nn.ReLU(inplace=True), nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(num_features=128, momentum=0.9), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), BasicBlock(inplanes=128, planes=128, stride=1), nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(num_features=256, momentum=0.9), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False), nn.BatchNorm2d(num_features=256, momentum=0.9), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), BasicBlock(inplanes=256, planes=256, stride=1))
        self.fc = nn.Linear(in_features=256, out_features=num_classes, bias=True)

    def forward(self, x):
        out = self.body(x)
        out = F.avg_pool2d(out, out.size()[3])
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out


convolutions = {'Conv2d': nn.Conv2d, 'Conv3d': nn.Conv3d, 'ConvTranspose2d': nn.ConvTranspose2d, 'ConvTranspose3d': nn.ConvTranspose3d}


def get_padding(kernel_size, stride):
    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = (kernel_size_np - stride_np + 1) / 2
    padding = tuple(int(p) for p in padding_np)
    return padding if len(padding) > 1 else padding[0]


def get_conv(in_channels, out_channels, kernel_size, stride, dim, bias=False):
    conv = convolutions[f'Conv{dim}d']
    padding = get_padding(kernel_size, stride)
    return conv(in_channels, out_channels, kernel_size, stride, padding, bias=bias)


normalizations = {'instancenorm3d': nn.InstanceNorm3d, 'instancenorm2d': nn.InstanceNorm2d, 'batchnorm3d': nn.BatchNorm3d, 'batchnorm2d': nn.BatchNorm2d}


def get_norm(name, out_channels):
    if 'groupnorm' in name:
        return nn.GroupNorm(32, out_channels, affine=True)
    return normalizations[name](out_channels, affine=True)


class ConvLayer(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride, **kwargs):
        super(ConvLayer, self).__init__()
        self.conv = get_conv(in_channels, out_channels, kernel_size, stride, kwargs['dim'])
        self.norm = get_norm(kwargs['norm'], out_channels)
        self.lrelu = nn.LeakyReLU(negative_slope=kwargs['negative_slope'], inplace=True)

    def forward(self, data):
        out = self.conv(data)
        out = self.norm(out)
        out = self.lrelu(out)
        return out


class ConvBlock(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride, **kwargs):
        super(ConvBlock, self).__init__()
        self.conv1 = ConvLayer(in_channels, out_channels, kernel_size, stride, **kwargs)
        self.conv2 = ConvLayer(out_channels, out_channels, kernel_size, 1, **kwargs)

    def forward(self, input_data):
        out = self.conv1(input_data)
        out = self.conv2(out)
        return out


class ResidBlock(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride, **kwargs):
        super(ResidBlock, self).__init__()
        self.conv1 = ConvLayer(in_channels, out_channels, kernel_size, stride, **kwargs)
        self.conv2 = get_conv(out_channels, out_channels, kernel_size, 1, kwargs['dim'])
        self.norm = get_norm(kwargs['norm'], out_channels)
        self.lrelu = nn.LeakyReLU(negative_slope=kwargs['negative_slope'], inplace=True)
        self.downsample = None
        if max(stride) > 1 or in_channels != out_channels:
            self.downsample = get_conv(in_channels, out_channels, kernel_size, stride, kwargs['dim'])
            self.norm_res = get_norm(kwargs['norm'], out_channels)

    def forward(self, input_data):
        residual = input_data
        out = self.conv1(input_data)
        out = self.conv2(out)
        out = self.norm(out)
        if self.downsample is not None:
            residual = self.downsample(residual)
            residual = self.norm_res(residual)
        out = self.lrelu(out + residual)
        return out


def get_output_padding(kernel_size, stride, padding):
    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = np.atleast_1d(padding)
    out_padding_np = 2 * padding_np + stride_np - kernel_size_np
    out_padding = tuple(int(p) for p in out_padding_np)
    return out_padding if len(out_padding) > 1 else out_padding[0]


def get_transp_conv(in_channels, out_channels, kernel_size, stride, dim):
    conv = convolutions[f'ConvTranspose{dim}d']
    padding = get_padding(kernel_size, stride)
    output_padding = get_output_padding(kernel_size, stride, padding)
    return conv(in_channels, out_channels, kernel_size, stride, padding, output_padding, bias=True)


class UpsampleBlock(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride, **kwargs):
        super(UpsampleBlock, self).__init__()
        self.transp_conv = get_transp_conv(in_channels, out_channels, stride, stride, kwargs['dim'])
        self.conv_block = ConvBlock(2 * out_channels, out_channels, kernel_size, 1, **kwargs)

    def forward(self, input_data, skip_data):
        out = self.transp_conv(input_data)
        out = torch.cat((out, skip_data), dim=1)
        out = self.conv_block(out)
        return out


class OutputBlock(nn.Module):

    def __init__(self, in_channels, out_channels, dim):
        super(OutputBlock, self).__init__()
        self.conv = get_conv(in_channels, out_channels, kernel_size=1, stride=1, dim=dim, bias=True)
        nn.init.constant_(self.conv.bias, 0)

    def forward(self, input_data):
        return self.conv(input_data)


def _stat_scores(preds: Tensor, targets: Tensor, class_index: int, argmax_dim: int=1) ->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    if preds.ndim == targets.ndim + 1:
        preds = to_categorical(preds, argmax_dim=argmax_dim)
    tp = ((preds == class_index) * (targets == class_index)).sum()
    fp = ((preds == class_index) * (targets != class_index)).sum()
    tn = ((preds != class_index) * (targets != class_index)).sum()
    fn = ((preds != class_index) * (targets == class_index)).sum()
    sup = (targets == class_index).sum()
    return tp, fp, tn, fn, sup


class Mean(torch.nn.Module):

    def forward(self, x):
        return torch.mean(x, dim=1)


class SimpleTransformerBase(torch.nn.Module):
    """Base encoding transformer model for testing"""

    def __init__(self, vocab_size: int=100, d_model: int=16):
        super().__init__()
        embedding = torch.nn.Embedding(vocab_size, 16)
        layer = torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=2, dim_feedforward=d_model, dropout=0.3)
        layer.__constants__ = []
        transformer = torch.nn.TransformerEncoder(layer, num_layers=2, norm=torch.nn.LayerNorm(d_model))
        transformer.__constants__ = []
        self.net = torch.nn.Sequential(embedding, transformer)
        self.embedding = embedding
        self.transformer = transformer

    def forward(self, batch: torch.Tensor) ->torch.Tensor:
        return self.net(batch)


class MinimalConditionalModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.choice1 = nn.Linear(1, 1, bias=False)
        self.choice2 = nn.Linear(1, 1, bias=False)
        self.choice3 = nn.Linear(1, 1, bias=False)
        nn.init.constant_(self.choice1.weight, 0)
        nn.init.constant_(self.choice2.weight, 0)
        nn.init.constant_(self.choice3.weight, 0)

    def forward(self, input: int):
        if input == 1:
            return self.choice1(Tensor([1]))
        if input == 2:
            return self.choice2(Tensor([1]))
        if input == 3:
            return self.choice3(Tensor([1]))
        raise Exception('Invalid input:', input)

    def loss(self, output: Tensor, target: Tensor):
        return (output - target) * (output - target)


class MyTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.relu = nn.ReLU()
        self.factor = 0.5

    def forward(self, x):
        x = torch.add(x, self.factor)
        return self.relu(x)


class AddModel(nn.Module):

    def forward(self, x, y):
        return x + y, torch.add(x, y), x.add(y)


class SimpleParallelLinears(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(64, 64)
        self.fc2 = nn.Linear(64, 64)

    def forward(self, x):
        y = self.fc1(x)
        z = self.fc2(x)
        return y + z


class ParallelLinears(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(64, 64)
        self.ln = nn.LayerNorm(64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 64)

    def forward(self, x):
        y = self.fc1(x)
        y = self.ln(y)
        y = self.relu(y)
        z = self.fc2(x)
        return y + z


class NotFusibleLinears(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(64, 64, bias=False)
        self.ln = nn.LayerNorm(64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 64)

    def forward(self, x):
        y = self.fc1(x)
        y = self.ln(y)
        y = self.relu(y)
        z = self.fc2(x)
        return y + z


class NotParallelLinears(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(64, 64)
        self.ln = nn.LayerNorm(64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 64)

    def forward(self, x):
        y = self.fc1(x)
        y = self.ln(y)
        y = self.relu(y)
        z = self.fc2(y)
        return x + z


class LinModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.lin1 = nn.Linear(256, 128)
        self.lin2 = nn.Linear(128, 256)

    def forward(self, x):
        x = self.lin1(x)
        x = self.lin2(x)
        return x


class RecursiveLinear(nn.Linear):

    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features)
        self.submodule = nn.Linear(in_features, out_features - 1)


class _CopyLinear(torch.nn.Module):

    def __init__(self, in_features: int, out_features: int):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = torch.nn.parameter.Parameter(torch.empty((out_features, in_features)))
        self.bias = None

    @staticmethod
    def from_linear(module: torch.nn.Module, module_index: int=-1):
        assert isinstance(module.in_features, int)
        assert isinstance(module.out_features, int)
        ret = _CopyLinear(in_features=module.in_features, out_features=module.out_features)
        with torch.no_grad():
            assert isinstance(module.weight, torch.Tensor)
            ret.weight.copy_(module.weight)
            ret.bias = module.bias
        return ret


class ParamTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(8, 8)
        self.fc2 = nn.Linear(16, 16)
        self.fc3 = nn.Linear(32, 32)
        self.fc4 = nn.Linear(64, 64)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AddModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (BlockStochasticModule,
     lambda: ([], {'main': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BlurMaxPool2d,
     lambda: ([], {'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BlurPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DiceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (EfficientNet,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (GyroDropoutLayer,
     lambda: ([], {'iters_per_epoch': 4, 'max_epoch': 4, 'p': 4, 'sigma': 4, 'tau': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Mean,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MyTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (RecursiveLinear,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ResNet9,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (SqueezeExcite,
     lambda: ([], {'in_channels': 4, 'latent_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SqueezeExcite2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SqueezeExciteConv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (WeightStandardizer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_mosaicml_composer(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])


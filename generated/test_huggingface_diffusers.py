import sys
_module = sys.modules[__name__]
del sys
base_classes = _module
benchmark_controlnet = _module
benchmark_ip_adapters = _module
benchmark_sd_img = _module
benchmark_sd_inpainting = _module
benchmark_t2i_adapter = _module
benchmark_t2i_lcm_lora = _module
benchmark_text_to_image = _module
push_results = _module
run_all = _module
utils = _module
_config = _module
test_dreambooth_lora_flux_advanced = _module
train_dreambooth_lora_flux_advanced = _module
train_dreambooth_lora_sd15_advanced = _module
train_dreambooth_lora_sdxl_advanced = _module
train_amused = _module
train_cogvideox_image_to_video_lora = _module
train_cogvideox_lora = _module
adaptive_mask_inpainting = _module
bit_diffusion = _module
checkpoint_merger = _module
clip_guided_images_mixing_stable_diffusion = _module
clip_guided_stable_diffusion = _module
clip_guided_stable_diffusion_img2img = _module
composable_stable_diffusion = _module
ddim_noise_comparative_analysis = _module
dps_pipeline = _module
edict_pipeline = _module
fresco_v2v = _module
gluegen = _module
hd_painter = _module
iadb = _module
imagic_stable_diffusion = _module
img2img_inpainting = _module
instaflow_one_step = _module
interpolate_stable_diffusion = _module
ip_adapter_face_id = _module
kohya_hires_fix = _module
latent_consistency_img2img = _module
latent_consistency_interpolate = _module
latent_consistency_txt2img = _module
llm_grounded_diffusion = _module
lpw_stable_diffusion = _module
lpw_stable_diffusion_onnx = _module
lpw_stable_diffusion_xl = _module
magic_mix = _module
marigold_depth_estimation = _module
masked_stable_diffusion_img2img = _module
masked_stable_diffusion_xl_img2img = _module
matryoshka = _module
mixture_canvas = _module
mixture_tiling = _module
multilingual_stable_diffusion = _module
one_step_unet = _module
pipeline_animatediff_controlnet = _module
pipeline_animatediff_img2video = _module
pipeline_animatediff_ipex = _module
pipeline_demofusion_sdxl = _module
pipeline_fabric = _module
pipeline_flux_differential_img2img = _module
pipeline_flux_with_cfg = _module
pipeline_hunyuandit_differential_img2img = _module
pipeline_kolors_differential_img2img = _module
pipeline_null_text_inversion = _module
pipeline_prompt2prompt = _module
pipeline_sdxl_style_aligned = _module
pipeline_stable_diffusion_3_differential_img2img = _module
pipeline_stable_diffusion_boxdiff = _module
pipeline_stable_diffusion_pag = _module
pipeline_stable_diffusion_upscale_ldm3d = _module
pipeline_stable_diffusion_xl_controlnet_adapter = _module
pipeline_stable_diffusion_xl_controlnet_adapter_inpaint = _module
pipeline_stable_diffusion_xl_differential_img2img = _module
pipeline_stable_diffusion_xl_instandid_img2img = _module
pipeline_stable_diffusion_xl_instantid = _module
pipeline_stable_diffusion_xl_ipex = _module
pipeline_zero1to3 = _module
regional_prompting_stable_diffusion = _module
rerender_a_video = _module
run_onnx_controlnet = _module
run_tensorrt_controlnet = _module
scheduling_ufogen = _module
sd_text2img_k_diffusion = _module
sde_drag = _module
seed_resize_stable_diffusion = _module
speech_to_image_diffusion = _module
stable_diffusion_comparison = _module
stable_diffusion_controlnet_img2img = _module
stable_diffusion_controlnet_inpaint = _module
stable_diffusion_controlnet_inpaint_img2img = _module
stable_diffusion_controlnet_reference = _module
stable_diffusion_ipex = _module
stable_diffusion_mega = _module
stable_diffusion_reference = _module
stable_diffusion_repaint = _module
stable_diffusion_tensorrt_img2img = _module
stable_diffusion_tensorrt_inpaint = _module
stable_diffusion_tensorrt_txt2img = _module
stable_diffusion_xl_reference = _module
stable_unclip = _module
text_inpainting = _module
tiled_upscaling = _module
unclip_image_interpolation = _module
unclip_text_interpolation = _module
wildcard_stable_diffusion = _module
conftest = _module
test_lcm_lora = _module
train_lcm_distill_lora_sd_wds = _module
train_lcm_distill_lora_sdxl = _module
train_lcm_distill_lora_sdxl_wds = _module
train_lcm_distill_sd_wds = _module
train_lcm_distill_sdxl_wds = _module
test_controlnet = _module
train_controlnet = _module
train_controlnet_flax = _module
train_controlnet_flux = _module
train_controlnet_sd3 = _module
train_controlnet_sdxl = _module
retrieve = _module
test_custom_diffusion = _module
train_custom_diffusion = _module
test_dreambooth = _module
test_dreambooth_flux = _module
test_dreambooth_lora = _module
test_dreambooth_lora_edm = _module
test_dreambooth_lora_flux = _module
test_dreambooth_lora_sd3 = _module
test_dreambooth_sd3 = _module
train_dreambooth = _module
train_dreambooth_flax = _module
train_dreambooth_flux = _module
train_dreambooth_lora = _module
train_dreambooth_lora_flux = _module
train_dreambooth_lora_sd3 = _module
train_dreambooth_lora_sdxl = _module
train_dreambooth_sd3 = _module
image_to_image = _module
inpainting = _module
test_instruct_pix2pix = _module
train_instruct_pix2pix = _module
train_instruct_pix2pix_sdxl = _module
train_text_to_image_decoder = _module
train_text_to_image_lora_decoder = _module
train_text_to_image_lora_prior = _module
train_text_to_image_prior = _module
diffusion_policy = _module
run_diffuser_locomotion = _module
inference = _module
train_dreambooth_colossalai = _module
train_cm_ct_unconditional = _module
train_controlnet_webdataset = _module
train_diffusion_dpo = _module
train_diffusion_dpo_sdxl = _module
train_diffusion_orpo_sdxl_lora = _module
train_diffusion_orpo_sdxl_lora_wds = _module
train_dreambooth_inpaint = _module
train_dreambooth_inpaint_lora = _module
compute_embeddings = _module
train_dreambooth_lora_flux_miniature = _module
dataset = _module
make_datasets = _module
train_gligen_text = _module
train_instruct_pix2pix_lora = _module
inference_bf16 = _module
textual_inversion_bf16 = _module
text2images = _module
textual_inversion = _module
train_text_to_image_lora = _module
train_multi_subject_dreambooth = _module
train_multi_subject_dreambooth_inpainting = _module
multi_token_clip = _module
textual_inversion = _module
textual_inversion_flax = _module
train_text_to_image = _module
textual_inversion = _module
train_unconditional = _module
controlnet_pixart_alpha = _module
pipeline_pixart_alpha_controlnet = _module
run_pixart_alpha_controlnet_pipeline = _module
train_pixart_controlnet_hf = _module
convert_original_promptdiffusion_to_diffusers = _module
pipeline_prompt_diffusion = _module
promptdiffusioncontrolnet = _module
train_text_to_image_xla = _module
pipeline_rdm = _module
retriever = _module
infer = _module
train_realfill = _module
train_dreambooth = _module
train_dreambooth_lora = _module
train_dreambooth_lora_sdxl = _module
train_text_to_image = _module
train_text_to_image_lora = _module
train_text_to_image_lora_sdxl = _module
train_text_to_image_sdxl = _module
compute_embeddings = _module
train_dreambooth_lora_sd3_miniature = _module
sdxl_single = _module
sdxl_single_aot = _module
vae_roundtrip = _module
test_t2i_adapter = _module
train_t2i_adapter_sdxl = _module
test_examples_utils = _module
test_text_to_image = _module
test_text_to_image_lora = _module
train_text_to_image = _module
train_text_to_image_flax = _module
train_text_to_image_lora = _module
train_text_to_image_lora_sdxl = _module
train_text_to_image_sdxl = _module
test_textual_inversion = _module
test_textual_inversion_sdxl = _module
textual_inversion = _module
textual_inversion_flax = _module
textual_inversion_sdxl = _module
test_unconditional = _module
train_unconditional = _module
discriminator = _module
test_vqgan = _module
train_vqgan = _module
text_to_image = _module
modeling_efficient_net_encoder = _module
train_text_to_image_lora_prior = _module
train_text_to_image_prior = _module
scripts = _module
change_naming_configs_and_checkpoints = _module
conversion_ldm_uncond = _module
convert_amused = _module
convert_animatediff_motion_lora_to_diffusers = _module
convert_animatediff_motion_module_to_diffusers = _module
convert_animatediff_sparsectrl_to_diffusers = _module
convert_asymmetric_vqgan_to_diffusers = _module
convert_aura_flow_to_diffusers = _module
convert_blipdiffusion_to_diffusers = _module
convert_cogvideox_to_diffusers = _module
convert_cogview3_to_diffusers = _module
convert_consistency_decoder = _module
convert_consistency_to_diffusers = _module
convert_dance_diffusion_to_diffusers = _module
convert_ddpm_original_checkpoint_to_diffusers = _module
convert_diffusers_sdxl_lora_to_webui = _module
convert_diffusers_to_original_sdxl = _module
convert_diffusers_to_original_stable_diffusion = _module
convert_dit_to_diffusers = _module
convert_flux_to_diffusers = _module
convert_gligen_to_diffusers = _module
convert_hunyuandit_controlnet_to_diffusers = _module
convert_hunyuandit_to_diffusers = _module
convert_i2vgen_to_diffusers = _module
convert_if = _module
convert_k_upscaler_to_diffusers = _module
convert_kakao_brain_unclip_to_diffusers = _module
convert_kandinsky3_unet = _module
convert_kandinsky_to_diffusers = _module
convert_ldm_original_checkpoint_to_diffusers = _module
convert_lora_safetensor_to_diffusers = _module
convert_lumina_to_diffusers = _module
convert_mochi_to_diffusers = _module
convert_models_diffuser_to_diffusers = _module
convert_ms_text_to_video_to_diffusers = _module
convert_music_spectrogram_to_diffusers = _module
convert_ncsnpp_original_checkpoint_to_diffusers = _module
convert_original_audioldm2_to_diffusers = _module
convert_original_audioldm_to_diffusers = _module
convert_original_controlnet_to_diffusers = _module
convert_original_musicldm_to_diffusers = _module
convert_original_stable_diffusion_to_diffusers = _module
convert_original_t2i_adapter = _module
convert_pixart_alpha_to_diffusers = _module
convert_pixart_sigma_to_diffusers = _module
convert_sd3_to_diffusers = _module
convert_shap_e_to_diffusers = _module
convert_stable_audio = _module
convert_stable_cascade = _module
convert_stable_cascade_lite = _module
convert_stable_diffusion_checkpoint_to_onnx = _module
convert_stable_diffusion_controlnet_to_onnx = _module
convert_stable_diffusion_controlnet_to_tensorrt = _module
convert_svd_to_diffusers = _module
convert_tiny_autoencoder_to_diffusers = _module
convert_unclip_txt2img_to_image_variation = _module
convert_unidiffuser_to_diffusers = _module
convert_vae_diff_to_onnx = _module
convert_vae_pt_to_diffusers = _module
convert_versatile_diffusion_to_diffusers = _module
convert_vq_diffusion_to_diffusers = _module
convert_wuerstchen = _module
convert_zero123_to_diffusers = _module
generate_logits = _module
setup = _module
diffusers = _module
callbacks = _module
commands = _module
diffusers_cli = _module
env = _module
fp16_safetensors = _module
configuration_utils = _module
dependency_versions_check = _module
dependency_versions_table = _module
experimental = _module
rl = _module
value_guided_sampling = _module
image_processor = _module
loaders = _module
ip_adapter = _module
lora_base = _module
lora_conversion_utils = _module
lora_pipeline = _module
peft = _module
single_file = _module
single_file_model = _module
single_file_utils = _module
textual_inversion = _module
unet = _module
unet_loader_utils = _module
utils = _module
models = _module
activations = _module
adapter = _module
attention = _module
attention_flax = _module
attention_processor = _module
autoencoders = _module
autoencoder_asym_kl = _module
autoencoder_kl = _module
autoencoder_kl_allegro = _module
autoencoder_kl_cogvideox = _module
autoencoder_kl_mochi = _module
autoencoder_kl_temporal_decoder = _module
autoencoder_oobleck = _module
autoencoder_tiny = _module
consistency_decoder_vae = _module
vae = _module
vq_model = _module
controlnet = _module
controlnet_flux = _module
controlnet_sd3 = _module
controlnet_sparsectrl = _module
controlnets = _module
controlnet = _module
controlnet_flax = _module
controlnet_flux = _module
controlnet_hunyuan = _module
controlnet_sd3 = _module
controlnet_sparsectrl = _module
controlnet_xs = _module
multicontrolnet = _module
downsampling = _module
embeddings = _module
embeddings_flax = _module
lora = _module
model_loading_utils = _module
modeling_flax_pytorch_utils = _module
modeling_flax_utils = _module
modeling_outputs = _module
modeling_pytorch_flax_utils = _module
modeling_utils = _module
normalization = _module
resnet = _module
resnet_flax = _module
transformers = _module
auraflow_transformer_2d = _module
cogvideox_transformer_3d = _module
dit_transformer_2d = _module
dual_transformer_2d = _module
hunyuan_transformer_2d = _module
latte_transformer_3d = _module
lumina_nextdit2d = _module
pixart_transformer_2d = _module
prior_transformer = _module
stable_audio_transformer = _module
t5_film_transformer = _module
transformer_2d = _module
transformer_allegro = _module
transformer_cogview3plus = _module
transformer_flux = _module
transformer_mochi = _module
transformer_sd3 = _module
transformer_temporal = _module
unets = _module
unet_1d = _module
unet_1d_blocks = _module
unet_2d = _module
unet_2d_blocks = _module
unet_2d_blocks_flax = _module
unet_2d_condition = _module
unet_2d_condition_flax = _module
unet_3d_blocks = _module
unet_3d_condition = _module
unet_i2vgen_xl = _module
unet_kandinsky3 = _module
unet_motion_model = _module
unet_spatio_temporal_condition = _module
unet_stable_cascade = _module
uvit_2d = _module
upsampling = _module
vae_flax = _module
optimization = _module
pipelines = _module
allegro = _module
pipeline_allegro = _module
pipeline_output = _module
amused = _module
pipeline_amused = _module
pipeline_amused_img2img = _module
pipeline_amused_inpaint = _module
animatediff = _module
pipeline_animatediff = _module
pipeline_animatediff_controlnet = _module
pipeline_animatediff_sdxl = _module
pipeline_animatediff_sparsectrl = _module
pipeline_animatediff_video2video = _module
pipeline_animatediff_video2video_controlnet = _module
pipeline_output = _module
audioldm = _module
pipeline_audioldm = _module
audioldm2 = _module
modeling_audioldm2 = _module
pipeline_audioldm2 = _module
aura_flow = _module
pipeline_aura_flow = _module
auto_pipeline = _module
blip_diffusion = _module
blip_image_processing = _module
modeling_blip2 = _module
modeling_ctx_clip = _module
pipeline_blip_diffusion = _module
cogvideo = _module
pipeline_cogvideox = _module
pipeline_cogvideox_fun_control = _module
pipeline_cogvideox_image2video = _module
pipeline_cogvideox_video2video = _module
pipeline_output = _module
cogview3 = _module
pipeline_cogview3plus = _module
consistency_models = _module
pipeline_consistency_models = _module
pipeline_controlnet = _module
pipeline_controlnet_blip_diffusion = _module
pipeline_controlnet_img2img = _module
pipeline_controlnet_inpaint = _module
pipeline_controlnet_inpaint_sd_xl = _module
pipeline_controlnet_sd_xl = _module
pipeline_controlnet_sd_xl_img2img = _module
pipeline_flax_controlnet = _module
controlnet_hunyuandit = _module
pipeline_hunyuandit_controlnet = _module
pipeline_stable_diffusion_3_controlnet = _module
pipeline_stable_diffusion_3_controlnet_inpainting = _module
pipeline_controlnet_xs = _module
pipeline_controlnet_xs_sd_xl = _module
dance_diffusion = _module
pipeline_dance_diffusion = _module
ddim = _module
pipeline_ddim = _module
ddpm = _module
pipeline_ddpm = _module
deepfloyd_if = _module
pipeline_if = _module
pipeline_if_img2img = _module
pipeline_if_img2img_superresolution = _module
pipeline_if_inpainting = _module
pipeline_if_inpainting_superresolution = _module
pipeline_if_superresolution = _module
safety_checker = _module
timesteps = _module
watermark = _module
deprecated = _module
alt_diffusion = _module
modeling_roberta_series = _module
pipeline_alt_diffusion = _module
pipeline_alt_diffusion_img2img = _module
audio_diffusion = _module
mel = _module
pipeline_audio_diffusion = _module
latent_diffusion_uncond = _module
pipeline_latent_diffusion_uncond = _module
pndm = _module
pipeline_pndm = _module
repaint = _module
pipeline_repaint = _module
score_sde_ve = _module
pipeline_score_sde_ve = _module
spectrogram_diffusion = _module
continuous_encoder = _module
midi_utils = _module
notes_encoder = _module
pipeline_spectrogram_diffusion = _module
stable_diffusion_variants = _module
pipeline_cycle_diffusion = _module
pipeline_onnx_stable_diffusion_inpaint_legacy = _module
pipeline_stable_diffusion_inpaint_legacy = _module
pipeline_stable_diffusion_model_editing = _module
pipeline_stable_diffusion_paradigms = _module
pipeline_stable_diffusion_pix2pix_zero = _module
stochastic_karras_ve = _module
pipeline_stochastic_karras_ve = _module
versatile_diffusion = _module
modeling_text_unet = _module
pipeline_versatile_diffusion = _module
pipeline_versatile_diffusion_dual_guided = _module
pipeline_versatile_diffusion_image_variation = _module
pipeline_versatile_diffusion_text_to_image = _module
vq_diffusion = _module
pipeline_vq_diffusion = _module
dit = _module
pipeline_dit = _module
flux = _module
pipeline_flux = _module
pipeline_flux_controlnet = _module
pipeline_flux_controlnet_image_to_image = _module
pipeline_flux_controlnet_inpainting = _module
pipeline_flux_img2img = _module
pipeline_flux_inpaint = _module
free_init_utils = _module
free_noise_utils = _module
hunyuandit = _module
pipeline_hunyuandit = _module
i2vgen_xl = _module
pipeline_i2vgen_xl = _module
kandinsky = _module
pipeline_kandinsky = _module
pipeline_kandinsky_combined = _module
pipeline_kandinsky_img2img = _module
pipeline_kandinsky_inpaint = _module
pipeline_kandinsky_prior = _module
text_encoder = _module
kandinsky2_2 = _module
pipeline_kandinsky2_2 = _module
pipeline_kandinsky2_2_combined = _module
pipeline_kandinsky2_2_controlnet = _module
pipeline_kandinsky2_2_controlnet_img2img = _module
pipeline_kandinsky2_2_img2img = _module
pipeline_kandinsky2_2_inpainting = _module
pipeline_kandinsky2_2_prior = _module
pipeline_kandinsky2_2_prior_emb2emb = _module
kandinsky3 = _module
convert_kandinsky3_unet = _module
pipeline_kandinsky3 = _module
pipeline_kandinsky3_img2img = _module
kolors = _module
pipeline_kolors = _module
pipeline_kolors_img2img = _module
text_encoder = _module
tokenizer = _module
latent_consistency_models = _module
pipeline_latent_consistency_img2img = _module
pipeline_latent_consistency_text2img = _module
latent_diffusion = _module
pipeline_latent_diffusion = _module
pipeline_latent_diffusion_superresolution = _module
latte = _module
pipeline_latte = _module
ledits_pp = _module
pipeline_leditspp_stable_diffusion = _module
pipeline_leditspp_stable_diffusion_xl = _module
lumina = _module
pipeline_lumina = _module
marigold = _module
marigold_image_processing = _module
pipeline_marigold_depth = _module
pipeline_marigold_normals = _module
mochi = _module
pipeline_mochi = _module
pipeline_output = _module
musicldm = _module
pipeline_musicldm = _module
onnx_utils = _module
pag = _module
pag_utils = _module
pipeline_pag_controlnet_sd = _module
pipeline_pag_controlnet_sd_inpaint = _module
pipeline_pag_controlnet_sd_xl = _module
pipeline_pag_controlnet_sd_xl_img2img = _module
pipeline_pag_hunyuandit = _module
pipeline_pag_kolors = _module
pipeline_pag_pixart_sigma = _module
pipeline_pag_sd = _module
pipeline_pag_sd_3 = _module
pipeline_pag_sd_animatediff = _module
pipeline_pag_sd_img2img = _module
pipeline_pag_sd_xl = _module
pipeline_pag_sd_xl_img2img = _module
pipeline_pag_sd_xl_inpaint = _module
paint_by_example = _module
image_encoder = _module
pipeline_paint_by_example = _module
pia = _module
pipeline_pia = _module
pipeline_flax_utils = _module
pipeline_loading_utils = _module
pipeline_utils = _module
pixart_alpha = _module
pipeline_pixart_alpha = _module
pipeline_pixart_sigma = _module
semantic_stable_diffusion = _module
pipeline_semantic_stable_diffusion = _module
shap_e = _module
camera = _module
pipeline_shap_e = _module
pipeline_shap_e_img2img = _module
renderer = _module
stable_audio = _module
modeling_stable_audio = _module
pipeline_stable_audio = _module
stable_cascade = _module
pipeline_stable_cascade = _module
pipeline_stable_cascade_combined = _module
pipeline_stable_cascade_prior = _module
stable_diffusion = _module
clip_image_project_model = _module
convert_from_ckpt = _module
pipeline_flax_stable_diffusion = _module
pipeline_flax_stable_diffusion_img2img = _module
pipeline_flax_stable_diffusion_inpaint = _module
pipeline_onnx_stable_diffusion = _module
pipeline_onnx_stable_diffusion_img2img = _module
pipeline_onnx_stable_diffusion_inpaint = _module
pipeline_onnx_stable_diffusion_upscale = _module
pipeline_stable_diffusion = _module
pipeline_stable_diffusion_depth2img = _module
pipeline_stable_diffusion_image_variation = _module
pipeline_stable_diffusion_img2img = _module
pipeline_stable_diffusion_inpaint = _module
pipeline_stable_diffusion_instruct_pix2pix = _module
pipeline_stable_diffusion_latent_upscale = _module
pipeline_stable_diffusion_upscale = _module
pipeline_stable_unclip = _module
pipeline_stable_unclip_img2img = _module
safety_checker = _module
safety_checker_flax = _module
stable_unclip_image_normalizer = _module
stable_diffusion_3 = _module
pipeline_stable_diffusion_3 = _module
pipeline_stable_diffusion_3_img2img = _module
pipeline_stable_diffusion_3_inpaint = _module
stable_diffusion_attend_and_excite = _module
pipeline_stable_diffusion_attend_and_excite = _module
stable_diffusion_diffedit = _module
pipeline_stable_diffusion_diffedit = _module
stable_diffusion_gligen = _module
pipeline_stable_diffusion_gligen = _module
pipeline_stable_diffusion_gligen_text_image = _module
stable_diffusion_k_diffusion = _module
pipeline_stable_diffusion_k_diffusion = _module
pipeline_stable_diffusion_xl_k_diffusion = _module
stable_diffusion_ldm3d = _module
pipeline_stable_diffusion_ldm3d = _module
stable_diffusion_panorama = _module
pipeline_stable_diffusion_panorama = _module
stable_diffusion_safe = _module
pipeline_stable_diffusion_safe = _module
safety_checker = _module
stable_diffusion_sag = _module
pipeline_stable_diffusion_sag = _module
stable_diffusion_xl = _module
pipeline_flax_stable_diffusion_xl = _module
pipeline_stable_diffusion_xl = _module
pipeline_stable_diffusion_xl_img2img = _module
pipeline_stable_diffusion_xl_inpaint = _module
pipeline_stable_diffusion_xl_instruct_pix2pix = _module
watermark = _module
stable_video_diffusion = _module
pipeline_stable_video_diffusion = _module
t2i_adapter = _module
pipeline_stable_diffusion_adapter = _module
pipeline_stable_diffusion_xl_adapter = _module
text_to_video_synthesis = _module
pipeline_output = _module
pipeline_text_to_video_synth = _module
pipeline_text_to_video_synth_img2img = _module
pipeline_text_to_video_zero = _module
pipeline_text_to_video_zero_sdxl = _module
unclip = _module
pipeline_unclip = _module
pipeline_unclip_image_variation = _module
text_proj = _module
unidiffuser = _module
modeling_text_decoder = _module
modeling_uvit = _module
pipeline_unidiffuser = _module
wuerstchen = _module
modeling_paella_vq_model = _module
modeling_wuerstchen_common = _module
modeling_wuerstchen_diffnext = _module
modeling_wuerstchen_prior = _module
pipeline_wuerstchen = _module
pipeline_wuerstchen_combined = _module
pipeline_wuerstchen_prior = _module
quantizers = _module
auto = _module
base = _module
bitsandbytes = _module
bnb_quantizer = _module
utils = _module
quantization_config = _module
schedulers = _module
scheduling_karras_ve = _module
scheduling_sde_vp = _module
scheduling_amused = _module
scheduling_consistency_decoder = _module
scheduling_consistency_models = _module
scheduling_cosine_dpmsolver_multistep = _module
scheduling_ddim = _module
scheduling_ddim_cogvideox = _module
scheduling_ddim_flax = _module
scheduling_ddim_inverse = _module
scheduling_ddim_parallel = _module
scheduling_ddpm = _module
scheduling_ddpm_flax = _module
scheduling_ddpm_parallel = _module
scheduling_ddpm_wuerstchen = _module
scheduling_deis_multistep = _module
scheduling_dpm_cogvideox = _module
scheduling_dpmsolver_multistep = _module
scheduling_dpmsolver_multistep_flax = _module
scheduling_dpmsolver_multistep_inverse = _module
scheduling_dpmsolver_sde = _module
scheduling_dpmsolver_singlestep = _module
scheduling_edm_dpmsolver_multistep = _module
scheduling_edm_euler = _module
scheduling_euler_ancestral_discrete = _module
scheduling_euler_discrete = _module
scheduling_euler_discrete_flax = _module
scheduling_flow_match_euler_discrete = _module
scheduling_flow_match_heun_discrete = _module
scheduling_heun_discrete = _module
scheduling_ipndm = _module
scheduling_k_dpm_2_ancestral_discrete = _module
scheduling_k_dpm_2_discrete = _module
scheduling_karras_ve_flax = _module
scheduling_lcm = _module
scheduling_lms_discrete = _module
scheduling_lms_discrete_flax = _module
scheduling_pndm = _module
scheduling_pndm_flax = _module
scheduling_repaint = _module
scheduling_sasolver = _module
scheduling_sde_ve = _module
scheduling_sde_ve_flax = _module
scheduling_tcd = _module
scheduling_unclip = _module
scheduling_unipc_multistep = _module
scheduling_utils = _module
scheduling_utils_flax = _module
scheduling_vq_diffusion = _module
training_utils = _module
accelerate_utils = _module
constants = _module
deprecation_utils = _module
doc_utils = _module
dummy_flax_and_transformers_objects = _module
dummy_flax_objects = _module
dummy_note_seq_objects = _module
dummy_onnx_objects = _module
dummy_pt_objects = _module
dummy_torch_and_librosa_objects = _module
dummy_torch_and_scipy_objects = _module
dummy_torch_and_torchsde_objects = _module
dummy_torch_and_transformers_and_k_diffusion_objects = _module
dummy_torch_and_transformers_and_onnx_objects = _module
dummy_torch_and_transformers_and_sentencepiece_objects = _module
dummy_torch_and_transformers_objects = _module
dummy_transformers_and_torch_and_note_seq_objects = _module
dynamic_modules_utils = _module
export_utils = _module
hub_utils = _module
import_utils = _module
loading_utils = _module
logging = _module
outputs = _module
peft_utils = _module
pil_utils = _module
state_dict_utils = _module
testing_utils = _module
torch_utils = _module
versions = _module
video_processor = _module
tests = _module
pipeline = _module
what_ever = _module
test_deprecated_utilities = _module
test_lora_layers_cogvideox = _module
test_lora_layers_flux = _module
test_lora_layers_sd = _module
test_lora_layers_sd3 = _module
test_lora_layers_sdxl = _module
utils = _module
test_models_vae = _module
test_models_vae_flax = _module
test_models_vq = _module
test_activations = _module
test_attention_processor = _module
test_layers_utils = _module
test_modeling_common = _module
test_modeling_common_flax = _module
test_models_dit_transformer2d = _module
test_models_pixart_transformer2d = _module
test_models_prior = _module
test_models_transformer_allegro = _module
test_models_transformer_aura_flow = _module
test_models_transformer_cogvideox = _module
test_models_transformer_cogview3plus = _module
test_models_transformer_flux = _module
test_models_transformer_hunyuan_dit = _module
test_models_transformer_latte = _module
test_models_transformer_lumina = _module
test_models_transformer_mochi = _module
test_models_transformer_sd3 = _module
test_models_transformer_temporal = _module
test_models_unet_1d = _module
test_models_unet_2d = _module
test_models_unet_2d_condition = _module
test_models_unet_2d_flax = _module
test_models_unet_3d_condition = _module
test_models_unet_controlnetxs = _module
test_models_unet_motion = _module
test_models_unet_spatiotemporal = _module
test_unet_2d_blocks = _module
test_unet_blocks_common = _module
test_check_copies = _module
test_check_dummies = _module
test_config = _module
test_dependencies = _module
test_ema = _module
test_hub_utils = _module
test_image_processor = _module
test_outputs = _module
test_training = _module
test_utils = _module
test_video_processor = _module
test_allegro = _module
test_amused = _module
test_amused_img2img = _module
test_amused_inpaint = _module
test_animatediff = _module
test_animatediff_controlnet = _module
test_animatediff_sdxl = _module
test_animatediff_sparsectrl = _module
test_animatediff_video2video = _module
test_animatediff_video2video_controlnet = _module
test_audioldm = _module
test_audioldm2 = _module
test_pipeline_aura_flow = _module
blipdiffusion = _module
test_blipdiffusion = _module
test_cogvideox = _module
test_cogvideox_fun_control = _module
test_cogvideox_image2video = _module
test_cogvideox_video2video = _module
test_cogview3plus = _module
test_consistency_models = _module
test_controlnet = _module
test_controlnet_blip_diffusion = _module
test_controlnet_img2img = _module
test_controlnet_inpaint = _module
test_controlnet_inpaint_sdxl = _module
test_controlnet_sdxl = _module
test_controlnet_sdxl_img2img = _module
test_flax_controlnet = _module
test_controlnet_flux = _module
test_controlnet_flux_img2img = _module
test_controlnet_flux_inpaint = _module
test_controlnet_hunyuandit = _module
test_controlnet_inpaint_sd3 = _module
test_controlnet_sd3 = _module
test_controlnetxs = _module
test_controlnetxs_sdxl = _module
test_dance_diffusion = _module
test_ddim = _module
test_ddpm = _module
deepfloyd_if = _module
test_if = _module
test_if_img2img = _module
test_if_img2img_superresolution = _module
test_if_inpainting = _module
test_if_inpainting_superresolution = _module
test_if_superresolution = _module
test_dit = _module
test_pipeline_flux = _module
test_pipeline_flux_img2img = _module
test_pipeline_flux_inpaint = _module
hunyuan_dit = _module
test_hunyuan_dit = _module
test_i2vgenxl = _module
test_ip_adapter_stable_diffusion = _module
test_kandinsky = _module
test_kandinsky_combined = _module
test_kandinsky_img2img = _module
test_kandinsky_inpaint = _module
test_kandinsky_prior = _module
test_kandinsky = _module
test_kandinsky_controlnet = _module
test_kandinsky_controlnet_img2img = _module
test_kandinsky_img2img = _module
test_kandinsky_inpaint = _module
test_kandinsky_prior = _module
test_kandinsky_prior_emb2emb = _module
test_kandinsky3 = _module
test_kandinsky3_img2img = _module
test_kolors = _module
test_kolors_img2img = _module
test_latent_consistency_models = _module
test_latent_consistency_models_img2img = _module
test_latent_diffusion = _module
test_latent_diffusion_superresolution = _module
test_latte = _module
test_ledits_pp_stable_diffusion = _module
test_ledits_pp_stable_diffusion_xl = _module
test_lumina_nextdit = _module
test_marigold_depth = _module
test_marigold_normals = _module
test_mochi = _module
test_musicldm = _module
test_pag_animatediff = _module
test_pag_controlnet_sd = _module
test_pag_controlnet_sd_inpaint = _module
test_pag_controlnet_sdxl = _module
test_pag_controlnet_sdxl_img2img = _module
test_pag_hunyuan_dit = _module
test_pag_kolors = _module
test_pag_pixart_sigma = _module
test_pag_sd = _module
test_pag_sd3 = _module
test_pag_sd_img2img = _module
test_pag_sdxl = _module
test_pag_sdxl_img2img = _module
test_pag_sdxl_inpaint = _module
test_paint_by_example = _module
test_pia = _module
pipeline_params = _module
test_pixart = _module
pixart_sigma = _module
test_pixart = _module
test_pndm = _module
test_semantic_diffusion = _module
test_shap_e = _module
test_shap_e_img2img = _module
test_stable_audio = _module
test_stable_cascade_combined = _module
test_stable_cascade_decoder = _module
test_stable_cascade_prior = _module
test_onnx_stable_diffusion = _module
test_onnx_stable_diffusion_img2img = _module
test_onnx_stable_diffusion_inpaint = _module
test_onnx_stable_diffusion_upscale = _module
test_stable_diffusion = _module
test_stable_diffusion_img2img = _module
test_stable_diffusion_inpaint = _module
test_stable_diffusion_instruction_pix2pix = _module
stable_diffusion_2 = _module
test_stable_diffusion = _module
test_stable_diffusion_attend_and_excite = _module
test_stable_diffusion_depth = _module
test_stable_diffusion_diffedit = _module
test_stable_diffusion_flax = _module
test_stable_diffusion_flax_inpaint = _module
test_stable_diffusion_inpaint = _module
test_stable_diffusion_latent_upscale = _module
test_stable_diffusion_upscale = _module
test_stable_diffusion_v_pred = _module
test_pipeline_stable_diffusion_3 = _module
test_pipeline_stable_diffusion_3_img2img = _module
test_pipeline_stable_diffusion_3_inpaint = _module
stable_diffusion_adapter = _module
test_stable_diffusion_adapter = _module
test_stable_diffusion_gligen = _module
stable_diffusion_gligen_text_image = _module
test_stable_diffusion_gligen_text_image = _module
stable_diffusion_image_variation = _module
test_stable_diffusion_image_variation = _module
test_stable_diffusion_k_diffusion = _module
test_stable_diffusion_ldm3d = _module
test_stable_diffusion_panorama = _module
test_safe_diffusion = _module
test_stable_diffusion_sag = _module
test_stable_diffusion_xl = _module
test_stable_diffusion_xl_adapter = _module
test_stable_diffusion_xl_img2img = _module
test_stable_diffusion_xl_inpaint = _module
test_stable_diffusion_xl_instruction_pix2pix = _module
test_stable_diffusion_xl_k_diffusion = _module
test_stable_unclip = _module
test_stable_unclip_img2img = _module
test_stable_video_diffusion = _module
test_pipeline_utils = _module
test_pipelines = _module
test_pipelines_auto = _module
test_pipelines_combined = _module
test_pipelines_common = _module
test_pipelines_flax = _module
test_pipelines_onnx_common = _module
test_text_to_video = _module
test_text_to_video_zero = _module
test_text_to_video_zero_sdxl = _module
test_video_to_video = _module
test_unclip = _module
test_unclip_image_variation = _module
test_unidiffuser = _module
test_wuerstchen_combined = _module
test_wuerstchen_decoder = _module
test_wuerstchen_prior = _module
bnb = _module
test_4bit = _module
test_mixed_int8 = _module
test_scheduler_consistency_model = _module
test_scheduler_ddim = _module
test_scheduler_ddim_inverse = _module
test_scheduler_ddim_parallel = _module
test_scheduler_ddpm = _module
test_scheduler_ddpm_parallel = _module
test_scheduler_deis = _module
test_scheduler_dpm_multi = _module
test_scheduler_dpm_multi_inverse = _module
test_scheduler_dpm_sde = _module
test_scheduler_dpm_single = _module
test_scheduler_edm_dpmsolver_multistep = _module
test_scheduler_edm_euler = _module
test_scheduler_euler = _module
test_scheduler_euler_ancestral = _module
test_scheduler_flax = _module
test_scheduler_heun = _module
test_scheduler_ipndm = _module
test_scheduler_kdpm2_ancestral = _module
test_scheduler_kdpm2_discrete = _module
test_scheduler_lcm = _module
test_scheduler_lms = _module
test_scheduler_pndm = _module
test_scheduler_sasolver = _module
test_scheduler_score_sde_ve = _module
test_scheduler_tcd = _module
test_scheduler_unclip = _module
test_scheduler_unipc = _module
test_scheduler_vq_diffusion = _module
test_schedulers = _module
single_file_testing_utils = _module
test_model_controlnet_single_file = _module
test_model_motion_adapter_single_file = _module
test_model_sd_cascade_unet_single_file = _module
test_model_vae_single_file = _module
test_stable_diffusion_controlnet_img2img_single_file = _module
test_stable_diffusion_controlnet_inpaint_single_file = _module
test_stable_diffusion_controlnet_single_file = _module
test_stable_diffusion_img2img_single_file = _module
test_stable_diffusion_inpaint_single_file = _module
test_stable_diffusion_single_file = _module
test_stable_diffusion_upscale_single_file = _module
test_stable_diffusion_xl_adapter_single_file = _module
test_stable_diffusion_xl_controlnet_single_file = _module
test_stable_diffusion_xl_img2img_single_file = _module
test_stable_diffusion_xl_instruct_pix2pix = _module
test_stable_diffusion_xl_single_file = _module
check_config_docstrings = _module
check_copies = _module
check_doc_toc = _module
check_dummies = _module
check_inits = _module
check_repo = _module
check_table = _module
custom_init_isort = _module
fetch_latest_release_branch = _module
fetch_torch_cuda_pipeline_test_matrix = _module
get_modified_files = _module
log_reports = _module
notify_benchmarking_status = _module
notify_community_pipelines_mirror = _module
notify_slack_about_release = _module
overwrite_expected_slice = _module
print_env = _module
release = _module
stale = _module
tests_fetcher = _module
update_metadata = _module

from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from typing import Dict


from typing import List


from typing import Union


import torch.utils.benchmark as benchmark


import logging


import copy


import itertools


import math


import random


import re


from typing import Optional


import numpy as np


import torch.utils.checkpoint


from torch.utils.data import Dataset


from torchvision import transforms


from torchvision.transforms.functional import crop


import warnings


import torch.nn.functional as F


from torch.utils.data import DataLoader


from torch.utils.data import default_collate


from typing import Tuple


import torchvision.transforms as TT


from torchvision.transforms import InterpolationMode


from torchvision.transforms.functional import resize


import inspect


from typing import Any


from typing import Callable


from torch.nn import functional as F


from torch import nn


from math import pi


import torch.utils.model_zoo


import torch.nn as nn


import numbers


import time


from collections.abc import Iterable


from torchvision import transforms as tfms


import matplotlib


from scipy.optimize import minimize


from torch.utils.data import TensorDataset


from copy import deepcopy


from enum import Enum


from numpy import exp


from numpy import pi


from numpy import sqrt


from types import FunctionType


import matplotlib.pyplot as plt


import torch.nn.functional as nnf


from torch.optim.adam import Adam


import abc


from collections.abc import Callable


import torchvision


import torchvision.transforms.functional as FF


import torchvision.transforms as T


from collections import OrderedDict


import types


import functools


import torchvision.transforms.functional as TF


from torch.utils.data import IterableDataset


import numpy.core.multiarray as multiarray


from torch.serialization import add_safe_globals


import pandas as pd


import torchvision.transforms as transforms


import torchvision.transforms as TS


from typing import Iterable


import uuid


from torch import dtype


from torch.nn import Module


import torchvision.transforms.v2 as transforms_v2


import typing


from torchvision.models import efficientnet_v2_l


from torchvision.models import efficientnet_v2_s


from typing import Literal


from torchvision.datasets.utils import download_url


import numpy as onp


from torch.onnx import export


from functools import partial


from collections import defaultdict


from torch.nn.utils import weight_norm


from math import gcd


from torch import Tensor


from functools import wraps


from torch.utils.checkpoint import checkpoint


from torch.optim import Optimizer


from torch.optim.lr_scheduler import LambdaLR


from math import acos


from math import sin


from typing import Mapping


from typing import MutableMapping


from typing import Sequence


import torch.fft as fft


from torch.nn import LayerNorm


from torch.nn.utils import skip_init


from itertools import repeat


from typing import get_args


from typing import get_origin


from math import ceil


from torch.nn.functional import grid_sample


from abc import ABC


from abc import abstractmethod


from typing import TYPE_CHECKING


from inspect import signature


import scipy.stats


from scipy import integrate


from uuid import uuid4


from itertools import chain


from types import ModuleType


import collections


import enum


from numpy.linalg import norm


from itertools import product


from torch.backends.cuda import sdp_kernel


class MakeCutouts(nn.Module):

    def __init__(self, cut_size, cut_power=1.0):
        super().__init__()
        self.cut_size = cut_size
        self.cut_power = cut_power

    def forward(self, pixel_values, num_cutouts):
        sideY, sideX = pixel_values.shape[2:4]
        max_size = min(sideX, sideY)
        min_size = min(sideX, sideY, self.cut_size)
        cutouts = []
        for _ in range(num_cutouts):
            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)
            offsetx = torch.randint(0, sideX - size + 1, ())
            offsety = torch.randint(0, sideY - size + 1, ())
            cutout = pixel_values[:, :, offsety:offsety + size, offsetx:offsetx + size]
            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))
        return torch.cat(cutouts)


class TranslatorBase(nn.Module):

    def __init__(self, num_tok, dim, dim_out, mult=2):
        super().__init__()
        self.dim_in = dim
        self.dim_out = dim_out
        self.net_tok = nn.Sequential(nn.Linear(num_tok, int(num_tok * mult)), nn.LayerNorm(int(num_tok * mult)), nn.GELU(), nn.Linear(int(num_tok * mult), int(num_tok * mult)), nn.LayerNorm(int(num_tok * mult)), nn.GELU(), nn.Linear(int(num_tok * mult), num_tok), nn.LayerNorm(num_tok))
        self.net_sen = nn.Sequential(nn.Linear(dim, int(dim * mult)), nn.LayerNorm(int(dim * mult)), nn.GELU(), nn.Linear(int(dim * mult), int(dim * mult)), nn.LayerNorm(int(dim * mult)), nn.GELU(), nn.Linear(int(dim * mult), dim_out), nn.LayerNorm(dim_out))

    def forward(self, x):
        if self.dim_in == self.dim_out:
            indentity_0 = x
            x = self.net_sen(x)
            x += indentity_0
            x = x.transpose(1, 2)
            indentity_1 = x
            x = self.net_tok(x)
            x += indentity_1
            x = x.transpose(1, 2)
        else:
            x = self.net_sen(x)
            x = x.transpose(1, 2)
            x = self.net_tok(x)
            x = x.transpose(1, 2)
        return x


class TranslatorBaseNoLN(nn.Module):

    def __init__(self, num_tok, dim, dim_out, mult=2):
        super().__init__()
        self.dim_in = dim
        self.dim_out = dim_out
        self.net_tok = nn.Sequential(nn.Linear(num_tok, int(num_tok * mult)), nn.GELU(), nn.Linear(int(num_tok * mult), int(num_tok * mult)), nn.GELU(), nn.Linear(int(num_tok * mult), num_tok))
        self.net_sen = nn.Sequential(nn.Linear(dim, int(dim * mult)), nn.GELU(), nn.Linear(int(dim * mult), int(dim * mult)), nn.GELU(), nn.Linear(int(dim * mult), dim_out))

    def forward(self, x):
        if self.dim_in == self.dim_out:
            indentity_0 = x
            x = self.net_sen(x)
            x += indentity_0
            x = x.transpose(1, 2)
            indentity_1 = x
            x = self.net_tok(x)
            x += indentity_1
            x = x.transpose(1, 2)
        else:
            x = self.net_sen(x)
            x = x.transpose(1, 2)
            x = self.net_tok(x)
            x = x.transpose(1, 2)
        return x


class TranslatorNoLN(nn.Module):

    def __init__(self, num_tok, dim, dim_out, mult=2, depth=5):
        super().__init__()
        self.blocks = nn.ModuleList([TranslatorBase(num_tok, dim, dim, mult=2) for d in range(depth)])
        self.gelu = nn.GELU()
        self.tail = TranslatorBaseNoLN(num_tok, dim, dim_out, mult=2)

    def forward(self, x):
        for block in self.blocks:
            x = block(x) + x
            x = self.gelu(x)
        x = self.tail(x)
        return x


class GaussianSmoothing(torch.nn.Module):
    """
    Arguments:
    Apply gaussian smoothing on a 1d, 2d or 3d tensor. Filtering is performed seperately for each channel in the input
    using a depthwise convolution.
        channels (int, sequence): Number of channels of the input tensors. Output will
            have this number of channels as well.
        kernel_size (int, sequence): Size of the gaussian kernel. sigma (float, sequence): Standard deviation of the
        gaussian kernel. dim (int, optional): The number of dimensions of the data.
            Default value is 2 (spatial).
    """

    def __init__(self, channels: 'int'=1, kernel_size: 'int'=3, sigma: 'float'=0.5, dim: 'int'=2):
        super().__init__()
        if isinstance(kernel_size, int):
            kernel_size = [kernel_size] * dim
        if isinstance(sigma, float):
            sigma = [sigma] * dim
        kernel = 1
        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])
        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):
            mean = (size - 1) / 2
            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * torch.exp(-((mgrid - mean) / (2 * std)) ** 2)
        kernel = kernel / torch.sum(kernel)
        kernel = kernel.view(1, 1, *kernel.size())
        kernel = kernel.repeat(channels, *([1] * (kernel.dim() - 1)))
        self.register_buffer('weight', kernel)
        self.groups = channels
        if dim == 1:
            self.conv = F.conv1d
        elif dim == 2:
            self.conv = F.conv2d
        elif dim == 3:
            self.conv = F.conv3d
        else:
            raise RuntimeError('Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim))

    def forward(self, input):
        """
        Arguments:
        Apply gaussian filter to input.
            input (torch.Tensor): Input to apply gaussian filter on.
        Returns:
            filtered (torch.Tensor): Filtered output.
        """
        return self.conv(input, weight=self.weight, groups=self.groups)


class ApproximateGELU(nn.Module):
    """
    The approximate form of the Gaussian Error Linear Unit (GELU). For more details, see section 2 of this
    [paper](https://arxiv.org/abs/1606.08415).

    Parameters:
        dim_in (`int`): The number of channels in the input.
        dim_out (`int`): The number of channels in the output.
        bias (`bool`, defaults to True): Whether to use a bias in the linear layer.
    """

    def __init__(self, dim_in: 'int', dim_out: 'int', bias: 'bool'=True):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out, bias=bias)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x = self.proj(x)
        return x * torch.sigmoid(1.702 * x)


def deprecate(*args, take_from: Optional[Union[Dict, Any]]=None, standard_warn=True, stacklevel=2):
    deprecated_kwargs = take_from
    values = ()
    if not isinstance(args[0], tuple):
        args = args,
    for attribute, version_name, message in args:
        if version.parse(version.parse(__version__).base_version) >= version.parse(version_name):
            raise ValueError(f"The deprecation tuple {attribute, version_name, message} should be removed since diffusers' version {__version__} is >= {version_name}")
        warning = None
        if isinstance(deprecated_kwargs, dict) and attribute in deprecated_kwargs:
            values += deprecated_kwargs.pop(attribute),
            warning = f'The `{attribute}` argument is deprecated and will be removed in version {version_name}.'
        elif hasattr(deprecated_kwargs, attribute):
            values += getattr(deprecated_kwargs, attribute),
            warning = f'The `{attribute}` attribute is deprecated and will be removed in version {version_name}.'
        elif deprecated_kwargs is None:
            warning = f'`{attribute}` is deprecated and will be removed in version {version_name}.'
        if warning is not None:
            warning = warning + ' ' if standard_warn else ''
            warnings.warn(warning + message, FutureWarning, stacklevel=stacklevel)
    if isinstance(deprecated_kwargs, dict) and len(deprecated_kwargs) > 0:
        call_frame = inspect.getouterframes(inspect.currentframe())[1]
        filename = call_frame.filename
        line_number = call_frame.lineno
        function = call_frame.function
        key, value = next(iter(deprecated_kwargs.items()))
        raise TypeError(f'{function} in {filename} line {line_number - 1} got an unexpected keyword argument `{key}`')
    if len(values) == 0:
        return
    elif len(values) == 1:
        return values[0]
    return values


def is_torch_npu_available():
    return _torch_npu_available


class GEGLU(nn.Module):
    """
    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function.

    Parameters:
        dim_in (`int`): The number of channels in the input.
        dim_out (`int`): The number of channels in the output.
        bias (`bool`, defaults to True): Whether to use a bias in the linear layer.
    """

    def __init__(self, dim_in: 'int', dim_out: 'int', bias: 'bool'=True):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out * 2, bias=bias)

    def gelu(self, gate: 'torch.Tensor') ->torch.Tensor:
        if gate.device.type != 'mps':
            return F.gelu(gate)
        return F.gelu(gate.to(dtype=torch.float32))

    def forward(self, hidden_states, *args, **kwargs):
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        hidden_states = self.proj(hidden_states)
        if is_torch_npu_available():
            return torch_npu.npu_geglu(hidden_states, dim=-1, approximate=1)[0]
        else:
            hidden_states, gate = hidden_states.chunk(2, dim=-1)
            return hidden_states * self.gelu(gate)


class GELU(nn.Module):
    """
    GELU activation function with tanh approximation support with `approximate="tanh"`.

    Parameters:
        dim_in (`int`): The number of channels in the input.
        dim_out (`int`): The number of channels in the output.
        approximate (`str`, *optional*, defaults to `"none"`): If `"tanh"`, use tanh approximation.
        bias (`bool`, defaults to True): Whether to use a bias in the linear layer.
    """

    def __init__(self, dim_in: 'int', dim_out: 'int', approximate: 'str'='none', bias: 'bool'=True):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out, bias=bias)
        self.approximate = approximate

    def gelu(self, gate: 'torch.Tensor') ->torch.Tensor:
        if gate.device.type != 'mps':
            return F.gelu(gate, approximate=self.approximate)
        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate)

    def forward(self, hidden_states):
        hidden_states = self.proj(hidden_states)
        hidden_states = self.gelu(hidden_states)
        return hidden_states


class SwiGLU(nn.Module):
    """
    A [variant](https://arxiv.org/abs/2002.05202) of the gated linear unit activation function. It's similar to `GEGLU`
    but uses SiLU / Swish instead of GeLU.

    Parameters:
        dim_in (`int`): The number of channels in the input.
        dim_out (`int`): The number of channels in the output.
        bias (`bool`, defaults to True): Whether to use a bias in the linear layer.
    """

    def __init__(self, dim_in: 'int', dim_out: 'int', bias: 'bool'=True):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out * 2, bias=bias)
        self.activation = nn.SiLU()

    def forward(self, hidden_states):
        hidden_states = self.proj(hidden_states)
        hidden_states, gate = hidden_states.chunk(2, dim=-1)
        return hidden_states * self.activation(gate)


class FeedForward(nn.Module):
    """
    A feed-forward layer.

    Parameters:
        dim (`int`): The number of channels in the input.
        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.
        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        final_dropout (`bool` *optional*, defaults to False): Apply a final dropout.
        bias (`bool`, defaults to True): Whether to use a bias in the linear layer.
    """

    def __init__(self, dim: 'int', dim_out: 'Optional[int]'=None, mult: 'int'=4, dropout: 'float'=0.0, activation_fn: 'str'='geglu', final_dropout: 'bool'=False, inner_dim=None, bias: 'bool'=True):
        super().__init__()
        if inner_dim is None:
            inner_dim = int(dim * mult)
        dim_out = dim_out if dim_out is not None else dim
        if activation_fn == 'gelu':
            act_fn = GELU(dim, inner_dim, bias=bias)
        if activation_fn == 'gelu-approximate':
            act_fn = GELU(dim, inner_dim, approximate='tanh', bias=bias)
        elif activation_fn == 'geglu':
            act_fn = GEGLU(dim, inner_dim, bias=bias)
        elif activation_fn == 'geglu-approximate':
            act_fn = ApproximateGELU(dim, inner_dim, bias=bias)
        elif activation_fn == 'swiglu':
            act_fn = SwiGLU(dim, inner_dim, bias=bias)
        self.net = nn.ModuleList([])
        self.net.append(act_fn)
        self.net.append(nn.Dropout(dropout))
        self.net.append(nn.Linear(inner_dim, dim_out, bias=bias))
        if final_dropout:
            self.net.append(nn.Dropout(dropout))

    def forward(self, hidden_states: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for module in self.net:
            hidden_states = module(hidden_states)
        return hidden_states


class IPAdapterFullImageProjection(nn.Module):

    def __init__(self, image_embed_dim=1024, cross_attention_dim=1024):
        super().__init__()
        self.ff = FeedForward(image_embed_dim, cross_attention_dim, mult=1, activation_fn='gelu')
        self.norm = nn.LayerNorm(cross_attention_dim)

    def forward(self, image_embeds: 'torch.Tensor'):
        return self.norm(self.ff(image_embeds))


class RMSNorm(torch.nn.Module):

    def __init__(self, normalized_shape, eps=1e-05, device=None, dtype=None, **kwargs):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.empty(normalized_shape, device=device, dtype=dtype))
        self.eps = eps

    def forward(self, hidden_states: 'torch.Tensor'):
        input_dtype = hidden_states.dtype
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)
        return self.weight * hidden_states


class Downsample2D(nn.Module):
    """A 2D downsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        padding (`int`, default `1`):
            padding for the convolution.
        name (`str`, default `conv`):
            name of the downsampling 2D layer.
    """

    def __init__(self, channels: 'int', use_conv: 'bool'=False, out_channels: 'Optional[int]'=None, padding: 'int'=1, name: 'str'='conv', kernel_size=3, norm_type=None, eps=None, elementwise_affine=None, bias=True):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.padding = padding
        stride = 2
        self.name = name
        if norm_type == 'ln_norm':
            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)
        elif norm_type == 'rms_norm':
            self.norm = RMSNorm(channels, eps, elementwise_affine)
        elif norm_type is None:
            self.norm = None
        else:
            raise ValueError(f'unknown norm_type: {norm_type}')
        if use_conv:
            conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)
        else:
            assert self.channels == self.out_channels
            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)
        if name == 'conv':
            self.Conv2d_0 = conv
            self.conv = conv
        elif name == 'Conv2d_0':
            self.conv = conv
        else:
            self.conv = conv

    def forward(self, hidden_states: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        assert hidden_states.shape[1] == self.channels
        if self.norm is not None:
            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        if self.use_conv and self.padding == 0:
            pad = 0, 1, 0, 1
            hidden_states = F.pad(hidden_states, pad, mode='constant', value=0)
        assert hidden_states.shape[1] == self.channels
        hidden_states = self.conv(hidden_states)
        return hidden_states


_torch_version = 'N/A'


STR_OPERATION_TO_FUNC = {'>': op.gt, '>=': op.ge, '==': op.eq, '!=': op.ne, '<=': op.le, '<': op.lt}


def compare_versions(library_or_version: 'Union[str, Version]', operation: 'str', requirement_version: 'str'):
    """
    Compares a library version to some requirement using a given operation.

    Args:
        library_or_version (`str` or `packaging.version.Version`):
            A library name or a version to check.
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`.
        requirement_version (`str`):
            The version to compare the library version against
    """
    if operation not in STR_OPERATION_TO_FUNC.keys():
        raise ValueError(f'`operation` must be one of {list(STR_OPERATION_TO_FUNC.keys())}, received {operation}')
    operation = STR_OPERATION_TO_FUNC[operation]
    if isinstance(library_or_version, str):
        library_or_version = parse(importlib_metadata.version(library_or_version))
    return operation(library_or_version, parse(requirement_version))


def is_torch_version(operation: 'str', version: 'str'):
    """
    Compares the current PyTorch version to a given reference with an operation.

    Args:
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`
        version (`str`):
            A string version of PyTorch
    """
    return compare_versions(parse(_torch_version), operation, version)


class Upsample2D(nn.Module):
    """A 2D upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        use_conv_transpose (`bool`, default `False`):
            option to use a convolution transpose.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        name (`str`, default `conv`):
            name of the upsampling 2D layer.
    """

    def __init__(self, channels: 'int', use_conv: 'bool'=False, use_conv_transpose: 'bool'=False, out_channels: 'Optional[int]'=None, name: 'str'='conv', kernel_size: 'Optional[int]'=None, padding=1, norm_type=None, eps=None, elementwise_affine=None, bias=True, interpolate=True):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.use_conv_transpose = use_conv_transpose
        self.name = name
        self.interpolate = interpolate
        if norm_type == 'ln_norm':
            self.norm = nn.LayerNorm(channels, eps, elementwise_affine)
        elif norm_type == 'rms_norm':
            self.norm = RMSNorm(channels, eps, elementwise_affine)
        elif norm_type is None:
            self.norm = None
        else:
            raise ValueError(f'unknown norm_type: {norm_type}')
        conv = None
        if use_conv_transpose:
            if kernel_size is None:
                kernel_size = 4
            conv = nn.ConvTranspose2d(channels, self.out_channels, kernel_size=kernel_size, stride=2, padding=padding, bias=bias)
        elif use_conv:
            if kernel_size is None:
                kernel_size = 3
            conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, padding=padding, bias=bias)
        if name == 'conv':
            self.conv = conv
        else:
            self.Conv2d_0 = conv

    def forward(self, hidden_states: 'torch.Tensor', output_size: 'Optional[int]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        assert hidden_states.shape[1] == self.channels
        if self.norm is not None:
            hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        if self.use_conv_transpose:
            return self.conv(hidden_states)
        dtype = hidden_states.dtype
        if dtype == torch.bfloat16 and is_torch_version('<', '2.1'):
            hidden_states = hidden_states
        if hidden_states.shape[0] >= 64:
            hidden_states = hidden_states.contiguous()
        if self.interpolate:
            if output_size is None:
                hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode='nearest')
            else:
                hidden_states = F.interpolate(hidden_states, size=output_size, mode='nearest')
        if dtype == torch.bfloat16 and is_torch_version('<', '2.1'):
            hidden_states = hidden_states
        if self.use_conv:
            if self.name == 'conv':
                hidden_states = self.conv(hidden_states)
            else:
                hidden_states = self.Conv2d_0(hidden_states)
        return hidden_states


def upfirdn2d_native(tensor: 'torch.Tensor', kernel: 'torch.Tensor', up: 'int'=1, down: 'int'=1, pad: 'Tuple[int, int]'=(0, 0)) ->torch.Tensor:
    up_x = up_y = up
    down_x = down_y = down
    pad_x0 = pad_y0 = pad[0]
    pad_x1 = pad_y1 = pad[1]
    _, channel, in_h, in_w = tensor.shape
    tensor = tensor.reshape(-1, in_h, in_w, 1)
    _, in_h, in_w, minor = tensor.shape
    kernel_h, kernel_w = kernel.shape
    out = tensor.view(-1, in_h, 1, in_w, 1, minor)
    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])
    out = out.view(-1, in_h * up_y, in_w * up_x, minor)
    out = F.pad(out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)])
    out = out
    out = out[:, max(-pad_y0, 0):out.shape[1] - max(-pad_y1, 0), max(-pad_x0, 0):out.shape[2] - max(-pad_x1, 0), :]
    out = out.permute(0, 3, 1, 2)
    out = out.reshape([-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1])
    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)
    out = F.conv2d(out, w)
    out = out.reshape(-1, minor, in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1, in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1)
    out = out.permute(0, 2, 3, 1)
    out = out[:, ::down_y, ::down_x, :]
    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1
    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1
    return out.view(-1, channel, out_h, out_w)


def downsample_2d(hidden_states: 'torch.Tensor', kernel: 'Optional[torch.Tensor]'=None, factor: 'int'=2, gain: 'float'=1) ->torch.Tensor:
    """Downsample2D a batch of 2D images with the given filter.
    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and downsamples each image with the
    given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the
    specified `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its
    shape is a multiple of the downsampling factor.

    Args:
        hidden_states (`torch.Tensor`)
            Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
        kernel (`torch.Tensor`, *optional*):
            FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] * factor`, which
            corresponds to average pooling.
        factor (`int`, *optional*, default to `2`):
            Integer downsampling factor.
        gain (`float`, *optional*, default to `1.0`):
            Scaling factor for signal magnitude.

    Returns:
        output (`torch.Tensor`):
            Tensor of the shape `[N, C, H // factor, W // factor]`
    """
    assert isinstance(factor, int) and factor >= 1
    if kernel is None:
        kernel = [1] * factor
    kernel = torch.tensor(kernel, dtype=torch.float32)
    if kernel.ndim == 1:
        kernel = torch.outer(kernel, kernel)
    kernel /= torch.sum(kernel)
    kernel = kernel * gain
    pad_value = kernel.shape[0] - factor
    output = upfirdn2d_native(hidden_states, kernel, down=factor, pad=((pad_value + 1) // 2, pad_value // 2))
    return output


ACTIVATION_FUNCTIONS = {'swish': nn.SiLU(), 'silu': nn.SiLU(), 'mish': nn.Mish(), 'gelu': nn.GELU(), 'relu': nn.ReLU()}


def get_activation(act_fn: 'str') ->nn.Module:
    """Helper function to get activation function from string.

    Args:
        act_fn (str): Name of activation function.

    Returns:
        nn.Module: Activation function.
    """
    act_fn = act_fn.lower()
    if act_fn in ACTIVATION_FUNCTIONS:
        return ACTIVATION_FUNCTIONS[act_fn]
    else:
        raise ValueError(f'Unsupported activation function: {act_fn}')


def upsample_2d(hidden_states: 'torch.Tensor', kernel: 'Optional[torch.Tensor]'=None, factor: 'int'=2, gain: 'float'=1) ->torch.Tensor:
    """Upsample2D a batch of 2D images with the given filter.
    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and upsamples each image with the given
    filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified
    `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is
    a: multiple of the upsampling factor.

    Args:
        hidden_states (`torch.Tensor`):
            Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
        kernel (`torch.Tensor`, *optional*):
            FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] * factor`, which
            corresponds to nearest-neighbor upsampling.
        factor (`int`, *optional*, default to `2`):
            Integer upsampling factor.
        gain (`float`, *optional*, default to `1.0`):
            Scaling factor for signal magnitude (default: 1.0).

    Returns:
        output (`torch.Tensor`):
            Tensor of the shape `[N, C, H * factor, W * factor]`
    """
    assert isinstance(factor, int) and factor >= 1
    if kernel is None:
        kernel = [1] * factor
    kernel = torch.tensor(kernel, dtype=torch.float32)
    if kernel.ndim == 1:
        kernel = torch.outer(kernel, kernel)
    kernel /= torch.sum(kernel)
    kernel = kernel * (gain * factor ** 2)
    pad_value = kernel.shape[0] - factor
    output = upfirdn2d_native(hidden_states, kernel, up=factor, pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2))
    return output


class ResnetBlock2D(nn.Module):
    """
    A Resnet block.

    Parameters:
        in_channels (`int`): The number of channels in the input.
        out_channels (`int`, *optional*, default to be `None`):
            The number of output channels for the first conv2d layer. If None, same as `in_channels`.
        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.
        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.
        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.
        groups_out (`int`, *optional*, default to None):
            The number of groups to use for the second normalization layer. if set to None, same as `groups`.
        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.
        non_linearity (`str`, *optional*, default to `"swish"`): the activation function to use.
        time_embedding_norm (`str`, *optional*, default to `"default"` ): Time scale shift config.
            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose "scale_shift" for a
            stronger conditioning with scale and shift.
        kernel (`torch.Tensor`, optional, default to None): FIR filter, see
            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].
        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.
        use_in_shortcut (`bool`, *optional*, default to `True`):
            If `True`, add a 1x1 nn.conv2d layer for skip-connection.
        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.
        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.
        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the
            `conv_shortcut` output.
        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.
            If None, same as `out_channels`.
    """

    def __init__(self, *, in_channels: int, out_channels: Optional[int]=None, conv_shortcut: bool=False, dropout: float=0.0, temb_channels: int=512, groups: int=32, groups_out: Optional[int]=None, pre_norm: bool=True, eps: float=1e-06, non_linearity: str='swish', skip_time_act: bool=False, time_embedding_norm: str='default', kernel: Optional[torch.Tensor]=None, output_scale_factor: float=1.0, use_in_shortcut: Optional[bool]=None, up: bool=False, down: bool=False, conv_shortcut_bias: bool=True, conv_2d_out_channels: Optional[int]=None):
        super().__init__()
        if time_embedding_norm == 'ada_group':
            raise ValueError('This class cannot be used with `time_embedding_norm==ada_group`, please use `ResnetBlockCondNorm2D` instead')
        if time_embedding_norm == 'spatial':
            raise ValueError('This class cannot be used with `time_embedding_norm==spatial`, please use `ResnetBlockCondNorm2D` instead')
        self.pre_norm = True
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut
        self.up = up
        self.down = down
        self.output_scale_factor = output_scale_factor
        self.time_embedding_norm = time_embedding_norm
        self.skip_time_act = skip_time_act
        if groups_out is None:
            groups_out = groups
        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
        if temb_channels is not None:
            if self.time_embedding_norm == 'default':
                self.time_emb_proj = nn.Linear(temb_channels, out_channels)
            elif self.time_embedding_norm == 'scale_shift':
                self.time_emb_proj = nn.Linear(temb_channels, 2 * out_channels)
            else:
                raise ValueError(f'unknown time_embedding_norm : {self.time_embedding_norm} ')
        else:
            self.time_emb_proj = None
        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)
        self.dropout = torch.nn.Dropout(dropout)
        conv_2d_out_channels = conv_2d_out_channels or out_channels
        self.conv2 = nn.Conv2d(out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1)
        self.nonlinearity = get_activation(non_linearity)
        self.upsample = self.downsample = None
        if self.up:
            if kernel == 'fir':
                fir_kernel = 1, 3, 3, 1
                self.upsample = lambda x: upsample_2d(x, kernel=fir_kernel)
            elif kernel == 'sde_vp':
                self.upsample = partial(F.interpolate, scale_factor=2.0, mode='nearest')
            else:
                self.upsample = Upsample2D(in_channels, use_conv=False)
        elif self.down:
            if kernel == 'fir':
                fir_kernel = 1, 3, 3, 1
                self.downsample = lambda x: downsample_2d(x, kernel=fir_kernel)
            elif kernel == 'sde_vp':
                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)
            else:
                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name='op')
        self.use_in_shortcut = self.in_channels != conv_2d_out_channels if use_in_shortcut is None else use_in_shortcut
        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = nn.Conv2d(in_channels, conv_2d_out_channels, kernel_size=1, stride=1, padding=0, bias=conv_shortcut_bias)

    def forward(self, input_tensor: 'torch.Tensor', temb: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        hidden_states = input_tensor
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        if self.upsample is not None:
            if hidden_states.shape[0] >= 64:
                input_tensor = input_tensor.contiguous()
                hidden_states = hidden_states.contiguous()
            input_tensor = self.upsample(input_tensor)
            hidden_states = self.upsample(hidden_states)
        elif self.downsample is not None:
            input_tensor = self.downsample(input_tensor)
            hidden_states = self.downsample(hidden_states)
        hidden_states = self.conv1(hidden_states)
        if self.time_emb_proj is not None:
            if not self.skip_time_act:
                temb = self.nonlinearity(temb)
            temb = self.time_emb_proj(temb)[:, :, None, None]
        if self.time_embedding_norm == 'default':
            if temb is not None:
                hidden_states = hidden_states + temb
            hidden_states = self.norm2(hidden_states)
        elif self.time_embedding_norm == 'scale_shift':
            if temb is None:
                raise ValueError(f' `temb` should not be None when `time_embedding_norm` is {self.time_embedding_norm}')
            time_scale, time_shift = torch.chunk(temb, 2, dim=1)
            hidden_states = self.norm2(hidden_states)
            hidden_states = hidden_states * (1 + time_scale) + time_shift
        else:
            hidden_states = self.norm2(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)
        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)
        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor
        return output_tensor


BITSANDBYTES_IMPORT_ERROR = """
{0} requires the bitsandbytes library but it was not found in your environment. You can install it with pip: `pip install bitsandbytes`
"""


BS4_IMPORT_ERROR = """
{0} requires the Beautiful Soup library but it was not found in your environment. You can install it with pip:
`pip install beautifulsoup4`. Please note that you may need to restart your runtime after installation.
"""


COMPEL_IMPORT_ERROR = """
{0} requires the compel library but it was not found in your environment. You can install it with pip: `pip install compel`
"""


FLAX_IMPORT_ERROR = """
{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the
installation page: https://github.com/google/flax and follow the ones that match your environment.
"""


FTFY_IMPORT_ERROR = """
{0} requires the ftfy library but it was not found in your environment. Checkout the instructions on the
installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
"""


IMAGEIO_IMPORT_ERROR = """
{0} requires the imageio library and ffmpeg but it was not found in your environment. You can install it with pip: `pip install imageio imageio-ffmpeg`
"""


INFLECT_IMPORT_ERROR = """
{0} requires the inflect library but it was not found in your environment. You can install it with pip: `pip install
inflect`
"""


INVISIBLE_WATERMARK_IMPORT_ERROR = """
{0} requires the invisible-watermark library but it was not found in your environment. You can install it with pip: `pip install invisible-watermark>=0.2.0`
"""


K_DIFFUSION_IMPORT_ERROR = """
{0} requires the k-diffusion library but it was not found in your environment. You can install it with pip: `pip
install k-diffusion`
"""


LIBROSA_IMPORT_ERROR = """
{0} requires the librosa library but it was not found in your environment.  Checkout the instructions on the
installation page: https://librosa.org/doc/latest/install.html and follow the ones that match your environment.
"""


NOTE_SEQ_IMPORT_ERROR = """
{0} requires the note-seq library but it was not found in your environment. You can install it with pip: `pip
install note-seq`
"""


ONNX_IMPORT_ERROR = """
{0} requires the onnxruntime library but it was not found in your environment. You can install it with pip: `pip
install onnxruntime`
"""


OPENCV_IMPORT_ERROR = """
{0} requires the OpenCV library but it was not found in your environment. You can install it with pip: `pip
install opencv-python`
"""


PEFT_IMPORT_ERROR = """
{0} requires the peft library but it was not found in your environment. You can install it with pip: `pip install peft`
"""


PYTORCH_IMPORT_ERROR = """
{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the
installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
"""


SAFETENSORS_IMPORT_ERROR = """
{0} requires the safetensors library but it was not found in your environment. You can install it with pip: `pip install safetensors`
"""


SCIPY_IMPORT_ERROR = """
{0} requires the scipy library but it was not found in your environment. You can install it with pip: `pip install
scipy`
"""


SENTENCEPIECE_IMPORT_ERROR = """
{0} requires the sentencepiece library but it was not found in your environment. You can install it with pip: `pip install sentencepiece`
"""


TENSORBOARD_IMPORT_ERROR = """
{0} requires the tensorboard library but it was not found in your environment. You can install it with pip: `pip
install tensorboard`
"""


TORCHSDE_IMPORT_ERROR = """
{0} requires the torchsde library but it was not found in your environment. You can install it with pip: `pip install torchsde`
"""


TRANSFORMERS_IMPORT_ERROR = """
{0} requires the transformers library but it was not found in your environment. You can install it with pip: `pip
install transformers`
"""


UNIDECODE_IMPORT_ERROR = """
{0} requires the unidecode library but it was not found in your environment. You can install it with pip: `pip install
Unidecode`
"""


WANDB_IMPORT_ERROR = """
{0} requires the wandb library but it was not found in your environment. You can install it with pip: `pip
install wandb`
"""


def is_bitsandbytes_available():
    return _bitsandbytes_available


def is_bs4_available():
    return _bs4_available


def is_compel_available():
    return _compel_available


def is_flax_available():
    return _flax_available


def is_ftfy_available():
    return _ftfy_available


def is_imageio_available():
    return _imageio_available


def is_inflect_available():
    return _inflect_available


def is_invisible_watermark_available():
    return _invisible_watermark_available


def is_k_diffusion_available():
    return _k_diffusion_available


def is_librosa_available():
    return _librosa_available


def is_note_seq_available():
    return _note_seq_available


def is_onnx_available():
    return _onnx_available


def is_opencv_available():
    return _opencv_available


def is_peft_available():
    return _peft_available


def is_safetensors_available():
    return _safetensors_available


def is_scipy_available():
    return _scipy_available


def is_sentencepiece_available():
    return _sentencepiece_available


def is_tensorboard_available():
    return _tensorboard_available


def is_torch_available():
    return _torch_available


def is_torchsde_available():
    return _torchsde_available


def is_transformers_available():
    return _transformers_available


def is_unidecode_available():
    return _unidecode_available


def is_wandb_available():
    return _wandb_available


BACKENDS_MAPPING = OrderedDict([('bs4', (is_bs4_available, BS4_IMPORT_ERROR)), ('flax', (is_flax_available, FLAX_IMPORT_ERROR)), ('inflect', (is_inflect_available, INFLECT_IMPORT_ERROR)), ('onnx', (is_onnx_available, ONNX_IMPORT_ERROR)), ('opencv', (is_opencv_available, OPENCV_IMPORT_ERROR)), ('scipy', (is_scipy_available, SCIPY_IMPORT_ERROR)), ('torch', (is_torch_available, PYTORCH_IMPORT_ERROR)), ('transformers', (is_transformers_available, TRANSFORMERS_IMPORT_ERROR)), ('unidecode', (is_unidecode_available, UNIDECODE_IMPORT_ERROR)), ('librosa', (is_librosa_available, LIBROSA_IMPORT_ERROR)), ('k_diffusion', (is_k_diffusion_available, K_DIFFUSION_IMPORT_ERROR)), ('note_seq', (is_note_seq_available, NOTE_SEQ_IMPORT_ERROR)), ('wandb', (is_wandb_available, WANDB_IMPORT_ERROR)), ('tensorboard', (is_tensorboard_available, TENSORBOARD_IMPORT_ERROR)), ('compel', (is_compel_available, COMPEL_IMPORT_ERROR)), ('ftfy', (is_ftfy_available, FTFY_IMPORT_ERROR)), ('torchsde', (is_torchsde_available, TORCHSDE_IMPORT_ERROR)), ('invisible_watermark', (is_invisible_watermark_available, INVISIBLE_WATERMARK_IMPORT_ERROR)), ('peft', (is_peft_available, PEFT_IMPORT_ERROR)), ('safetensors', (is_safetensors_available, SAFETENSORS_IMPORT_ERROR)), ('bitsandbytes', (is_bitsandbytes_available, BITSANDBYTES_IMPORT_ERROR)), ('sentencepiece', (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)), ('imageio', (is_imageio_available, IMAGEIO_IMPORT_ERROR))])


def is_transformers_version(operation: 'str', version: 'str'):
    """
    Compares the current Transformers version to a given reference with an operation.

    Args:
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`
        version (`str`):
            A version string
    """
    if not _transformers_available:
        return False
    return compare_versions(parse(_transformers_version), operation, version)


def requires_backends(obj, backends):
    if not isinstance(backends, (list, tuple)):
        backends = [backends]
    name = obj.__name__ if hasattr(obj, '__name__') else obj.__class__.__name__
    checks = (BACKENDS_MAPPING[backend] for backend in backends)
    failed = [msg.format(name) for available, msg in checks if not available()]
    if failed:
        raise ImportError(''.join(failed))
    if name in ['VersatileDiffusionTextToImagePipeline', 'VersatileDiffusionPipeline', 'VersatileDiffusionDualGuidedPipeline', 'StableDiffusionImageVariationPipeline', 'UnCLIPPipeline'] and is_transformers_version('<', '4.25.0'):
        raise ImportError(f'You need to install `transformers>=4.25` in order to use {name}: \n```\n pip install --upgrade transformers \n```')
    if name in ['StableDiffusionDepth2ImgPipeline', 'StableDiffusionPix2PixZeroPipeline'] and is_transformers_version('<', '4.26.0'):
        raise ImportError(f'You need to install `transformers>=4.26` in order to use {name}: \n```\n pip install --upgrade transformers \n```')


class DummyObject(type):
    """
    Metaclass for the dummy objects. Any class inheriting from it will return the ImportError generated by
    `requires_backend` each time a user tries to access any method of that class.
    """

    def __getattr__(cls, key):
        if key.startswith('_') and key not in ['_load_connected_pipes', '_is_onnx']:
            return super().__getattr__(cls, key)
        requires_backends(cls, cls._backends)


class Transformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class CrossAttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads=1, cross_attention_dim=1280, output_scale_factor=1.0, downsample_padding=1, add_downsample=True, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(cross_attention_dim, int):
            cross_attention_dim = cross_attention_dim,
        if isinstance(cross_attention_dim, (list, tuple)) and len(cross_attention_dim) > 4:
            raise ValueError(f'Only up to 4 cross-attention layers are supported. Ensure that the length of cross-attention dims is less than or equal to 4. Got cross-attention dims {cross_attention_dim} of length {len(cross_attention_dim)}')
        self.cross_attention_dim = cross_attention_dim
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            for j in range(len(cross_attention_dim)):
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block, cross_attention_dim=cross_attention_dim[j], norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, double_self_attention=True if cross_attention_dim[j] is None else False))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, encoder_hidden_states_1: 'Optional[torch.Tensor]'=None, encoder_attention_mask_1: 'Optional[torch.Tensor]'=None):
        output_states = ()
        num_layers = len(self.resnets)
        num_attention_per_layer = len(self.attentions) // num_layers
        encoder_hidden_states_1 = encoder_hidden_states_1 if encoder_hidden_states_1 is not None else encoder_hidden_states
        encoder_attention_mask_1 = encoder_attention_mask_1 if encoder_hidden_states_1 is not None else encoder_attention_mask
        for i in range(num_layers):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(self.resnets[i]), hidden_states, temb, **ckpt_kwargs)
                for idx, cross_attention_dim in enumerate(self.cross_attention_dim):
                    if cross_attention_dim is not None and idx <= 1:
                        forward_encoder_hidden_states = encoder_hidden_states
                        forward_encoder_attention_mask = encoder_attention_mask
                    elif cross_attention_dim is not None and idx > 1:
                        forward_encoder_hidden_states = encoder_hidden_states_1
                        forward_encoder_attention_mask = encoder_attention_mask_1
                    else:
                        forward_encoder_hidden_states = None
                        forward_encoder_attention_mask = None
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(self.attentions[i * num_attention_per_layer + idx], return_dict=False), hidden_states, forward_encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, forward_encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = self.resnets[i](hidden_states, temb)
                for idx, cross_attention_dim in enumerate(self.cross_attention_dim):
                    if cross_attention_dim is not None and idx <= 1:
                        forward_encoder_hidden_states = encoder_hidden_states
                        forward_encoder_attention_mask = encoder_attention_mask
                    elif cross_attention_dim is not None and idx > 1:
                        forward_encoder_hidden_states = encoder_hidden_states_1
                        forward_encoder_attention_mask = encoder_attention_mask_1
                    else:
                        forward_encoder_hidden_states = None
                        forward_encoder_attention_mask = None
                    hidden_states = self.attentions[i * num_attention_per_layer + idx](hidden_states, attention_mask=attention_mask, encoder_hidden_states=forward_encoder_hidden_states, encoder_attention_mask=forward_encoder_attention_mask, return_dict=False)[0]
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class UNetMidBlock2DCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads=1, output_scale_factor=1.0, cross_attention_dim=1280, use_linear_projection=False, upcast_attention=False):
        super().__init__()
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        if isinstance(cross_attention_dim, int):
            cross_attention_dim = cross_attention_dim,
        if isinstance(cross_attention_dim, (list, tuple)) and len(cross_attention_dim) > 4:
            raise ValueError(f'Only up to 4 cross-attention layers are supported. Ensure that the length of cross-attention dims is less than or equal to 4. Got cross-attention dims {cross_attention_dim} of length {len(cross_attention_dim)}')
        self.cross_attention_dim = cross_attention_dim
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        for i in range(num_layers):
            for j in range(len(cross_attention_dim)):
                attentions.append(Transformer2DModel(num_attention_heads, in_channels // num_attention_heads, in_channels=in_channels, num_layers=transformer_layers_per_block, cross_attention_dim=cross_attention_dim[j], norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention, double_self_attention=True if cross_attention_dim[j] is None else False))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, encoder_hidden_states_1: 'Optional[torch.Tensor]'=None, encoder_attention_mask_1: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        num_attention_per_layer = len(self.attentions) // (len(self.resnets) - 1)
        encoder_hidden_states_1 = encoder_hidden_states_1 if encoder_hidden_states_1 is not None else encoder_hidden_states
        encoder_attention_mask_1 = encoder_attention_mask_1 if encoder_hidden_states_1 is not None else encoder_attention_mask
        for i in range(len(self.resnets[1:])):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                for idx, cross_attention_dim in enumerate(self.cross_attention_dim):
                    if cross_attention_dim is not None and idx <= 1:
                        forward_encoder_hidden_states = encoder_hidden_states
                        forward_encoder_attention_mask = encoder_attention_mask
                    elif cross_attention_dim is not None and idx > 1:
                        forward_encoder_hidden_states = encoder_hidden_states_1
                        forward_encoder_attention_mask = encoder_attention_mask_1
                    else:
                        forward_encoder_hidden_states = None
                        forward_encoder_attention_mask = None
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(self.attentions[i * num_attention_per_layer + idx], return_dict=False), hidden_states, forward_encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, forward_encoder_attention_mask, **ckpt_kwargs)[0]
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(self.resnets[i + 1]), hidden_states, temb, **ckpt_kwargs)
            else:
                for idx, cross_attention_dim in enumerate(self.cross_attention_dim):
                    if cross_attention_dim is not None and idx <= 1:
                        forward_encoder_hidden_states = encoder_hidden_states
                        forward_encoder_attention_mask = encoder_attention_mask
                    elif cross_attention_dim is not None and idx > 1:
                        forward_encoder_hidden_states = encoder_hidden_states_1
                        forward_encoder_attention_mask = encoder_attention_mask_1
                    else:
                        forward_encoder_hidden_states = None
                        forward_encoder_attention_mask = None
                    hidden_states = self.attentions[i * num_attention_per_layer + idx](hidden_states, attention_mask=attention_mask, encoder_hidden_states=forward_encoder_hidden_states, encoder_attention_mask=forward_encoder_attention_mask, return_dict=False)[0]
                hidden_states = self.resnets[i + 1](hidden_states, temb)
        return hidden_states


class CrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads=1, cross_attention_dim=1280, output_scale_factor=1.0, add_upsample=True, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(cross_attention_dim, int):
            cross_attention_dim = cross_attention_dim,
        if isinstance(cross_attention_dim, (list, tuple)) and len(cross_attention_dim) > 4:
            raise ValueError(f'Only up to 4 cross-attention layers are supported. Ensure that the length of cross-attention dims is less than or equal to 4. Got cross-attention dims {cross_attention_dim} of length {len(cross_attention_dim)}')
        self.cross_attention_dim = cross_attention_dim
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            for j in range(len(cross_attention_dim)):
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block, cross_attention_dim=cross_attention_dim[j], norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, double_self_attention=True if cross_attention_dim[j] is None else False))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, encoder_hidden_states_1: 'Optional[torch.Tensor]'=None, encoder_attention_mask_1: 'Optional[torch.Tensor]'=None):
        num_layers = len(self.resnets)
        num_attention_per_layer = len(self.attentions) // num_layers
        encoder_hidden_states_1 = encoder_hidden_states_1 if encoder_hidden_states_1 is not None else encoder_hidden_states
        encoder_attention_mask_1 = encoder_attention_mask_1 if encoder_hidden_states_1 is not None else encoder_attention_mask
        for i in range(num_layers):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(self.resnets[i]), hidden_states, temb, **ckpt_kwargs)
                for idx, cross_attention_dim in enumerate(self.cross_attention_dim):
                    if cross_attention_dim is not None and idx <= 1:
                        forward_encoder_hidden_states = encoder_hidden_states
                        forward_encoder_attention_mask = encoder_attention_mask
                    elif cross_attention_dim is not None and idx > 1:
                        forward_encoder_hidden_states = encoder_hidden_states_1
                        forward_encoder_attention_mask = encoder_attention_mask_1
                    else:
                        forward_encoder_hidden_states = None
                        forward_encoder_attention_mask = None
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(self.attentions[i * num_attention_per_layer + idx], return_dict=False), hidden_states, forward_encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, forward_encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = self.resnets[i](hidden_states, temb)
                for idx, cross_attention_dim in enumerate(self.cross_attention_dim):
                    if cross_attention_dim is not None and idx <= 1:
                        forward_encoder_hidden_states = encoder_hidden_states
                        forward_encoder_attention_mask = encoder_attention_mask
                    elif cross_attention_dim is not None and idx > 1:
                        forward_encoder_hidden_states = encoder_hidden_states_1
                        forward_encoder_attention_mask = encoder_attention_mask_1
                    else:
                        forward_encoder_hidden_states = None
                        forward_encoder_attention_mask = None
                    hidden_states = self.attentions[i * num_attention_per_layer + idx](hidden_states, attention_mask=attention_mask, encoder_hidden_states=forward_encoder_hidden_states, encoder_attention_mask=forward_encoder_attention_mask, return_dict=False)[0]
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class AttnAddedKVProcessor:
    """
    Processor for performing attention-related computations with extra learnable key and value matrices for the text
    encoder.
    """

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class AttnAddedKVProcessor2_0:
    """
    Processor for performing scaled dot-product attention (enabled by default if you're using PyTorch 2.0), with extra
    learnable key and value matrices for the text encoder.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnAddedKVProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size, out_dim=4)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        query = attn.head_to_batch_dim(query, out_dim=4)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj, out_dim=4)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj, out_dim=4)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key, out_dim=4)
            value = attn.head_to_batch_dim(value, out_dim=4)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=2)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=2)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, residual.shape[1])
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class AttnProcessor:
    """
    Default processor for performing attention-related computations.
    """

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class AttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class AttnProcessorNPU:
    """
    Processor for implementing flash attention using torch_npu. Torch_npu supports only fp16 and bf16 data types. If
    fp32 is used, F.scaled_dot_product_attention will be used for computation, but the acceleration effect on NPU is
    not significant.

    """

    def __init__(self):
        if not is_torch_npu_available():
            raise ImportError('AttnProcessorNPU requires torch_npu extensions and is supported only on npu devices.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if query.dtype in (torch.float16, torch.bfloat16):
            hidden_states = torch_npu.npu_fusion_attention(query, key, value, attn.heads, input_layout='BNSD', pse=None, atten_mask=attention_mask, scale=1.0 / math.sqrt(query.shape[-1]), pre_tockens=65536, next_tockens=65536, keep_prob=1.0, sync=False, inner_precise=0)[0]
        else:
            hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class CustomDiffusionAttnProcessor(nn.Module):
    """
    Processor for implementing attention for the Custom Diffusion method.

    Args:
        train_kv (`bool`, defaults to `True`):
            Whether to newly train the key and value matrices corresponding to the text features.
        train_q_out (`bool`, defaults to `True`):
            Whether to newly train query matrices corresponding to the latent image features.
        hidden_size (`int`, *optional*, defaults to `None`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*, defaults to `None`):
            The number of channels in the `encoder_hidden_states`.
        out_bias (`bool`, defaults to `True`):
            Whether to include the bias parameter in `train_q_out`.
        dropout (`float`, *optional*, defaults to 0.0):
            The dropout probability to use.
    """

    def __init__(self, train_kv: 'bool'=True, train_q_out: 'bool'=True, hidden_size: 'Optional[int]'=None, cross_attention_dim: 'Optional[int]'=None, out_bias: 'bool'=True, dropout: 'float'=0.0):
        super().__init__()
        self.train_kv = train_kv
        self.train_q_out = train_q_out
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        if self.train_kv:
            self.to_k_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
            self.to_v_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        if self.train_q_out:
            self.to_q_custom_diffusion = nn.Linear(hidden_size, hidden_size, bias=False)
            self.to_out_custom_diffusion = nn.ModuleList([])
            self.to_out_custom_diffusion.append(nn.Linear(hidden_size, hidden_size, bias=out_bias))
            self.to_out_custom_diffusion.append(nn.Dropout(dropout))

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if self.train_q_out:
            query = self.to_q_custom_diffusion(hidden_states)
        else:
            query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            crossattn = False
            encoder_hidden_states = hidden_states
        else:
            crossattn = True
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        if self.train_kv:
            key = self.to_k_custom_diffusion(encoder_hidden_states)
            value = self.to_v_custom_diffusion(encoder_hidden_states)
            key = key
            value = value
        else:
            key = attn.to_k(encoder_hidden_states)
            value = attn.to_v(encoder_hidden_states)
        if crossattn:
            detach = torch.ones_like(key)
            detach[:, :1, :] = detach[:, :1, :] * 0.0
            key = detach * key + (1 - detach) * key.detach()
            value = detach * value + (1 - detach) * value.detach()
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        if self.train_q_out:
            hidden_states = self.to_out_custom_diffusion[0](hidden_states)
            hidden_states = self.to_out_custom_diffusion[1](hidden_states)
        else:
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
        return hidden_states


class CustomDiffusionAttnProcessor2_0(nn.Module):
    """
    Processor for implementing attention for the Custom Diffusion method using PyTorch 2.0’s memory-efficient scaled
    dot-product attention.

    Args:
        train_kv (`bool`, defaults to `True`):
            Whether to newly train the key and value matrices corresponding to the text features.
        train_q_out (`bool`, defaults to `True`):
            Whether to newly train query matrices corresponding to the latent image features.
        hidden_size (`int`, *optional*, defaults to `None`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*, defaults to `None`):
            The number of channels in the `encoder_hidden_states`.
        out_bias (`bool`, defaults to `True`):
            Whether to include the bias parameter in `train_q_out`.
        dropout (`float`, *optional*, defaults to 0.0):
            The dropout probability to use.
    """

    def __init__(self, train_kv: 'bool'=True, train_q_out: 'bool'=True, hidden_size: 'Optional[int]'=None, cross_attention_dim: 'Optional[int]'=None, out_bias: 'bool'=True, dropout: 'float'=0.0):
        super().__init__()
        self.train_kv = train_kv
        self.train_q_out = train_q_out
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        if self.train_kv:
            self.to_k_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
            self.to_v_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        if self.train_q_out:
            self.to_q_custom_diffusion = nn.Linear(hidden_size, hidden_size, bias=False)
            self.to_out_custom_diffusion = nn.ModuleList([])
            self.to_out_custom_diffusion.append(nn.Linear(hidden_size, hidden_size, bias=out_bias))
            self.to_out_custom_diffusion.append(nn.Dropout(dropout))

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if self.train_q_out:
            query = self.to_q_custom_diffusion(hidden_states)
        else:
            query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            crossattn = False
            encoder_hidden_states = hidden_states
        else:
            crossattn = True
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        if self.train_kv:
            key = self.to_k_custom_diffusion(encoder_hidden_states)
            value = self.to_v_custom_diffusion(encoder_hidden_states)
            key = key
            value = value
        else:
            key = attn.to_k(encoder_hidden_states)
            value = attn.to_v(encoder_hidden_states)
        if crossattn:
            detach = torch.ones_like(key)
            detach[:, :1, :] = detach[:, :1, :] * 0.0
            key = detach * key + (1 - detach) * key.detach()
            value = detach * value + (1 - detach) * value.detach()
        inner_dim = hidden_states.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        if self.train_q_out:
            hidden_states = self.to_out_custom_diffusion[0](hidden_states)
            hidden_states = self.to_out_custom_diffusion[1](hidden_states)
        else:
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
        return hidden_states


class CustomDiffusionXFormersAttnProcessor(nn.Module):
    """
    Processor for implementing memory efficient attention using xFormers for the Custom Diffusion method.

    Args:
    train_kv (`bool`, defaults to `True`):
        Whether to newly train the key and value matrices corresponding to the text features.
    train_q_out (`bool`, defaults to `True`):
        Whether to newly train query matrices corresponding to the latent image features.
    hidden_size (`int`, *optional*, defaults to `None`):
        The hidden size of the attention layer.
    cross_attention_dim (`int`, *optional*, defaults to `None`):
        The number of channels in the `encoder_hidden_states`.
    out_bias (`bool`, defaults to `True`):
        Whether to include the bias parameter in `train_q_out`.
    dropout (`float`, *optional*, defaults to 0.0):
        The dropout probability to use.
    attention_op (`Callable`, *optional*, defaults to `None`):
        The base
        [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to use
        as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best operator.
    """

    def __init__(self, train_kv: 'bool'=True, train_q_out: 'bool'=False, hidden_size: 'Optional[int]'=None, cross_attention_dim: 'Optional[int]'=None, out_bias: 'bool'=True, dropout: 'float'=0.0, attention_op: 'Optional[Callable]'=None):
        super().__init__()
        self.train_kv = train_kv
        self.train_q_out = train_q_out
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.attention_op = attention_op
        if self.train_kv:
            self.to_k_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
            self.to_v_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        if self.train_q_out:
            self.to_q_custom_diffusion = nn.Linear(hidden_size, hidden_size, bias=False)
            self.to_out_custom_diffusion = nn.ModuleList([])
            self.to_out_custom_diffusion.append(nn.Linear(hidden_size, hidden_size, bias=out_bias))
            self.to_out_custom_diffusion.append(nn.Dropout(dropout))

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if self.train_q_out:
            query = self.to_q_custom_diffusion(hidden_states)
        else:
            query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            crossattn = False
            encoder_hidden_states = hidden_states
        else:
            crossattn = True
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        if self.train_kv:
            key = self.to_k_custom_diffusion(encoder_hidden_states)
            value = self.to_v_custom_diffusion(encoder_hidden_states)
            key = key
            value = value
        else:
            key = attn.to_k(encoder_hidden_states)
            value = attn.to_v(encoder_hidden_states)
        if crossattn:
            detach = torch.ones_like(key)
            detach[:, :1, :] = detach[:, :1, :] * 0.0
            key = detach * key + (1 - detach) * key.detach()
            value = detach * value + (1 - detach) * value.detach()
        query = attn.head_to_batch_dim(query).contiguous()
        key = attn.head_to_batch_dim(key).contiguous()
        value = attn.head_to_batch_dim(value).contiguous()
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = hidden_states
        hidden_states = attn.batch_to_head_dim(hidden_states)
        if self.train_q_out:
            hidden_states = self.to_out_custom_diffusion[0](hidden_states)
            hidden_states = self.to_out_custom_diffusion[1](hidden_states)
        else:
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
        return hidden_states


class FP32LayerNorm(nn.LayerNorm):

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        origin_dtype = inputs.dtype
        return F.layer_norm(inputs.float(), self.normalized_shape, self.weight.float() if self.weight is not None else None, self.bias.float() if self.bias is not None else None, self.eps)


class LpNorm(nn.Module):

    def __init__(self, p: 'int'=2, dim: 'int'=-1, eps: 'float'=1e-12):
        super().__init__()
        self.p = p
        self.dim = dim
        self.eps = eps

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        return F.normalize(hidden_states, p=self.p, dim=self.dim, eps=self.eps)


class SlicedAttnAddedKVProcessor:
    """
    Processor for implementing sliced attention with extra learnable key and value matrices for the text encoder.

    Args:
        slice_size (`int`, *optional*):
            The number of steps to compute attention. Uses as many slices as `attention_head_dim // slice_size`, and
            `attention_head_dim` must be a multiple of the `slice_size`.
    """

    def __init__(self, slice_size):
        self.slice_size = slice_size

    def __call__(self, attn: "'Attention'", hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        dim = query.shape[-1]
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        batch_size_attention, query_tokens, _ = query.shape
        hidden_states = torch.zeros((batch_size_attention, query_tokens, dim // attn.heads), device=query.device, dtype=query.dtype)
        for i in range((batch_size_attention - 1) // self.slice_size + 1):
            start_idx = i * self.slice_size
            end_idx = (i + 1) * self.slice_size
            query_slice = query[start_idx:end_idx]
            key_slice = key[start_idx:end_idx]
            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None
            attn_slice = attn.get_attention_scores(query_slice, key_slice, attn_mask_slice)
            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])
            hidden_states[start_idx:end_idx] = attn_slice
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class SlicedAttnProcessor:
    """
    Processor for implementing sliced attention.

    Args:
        slice_size (`int`, *optional*):
            The number of steps to compute attention. Uses as many slices as `attention_head_dim // slice_size`, and
            `attention_head_dim` must be a multiple of the `slice_size`.
    """

    def __init__(self, slice_size: 'int'):
        self.slice_size = slice_size

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        dim = query.shape[-1]
        query = attn.head_to_batch_dim(query)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        batch_size_attention, query_tokens, _ = query.shape
        hidden_states = torch.zeros((batch_size_attention, query_tokens, dim // attn.heads), device=query.device, dtype=query.dtype)
        for i in range((batch_size_attention - 1) // self.slice_size + 1):
            start_idx = i * self.slice_size
            end_idx = (i + 1) * self.slice_size
            query_slice = query[start_idx:end_idx]
            key_slice = key[start_idx:end_idx]
            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None
            attn_slice = attn.get_attention_scores(query_slice, key_slice, attn_mask_slice)
            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])
            hidden_states[start_idx:end_idx] = attn_slice
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class SpatialNorm(nn.Module):
    """
    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002.

    Args:
        f_channels (`int`):
            The number of channels for input to group normalization layer, and output of the spatial norm layer.
        zq_channels (`int`):
            The number of channels for the quantized vector as described in the paper.
    """

    def __init__(self, f_channels: 'int', zq_channels: 'int'):
        super().__init__()
        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=32, eps=1e-06, affine=True)
        self.conv_y = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)
        self.conv_b = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)

    def forward(self, f: 'torch.Tensor', zq: 'torch.Tensor') ->torch.Tensor:
        f_size = f.shape[-2:]
        zq = F.interpolate(zq, size=f_size, mode='nearest')
        norm_f = self.norm_layer(f)
        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)
        return new_f


class XFormersAttnAddedKVProcessor:
    """
    Processor for implementing memory efficient attention using xFormers.

    Args:
        attention_op (`Callable`, *optional*, defaults to `None`):
            The base
            [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to
            use as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best
            operator.
    """

    def __init__(self, attention_op: 'Optional[Callable]'=None):
        self.attention_op = attention_op

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = hidden_states
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class XFormersAttnProcessor:
    """
    Processor for implementing memory efficient attention using xFormers.

    Args:
        attention_op (`Callable`, *optional*, defaults to `None`):
            The base
            [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to
            use as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best
            operator.
    """

    def __init__(self, attention_op: 'Optional[Callable]'=None):
        self.attention_op = attention_op

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, key_tokens, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, key_tokens, batch_size)
        if attention_mask is not None:
            _, query_tokens, _ = hidden_states.shape
            attention_mask = attention_mask.expand(-1, query_tokens, -1)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query = attn.head_to_batch_dim(query).contiguous()
        key = attn.head_to_batch_dim(key).contiguous()
        value = attn.head_to_batch_dim(value).contiguous()
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = hidden_states
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


def is_xformers_available():
    return _xformers_available


class MatryoshkaFeedForward(nn.Module):
    """
    A feed-forward layer for the Matryoshka models.

    Parameters:"""

    def __init__(self, dim: 'int'):
        super().__init__()
        self.group_norm = nn.GroupNorm(32, dim)
        self.linear_gelu = GELU(dim, dim * 4)
        self.linear_out = nn.Linear(dim * 4, dim)

    def forward(self, x):
        batch_size, channels, *spatial_dims = x.shape
        x = self.group_norm(x)
        x = x.view(batch_size, channels, -1).permute(0, 2, 1)
        x = self.linear_out(self.linear_gelu(x))
        x = x.permute(0, 2, 1).view(batch_size, channels, *spatial_dims)
        return x


class MatryoshkaFusedAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). It uses
    fused projection layers. For self-attention modules, all projection matrices (i.e., query, key, value) are fused.
    For cross-attention modules, key and value projection matrices are fused.

    <Tip warning={true}>

    This API is currently 🧪 experimental in nature and can change in future.

    </Tip>
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('MatryoshkaFusedAttnProcessor2_0 requires PyTorch 2.x, to use it. Please upgrade PyTorch to > 2.x.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, self_attention_query: 'Optional[torch.Tensor]'=None, self_attention_output: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states)
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2).contiguous()
        if encoder_hidden_states is None:
            qkv = attn.to_qkv(hidden_states)
            split_size = qkv.shape[-1] // 3
            query, key, value = torch.split(qkv, split_size, dim=-1)
        else:
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
            if self_attention_query is not None:
                query = self_attention_query
            else:
                query = attn.to_q(hidden_states)
            kv = attn.to_kv(encoder_hidden_states)
            split_size = kv.shape[-1] // 2
            key, value = torch.split(kv, split_size, dim=-1)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        if self_attention_output is None:
            query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states
        if self_attention_output is not None:
            hidden_states = hidden_states + self_attention_output
            hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states if self_attention_output is not None else (hidden_states, query)


def _chunked_feed_forward(ff: 'nn.Module', hidden_states: 'torch.Tensor', chunk_dim: 'int', chunk_size: 'int'):
    if hidden_states.shape[chunk_dim] % chunk_size != 0:
        raise ValueError(f'`hidden_states` dimension to be chunked: {hidden_states.shape[chunk_dim]} has to be divisible by chunk size: {chunk_size}. Make sure to set an appropriate `chunk_size` when calling `unet.enable_forward_chunking`.')
    num_chunks = hidden_states.shape[chunk_dim] // chunk_size
    ff_output = torch.cat([ff(hid_slice) for hid_slice in hidden_states.chunk(num_chunks, dim=chunk_dim)], dim=chunk_dim)
    return ff_output


class MatryoshkaTransformerBlock(nn.Module):
    """
    Matryoshka Transformer block.

    Parameters:
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', cross_attention_dim: 'Optional[int]'=None, upcast_attention: 'bool'=False, use_attention_ffn: 'bool'=True):
        super().__init__()
        self.dim = dim
        self.num_attention_heads = num_attention_heads
        self.attention_head_dim = attention_head_dim
        self.cross_attention_dim = cross_attention_dim
        self.attn1 = Attention(query_dim=dim, cross_attention_dim=None, heads=num_attention_heads, dim_head=attention_head_dim, norm_num_groups=32, bias=True, upcast_attention=upcast_attention, pre_only=True, processor=MatryoshkaFusedAttnProcessor2_0())
        self.attn1.fuse_projections()
        del self.attn1.to_q
        del self.attn1.to_k
        del self.attn1.to_v
        if cross_attention_dim is not None and cross_attention_dim > 0:
            self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim, cross_attention_norm='layer_norm', heads=num_attention_heads, dim_head=attention_head_dim, bias=True, upcast_attention=upcast_attention, pre_only=True, processor=MatryoshkaFusedAttnProcessor2_0())
            self.attn2.fuse_projections()
            del self.attn2.to_q
            del self.attn2.to_k
            del self.attn2.to_v
        self.proj_out = nn.Linear(dim, dim)
        if use_attention_ffn:
            self.ff = MatryoshkaFeedForward(dim)
        else:
            self.ff = None
        self._chunk_size = None
        self._chunk_dim = 0

    def set_chunk_feed_forward(self, chunk_size: 'Optional[int]', dim: 'int'=0):
        self._chunk_size = chunk_size
        self._chunk_dim = dim

    def forward(self, hidden_states: 'torch.Tensor', attention_mask: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, timestep: 'Optional[torch.LongTensor]'=None, cross_attention_kwargs: 'Dict[str, Any]'=None, class_labels: 'Optional[torch.LongTensor]'=None, added_cond_kwargs: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        batch_size, channels, *spatial_dims = hidden_states.shape
        attn_output, query = self.attn1(hidden_states)
        if self.cross_attention_dim is not None and self.cross_attention_dim > 0:
            attn_output_cond = self.attn2(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=encoder_attention_mask, self_attention_output=attn_output, self_attention_query=query)
        attn_output_cond = self.proj_out(attn_output_cond)
        attn_output_cond = attn_output_cond.permute(0, 2, 1).reshape(batch_size, channels, *spatial_dims)
        hidden_states = hidden_states + attn_output_cond
        if self.ff is not None:
            if self._chunk_size is not None:
                ff_output = _chunked_feed_forward(self.ff, hidden_states, self._chunk_dim, self._chunk_size)
            else:
                ff_output = self.ff(hidden_states)
            hidden_states = ff_output + hidden_states
        return hidden_states


class TimestepEmbedding(nn.Module):

    def __init__(self, in_channels: 'int', time_embed_dim: 'int', act_fn: 'str'='silu', out_dim: 'int'=None, post_act_fn: 'Optional[str]'=None, cond_proj_dim=None, sample_proj_bias=True):
        super().__init__()
        self.linear_1 = nn.Linear(in_channels, time_embed_dim, sample_proj_bias)
        if cond_proj_dim is not None:
            self.cond_proj = nn.Linear(cond_proj_dim, in_channels, bias=False)
        else:
            self.cond_proj = None
        self.act = get_activation(act_fn)
        if out_dim is not None:
            time_embed_dim_out = out_dim
        else:
            time_embed_dim_out = time_embed_dim
        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim_out, sample_proj_bias)
        if post_act_fn is None:
            self.post_act = None
        else:
            self.post_act = get_activation(post_act_fn)

    def forward(self, sample, condition=None):
        if condition is not None:
            sample = sample + self.cond_proj(condition)
        sample = self.linear_1(sample)
        if self.act is not None:
            sample = self.act(sample)
        sample = self.linear_2(sample)
        if self.post_act is not None:
            sample = self.post_act(sample)
        return sample


def get_timestep_embedding(timesteps: 'torch.Tensor', embedding_dim: 'int', flip_sin_to_cos: 'bool'=False, downscale_freq_shift: 'float'=1, scale: 'float'=1, max_period: 'int'=10000):
    """
    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.

    Args
        timesteps (torch.Tensor):
            a 1-D Tensor of N indices, one per batch element. These may be fractional.
        embedding_dim (int):
            the dimension of the output.
        flip_sin_to_cos (bool):
            Whether the embedding order should be `cos, sin` (if True) or `sin, cos` (if False)
        downscale_freq_shift (float):
            Controls the delta between frequencies between dimensions
        scale (float):
            Scaling factor applied to the embeddings.
        max_period (int):
            Controls the maximum frequency of the embeddings
    Returns
        torch.Tensor: an [N x dim] Tensor of positional embeddings.
    """
    assert len(timesteps.shape) == 1, 'Timesteps should be a 1d-array'
    half_dim = embedding_dim // 2
    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32, device=timesteps.device)
    exponent = exponent / (half_dim - downscale_freq_shift)
    emb = torch.exp(exponent)
    emb = timesteps[:, None].float() * emb[None, :]
    emb = scale * emb
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)
    if flip_sin_to_cos:
        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)
    if embedding_dim % 2 == 1:
        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))
    return emb


class Timesteps(nn.Module):

    def __init__(self, num_channels: 'int', flip_sin_to_cos: 'bool', downscale_freq_shift: 'float', scale: 'int'=1):
        super().__init__()
        self.num_channels = num_channels
        self.flip_sin_to_cos = flip_sin_to_cos
        self.downscale_freq_shift = downscale_freq_shift
        self.scale = scale

    def forward(self, timesteps):
        t_emb = get_timestep_embedding(timesteps, self.num_channels, flip_sin_to_cos=self.flip_sin_to_cos, downscale_freq_shift=self.downscale_freq_shift, scale=self.scale)
        return t_emb


class MatryoshkaCombinedTimestepTextEmbedding(nn.Module):

    def __init__(self, addition_time_embed_dim, cross_attention_dim, time_embed_dim, type):
        super().__init__()
        if type == 'unet':
            self.cond_emb = nn.Linear(cross_attention_dim, time_embed_dim, bias=False)
        elif type == 'nested_unet':
            self.cond_emb = None
        self.add_time_proj = Timesteps(addition_time_embed_dim, flip_sin_to_cos=False, downscale_freq_shift=0)
        self.add_timestep_embedder = TimestepEmbedding(addition_time_embed_dim, time_embed_dim)

    def forward(self, emb, encoder_hidden_states, added_cond_kwargs):
        conditioning_mask = added_cond_kwargs.get('conditioning_mask', None)
        masked_cross_attention = added_cond_kwargs.get('masked_cross_attention', False)
        if self.cond_emb is not None and not added_cond_kwargs.get('from_nested', False):
            if conditioning_mask is None:
                y = encoder_hidden_states.mean(dim=1)
            else:
                y = (conditioning_mask.unsqueeze(-1) * encoder_hidden_states).sum(dim=1) / conditioning_mask.sum(dim=1, keepdim=True)
            cond_emb = self.cond_emb(y)
        else:
            cond_emb = None
        if not masked_cross_attention:
            conditioning_mask = None
        micro = added_cond_kwargs.get('micro_conditioning_scale', None)
        if micro is not None:
            temb = self.add_time_proj(torch.tensor([micro], device=emb.device, dtype=emb.dtype))
            temb_micro_conditioning = self.add_timestep_embedder(temb)
            return temb_micro_conditioning, conditioning_mask, cond_emb
        return None, conditioning_mask, cond_emb


def reshape_tensor(x, heads):
    bs, length, width = x.shape
    x = x.view(bs, length, heads, -1)
    x = x.transpose(1, 2)
    x = x.reshape(bs, heads, length, -1)
    return x


class PerceiverAttention(nn.Module):

    def __init__(self, *, dim, dim_head=64, heads=8):
        super().__init__()
        self.scale = dim_head ** -0.5
        self.dim_head = dim_head
        self.heads = heads
        inner_dim = dim_head * heads
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.to_q = nn.Linear(dim, inner_dim, bias=False)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)
        self.to_out = nn.Linear(inner_dim, dim, bias=False)

    def forward(self, x, latents):
        """
        Args:
            x (torch.Tensor): image features
                shape (b, n1, D)
            latent (torch.Tensor): latent features
                shape (b, n2, D)
        """
        x = self.norm1(x)
        latents = self.norm2(latents)
        b, l, _ = latents.shape
        q = self.to_q(latents)
        kv_input = torch.cat((x, latents), dim=-2)
        k, v = self.to_kv(kv_input).chunk(2, dim=-1)
        q = reshape_tensor(q, self.heads)
        k = reshape_tensor(k, self.heads)
        v = reshape_tensor(v, self.heads)
        scale = 1 / math.sqrt(math.sqrt(self.dim_head))
        weight = q * scale @ (k * scale).transpose(-2, -1)
        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
        out = weight @ v
        out = out.permute(0, 2, 1, 3).reshape(b, l, -1)
        return self.to_out(out)


class Resampler(nn.Module):

    def __init__(self, dim=1024, depth=8, dim_head=64, heads=16, num_queries=8, embedding_dim=768, output_dim=1024, ff_mult=4):
        super().__init__()
        self.latents = nn.Parameter(torch.randn(1, num_queries, dim) / dim ** 0.5)
        self.proj_in = nn.Linear(embedding_dim, dim)
        self.proj_out = nn.Linear(dim, output_dim)
        self.norm_out = nn.LayerNorm(output_dim)
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads), FeedForward(dim=dim, mult=ff_mult)]))

    def forward(self, x):
        latents = self.latents.repeat(x.size(0), 1, 1)
        x = self.proj_in(x)
        for attn, ff in self.layers:
            latents = attn(x, latents) + latents
            latents = ff(latents) + latents
        latents = self.proj_out(latents)
        return self.norm_out(latents)


class IPAttnProcessor(nn.Module):
    """
    Attention processor for IP-Adapater.
    Args:
        hidden_size (`int`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`):
            The number of channels in the `encoder_hidden_states`.
        scale (`float`, defaults to 1.0):
            the weight scale of image prompt.
        num_tokens (`int`, defaults to 4 when do ip_adapter_plus it should be 16):
            The context length of the image features.
    """

    def __init__(self, hidden_size, cross_attention_dim=None, scale=1.0, num_tokens=4):
        super().__init__()
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.scale = scale
        self.num_tokens = num_tokens
        self.to_k_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        self.to_v_ip = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)

    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        else:
            end_pos = encoder_hidden_states.shape[1] - self.num_tokens
            encoder_hidden_states, ip_hidden_states = encoder_hidden_states[:, :end_pos, :], encoder_hidden_states[:, end_pos:, :]
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        if xformers_available:
            hidden_states = self._memory_efficient_attention_xformers(query, key, value, attention_mask)
        else:
            attention_probs = attn.get_attention_scores(query, key, attention_mask)
            hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        ip_key = self.to_k_ip(ip_hidden_states)
        ip_value = self.to_v_ip(ip_hidden_states)
        ip_key = attn.head_to_batch_dim(ip_key)
        ip_value = attn.head_to_batch_dim(ip_value)
        if xformers_available:
            ip_hidden_states = self._memory_efficient_attention_xformers(query, ip_key, ip_value, None)
        else:
            ip_attention_probs = attn.get_attention_scores(query, ip_key, None)
            ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)
        ip_hidden_states = attn.batch_to_head_dim(ip_hidden_states)
        hidden_states = hidden_states + self.scale * ip_hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states

    def _memory_efficient_attention_xformers(self, query, key, value, attention_mask):
        query = query.contiguous()
        key = key.contiguous()
        value = value.contiguous()
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask)
        return hidden_states


def rearrange_3(tensor, f):
    F, D, C = tensor.size()
    return torch.reshape(tensor, (F // f, f, D, C))


def rearrange_4(tensor):
    B, F, D, C = tensor.size()
    return torch.reshape(tensor, (B * F, D, C))


class CrossFrameAttnProcessor:
    """
    Cross frame attention processor. Each frame attends the first frame.

    Args:
        batch_size: The number that represents actual batch size, other than the frames.
            For example, calling unet with a single prompt and num_images_per_prompt=1, batch_size should be equal to
            2, due to classifier-free guidance.
    """

    def __init__(self, batch_size=2):
        self.batch_size = batch_size

    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        query = attn.to_q(hidden_states)
        is_cross_attention = encoder_hidden_states is not None
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        if not is_cross_attention:
            video_length = key.size()[0] // self.batch_size
            first_frame_index = [0] * video_length
            key = rearrange_3(key, video_length)
            key = key[:, first_frame_index]
            value = rearrange_3(value, video_length)
            value = value[:, first_frame_index]
            key = rearrange_4(key)
            value = rearrange_4(value)
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        return hidden_states


class TorchVAEEncoder(torch.nn.Module):

    def __init__(self, model):
        super().__init__()
        self.vae_encoder = model

    def forward(self, x):
        return self.vae_encoder.encode(x).latent_dist.sample()


class ObservationEncoder(nn.Module):
    """
    Converts raw robot observations (positions/angles) into a more compact representation

    state_dim (int): Dimension of the input state vector (default: 5)
        [robot_x, robot_y, block_x, block_y, block_angle]

    - Input shape: (batch_size, state_dim)
    - Output shape: (batch_size, 256)
    """

    def __init__(self, state_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(state_dim, 512), nn.ReLU(), nn.Linear(512, 256))

    def forward(self, x):
        return self.net(x)


class ObservationProjection(nn.Module):
    """
    Takes the encoded observation and transforms it into 32 values that represent the current robot/block situation.
    These values are used as additional contextual information during the diffusion model's trajectory generation.

    - Input: 256-dim vector (padded to 512)
            Shape: (batch_size, 256)
    - Output: 32 contextual information values for the diffusion model
            Shape: (batch_size, 32)
    """

    def __init__(self):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(32, 512))
        self.bias = nn.Parameter(torch.zeros(32))

    def forward(self, x):
        if x.size(-1) == 256:
            x = torch.cat([x, torch.zeros(*x.shape[:-1], 256, device=x.device)], dim=-1)
        return nn.functional.linear(x, self.weight, self.bias)


class AdaLayerNorm(nn.Module):
    """
    Norm layer modified to incorporate timestep embeddings.

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`, *optional*): The size of the embeddings dictionary.
        output_dim (`int`, *optional*):
        norm_elementwise_affine (`bool`, defaults to `False):
        norm_eps (`bool`, defaults to `False`):
        chunk_dim (`int`, defaults to `0`):
    """

    def __init__(self, embedding_dim: 'int', num_embeddings: 'Optional[int]'=None, output_dim: 'Optional[int]'=None, norm_elementwise_affine: 'bool'=False, norm_eps: 'float'=1e-05, chunk_dim: 'int'=0):
        super().__init__()
        self.chunk_dim = chunk_dim
        output_dim = output_dim or embedding_dim * 2
        if num_embeddings is not None:
            self.emb = nn.Embedding(num_embeddings, embedding_dim)
        else:
            self.emb = None
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim // 2, norm_eps, norm_elementwise_affine)

    def forward(self, x: 'torch.Tensor', timestep: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        if self.emb is not None:
            temb = self.emb(timestep)
        temb = self.linear(self.silu(temb))
        if self.chunk_dim == 1:
            shift, scale = temb.chunk(2, dim=1)
            shift = shift[:, None, :]
            scale = scale[:, None, :]
        else:
            scale, shift = temb.chunk(2, dim=0)
        x = self.norm(x) * (1 + scale) + shift
        return x


class AdaLayerNormContinuous(nn.Module):

    def __init__(self, embedding_dim: 'int', conditioning_embedding_dim: 'int', elementwise_affine=True, eps=1e-05, bias=True, norm_type='layer_norm'):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(conditioning_embedding_dim, embedding_dim * 2, bias=bias)
        if norm_type == 'layer_norm':
            self.norm = LayerNorm(embedding_dim, eps, elementwise_affine, bias)
        elif norm_type == 'rms_norm':
            self.norm = RMSNorm(embedding_dim, eps, elementwise_affine)
        else:
            raise ValueError(f'unknown norm_type {norm_type}')

    def forward(self, x: 'torch.Tensor', conditioning_embedding: 'torch.Tensor') ->torch.Tensor:
        emb = self.linear(self.silu(conditioning_embedding))
        scale, shift = torch.chunk(emb, 2, dim=1)
        x = self.norm(x) * (1 + scale)[:, None, :] + shift[:, None, :]
        return x


class LabelEmbedding(nn.Module):
    """
    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.

    Args:
        num_classes (`int`): The number of classes.
        hidden_size (`int`): The size of the vector embeddings.
        dropout_prob (`float`): The probability of dropping a label.
    """

    def __init__(self, num_classes, hidden_size, dropout_prob):
        super().__init__()
        use_cfg_embedding = dropout_prob > 0
        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)
        self.num_classes = num_classes
        self.dropout_prob = dropout_prob

    def token_drop(self, labels, force_drop_ids=None):
        """
        Drops labels to enable classifier-free guidance.
        """
        if force_drop_ids is None:
            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob
        else:
            drop_ids = torch.tensor(force_drop_ids == 1)
        labels = torch.where(drop_ids, self.num_classes, labels)
        return labels

    def forward(self, labels: 'torch.LongTensor', force_drop_ids=None):
        use_dropout = self.dropout_prob > 0
        if self.training and use_dropout or force_drop_ids is not None:
            labels = self.token_drop(labels, force_drop_ids)
        embeddings = self.embedding_table(labels)
        return embeddings


class CombinedTimestepLabelEmbeddings(nn.Module):

    def __init__(self, num_classes, embedding_dim, class_dropout_prob=0.1):
        super().__init__()
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=1)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.class_embedder = LabelEmbedding(num_classes, embedding_dim, class_dropout_prob)

    def forward(self, timestep, class_labels, hidden_dtype=None):
        timesteps_proj = self.time_proj(timestep)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        class_labels = self.class_embedder(class_labels)
        conditioning = timesteps_emb + class_labels
        return conditioning


class AdaLayerNormZero(nn.Module):
    """
    Norm layer adaptive layer norm zero (adaLN-Zero).

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
    """

    def __init__(self, embedding_dim: 'int', num_embeddings: 'Optional[int]'=None, norm_type='layer_norm', bias=True):
        super().__init__()
        if num_embeddings is not None:
            self.emb = CombinedTimestepLabelEmbeddings(num_embeddings, embedding_dim)
        else:
            self.emb = None
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=bias)
        if norm_type == 'layer_norm':
            self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-06)
        elif norm_type == 'fp32_layer_norm':
            self.norm = FP32LayerNorm(embedding_dim, elementwise_affine=False, bias=False)
        else:
            raise ValueError(f"Unsupported `norm_type` ({norm_type}) provided. Supported ones are: 'layer_norm', 'fp32_layer_norm'.")

    def forward(self, x: 'torch.Tensor', timestep: 'Optional[torch.Tensor]'=None, class_labels: 'Optional[torch.LongTensor]'=None, hidden_dtype: 'Optional[torch.dtype]'=None, emb: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        if self.emb is not None:
            emb = self.emb(timestep, class_labels, hidden_dtype=hidden_dtype)
        emb = self.linear(self.silu(emb))
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = emb.chunk(6, dim=1)
        x = self.norm(x) * (1 + scale_msa[:, None]) + shift_msa[:, None]
        return x, gate_msa, shift_mlp, scale_mlp, gate_mlp


class SinusoidalPositionalEmbedding(nn.Module):
    """Apply positional information to a sequence of embeddings.

    Takes in a sequence of embeddings with shape (batch_size, seq_length, embed_dim) and adds positional embeddings to
    them

    Args:
        embed_dim: (int): Dimension of the positional embedding.
        max_seq_length: Maximum sequence length to apply positional embeddings

    """

    def __init__(self, embed_dim: 'int', max_seq_length: 'int'=32):
        super().__init__()
        position = torch.arange(max_seq_length).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))
        pe = torch.zeros(1, max_seq_length, embed_dim)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        _, seq_length, _ = x.shape
        x = x + self.pe[:, :seq_length]
        return x


class PixArtControlNetAdapterBlock(nn.Module):

    def __init__(self, block_index, num_attention_heads: 'int'=16, attention_head_dim: 'int'=72, dropout: 'float'=0.0, cross_attention_dim: 'Optional[int]'=1152, attention_bias: 'bool'=True, activation_fn: 'str'='gelu-approximate', num_embeds_ada_norm: 'Optional[int]'=1000, upcast_attention: 'bool'=False, norm_type: 'str'='ada_norm_single', norm_elementwise_affine: 'bool'=False, norm_eps: 'float'=1e-06, attention_type: 'Optional[str]'='default'):
        super().__init__()
        self.block_index = block_index
        self.inner_dim = num_attention_heads * attention_head_dim
        if self.block_index == 0:
            self.before_proj = nn.Linear(self.inner_dim, self.inner_dim)
            nn.init.zeros_(self.before_proj.weight)
            nn.init.zeros_(self.before_proj.bias)
        self.transformer_block = BasicTransformerBlock(self.inner_dim, num_attention_heads, attention_head_dim, dropout=dropout, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, num_embeds_ada_norm=num_embeds_ada_norm, attention_bias=attention_bias, upcast_attention=upcast_attention, norm_type=norm_type, norm_elementwise_affine=norm_elementwise_affine, norm_eps=norm_eps, attention_type=attention_type)
        self.after_proj = nn.Linear(self.inner_dim, self.inner_dim)
        nn.init.zeros_(self.after_proj.weight)
        nn.init.zeros_(self.after_proj.bias)

    def train(self, mode: 'bool'=True):
        self.transformer_block.train(mode)
        if self.block_index == 0:
            self.before_proj.train(mode)
        self.after_proj.train(mode)

    def forward(self, hidden_states: 'torch.Tensor', controlnet_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, timestep: 'Optional[torch.LongTensor]'=None, added_cond_kwargs: 'Dict[str, torch.Tensor]'=None, cross_attention_kwargs: 'Dict[str, Any]'=None, attention_mask: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None):
        if self.block_index == 0:
            controlnet_states = self.before_proj(controlnet_states)
            controlnet_states = hidden_states + controlnet_states
        controlnet_states_down = self.transformer_block(hidden_states=controlnet_states, encoder_hidden_states=encoder_hidden_states, timestep=timestep, added_cond_kwargs=added_cond_kwargs, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, class_labels=None)
        controlnet_states_left = self.after_proj(controlnet_states_down)
        return controlnet_states_left, controlnet_states_down


class TimestepEmbedding_(nn.Module):

    def __init__(self, n_time=1024, n_emb=320, n_out=1280) ->None:
        super().__init__()
        self.emb = nn.Embedding(n_time, n_emb)
        self.f_1 = nn.Linear(n_emb, n_out)
        self.f_2 = nn.Linear(n_out, n_out)

    def forward(self, x) ->torch.Tensor:
        x = self.emb(x)
        x = self.f_1(x)
        x = F.silu(x)
        return self.f_2(x)


class ImageEmbedding(nn.Module):

    def __init__(self, in_channels=7, out_channels=320) ->None:
        super().__init__()
        self.f = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x) ->torch.Tensor:
        return self.f(x)


class ImageUnembedding(nn.Module):

    def __init__(self, in_channels=320, out_channels=6) ->None:
        super().__init__()
        self.gn = nn.GroupNorm(32, in_channels)
        self.f = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x) ->torch.Tensor:
        return self.f(F.silu(self.gn(x)))


class ConvResblock(nn.Module):

    def __init__(self, in_features=320, out_features=320) ->None:
        super().__init__()
        self.f_t = nn.Linear(1280, out_features * 2)
        self.gn_1 = nn.GroupNorm(32, in_features)
        self.f_1 = nn.Conv2d(in_features, out_features, kernel_size=3, padding=1)
        self.gn_2 = nn.GroupNorm(32, out_features)
        self.f_2 = nn.Conv2d(out_features, out_features, kernel_size=3, padding=1)
        skip_conv = in_features != out_features
        self.f_s = nn.Conv2d(in_features, out_features, kernel_size=1, padding=0) if skip_conv else nn.Identity()

    def forward(self, x, t):
        x_skip = x
        t = self.f_t(F.silu(t))
        t = t.chunk(2, dim=1)
        t_1 = t[0].unsqueeze(dim=2).unsqueeze(dim=3) + 1
        t_2 = t[1].unsqueeze(dim=2).unsqueeze(dim=3)
        gn_1 = F.silu(self.gn_1(x))
        f_1 = self.f_1(gn_1)
        gn_2 = self.gn_2(f_1)
        return self.f_s(x_skip) + self.f_2(F.silu(gn_2 * t_1 + t_2))


class Downsample(nn.Module):

    def __init__(self, in_channels=320) ->None:
        super().__init__()
        self.f_t = nn.Linear(1280, in_channels * 2)
        self.gn_1 = nn.GroupNorm(32, in_channels)
        self.f_1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.gn_2 = nn.GroupNorm(32, in_channels)
        self.f_2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x, t) ->torch.Tensor:
        x_skip = x
        t = self.f_t(F.silu(t))
        t_1, t_2 = t.chunk(2, dim=1)
        t_1 = t_1.unsqueeze(2).unsqueeze(3) + 1
        t_2 = t_2.unsqueeze(2).unsqueeze(3)
        gn_1 = F.silu(self.gn_1(x))
        avg_pool2d = F.avg_pool2d(gn_1, kernel_size=(2, 2), stride=None)
        f_1 = self.f_1(avg_pool2d)
        gn_2 = self.gn_2(f_1)
        f_2 = self.f_2(F.silu(t_2 + t_1 * gn_2))
        return f_2 + F.avg_pool2d(x_skip, kernel_size=(2, 2), stride=None)


class Upsample(nn.Module):

    def __init__(self, in_channels=1024) ->None:
        super().__init__()
        self.f_t = nn.Linear(1280, in_channels * 2)
        self.gn_1 = nn.GroupNorm(32, in_channels)
        self.f_1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)
        self.gn_2 = nn.GroupNorm(32, in_channels)
        self.f_2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)

    def forward(self, x, t) ->torch.Tensor:
        x_skip = x
        t = self.f_t(F.silu(t))
        t_1, t_2 = t.chunk(2, dim=1)
        t_1 = t_1.unsqueeze(2).unsqueeze(3) + 1
        t_2 = t_2.unsqueeze(2).unsqueeze(3)
        gn_1 = F.silu(self.gn_1(x))
        upsample = F.upsample_nearest(gn_1, scale_factor=2)
        f_1 = self.f_1(upsample)
        gn_2 = self.gn_2(f_1)
        f_2 = self.f_2(F.silu(t_2 + t_1 * gn_2))
        return f_2 + F.upsample_nearest(x_skip, scale_factor=2)


class ConvUNetVAE(nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.embed_image = ImageEmbedding()
        self.embed_time = TimestepEmbedding_()
        down_0 = nn.ModuleList([ConvResblock(320, 320), ConvResblock(320, 320), ConvResblock(320, 320), Downsample(320)])
        down_1 = nn.ModuleList([ConvResblock(320, 640), ConvResblock(640, 640), ConvResblock(640, 640), Downsample(640)])
        down_2 = nn.ModuleList([ConvResblock(640, 1024), ConvResblock(1024, 1024), ConvResblock(1024, 1024), Downsample(1024)])
        down_3 = nn.ModuleList([ConvResblock(1024, 1024), ConvResblock(1024, 1024), ConvResblock(1024, 1024)])
        self.down = nn.ModuleList([down_0, down_1, down_2, down_3])
        self.mid = nn.ModuleList([ConvResblock(1024, 1024), ConvResblock(1024, 1024)])
        up_3 = nn.ModuleList([ConvResblock(1024 * 2, 1024), ConvResblock(1024 * 2, 1024), ConvResblock(1024 * 2, 1024), ConvResblock(1024 * 2, 1024), Upsample(1024)])
        up_2 = nn.ModuleList([ConvResblock(1024 * 2, 1024), ConvResblock(1024 * 2, 1024), ConvResblock(1024 * 2, 1024), ConvResblock(1024 + 640, 1024), Upsample(1024)])
        up_1 = nn.ModuleList([ConvResblock(1024 + 640, 640), ConvResblock(640 * 2, 640), ConvResblock(640 * 2, 640), ConvResblock(320 + 640, 640), Upsample(640)])
        up_0 = nn.ModuleList([ConvResblock(320 + 640, 320), ConvResblock(320 * 2, 320), ConvResblock(320 * 2, 320), ConvResblock(320 * 2, 320)])
        self.up = nn.ModuleList([up_0, up_1, up_2, up_3])
        self.output = ImageUnembedding()

    def forward(self, x, t, features) ->torch.Tensor:
        converted = hasattr(self, 'converted') and self.converted
        x = torch.cat([x, F.upsample_nearest(features, scale_factor=8)], dim=1)
        if converted:
            t = self.time_embedding(self.time_proj(t))
        else:
            t = self.embed_time(t)
        x = self.embed_image(x)
        skips = [x]
        for i, down in enumerate(self.down):
            if converted and i in [0, 1, 2, 3]:
                x, skips_ = down(x, t)
                for skip in skips_:
                    skips.append(skip)
            else:
                for block in down:
                    x = block(x, t)
                    skips.append(x)
            None
        if converted:
            x = self.mid(x, t)
        else:
            for i in range(2):
                x = self.mid[i](x, t)
        None
        for i, up in enumerate(self.up[::-1]):
            if converted and i in [0, 1, 2, 3]:
                skip_4 = skips.pop()
                skip_3 = skips.pop()
                skip_2 = skips.pop()
                skip_1 = skips.pop()
                skips_ = skip_1, skip_2, skip_3, skip_4
                x = up(x, skips_, t)
            else:
                for block in up:
                    if isinstance(block, ConvResblock):
                        x = torch.concat([x, skips.pop()], dim=1)
                    x = block(x, t)
        return self.output(x)


class DiffusionUncond(nn.Module):

    def __init__(self, global_args):
        super().__init__()
        self.diffusion = DiffusionAttnUnet1D(global_args, n_attn_layers=4)
        self.diffusion_ema = deepcopy(self.diffusion)
        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)


class UNet2DConditionControlNetModel(torch.nn.Module):

    def __init__(self, unet, controlnets: 'ControlNetModel'):
        super().__init__()
        self.unet = unet
        self.controlnets = controlnets

    def forward(self, sample, timestep, encoder_hidden_states, controlnet_conds, controlnet_scales):
        for i, (controlnet_cond, conditioning_scale, controlnet) in enumerate(zip(controlnet_conds, controlnet_scales, self.controlnets)):
            down_samples, mid_sample = controlnet(sample, timestep, encoder_hidden_states=encoder_hidden_states, controlnet_cond=controlnet_cond, conditioning_scale=conditioning_scale, return_dict=False)
            if i == 0:
                down_block_res_samples, mid_block_res_sample = down_samples, mid_sample
            else:
                down_block_res_samples = [(samples_prev + samples_curr) for samples_prev, samples_curr in zip(down_block_res_samples, down_samples)]
                mid_block_res_sample += mid_sample
        noise_pred = self.unet(sample, timestep, encoder_hidden_states=encoder_hidden_states, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, return_dict=False)[0]
        return noise_pred


class UNet2DConditionXLControlNetModel(torch.nn.Module):

    def __init__(self, unet, controlnets: 'ControlNetModel'):
        super().__init__()
        self.unet = unet
        self.controlnets = controlnets

    def forward(self, sample, timestep, encoder_hidden_states, controlnet_conds, controlnet_scales, text_embeds, time_ids):
        added_cond_kwargs = {'text_embeds': text_embeds, 'time_ids': time_ids}
        for i, (controlnet_cond, conditioning_scale, controlnet) in enumerate(zip(controlnet_conds, controlnet_scales, self.controlnets)):
            down_samples, mid_sample = controlnet(sample, timestep, encoder_hidden_states=encoder_hidden_states, controlnet_cond=controlnet_cond, conditioning_scale=conditioning_scale, added_cond_kwargs=added_cond_kwargs, return_dict=False)
            if i == 0:
                down_block_res_samples, mid_block_res_sample = down_samples, mid_sample
            else:
                down_block_res_samples = [(samples_prev + samples_curr) for samples_prev, samples_curr in zip(down_block_res_samples, down_samples)]
                mid_block_res_sample += mid_sample
        noise_pred = self.unet(sample, timestep, encoder_hidden_states=encoder_hidden_states, down_block_additional_residuals=down_block_res_samples, mid_block_additional_residual=mid_block_res_sample, added_cond_kwargs=added_cond_kwargs, return_dict=False)[0]
        return noise_pred


class AttnProcsLayers(torch.nn.Module):

    def __init__(self, state_dict: 'Dict[str, torch.Tensor]'):
        super().__init__()
        self.layers = torch.nn.ModuleList(state_dict.values())
        self.mapping = dict(enumerate(state_dict.keys()))
        self.rev_mapping = {v: k for k, v in enumerate(state_dict.keys())}
        self.split_keys = ['.processor', '.self_attn']

        def map_to(module, state_dict, *args, **kwargs):
            new_state_dict = {}
            for key, value in state_dict.items():
                num = int(key.split('.')[1])
                new_key = key.replace(f'layers.{num}', module.mapping[num])
                new_state_dict[new_key] = value
            return new_state_dict

        def remap_key(key, state_dict):
            for k in self.split_keys:
                if k in key:
                    return key.split(k)[0] + k
            raise ValueError(f'There seems to be a problem with the state_dict: {set(state_dict.keys())}. {key} has to have one of {self.split_keys}.')

        def map_from(module, state_dict, *args, **kwargs):
            all_keys = list(state_dict.keys())
            for key in all_keys:
                replace_key = remap_key(key, state_dict)
                new_key = key.replace(replace_key, f'layers.{module.rev_mapping[replace_key]}')
                state_dict[new_key] = state_dict[key]
                del state_dict[key]
        self._register_state_dict_hook(map_to)
        self._register_load_state_dict_pre_hook(map_from, with_module=True)


class FP32SiLU(nn.Module):
    """
    SiLU activation function with input upcasted to torch.float32.
    """

    def __init__(self):
        super().__init__()

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        return F.silu(inputs.float(), inplace=False)


class AdapterResnetBlock(nn.Module):
    """
    An `AdapterResnetBlock` is a helper model that implements a ResNet-like block.

    Args:
        channels (`int`):
            Number of channels of AdapterResnetBlock's input and output.
    """

    def __init__(self, channels: 'int'):
        super().__init__()
        self.block1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.act = nn.ReLU()
        self.block2 = nn.Conv2d(channels, channels, kernel_size=1)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        This method takes input tensor x and applies a convolutional layer, ReLU activation, and another convolutional
        layer on the input tensor. It returns addition with the input tensor.
        """
        h = self.act(self.block1(x))
        h = self.block2(h)
        return h + x


class AdapterBlock(nn.Module):
    """
    An AdapterBlock is a helper model that contains multiple ResNet-like blocks. It is used in the `FullAdapter` and
    `FullAdapterXL` models.

    Args:
        in_channels (`int`):
            Number of channels of AdapterBlock's input.
        out_channels (`int`):
            Number of channels of AdapterBlock's output.
        num_res_blocks (`int`):
            Number of ResNet blocks in the AdapterBlock.
        down (`bool`, *optional*, defaults to `False`):
            If `True`, perform downsampling on AdapterBlock's input.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', num_res_blocks: 'int', down: 'bool'=False):
        super().__init__()
        self.downsample = None
        if down:
            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True)
        self.in_conv = None
        if in_channels != out_channels:
            self.in_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.resnets = nn.Sequential(*[AdapterResnetBlock(out_channels) for _ in range(num_res_blocks)])

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        This method takes tensor x as input and performs operations downsampling and convolutional layers if the
        self.downsample and self.in_conv properties of AdapterBlock model are specified. Then it applies a series of
        residual blocks to the input tensor.
        """
        if self.downsample is not None:
            x = self.downsample(x)
        if self.in_conv is not None:
            x = self.in_conv(x)
        x = self.resnets(x)
        return x


class FullAdapter(nn.Module):
    """
    See [`T2IAdapter`] for more information.
    """

    def __init__(self, in_channels: 'int'=3, channels: 'List[int]'=[320, 640, 1280, 1280], num_res_blocks: 'int'=2, downscale_factor: 'int'=8):
        super().__init__()
        in_channels = in_channels * downscale_factor ** 2
        self.unshuffle = nn.PixelUnshuffle(downscale_factor)
        self.conv_in = nn.Conv2d(in_channels, channels[0], kernel_size=3, padding=1)
        self.body = nn.ModuleList([AdapterBlock(channels[0], channels[0], num_res_blocks), *[AdapterBlock(channels[i - 1], channels[i], num_res_blocks, down=True) for i in range(1, len(channels))]])
        self.total_downscale_factor = downscale_factor * 2 ** (len(channels) - 1)

    def forward(self, x: 'torch.Tensor') ->List[torch.Tensor]:
        """
        This method processes the input tensor `x` through the FullAdapter model and performs operations including
        pixel unshuffling, convolution, and a stack of AdapterBlocks. It returns a list of feature tensors, each
        capturing information at a different stage of processing within the FullAdapter model. The number of feature
        tensors in the list is determined by the number of downsample blocks specified during initialization.
        """
        x = self.unshuffle(x)
        x = self.conv_in(x)
        features = []
        for block in self.body:
            x = block(x)
            features.append(x)
        return features


class FullAdapterXL(nn.Module):
    """
    See [`T2IAdapter`] for more information.
    """

    def __init__(self, in_channels: 'int'=3, channels: 'List[int]'=[320, 640, 1280, 1280], num_res_blocks: 'int'=2, downscale_factor: 'int'=16):
        super().__init__()
        in_channels = in_channels * downscale_factor ** 2
        self.unshuffle = nn.PixelUnshuffle(downscale_factor)
        self.conv_in = nn.Conv2d(in_channels, channels[0], kernel_size=3, padding=1)
        self.body = []
        for i in range(len(channels)):
            if i == 1:
                self.body.append(AdapterBlock(channels[i - 1], channels[i], num_res_blocks))
            elif i == 2:
                self.body.append(AdapterBlock(channels[i - 1], channels[i], num_res_blocks, down=True))
            else:
                self.body.append(AdapterBlock(channels[i], channels[i], num_res_blocks))
        self.body = nn.ModuleList(self.body)
        self.total_downscale_factor = downscale_factor * 2

    def forward(self, x: 'torch.Tensor') ->List[torch.Tensor]:
        """
        This method takes the tensor x as input and processes it through FullAdapterXL model. It consists of operations
        including unshuffling pixels, applying convolution layer and appending each block into list of feature tensors.
        """
        x = self.unshuffle(x)
        x = self.conv_in(x)
        features = []
        for block in self.body:
            x = block(x)
            features.append(x)
        return features


class LightAdapterResnetBlock(nn.Module):
    """
    A `LightAdapterResnetBlock` is a helper model that implements a ResNet-like block with a slightly different
    architecture than `AdapterResnetBlock`.

    Args:
        channels (`int`):
            Number of channels of LightAdapterResnetBlock's input and output.
    """

    def __init__(self, channels: 'int'):
        super().__init__()
        self.block1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.act = nn.ReLU()
        self.block2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        This function takes input tensor x and processes it through one convolutional layer, ReLU activation, and
        another convolutional layer and adds it to input tensor.
        """
        h = self.act(self.block1(x))
        h = self.block2(h)
        return h + x


class LightAdapterBlock(nn.Module):
    """
    A `LightAdapterBlock` is a helper model that contains multiple `LightAdapterResnetBlocks`. It is used in the
    `LightAdapter` model.

    Args:
        in_channels (`int`):
            Number of channels of LightAdapterBlock's input.
        out_channels (`int`):
            Number of channels of LightAdapterBlock's output.
        num_res_blocks (`int`):
            Number of LightAdapterResnetBlocks in the LightAdapterBlock.
        down (`bool`, *optional*, defaults to `False`):
            If `True`, perform downsampling on LightAdapterBlock's input.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', num_res_blocks: 'int', down: 'bool'=False):
        super().__init__()
        mid_channels = out_channels // 4
        self.downsample = None
        if down:
            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True)
        self.in_conv = nn.Conv2d(in_channels, mid_channels, kernel_size=1)
        self.resnets = nn.Sequential(*[LightAdapterResnetBlock(mid_channels) for _ in range(num_res_blocks)])
        self.out_conv = nn.Conv2d(mid_channels, out_channels, kernel_size=1)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        This method takes tensor x as input and performs downsampling if required. Then it applies in convolution
        layer, a sequence of residual blocks, and out convolutional layer.
        """
        if self.downsample is not None:
            x = self.downsample(x)
        x = self.in_conv(x)
        x = self.resnets(x)
        x = self.out_conv(x)
        return x


class LightAdapter(nn.Module):
    """
    See [`T2IAdapter`] for more information.
    """

    def __init__(self, in_channels: 'int'=3, channels: 'List[int]'=[320, 640, 1280], num_res_blocks: 'int'=4, downscale_factor: 'int'=8):
        super().__init__()
        in_channels = in_channels * downscale_factor ** 2
        self.unshuffle = nn.PixelUnshuffle(downscale_factor)
        self.body = nn.ModuleList([LightAdapterBlock(in_channels, channels[0], num_res_blocks), *[LightAdapterBlock(channels[i], channels[i + 1], num_res_blocks, down=True) for i in range(len(channels) - 1)], LightAdapterBlock(channels[-1], channels[-1], num_res_blocks, down=True)])
        self.total_downscale_factor = downscale_factor * 2 ** len(channels)

    def forward(self, x: 'torch.Tensor') ->List[torch.Tensor]:
        """
        This method takes the input tensor x and performs downscaling and appends it in list of feature tensors. Each
        feature tensor corresponds to a different level of processing within the LightAdapter.
        """
        x = self.unshuffle(x)
        features = []
        for block in self.body:
            x = block(x)
            features.append(x)
        return features


class JointAttnProcessor2_0:
    """Attention processor used typically in processing the SD3-like self-attention projections."""

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'torch.FloatTensor'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, *args, **kwargs) ->torch.FloatTensor:
        residual = hidden_states
        batch_size = hidden_states.shape[0]
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if encoder_hidden_states is not None:
            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)
            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            if attn.norm_added_q is not None:
                encoder_hidden_states_query_proj = attn.norm_added_q(encoder_hidden_states_query_proj)
            if attn.norm_added_k is not None:
                encoder_hidden_states_key_proj = attn.norm_added_k(encoder_hidden_states_key_proj)
            query = torch.cat([query, encoder_hidden_states_query_proj], dim=2)
            key = torch.cat([key, encoder_hidden_states_key_proj], dim=2)
            value = torch.cat([value, encoder_hidden_states_value_proj], dim=2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        if encoder_hidden_states is not None:
            hidden_states, encoder_hidden_states = hidden_states[:, :residual.shape[1]], hidden_states[:, residual.shape[1]:]
            if not attn.context_pre_only:
                encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if encoder_hidden_states is not None:
            return hidden_states, encoder_hidden_states
        else:
            return hidden_states


class SD35AdaLayerNormZeroX(nn.Module):
    """
    Norm layer adaptive layer norm zero (AdaLN-Zero).

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
    """

    def __init__(self, embedding_dim: 'int', norm_type: 'str'='layer_norm', bias: 'bool'=True) ->None:
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, 9 * embedding_dim, bias=bias)
        if norm_type == 'layer_norm':
            self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-06)
        else:
            raise ValueError(f"Unsupported `norm_type` ({norm_type}) provided. Supported ones are: 'layer_norm'.")

    def forward(self, hidden_states: 'torch.Tensor', emb: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, ...]:
        emb = self.linear(self.silu(emb))
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp, shift_msa2, scale_msa2, gate_msa2 = emb.chunk(9, dim=1)
        norm_hidden_states = self.norm(hidden_states)
        hidden_states = norm_hidden_states * (1 + scale_msa[:, None]) + shift_msa[:, None]
        norm_hidden_states2 = norm_hidden_states * (1 + scale_msa2[:, None]) + shift_msa2[:, None]
        return hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp, norm_hidden_states2, gate_msa2


class LuminaFeedForward(nn.Module):
    """
    A feed-forward layer.

    Parameters:
        hidden_size (`int`):
            The dimensionality of the hidden layers in the model. This parameter determines the width of the model's
            hidden representations.
        intermediate_size (`int`): The intermediate dimension of the feedforward layer.
        multiple_of (`int`, *optional*): Value to ensure hidden dimension is a multiple
            of this value.
        ffn_dim_multiplier (float, *optional*): Custom multiplier for hidden
            dimension. Defaults to None.
    """

    def __init__(self, dim: 'int', inner_dim: 'int', multiple_of: 'Optional[int]'=256, ffn_dim_multiplier: 'Optional[float]'=None):
        super().__init__()
        inner_dim = int(2 * inner_dim / 3)
        if ffn_dim_multiplier is not None:
            inner_dim = int(ffn_dim_multiplier * inner_dim)
        inner_dim = multiple_of * ((inner_dim + multiple_of - 1) // multiple_of)
        self.linear_1 = nn.Linear(dim, inner_dim, bias=False)
        self.linear_2 = nn.Linear(inner_dim, dim, bias=False)
        self.linear_3 = nn.Linear(dim, inner_dim, bias=False)
        self.silu = FP32SiLU()

    def forward(self, x):
        return self.linear_2(self.silu(self.linear_1(x)) * self.linear_3(x))


class SkipFFTransformerBlock(nn.Module):

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', kv_input_dim: 'int', kv_input_dim_proj_use_bias: 'bool', dropout=0.0, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, attention_out_bias: 'bool'=True):
        super().__init__()
        if kv_input_dim != dim:
            self.kv_mapper = nn.Linear(kv_input_dim, dim, kv_input_dim_proj_use_bias)
        else:
            self.kv_mapper = None
        self.norm1 = RMSNorm(dim, 1e-06)
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=cross_attention_dim, out_bias=attention_out_bias)
        self.norm2 = RMSNorm(dim, 1e-06)
        self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, out_bias=attention_out_bias)

    def forward(self, hidden_states, encoder_hidden_states, cross_attention_kwargs):
        cross_attention_kwargs = cross_attention_kwargs.copy() if cross_attention_kwargs is not None else {}
        if self.kv_mapper is not None:
            encoder_hidden_states = self.kv_mapper(F.silu(encoder_hidden_states))
        norm_hidden_states = self.norm1(hidden_states)
        attn_output = self.attn1(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, **cross_attention_kwargs)
        hidden_states = attn_output + hidden_states
        norm_hidden_states = self.norm2(hidden_states)
        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, **cross_attention_kwargs)
        hidden_states = attn_output + hidden_states
        return hidden_states


def _query_chunk_attention(query, key, value, precision, key_chunk_size: 'int'=4096):
    """Multi-head dot product attention with a limited number of queries."""
    num_kv, num_heads, k_features = key.shape[-3:]
    v_features = value.shape[-1]
    key_chunk_size = min(key_chunk_size, num_kv)
    query = query / jnp.sqrt(k_features)

    @functools.partial(jax.checkpoint, prevent_cse=False)
    def summarize_chunk(query, key, value):
        attn_weights = jnp.einsum('...qhd,...khd->...qhk', query, key, precision=precision)
        max_score = jnp.max(attn_weights, axis=-1, keepdims=True)
        max_score = jax.lax.stop_gradient(max_score)
        exp_weights = jnp.exp(attn_weights - max_score)
        exp_values = jnp.einsum('...vhf,...qhv->...qhf', value, exp_weights, precision=precision)
        max_score = jnp.einsum('...qhk->...qh', max_score)
        return exp_values, exp_weights.sum(axis=-1), max_score

    def chunk_scanner(chunk_idx):
        key_chunk = jax.lax.dynamic_slice(operand=key, start_indices=[0] * (key.ndim - 3) + [chunk_idx, 0, 0], slice_sizes=list(key.shape[:-3]) + [key_chunk_size, num_heads, k_features])
        value_chunk = jax.lax.dynamic_slice(operand=value, start_indices=[0] * (value.ndim - 3) + [chunk_idx, 0, 0], slice_sizes=list(value.shape[:-3]) + [key_chunk_size, num_heads, v_features])
        return summarize_chunk(query, key_chunk, value_chunk)
    chunk_values, chunk_weights, chunk_max = jax.lax.map(f=chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size))
    global_max = jnp.max(chunk_max, axis=0, keepdims=True)
    max_diffs = jnp.exp(chunk_max - global_max)
    chunk_values *= jnp.expand_dims(max_diffs, axis=-1)
    chunk_weights *= max_diffs
    all_values = chunk_values.sum(axis=0)
    all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)
    return all_values / all_weights


CONFIG_NAME = 'config.json'


class FrozenDict(OrderedDict):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for key, value in self.items():
            setattr(self, key, value)
        self.__frozen = True

    def __delitem__(self, *args, **kwargs):
        raise Exception(f'You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.')

    def setdefault(self, *args, **kwargs):
        raise Exception(f'You cannot use ``setdefault`` on a {self.__class__.__name__} instance.')

    def pop(self, *args, **kwargs):
        raise Exception(f'You cannot use ``pop`` on a {self.__class__.__name__} instance.')

    def update(self, *args, **kwargs):
        raise Exception(f'You cannot use ``update`` on a {self.__class__.__name__} instance.')

    def __setattr__(self, name, value):
        if hasattr(self, '__frozen') and self.__frozen:
            raise Exception(f'You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.')
        super().__setattr__(name, value)

    def __setitem__(self, name, value):
        if hasattr(self, '__frozen') and self.__frozen:
            raise Exception(f'You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.')
        super().__setitem__(name, value)


def extract_commit_hash(resolved_file: 'Optional[str]', commit_hash: 'Optional[str]'=None):
    """
    Extracts the commit hash from a resolved filename toward a cache file.
    """
    if resolved_file is None or commit_hash is not None:
        return commit_hash
    resolved_file = str(Path(resolved_file).as_posix())
    search = re.search('snapshots/([^/]+)/', resolved_file)
    if search is None:
        return None
    commit_hash = search.groups()[0]
    return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None


ENV_VARS_TRUE_VALUES = {'1', 'ON', 'YES', 'TRUE'}


SESSION_ID = uuid4().hex


_flax_version = 'N/A'


_jax_version = 'N/A'


_onnxruntime_version = 'N/A'


def http_user_agent(user_agent: 'Union[Dict, str, None]'=None) ->str:
    """
    Formats a user-agent string with basic info about a request.
    """
    ua = f'diffusers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}'
    if HF_HUB_DISABLE_TELEMETRY or HF_HUB_OFFLINE:
        return ua + '; telemetry/off'
    if is_torch_available():
        ua += f'; torch/{_torch_version}'
    if is_flax_available():
        ua += f'; jax/{_jax_version}'
        ua += f'; flax/{_flax_version}'
    if is_onnx_available():
        ua += f'; onnxruntime/{_onnxruntime_version}'
    if os.environ.get('DIFFUSERS_IS_CI', '').upper() in ENV_VARS_TRUE_VALUES:
        ua += '; is_ci/true'
    if isinstance(user_agent, dict):
        ua += '; ' + '; '.join(f'{k}/{v}' for k, v in user_agent.items())
    elif isinstance(user_agent, str):
        ua += '; ' + user_agent
    return ua


def is_valid_image(image) ->bool:
    """
    Checks if the input is a valid image.

    A valid image can be:
    - A `PIL.Image.Image`.
    - A 2D or 3D `np.ndarray` or `torch.Tensor` (grayscale or color image).

    Args:
        image (`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`):
            The image to validate. It can be a PIL image, a NumPy array, or a torch tensor.

    Returns:
        `bool`:
            `True` if the input is a valid image, `False` otherwise.
    """
    return isinstance(image, PIL.Image.Image) or isinstance(image, (np.ndarray, torch.Tensor)) and image.ndim in (2, 3)


def is_valid_image_imagelist(images):
    """
    Checks if the input is a valid image or list of images.

    The input can be one of the following formats:
    - A 4D tensor or numpy array (batch of images).
    - A valid single image: `PIL.Image.Image`, 2D `np.ndarray` or `torch.Tensor` (grayscale image), 3D `np.ndarray` or
      `torch.Tensor`.
    - A list of valid images.

    Args:
        images (`Union[np.ndarray, torch.Tensor, PIL.Image.Image, List]`):
            The image(s) to check. Can be a batch of images (4D tensor/array), a single image, or a list of valid
            images.

    Returns:
        `bool`:
            `True` if the input is valid, `False` otherwise.
    """
    if isinstance(images, (np.ndarray, torch.Tensor)) and images.ndim == 4:
        return True
    elif is_valid_image(images):
        return True
    elif isinstance(images, list):
        return all(is_valid_image(image) for image in images)
    return False


def register_to_config(init):
    """
    Decorator to apply on the init of classes inheriting from [`ConfigMixin`] so that all the arguments are
    automatically sent to `self.register_for_config`. To ignore a specific argument accepted by the init but that
    shouldn't be registered in the config, use the `ignore_for_config` class variable

    Warning: Once decorated, all private arguments (beginning with an underscore) are trashed and not sent to the init!
    """

    @functools.wraps(init)
    def inner_init(self, *args, **kwargs):
        init_kwargs = {k: v for k, v in kwargs.items() if not k.startswith('_')}
        config_init_kwargs = {k: v for k, v in kwargs.items() if k.startswith('_')}
        if not isinstance(self, ConfigMixin):
            raise RuntimeError(f'`@register_for_config` was applied to {self.__class__.__name__} init method, but this class does not inherit from `ConfigMixin`.')
        ignore = getattr(self, 'ignore_for_config', [])
        new_kwargs = {}
        signature = inspect.signature(init)
        parameters = {name: p.default for i, (name, p) in enumerate(signature.parameters.items()) if i > 0 and name not in ignore}
        for arg, name in zip(args, parameters.keys()):
            new_kwargs[name] = arg
        new_kwargs.update({k: init_kwargs.get(k, default) for k, default in parameters.items() if k not in ignore and k not in new_kwargs})
        if len(set(new_kwargs.keys()) - set(init_kwargs)) > 0:
            new_kwargs['_use_default_values'] = list(set(new_kwargs.keys()) - set(init_kwargs))
        new_kwargs = {**config_init_kwargs, **new_kwargs}
        getattr(self, 'register_to_config')(**new_kwargs)
        init(self, *args, **init_kwargs)
    return inner_init


class IPAdapterAttnProcessor(nn.Module):
    """
    Attention processor for Multiple IP-Adapters.

    Args:
        hidden_size (`int`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`):
            The number of channels in the `encoder_hidden_states`.
        num_tokens (`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`):
            The context length of the image features.
        scale (`float` or List[`float`], defaults to 1.0):
            the weight scale of image prompt.
    """

    def __init__(self, hidden_size, cross_attention_dim=None, num_tokens=(4,), scale=1.0):
        super().__init__()
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        if not isinstance(num_tokens, (tuple, list)):
            num_tokens = [num_tokens]
        self.num_tokens = num_tokens
        if not isinstance(scale, list):
            scale = [scale] * len(num_tokens)
        if len(scale) != len(num_tokens):
            raise ValueError('`scale` should be a list of integers with the same length as `num_tokens`.')
        self.scale = scale
        self.to_k_ip = nn.ModuleList([nn.Linear(cross_attention_dim, hidden_size, bias=False) for _ in range(len(num_tokens))])
        self.to_v_ip = nn.ModuleList([nn.Linear(cross_attention_dim, hidden_size, bias=False) for _ in range(len(num_tokens))])

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, scale: 'float'=1.0, ip_adapter_masks: 'Optional[torch.Tensor]'=None):
        residual = hidden_states
        if encoder_hidden_states is not None:
            if isinstance(encoder_hidden_states, tuple):
                encoder_hidden_states, ip_hidden_states = encoder_hidden_states
            else:
                deprecation_message = 'You have passed a tensor as `encoder_hidden_states`. This is deprecated and will be removed in a future release. Please make sure to update your script to pass `encoder_hidden_states` as a tuple to suppress this warning.'
                deprecate('encoder_hidden_states not a tuple', '1.0.0', deprecation_message, standard_warn=False)
                end_pos = encoder_hidden_states.shape[1] - self.num_tokens[0]
                encoder_hidden_states, ip_hidden_states = encoder_hidden_states[:, :end_pos, :], [encoder_hidden_states[:, end_pos:, :]]
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        if ip_adapter_masks is not None:
            if not isinstance(ip_adapter_masks, List):
                ip_adapter_masks = list(ip_adapter_masks.unsqueeze(1))
            if not len(ip_adapter_masks) == len(self.scale) == len(ip_hidden_states):
                raise ValueError(f'Length of ip_adapter_masks array ({len(ip_adapter_masks)}) must match length of self.scale array ({len(self.scale)}) and number of ip_hidden_states ({len(ip_hidden_states)})')
            else:
                for index, (mask, scale, ip_state) in enumerate(zip(ip_adapter_masks, self.scale, ip_hidden_states)):
                    if not isinstance(mask, torch.Tensor) or mask.ndim != 4:
                        raise ValueError('Each element of the ip_adapter_masks array should be a tensor with shape [1, num_images_for_ip_adapter, height, width]. Please use `IPAdapterMaskProcessor` to preprocess your mask')
                    if mask.shape[1] != ip_state.shape[1]:
                        raise ValueError(f'Number of masks ({mask.shape[1]}) does not match number of ip images ({ip_state.shape[1]}) at index {index}')
                    if isinstance(scale, list) and not len(scale) == mask.shape[1]:
                        raise ValueError(f'Number of masks ({mask.shape[1]}) does not match number of scales ({len(scale)}) at index {index}')
        else:
            ip_adapter_masks = [None] * len(self.scale)
        for current_ip_hidden_states, scale, to_k_ip, to_v_ip, mask in zip(ip_hidden_states, self.scale, self.to_k_ip, self.to_v_ip, ip_adapter_masks):
            skip = False
            if isinstance(scale, list):
                if all(s == 0 for s in scale):
                    skip = True
            elif scale == 0:
                skip = True
            if not skip:
                if mask is not None:
                    if not isinstance(scale, list):
                        scale = [scale] * mask.shape[1]
                    current_num_images = mask.shape[1]
                    for i in range(current_num_images):
                        ip_key = to_k_ip(current_ip_hidden_states[:, i, :, :])
                        ip_value = to_v_ip(current_ip_hidden_states[:, i, :, :])
                        ip_key = attn.head_to_batch_dim(ip_key)
                        ip_value = attn.head_to_batch_dim(ip_value)
                        ip_attention_probs = attn.get_attention_scores(query, ip_key, None)
                        _current_ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)
                        _current_ip_hidden_states = attn.batch_to_head_dim(_current_ip_hidden_states)
                        mask_downsample = IPAdapterMaskProcessor.downsample(mask[:, i, :, :], batch_size, _current_ip_hidden_states.shape[1], _current_ip_hidden_states.shape[2])
                        mask_downsample = mask_downsample
                        hidden_states = hidden_states + scale[i] * (_current_ip_hidden_states * mask_downsample)
                else:
                    ip_key = to_k_ip(current_ip_hidden_states)
                    ip_value = to_v_ip(current_ip_hidden_states)
                    ip_key = attn.head_to_batch_dim(ip_key)
                    ip_value = attn.head_to_batch_dim(ip_value)
                    ip_attention_probs = attn.get_attention_scores(query, ip_key, None)
                    current_ip_hidden_states = torch.bmm(ip_attention_probs, ip_value)
                    current_ip_hidden_states = attn.batch_to_head_dim(current_ip_hidden_states)
                    hidden_states = hidden_states + scale * current_ip_hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class IPAdapterAttnProcessor2_0(torch.nn.Module):
    """
    Attention processor for IP-Adapter for PyTorch 2.0.

    Args:
        hidden_size (`int`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`):
            The number of channels in the `encoder_hidden_states`.
        num_tokens (`int`, `Tuple[int]` or `List[int]`, defaults to `(4,)`):
            The context length of the image features.
        scale (`float` or `List[float]`, defaults to 1.0):
            the weight scale of image prompt.
    """

    def __init__(self, hidden_size, cross_attention_dim=None, num_tokens=(4,), scale=1.0):
        super().__init__()
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError(f'{self.__class__.__name__} requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        if not isinstance(num_tokens, (tuple, list)):
            num_tokens = [num_tokens]
        self.num_tokens = num_tokens
        if not isinstance(scale, list):
            scale = [scale] * len(num_tokens)
        if len(scale) != len(num_tokens):
            raise ValueError('`scale` should be a list of integers with the same length as `num_tokens`.')
        self.scale = scale
        self.to_k_ip = nn.ModuleList([nn.Linear(cross_attention_dim, hidden_size, bias=False) for _ in range(len(num_tokens))])
        self.to_v_ip = nn.ModuleList([nn.Linear(cross_attention_dim, hidden_size, bias=False) for _ in range(len(num_tokens))])

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, scale: 'float'=1.0, ip_adapter_masks: 'Optional[torch.Tensor]'=None):
        residual = hidden_states
        if encoder_hidden_states is not None:
            if isinstance(encoder_hidden_states, tuple):
                encoder_hidden_states, ip_hidden_states = encoder_hidden_states
            else:
                deprecation_message = 'You have passed a tensor as `encoder_hidden_states`. This is deprecated and will be removed in a future release. Please make sure to update your script to pass `encoder_hidden_states` as a tuple to suppress this warning.'
                deprecate('encoder_hidden_states not a tuple', '1.0.0', deprecation_message, standard_warn=False)
                end_pos = encoder_hidden_states.shape[1] - self.num_tokens[0]
                encoder_hidden_states, ip_hidden_states = encoder_hidden_states[:, :end_pos, :], [encoder_hidden_states[:, end_pos:, :]]
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        if ip_adapter_masks is not None:
            if not isinstance(ip_adapter_masks, List):
                ip_adapter_masks = list(ip_adapter_masks.unsqueeze(1))
            if not len(ip_adapter_masks) == len(self.scale) == len(ip_hidden_states):
                raise ValueError(f'Length of ip_adapter_masks array ({len(ip_adapter_masks)}) must match length of self.scale array ({len(self.scale)}) and number of ip_hidden_states ({len(ip_hidden_states)})')
            else:
                for index, (mask, scale, ip_state) in enumerate(zip(ip_adapter_masks, self.scale, ip_hidden_states)):
                    if not isinstance(mask, torch.Tensor) or mask.ndim != 4:
                        raise ValueError('Each element of the ip_adapter_masks array should be a tensor with shape [1, num_images_for_ip_adapter, height, width]. Please use `IPAdapterMaskProcessor` to preprocess your mask')
                    if mask.shape[1] != ip_state.shape[1]:
                        raise ValueError(f'Number of masks ({mask.shape[1]}) does not match number of ip images ({ip_state.shape[1]}) at index {index}')
                    if isinstance(scale, list) and not len(scale) == mask.shape[1]:
                        raise ValueError(f'Number of masks ({mask.shape[1]}) does not match number of scales ({len(scale)}) at index {index}')
        else:
            ip_adapter_masks = [None] * len(self.scale)
        for current_ip_hidden_states, scale, to_k_ip, to_v_ip, mask in zip(ip_hidden_states, self.scale, self.to_k_ip, self.to_v_ip, ip_adapter_masks):
            skip = False
            if isinstance(scale, list):
                if all(s == 0 for s in scale):
                    skip = True
            elif scale == 0:
                skip = True
            if not skip:
                if mask is not None:
                    if not isinstance(scale, list):
                        scale = [scale] * mask.shape[1]
                    current_num_images = mask.shape[1]
                    for i in range(current_num_images):
                        ip_key = to_k_ip(current_ip_hidden_states[:, i, :, :])
                        ip_value = to_v_ip(current_ip_hidden_states[:, i, :, :])
                        ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                        ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                        _current_ip_hidden_states = F.scaled_dot_product_attention(query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False)
                        _current_ip_hidden_states = _current_ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
                        _current_ip_hidden_states = _current_ip_hidden_states
                        mask_downsample = IPAdapterMaskProcessor.downsample(mask[:, i, :, :], batch_size, _current_ip_hidden_states.shape[1], _current_ip_hidden_states.shape[2])
                        mask_downsample = mask_downsample
                        hidden_states = hidden_states + scale[i] * (_current_ip_hidden_states * mask_downsample)
                else:
                    ip_key = to_k_ip(current_ip_hidden_states)
                    ip_value = to_v_ip(current_ip_hidden_states)
                    ip_key = ip_key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                    ip_value = ip_value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                    current_ip_hidden_states = F.scaled_dot_product_attention(query, ip_key, ip_value, attn_mask=None, dropout_p=0.0, is_causal=False)
                    current_ip_hidden_states = current_ip_hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
                    current_ip_hidden_states = current_ip_hidden_states
                    hidden_states = hidden_states + scale * current_ip_hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class AllegroTemporalConvLayer(nn.Module):
    """
    Temporal convolutional layer that can be used for video (sequence of images) input. Code adapted from:
    https://github.com/modelscope/modelscope/blob/1509fdb973e5871f37148a4b5e5964cafd43e64d/modelscope/models/multi_modal/video_synthesis/unet_sd.py#L1016
    """

    def __init__(self, in_dim: 'int', out_dim: 'Optional[int]'=None, dropout: 'float'=0.0, norm_num_groups: 'int'=32, up_sample: 'bool'=False, down_sample: 'bool'=False, stride: 'int'=1) ->None:
        super().__init__()
        out_dim = out_dim or in_dim
        pad_h = pad_w = int((stride - 1) * 0.5)
        pad_t = 0
        self.down_sample = down_sample
        self.up_sample = up_sample
        if down_sample:
            self.conv1 = nn.Sequential(nn.GroupNorm(norm_num_groups, in_dim), nn.SiLU(), nn.Conv3d(in_dim, out_dim, (2, stride, stride), stride=(2, 1, 1), padding=(0, pad_h, pad_w)))
        elif up_sample:
            self.conv1 = nn.Sequential(nn.GroupNorm(norm_num_groups, in_dim), nn.SiLU(), nn.Conv3d(in_dim, out_dim * 2, (1, stride, stride), padding=(0, pad_h, pad_w)))
        else:
            self.conv1 = nn.Sequential(nn.GroupNorm(norm_num_groups, in_dim), nn.SiLU(), nn.Conv3d(in_dim, out_dim, (3, stride, stride), padding=(pad_t, pad_h, pad_w)))
        self.conv2 = nn.Sequential(nn.GroupNorm(norm_num_groups, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, stride, stride), padding=(pad_t, pad_h, pad_w)))
        self.conv3 = nn.Sequential(nn.GroupNorm(norm_num_groups, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, stride, stride), padding=(pad_t, pad_h, pad_h)))
        self.conv4 = nn.Sequential(nn.GroupNorm(norm_num_groups, out_dim), nn.SiLU(), nn.Conv3d(out_dim, in_dim, (3, stride, stride), padding=(pad_t, pad_h, pad_h)))

    @staticmethod
    def _pad_temporal_dim(hidden_states: 'torch.Tensor') ->torch.Tensor:
        hidden_states = torch.cat((hidden_states[:, :, 0:1], hidden_states), dim=2)
        hidden_states = torch.cat((hidden_states, hidden_states[:, :, -1:]), dim=2)
        return hidden_states

    def forward(self, hidden_states: 'torch.Tensor', batch_size: 'int') ->torch.Tensor:
        hidden_states = hidden_states.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        if self.down_sample:
            identity = hidden_states[:, :, ::2]
        elif self.up_sample:
            identity = hidden_states.repeat_interleave(2, dim=2)
        else:
            identity = hidden_states
        if self.down_sample or self.up_sample:
            hidden_states = self.conv1(hidden_states)
        else:
            hidden_states = self._pad_temporal_dim(hidden_states)
            hidden_states = self.conv1(hidden_states)
        if self.up_sample:
            hidden_states = hidden_states.unflatten(1, (2, -1)).permute(0, 2, 3, 1, 4, 5).flatten(2, 3)
        hidden_states = self._pad_temporal_dim(hidden_states)
        hidden_states = self.conv2(hidden_states)
        hidden_states = self._pad_temporal_dim(hidden_states)
        hidden_states = self.conv3(hidden_states)
        hidden_states = self._pad_temporal_dim(hidden_states)
        hidden_states = self.conv4(hidden_states)
        hidden_states = identity + hidden_states
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).flatten(0, 1)
        return hidden_states


class AllegroDownBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, spatial_downsample: 'bool'=True, temporal_downsample: 'bool'=False, downsample_padding: 'int'=1):
        super().__init__()
        resnets = []
        temp_convs = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(AllegroTemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        if temporal_downsample:
            self.temp_convs_down = AllegroTemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups, down_sample=True, stride=3)
        self.add_temp_downsample = temporal_downsample
        if spatial_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        batch_size = hidden_states.shape[0]
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).flatten(0, 1)
        for resnet, temp_conv in zip(self.resnets, self.temp_convs):
            hidden_states = resnet(hidden_states, temb=None)
            hidden_states = temp_conv(hidden_states, batch_size=batch_size)
        if self.add_temp_downsample:
            hidden_states = self.temp_convs_down(hidden_states, batch_size=batch_size)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        hidden_states = hidden_states.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        return hidden_states


class AllegroUpBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, spatial_upsample: 'bool'=True, temporal_upsample: 'bool'=False, temb_channels: 'Optional[int]'=None):
        super().__init__()
        resnets = []
        temp_convs = []
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(AllegroTemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.add_temp_upsample = temporal_upsample
        if temporal_upsample:
            self.temp_conv_up = AllegroTemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups, up_sample=True, stride=3)
        if spatial_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        batch_size = hidden_states.shape[0]
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).flatten(0, 1)
        for resnet, temp_conv in zip(self.resnets, self.temp_convs):
            hidden_states = resnet(hidden_states, temb=None)
            hidden_states = temp_conv(hidden_states, batch_size=batch_size)
        if self.add_temp_upsample:
            hidden_states = self.temp_conv_up(hidden_states, batch_size=batch_size)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        hidden_states = hidden_states.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        return hidden_states


class AllegroMidBlock3DConv(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, add_attention: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0):
        super().__init__()
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        temp_convs = [AllegroTemporalConvLayer(in_channels, in_channels, dropout=0.1, norm_num_groups=resnet_groups)]
        attentions = []
        if attention_head_dim is None:
            attention_head_dim = in_channels
        for _ in range(num_layers):
            if add_attention:
                attentions.append(Attention(in_channels, heads=in_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups if resnet_time_scale_shift == 'default' else None, spatial_norm_dim=temb_channels if resnet_time_scale_shift == 'spatial' else None, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
            else:
                attentions.append(None)
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(AllegroTemporalConvLayer(in_channels, in_channels, dropout=0.1, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        batch_size = hidden_states.shape[0]
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).flatten(0, 1)
        hidden_states = self.resnets[0](hidden_states, temb=None)
        hidden_states = self.temp_convs[0](hidden_states, batch_size=batch_size)
        for attn, resnet, temp_conv in zip(self.attentions, self.resnets[1:], self.temp_convs[1:]):
            hidden_states = attn(hidden_states)
            hidden_states = resnet(hidden_states, temb=None)
            hidden_states = temp_conv(hidden_states, batch_size=batch_size)
        hidden_states = hidden_states.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        return hidden_states


class AllegroEncoder3D(nn.Module):

    def __init__(self, in_channels: 'int'=3, out_channels: 'int'=3, down_block_types: 'Tuple[str, ...]'=('AllegroDownBlock3D', 'AllegroDownBlock3D', 'AllegroDownBlock3D', 'AllegroDownBlock3D'), block_out_channels: 'Tuple[int, ...]'=(128, 256, 512, 512), temporal_downsample_blocks: 'Tuple[bool, ...]'=[True, True, False, False], layers_per_block: 'int'=2, norm_num_groups: 'int'=32, act_fn: 'str'='silu', double_z: 'bool'=True):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)
        self.temp_conv_in = nn.Conv3d(in_channels=block_out_channels[0], out_channels=block_out_channels[0], kernel_size=(3, 1, 1), padding=(1, 0, 0))
        self.down_blocks = nn.ModuleList([])
        output_channel = block_out_channels[0]
        for i, down_block_type in enumerate(down_block_types):
            input_channel = output_channel
            output_channel = block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            if down_block_type == 'AllegroDownBlock3D':
                down_block = AllegroDownBlock3D(num_layers=layers_per_block, in_channels=input_channel, out_channels=output_channel, spatial_downsample=not is_final_block, temporal_downsample=temporal_downsample_blocks[i], resnet_eps=1e-06, downsample_padding=0, resnet_act_fn=act_fn, resnet_groups=norm_num_groups)
            else:
                raise ValueError('Invalid `down_block_type` encountered. Must be `AllegroDownBlock3D`')
            self.down_blocks.append(down_block)
        self.mid_block = AllegroMidBlock3DConv(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default', attention_head_dim=block_out_channels[-1], resnet_groups=norm_num_groups, temb_channels=None)
        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        conv_out_channels = 2 * out_channels if double_z else out_channels
        self.temp_conv_out = nn.Conv3d(block_out_channels[-1], block_out_channels[-1], (3, 1, 1), padding=(1, 0, 0))
        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor') ->torch.Tensor:
        batch_size = sample.shape[0]
        sample = sample.permute(0, 2, 1, 3, 4).flatten(0, 1)
        sample = self.conv_in(sample)
        sample = sample.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        residual = sample
        sample = self.temp_conv_in(sample)
        sample = sample + residual
        if self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            for down_block in self.down_blocks:
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), sample)
            sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample)
        else:
            for down_block in self.down_blocks:
                sample = down_block(sample)
            sample = self.mid_block(sample)
        sample = sample.permute(0, 2, 1, 3, 4).flatten(0, 1)
        sample = self.conv_norm_out(sample)
        sample = self.conv_act(sample)
        sample = sample.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        residual = sample
        sample = self.temp_conv_out(sample)
        sample = sample + residual
        sample = sample.permute(0, 2, 1, 3, 4).flatten(0, 1)
        sample = self.conv_out(sample)
        sample = sample.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        return sample


class AllegroDecoder3D(nn.Module):

    def __init__(self, in_channels: 'int'=4, out_channels: 'int'=3, up_block_types: 'Tuple[str, ...]'=('AllegroUpBlock3D', 'AllegroUpBlock3D', 'AllegroUpBlock3D', 'AllegroUpBlock3D'), temporal_upsample_blocks: 'Tuple[bool, ...]'=[False, True, True, False], block_out_channels: 'Tuple[int, ...]'=(128, 256, 512, 512), layers_per_block: 'int'=2, norm_num_groups: 'int'=32, act_fn: 'str'='silu', norm_type: 'str'='group'):
        super().__init__()
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)
        self.temp_conv_in = nn.Conv3d(block_out_channels[-1], block_out_channels[-1], (3, 1, 1), padding=(1, 0, 0))
        self.mid_block = None
        self.up_blocks = nn.ModuleList([])
        temb_channels = in_channels if norm_type == 'spatial' else None
        self.mid_block = AllegroMidBlock3DConv(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default' if norm_type == 'group' else norm_type, attention_head_dim=block_out_channels[-1], resnet_groups=norm_num_groups, temb_channels=temb_channels)
        reversed_block_out_channels = list(reversed(block_out_channels))
        output_channel = reversed_block_out_channels[0]
        for i, up_block_type in enumerate(up_block_types):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            if up_block_type == 'AllegroUpBlock3D':
                up_block = AllegroUpBlock3D(num_layers=layers_per_block + 1, in_channels=prev_output_channel, out_channels=output_channel, spatial_upsample=not is_final_block, temporal_upsample=temporal_upsample_blocks[i], resnet_eps=1e-06, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, temb_channels=temb_channels, resnet_time_scale_shift=norm_type)
            else:
                raise ValueError('Invalid `UP_block_type` encountered. Must be `AllegroUpBlock3D`')
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel
        if norm_type == 'spatial':
            self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)
        else:
            self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        self.temp_conv_out = nn.Conv3d(block_out_channels[0], block_out_channels[0], (3, 1, 1), padding=(1, 0, 0))
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor') ->torch.Tensor:
        batch_size = sample.shape[0]
        sample = sample.permute(0, 2, 1, 3, 4).flatten(0, 1)
        sample = self.conv_in(sample)
        sample = sample.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        residual = sample
        sample = self.temp_conv_in(sample)
        sample = sample + residual
        upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
        if self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample)
            for up_block in self.up_blocks:
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample)
        else:
            sample = self.mid_block(sample)
            sample = sample
            for up_block in self.up_blocks:
                sample = up_block(sample)
        sample = sample.permute(0, 2, 1, 3, 4).flatten(0, 1)
        sample = self.conv_norm_out(sample)
        sample = self.conv_act(sample)
        sample = sample.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        residual = sample
        sample = self.temp_conv_out(sample)
        sample = sample + residual
        sample = sample.permute(0, 2, 1, 3, 4).flatten(0, 1)
        sample = self.conv_out(sample)
        sample = sample.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        return sample


class CogVideoXSafeConv3d(nn.Conv3d):
    """
    A 3D convolution layer that splits the input tensor into smaller parts to avoid OOM in CogVideoX Model.
    """

    def forward(self, input: 'torch.Tensor') ->torch.Tensor:
        memory_count = input.shape[0] * input.shape[1] * input.shape[2] * input.shape[3] * input.shape[4] * 2 / 1024 ** 3
        if memory_count > 2:
            kernel_size = self.kernel_size[0]
            part_num = int(memory_count / 2) + 1
            input_chunks = torch.chunk(input, part_num, dim=2)
            if kernel_size > 1:
                input_chunks = [input_chunks[0]] + [torch.cat((input_chunks[i - 1][:, :, -kernel_size + 1:], input_chunks[i]), dim=2) for i in range(1, len(input_chunks))]
            output_chunks = []
            for input_chunk in input_chunks:
                output_chunks.append(super().forward(input_chunk))
            output = torch.cat(output_chunks, dim=2)
            return output
        else:
            return super().forward(input)


class CogVideoXCausalConv3d(nn.Module):
    """A 3D causal convolution layer that pads the input tensor to ensure causality in CogVideoX Model.

    Args:
        in_channels (`int`): Number of channels in the input tensor.
        out_channels (`int`): Number of output channels produced by the convolution.
        kernel_size (`int` or `Tuple[int, int, int]`): Kernel size of the convolutional kernel.
        stride (`int`, defaults to `1`): Stride of the convolution.
        dilation (`int`, defaults to `1`): Dilation rate of the convolution.
        pad_mode (`str`, defaults to `"constant"`): Padding mode.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'Union[int, Tuple[int, int, int]]', stride: 'int'=1, dilation: 'int'=1, pad_mode: 'str'='constant'):
        super().__init__()
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size,) * 3
        time_kernel_size, height_kernel_size, width_kernel_size = kernel_size
        time_pad = time_kernel_size - 1
        height_pad = (height_kernel_size - 1) // 2
        width_pad = (width_kernel_size - 1) // 2
        self.pad_mode = pad_mode
        self.height_pad = height_pad
        self.width_pad = width_pad
        self.time_pad = time_pad
        self.time_causal_padding = width_pad, width_pad, height_pad, height_pad, time_pad, 0
        self.temporal_dim = 2
        self.time_kernel_size = time_kernel_size
        stride = stride if isinstance(stride, tuple) else (stride, 1, 1)
        dilation = dilation, 1, 1
        self.conv = CogVideoXSafeConv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, dilation=dilation)

    def fake_context_parallel_forward(self, inputs: 'torch.Tensor', conv_cache: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        if self.pad_mode == 'replicate':
            inputs = F.pad(inputs, self.time_causal_padding, mode='replicate')
        else:
            kernel_size = self.time_kernel_size
            if kernel_size > 1:
                cached_inputs = [conv_cache] if conv_cache is not None else [inputs[:, :, :1]] * (kernel_size - 1)
                inputs = torch.cat(cached_inputs + [inputs], dim=2)
        return inputs

    def forward(self, inputs: 'torch.Tensor', conv_cache: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        inputs = self.fake_context_parallel_forward(inputs, conv_cache)
        if self.pad_mode == 'replicate':
            conv_cache = None
        else:
            padding_2d = self.width_pad, self.width_pad, self.height_pad, self.height_pad
            conv_cache = inputs[:, :, -self.time_kernel_size + 1:].clone()
            inputs = F.pad(inputs, padding_2d, mode='constant', value=0)
        output = self.conv(inputs)
        return output, conv_cache


class CogVideoXSpatialNorm3D(nn.Module):
    """
    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002. This implementation is specific
    to 3D-video like data.

    CogVideoXSafeConv3d is used instead of nn.Conv3d to avoid OOM in CogVideoX Model.

    Args:
        f_channels (`int`):
            The number of channels for input to group normalization layer, and output of the spatial norm layer.
        zq_channels (`int`):
            The number of channels for the quantized vector as described in the paper.
        groups (`int`):
            Number of groups to separate the channels into for group normalization.
    """

    def __init__(self, f_channels: 'int', zq_channels: 'int', groups: 'int'=32):
        super().__init__()
        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=groups, eps=1e-06, affine=True)
        self.conv_y = CogVideoXCausalConv3d(zq_channels, f_channels, kernel_size=1, stride=1)
        self.conv_b = CogVideoXCausalConv3d(zq_channels, f_channels, kernel_size=1, stride=1)

    def forward(self, f: 'torch.Tensor', zq: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        if f.shape[2] > 1 and f.shape[2] % 2 == 1:
            f_first, f_rest = f[:, :, :1], f[:, :, 1:]
            f_first_size, f_rest_size = f_first.shape[-3:], f_rest.shape[-3:]
            z_first, z_rest = zq[:, :, :1], zq[:, :, 1:]
            z_first = F.interpolate(z_first, size=f_first_size)
            z_rest = F.interpolate(z_rest, size=f_rest_size)
            zq = torch.cat([z_first, z_rest], dim=2)
        else:
            zq = F.interpolate(zq, size=f.shape[-3:])
        conv_y, new_conv_cache['conv_y'] = self.conv_y(zq, conv_cache=conv_cache.get('conv_y'))
        conv_b, new_conv_cache['conv_b'] = self.conv_b(zq, conv_cache=conv_cache.get('conv_b'))
        norm_f = self.norm_layer(f)
        new_f = norm_f * conv_y + conv_b
        return new_f, new_conv_cache


class CogVideoXResnetBlock3D(nn.Module):
    """
    A 3D ResNet block used in the CogVideoX model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        dropout (`float`, defaults to `0.0`):
            Dropout rate.
        temb_channels (`int`, defaults to `512`):
            Number of time embedding channels.
        groups (`int`, defaults to `32`):
            Number of groups to separate the channels into for group normalization.
        eps (`float`, defaults to `1e-6`):
            Epsilon value for normalization layers.
        non_linearity (`str`, defaults to `"swish"`):
            Activation function to use.
        conv_shortcut (bool, defaults to `False`):
            Whether or not to use a convolution shortcut.
        spatial_norm_dim (`int`, *optional*):
            The dimension to use for spatial norm if it is to be used instead of group norm.
        pad_mode (str, defaults to `"first"`):
            Padding mode.
    """

    def __init__(self, in_channels: 'int', out_channels: 'Optional[int]'=None, dropout: 'float'=0.0, temb_channels: 'int'=512, groups: 'int'=32, eps: 'float'=1e-06, non_linearity: 'str'='swish', conv_shortcut: 'bool'=False, spatial_norm_dim: 'Optional[int]'=None, pad_mode: 'str'='first'):
        super().__init__()
        out_channels = out_channels or in_channels
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.nonlinearity = get_activation(non_linearity)
        self.use_conv_shortcut = conv_shortcut
        self.spatial_norm_dim = spatial_norm_dim
        if spatial_norm_dim is None:
            self.norm1 = nn.GroupNorm(num_channels=in_channels, num_groups=groups, eps=eps)
            self.norm2 = nn.GroupNorm(num_channels=out_channels, num_groups=groups, eps=eps)
        else:
            self.norm1 = CogVideoXSpatialNorm3D(f_channels=in_channels, zq_channels=spatial_norm_dim, groups=groups)
            self.norm2 = CogVideoXSpatialNorm3D(f_channels=out_channels, zq_channels=spatial_norm_dim, groups=groups)
        self.conv1 = CogVideoXCausalConv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, pad_mode=pad_mode)
        if temb_channels > 0:
            self.temb_proj = nn.Linear(in_features=temb_channels, out_features=out_channels)
        self.dropout = nn.Dropout(dropout)
        self.conv2 = CogVideoXCausalConv3d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, pad_mode=pad_mode)
        if self.in_channels != self.out_channels:
            if self.use_conv_shortcut:
                self.conv_shortcut = CogVideoXCausalConv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, pad_mode=pad_mode)
            else:
                self.conv_shortcut = CogVideoXSafeConv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)

    def forward(self, inputs: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, zq: 'Optional[torch.Tensor]'=None, conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states = inputs
        if zq is not None:
            hidden_states, new_conv_cache['norm1'] = self.norm1(hidden_states, zq, conv_cache=conv_cache.get('norm1'))
        else:
            hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states, new_conv_cache['conv1'] = self.conv1(hidden_states, conv_cache=conv_cache.get('conv1'))
        if temb is not None:
            hidden_states = hidden_states + self.temb_proj(self.nonlinearity(temb))[:, :, None, None, None]
        if zq is not None:
            hidden_states, new_conv_cache['norm2'] = self.norm2(hidden_states, zq, conv_cache=conv_cache.get('norm2'))
        else:
            hidden_states = self.norm2(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states, new_conv_cache['conv2'] = self.conv2(hidden_states, conv_cache=conv_cache.get('conv2'))
        if self.in_channels != self.out_channels:
            if self.use_conv_shortcut:
                inputs, new_conv_cache['conv_shortcut'] = self.conv_shortcut(inputs, conv_cache=conv_cache.get('conv_shortcut'))
            else:
                inputs = self.conv_shortcut(inputs)
        hidden_states = hidden_states + inputs
        return hidden_states, new_conv_cache


class CogVideoXDownsample3D(nn.Module):
    """
    A 3D Downsampling layer using in [CogVideoX]() by Tsinghua University & ZhipuAI

    Args:
        in_channels (`int`):
            Number of channels in the input image.
        out_channels (`int`):
            Number of channels produced by the convolution.
        kernel_size (`int`, defaults to `3`):
            Size of the convolving kernel.
        stride (`int`, defaults to `2`):
            Stride of the convolution.
        padding (`int`, defaults to `0`):
            Padding added to all four sides of the input.
        compress_time (`bool`, defaults to `False`):
            Whether or not to compress the time dimension.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'int'=3, stride: 'int'=2, padding: 'int'=0, compress_time: 'bool'=False):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)
        self.compress_time = compress_time

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        if self.compress_time:
            batch_size, channels, frames, height, width = x.shape
            x = x.permute(0, 3, 4, 1, 2).reshape(batch_size * height * width, channels, frames)
            if x.shape[-1] % 2 == 1:
                x_first, x_rest = x[..., 0], x[..., 1:]
                if x_rest.shape[-1] > 0:
                    x_rest = F.avg_pool1d(x_rest, kernel_size=2, stride=2)
                x = torch.cat([x_first[..., None], x_rest], dim=-1)
                x = x.reshape(batch_size, height, width, channels, x.shape[-1]).permute(0, 3, 4, 1, 2)
            else:
                x = F.avg_pool1d(x, kernel_size=2, stride=2)
                x = x.reshape(batch_size, height, width, channels, x.shape[-1]).permute(0, 3, 4, 1, 2)
        pad = 0, 1, 0, 1
        x = F.pad(x, pad, mode='constant', value=0)
        batch_size, channels, frames, height, width = x.shape
        x = x.permute(0, 2, 1, 3, 4).reshape(batch_size * frames, channels, height, width)
        x = self.conv(x)
        x = x.reshape(batch_size, frames, x.shape[1], x.shape[2], x.shape[3]).permute(0, 2, 1, 3, 4)
        return x


class CogVideoXDownBlock3D(nn.Module):
    """
    A downsampling block used in the CogVideoX model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        temb_channels (`int`, defaults to `512`):
            Number of time embedding channels.
        num_layers (`int`, defaults to `1`):
            Number of resnet layers.
        dropout (`float`, defaults to `0.0`):
            Dropout rate.
        resnet_eps (`float`, defaults to `1e-6`):
            Epsilon value for normalization layers.
        resnet_act_fn (`str`, defaults to `"swish"`):
            Activation function to use.
        resnet_groups (`int`, defaults to `32`):
            Number of groups to separate the channels into for group normalization.
        add_downsample (`bool`, defaults to `True`):
            Whether or not to use a downsampling layer. If not used, output dimension would be same as input dimension.
        compress_time (`bool`, defaults to `False`):
            Whether or not to downsample across temporal dimension.
        pad_mode (str, defaults to `"first"`):
            Padding mode.
    """
    _supports_gradient_checkpointing = True

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, add_downsample: 'bool'=True, downsample_padding: 'int'=0, compress_time: 'bool'=False, pad_mode: 'str'='first'):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channel = in_channels if i == 0 else out_channels
            resnets.append(CogVideoXResnetBlock3D(in_channels=in_channel, out_channels=out_channels, dropout=dropout, temb_channels=temb_channels, groups=resnet_groups, eps=resnet_eps, non_linearity=resnet_act_fn, pad_mode=pad_mode))
        self.resnets = nn.ModuleList(resnets)
        self.downsamplers = None
        if add_downsample:
            self.downsamplers = nn.ModuleList([CogVideoXDownsample3D(out_channels, out_channels, padding=downsample_padding, compress_time=compress_time)])
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, zq: 'Optional[torch.Tensor]'=None, conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `CogVideoXDownBlock3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        for i, resnet in enumerate(self.resnets):
            conv_cache_key = f'resnet_{i}'
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def create_forward(*inputs):
                        return module(*inputs)
                    return create_forward
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, zq, conv_cache=conv_cache.get(conv_cache_key))
            else:
                hidden_states, new_conv_cache[conv_cache_key] = resnet(hidden_states, temb, zq, conv_cache=conv_cache.get(conv_cache_key))
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states, new_conv_cache


class CogVideoXMidBlock3D(nn.Module):
    """
    A middle block used in the CogVideoX model.

    Args:
        in_channels (`int`):
            Number of input channels.
        temb_channels (`int`, defaults to `512`):
            Number of time embedding channels.
        dropout (`float`, defaults to `0.0`):
            Dropout rate.
        num_layers (`int`, defaults to `1`):
            Number of resnet layers.
        resnet_eps (`float`, defaults to `1e-6`):
            Epsilon value for normalization layers.
        resnet_act_fn (`str`, defaults to `"swish"`):
            Activation function to use.
        resnet_groups (`int`, defaults to `32`):
            Number of groups to separate the channels into for group normalization.
        spatial_norm_dim (`int`, *optional*):
            The dimension to use for spatial norm if it is to be used instead of group norm.
        pad_mode (str, defaults to `"first"`):
            Padding mode.
    """
    _supports_gradient_checkpointing = True

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, spatial_norm_dim: 'Optional[int]'=None, pad_mode: 'str'='first'):
        super().__init__()
        resnets = []
        for _ in range(num_layers):
            resnets.append(CogVideoXResnetBlock3D(in_channels=in_channels, out_channels=in_channels, dropout=dropout, temb_channels=temb_channels, groups=resnet_groups, eps=resnet_eps, spatial_norm_dim=spatial_norm_dim, non_linearity=resnet_act_fn, pad_mode=pad_mode))
        self.resnets = nn.ModuleList(resnets)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, zq: 'Optional[torch.Tensor]'=None, conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `CogVideoXMidBlock3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        for i, resnet in enumerate(self.resnets):
            conv_cache_key = f'resnet_{i}'
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def create_forward(*inputs):
                        return module(*inputs)
                    return create_forward
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, zq, conv_cache=conv_cache.get(conv_cache_key))
            else:
                hidden_states, new_conv_cache[conv_cache_key] = resnet(hidden_states, temb, zq, conv_cache=conv_cache.get(conv_cache_key))
        return hidden_states, new_conv_cache


class CogVideoXUpsample3D(nn.Module):
    """
    A 3D Upsample layer using in CogVideoX by Tsinghua University & ZhipuAI # Todo: Wait for paper relase.

    Args:
        in_channels (`int`):
            Number of channels in the input image.
        out_channels (`int`):
            Number of channels produced by the convolution.
        kernel_size (`int`, defaults to `3`):
            Size of the convolving kernel.
        stride (`int`, defaults to `1`):
            Stride of the convolution.
        padding (`int`, defaults to `1`):
            Padding added to all four sides of the input.
        compress_time (`bool`, defaults to `False`):
            Whether or not to compress the time dimension.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'int'=3, stride: 'int'=1, padding: 'int'=1, compress_time: 'bool'=False) ->None:
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)
        self.compress_time = compress_time

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        if self.compress_time:
            if inputs.shape[2] > 1 and inputs.shape[2] % 2 == 1:
                x_first, x_rest = inputs[:, :, 0], inputs[:, :, 1:]
                x_first = F.interpolate(x_first, scale_factor=2.0)
                x_rest = F.interpolate(x_rest, scale_factor=2.0)
                x_first = x_first[:, :, None, :, :]
                inputs = torch.cat([x_first, x_rest], dim=2)
            elif inputs.shape[2] > 1:
                inputs = F.interpolate(inputs, scale_factor=2.0)
            else:
                inputs = inputs.squeeze(2)
                inputs = F.interpolate(inputs, scale_factor=2.0)
                inputs = inputs[:, :, None, :, :]
        else:
            b, c, t, h, w = inputs.shape
            inputs = inputs.permute(0, 2, 1, 3, 4).reshape(b * t, c, h, w)
            inputs = F.interpolate(inputs, scale_factor=2.0)
            inputs = inputs.reshape(b, t, c, *inputs.shape[2:]).permute(0, 2, 1, 3, 4)
        b, c, t, h, w = inputs.shape
        inputs = inputs.permute(0, 2, 1, 3, 4).reshape(b * t, c, h, w)
        inputs = self.conv(inputs)
        inputs = inputs.reshape(b, t, *inputs.shape[1:]).permute(0, 2, 1, 3, 4)
        return inputs


class CogVideoXUpBlock3D(nn.Module):
    """
    An upsampling block used in the CogVideoX model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        temb_channels (`int`, defaults to `512`):
            Number of time embedding channels.
        dropout (`float`, defaults to `0.0`):
            Dropout rate.
        num_layers (`int`, defaults to `1`):
            Number of resnet layers.
        resnet_eps (`float`, defaults to `1e-6`):
            Epsilon value for normalization layers.
        resnet_act_fn (`str`, defaults to `"swish"`):
            Activation function to use.
        resnet_groups (`int`, defaults to `32`):
            Number of groups to separate the channels into for group normalization.
        spatial_norm_dim (`int`, defaults to `16`):
            The dimension to use for spatial norm if it is to be used instead of group norm.
        add_upsample (`bool`, defaults to `True`):
            Whether or not to use a upsampling layer. If not used, output dimension would be same as input dimension.
        compress_time (`bool`, defaults to `False`):
            Whether or not to downsample across temporal dimension.
        pad_mode (str, defaults to `"first"`):
            Padding mode.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, spatial_norm_dim: 'int'=16, add_upsample: 'bool'=True, upsample_padding: 'int'=1, compress_time: 'bool'=False, pad_mode: 'str'='first'):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channel = in_channels if i == 0 else out_channels
            resnets.append(CogVideoXResnetBlock3D(in_channels=in_channel, out_channels=out_channels, dropout=dropout, temb_channels=temb_channels, groups=resnet_groups, eps=resnet_eps, non_linearity=resnet_act_fn, spatial_norm_dim=spatial_norm_dim, pad_mode=pad_mode))
        self.resnets = nn.ModuleList(resnets)
        self.upsamplers = None
        if add_upsample:
            self.upsamplers = nn.ModuleList([CogVideoXUpsample3D(out_channels, out_channels, padding=upsample_padding, compress_time=compress_time)])
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, zq: 'Optional[torch.Tensor]'=None, conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `CogVideoXUpBlock3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        for i, resnet in enumerate(self.resnets):
            conv_cache_key = f'resnet_{i}'
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def create_forward(*inputs):
                        return module(*inputs)
                    return create_forward
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, zq, conv_cache=conv_cache.get(conv_cache_key))
            else:
                hidden_states, new_conv_cache[conv_cache_key] = resnet(hidden_states, temb, zq, conv_cache=conv_cache.get(conv_cache_key))
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states, new_conv_cache


class CogVideoXEncoder3D(nn.Module):
    """
    The `CogVideoXEncoder3D` layer of a variational autoencoder that encodes its input into a latent representation.

    Args:
        in_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_channels (`int`, *optional*, defaults to 3):
            The number of output channels.
        down_block_types (`Tuple[str, ...]`, *optional*, defaults to `("DownEncoderBlock2D",)`):
            The types of down blocks to use. See `~diffusers.models.unet_2d_blocks.get_down_block` for available
            options.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(64,)`):
            The number of output channels for each block.
        act_fn (`str`, *optional*, defaults to `"silu"`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
        layers_per_block (`int`, *optional*, defaults to 2):
            The number of layers per block.
        norm_num_groups (`int`, *optional*, defaults to 32):
            The number of groups for normalization.
    """
    _supports_gradient_checkpointing = True

    def __init__(self, in_channels: 'int'=3, out_channels: 'int'=16, down_block_types: 'Tuple[str, ...]'=('CogVideoXDownBlock3D', 'CogVideoXDownBlock3D', 'CogVideoXDownBlock3D', 'CogVideoXDownBlock3D'), block_out_channels: 'Tuple[int, ...]'=(128, 256, 256, 512), layers_per_block: 'int'=3, act_fn: 'str'='silu', norm_eps: 'float'=1e-06, norm_num_groups: 'int'=32, dropout: 'float'=0.0, pad_mode: 'str'='first', temporal_compression_ratio: 'float'=4):
        super().__init__()
        temporal_compress_level = int(np.log2(temporal_compression_ratio))
        self.conv_in = CogVideoXCausalConv3d(in_channels, block_out_channels[0], kernel_size=3, pad_mode=pad_mode)
        self.down_blocks = nn.ModuleList([])
        output_channel = block_out_channels[0]
        for i, down_block_type in enumerate(down_block_types):
            input_channel = output_channel
            output_channel = block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            compress_time = i < temporal_compress_level
            if down_block_type == 'CogVideoXDownBlock3D':
                down_block = CogVideoXDownBlock3D(in_channels=input_channel, out_channels=output_channel, temb_channels=0, dropout=dropout, num_layers=layers_per_block, resnet_eps=norm_eps, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, add_downsample=not is_final_block, compress_time=compress_time)
            else:
                raise ValueError('Invalid `down_block_type` encountered. Must be `CogVideoXDownBlock3D`')
            self.down_blocks.append(down_block)
        self.mid_block = CogVideoXMidBlock3D(in_channels=block_out_channels[-1], temb_channels=0, dropout=dropout, num_layers=2, resnet_eps=norm_eps, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, pad_mode=pad_mode)
        self.norm_out = nn.GroupNorm(norm_num_groups, block_out_channels[-1], eps=1e-06)
        self.conv_act = nn.SiLU()
        self.conv_out = CogVideoXCausalConv3d(block_out_channels[-1], 2 * out_channels, kernel_size=3, pad_mode=pad_mode)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """The forward method of the `CogVideoXEncoder3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states, new_conv_cache['conv_in'] = self.conv_in(sample, conv_cache=conv_cache.get('conv_in'))
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            for i, down_block in enumerate(self.down_blocks):
                conv_cache_key = f'down_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), hidden_states, temb, None, conv_cache=conv_cache.get(conv_cache_key))
            hidden_states, new_conv_cache['mid_block'] = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), hidden_states, temb, None, conv_cache=conv_cache.get('mid_block'))
        else:
            for i, down_block in enumerate(self.down_blocks):
                conv_cache_key = f'down_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = down_block(hidden_states, temb, None, conv_cache=conv_cache.get(conv_cache_key))
            hidden_states, new_conv_cache['mid_block'] = self.mid_block(hidden_states, temb, None, conv_cache=conv_cache.get('mid_block'))
        hidden_states = self.norm_out(hidden_states)
        hidden_states = self.conv_act(hidden_states)
        hidden_states, new_conv_cache['conv_out'] = self.conv_out(hidden_states, conv_cache=conv_cache.get('conv_out'))
        return hidden_states, new_conv_cache


class CogVideoXDecoder3D(nn.Module):
    """
    The `CogVideoXDecoder3D` layer of a variational autoencoder that decodes its latent representation into an output
    sample.

    Args:
        in_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_channels (`int`, *optional*, defaults to 3):
            The number of output channels.
        up_block_types (`Tuple[str, ...]`, *optional*, defaults to `("UpDecoderBlock2D",)`):
            The types of up blocks to use. See `~diffusers.models.unet_2d_blocks.get_up_block` for available options.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(64,)`):
            The number of output channels for each block.
        act_fn (`str`, *optional*, defaults to `"silu"`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
        layers_per_block (`int`, *optional*, defaults to 2):
            The number of layers per block.
        norm_num_groups (`int`, *optional*, defaults to 32):
            The number of groups for normalization.
    """
    _supports_gradient_checkpointing = True

    def __init__(self, in_channels: 'int'=16, out_channels: 'int'=3, up_block_types: 'Tuple[str, ...]'=('CogVideoXUpBlock3D', 'CogVideoXUpBlock3D', 'CogVideoXUpBlock3D', 'CogVideoXUpBlock3D'), block_out_channels: 'Tuple[int, ...]'=(128, 256, 256, 512), layers_per_block: 'int'=3, act_fn: 'str'='silu', norm_eps: 'float'=1e-06, norm_num_groups: 'int'=32, dropout: 'float'=0.0, pad_mode: 'str'='first', temporal_compression_ratio: 'float'=4):
        super().__init__()
        reversed_block_out_channels = list(reversed(block_out_channels))
        self.conv_in = CogVideoXCausalConv3d(in_channels, reversed_block_out_channels[0], kernel_size=3, pad_mode=pad_mode)
        self.mid_block = CogVideoXMidBlock3D(in_channels=reversed_block_out_channels[0], temb_channels=0, num_layers=2, resnet_eps=norm_eps, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, spatial_norm_dim=in_channels, pad_mode=pad_mode)
        self.up_blocks = nn.ModuleList([])
        output_channel = reversed_block_out_channels[0]
        temporal_compress_level = int(np.log2(temporal_compression_ratio))
        for i, up_block_type in enumerate(up_block_types):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            compress_time = i < temporal_compress_level
            if up_block_type == 'CogVideoXUpBlock3D':
                up_block = CogVideoXUpBlock3D(in_channels=prev_output_channel, out_channels=output_channel, temb_channels=0, dropout=dropout, num_layers=layers_per_block + 1, resnet_eps=norm_eps, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, spatial_norm_dim=in_channels, add_upsample=not is_final_block, compress_time=compress_time, pad_mode=pad_mode)
                prev_output_channel = output_channel
            else:
                raise ValueError('Invalid `up_block_type` encountered. Must be `CogVideoXUpBlock3D`')
            self.up_blocks.append(up_block)
        self.norm_out = CogVideoXSpatialNorm3D(reversed_block_out_channels[-1], in_channels, groups=norm_num_groups)
        self.conv_act = nn.SiLU()
        self.conv_out = CogVideoXCausalConv3d(reversed_block_out_channels[-1], out_channels, kernel_size=3, pad_mode=pad_mode)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """The forward method of the `CogVideoXDecoder3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states, new_conv_cache['conv_in'] = self.conv_in(sample, conv_cache=conv_cache.get('conv_in'))
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            hidden_states, new_conv_cache['mid_block'] = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), hidden_states, temb, sample, conv_cache=conv_cache.get('mid_block'))
            for i, up_block in enumerate(self.up_blocks):
                conv_cache_key = f'up_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), hidden_states, temb, sample, conv_cache=conv_cache.get(conv_cache_key))
        else:
            hidden_states, new_conv_cache['mid_block'] = self.mid_block(hidden_states, temb, sample, conv_cache=conv_cache.get('mid_block'))
            for i, up_block in enumerate(self.up_blocks):
                conv_cache_key = f'up_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = up_block(hidden_states, temb, sample, conv_cache=conv_cache.get(conv_cache_key))
        hidden_states, new_conv_cache['norm_out'] = self.norm_out(hidden_states, sample, conv_cache=conv_cache.get('norm_out'))
        hidden_states = self.conv_act(hidden_states)
        hidden_states, new_conv_cache['conv_out'] = self.conv_out(hidden_states, conv_cache=conv_cache.get('conv_out'))
        return hidden_states, new_conv_cache


class MochiChunkedGroupNorm3D(nn.Module):
    """
    Applies per-frame group normalization for 5D video inputs. It also supports memory-efficient chunked group
    normalization.

    Args:
        num_channels (int): Number of channels expected in input
        num_groups (int, optional): Number of groups to separate the channels into. Default: 32
        affine (bool, optional): If True, this module has learnable affine parameters. Default: True
        chunk_size (int, optional): Size of each chunk for processing. Default: 8

    """

    def __init__(self, num_channels: 'int', num_groups: 'int'=32, affine: 'bool'=True, chunk_size: 'int'=8):
        super().__init__()
        self.norm_layer = nn.GroupNorm(num_channels=num_channels, num_groups=num_groups, affine=affine)
        self.chunk_size = chunk_size

    def forward(self, x: 'torch.Tensor'=None) ->torch.Tensor:
        batch_size = x.size(0)
        x = x.permute(0, 2, 1, 3, 4).flatten(0, 1)
        output = torch.cat([self.norm_layer(chunk) for chunk in x.split(self.chunk_size, dim=0)], dim=0)
        output = output.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3, 4)
        return output


class MochiResnetBlock3D(nn.Module):
    """
    A 3D ResNet block used in the Mochi model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        non_linearity (`str`, defaults to `"swish"`):
            Activation function to use.
    """

    def __init__(self, in_channels: 'int', out_channels: 'Optional[int]'=None, act_fn: 'str'='swish'):
        super().__init__()
        out_channels = out_channels or in_channels
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.nonlinearity = get_activation(act_fn)
        self.norm1 = MochiChunkedGroupNorm3D(num_channels=in_channels)
        self.conv1 = CogVideoXCausalConv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, pad_mode='replicate')
        self.norm2 = MochiChunkedGroupNorm3D(num_channels=out_channels)
        self.conv2 = CogVideoXCausalConv3d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, pad_mode='replicate')

    def forward(self, inputs: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states = inputs
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states, new_conv_cache['conv1'] = self.conv1(hidden_states, conv_cache=conv_cache.get('conv1'))
        hidden_states = self.norm2(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states, new_conv_cache['conv2'] = self.conv2(hidden_states, conv_cache=conv_cache.get('conv2'))
        hidden_states = hidden_states + inputs
        return hidden_states, new_conv_cache


class MochiVaeAttnProcessor2_0:
    """
    Attention processor used in Mochi VAE.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        is_single_frame = hidden_states.shape[1] == 1
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if is_single_frame:
            hidden_states = attn.to_v(hidden_states)
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
            if attn.residual_connection:
                hidden_states = hidden_states + residual
            hidden_states = hidden_states / attn.rescale_output_factor
            return hidden_states
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=attn.is_causal)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class MochiDownBlock3D(nn.Module):
    """
    An downsampling block used in the Mochi model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        num_layers (`int`, defaults to `1`):
            Number of resnet blocks in the block.
        temporal_expansion (`int`, defaults to `2`):
            Temporal expansion factor.
        spatial_expansion (`int`, defaults to `2`):
            Spatial expansion factor.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', num_layers: 'int'=1, temporal_expansion: 'int'=2, spatial_expansion: 'int'=2, add_attention: 'bool'=True):
        super().__init__()
        self.temporal_expansion = temporal_expansion
        self.spatial_expansion = spatial_expansion
        self.conv_in = CogVideoXCausalConv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=(temporal_expansion, spatial_expansion, spatial_expansion), stride=(temporal_expansion, spatial_expansion, spatial_expansion), pad_mode='replicate')
        resnets = []
        norms = []
        attentions = []
        for _ in range(num_layers):
            resnets.append(MochiResnetBlock3D(in_channels=out_channels))
            if add_attention:
                norms.append(MochiChunkedGroupNorm3D(num_channels=out_channels))
                attentions.append(Attention(query_dim=out_channels, heads=out_channels // 32, dim_head=32, qk_norm='l2', is_causal=True, processor=MochiVaeAttnProcessor2_0()))
            else:
                norms.append(None)
                attentions.append(None)
        self.resnets = nn.ModuleList(resnets)
        self.norms = nn.ModuleList(norms)
        self.attentions = nn.ModuleList(attentions)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None, chunk_size: 'int'=2 ** 15) ->torch.Tensor:
        """Forward method of the `MochiUpBlock3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states, new_conv_cache['conv_in'] = self.conv_in(hidden_states)
        for i, (resnet, norm, attn) in enumerate(zip(self.resnets, self.norms, self.attentions)):
            conv_cache_key = f'resnet_{i}'
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def create_forward(*inputs):
                        return module(*inputs)
                    return create_forward
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, conv_cache=conv_cache.get(conv_cache_key))
            else:
                hidden_states, new_conv_cache[conv_cache_key] = resnet(hidden_states, conv_cache=conv_cache.get(conv_cache_key))
            if attn is not None:
                residual = hidden_states
                hidden_states = norm(hidden_states)
                batch_size, num_channels, num_frames, height, width = hidden_states.shape
                hidden_states = hidden_states.permute(0, 3, 4, 2, 1).flatten(0, 2).contiguous()
                if hidden_states.size(0) <= chunk_size:
                    hidden_states = attn(hidden_states)
                else:
                    hidden_states_chunks = []
                    for i in range(0, hidden_states.size(0), chunk_size):
                        hidden_states_chunk = hidden_states[i:i + chunk_size]
                        hidden_states_chunk = attn(hidden_states_chunk)
                        hidden_states_chunks.append(hidden_states_chunk)
                    hidden_states = torch.cat(hidden_states_chunks)
                hidden_states = hidden_states.unflatten(0, (batch_size, height, width)).permute(0, 4, 3, 1, 2)
                hidden_states = residual + hidden_states
        return hidden_states, new_conv_cache


class MochiMidBlock3D(nn.Module):
    """
    A middle block used in the Mochi model.

    Args:
        in_channels (`int`):
            Number of input channels.
        num_layers (`int`, defaults to `3`):
            Number of resnet blocks in the block.
    """

    def __init__(self, in_channels: 'int', num_layers: 'int'=3, add_attention: 'bool'=True):
        super().__init__()
        resnets = []
        norms = []
        attentions = []
        for _ in range(num_layers):
            resnets.append(MochiResnetBlock3D(in_channels=in_channels))
            if add_attention:
                norms.append(MochiChunkedGroupNorm3D(num_channels=in_channels))
                attentions.append(Attention(query_dim=in_channels, heads=in_channels // 32, dim_head=32, qk_norm='l2', is_causal=True, processor=MochiVaeAttnProcessor2_0()))
            else:
                norms.append(None)
                attentions.append(None)
        self.resnets = nn.ModuleList(resnets)
        self.norms = nn.ModuleList(norms)
        self.attentions = nn.ModuleList(attentions)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `MochiMidBlock3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        for i, (resnet, norm, attn) in enumerate(zip(self.resnets, self.norms, self.attentions)):
            conv_cache_key = f'resnet_{i}'
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def create_forward(*inputs):
                        return module(*inputs)
                    return create_forward
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, conv_cache=conv_cache.get(conv_cache_key))
            else:
                hidden_states, new_conv_cache[conv_cache_key] = resnet(hidden_states, conv_cache=conv_cache.get(conv_cache_key))
            if attn is not None:
                residual = hidden_states
                hidden_states = norm(hidden_states)
                batch_size, num_channels, num_frames, height, width = hidden_states.shape
                hidden_states = hidden_states.permute(0, 3, 4, 2, 1).flatten(0, 2).contiguous()
                hidden_states = attn(hidden_states)
                hidden_states = hidden_states.unflatten(0, (batch_size, height, width)).permute(0, 4, 3, 1, 2)
                hidden_states = residual + hidden_states
        return hidden_states, new_conv_cache


class MochiUpBlock3D(nn.Module):
    """
    An upsampling block used in the Mochi model.

    Args:
        in_channels (`int`):
            Number of input channels.
        out_channels (`int`, *optional*):
            Number of output channels. If None, defaults to `in_channels`.
        num_layers (`int`, defaults to `1`):
            Number of resnet blocks in the block.
        temporal_expansion (`int`, defaults to `2`):
            Temporal expansion factor.
        spatial_expansion (`int`, defaults to `2`):
            Spatial expansion factor.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', num_layers: 'int'=1, temporal_expansion: 'int'=2, spatial_expansion: 'int'=2):
        super().__init__()
        self.temporal_expansion = temporal_expansion
        self.spatial_expansion = spatial_expansion
        resnets = []
        for _ in range(num_layers):
            resnets.append(MochiResnetBlock3D(in_channels=in_channels))
        self.resnets = nn.ModuleList(resnets)
        self.proj = nn.Linear(in_channels, out_channels * temporal_expansion * spatial_expansion ** 2)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `MochiUpBlock3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        for i, resnet in enumerate(self.resnets):
            conv_cache_key = f'resnet_{i}'
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def create_forward(*inputs):
                        return module(*inputs)
                    return create_forward
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, conv_cache=conv_cache.get(conv_cache_key))
            else:
                hidden_states, new_conv_cache[conv_cache_key] = resnet(hidden_states, conv_cache=conv_cache.get(conv_cache_key))
        hidden_states = hidden_states.permute(0, 2, 3, 4, 1)
        hidden_states = self.proj(hidden_states)
        hidden_states = hidden_states.permute(0, 4, 1, 2, 3)
        batch_size, num_channels, num_frames, height, width = hidden_states.shape
        st = self.temporal_expansion
        sh = self.spatial_expansion
        sw = self.spatial_expansion
        hidden_states = hidden_states.view(batch_size, -1, st, sh, sw, num_frames, height, width)
        hidden_states = hidden_states.permute(0, 1, 5, 2, 6, 3, 7, 4).contiguous()
        hidden_states = hidden_states.view(batch_size, -1, num_frames * st, height * sh, width * sw)
        return hidden_states, new_conv_cache


class FourierFeatures(nn.Module):

    def __init__(self, start: 'int'=6, stop: 'int'=8, step: 'int'=1):
        super().__init__()
        self.start = start
        self.stop = stop
        self.step = step

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        """Forward method of the `FourierFeatures` class."""
        num_channels = inputs.shape[1]
        num_freqs = (self.stop - self.start) // self.step
        freqs = torch.arange(self.start, self.stop, self.step, dtype=inputs.dtype, device=inputs.device)
        w = torch.pow(2.0, freqs) * (2 * torch.pi)
        w = w.repeat(num_channels)[None, :, None, None, None]
        h = inputs.repeat_interleave(num_freqs, dim=1)
        h = w * h
        return torch.cat([inputs, torch.sin(h), torch.cos(h)], dim=1)


class MochiEncoder3D(nn.Module):
    """
    The `MochiEncoder3D` layer of a variational autoencoder that encodes input video samples to its latent
    representation.

    Args:
        in_channels (`int`, *optional*):
            The number of input channels.
        out_channels (`int`, *optional*):
            The number of output channels.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(128, 256, 512, 768)`):
            The number of output channels for each block.
        layers_per_block (`Tuple[int, ...]`, *optional*, defaults to `(3, 3, 4, 6, 3)`):
            The number of resnet blocks for each block.
        temporal_expansions (`Tuple[int, ...]`, *optional*, defaults to `(1, 2, 3)`):
            The temporal expansion factor for each of the up blocks.
        spatial_expansions (`Tuple[int, ...]`, *optional*, defaults to `(2, 2, 2)`):
            The spatial expansion factor for each of the up blocks.
        non_linearity (`str`, *optional*, defaults to `"swish"`):
            The non-linearity to use in the decoder.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', block_out_channels: 'Tuple[int, ...]'=(128, 256, 512, 768), layers_per_block: 'Tuple[int, ...]'=(3, 3, 4, 6, 3), temporal_expansions: 'Tuple[int, ...]'=(1, 2, 3), spatial_expansions: 'Tuple[int, ...]'=(2, 2, 2), add_attention_block: 'Tuple[bool, ...]'=(False, True, True, True, True), act_fn: 'str'='swish'):
        super().__init__()
        self.nonlinearity = get_activation(act_fn)
        self.fourier_features = FourierFeatures()
        self.proj_in = nn.Linear(in_channels, block_out_channels[0])
        self.block_in = MochiMidBlock3D(in_channels=block_out_channels[0], num_layers=layers_per_block[0], add_attention=add_attention_block[0])
        down_blocks = []
        for i in range(len(block_out_channels) - 1):
            down_block = MochiDownBlock3D(in_channels=block_out_channels[i], out_channels=block_out_channels[i + 1], num_layers=layers_per_block[i + 1], temporal_expansion=temporal_expansions[i], spatial_expansion=spatial_expansions[i], add_attention=add_attention_block[i + 1])
            down_blocks.append(down_block)
        self.down_blocks = nn.ModuleList(down_blocks)
        self.block_out = MochiMidBlock3D(in_channels=block_out_channels[-1], num_layers=layers_per_block[-1], add_attention=add_attention_block[-1])
        self.norm_out = MochiChunkedGroupNorm3D(block_out_channels[-1])
        self.proj_out = nn.Linear(block_out_channels[-1], 2 * out_channels, bias=False)

    def forward(self, hidden_states: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `MochiEncoder3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states = self.fourier_features(hidden_states)
        hidden_states = hidden_states.permute(0, 2, 3, 4, 1)
        hidden_states = self.proj_in(hidden_states)
        hidden_states = hidden_states.permute(0, 4, 1, 2, 3)
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def create_forward(*inputs):
                    return module(*inputs)
                return create_forward
            hidden_states, new_conv_cache['block_in'] = torch.utils.checkpoint.checkpoint(create_custom_forward(self.block_in), hidden_states, conv_cache=conv_cache.get('block_in'))
            for i, down_block in enumerate(self.down_blocks):
                conv_cache_key = f'down_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), hidden_states, conv_cache=conv_cache.get(conv_cache_key))
        else:
            hidden_states, new_conv_cache['block_in'] = self.block_in(hidden_states, conv_cache=conv_cache.get('block_in'))
            for i, down_block in enumerate(self.down_blocks):
                conv_cache_key = f'down_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = down_block(hidden_states, conv_cache=conv_cache.get(conv_cache_key))
        hidden_states, new_conv_cache['block_out'] = self.block_out(hidden_states, conv_cache=conv_cache.get('block_out'))
        hidden_states = self.norm_out(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = hidden_states.permute(0, 2, 3, 4, 1)
        hidden_states = self.proj_out(hidden_states)
        hidden_states = hidden_states.permute(0, 4, 1, 2, 3)
        return hidden_states, new_conv_cache


class MochiDecoder3D(nn.Module):
    """
    The `MochiDecoder3D` layer of a variational autoencoder that decodes its latent representation into an output
    sample.

    Args:
        in_channels (`int`, *optional*):
            The number of input channels.
        out_channels (`int`, *optional*):
            The number of output channels.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(128, 256, 512, 768)`):
            The number of output channels for each block.
        layers_per_block (`Tuple[int, ...]`, *optional*, defaults to `(3, 3, 4, 6, 3)`):
            The number of resnet blocks for each block.
        temporal_expansions (`Tuple[int, ...]`, *optional*, defaults to `(1, 2, 3)`):
            The temporal expansion factor for each of the up blocks.
        spatial_expansions (`Tuple[int, ...]`, *optional*, defaults to `(2, 2, 2)`):
            The spatial expansion factor for each of the up blocks.
        non_linearity (`str`, *optional*, defaults to `"swish"`):
            The non-linearity to use in the decoder.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', block_out_channels: 'Tuple[int, ...]'=(128, 256, 512, 768), layers_per_block: 'Tuple[int, ...]'=(3, 3, 4, 6, 3), temporal_expansions: 'Tuple[int, ...]'=(1, 2, 3), spatial_expansions: 'Tuple[int, ...]'=(2, 2, 2), act_fn: 'str'='swish'):
        super().__init__()
        self.nonlinearity = get_activation(act_fn)
        self.conv_in = nn.Conv3d(in_channels, block_out_channels[-1], kernel_size=(1, 1, 1))
        self.block_in = MochiMidBlock3D(in_channels=block_out_channels[-1], num_layers=layers_per_block[-1], add_attention=False)
        up_blocks = []
        for i in range(len(block_out_channels) - 1):
            up_block = MochiUpBlock3D(in_channels=block_out_channels[-i - 1], out_channels=block_out_channels[-i - 2], num_layers=layers_per_block[-i - 2], temporal_expansion=temporal_expansions[-i - 1], spatial_expansion=spatial_expansions[-i - 1])
            up_blocks.append(up_block)
        self.up_blocks = nn.ModuleList(up_blocks)
        self.block_out = MochiMidBlock3D(in_channels=block_out_channels[0], num_layers=layers_per_block[0], add_attention=False)
        self.proj_out = nn.Linear(block_out_channels[0], out_channels)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', conv_cache: 'Optional[Dict[str, torch.Tensor]]'=None) ->torch.Tensor:
        """Forward method of the `MochiDecoder3D` class."""
        new_conv_cache = {}
        conv_cache = conv_cache or {}
        hidden_states = self.conv_in(hidden_states)
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def create_forward(*inputs):
                    return module(*inputs)
                return create_forward
            hidden_states, new_conv_cache['block_in'] = torch.utils.checkpoint.checkpoint(create_custom_forward(self.block_in), hidden_states, conv_cache=conv_cache.get('block_in'))
            for i, up_block in enumerate(self.up_blocks):
                conv_cache_key = f'up_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), hidden_states, conv_cache=conv_cache.get(conv_cache_key))
        else:
            hidden_states, new_conv_cache['block_in'] = self.block_in(hidden_states, conv_cache=conv_cache.get('block_in'))
            for i, up_block in enumerate(self.up_blocks):
                conv_cache_key = f'up_block_{i}'
                hidden_states, new_conv_cache[conv_cache_key] = up_block(hidden_states, conv_cache=conv_cache.get(conv_cache_key))
        hidden_states, new_conv_cache['block_out'] = self.block_out(hidden_states, conv_cache=conv_cache.get('block_out'))
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = hidden_states.permute(0, 2, 3, 4, 1)
        hidden_states = self.proj_out(hidden_states)
        hidden_states = hidden_states.permute(0, 4, 1, 2, 3)
        return hidden_states, new_conv_cache


class AlphaBlender(nn.Module):
    """
    A module to blend spatial and temporal features.

    Parameters:
        alpha (`float`): The initial value of the blending factor.
        merge_strategy (`str`, *optional*, defaults to `learned_with_images`):
            The merge strategy to use for the temporal mixing.
        switch_spatial_to_temporal_mix (`bool`, *optional*, defaults to `False`):
            If `True`, switch the spatial and temporal mixing.
    """
    strategies = ['learned', 'fixed', 'learned_with_images']

    def __init__(self, alpha: 'float', merge_strategy: 'str'='learned_with_images', switch_spatial_to_temporal_mix: 'bool'=False):
        super().__init__()
        self.merge_strategy = merge_strategy
        self.switch_spatial_to_temporal_mix = switch_spatial_to_temporal_mix
        if merge_strategy not in self.strategies:
            raise ValueError(f'merge_strategy needs to be in {self.strategies}')
        if self.merge_strategy == 'fixed':
            self.register_buffer('mix_factor', torch.Tensor([alpha]))
        elif self.merge_strategy == 'learned' or self.merge_strategy == 'learned_with_images':
            self.register_parameter('mix_factor', torch.nn.Parameter(torch.Tensor([alpha])))
        else:
            raise ValueError(f'Unknown merge strategy {self.merge_strategy}')

    def get_alpha(self, image_only_indicator: 'torch.Tensor', ndims: 'int') ->torch.Tensor:
        if self.merge_strategy == 'fixed':
            alpha = self.mix_factor
        elif self.merge_strategy == 'learned':
            alpha = torch.sigmoid(self.mix_factor)
        elif self.merge_strategy == 'learned_with_images':
            if image_only_indicator is None:
                raise ValueError('Please provide image_only_indicator to use learned_with_images merge strategy')
            alpha = torch.where(image_only_indicator.bool(), torch.ones(1, 1, device=image_only_indicator.device), torch.sigmoid(self.mix_factor)[..., None])
            if ndims == 5:
                alpha = alpha[:, None, :, None, None]
            elif ndims == 3:
                alpha = alpha.reshape(-1)[:, None, None]
            else:
                raise ValueError(f'Unexpected ndims {ndims}. Dimensions should be 3 or 5')
        else:
            raise NotImplementedError
        return alpha

    def forward(self, x_spatial: 'torch.Tensor', x_temporal: 'torch.Tensor', image_only_indicator: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        alpha = self.get_alpha(image_only_indicator, x_spatial.ndim)
        alpha = alpha
        if self.switch_spatial_to_temporal_mix:
            alpha = 1.0 - alpha
        x = alpha * x_spatial + (1.0 - alpha) * x_temporal
        return x


class TemporalResnetBlock(nn.Module):
    """
    A Resnet block.

    Parameters:
        in_channels (`int`): The number of channels in the input.
        out_channels (`int`, *optional*, default to be `None`):
            The number of output channels for the first conv2d layer. If None, same as `in_channels`.
        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.
        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.
    """

    def __init__(self, in_channels: 'int', out_channels: 'Optional[int]'=None, temb_channels: 'int'=512, eps: 'float'=1e-06):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        kernel_size = 3, 1, 1
        padding = [(k // 2) for k in kernel_size]
        self.norm1 = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=eps, affine=True)
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding)
        if temb_channels is not None:
            self.time_emb_proj = nn.Linear(temb_channels, out_channels)
        else:
            self.time_emb_proj = None
        self.norm2 = torch.nn.GroupNorm(num_groups=32, num_channels=out_channels, eps=eps, affine=True)
        self.dropout = torch.nn.Dropout(0.0)
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding)
        self.nonlinearity = get_activation('silu')
        self.use_in_shortcut = self.in_channels != out_channels
        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)

    def forward(self, input_tensor: 'torch.Tensor', temb: 'torch.Tensor') ->torch.Tensor:
        hidden_states = input_tensor
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.conv1(hidden_states)
        if self.time_emb_proj is not None:
            temb = self.nonlinearity(temb)
            temb = self.time_emb_proj(temb)[:, :, :, None, None]
            temb = temb.permute(0, 2, 1, 3, 4)
            hidden_states = hidden_states + temb
        hidden_states = self.norm2(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)
        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)
        output_tensor = input_tensor + hidden_states
        return output_tensor


class SpatioTemporalResBlock(nn.Module):
    """
    A SpatioTemporal Resnet block.

    Parameters:
        in_channels (`int`): The number of channels in the input.
        out_channels (`int`, *optional*, default to be `None`):
            The number of output channels for the first conv2d layer. If None, same as `in_channels`.
        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.
        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the spatial resenet.
        temporal_eps (`float`, *optional*, defaults to `eps`): The epsilon to use for the temporal resnet.
        merge_factor (`float`, *optional*, defaults to `0.5`): The merge factor to use for the temporal mixing.
        merge_strategy (`str`, *optional*, defaults to `learned_with_images`):
            The merge strategy to use for the temporal mixing.
        switch_spatial_to_temporal_mix (`bool`, *optional*, defaults to `False`):
            If `True`, switch the spatial and temporal mixing.
    """

    def __init__(self, in_channels: 'int', out_channels: 'Optional[int]'=None, temb_channels: 'int'=512, eps: 'float'=1e-06, temporal_eps: 'Optional[float]'=None, merge_factor: 'float'=0.5, merge_strategy='learned_with_images', switch_spatial_to_temporal_mix: 'bool'=False):
        super().__init__()
        self.spatial_res_block = ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=eps)
        self.temporal_res_block = TemporalResnetBlock(in_channels=out_channels if out_channels is not None else in_channels, out_channels=out_channels if out_channels is not None else in_channels, temb_channels=temb_channels, eps=temporal_eps if temporal_eps is not None else eps)
        self.time_mixer = AlphaBlender(alpha=merge_factor, merge_strategy=merge_strategy, switch_spatial_to_temporal_mix=switch_spatial_to_temporal_mix)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None):
        num_frames = image_only_indicator.shape[-1]
        hidden_states = self.spatial_res_block(hidden_states, temb)
        batch_frames, channels, height, width = hidden_states.shape
        batch_size = batch_frames // num_frames
        hidden_states_mix = hidden_states[None, :].reshape(batch_size, num_frames, channels, height, width).permute(0, 2, 1, 3, 4)
        hidden_states = hidden_states[None, :].reshape(batch_size, num_frames, channels, height, width).permute(0, 2, 1, 3, 4)
        if temb is not None:
            temb = temb.reshape(batch_size, num_frames, -1)
        hidden_states = self.temporal_res_block(hidden_states, temb)
        hidden_states = self.time_mixer(x_spatial=hidden_states_mix, x_temporal=hidden_states, image_only_indicator=image_only_indicator)
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).reshape(batch_frames, channels, height, width)
        return hidden_states


class MidBlockTemporalDecoder(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', attention_head_dim: 'int'=512, num_layers: 'int'=1, upcast_attention: 'bool'=False):
        super().__init__()
        resnets = []
        attentions = []
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            resnets.append(SpatioTemporalResBlock(in_channels=input_channels, out_channels=out_channels, temb_channels=None, eps=1e-06, temporal_eps=1e-05, merge_factor=0.0, merge_strategy='learned', switch_spatial_to_temporal_mix=True))
        attentions.append(Attention(query_dim=in_channels, heads=in_channels // attention_head_dim, dim_head=attention_head_dim, eps=1e-06, upcast_attention=upcast_attention, norm_num_groups=32, bias=True, residual_connection=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', image_only_indicator: 'torch.Tensor'):
        hidden_states = self.resnets[0](hidden_states, image_only_indicator=image_only_indicator)
        for resnet, attn in zip(self.resnets[1:], self.attentions):
            hidden_states = attn(hidden_states)
            hidden_states = resnet(hidden_states, image_only_indicator=image_only_indicator)
        return hidden_states


class UpBlockTemporalDecoder(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', num_layers: 'int'=1, add_upsample: 'bool'=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            resnets.append(SpatioTemporalResBlock(in_channels=input_channels, out_channels=out_channels, temb_channels=None, eps=1e-06, temporal_eps=1e-05, merge_factor=0.0, merge_strategy='learned', switch_spatial_to_temporal_mix=True))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None

    def forward(self, hidden_states: 'torch.Tensor', image_only_indicator: 'torch.Tensor') ->torch.Tensor:
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, image_only_indicator=image_only_indicator)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class TemporalDecoder(nn.Module):

    def __init__(self, in_channels: 'int'=4, out_channels: 'int'=3, block_out_channels: 'Tuple[int]'=(128, 256, 512, 512), layers_per_block: 'int'=2):
        super().__init__()
        self.layers_per_block = layers_per_block
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)
        self.mid_block = MidBlockTemporalDecoder(num_layers=self.layers_per_block, in_channels=block_out_channels[-1], out_channels=block_out_channels[-1], attention_head_dim=block_out_channels[-1])
        self.up_blocks = nn.ModuleList([])
        reversed_block_out_channels = list(reversed(block_out_channels))
        output_channel = reversed_block_out_channels[0]
        for i in range(len(block_out_channels)):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            up_block = UpBlockTemporalDecoder(num_layers=self.layers_per_block + 1, in_channels=prev_output_channel, out_channels=output_channel, add_upsample=not is_final_block)
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel
        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=32, eps=1e-06)
        self.conv_act = nn.SiLU()
        self.conv_out = torch.nn.Conv2d(in_channels=block_out_channels[0], out_channels=out_channels, kernel_size=3, padding=1)
        conv_out_kernel_size = 3, 1, 1
        padding = [int(k // 2) for k in conv_out_kernel_size]
        self.time_conv_out = torch.nn.Conv3d(in_channels=out_channels, out_channels=out_channels, kernel_size=conv_out_kernel_size, padding=padding)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor', image_only_indicator: 'torch.Tensor', num_frames: 'int'=1) ->torch.Tensor:
        """The forward method of the `Decoder` class."""
        sample = self.conv_in(sample)
        upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, image_only_indicator, use_reentrant=False)
                sample = sample
                for up_block in self.up_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, image_only_indicator, use_reentrant=False)
            else:
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, image_only_indicator)
                sample = sample
                for up_block in self.up_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, image_only_indicator)
        else:
            sample = self.mid_block(sample, image_only_indicator=image_only_indicator)
            sample = sample
            for up_block in self.up_blocks:
                sample = up_block(sample, image_only_indicator=image_only_indicator)
        sample = self.conv_norm_out(sample)
        sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        batch_frames, channels, height, width = sample.shape
        batch_size = batch_frames // num_frames
        sample = sample[None, :].reshape(batch_size, num_frames, channels, height, width).permute(0, 2, 1, 3, 4)
        sample = self.time_conv_out(sample)
        sample = sample.permute(0, 2, 1, 3, 4).reshape(batch_frames, channels, height, width)
        return sample


class Snake1d(nn.Module):
    """
    A 1-dimensional Snake activation function module.
    """

    def __init__(self, hidden_dim, logscale=True):
        super().__init__()
        self.alpha = nn.Parameter(torch.zeros(1, hidden_dim, 1))
        self.beta = nn.Parameter(torch.zeros(1, hidden_dim, 1))
        self.alpha.requires_grad = True
        self.beta.requires_grad = True
        self.logscale = logscale

    def forward(self, hidden_states):
        shape = hidden_states.shape
        alpha = self.alpha if not self.logscale else torch.exp(self.alpha)
        beta = self.beta if not self.logscale else torch.exp(self.beta)
        hidden_states = hidden_states.reshape(shape[0], shape[1], -1)
        hidden_states = hidden_states + (beta + 1e-09).reciprocal() * torch.sin(alpha * hidden_states).pow(2)
        hidden_states = hidden_states.reshape(shape)
        return hidden_states


class OobleckResidualUnit(nn.Module):
    """
    A residual unit composed of Snake1d and weight-normalized Conv1d layers with dilations.
    """

    def __init__(self, dimension: 'int'=16, dilation: 'int'=1):
        super().__init__()
        pad = (7 - 1) * dilation // 2
        self.snake1 = Snake1d(dimension)
        self.conv1 = weight_norm(nn.Conv1d(dimension, dimension, kernel_size=7, dilation=dilation, padding=pad))
        self.snake2 = Snake1d(dimension)
        self.conv2 = weight_norm(nn.Conv1d(dimension, dimension, kernel_size=1))

    def forward(self, hidden_state):
        """
        Forward pass through the residual unit.

        Args:
            hidden_state (`torch.Tensor` of shape `(batch_size, channels, time_steps)`):
                Input tensor .

        Returns:
            output_tensor (`torch.Tensor` of shape `(batch_size, channels, time_steps)`)
                Input tensor after passing through the residual unit.
        """
        output_tensor = hidden_state
        output_tensor = self.conv1(self.snake1(output_tensor))
        output_tensor = self.conv2(self.snake2(output_tensor))
        padding = (hidden_state.shape[-1] - output_tensor.shape[-1]) // 2
        if padding > 0:
            hidden_state = hidden_state[..., padding:-padding]
        output_tensor = hidden_state + output_tensor
        return output_tensor


class OobleckEncoderBlock(nn.Module):
    """Encoder block used in Oobleck encoder."""

    def __init__(self, input_dim, output_dim, stride: 'int'=1):
        super().__init__()
        self.res_unit1 = OobleckResidualUnit(input_dim, dilation=1)
        self.res_unit2 = OobleckResidualUnit(input_dim, dilation=3)
        self.res_unit3 = OobleckResidualUnit(input_dim, dilation=9)
        self.snake1 = Snake1d(input_dim)
        self.conv1 = weight_norm(nn.Conv1d(input_dim, output_dim, kernel_size=2 * stride, stride=stride, padding=math.ceil(stride / 2)))

    def forward(self, hidden_state):
        hidden_state = self.res_unit1(hidden_state)
        hidden_state = self.res_unit2(hidden_state)
        hidden_state = self.snake1(self.res_unit3(hidden_state))
        hidden_state = self.conv1(hidden_state)
        return hidden_state


class OobleckDecoderBlock(nn.Module):
    """Decoder block used in Oobleck decoder."""

    def __init__(self, input_dim, output_dim, stride: 'int'=1):
        super().__init__()
        self.snake1 = Snake1d(input_dim)
        self.conv_t1 = weight_norm(nn.ConvTranspose1d(input_dim, output_dim, kernel_size=2 * stride, stride=stride, padding=math.ceil(stride / 2)))
        self.res_unit1 = OobleckResidualUnit(output_dim, dilation=1)
        self.res_unit2 = OobleckResidualUnit(output_dim, dilation=3)
        self.res_unit3 = OobleckResidualUnit(output_dim, dilation=9)

    def forward(self, hidden_state):
        hidden_state = self.snake1(hidden_state)
        hidden_state = self.conv_t1(hidden_state)
        hidden_state = self.res_unit1(hidden_state)
        hidden_state = self.res_unit2(hidden_state)
        hidden_state = self.res_unit3(hidden_state)
        return hidden_state


class OobleckEncoder(nn.Module):
    """Oobleck Encoder"""

    def __init__(self, encoder_hidden_size, audio_channels, downsampling_ratios, channel_multiples):
        super().__init__()
        strides = downsampling_ratios
        channel_multiples = [1] + channel_multiples
        self.conv1 = weight_norm(nn.Conv1d(audio_channels, encoder_hidden_size, kernel_size=7, padding=3))
        self.block = []
        for stride_index, stride in enumerate(strides):
            self.block += [OobleckEncoderBlock(input_dim=encoder_hidden_size * channel_multiples[stride_index], output_dim=encoder_hidden_size * channel_multiples[stride_index + 1], stride=stride)]
        self.block = nn.ModuleList(self.block)
        d_model = encoder_hidden_size * channel_multiples[-1]
        self.snake1 = Snake1d(d_model)
        self.conv2 = weight_norm(nn.Conv1d(d_model, encoder_hidden_size, kernel_size=3, padding=1))

    def forward(self, hidden_state):
        hidden_state = self.conv1(hidden_state)
        for module in self.block:
            hidden_state = module(hidden_state)
        hidden_state = self.snake1(hidden_state)
        hidden_state = self.conv2(hidden_state)
        return hidden_state


class OobleckDecoder(nn.Module):
    """Oobleck Decoder"""

    def __init__(self, channels, input_channels, audio_channels, upsampling_ratios, channel_multiples):
        super().__init__()
        strides = upsampling_ratios
        channel_multiples = [1] + channel_multiples
        self.conv1 = weight_norm(nn.Conv1d(input_channels, channels * channel_multiples[-1], kernel_size=7, padding=3))
        block = []
        for stride_index, stride in enumerate(strides):
            block += [OobleckDecoderBlock(input_dim=channels * channel_multiples[len(strides) - stride_index], output_dim=channels * channel_multiples[len(strides) - stride_index - 1], stride=stride)]
        self.block = nn.ModuleList(block)
        output_dim = channels
        self.snake1 = Snake1d(output_dim)
        self.conv2 = weight_norm(nn.Conv1d(channels, audio_channels, kernel_size=7, padding=3, bias=False))

    def forward(self, hidden_state):
        hidden_state = self.conv1(hidden_state)
        for layer in self.block:
            hidden_state = layer(hidden_state)
        hidden_state = self.snake1(hidden_state)
        hidden_state = self.conv2(hidden_state)
        return hidden_state


class AdaGroupNorm(nn.Module):
    """
    GroupNorm layer modified to incorporate timestep embeddings.

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
        num_groups (`int`): The number of groups to separate the channels into.
        act_fn (`str`, *optional*, defaults to `None`): The activation function to use.
        eps (`float`, *optional*, defaults to `1e-5`): The epsilon value to use for numerical stability.
    """

    def __init__(self, embedding_dim: 'int', out_dim: 'int', num_groups: 'int', act_fn: 'Optional[str]'=None, eps: 'float'=1e-05):
        super().__init__()
        self.num_groups = num_groups
        self.eps = eps
        if act_fn is None:
            self.act = None
        else:
            self.act = get_activation(act_fn)
        self.linear = nn.Linear(embedding_dim, out_dim * 2)

    def forward(self, x: 'torch.Tensor', emb: 'torch.Tensor') ->torch.Tensor:
        if self.act:
            emb = self.act(emb)
        emb = self.linear(emb)
        emb = emb[:, :, None, None]
        scale, shift = emb.chunk(2, dim=1)
        x = F.group_norm(x, self.num_groups, eps=self.eps)
        x = x * (1 + scale) + shift
        return x


class ResnetBlockCondNorm2D(nn.Module):
    """
    A Resnet block that use normalization layer that incorporate conditioning information.

    Parameters:
        in_channels (`int`): The number of channels in the input.
        out_channels (`int`, *optional*, default to be `None`):
            The number of output channels for the first conv2d layer. If None, same as `in_channels`.
        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.
        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.
        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.
        groups_out (`int`, *optional*, default to None):
            The number of groups to use for the second normalization layer. if set to None, same as `groups`.
        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.
        non_linearity (`str`, *optional*, default to `"swish"`): the activation function to use.
        time_embedding_norm (`str`, *optional*, default to `"ada_group"` ):
            The normalization layer for time embedding `temb`. Currently only support "ada_group" or "spatial".
        kernel (`torch.Tensor`, optional, default to None): FIR filter, see
            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].
        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.
        use_in_shortcut (`bool`, *optional*, default to `True`):
            If `True`, add a 1x1 nn.conv2d layer for skip-connection.
        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.
        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.
        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the
            `conv_shortcut` output.
        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.
            If None, same as `out_channels`.
    """

    def __init__(self, *, in_channels: int, out_channels: Optional[int]=None, conv_shortcut: bool=False, dropout: float=0.0, temb_channels: int=512, groups: int=32, groups_out: Optional[int]=None, eps: float=1e-06, non_linearity: str='swish', time_embedding_norm: str='ada_group', output_scale_factor: float=1.0, use_in_shortcut: Optional[bool]=None, up: bool=False, down: bool=False, conv_shortcut_bias: bool=True, conv_2d_out_channels: Optional[int]=None):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut
        self.up = up
        self.down = down
        self.output_scale_factor = output_scale_factor
        self.time_embedding_norm = time_embedding_norm
        if groups_out is None:
            groups_out = groups
        if self.time_embedding_norm == 'ada_group':
            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)
        elif self.time_embedding_norm == 'spatial':
            self.norm1 = SpatialNorm(in_channels, temb_channels)
        else:
            raise ValueError(f' unsupported time_embedding_norm: {self.time_embedding_norm}')
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
        if self.time_embedding_norm == 'ada_group':
            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)
        elif self.time_embedding_norm == 'spatial':
            self.norm2 = SpatialNorm(out_channels, temb_channels)
        else:
            raise ValueError(f' unsupported time_embedding_norm: {self.time_embedding_norm}')
        self.dropout = torch.nn.Dropout(dropout)
        conv_2d_out_channels = conv_2d_out_channels or out_channels
        self.conv2 = nn.Conv2d(out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1)
        self.nonlinearity = get_activation(non_linearity)
        self.upsample = self.downsample = None
        if self.up:
            self.upsample = Upsample2D(in_channels, use_conv=False)
        elif self.down:
            self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name='op')
        self.use_in_shortcut = self.in_channels != conv_2d_out_channels if use_in_shortcut is None else use_in_shortcut
        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = nn.Conv2d(in_channels, conv_2d_out_channels, kernel_size=1, stride=1, padding=0, bias=conv_shortcut_bias)

    def forward(self, input_tensor: 'torch.Tensor', temb: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        hidden_states = input_tensor
        hidden_states = self.norm1(hidden_states, temb)
        hidden_states = self.nonlinearity(hidden_states)
        if self.upsample is not None:
            if hidden_states.shape[0] >= 64:
                input_tensor = input_tensor.contiguous()
                hidden_states = hidden_states.contiguous()
            input_tensor = self.upsample(input_tensor)
            hidden_states = self.upsample(hidden_states)
        elif self.downsample is not None:
            input_tensor = self.downsample(input_tensor)
            hidden_states = self.downsample(hidden_states)
        hidden_states = self.conv1(hidden_states)
        hidden_states = self.norm2(hidden_states, temb)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)
        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)
        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor
        return output_tensor


class UNetMidBlock2D(nn.Module):
    """
    A 2D UNet mid-block [`UNetMidBlock2D`] with multiple residual blocks and optional attention blocks.

    Args:
        in_channels (`int`): The number of input channels.
        temb_channels (`int`): The number of temporal embedding channels.
        dropout (`float`, *optional*, defaults to 0.0): The dropout rate.
        num_layers (`int`, *optional*, defaults to 1): The number of residual blocks.
        resnet_eps (`float`, *optional*, 1e-6 ): The epsilon value for the resnet blocks.
        resnet_time_scale_shift (`str`, *optional*, defaults to `default`):
            The type of normalization to apply to the time embeddings. This can help to improve the performance of the
            model on tasks with long-range temporal dependencies.
        resnet_act_fn (`str`, *optional*, defaults to `swish`): The activation function for the resnet blocks.
        resnet_groups (`int`, *optional*, defaults to 32):
            The number of groups to use in the group normalization layers of the resnet blocks.
        attn_groups (`Optional[int]`, *optional*, defaults to None): The number of groups for the attention blocks.
        resnet_pre_norm (`bool`, *optional*, defaults to `True`):
            Whether to use pre-normalization for the resnet blocks.
        add_attention (`bool`, *optional*, defaults to `True`): Whether to add attention blocks.
        attention_head_dim (`int`, *optional*, defaults to 1):
            Dimension of a single attention head. The number of attention heads is determined based on this value and
            the number of input channels.
        output_scale_factor (`float`, *optional*, defaults to 1.0): The output scale factor.

    Returns:
        `torch.Tensor`: The output of the last residual block, which is a tensor of shape `(batch_size, in_channels,
        height, width)`.

    """

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, attn_groups: 'Optional[int]'=None, resnet_pre_norm: 'bool'=True, add_attention: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0):
        super().__init__()
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.add_attention = add_attention
        if attn_groups is None:
            attn_groups = resnet_groups if resnet_time_scale_shift == 'default' else None
        if resnet_time_scale_shift == 'spatial':
            resnets = [ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor)]
        else:
            resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {in_channels}.')
            attention_head_dim = in_channels
        for _ in range(num_layers):
            if self.add_attention:
                attentions.append(Attention(in_channels, heads=in_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=attn_groups, spatial_norm_dim=temb_channels if resnet_time_scale_shift == 'spatial' else None, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
            else:
                attentions.append(None)
            if resnet_time_scale_shift == 'spatial':
                resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor))
            else:
                resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            if attn is not None:
                hidden_states = attn(hidden_states, temb=temb)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class DualTransformer2DModel(nn.Module):
    """
    Dual transformer wrapper that combines two `Transformer2DModel`s for mixed inference.

    Parameters:
        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.
        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.
        in_channels (`int`, *optional*):
            Pass if the input is continuous. The number of channels in the input and output.
        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.
        dropout (`float`, *optional*, defaults to 0.1): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The number of encoder_hidden_states dimensions to use.
        sample_size (`int`, *optional*): Pass if the input is discrete. The width of the latent images.
            Note that this is fixed at training time as it is used for learning a number of position embeddings. See
            `ImagePositionalEmbeddings`.
        num_vector_embeds (`int`, *optional*):
            Pass if the input is discrete. The number of classes of the vector embeddings of the latent pixels.
            Includes the class for the masked latent pixel.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        num_embeds_ada_norm ( `int`, *optional*): Pass if at least one of the norm_layers is `AdaLayerNorm`.
            The number of diffusion steps used during training. Note that this is fixed at training time as it is used
            to learn a number of embeddings that are added to the hidden states. During inference, you can denoise for
            up to but not more than steps than `num_embeds_ada_norm`.
        attention_bias (`bool`, *optional*):
            Configure if the TransformerBlocks' attention should contain a bias parameter.
    """

    def __init__(self, num_attention_heads: 'int'=16, attention_head_dim: 'int'=88, in_channels: 'Optional[int]'=None, num_layers: 'int'=1, dropout: 'float'=0.0, norm_num_groups: 'int'=32, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, sample_size: 'Optional[int]'=None, num_vector_embeds: 'Optional[int]'=None, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None):
        super().__init__()
        self.transformers = nn.ModuleList([Transformer2DModel(num_attention_heads=num_attention_heads, attention_head_dim=attention_head_dim, in_channels=in_channels, num_layers=num_layers, dropout=dropout, norm_num_groups=norm_num_groups, cross_attention_dim=cross_attention_dim, attention_bias=attention_bias, sample_size=sample_size, num_vector_embeds=num_vector_embeds, activation_fn=activation_fn, num_embeds_ada_norm=num_embeds_ada_norm) for _ in range(2)])
        self.mix_ratio = 0.5
        self.condition_lengths = [77, 257]
        self.transformer_index_for_condition = [1, 0]

    def forward(self, hidden_states, encoder_hidden_states, timestep=None, attention_mask=None, cross_attention_kwargs=None, return_dict: 'bool'=True):
        """
        Args:
            hidden_states ( When discrete, `torch.LongTensor` of shape `(batch size, num latent pixels)`.
                When continuous, `torch.Tensor` of shape `(batch size, channel, height, width)`): Input hidden_states.
            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):
                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
                self-attention.
            timestep ( `torch.long`, *optional*):
                Optional timestep to be applied as an embedding in AdaLayerNorm's. Used to indicate denoising step.
            attention_mask (`torch.Tensor`, *optional*):
                Optional attention mask to be applied in Attention.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`models.unets.unet_2d_condition.UNet2DConditionOutput`] instead of a plain
                tuple.

        Returns:
            [`~models.transformers.transformer_2d.Transformer2DModelOutput`] or `tuple`:
            [`~models.transformers.transformer_2d.Transformer2DModelOutput`] if `return_dict` is True, otherwise a
            `tuple`. When returning a tuple, the first element is the sample tensor.
        """
        input_states = hidden_states
        encoded_states = []
        tokens_start = 0
        for i in range(2):
            condition_state = encoder_hidden_states[:, tokens_start:tokens_start + self.condition_lengths[i]]
            transformer_index = self.transformer_index_for_condition[i]
            encoded_state = self.transformers[transformer_index](input_states, encoder_hidden_states=condition_state, timestep=timestep, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            encoded_states.append(encoded_state - input_states)
            tokens_start += self.condition_lengths[i]
        output_states = encoded_states[0] * self.mix_ratio + encoded_states[1] * (1 - self.mix_ratio)
        output_states = output_states + input_states
        if not return_dict:
            return output_states,
        return Transformer2DModelOutput(sample=output_states)


class LinearMultiDim(nn.Linear):

    def __init__(self, in_features, out_features=None, second_dim=4, *args, **kwargs):
        in_features = [in_features, second_dim, 1] if isinstance(in_features, int) else list(in_features)
        if out_features is None:
            out_features = in_features
        out_features = [out_features, second_dim, 1] if isinstance(out_features, int) else list(out_features)
        self.in_features_multidim = in_features
        self.out_features_multidim = out_features
        super().__init__(np.array(in_features).prod(), np.array(out_features).prod())

    def forward(self, input_tensor, *args, **kwargs):
        shape = input_tensor.shape
        n_dim = len(self.in_features_multidim)
        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_features)
        output_tensor = super().forward(input_tensor)
        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_features_multidim)
        return output_tensor


class ResnetBlockFlat(nn.Module):

    def __init__(self, *, in_channels, out_channels=None, dropout=0.0, temb_channels=512, groups=32, groups_out=None, pre_norm=True, eps=1e-06, time_embedding_norm='default', use_in_shortcut=None, second_dim=4, **kwargs):
        super().__init__()
        self.pre_norm = pre_norm
        self.pre_norm = True
        in_channels = [in_channels, second_dim, 1] if isinstance(in_channels, int) else list(in_channels)
        self.in_channels_prod = np.array(in_channels).prod()
        self.channels_multidim = in_channels
        if out_channels is not None:
            out_channels = [out_channels, second_dim, 1] if isinstance(out_channels, int) else list(out_channels)
            out_channels_prod = np.array(out_channels).prod()
            self.out_channels_multidim = out_channels
        else:
            out_channels_prod = self.in_channels_prod
            self.out_channels_multidim = self.channels_multidim
        self.time_embedding_norm = time_embedding_norm
        if groups_out is None:
            groups_out = groups
        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=self.in_channels_prod, eps=eps, affine=True)
        self.conv1 = torch.nn.Conv2d(self.in_channels_prod, out_channels_prod, kernel_size=1, padding=0)
        if temb_channels is not None:
            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels_prod)
        else:
            self.time_emb_proj = None
        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels_prod, eps=eps, affine=True)
        self.dropout = torch.nn.Dropout(dropout)
        self.conv2 = torch.nn.Conv2d(out_channels_prod, out_channels_prod, kernel_size=1, padding=0)
        self.nonlinearity = nn.SiLU()
        self.use_in_shortcut = self.in_channels_prod != out_channels_prod if use_in_shortcut is None else use_in_shortcut
        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = torch.nn.Conv2d(self.in_channels_prod, out_channels_prod, kernel_size=1, stride=1, padding=0)

    def forward(self, input_tensor, temb):
        shape = input_tensor.shape
        n_dim = len(self.channels_multidim)
        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_channels_prod, 1, 1)
        input_tensor = input_tensor.view(-1, self.in_channels_prod, 1, 1)
        hidden_states = input_tensor
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.conv1(hidden_states)
        if temb is not None:
            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]
            hidden_states = hidden_states + temb
        hidden_states = self.norm2(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)
        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)
        output_tensor = input_tensor + hidden_states
        output_tensor = output_tensor.view(*shape[0:-n_dim], -1)
        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_channels_multidim)
        return output_tensor


class CrossAttnDownBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, downsample_padding: 'int'=1, add_downsample: 'bool'=True, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False, attention_type: 'str'='default'):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, attention_type=attention_type))
            else:
                attentions.append(DualTransformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, additional_residuals: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        output_states = ()
        blocks = list(zip(self.resnets, self.attentions))
        for i, (resnet, attn) in enumerate(blocks):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            if i == len(blocks) - 1 and additional_residuals is not None:
                hidden_states = hidden_states + additional_residuals
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class DownBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, downsample_padding: 'int'=1):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


def get_down_block(down_block_type, num_layers, in_channels, out_channels, temb_channels, add_downsample, resnet_eps, resnet_act_fn, num_attention_heads, transformer_layers_per_block, attention_type, attention_head_dim, resnet_groups=None, cross_attention_dim=None, downsample_padding=None, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False, resnet_time_scale_shift='default', resnet_skip_time_act=False, resnet_out_scale_factor=1.0, cross_attention_norm=None, dropout=0.0):
    down_block_type = down_block_type[7:] if down_block_type.startswith('UNetRes') else down_block_type
    if down_block_type == 'DownBlockFlat':
        return DownBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, dropout=dropout, add_downsample=add_downsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, downsample_padding=downsample_padding, resnet_time_scale_shift=resnet_time_scale_shift)
    elif down_block_type == 'CrossAttnDownBlockFlat':
        if cross_attention_dim is None:
            raise ValueError('cross_attention_dim must be specified for CrossAttnDownBlockFlat')
        return CrossAttnDownBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, dropout=dropout, add_downsample=add_downsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, downsample_padding=downsample_padding, cross_attention_dim=cross_attention_dim, num_attention_heads=num_attention_heads, dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, resnet_time_scale_shift=resnet_time_scale_shift)
    raise ValueError(f'{down_block_type} is not supported.')


class Encoder(nn.Module):
    """
    The `Encoder` layer of a variational autoencoder that encodes its input into a latent representation.

    Args:
        in_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_channels (`int`, *optional*, defaults to 3):
            The number of output channels.
        down_block_types (`Tuple[str, ...]`, *optional*, defaults to `("DownEncoderBlock2D",)`):
            The types of down blocks to use. See `~diffusers.models.unet_2d_blocks.get_down_block` for available
            options.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(64,)`):
            The number of output channels for each block.
        layers_per_block (`int`, *optional*, defaults to 2):
            The number of layers per block.
        norm_num_groups (`int`, *optional*, defaults to 32):
            The number of groups for normalization.
        act_fn (`str`, *optional*, defaults to `"silu"`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
        double_z (`bool`, *optional*, defaults to `True`):
            Whether to double the number of output channels for the last block.
    """

    def __init__(self, in_channels: 'int'=3, out_channels: 'int'=3, down_block_types: 'Tuple[str, ...]'=('DownEncoderBlock2D',), block_out_channels: 'Tuple[int, ...]'=(64,), layers_per_block: 'int'=2, norm_num_groups: 'int'=32, act_fn: 'str'='silu', double_z: 'bool'=True, mid_block_add_attention=True):
        super().__init__()
        self.layers_per_block = layers_per_block
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)
        self.down_blocks = nn.ModuleList([])
        output_channel = block_out_channels[0]
        for i, down_block_type in enumerate(down_block_types):
            input_channel = output_channel
            output_channel = block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            down_block = get_down_block(down_block_type, num_layers=self.layers_per_block, in_channels=input_channel, out_channels=output_channel, add_downsample=not is_final_block, resnet_eps=1e-06, downsample_padding=0, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, attention_head_dim=output_channel, temb_channels=None)
            self.down_blocks.append(down_block)
        self.mid_block = UNetMidBlock2D(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default', attention_head_dim=block_out_channels[-1], resnet_groups=norm_num_groups, temb_channels=None, add_attention=mid_block_add_attention)
        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        conv_out_channels = 2 * out_channels if double_z else out_channels
        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor') ->torch.Tensor:
        """The forward method of the `Encoder` class."""
        sample = self.conv_in(sample)
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                for down_block in self.down_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), sample, use_reentrant=False)
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, use_reentrant=False)
            else:
                for down_block in self.down_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), sample)
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample)
        else:
            for down_block in self.down_blocks:
                sample = down_block(sample)
            sample = self.mid_block(sample)
        sample = self.conv_norm_out(sample)
        sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        return sample


def fourier_filter(x_in: "'torch.Tensor'", threshold: 'int', scale: 'int') ->'torch.Tensor':
    """Fourier filter as introduced in FreeU (https://arxiv.org/abs/2309.11497).

    This version of the method comes from here:
    https://github.com/huggingface/diffusers/pull/5164#issuecomment-1732638706
    """
    x = x_in
    B, C, H, W = x.shape
    if W & W - 1 != 0 or H & H - 1 != 0:
        x = x
    x_freq = fftn(x, dim=(-2, -1))
    x_freq = fftshift(x_freq, dim=(-2, -1))
    B, C, H, W = x_freq.shape
    mask = torch.ones((B, C, H, W), device=x.device)
    crow, ccol = H // 2, W // 2
    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale
    x_freq = x_freq * mask
    x_freq = ifftshift(x_freq, dim=(-2, -1))
    x_filtered = ifftn(x_freq, dim=(-2, -1)).real
    return x_filtered


def apply_freeu(resolution_idx: 'int', hidden_states: "'torch.Tensor'", res_hidden_states: "'torch.Tensor'", **freeu_kwargs) ->Tuple['torch.Tensor', 'torch.Tensor']:
    """Applies the FreeU mechanism as introduced in https:
    //arxiv.org/abs/2309.11497. Adapted from the official code repository: https://github.com/ChenyangSi/FreeU.

    Args:
        resolution_idx (`int`): Integer denoting the UNet block where FreeU is being applied.
        hidden_states (`torch.Tensor`): Inputs to the underlying block.
        res_hidden_states (`torch.Tensor`): Features from the skip block corresponding to the underlying block.
        s1 (`float`): Scaling factor for stage 1 to attenuate the contributions of the skip features.
        s2 (`float`): Scaling factor for stage 2 to attenuate the contributions of the skip features.
        b1 (`float`): Scaling factor for stage 1 to amplify the contributions of backbone features.
        b2 (`float`): Scaling factor for stage 2 to amplify the contributions of backbone features.
    """
    if resolution_idx == 0:
        num_half_channels = hidden_states.shape[1] // 2
        hidden_states[:, :num_half_channels] = hidden_states[:, :num_half_channels] * freeu_kwargs['b1']
        res_hidden_states = fourier_filter(res_hidden_states, threshold=1, scale=freeu_kwargs['s1'])
    if resolution_idx == 1:
        num_half_channels = hidden_states.shape[1] // 2
        hidden_states[:, :num_half_channels] = hidden_states[:, :num_half_channels] * freeu_kwargs['b2']
        res_hidden_states = fourier_filter(res_hidden_states, threshold=1, scale=freeu_kwargs['s2'])
    return hidden_states, res_hidden_states


class CrossAttnUpBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False, attention_type: 'str'='default'):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, attention_type=attention_type))
            else:
                attentions.append(DualTransformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


def get_up_block(up_block_type, num_layers, in_channels, out_channels, prev_output_channel, temb_channels, add_upsample, resnet_eps, resnet_act_fn, num_attention_heads, transformer_layers_per_block, resolution_idx, attention_type, attention_head_dim, resnet_groups=None, cross_attention_dim=None, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False, resnet_time_scale_shift='default', resnet_skip_time_act=False, resnet_out_scale_factor=1.0, cross_attention_norm=None, dropout=0.0):
    up_block_type = up_block_type[7:] if up_block_type.startswith('UNetRes') else up_block_type
    if up_block_type == 'UpBlockFlat':
        return UpBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, prev_output_channel=prev_output_channel, temb_channels=temb_channels, dropout=dropout, add_upsample=add_upsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, resnet_time_scale_shift=resnet_time_scale_shift)
    elif up_block_type == 'CrossAttnUpBlockFlat':
        if cross_attention_dim is None:
            raise ValueError('cross_attention_dim must be specified for CrossAttnUpBlockFlat')
        return CrossAttnUpBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, prev_output_channel=prev_output_channel, temb_channels=temb_channels, dropout=dropout, add_upsample=add_upsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, cross_attention_dim=cross_attention_dim, num_attention_heads=num_attention_heads, dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, resnet_time_scale_shift=resnet_time_scale_shift)
    raise ValueError(f'{up_block_type} is not supported.')


class Decoder(nn.Module):
    """
    The `Decoder` layer of a variational autoencoder that decodes its latent representation into an output sample.

    Args:
        in_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_channels (`int`, *optional*, defaults to 3):
            The number of output channels.
        up_block_types (`Tuple[str, ...]`, *optional*, defaults to `("UpDecoderBlock2D",)`):
            The types of up blocks to use. See `~diffusers.models.unet_2d_blocks.get_up_block` for available options.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(64,)`):
            The number of output channels for each block.
        layers_per_block (`int`, *optional*, defaults to 2):
            The number of layers per block.
        norm_num_groups (`int`, *optional*, defaults to 32):
            The number of groups for normalization.
        act_fn (`str`, *optional*, defaults to `"silu"`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
        norm_type (`str`, *optional*, defaults to `"group"`):
            The normalization type to use. Can be either `"group"` or `"spatial"`.
    """

    def __init__(self, in_channels: 'int'=3, out_channels: 'int'=3, up_block_types: 'Tuple[str, ...]'=('UpDecoderBlock2D',), block_out_channels: 'Tuple[int, ...]'=(64,), layers_per_block: 'int'=2, norm_num_groups: 'int'=32, act_fn: 'str'='silu', norm_type: 'str'='group', mid_block_add_attention=True):
        super().__init__()
        self.layers_per_block = layers_per_block
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)
        self.up_blocks = nn.ModuleList([])
        temb_channels = in_channels if norm_type == 'spatial' else None
        self.mid_block = UNetMidBlock2D(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default' if norm_type == 'group' else norm_type, attention_head_dim=block_out_channels[-1], resnet_groups=norm_num_groups, temb_channels=temb_channels, add_attention=mid_block_add_attention)
        reversed_block_out_channels = list(reversed(block_out_channels))
        output_channel = reversed_block_out_channels[0]
        for i, up_block_type in enumerate(up_block_types):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            up_block = get_up_block(up_block_type, num_layers=self.layers_per_block + 1, in_channels=prev_output_channel, out_channels=output_channel, prev_output_channel=None, add_upsample=not is_final_block, resnet_eps=1e-06, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, attention_head_dim=output_channel, temb_channels=temb_channels, resnet_time_scale_shift=norm_type)
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel
        if norm_type == 'spatial':
            self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)
        else:
            self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, sample: 'torch.Tensor', latent_embeds: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        """The forward method of the `Decoder` class."""
        sample = self.conv_in(sample)
        upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, latent_embeds, use_reentrant=False)
                sample = sample
                for up_block in self.up_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds, use_reentrant=False)
            else:
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, latent_embeds)
                sample = sample
                for up_block in self.up_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds)
        else:
            sample = self.mid_block(sample, latent_embeds)
            sample = sample
            for up_block in self.up_blocks:
                sample = up_block(sample, latent_embeds)
        if latent_embeds is None:
            sample = self.conv_norm_out(sample)
        else:
            sample = self.conv_norm_out(sample, latent_embeds)
        sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        return sample


class UpSample(nn.Module):
    """
    The `UpSample` layer of a variational autoencoder that upsamples its input.

    Args:
        in_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_channels (`int`, *optional*, defaults to 3):
            The number of output channels.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int') ->None:
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """The forward method of the `UpSample` class."""
        x = torch.relu(x)
        x = self.deconv(x)
        return x


class MaskConditionEncoder(nn.Module):
    """
    used in AsymmetricAutoencoderKL
    """

    def __init__(self, in_ch: 'int', out_ch: 'int'=192, res_ch: 'int'=768, stride: 'int'=16) ->None:
        super().__init__()
        channels = []
        while stride > 1:
            stride = stride // 2
            in_ch_ = out_ch * 2
            if out_ch > res_ch:
                out_ch = res_ch
            if stride == 1:
                in_ch_ = res_ch
            channels.append((in_ch_, out_ch))
            out_ch *= 2
        out_channels = []
        for _in_ch, _out_ch in channels:
            out_channels.append(_out_ch)
        out_channels.append(channels[-1][0])
        layers = []
        in_ch_ = in_ch
        for l in range(len(out_channels)):
            out_ch_ = out_channels[l]
            if l == 0 or l == 1:
                layers.append(nn.Conv2d(in_ch_, out_ch_, kernel_size=3, stride=1, padding=1))
            else:
                layers.append(nn.Conv2d(in_ch_, out_ch_, kernel_size=4, stride=2, padding=1))
            in_ch_ = out_ch_
        self.layers = nn.Sequential(*layers)

    def forward(self, x: 'torch.Tensor', mask=None) ->torch.Tensor:
        """The forward method of the `MaskConditionEncoder` class."""
        out = {}
        for l in range(len(self.layers)):
            layer = self.layers[l]
            x = layer(x)
            out[str(tuple(x.shape))] = x
            x = torch.relu(x)
        return out


class MaskConditionDecoder(nn.Module):
    """The `MaskConditionDecoder` should be used in combination with [`AsymmetricAutoencoderKL`] to enhance the model's
    decoder with a conditioner on the mask and masked image.

    Args:
        in_channels (`int`, *optional*, defaults to 3):
            The number of input channels.
        out_channels (`int`, *optional*, defaults to 3):
            The number of output channels.
        up_block_types (`Tuple[str, ...]`, *optional*, defaults to `("UpDecoderBlock2D",)`):
            The types of up blocks to use. See `~diffusers.models.unet_2d_blocks.get_up_block` for available options.
        block_out_channels (`Tuple[int, ...]`, *optional*, defaults to `(64,)`):
            The number of output channels for each block.
        layers_per_block (`int`, *optional*, defaults to 2):
            The number of layers per block.
        norm_num_groups (`int`, *optional*, defaults to 32):
            The number of groups for normalization.
        act_fn (`str`, *optional*, defaults to `"silu"`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
        norm_type (`str`, *optional*, defaults to `"group"`):
            The normalization type to use. Can be either `"group"` or `"spatial"`.
    """

    def __init__(self, in_channels: 'int'=3, out_channels: 'int'=3, up_block_types: 'Tuple[str, ...]'=('UpDecoderBlock2D',), block_out_channels: 'Tuple[int, ...]'=(64,), layers_per_block: 'int'=2, norm_num_groups: 'int'=32, act_fn: 'str'='silu', norm_type: 'str'='group'):
        super().__init__()
        self.layers_per_block = layers_per_block
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)
        self.up_blocks = nn.ModuleList([])
        temb_channels = in_channels if norm_type == 'spatial' else None
        self.mid_block = UNetMidBlock2D(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default' if norm_type == 'group' else norm_type, attention_head_dim=block_out_channels[-1], resnet_groups=norm_num_groups, temb_channels=temb_channels)
        reversed_block_out_channels = list(reversed(block_out_channels))
        output_channel = reversed_block_out_channels[0]
        for i, up_block_type in enumerate(up_block_types):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            up_block = get_up_block(up_block_type, num_layers=self.layers_per_block + 1, in_channels=prev_output_channel, out_channels=output_channel, prev_output_channel=None, add_upsample=not is_final_block, resnet_eps=1e-06, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, attention_head_dim=output_channel, temb_channels=temb_channels, resnet_time_scale_shift=norm_type)
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel
        self.condition_encoder = MaskConditionEncoder(in_ch=out_channels, out_ch=block_out_channels[0], res_ch=block_out_channels[-1])
        if norm_type == 'spatial':
            self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)
        else:
            self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, z: 'torch.Tensor', image: 'Optional[torch.Tensor]'=None, mask: 'Optional[torch.Tensor]'=None, latent_embeds: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        """The forward method of the `MaskConditionDecoder` class."""
        sample = z
        sample = self.conv_in(sample)
        upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, latent_embeds, use_reentrant=False)
                sample = sample
                if image is not None and mask is not None:
                    masked_image = (1 - mask) * image
                    im_x = torch.utils.checkpoint.checkpoint(create_custom_forward(self.condition_encoder), masked_image, mask, use_reentrant=False)
                for up_block in self.up_blocks:
                    if image is not None and mask is not None:
                        sample_ = im_x[str(tuple(sample.shape))]
                        mask_ = nn.functional.interpolate(mask, size=sample.shape[-2:], mode='nearest')
                        sample = sample * mask_ + sample_ * (1 - mask_)
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds, use_reentrant=False)
                if image is not None and mask is not None:
                    sample = sample * mask + im_x[str(tuple(sample.shape))] * (1 - mask)
            else:
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, latent_embeds)
                sample = sample
                if image is not None and mask is not None:
                    masked_image = (1 - mask) * image
                    im_x = torch.utils.checkpoint.checkpoint(create_custom_forward(self.condition_encoder), masked_image, mask)
                for up_block in self.up_blocks:
                    if image is not None and mask is not None:
                        sample_ = im_x[str(tuple(sample.shape))]
                        mask_ = nn.functional.interpolate(mask, size=sample.shape[-2:], mode='nearest')
                        sample = sample * mask_ + sample_ * (1 - mask_)
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds)
                if image is not None and mask is not None:
                    sample = sample * mask + im_x[str(tuple(sample.shape))] * (1 - mask)
        else:
            sample = self.mid_block(sample, latent_embeds)
            sample = sample
            if image is not None and mask is not None:
                masked_image = (1 - mask) * image
                im_x = self.condition_encoder(masked_image, mask)
            for up_block in self.up_blocks:
                if image is not None and mask is not None:
                    sample_ = im_x[str(tuple(sample.shape))]
                    mask_ = nn.functional.interpolate(mask, size=sample.shape[-2:], mode='nearest')
                    sample = sample * mask_ + sample_ * (1 - mask_)
                sample = up_block(sample, latent_embeds)
            if image is not None and mask is not None:
                sample = sample * mask + im_x[str(tuple(sample.shape))] * (1 - mask)
        if latent_embeds is None:
            sample = self.conv_norm_out(sample)
        else:
            sample = self.conv_norm_out(sample, latent_embeds)
        sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        return sample


class VectorQuantizer(nn.Module):
    """
    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly avoids costly matrix
    multiplications and allows for post-hoc remapping of indices.
    """

    def __init__(self, n_e: 'int', vq_embed_dim: 'int', beta: 'float', remap=None, unknown_index: 'str'='random', sane_index_shape: 'bool'=False, legacy: 'bool'=True):
        super().__init__()
        self.n_e = n_e
        self.vq_embed_dim = vq_embed_dim
        self.beta = beta
        self.legacy = legacy
        self.embedding = nn.Embedding(self.n_e, self.vq_embed_dim)
        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)
        self.remap = remap
        if self.remap is not None:
            self.register_buffer('used', torch.tensor(np.load(self.remap)))
            self.used: 'torch.Tensor'
            self.re_embed = self.used.shape[0]
            self.unknown_index = unknown_index
            if self.unknown_index == 'extra':
                self.unknown_index = self.re_embed
                self.re_embed = self.re_embed + 1
            None
        else:
            self.re_embed = n_e
        self.sane_index_shape = sane_index_shape

    def remap_to_used(self, inds: 'torch.LongTensor') ->torch.LongTensor:
        ishape = inds.shape
        assert len(ishape) > 1
        inds = inds.reshape(ishape[0], -1)
        used = self.used
        match = (inds[:, :, None] == used[None, None, ...]).long()
        new = match.argmax(-1)
        unknown = match.sum(2) < 1
        if self.unknown_index == 'random':
            new[unknown] = torch.randint(0, self.re_embed, size=new[unknown].shape)
        else:
            new[unknown] = self.unknown_index
        return new.reshape(ishape)

    def unmap_to_all(self, inds: 'torch.LongTensor') ->torch.LongTensor:
        ishape = inds.shape
        assert len(ishape) > 1
        inds = inds.reshape(ishape[0], -1)
        used = self.used
        if self.re_embed > self.used.shape[0]:
            inds[inds >= self.used.shape[0]] = 0
        back = torch.gather(used[None, :][inds.shape[0] * [0], :], 1, inds)
        return back.reshape(ishape)

    def forward(self, z: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor, Tuple]:
        z = z.permute(0, 2, 3, 1).contiguous()
        z_flattened = z.view(-1, self.vq_embed_dim)
        min_encoding_indices = torch.argmin(torch.cdist(z_flattened, self.embedding.weight), dim=1)
        z_q = self.embedding(min_encoding_indices).view(z.shape)
        perplexity = None
        min_encodings = None
        if not self.legacy:
            loss = self.beta * torch.mean((z_q.detach() - z) ** 2) + torch.mean((z_q - z.detach()) ** 2)
        else:
            loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean((z_q - z.detach()) ** 2)
        z_q: 'torch.Tensor' = z + (z_q - z).detach()
        z_q = z_q.permute(0, 3, 1, 2).contiguous()
        if self.remap is not None:
            min_encoding_indices = min_encoding_indices.reshape(z.shape[0], -1)
            min_encoding_indices = self.remap_to_used(min_encoding_indices)
            min_encoding_indices = min_encoding_indices.reshape(-1, 1)
        if self.sane_index_shape:
            min_encoding_indices = min_encoding_indices.reshape(z_q.shape[0], z_q.shape[2], z_q.shape[3])
        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)

    def get_codebook_entry(self, indices: 'torch.LongTensor', shape: 'Tuple[int, ...]') ->torch.Tensor:
        if self.remap is not None:
            indices = indices.reshape(shape[0], -1)
            indices = self.unmap_to_all(indices)
            indices = indices.reshape(-1)
        z_q: 'torch.Tensor' = self.embedding(indices)
        if shape is not None:
            z_q = z_q.view(shape)
            z_q = z_q.permute(0, 3, 1, 2).contiguous()
        return z_q


class AutoencoderTinyBlock(nn.Module):
    """
    Tiny Autoencoder block used in [`AutoencoderTiny`]. It is a mini residual module consisting of plain conv + ReLU
    blocks.

    Args:
        in_channels (`int`): The number of input channels.
        out_channels (`int`): The number of output channels.
        act_fn (`str`):
            ` The activation function to use. Supported values are `"swish"`, `"mish"`, `"gelu"`, and `"relu"`.

    Returns:
        `torch.Tensor`: A tensor with the same shape as the input tensor, but with the number of channels equal to
        `out_channels`.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', act_fn: 'str'):
        super().__init__()
        act_fn = get_activation(act_fn)
        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), act_fn, nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), act_fn, nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))
        self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else nn.Identity()
        self.fuse = nn.ReLU()

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        return self.fuse(self.conv(x) + self.skip(x))


class EncoderTiny(nn.Module):
    """
    The `EncoderTiny` layer is a simpler version of the `Encoder` layer.

    Args:
        in_channels (`int`):
            The number of input channels.
        out_channels (`int`):
            The number of output channels.
        num_blocks (`Tuple[int, ...]`):
            Each value of the tuple represents a Conv2d layer followed by `value` number of `AutoencoderTinyBlock`'s to
            use.
        block_out_channels (`Tuple[int, ...]`):
            The number of output channels for each block.
        act_fn (`str`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', num_blocks: 'Tuple[int, ...]', block_out_channels: 'Tuple[int, ...]', act_fn: 'str'):
        super().__init__()
        layers = []
        for i, num_block in enumerate(num_blocks):
            num_channels = block_out_channels[i]
            if i == 0:
                layers.append(nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1))
            else:
                layers.append(nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, stride=2, bias=False))
            for _ in range(num_block):
                layers.append(AutoencoderTinyBlock(num_channels, num_channels, act_fn))
        layers.append(nn.Conv2d(block_out_channels[-1], out_channels, kernel_size=3, padding=1))
        self.layers = nn.Sequential(*layers)
        self.gradient_checkpointing = False

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """The forward method of the `EncoderTiny` class."""
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                x = torch.utils.checkpoint.checkpoint(create_custom_forward(self.layers), x, use_reentrant=False)
            else:
                x = torch.utils.checkpoint.checkpoint(create_custom_forward(self.layers), x)
        else:
            x = self.layers(x.add(1).div(2))
        return x


class DecoderTiny(nn.Module):
    """
    The `DecoderTiny` layer is a simpler version of the `Decoder` layer.

    Args:
        in_channels (`int`):
            The number of input channels.
        out_channels (`int`):
            The number of output channels.
        num_blocks (`Tuple[int, ...]`):
            Each value of the tuple represents a Conv2d layer followed by `value` number of `AutoencoderTinyBlock`'s to
            use.
        block_out_channels (`Tuple[int, ...]`):
            The number of output channels for each block.
        upsampling_scaling_factor (`int`):
            The scaling factor to use for upsampling.
        act_fn (`str`):
            The activation function to use. See `~diffusers.models.activations.get_activation` for available options.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', num_blocks: 'Tuple[int, ...]', block_out_channels: 'Tuple[int, ...]', upsampling_scaling_factor: 'int', act_fn: 'str', upsample_fn: 'str'):
        super().__init__()
        layers = [nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, padding=1), get_activation(act_fn)]
        for i, num_block in enumerate(num_blocks):
            is_final_block = i == len(num_blocks) - 1
            num_channels = block_out_channels[i]
            for _ in range(num_block):
                layers.append(AutoencoderTinyBlock(num_channels, num_channels, act_fn))
            if not is_final_block:
                layers.append(nn.Upsample(scale_factor=upsampling_scaling_factor, mode=upsample_fn))
            conv_out_channel = num_channels if not is_final_block else out_channels
            layers.append(nn.Conv2d(num_channels, conv_out_channel, kernel_size=3, padding=1, bias=is_final_block))
        self.layers = nn.Sequential(*layers)
        self.gradient_checkpointing = False

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """The forward method of the `DecoderTiny` class."""
        x = torch.tanh(x / 3) * 3
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                x = torch.utils.checkpoint.checkpoint(create_custom_forward(self.layers), x, use_reentrant=False)
            else:
                x = torch.utils.checkpoint.checkpoint(create_custom_forward(self.layers), x)
        else:
            x = self.layers(x)
        return x.mul(2).sub(1)


def zero_module(module):
    for p in module.parameters():
        nn.init.zeros_(p)
    return module


class ControlNetConditioningEmbedding(nn.Module):
    """
    Quoting from https://arxiv.org/abs/2302.05543: "Stable Diffusion uses a pre-processing method similar to VQ-GAN
    [11] to convert the entire dataset of 512 × 512 images into smaller 64 × 64 “latent images” for stabilized
    training. This requires ControlNets to convert image-based conditions to 64 × 64 feature space to match the
    convolution size. We use a tiny network E(·) of four convolution layers with 4 × 4 kernels and 2 × 2 strides
    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full
    model) to encode image-space conditions ... into feature maps ..."
    """

    def __init__(self, conditioning_embedding_channels: 'int', conditioning_channels: 'int'=3, block_out_channels: 'Tuple[int, ...]'=(16, 32, 96, 256)):
        super().__init__()
        self.conv_in = nn.Conv2d(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1)
        self.blocks = nn.ModuleList([])
        for i in range(len(block_out_channels) - 1):
            channel_in = block_out_channels[i]
            channel_out = block_out_channels[i + 1]
            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))
            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1, stride=2))
        self.conv_out = zero_module(nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1))

    def forward(self, conditioning):
        embedding = self.conv_in(conditioning)
        embedding = F.silu(embedding)
        for block in self.blocks:
            embedding = block(embedding)
            embedding = F.silu(embedding)
        embedding = self.conv_out(embedding)
        return embedding


class BaseOutput(OrderedDict):
    """
    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a
    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular
    Python dictionary.

    <Tip warning={true}>

    You can't unpack a [`BaseOutput`] directly. Use the [`~utils.BaseOutput.to_tuple`] method to convert it to a tuple
    first.

    </Tip>
    """

    def __init_subclass__(cls) ->None:
        """Register subclasses as pytree nodes.

        This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with
        `static_graph=True` with modules that output `ModelOutput` subclasses.
        """
        if is_torch_available():
            import torch.utils._pytree
            if is_torch_version('<', '2.2'):
                torch.utils._pytree._register_pytree_node(cls, torch.utils._pytree._dict_flatten, lambda values, context: cls(**torch.utils._pytree._dict_unflatten(values, context)))
            else:
                torch.utils._pytree.register_pytree_node(cls, torch.utils._pytree._dict_flatten, lambda values, context: cls(**torch.utils._pytree._dict_unflatten(values, context)))

    def __post_init__(self) ->None:
        class_fields = fields(self)
        if not len(class_fields):
            raise ValueError(f'{self.__class__.__name__} has no fields.')
        first_field = getattr(self, class_fields[0].name)
        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])
        if other_fields_are_none and isinstance(first_field, dict):
            for key, value in first_field.items():
                self[key] = value
        else:
            for field in class_fields:
                v = getattr(self, field.name)
                if v is not None:
                    self[field.name] = v

    def __delitem__(self, *args, **kwargs):
        raise Exception(f'You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.')

    def setdefault(self, *args, **kwargs):
        raise Exception(f'You cannot use ``setdefault`` on a {self.__class__.__name__} instance.')

    def pop(self, *args, **kwargs):
        raise Exception(f'You cannot use ``pop`` on a {self.__class__.__name__} instance.')

    def update(self, *args, **kwargs):
        raise Exception(f'You cannot use ``update`` on a {self.__class__.__name__} instance.')

    def __getitem__(self, k: 'Any') ->Any:
        if isinstance(k, str):
            inner_dict = dict(self.items())
            return inner_dict[k]
        else:
            return self.to_tuple()[k]

    def __setattr__(self, name: 'Any', value: 'Any') ->None:
        if name in self.keys() and value is not None:
            super().__setitem__(name, value)
        super().__setattr__(name, value)

    def __setitem__(self, key, value):
        super().__setitem__(key, value)
        super().__setattr__(key, value)

    def __reduce__(self):
        if not is_dataclass(self):
            return super().__reduce__()
        callable, _args, *remaining = super().__reduce__()
        args = tuple(getattr(self, field.name) for field in fields(self))
        return callable, args, *remaining

    def to_tuple(self) ->Tuple[Any, ...]:
        """
        Convert self to a tuple containing all the attributes/keys that are not `None`.
        """
        return tuple(self[k] for k in self.keys())


FLAX_WEIGHTS_NAME = 'diffusion_flax_model.msgpack'


class PushToHubMixin:
    """
    A Mixin to push a model, scheduler, or pipeline to the Hugging Face Hub.
    """

    def _upload_folder(self, working_dir: 'Union[str, os.PathLike]', repo_id: 'str', token: 'Optional[str]'=None, commit_message: 'Optional[str]'=None, create_pr: 'bool'=False):
        """
        Uploads all files in `working_dir` to `repo_id`.
        """
        if commit_message is None:
            if 'Model' in self.__class__.__name__:
                commit_message = 'Upload model'
            elif 'Scheduler' in self.__class__.__name__:
                commit_message = 'Upload scheduler'
            else:
                commit_message = f'Upload {self.__class__.__name__}'
        logger.info(f'Uploading the files of {working_dir} to {repo_id}.')
        return upload_folder(repo_id=repo_id, folder_path=working_dir, token=token, commit_message=commit_message, create_pr=create_pr)

    def push_to_hub(self, repo_id: 'str', commit_message: 'Optional[str]'=None, private: 'Optional[bool]'=None, token: 'Optional[str]'=None, create_pr: 'bool'=False, safe_serialization: 'bool'=True, variant: 'Optional[str]'=None) ->str:
        """
        Upload model, scheduler, or pipeline files to the 🤗 Hugging Face Hub.

        Parameters:
            repo_id (`str`):
                The name of the repository you want to push your model, scheduler, or pipeline files to. It should
                contain your organization name when pushing to an organization. `repo_id` can also be a path to a local
                directory.
            commit_message (`str`, *optional*):
                Message to commit while pushing. Default to `"Upload {object}"`.
            private (`bool`, *optional*):
                Whether or not the repository created should be private.
            token (`str`, *optional*):
                The token to use as HTTP bearer authorization for remote files. The token generated when running
                `huggingface-cli login` (stored in `~/.huggingface`).
            create_pr (`bool`, *optional*, defaults to `False`):
                Whether or not to create a PR with the uploaded files or directly commit.
            safe_serialization (`bool`, *optional*, defaults to `True`):
                Whether or not to convert the model weights to the `safetensors` format.
            variant (`str`, *optional*):
                If specified, weights are saved in the format `pytorch_model.<variant>.bin`.

        Examples:

        ```python
        from diffusers import UNet2DConditionModel

        unet = UNet2DConditionModel.from_pretrained("stabilityai/stable-diffusion-2", subfolder="unet")

        # Push the `unet` to your namespace with the name "my-finetuned-unet".
        unet.push_to_hub("my-finetuned-unet")

        # Push the `unet` to an organization with the name "my-finetuned-unet".
        unet.push_to_hub("your-org/my-finetuned-unet")
        ```
        """
        repo_id = create_repo(repo_id, private=private, token=token, exist_ok=True).repo_id
        model_card = load_or_create_model_card(repo_id, token=token)
        model_card = populate_model_card(model_card)
        save_kwargs = {'safe_serialization': safe_serialization}
        if 'Scheduler' not in self.__class__.__name__:
            save_kwargs.update({'variant': variant})
        with tempfile.TemporaryDirectory() as tmpdir:
            self.save_pretrained(tmpdir, **save_kwargs)
            model_card.save(os.path.join(tmpdir, 'README.md'))
            return self._upload_folder(tmpdir, repo_id, token=token, commit_message=commit_message, create_pr=create_pr)


WEIGHTS_NAME = 'diffusion_pytorch_model.bin'


def rename_key(key):
    regex = '\\w+[.]\\d+'
    pats = re.findall(regex, key)
    for pat in pats:
        key = key.replace(pat, '_'.join(pat.split('.')))
    return key


def rename_key_and_reshape_tensor(pt_tuple_key, pt_tensor, random_flax_state_dict):
    """Rename PT weight names to corresponding Flax weight names and reshape tensor if necessary"""
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('scale',)
    if len(pt_tuple_key) > 1:
        for rename_from, rename_to in (('to_out_0', 'proj_attn'), ('to_k', 'key'), ('to_v', 'value'), ('to_q', 'query')):
            if pt_tuple_key[-2] == rename_from:
                weight_name = pt_tuple_key[-1]
                weight_name = 'kernel' if weight_name == 'weight' else weight_name
                renamed_pt_tuple_key = pt_tuple_key[:-2] + (rename_to, weight_name)
                if renamed_pt_tuple_key in random_flax_state_dict:
                    assert random_flax_state_dict[renamed_pt_tuple_key].shape == pt_tensor.T.shape
                    return renamed_pt_tuple_key, pt_tensor.T
    if any('norm' in str_ for str_ in pt_tuple_key) and pt_tuple_key[-1] == 'bias' and pt_tuple_key[:-1] + ('bias',) not in random_flax_state_dict and pt_tuple_key[:-1] + ('scale',) in random_flax_state_dict:
        renamed_pt_tuple_key = pt_tuple_key[:-1] + ('scale',)
        return renamed_pt_tuple_key, pt_tensor
    elif pt_tuple_key[-1] in ['weight', 'gamma'] and pt_tuple_key[:-1] + ('scale',) in random_flax_state_dict:
        renamed_pt_tuple_key = pt_tuple_key[:-1] + ('scale',)
        return renamed_pt_tuple_key, pt_tensor
    if pt_tuple_key[-1] == 'weight' and pt_tuple_key[:-1] + ('embedding',) in random_flax_state_dict:
        pt_tuple_key = pt_tuple_key[:-1] + ('embedding',)
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('kernel',)
    if pt_tuple_key[-1] == 'weight' and pt_tensor.ndim == 4:
        pt_tensor = pt_tensor.transpose(2, 3, 1, 0)
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('kernel',)
    if pt_tuple_key[-1] == 'weight':
        pt_tensor = pt_tensor.T
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('weight',)
    if pt_tuple_key[-1] == 'gamma':
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('bias',)
    if pt_tuple_key[-1] == 'beta':
        return renamed_pt_tuple_key, pt_tensor
    return pt_tuple_key, pt_tensor


def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model, init_key=42):
    pt_state_dict = {k: v.numpy() for k, v in pt_state_dict.items()}
    random_flax_params = flax_model.init_weights(PRNGKey(init_key))
    random_flax_state_dict = flatten_dict(random_flax_params)
    flax_state_dict = {}
    for pt_key, pt_tensor in pt_state_dict.items():
        renamed_pt_key = rename_key(pt_key)
        pt_tuple_key = tuple(renamed_pt_key.split('.'))
        flax_key, flax_tensor = rename_key_and_reshape_tensor(pt_tuple_key, pt_tensor, random_flax_state_dict)
        if flax_key in random_flax_state_dict:
            if flax_tensor.shape != random_flax_state_dict[flax_key].shape:
                raise ValueError(f'PyTorch checkpoint seems to be incorrect. Weight {pt_key} was expected to be of shape {random_flax_state_dict[flax_key].shape}, but is {flax_tensor.shape}.')
        flax_state_dict[flax_key] = jnp.asarray(flax_tensor)
    return unflatten_dict(flax_state_dict)


SAFETENSORS_FILE_EXTENSION = 'safetensors'


def load_state_dict(checkpoint_file: 'Union[str, os.PathLike]', variant: 'Optional[str]'=None):
    """
    Reads a checkpoint file, returning properly formatted errors if they arise.
    """
    if isinstance(checkpoint_file, dict):
        return checkpoint_file
    try:
        file_extension = os.path.basename(checkpoint_file).split('.')[-1]
        if file_extension == SAFETENSORS_FILE_EXTENSION:
            return safetensors.torch.load_file(checkpoint_file, device='cpu')
        else:
            weights_only_kwarg = {'weights_only': True} if is_torch_version('>=', '1.13') else {}
            return torch.load(checkpoint_file, map_location='cpu', **weights_only_kwarg)
    except Exception as e:
        try:
            with open(checkpoint_file) as f:
                if f.read().startswith('version'):
                    raise OSError('You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run `git lfs install` followed by `git lfs pull` in the folder you cloned.')
                else:
                    raise ValueError(f'Unable to locate the file {checkpoint_file} which is necessary to load this pretrained model. Make sure you have saved the model properly.') from e
        except (UnicodeDecodeError, ValueError):
            raise OSError(f"Unable to load weights from checkpoint file for '{checkpoint_file}' at '{checkpoint_file}'. ")


class SparseControlNetConditioningEmbedding(nn.Module):

    def __init__(self, conditioning_embedding_channels: 'int', conditioning_channels: 'int'=3, block_out_channels: 'Tuple[int, ...]'=(16, 32, 96, 256)):
        super().__init__()
        self.conv_in = nn.Conv2d(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1)
        self.blocks = nn.ModuleList([])
        for i in range(len(block_out_channels) - 1):
            channel_in = block_out_channels[i]
            channel_out = block_out_channels[i + 1]
            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))
            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1, stride=2))
        self.conv_out = zero_module(nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1))

    def forward(self, conditioning: 'torch.Tensor') ->torch.Tensor:
        embedding = self.conv_in(conditioning)
        embedding = F.silu(embedding)
        for block in self.blocks:
            embedding = block(embedding)
            embedding = F.silu(embedding)
        embedding = self.conv_out(embedding)
        return embedding


class DownBlockControlNetXSAdapter(nn.Module):
    """Components that together with corresponding components from the base model will form a
    `ControlNetXSCrossAttnDownBlock2D`"""

    def __init__(self, resnets: 'nn.ModuleList', base_to_ctrl: 'nn.ModuleList', ctrl_to_base: 'nn.ModuleList', attentions: 'Optional[nn.ModuleList]'=None, downsampler: 'Optional[nn.Conv2d]'=None):
        super().__init__()
        self.resnets = resnets
        self.base_to_ctrl = base_to_ctrl
        self.ctrl_to_base = ctrl_to_base
        self.attentions = attentions
        self.downsamplers = downsampler


class MidBlockControlNetXSAdapter(nn.Module):
    """Components that together with corresponding components from the base model will form a
    `ControlNetXSCrossAttnMidBlock2D`"""

    def __init__(self, midblock: 'UNetMidBlock2DCrossAttn', base_to_ctrl: 'nn.ModuleList', ctrl_to_base: 'nn.ModuleList'):
        super().__init__()
        self.midblock = midblock
        self.base_to_ctrl = base_to_ctrl
        self.ctrl_to_base = ctrl_to_base


class UpBlockControlNetXSAdapter(nn.Module):
    """Components that together with corresponding components from the base model will form a `ControlNetXSCrossAttnUpBlock2D`"""

    def __init__(self, ctrl_to_base: 'nn.ModuleList'):
        super().__init__()
        self.ctrl_to_base = ctrl_to_base


def find_largest_factor(number, max_factor):
    factor = max_factor
    if factor >= number:
        return number
    while factor != 0:
        residual = number % factor
        if residual == 0:
            return factor
        factor -= 1


def make_zero_conv(in_channels, out_channels=None):
    return zero_module(nn.Conv2d(in_channels, out_channels, 1, padding=0))


class ControlNetXSCrossAttnDownBlock2D(nn.Module):

    def __init__(self, base_in_channels: 'int', base_out_channels: 'int', ctrl_in_channels: 'int', ctrl_out_channels: 'int', temb_channels: 'int', norm_num_groups: 'int'=32, ctrl_max_norm_num_groups: 'int'=32, has_crossattn=True, transformer_layers_per_block: 'Optional[Union[int, Tuple[int]]]'=1, base_num_attention_heads: 'Optional[int]'=1, ctrl_num_attention_heads: 'Optional[int]'=1, cross_attention_dim: 'Optional[int]'=1024, add_downsample: 'bool'=True, upcast_attention: 'Optional[bool]'=False, use_linear_projection: 'Optional[bool]'=True):
        super().__init__()
        base_resnets = []
        base_attentions = []
        ctrl_resnets = []
        ctrl_attentions = []
        ctrl_to_base = []
        base_to_ctrl = []
        num_layers = 2
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        for i in range(num_layers):
            base_in_channels = base_in_channels if i == 0 else base_out_channels
            ctrl_in_channels = ctrl_in_channels if i == 0 else ctrl_out_channels
            base_to_ctrl.append(make_zero_conv(base_in_channels, base_in_channels))
            base_resnets.append(ResnetBlock2D(in_channels=base_in_channels, out_channels=base_out_channels, temb_channels=temb_channels, groups=norm_num_groups))
            ctrl_resnets.append(ResnetBlock2D(in_channels=ctrl_in_channels + base_in_channels, out_channels=ctrl_out_channels, temb_channels=temb_channels, groups=find_largest_factor(ctrl_in_channels + base_in_channels, max_factor=ctrl_max_norm_num_groups), groups_out=find_largest_factor(ctrl_out_channels, max_factor=ctrl_max_norm_num_groups), eps=1e-05))
            if has_crossattn:
                base_attentions.append(Transformer2DModel(base_num_attention_heads, base_out_channels // base_num_attention_heads, in_channels=base_out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention, norm_num_groups=norm_num_groups))
                ctrl_attentions.append(Transformer2DModel(ctrl_num_attention_heads, ctrl_out_channels // ctrl_num_attention_heads, in_channels=ctrl_out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention, norm_num_groups=find_largest_factor(ctrl_out_channels, max_factor=ctrl_max_norm_num_groups)))
            ctrl_to_base.append(make_zero_conv(ctrl_out_channels, base_out_channels))
        if add_downsample:
            base_to_ctrl.append(make_zero_conv(base_out_channels, base_out_channels))
            self.base_downsamplers = Downsample2D(base_out_channels, use_conv=True, out_channels=base_out_channels, name='op')
            self.ctrl_downsamplers = Downsample2D(ctrl_out_channels + base_out_channels, use_conv=True, out_channels=ctrl_out_channels, name='op')
            ctrl_to_base.append(make_zero_conv(ctrl_out_channels, base_out_channels))
        else:
            self.base_downsamplers = None
            self.ctrl_downsamplers = None
        self.base_resnets = nn.ModuleList(base_resnets)
        self.ctrl_resnets = nn.ModuleList(ctrl_resnets)
        self.base_attentions = nn.ModuleList(base_attentions) if has_crossattn else [None] * num_layers
        self.ctrl_attentions = nn.ModuleList(ctrl_attentions) if has_crossattn else [None] * num_layers
        self.base_to_ctrl = nn.ModuleList(base_to_ctrl)
        self.ctrl_to_base = nn.ModuleList(ctrl_to_base)
        self.gradient_checkpointing = False

    @classmethod
    def from_modules(cls, base_downblock: 'CrossAttnDownBlock2D', ctrl_downblock: 'DownBlockControlNetXSAdapter'):

        def get_first_cross_attention(block):
            return block.attentions[0].transformer_blocks[0].attn2
        base_in_channels = base_downblock.resnets[0].in_channels
        base_out_channels = base_downblock.resnets[0].out_channels
        ctrl_in_channels = ctrl_downblock.resnets[0].in_channels - base_in_channels
        ctrl_out_channels = ctrl_downblock.resnets[0].out_channels
        temb_channels = base_downblock.resnets[0].time_emb_proj.in_features
        num_groups = base_downblock.resnets[0].norm1.num_groups
        ctrl_num_groups = ctrl_downblock.resnets[0].norm1.num_groups
        if hasattr(base_downblock, 'attentions'):
            has_crossattn = True
            transformer_layers_per_block = len(base_downblock.attentions[0].transformer_blocks)
            base_num_attention_heads = get_first_cross_attention(base_downblock).heads
            ctrl_num_attention_heads = get_first_cross_attention(ctrl_downblock).heads
            cross_attention_dim = get_first_cross_attention(base_downblock).cross_attention_dim
            upcast_attention = get_first_cross_attention(base_downblock).upcast_attention
            use_linear_projection = base_downblock.attentions[0].use_linear_projection
        else:
            has_crossattn = False
            transformer_layers_per_block = None
            base_num_attention_heads = None
            ctrl_num_attention_heads = None
            cross_attention_dim = None
            upcast_attention = None
            use_linear_projection = None
        add_downsample = base_downblock.downsamplers is not None
        model = cls(base_in_channels=base_in_channels, base_out_channels=base_out_channels, ctrl_in_channels=ctrl_in_channels, ctrl_out_channels=ctrl_out_channels, temb_channels=temb_channels, norm_num_groups=num_groups, ctrl_max_norm_num_groups=ctrl_num_groups, has_crossattn=has_crossattn, transformer_layers_per_block=transformer_layers_per_block, base_num_attention_heads=base_num_attention_heads, ctrl_num_attention_heads=ctrl_num_attention_heads, cross_attention_dim=cross_attention_dim, add_downsample=add_downsample, upcast_attention=upcast_attention, use_linear_projection=use_linear_projection)
        model.base_resnets.load_state_dict(base_downblock.resnets.state_dict())
        model.ctrl_resnets.load_state_dict(ctrl_downblock.resnets.state_dict())
        if has_crossattn:
            model.base_attentions.load_state_dict(base_downblock.attentions.state_dict())
            model.ctrl_attentions.load_state_dict(ctrl_downblock.attentions.state_dict())
        if add_downsample:
            model.base_downsamplers.load_state_dict(base_downblock.downsamplers[0].state_dict())
            model.ctrl_downsamplers.load_state_dict(ctrl_downblock.downsamplers.state_dict())
        model.base_to_ctrl.load_state_dict(ctrl_downblock.base_to_ctrl.state_dict())
        model.ctrl_to_base.load_state_dict(ctrl_downblock.ctrl_to_base.state_dict())
        return model

    def freeze_base_params(self) ->None:
        """Freeze the weights of the parts belonging to the base UNet2DConditionModel, and leave everything else unfrozen for fine
        tuning."""
        for param in self.parameters():
            param.requires_grad = True
        base_parts = [self.base_resnets]
        if isinstance(self.base_attentions, nn.ModuleList):
            base_parts.append(self.base_attentions)
        if self.base_downsamplers is not None:
            base_parts.append(self.base_downsamplers)
        for part in base_parts:
            for param in part.parameters():
                param.requires_grad = False

    def forward(self, hidden_states_base: 'Tensor', temb: 'Tensor', encoder_hidden_states: 'Optional[Tensor]'=None, hidden_states_ctrl: 'Optional[Tensor]'=None, conditioning_scale: 'Optional[float]'=1.0, attention_mask: 'Optional[Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[Tensor]'=None, apply_control: 'bool'=True) ->Tuple[Tensor, Tensor, Tuple[Tensor, ...], Tuple[Tensor, ...]]:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        h_base = hidden_states_base
        h_ctrl = hidden_states_ctrl
        base_output_states = ()
        ctrl_output_states = ()
        base_blocks = list(zip(self.base_resnets, self.base_attentions))
        ctrl_blocks = list(zip(self.ctrl_resnets, self.ctrl_attentions))

        def create_custom_forward(module, return_dict=None):

            def custom_forward(*inputs):
                if return_dict is not None:
                    return module(*inputs, return_dict=return_dict)
                else:
                    return module(*inputs)
            return custom_forward
        for (b_res, b_attn), (c_res, c_attn), b2c, c2b in zip(base_blocks, ctrl_blocks, self.base_to_ctrl, self.ctrl_to_base):
            if apply_control:
                h_ctrl = torch.cat([h_ctrl, b2c(h_base)], dim=1)
            if self.training and self.gradient_checkpointing:
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                h_base = torch.utils.checkpoint.checkpoint(create_custom_forward(b_res), h_base, temb, **ckpt_kwargs)
            else:
                h_base = b_res(h_base, temb)
            if b_attn is not None:
                h_base = b_attn(h_base, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            if apply_control:
                if self.training and self.gradient_checkpointing:
                    ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                    h_ctrl = torch.utils.checkpoint.checkpoint(create_custom_forward(c_res), h_ctrl, temb, **ckpt_kwargs)
                else:
                    h_ctrl = c_res(h_ctrl, temb)
                if c_attn is not None:
                    h_ctrl = c_attn(h_ctrl, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            if apply_control:
                h_base = h_base + c2b(h_ctrl) * conditioning_scale
            base_output_states = base_output_states + (h_base,)
            ctrl_output_states = ctrl_output_states + (h_ctrl,)
        if self.base_downsamplers is not None:
            b2c = self.base_to_ctrl[-1]
            c2b = self.ctrl_to_base[-1]
            if apply_control:
                h_ctrl = torch.cat([h_ctrl, b2c(h_base)], dim=1)
            h_base = self.base_downsamplers(h_base)
            if apply_control:
                h_ctrl = self.ctrl_downsamplers(h_ctrl)
            if apply_control:
                h_base = h_base + c2b(h_ctrl) * conditioning_scale
            base_output_states = base_output_states + (h_base,)
            ctrl_output_states = ctrl_output_states + (h_ctrl,)
        return h_base, h_ctrl, base_output_states, ctrl_output_states


class ControlNetXSCrossAttnMidBlock2D(nn.Module):

    def __init__(self, base_channels: 'int', ctrl_channels: 'int', temb_channels: 'Optional[int]'=None, norm_num_groups: 'int'=32, ctrl_max_norm_num_groups: 'int'=32, transformer_layers_per_block: 'int'=1, base_num_attention_heads: 'Optional[int]'=1, ctrl_num_attention_heads: 'Optional[int]'=1, cross_attention_dim: 'Optional[int]'=1024, upcast_attention: 'bool'=False, use_linear_projection: 'Optional[bool]'=True):
        super().__init__()
        self.base_to_ctrl = make_zero_conv(base_channels, base_channels)
        self.base_midblock = UNetMidBlock2DCrossAttn(transformer_layers_per_block=transformer_layers_per_block, in_channels=base_channels, temb_channels=temb_channels, resnet_groups=norm_num_groups, cross_attention_dim=cross_attention_dim, num_attention_heads=base_num_attention_heads, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention)
        self.ctrl_midblock = UNetMidBlock2DCrossAttn(transformer_layers_per_block=transformer_layers_per_block, in_channels=ctrl_channels + base_channels, out_channels=ctrl_channels, temb_channels=temb_channels, resnet_groups=find_largest_factor(gcd(ctrl_channels, ctrl_channels + base_channels), ctrl_max_norm_num_groups), cross_attention_dim=cross_attention_dim, num_attention_heads=ctrl_num_attention_heads, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention)
        self.ctrl_to_base = make_zero_conv(ctrl_channels, base_channels)
        self.gradient_checkpointing = False

    @classmethod
    def from_modules(cls, base_midblock: 'UNetMidBlock2DCrossAttn', ctrl_midblock: 'MidBlockControlNetXSAdapter'):
        base_to_ctrl = ctrl_midblock.base_to_ctrl
        ctrl_to_base = ctrl_midblock.ctrl_to_base
        ctrl_midblock = ctrl_midblock.midblock

        def get_first_cross_attention(midblock):
            return midblock.attentions[0].transformer_blocks[0].attn2
        base_channels = ctrl_to_base.out_channels
        ctrl_channels = ctrl_to_base.in_channels
        transformer_layers_per_block = len(base_midblock.attentions[0].transformer_blocks)
        temb_channels = base_midblock.resnets[0].time_emb_proj.in_features
        num_groups = base_midblock.resnets[0].norm1.num_groups
        ctrl_num_groups = ctrl_midblock.resnets[0].norm1.num_groups
        base_num_attention_heads = get_first_cross_attention(base_midblock).heads
        ctrl_num_attention_heads = get_first_cross_attention(ctrl_midblock).heads
        cross_attention_dim = get_first_cross_attention(base_midblock).cross_attention_dim
        upcast_attention = get_first_cross_attention(base_midblock).upcast_attention
        use_linear_projection = base_midblock.attentions[0].use_linear_projection
        model = cls(base_channels=base_channels, ctrl_channels=ctrl_channels, temb_channels=temb_channels, norm_num_groups=num_groups, ctrl_max_norm_num_groups=ctrl_num_groups, transformer_layers_per_block=transformer_layers_per_block, base_num_attention_heads=base_num_attention_heads, ctrl_num_attention_heads=ctrl_num_attention_heads, cross_attention_dim=cross_attention_dim, upcast_attention=upcast_attention, use_linear_projection=use_linear_projection)
        model.base_to_ctrl.load_state_dict(base_to_ctrl.state_dict())
        model.base_midblock.load_state_dict(base_midblock.state_dict())
        model.ctrl_midblock.load_state_dict(ctrl_midblock.state_dict())
        model.ctrl_to_base.load_state_dict(ctrl_to_base.state_dict())
        return model

    def freeze_base_params(self) ->None:
        """Freeze the weights of the parts belonging to the base UNet2DConditionModel, and leave everything else unfrozen for fine
        tuning."""
        for param in self.parameters():
            param.requires_grad = True
        for param in self.base_midblock.parameters():
            param.requires_grad = False

    def forward(self, hidden_states_base: 'Tensor', temb: 'Tensor', encoder_hidden_states: 'Tensor', hidden_states_ctrl: 'Optional[Tensor]'=None, conditioning_scale: 'Optional[float]'=1.0, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, attention_mask: 'Optional[Tensor]'=None, encoder_attention_mask: 'Optional[Tensor]'=None, apply_control: 'bool'=True) ->Tuple[Tensor, Tensor]:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        h_base = hidden_states_base
        h_ctrl = hidden_states_ctrl
        joint_args = {'temb': temb, 'encoder_hidden_states': encoder_hidden_states, 'attention_mask': attention_mask, 'cross_attention_kwargs': cross_attention_kwargs, 'encoder_attention_mask': encoder_attention_mask}
        if apply_control:
            h_ctrl = torch.cat([h_ctrl, self.base_to_ctrl(h_base)], dim=1)
        h_base = self.base_midblock(h_base, **joint_args)
        if apply_control:
            h_ctrl = self.ctrl_midblock(h_ctrl, **joint_args)
            h_base = h_base + self.ctrl_to_base(h_ctrl) * conditioning_scale
        return h_base, h_ctrl


class ControlNetXSCrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', ctrl_skip_channels: 'List[int]', temb_channels: 'int', norm_num_groups: 'int'=32, resolution_idx: 'Optional[int]'=None, has_crossattn=True, transformer_layers_per_block: 'int'=1, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1024, add_upsample: 'bool'=True, upcast_attention: 'bool'=False, use_linear_projection: 'Optional[bool]'=True):
        super().__init__()
        resnets = []
        attentions = []
        ctrl_to_base = []
        num_layers = 3
        self.has_cross_attention = has_crossattn
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            ctrl_to_base.append(make_zero_conv(ctrl_skip_channels[i], resnet_in_channels))
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, groups=norm_num_groups))
            if has_crossattn:
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention, norm_num_groups=norm_num_groups))
        self.resnets = nn.ModuleList(resnets)
        self.attentions = nn.ModuleList(attentions) if has_crossattn else [None] * num_layers
        self.ctrl_to_base = nn.ModuleList(ctrl_to_base)
        if add_upsample:
            self.upsamplers = Upsample2D(out_channels, use_conv=True, out_channels=out_channels)
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    @classmethod
    def from_modules(cls, base_upblock: 'CrossAttnUpBlock2D', ctrl_upblock: 'UpBlockControlNetXSAdapter'):
        ctrl_to_base_skip_connections = ctrl_upblock.ctrl_to_base

        def get_first_cross_attention(block):
            return block.attentions[0].transformer_blocks[0].attn2
        out_channels = base_upblock.resnets[0].out_channels
        in_channels = base_upblock.resnets[-1].in_channels - out_channels
        prev_output_channels = base_upblock.resnets[0].in_channels - out_channels
        ctrl_skip_channelss = [c.in_channels for c in ctrl_to_base_skip_connections]
        temb_channels = base_upblock.resnets[0].time_emb_proj.in_features
        num_groups = base_upblock.resnets[0].norm1.num_groups
        resolution_idx = base_upblock.resolution_idx
        if hasattr(base_upblock, 'attentions'):
            has_crossattn = True
            transformer_layers_per_block = len(base_upblock.attentions[0].transformer_blocks)
            num_attention_heads = get_first_cross_attention(base_upblock).heads
            cross_attention_dim = get_first_cross_attention(base_upblock).cross_attention_dim
            upcast_attention = get_first_cross_attention(base_upblock).upcast_attention
            use_linear_projection = base_upblock.attentions[0].use_linear_projection
        else:
            has_crossattn = False
            transformer_layers_per_block = None
            num_attention_heads = None
            cross_attention_dim = None
            upcast_attention = None
            use_linear_projection = None
        add_upsample = base_upblock.upsamplers is not None
        model = cls(in_channels=in_channels, out_channels=out_channels, prev_output_channel=prev_output_channels, ctrl_skip_channels=ctrl_skip_channelss, temb_channels=temb_channels, norm_num_groups=num_groups, resolution_idx=resolution_idx, has_crossattn=has_crossattn, transformer_layers_per_block=transformer_layers_per_block, num_attention_heads=num_attention_heads, cross_attention_dim=cross_attention_dim, add_upsample=add_upsample, upcast_attention=upcast_attention, use_linear_projection=use_linear_projection)
        model.resnets.load_state_dict(base_upblock.resnets.state_dict())
        if has_crossattn:
            model.attentions.load_state_dict(base_upblock.attentions.state_dict())
        if add_upsample:
            model.upsamplers.load_state_dict(base_upblock.upsamplers[0].state_dict())
        model.ctrl_to_base.load_state_dict(ctrl_to_base_skip_connections.state_dict())
        return model

    def freeze_base_params(self) ->None:
        """Freeze the weights of the parts belonging to the base UNet2DConditionModel, and leave everything else unfrozen for fine
        tuning."""
        for param in self.parameters():
            param.requires_grad = True
        base_parts = [self.resnets]
        if isinstance(self.attentions, nn.ModuleList):
            base_parts.append(self.attentions)
        if self.upsamplers is not None:
            base_parts.append(self.upsamplers)
        for part in base_parts:
            for param in part.parameters():
                param.requires_grad = False

    def forward(self, hidden_states: 'Tensor', res_hidden_states_tuple_base: 'Tuple[Tensor, ...]', res_hidden_states_tuple_ctrl: 'Tuple[Tensor, ...]', temb: 'Tensor', encoder_hidden_states: 'Optional[Tensor]'=None, conditioning_scale: 'Optional[float]'=1.0, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, attention_mask: 'Optional[Tensor]'=None, upsample_size: 'Optional[int]'=None, encoder_attention_mask: 'Optional[Tensor]'=None, apply_control: 'bool'=True) ->Tensor:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)

        def create_custom_forward(module, return_dict=None):

            def custom_forward(*inputs):
                if return_dict is not None:
                    return module(*inputs, return_dict=return_dict)
                else:
                    return module(*inputs)
            return custom_forward

        def maybe_apply_freeu_to_subblock(hidden_states, res_h_base):
            if is_freeu_enabled:
                return apply_freeu(self.resolution_idx, hidden_states, res_h_base, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            else:
                return hidden_states, res_h_base
        for resnet, attn, c2b, res_h_base, res_h_ctrl in zip(self.resnets, self.attentions, self.ctrl_to_base, reversed(res_hidden_states_tuple_base), reversed(res_hidden_states_tuple_ctrl)):
            if apply_control:
                hidden_states += c2b(res_h_ctrl) * conditioning_scale
            hidden_states, res_h_base = maybe_apply_freeu_to_subblock(hidden_states, res_h_base)
            hidden_states = torch.cat([hidden_states, res_h_base], dim=1)
            if self.training and self.gradient_checkpointing:
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
            else:
                hidden_states = resnet(hidden_states, temb)
            if attn is not None:
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
        if self.upsamplers is not None:
            hidden_states = self.upsamplers(hidden_states, upsample_size)
        return hidden_states


class Downsample1D(nn.Module):
    """A 1D downsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        padding (`int`, default `1`):
            padding for the convolution.
        name (`str`, default `conv`):
            name of the downsampling 1D layer.
    """

    def __init__(self, channels: 'int', use_conv: 'bool'=False, out_channels: 'Optional[int]'=None, padding: 'int'=1, name: 'str'='conv'):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.padding = padding
        stride = 2
        self.name = name
        if use_conv:
            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, stride=stride, padding=padding)
        else:
            assert self.channels == self.out_channels
            self.conv = nn.AvgPool1d(kernel_size=stride, stride=stride)

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        assert inputs.shape[1] == self.channels
        return self.conv(inputs)


class FirDownsample2D(nn.Module):
    """A 2D FIR downsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        fir_kernel (`tuple`, default `(1, 3, 3, 1)`):
            kernel for the FIR filter.
    """

    def __init__(self, channels: 'Optional[int]'=None, out_channels: 'Optional[int]'=None, use_conv: 'bool'=False, fir_kernel: 'Tuple[int, int, int, int]'=(1, 3, 3, 1)):
        super().__init__()
        out_channels = out_channels if out_channels else channels
        if use_conv:
            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.fir_kernel = fir_kernel
        self.use_conv = use_conv
        self.out_channels = out_channels

    def _downsample_2d(self, hidden_states: 'torch.Tensor', weight: 'Optional[torch.Tensor]'=None, kernel: 'Optional[torch.Tensor]'=None, factor: 'int'=2, gain: 'float'=1) ->torch.Tensor:
        """Fused `Conv2d()` followed by `downsample_2d()`.
        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more
        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of
        arbitrary order.

        Args:
            hidden_states (`torch.Tensor`):
                Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
            weight (`torch.Tensor`, *optional*):
                Weight tensor of the shape `[filterH, filterW, inChannels, outChannels]`. Grouped convolution can be
                performed by `inChannels = x.shape[0] // numGroups`.
            kernel (`torch.Tensor`, *optional*):
                FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] * factor`, which
                corresponds to average pooling.
            factor (`int`, *optional*, default to `2`):
                Integer downsampling factor.
            gain (`float`, *optional*, default to `1.0`):
                Scaling factor for signal magnitude.

        Returns:
            output (`torch.Tensor`):
                Tensor of the shape `[N, C, H // factor, W // factor]` or `[N, H // factor, W // factor, C]`, and same
                datatype as `x`.
        """
        assert isinstance(factor, int) and factor >= 1
        if kernel is None:
            kernel = [1] * factor
        kernel = torch.tensor(kernel, dtype=torch.float32)
        if kernel.ndim == 1:
            kernel = torch.outer(kernel, kernel)
        kernel /= torch.sum(kernel)
        kernel = kernel * gain
        if self.use_conv:
            _, _, convH, convW = weight.shape
            pad_value = kernel.shape[0] - factor + (convW - 1)
            stride_value = [factor, factor]
            upfirdn_input = upfirdn2d_native(hidden_states, torch.tensor(kernel, device=hidden_states.device), pad=((pad_value + 1) // 2, pad_value // 2))
            output = F.conv2d(upfirdn_input, weight, stride=stride_value, padding=0)
        else:
            pad_value = kernel.shape[0] - factor
            output = upfirdn2d_native(hidden_states, torch.tensor(kernel, device=hidden_states.device), down=factor, pad=((pad_value + 1) // 2, pad_value // 2))
        return output

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        if self.use_conv:
            downsample_input = self._downsample_2d(hidden_states, weight=self.Conv2d_0.weight, kernel=self.fir_kernel)
            hidden_states = downsample_input + self.Conv2d_0.bias.reshape(1, -1, 1, 1)
        else:
            hidden_states = self._downsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)
        return hidden_states


class KDownsample2D(nn.Module):
    """A 2D K-downsampling layer.

    Parameters:
        pad_mode (`str`, *optional*, default to `"reflect"`): the padding mode to use.
    """

    def __init__(self, pad_mode: 'str'='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]])
        self.pad = kernel_1d.shape[1] // 2 - 1
        self.register_buffer('kernel', kernel_1d.T @ kernel_1d, persistent=False)

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        inputs = F.pad(inputs, (self.pad,) * 4, self.pad_mode)
        weight = inputs.new_zeros([inputs.shape[1], inputs.shape[1], self.kernel.shape[0], self.kernel.shape[1]])
        indices = torch.arange(inputs.shape[1], device=inputs.device)
        kernel = self.kernel[None, :].expand(inputs.shape[1], -1, -1)
        weight[indices, indices] = kernel
        return F.conv2d(inputs, weight, stride=2)


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)
    """
    if embed_dim % 2 != 0:
        raise ValueError('embed_dim must be divisible by 2')
    omega = np.arange(embed_dim // 2, dtype=np.float64)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000 ** omega
    pos = pos.reshape(-1)
    out = np.einsum('m,d->md', pos, omega)
    emb_sin = np.sin(out)
    emb_cos = np.cos(out)
    emb = np.concatenate([emb_sin, emb_cos], axis=1)
    return emb


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    if embed_dim % 2 != 0:
        raise ValueError('embed_dim must be divisible by 2')
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])
    emb = np.concatenate([emb_h, emb_w], axis=1)
    return emb


def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0, interpolation_scale=1.0, base_size=16):
    """
    grid_size: int of the grid height and width return: pos_embed: [grid_size*grid_size, embed_dim] or
    [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    if isinstance(grid_size, int):
        grid_size = grid_size, grid_size
    grid_h = np.arange(grid_size[0], dtype=np.float32) / (grid_size[0] / base_size) / interpolation_scale
    grid_w = np.arange(grid_size[1], dtype=np.float32) / (grid_size[1] / base_size) / interpolation_scale
    grid = np.meshgrid(grid_w, grid_h)
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, grid_size[1], grid_size[0]])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token and extra_tokens > 0:
        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)
    return pos_embed


class PatchEmbed(nn.Module):
    """2D Image to Patch Embedding"""

    def __init__(self, height=224, width=224, patch_size=16, in_channels=3, embed_dim=768, layer_norm=False, flatten=True, bias=True, use_pos_embed=True):
        super().__init__()
        num_patches = height // patch_size * (width // patch_size)
        self.flatten = flatten
        self.layer_norm = layer_norm
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=(patch_size, patch_size), stride=patch_size, bias=bias)
        if layer_norm:
            self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-06)
        else:
            self.norm = None
        self.use_pos_embed = use_pos_embed
        if self.use_pos_embed:
            pos_embed = get_2d_sincos_pos_embed(embed_dim, int(num_patches ** 0.5))
            self.register_buffer('pos_embed', torch.from_numpy(pos_embed).float().unsqueeze(0), persistent=False)

    def forward(self, latent):
        latent = self.proj(latent)
        if self.flatten:
            latent = latent.flatten(2).transpose(1, 2)
        if self.layer_norm:
            latent = self.norm(latent)
        if self.use_pos_embed:
            return latent + self.pos_embed
        else:
            return latent


class LuminaPatchEmbed(nn.Module):
    """2D Image to Patch Embedding with support for Lumina-T2X"""

    def __init__(self, patch_size=2, in_channels=4, embed_dim=768, bias=True):
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Linear(in_features=patch_size * patch_size * in_channels, out_features=embed_dim, bias=bias)

    def forward(self, x, freqs_cis):
        """
        Patchifies and embeds the input tensor(s).

        Args:
            x (List[torch.Tensor] | torch.Tensor): The input tensor(s) to be patchified and embedded.

        Returns:
            Tuple[torch.Tensor, torch.Tensor, List[Tuple[int, int]], torch.Tensor]: A tuple containing the patchified
            and embedded tensor(s), the mask indicating the valid patches, the original image size(s), and the
            frequency tensor(s).
        """
        freqs_cis = freqs_cis
        patch_height = patch_width = self.patch_size
        batch_size, channel, height, width = x.size()
        height_tokens, width_tokens = height // patch_height, width // patch_width
        x = x.view(batch_size, channel, height_tokens, patch_height, width_tokens, patch_width).permute(0, 2, 4, 1, 3, 5)
        x = x.flatten(3)
        x = self.proj(x)
        x = x.flatten(1, 2)
        mask = torch.ones(x.shape[0], x.shape[1], dtype=torch.int32, device=x.device)
        return x, mask, [(height, width)] * batch_size, freqs_cis[:height_tokens, :width_tokens].flatten(0, 1).unsqueeze(0)


def get_3d_sincos_pos_embed(embed_dim: 'int', spatial_size: 'Union[int, Tuple[int, int]]', temporal_size: 'int', spatial_interpolation_scale: 'float'=1.0, temporal_interpolation_scale: 'float'=1.0) ->np.ndarray:
    """
    Args:
        embed_dim (`int`):
        spatial_size (`int` or `Tuple[int, int]`):
        temporal_size (`int`):
        spatial_interpolation_scale (`float`, defaults to 1.0):
        temporal_interpolation_scale (`float`, defaults to 1.0):
    """
    if embed_dim % 4 != 0:
        raise ValueError('`embed_dim` must be divisible by 4')
    if isinstance(spatial_size, int):
        spatial_size = spatial_size, spatial_size
    embed_dim_spatial = 3 * embed_dim // 4
    embed_dim_temporal = embed_dim // 4
    grid_h = np.arange(spatial_size[1], dtype=np.float32) / spatial_interpolation_scale
    grid_w = np.arange(spatial_size[0], dtype=np.float32) / spatial_interpolation_scale
    grid = np.meshgrid(grid_w, grid_h)
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, spatial_size[1], spatial_size[0]])
    pos_embed_spatial = get_2d_sincos_pos_embed_from_grid(embed_dim_spatial, grid)
    grid_t = np.arange(temporal_size, dtype=np.float32) / temporal_interpolation_scale
    pos_embed_temporal = get_1d_sincos_pos_embed_from_grid(embed_dim_temporal, grid_t)
    pos_embed_spatial = pos_embed_spatial[np.newaxis, :, :]
    pos_embed_spatial = np.repeat(pos_embed_spatial, temporal_size, axis=0)
    pos_embed_temporal = pos_embed_temporal[:, np.newaxis, :]
    pos_embed_temporal = np.repeat(pos_embed_temporal, spatial_size[0] * spatial_size[1], axis=1)
    pos_embed = np.concatenate([pos_embed_temporal, pos_embed_spatial], axis=-1)
    return pos_embed


class CogVideoXPatchEmbed(nn.Module):

    def __init__(self, patch_size: 'int'=2, in_channels: 'int'=16, embed_dim: 'int'=1920, text_embed_dim: 'int'=4096, bias: 'bool'=True, sample_width: 'int'=90, sample_height: 'int'=60, sample_frames: 'int'=49, temporal_compression_ratio: 'int'=4, max_text_seq_length: 'int'=226, spatial_interpolation_scale: 'float'=1.875, temporal_interpolation_scale: 'float'=1.0, use_positional_embeddings: 'bool'=True, use_learned_positional_embeddings: 'bool'=True) ->None:
        super().__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.sample_height = sample_height
        self.sample_width = sample_width
        self.sample_frames = sample_frames
        self.temporal_compression_ratio = temporal_compression_ratio
        self.max_text_seq_length = max_text_seq_length
        self.spatial_interpolation_scale = spatial_interpolation_scale
        self.temporal_interpolation_scale = temporal_interpolation_scale
        self.use_positional_embeddings = use_positional_embeddings
        self.use_learned_positional_embeddings = use_learned_positional_embeddings
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=(patch_size, patch_size), stride=patch_size, bias=bias)
        self.text_proj = nn.Linear(text_embed_dim, embed_dim)
        if use_positional_embeddings or use_learned_positional_embeddings:
            persistent = use_learned_positional_embeddings
            pos_embedding = self._get_positional_embeddings(sample_height, sample_width, sample_frames)
            self.register_buffer('pos_embedding', pos_embedding, persistent=persistent)

    def _get_positional_embeddings(self, sample_height: 'int', sample_width: 'int', sample_frames: 'int') ->torch.Tensor:
        post_patch_height = sample_height // self.patch_size
        post_patch_width = sample_width // self.patch_size
        post_time_compression_frames = (sample_frames - 1) // self.temporal_compression_ratio + 1
        num_patches = post_patch_height * post_patch_width * post_time_compression_frames
        pos_embedding = get_3d_sincos_pos_embed(self.embed_dim, (post_patch_width, post_patch_height), post_time_compression_frames, self.spatial_interpolation_scale, self.temporal_interpolation_scale)
        pos_embedding = torch.from_numpy(pos_embedding).flatten(0, 1)
        joint_pos_embedding = torch.zeros(1, self.max_text_seq_length + num_patches, self.embed_dim, requires_grad=False)
        joint_pos_embedding.data[:, self.max_text_seq_length:].copy_(pos_embedding)
        return joint_pos_embedding

    def forward(self, text_embeds: 'torch.Tensor', image_embeds: 'torch.Tensor'):
        """
        Args:
            text_embeds (`torch.Tensor`):
                Input text embeddings. Expected shape: (batch_size, seq_length, embedding_dim).
            image_embeds (`torch.Tensor`):
                Input image embeddings. Expected shape: (batch_size, num_frames, channels, height, width).
        """
        text_embeds = self.text_proj(text_embeds)
        batch, num_frames, channels, height, width = image_embeds.shape
        image_embeds = image_embeds.reshape(-1, channels, height, width)
        image_embeds = self.proj(image_embeds)
        image_embeds = image_embeds.view(batch, num_frames, *image_embeds.shape[1:])
        image_embeds = image_embeds.flatten(3).transpose(2, 3)
        image_embeds = image_embeds.flatten(1, 2)
        embeds = torch.cat([text_embeds, image_embeds], dim=1).contiguous()
        if self.use_positional_embeddings or self.use_learned_positional_embeddings:
            if self.use_learned_positional_embeddings and (self.sample_width != width or self.sample_height != height):
                raise ValueError("It is currently not possible to generate videos at a different resolution that the defaults. This should only be the case with 'THUDM/CogVideoX-5b-I2V'.If you think this is incorrect, please open an issue at https://github.com/huggingface/diffusers/issues.")
            pre_time_compression_frames = (num_frames - 1) * self.temporal_compression_ratio + 1
            if self.sample_height != height or self.sample_width != width or self.sample_frames != pre_time_compression_frames:
                pos_embedding = self._get_positional_embeddings(height, width, pre_time_compression_frames)
                pos_embedding = pos_embedding
            else:
                pos_embedding = self.pos_embedding
            embeds = embeds + pos_embedding
        return embeds


class CogView3PlusPatchEmbed(nn.Module):

    def __init__(self, in_channels: 'int'=16, hidden_size: 'int'=2560, patch_size: 'int'=2, text_hidden_size: 'int'=4096, pos_embed_max_size: 'int'=128):
        super().__init__()
        self.in_channels = in_channels
        self.hidden_size = hidden_size
        self.patch_size = patch_size
        self.text_hidden_size = text_hidden_size
        self.pos_embed_max_size = pos_embed_max_size
        self.proj = nn.Linear(in_channels * patch_size ** 2, hidden_size)
        self.text_proj = nn.Linear(text_hidden_size, hidden_size)
        pos_embed = get_2d_sincos_pos_embed(hidden_size, pos_embed_max_size, base_size=pos_embed_max_size)
        pos_embed = pos_embed.reshape(pos_embed_max_size, pos_embed_max_size, hidden_size)
        self.register_buffer('pos_embed', torch.from_numpy(pos_embed).float(), persistent=False)

    def forward(self, hidden_states: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor') ->torch.Tensor:
        batch_size, channel, height, width = hidden_states.shape
        if height % self.patch_size != 0 or width % self.patch_size != 0:
            raise ValueError('Height and width must be divisible by patch size')
        height = height // self.patch_size
        width = width // self.patch_size
        hidden_states = hidden_states.view(batch_size, channel, height, self.patch_size, width, self.patch_size)
        hidden_states = hidden_states.permute(0, 2, 4, 1, 3, 5).contiguous()
        hidden_states = hidden_states.view(batch_size, height * width, channel * self.patch_size * self.patch_size)
        hidden_states = self.proj(hidden_states)
        encoder_hidden_states = self.text_proj(encoder_hidden_states)
        hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)
        text_length = encoder_hidden_states.shape[1]
        image_pos_embed = self.pos_embed[:height, :width].reshape(height * width, -1)
        text_pos_embed = torch.zeros((text_length, self.hidden_size), dtype=image_pos_embed.dtype, device=image_pos_embed.device)
        pos_embed = torch.cat([text_pos_embed, image_pos_embed], dim=0)[None, ...]
        return hidden_states + pos_embed


def get_1d_rotary_pos_embed(dim: 'int', pos: 'Union[np.ndarray, int]', theta: 'float'=10000.0, use_real=False, linear_factor=1.0, ntk_factor=1.0, repeat_interleave_real=True, freqs_dtype=torch.float32):
    """
    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.

    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim' and the end
    index 'end'. The 'theta' parameter scales the frequencies. The returned tensor contains complex values in complex64
    data type.

    Args:
        dim (`int`): Dimension of the frequency tensor.
        pos (`np.ndarray` or `int`): Position indices for the frequency tensor. [S] or scalar
        theta (`float`, *optional*, defaults to 10000.0):
            Scaling factor for frequency computation. Defaults to 10000.0.
        use_real (`bool`, *optional*):
            If True, return real part and imaginary part separately. Otherwise, return complex numbers.
        linear_factor (`float`, *optional*, defaults to 1.0):
            Scaling factor for the context extrapolation. Defaults to 1.0.
        ntk_factor (`float`, *optional*, defaults to 1.0):
            Scaling factor for the NTK-Aware RoPE. Defaults to 1.0.
        repeat_interleave_real (`bool`, *optional*, defaults to `True`):
            If `True` and `use_real`, real part and imaginary part are each interleaved with themselves to reach `dim`.
            Otherwise, they are concateanted with themselves.
        freqs_dtype (`torch.float32` or `torch.float64`, *optional*, defaults to `torch.float32`):
            the dtype of the frequency tensor.
    Returns:
        `torch.Tensor`: Precomputed frequency tensor with complex exponentials. [S, D/2]
    """
    assert dim % 2 == 0
    if isinstance(pos, int):
        pos = torch.arange(pos)
    if isinstance(pos, np.ndarray):
        pos = torch.from_numpy(pos)
    theta = theta * ntk_factor
    freqs = 1.0 / theta ** (torch.arange(0, dim, 2, dtype=freqs_dtype, device=pos.device)[:dim // 2] / dim) / linear_factor
    freqs = torch.outer(pos, freqs)
    if use_real and repeat_interleave_real:
        freqs_cos = freqs.cos().repeat_interleave(2, dim=1).float()
        freqs_sin = freqs.sin().repeat_interleave(2, dim=1).float()
        return freqs_cos, freqs_sin
    elif use_real:
        freqs_cos = torch.cat([freqs.cos(), freqs.cos()], dim=-1).float()
        freqs_sin = torch.cat([freqs.sin(), freqs.sin()], dim=-1).float()
        return freqs_cos, freqs_sin
    else:
        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
        return freqs_cis


class FluxPosEmbed(nn.Module):

    def __init__(self, theta: 'int', axes_dim: 'List[int]'):
        super().__init__()
        self.theta = theta
        self.axes_dim = axes_dim

    def forward(self, ids: 'torch.Tensor') ->torch.Tensor:
        n_axes = ids.shape[-1]
        cos_out = []
        sin_out = []
        pos = ids.float()
        is_mps = ids.device.type == 'mps'
        freqs_dtype = torch.float32 if is_mps else torch.float64
        for i in range(n_axes):
            cos, sin = get_1d_rotary_pos_embed(self.axes_dim[i], pos[:, i], repeat_interleave_real=True, use_real=True, freqs_dtype=freqs_dtype)
            cos_out.append(cos)
            sin_out.append(sin)
        freqs_cos = torch.cat(cos_out, dim=-1)
        freqs_sin = torch.cat(sin_out, dim=-1)
        return freqs_cos, freqs_sin


class GaussianFourierProjection(nn.Module):
    """Gaussian Fourier embeddings for noise levels."""

    def __init__(self, embedding_size: 'int'=256, scale: 'float'=1.0, set_W_to_weight=True, log=True, flip_sin_to_cos=False):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)
        self.log = log
        self.flip_sin_to_cos = flip_sin_to_cos
        if set_W_to_weight:
            del self.weight
            self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)
            self.weight = self.W
            del self.W

    def forward(self, x):
        if self.log:
            x = torch.log(x)
        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi
        if self.flip_sin_to_cos:
            out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)
        else:
            out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
        return out


class ImagePositionalEmbeddings(nn.Module):
    """
    Converts latent image classes into vector embeddings. Sums the vector embeddings with positional embeddings for the
    height and width of the latent space.

    For more details, see figure 10 of the dall-e paper: https://arxiv.org/abs/2102.12092

    For VQ-diffusion:

    Output vector embeddings are used as input for the transformer.

    Note that the vector embeddings for the transformer are different than the vector embeddings from the VQVAE.

    Args:
        num_embed (`int`):
            Number of embeddings for the latent pixels embeddings.
        height (`int`):
            Height of the latent image i.e. the number of height embeddings.
        width (`int`):
            Width of the latent image i.e. the number of width embeddings.
        embed_dim (`int`):
            Dimension of the produced vector embeddings. Used for the latent pixel, height, and width embeddings.
    """

    def __init__(self, num_embed: 'int', height: 'int', width: 'int', embed_dim: 'int'):
        super().__init__()
        self.height = height
        self.width = width
        self.num_embed = num_embed
        self.embed_dim = embed_dim
        self.emb = nn.Embedding(self.num_embed, embed_dim)
        self.height_emb = nn.Embedding(self.height, embed_dim)
        self.width_emb = nn.Embedding(self.width, embed_dim)

    def forward(self, index):
        emb = self.emb(index)
        height_emb = self.height_emb(torch.arange(self.height, device=index.device).view(1, self.height))
        height_emb = height_emb.unsqueeze(2)
        width_emb = self.width_emb(torch.arange(self.width, device=index.device).view(1, self.width))
        width_emb = width_emb.unsqueeze(1)
        pos_emb = height_emb + width_emb
        pos_emb = pos_emb.view(1, self.height * self.width, -1)
        emb = emb + pos_emb[:, :emb.shape[1], :]
        return emb


class TextImageProjection(nn.Module):

    def __init__(self, text_embed_dim: 'int'=1024, image_embed_dim: 'int'=768, cross_attention_dim: 'int'=768, num_image_text_embeds: 'int'=10):
        super().__init__()
        self.num_image_text_embeds = num_image_text_embeds
        self.image_embeds = nn.Linear(image_embed_dim, self.num_image_text_embeds * cross_attention_dim)
        self.text_proj = nn.Linear(text_embed_dim, cross_attention_dim)

    def forward(self, text_embeds: 'torch.Tensor', image_embeds: 'torch.Tensor'):
        batch_size = text_embeds.shape[0]
        image_text_embeds = self.image_embeds(image_embeds)
        image_text_embeds = image_text_embeds.reshape(batch_size, self.num_image_text_embeds, -1)
        text_embeds = self.text_proj(text_embeds)
        return torch.cat([image_text_embeds, text_embeds], dim=1)


class ImageProjection(nn.Module):

    def __init__(self, image_embed_dim: 'int'=768, cross_attention_dim: 'int'=768, num_image_text_embeds: 'int'=32):
        super().__init__()
        self.num_image_text_embeds = num_image_text_embeds
        self.image_embeds = nn.Linear(image_embed_dim, self.num_image_text_embeds * cross_attention_dim)
        self.norm = nn.LayerNorm(cross_attention_dim)

    def forward(self, image_embeds: 'torch.Tensor'):
        batch_size = image_embeds.shape[0]
        image_embeds = self.image_embeds(image_embeds)
        image_embeds = image_embeds.reshape(batch_size, self.num_image_text_embeds, -1)
        image_embeds = self.norm(image_embeds)
        return image_embeds


class IPAdapterFaceIDImageProjection(nn.Module):

    def __init__(self, image_embed_dim=1024, cross_attention_dim=1024, mult=1, num_tokens=1):
        super().__init__()
        self.num_tokens = num_tokens
        self.cross_attention_dim = cross_attention_dim
        self.ff = FeedForward(image_embed_dim, cross_attention_dim * num_tokens, mult=mult, activation_fn='gelu')
        self.norm = nn.LayerNorm(cross_attention_dim)

    def forward(self, image_embeds: 'torch.Tensor'):
        x = self.ff(image_embeds)
        x = x.reshape(-1, self.num_tokens, self.cross_attention_dim)
        return self.norm(x)


class PixArtAlphaTextProjection(nn.Module):
    """
    Projects caption embeddings. Also handles dropout for classifier-free guidance.

    Adapted from https://github.com/PixArt-alpha/PixArt-alpha/blob/master/diffusion/model/nets/PixArt_blocks.py
    """

    def __init__(self, in_features, hidden_size, out_features=None, act_fn='gelu_tanh'):
        super().__init__()
        if out_features is None:
            out_features = hidden_size
        self.linear_1 = nn.Linear(in_features=in_features, out_features=hidden_size, bias=True)
        if act_fn == 'gelu_tanh':
            self.act_1 = nn.GELU(approximate='tanh')
        elif act_fn == 'silu':
            self.act_1 = nn.SiLU()
        elif act_fn == 'silu_fp32':
            self.act_1 = FP32SiLU()
        else:
            raise ValueError(f'Unknown activation function: {act_fn}')
        self.linear_2 = nn.Linear(in_features=hidden_size, out_features=out_features, bias=True)

    def forward(self, caption):
        hidden_states = self.linear_1(caption)
        hidden_states = self.act_1(hidden_states)
        hidden_states = self.linear_2(hidden_states)
        return hidden_states


class CombinedTimestepTextProjEmbeddings(nn.Module):

    def __init__(self, embedding_dim, pooled_projection_dim):
        super().__init__()
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn='silu')

    def forward(self, timestep, pooled_projection):
        timesteps_proj = self.time_proj(timestep)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        pooled_projections = self.text_embedder(pooled_projection)
        conditioning = timesteps_emb + pooled_projections
        return conditioning


class CombinedTimestepGuidanceTextProjEmbeddings(nn.Module):

    def __init__(self, embedding_dim, pooled_projection_dim):
        super().__init__()
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.guidance_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn='silu')

    def forward(self, timestep, guidance, pooled_projection):
        timesteps_proj = self.time_proj(timestep)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        guidance_proj = self.time_proj(guidance)
        guidance_emb = self.guidance_embedder(guidance_proj)
        time_guidance_emb = timesteps_emb + guidance_emb
        pooled_projections = self.text_embedder(pooled_projection)
        conditioning = time_guidance_emb + pooled_projections
        return conditioning


class CogView3CombinedTimestepSizeEmbeddings(nn.Module):

    def __init__(self, embedding_dim: 'int', condition_dim: 'int', pooled_projection_dim: 'int', timesteps_dim: 'int'=256):
        super().__init__()
        self.time_proj = Timesteps(num_channels=timesteps_dim, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.condition_proj = Timesteps(num_channels=condition_dim, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=timesteps_dim, time_embed_dim=embedding_dim)
        self.condition_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn='silu')

    def forward(self, timestep: 'torch.Tensor', original_size: 'torch.Tensor', target_size: 'torch.Tensor', crop_coords: 'torch.Tensor', hidden_dtype: 'torch.dtype') ->torch.Tensor:
        timesteps_proj = self.time_proj(timestep)
        original_size_proj = self.condition_proj(original_size.flatten()).view(original_size.size(0), -1)
        crop_coords_proj = self.condition_proj(crop_coords.flatten()).view(crop_coords.size(0), -1)
        target_size_proj = self.condition_proj(target_size.flatten()).view(target_size.size(0), -1)
        condition_proj = torch.cat([original_size_proj, crop_coords_proj, target_size_proj], dim=1)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        condition_emb = self.condition_embedder(condition_proj)
        conditioning = timesteps_emb + condition_emb
        return conditioning


class HunyuanDiTAttentionPool(nn.Module):

    def __init__(self, spacial_dim: 'int', embed_dim: 'int', num_heads: 'int', output_dim: 'int'=None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.permute(1, 0, 2)
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)
        x = x + self.positional_embedding[:, None, :]
        x, _ = F.multi_head_attention_forward(query=x[:1], key=x, value=x, embed_dim_to_check=x.shape[-1], num_heads=self.num_heads, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, in_proj_weight=None, in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]), bias_k=None, bias_v=None, add_zero_attn=False, dropout_p=0, out_proj_weight=self.c_proj.weight, out_proj_bias=self.c_proj.bias, use_separate_proj_weight=True, training=self.training, need_weights=False)
        return x.squeeze(0)


class HunyuanCombinedTimestepTextSizeStyleEmbedding(nn.Module):

    def __init__(self, embedding_dim, pooled_projection_dim=1024, seq_len=256, cross_attention_dim=2048, use_style_cond_and_image_meta_size=True):
        super().__init__()
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.size_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.pooler = HunyuanDiTAttentionPool(seq_len, cross_attention_dim, num_heads=8, output_dim=pooled_projection_dim)
        self.use_style_cond_and_image_meta_size = use_style_cond_and_image_meta_size
        if use_style_cond_and_image_meta_size:
            self.style_embedder = nn.Embedding(1, embedding_dim)
            extra_in_dim = 256 * 6 + embedding_dim + pooled_projection_dim
        else:
            extra_in_dim = pooled_projection_dim
        self.extra_embedder = PixArtAlphaTextProjection(in_features=extra_in_dim, hidden_size=embedding_dim * 4, out_features=embedding_dim, act_fn='silu_fp32')

    def forward(self, timestep, encoder_hidden_states, image_meta_size, style, hidden_dtype=None):
        timesteps_proj = self.time_proj(timestep)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        pooled_projections = self.pooler(encoder_hidden_states)
        if self.use_style_cond_and_image_meta_size:
            image_meta_size = self.size_proj(image_meta_size.view(-1))
            image_meta_size = image_meta_size
            image_meta_size = image_meta_size.view(-1, 6 * 256)
            style_embedding = self.style_embedder(style)
            extra_cond = torch.cat([pooled_projections, image_meta_size, style_embedding], dim=1)
        else:
            extra_cond = torch.cat([pooled_projections], dim=1)
        conditioning = timesteps_emb + self.extra_embedder(extra_cond)
        return conditioning


class LuminaCombinedTimestepCaptionEmbedding(nn.Module):

    def __init__(self, hidden_size=4096, cross_attention_dim=2048, frequency_embedding_size=256):
        super().__init__()
        self.time_proj = Timesteps(num_channels=frequency_embedding_size, flip_sin_to_cos=True, downscale_freq_shift=0.0)
        self.timestep_embedder = TimestepEmbedding(in_channels=frequency_embedding_size, time_embed_dim=hidden_size)
        self.caption_embedder = nn.Sequential(nn.LayerNorm(cross_attention_dim), nn.Linear(cross_attention_dim, hidden_size, bias=True))

    def forward(self, timestep, caption_feat, caption_mask):
        time_freq = self.time_proj(timestep)
        time_embed = self.timestep_embedder(time_freq)
        caption_mask_float = caption_mask.float().unsqueeze(-1)
        caption_feats_pool = (caption_feat * caption_mask_float).sum(dim=1) / caption_mask_float.sum(dim=1)
        caption_feats_pool = caption_feats_pool
        caption_embed = self.caption_embedder(caption_feats_pool)
        conditioning = time_embed + caption_embed
        return conditioning


class MochiAttentionPool(nn.Module):

    def __init__(self, num_attention_heads: 'int', embed_dim: 'int', output_dim: 'Optional[int]'=None) ->None:
        super().__init__()
        self.output_dim = output_dim or embed_dim
        self.num_attention_heads = num_attention_heads
        self.to_kv = nn.Linear(embed_dim, 2 * embed_dim)
        self.to_q = nn.Linear(embed_dim, embed_dim)
        self.to_out = nn.Linear(embed_dim, self.output_dim)

    @staticmethod
    def pool_tokens(x: 'torch.Tensor', mask: 'torch.Tensor', *, keepdim=False) ->torch.Tensor:
        """
        Pool tokens in x using mask.

        NOTE: We assume x does not require gradients.

        Args:
            x: (B, L, D) tensor of tokens.
            mask: (B, L) boolean tensor indicating which tokens are not padding.

        Returns:
            pooled: (B, D) tensor of pooled tokens.
        """
        assert x.size(1) == mask.size(1)
        assert x.size(0) == mask.size(0)
        mask = mask[:, :, None]
        mask = mask / mask.sum(dim=1, keepdim=True).clamp(min=1)
        pooled = (x * mask).sum(dim=1, keepdim=keepdim)
        return pooled

    def forward(self, x: 'torch.Tensor', mask: 'torch.BoolTensor') ->torch.Tensor:
        """
        Args:
            x (`torch.Tensor`):
                Tensor of shape `(B, S, D)` of input tokens.
            mask (`torch.Tensor`):
                Boolean ensor of shape `(B, S)` indicating which tokens are not padding.

        Returns:
            `torch.Tensor`:
                `(B, D)` tensor of pooled tokens.
        """
        D = x.size(2)
        attn_mask = mask[:, None, None, :].bool()
        attn_mask = F.pad(attn_mask, (1, 0), value=True)
        x_pool = self.pool_tokens(x, mask, keepdim=True)
        x = torch.cat([x_pool, x], dim=1)
        kv = self.to_kv(x)
        q = self.to_q(x[:, 0])
        head_dim = D // self.num_attention_heads
        kv = kv.unflatten(2, (2, self.num_attention_heads, head_dim))
        kv = kv.transpose(1, 3)
        k, v = kv.unbind(2)
        q = q.unflatten(1, (self.num_attention_heads, head_dim))
        q = q.unsqueeze(2)
        x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, dropout_p=0.0)
        x = x.squeeze(2).flatten(1, 2)
        x = self.to_out(x)
        return x


class MochiCombinedTimestepCaptionEmbedding(nn.Module):

    def __init__(self, embedding_dim: 'int', pooled_projection_dim: 'int', text_embed_dim: 'int', time_embed_dim: 'int'=256, num_attention_heads: 'int'=8) ->None:
        super().__init__()
        self.time_proj = Timesteps(num_channels=time_embed_dim, flip_sin_to_cos=True, downscale_freq_shift=0.0)
        self.timestep_embedder = TimestepEmbedding(in_channels=time_embed_dim, time_embed_dim=embedding_dim)
        self.pooler = MochiAttentionPool(num_attention_heads=num_attention_heads, embed_dim=text_embed_dim, output_dim=embedding_dim)
        self.caption_proj = nn.Linear(text_embed_dim, pooled_projection_dim)

    def forward(self, timestep: 'torch.LongTensor', encoder_hidden_states: 'torch.Tensor', encoder_attention_mask: 'torch.Tensor', hidden_dtype: 'Optional[torch.dtype]'=None):
        time_proj = self.time_proj(timestep)
        time_emb = self.timestep_embedder(time_proj)
        pooled_projections = self.pooler(encoder_hidden_states, encoder_attention_mask)
        caption_proj = self.caption_proj(encoder_hidden_states)
        conditioning = time_emb + pooled_projections
        return conditioning, caption_proj


class AttentionPooling(nn.Module):

    def __init__(self, num_heads, embed_dim, dtype=None):
        super().__init__()
        self.dtype = dtype
        self.positional_embedding = nn.Parameter(torch.randn(1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim, dtype=self.dtype)
        self.q_proj = nn.Linear(embed_dim, embed_dim, dtype=self.dtype)
        self.v_proj = nn.Linear(embed_dim, embed_dim, dtype=self.dtype)
        self.num_heads = num_heads
        self.dim_per_head = embed_dim // self.num_heads

    def forward(self, x):
        bs, length, width = x.size()

        def shape(x):
            x = x.view(bs, -1, self.num_heads, self.dim_per_head)
            x = x.transpose(1, 2)
            x = x.reshape(bs * self.num_heads, -1, self.dim_per_head)
            x = x.transpose(1, 2)
            return x
        class_token = x.mean(dim=1, keepdim=True) + self.positional_embedding
        x = torch.cat([class_token, x], dim=1)
        q = shape(self.q_proj(class_token))
        k = shape(self.k_proj(x))
        v = shape(self.v_proj(x))
        scale = 1 / math.sqrt(math.sqrt(self.dim_per_head))
        weight = torch.einsum('bct,bcs->bts', q * scale, k * scale)
        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
        a = torch.einsum('bts,bcs->bct', weight, v)
        a = a.reshape(bs, -1, 1).transpose(1, 2)
        return a[:, 0, :]


class TextTimeEmbedding(nn.Module):

    def __init__(self, encoder_dim: 'int', time_embed_dim: 'int', num_heads: 'int'=64):
        super().__init__()
        self.norm1 = nn.LayerNorm(encoder_dim)
        self.pool = AttentionPooling(num_heads, encoder_dim)
        self.proj = nn.Linear(encoder_dim, time_embed_dim)
        self.norm2 = nn.LayerNorm(time_embed_dim)

    def forward(self, hidden_states):
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.pool(hidden_states)
        hidden_states = self.proj(hidden_states)
        hidden_states = self.norm2(hidden_states)
        return hidden_states


class TextImageTimeEmbedding(nn.Module):

    def __init__(self, text_embed_dim: 'int'=768, image_embed_dim: 'int'=768, time_embed_dim: 'int'=1536):
        super().__init__()
        self.text_proj = nn.Linear(text_embed_dim, time_embed_dim)
        self.text_norm = nn.LayerNorm(time_embed_dim)
        self.image_proj = nn.Linear(image_embed_dim, time_embed_dim)

    def forward(self, text_embeds: 'torch.Tensor', image_embeds: 'torch.Tensor'):
        time_text_embeds = self.text_proj(text_embeds)
        time_text_embeds = self.text_norm(time_text_embeds)
        time_image_embeds = self.image_proj(image_embeds)
        return time_image_embeds + time_text_embeds


class ImageTimeEmbedding(nn.Module):

    def __init__(self, image_embed_dim: 'int'=768, time_embed_dim: 'int'=1536):
        super().__init__()
        self.image_proj = nn.Linear(image_embed_dim, time_embed_dim)
        self.image_norm = nn.LayerNorm(time_embed_dim)

    def forward(self, image_embeds: 'torch.Tensor'):
        time_image_embeds = self.image_proj(image_embeds)
        time_image_embeds = self.image_norm(time_image_embeds)
        return time_image_embeds


class ImageHintTimeEmbedding(nn.Module):

    def __init__(self, image_embed_dim: 'int'=768, time_embed_dim: 'int'=1536):
        super().__init__()
        self.image_proj = nn.Linear(image_embed_dim, time_embed_dim)
        self.image_norm = nn.LayerNorm(time_embed_dim)
        self.input_hint_block = nn.Sequential(nn.Conv2d(3, 16, 3, padding=1), nn.SiLU(), nn.Conv2d(16, 16, 3, padding=1), nn.SiLU(), nn.Conv2d(16, 32, 3, padding=1, stride=2), nn.SiLU(), nn.Conv2d(32, 32, 3, padding=1), nn.SiLU(), nn.Conv2d(32, 96, 3, padding=1, stride=2), nn.SiLU(), nn.Conv2d(96, 96, 3, padding=1), nn.SiLU(), nn.Conv2d(96, 256, 3, padding=1, stride=2), nn.SiLU(), nn.Conv2d(256, 4, 3, padding=1))

    def forward(self, image_embeds: 'torch.Tensor', hint: 'torch.Tensor'):
        time_image_embeds = self.image_proj(image_embeds)
        time_image_embeds = self.image_norm(time_image_embeds)
        hint = self.input_hint_block(hint)
        return time_image_embeds, hint


class FourierEmbedder(nn.Module):

    def __init__(self, num_freqs=64, temperature=100):
        super().__init__()
        self.num_freqs = num_freqs
        self.temperature = temperature
        freq_bands = temperature ** (torch.arange(num_freqs) / num_freqs)
        freq_bands = freq_bands[None, None, None]
        self.register_buffer('freq_bands', freq_bands, persistent=False)

    def __call__(self, x):
        x = self.freq_bands * x.unsqueeze(-1)
        return torch.stack((x.sin(), x.cos()), dim=-1).permute(0, 1, 3, 4, 2).reshape(*x.shape[:2], -1)


class GLIGENTextBoundingboxProjection(nn.Module):

    def __init__(self, positive_len, out_dim, feature_type, fourier_freqs=8):
        super().__init__()
        self.positive_len = positive_len
        self.out_dim = out_dim
        self.fourier_embedder = FourierEmbedder(num_freqs=fourier_freqs)
        self.position_dim = fourier_freqs * 2 * 4
        if isinstance(out_dim, tuple):
            out_dim = out_dim[0]
        if feature_type == 'text-only':
            self.linears = nn.Sequential(nn.Linear(self.positive_len + self.position_dim, 512), nn.SiLU(), nn.Linear(512, 512), nn.SiLU(), nn.Linear(512, out_dim))
            self.null_positive_feature = torch.nn.Parameter(torch.zeros([self.positive_len]))
        elif feature_type == 'text-image':
            self.linears_text = nn.Sequential(nn.Linear(self.positive_len + self.position_dim, 512), nn.SiLU(), nn.Linear(512, 512), nn.SiLU(), nn.Linear(512, out_dim))
            self.linears_image = nn.Sequential(nn.Linear(self.positive_len + self.position_dim, 512), nn.SiLU(), nn.Linear(512, 512), nn.SiLU(), nn.Linear(512, out_dim))
            self.null_text_feature = torch.nn.Parameter(torch.zeros([self.positive_len]))
            self.null_image_feature = torch.nn.Parameter(torch.zeros([self.positive_len]))
        self.null_position_feature = torch.nn.Parameter(torch.zeros([self.position_dim]))

    def forward(self, boxes, masks, positive_embeddings=None, phrases_masks=None, image_masks=None, phrases_embeddings=None, image_embeddings=None):
        masks = masks.unsqueeze(-1)
        xyxy_embedding = self.fourier_embedder(boxes)
        xyxy_null = self.null_position_feature.view(1, 1, -1)
        xyxy_embedding = xyxy_embedding * masks + (1 - masks) * xyxy_null
        if positive_embeddings:
            positive_null = self.null_positive_feature.view(1, 1, -1)
            positive_embeddings = positive_embeddings * masks + (1 - masks) * positive_null
            objs = self.linears(torch.cat([positive_embeddings, xyxy_embedding], dim=-1))
        else:
            phrases_masks = phrases_masks.unsqueeze(-1)
            image_masks = image_masks.unsqueeze(-1)
            text_null = self.null_text_feature.view(1, 1, -1)
            image_null = self.null_image_feature.view(1, 1, -1)
            phrases_embeddings = phrases_embeddings * phrases_masks + (1 - phrases_masks) * text_null
            image_embeddings = image_embeddings * image_masks + (1 - image_masks) * image_null
            objs_text = self.linears_text(torch.cat([phrases_embeddings, xyxy_embedding], dim=-1))
            objs_image = self.linears_image(torch.cat([image_embeddings, xyxy_embedding], dim=-1))
            objs = torch.cat([objs_text, objs_image], dim=1)
        return objs


class PixArtAlphaCombinedTimestepSizeEmbeddings(nn.Module):
    """
    For PixArt-Alpha.

    Reference:
    https://github.com/PixArt-alpha/PixArt-alpha/blob/0f55e922376d8b797edd44d25d0e7464b260dcab/diffusion/model/nets/PixArtMS.py#L164C9-L168C29
    """

    def __init__(self, embedding_dim, size_emb_dim, use_additional_conditions: 'bool'=False):
        super().__init__()
        self.outdim = size_emb_dim
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.use_additional_conditions = use_additional_conditions
        if use_additional_conditions:
            self.additional_condition_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)
            self.resolution_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=size_emb_dim)
            self.aspect_ratio_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=size_emb_dim)

    def forward(self, timestep, resolution, aspect_ratio, batch_size, hidden_dtype):
        timesteps_proj = self.time_proj(timestep)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        if self.use_additional_conditions:
            resolution_emb = self.additional_condition_proj(resolution.flatten())
            resolution_emb = self.resolution_embedder(resolution_emb).reshape(batch_size, -1)
            aspect_ratio_emb = self.additional_condition_proj(aspect_ratio.flatten())
            aspect_ratio_emb = self.aspect_ratio_embedder(aspect_ratio_emb).reshape(batch_size, -1)
            conditioning = timesteps_emb + torch.cat([resolution_emb, aspect_ratio_emb], dim=1)
        else:
            conditioning = timesteps_emb
        return conditioning


class IPAdapterPlusImageProjectionBlock(nn.Module):

    def __init__(self, embed_dims: 'int'=768, dim_head: 'int'=64, heads: 'int'=16, ffn_ratio: 'float'=4) ->None:
        super().__init__()
        self.ln0 = nn.LayerNorm(embed_dims)
        self.ln1 = nn.LayerNorm(embed_dims)
        self.attn = Attention(query_dim=embed_dims, dim_head=dim_head, heads=heads, out_bias=False)
        self.ff = nn.Sequential(nn.LayerNorm(embed_dims), FeedForward(embed_dims, embed_dims, activation_fn='gelu', mult=ffn_ratio, bias=False))

    def forward(self, x, latents, residual):
        encoder_hidden_states = self.ln0(x)
        latents = self.ln1(latents)
        encoder_hidden_states = torch.cat([encoder_hidden_states, latents], dim=-2)
        latents = self.attn(latents, encoder_hidden_states) + residual
        latents = self.ff(latents) + latents
        return latents


class IPAdapterPlusImageProjection(nn.Module):
    """Resampler of IP-Adapter Plus.

    Args:
        embed_dims (int): The feature dimension. Defaults to 768. output_dims (int): The number of output channels,
        that is the same
            number of the channels in the `unet.config.cross_attention_dim`. Defaults to 1024.
        hidden_dims (int):
            The number of hidden channels. Defaults to 1280. depth (int): The number of blocks. Defaults
        to 8. dim_head (int): The number of head channels. Defaults to 64. heads (int): Parallel attention heads.
        Defaults to 16. num_queries (int):
            The number of queries. Defaults to 8. ffn_ratio (float): The expansion ratio
        of feedforward network hidden
            layer channels. Defaults to 4.
    """

    def __init__(self, embed_dims: 'int'=768, output_dims: 'int'=1024, hidden_dims: 'int'=1280, depth: 'int'=4, dim_head: 'int'=64, heads: 'int'=16, num_queries: 'int'=8, ffn_ratio: 'float'=4) ->None:
        super().__init__()
        self.latents = nn.Parameter(torch.randn(1, num_queries, hidden_dims) / hidden_dims ** 0.5)
        self.proj_in = nn.Linear(embed_dims, hidden_dims)
        self.proj_out = nn.Linear(hidden_dims, output_dims)
        self.norm_out = nn.LayerNorm(output_dims)
        self.layers = nn.ModuleList([IPAdapterPlusImageProjectionBlock(hidden_dims, dim_head, heads, ffn_ratio) for _ in range(depth)])

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward pass.

        Args:
            x (torch.Tensor): Input Tensor.
        Returns:
            torch.Tensor: Output Tensor.
        """
        latents = self.latents.repeat(x.size(0), 1, 1)
        x = self.proj_in(x)
        for block in self.layers:
            residual = latents
            latents = block(x, latents, residual)
        latents = self.proj_out(latents)
        return self.norm_out(latents)


class IPAdapterFaceIDPlusImageProjection(nn.Module):
    """FacePerceiverResampler of IP-Adapter Plus.

    Args:
        embed_dims (int): The feature dimension. Defaults to 768. output_dims (int): The number of output channels,
        that is the same
            number of the channels in the `unet.config.cross_attention_dim`. Defaults to 1024.
        hidden_dims (int):
            The number of hidden channels. Defaults to 1280. depth (int): The number of blocks. Defaults
        to 8. dim_head (int): The number of head channels. Defaults to 64. heads (int): Parallel attention heads.
        Defaults to 16. num_tokens (int): Number of tokens num_queries (int): The number of queries. Defaults to 8.
        ffn_ratio (float): The expansion ratio of feedforward network hidden
            layer channels. Defaults to 4.
        ffproj_ratio (float): The expansion ratio of feedforward network hidden
            layer channels (for ID embeddings). Defaults to 4.
    """

    def __init__(self, embed_dims: 'int'=768, output_dims: 'int'=768, hidden_dims: 'int'=1280, id_embeddings_dim: 'int'=512, depth: 'int'=4, dim_head: 'int'=64, heads: 'int'=16, num_tokens: 'int'=4, num_queries: 'int'=8, ffn_ratio: 'float'=4, ffproj_ratio: 'int'=2) ->None:
        super().__init__()
        self.num_tokens = num_tokens
        self.embed_dim = embed_dims
        self.clip_embeds = None
        self.shortcut = False
        self.shortcut_scale = 1.0
        self.proj = FeedForward(id_embeddings_dim, embed_dims * num_tokens, activation_fn='gelu', mult=ffproj_ratio)
        self.norm = nn.LayerNorm(embed_dims)
        self.proj_in = nn.Linear(hidden_dims, embed_dims)
        self.proj_out = nn.Linear(embed_dims, output_dims)
        self.norm_out = nn.LayerNorm(output_dims)
        self.layers = nn.ModuleList([IPAdapterPlusImageProjectionBlock(embed_dims, dim_head, heads, ffn_ratio) for _ in range(depth)])

    def forward(self, id_embeds: 'torch.Tensor') ->torch.Tensor:
        """Forward pass.

        Args:
            id_embeds (torch.Tensor): Input Tensor (ID embeds).
        Returns:
            torch.Tensor: Output Tensor.
        """
        id_embeds = id_embeds
        id_embeds = self.proj(id_embeds)
        id_embeds = id_embeds.reshape(-1, self.num_tokens, self.embed_dim)
        id_embeds = self.norm(id_embeds)
        latents = id_embeds
        clip_embeds = self.proj_in(self.clip_embeds)
        x = clip_embeds.reshape(-1, clip_embeds.shape[2], clip_embeds.shape[3])
        for block in self.layers:
            residual = latents
            latents = block(x, latents, residual)
        latents = self.proj_out(latents)
        out = self.norm_out(latents)
        if self.shortcut:
            out = id_embeds + self.shortcut_scale * out
        return out


class MultiIPAdapterImageProjection(nn.Module):

    def __init__(self, IPAdapterImageProjectionLayers: 'Union[List[nn.Module], Tuple[nn.Module]]'):
        super().__init__()
        self.image_projection_layers = nn.ModuleList(IPAdapterImageProjectionLayers)

    def forward(self, image_embeds: 'List[torch.Tensor]'):
        projected_image_embeds = []
        if not isinstance(image_embeds, list):
            deprecation_message = 'You have passed a tensor as `image_embeds`.This is deprecated and will be removed in a future release. Please make sure to update your script to pass `image_embeds` as a list of tensors to suppress this warning.'
            deprecate('image_embeds not a list', '1.0.0', deprecation_message, standard_warn=False)
            image_embeds = [image_embeds.unsqueeze(1)]
        if len(image_embeds) != len(self.image_projection_layers):
            raise ValueError(f'image_embeds must have the same length as image_projection_layers, got {len(image_embeds)} and {len(self.image_projection_layers)}')
        for image_embed, image_projection_layer in zip(image_embeds, self.image_projection_layers):
            batch_size, num_images = image_embed.shape[0], image_embed.shape[1]
            image_embed = image_embed.reshape((batch_size * num_images,) + image_embed.shape[2:])
            image_embed = image_projection_layer(image_embed)
            image_embed = image_embed.reshape((batch_size, num_images) + image_embed.shape[1:])
            projected_image_embeds.append(image_embed)
        return projected_image_embeds


class LoRALinearLayer(nn.Module):
    """
    A linear layer that is used with LoRA.

    Parameters:
        in_features (`int`):
            Number of input features.
        out_features (`int`):
            Number of output features.
        rank (`int`, `optional`, defaults to 4):
            The rank of the LoRA layer.
        network_alpha (`float`, `optional`, defaults to `None`):
            The value of the network alpha used for stable learning and preventing underflow. This value has the same
            meaning as the `--network_alpha` option in the kohya-ss trainer script. See
            https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning
        device (`torch.device`, `optional`, defaults to `None`):
            The device to use for the layer's weights.
        dtype (`torch.dtype`, `optional`, defaults to `None`):
            The dtype to use for the layer's weights.
    """

    def __init__(self, in_features: 'int', out_features: 'int', rank: 'int'=4, network_alpha: 'Optional[float]'=None, device: 'Optional[Union[torch.device, str]]'=None, dtype: 'Optional[torch.dtype]'=None):
        super().__init__()
        deprecation_message = 'Use of `LoRALinearLayer` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('LoRALinearLayer', '1.0.0', deprecation_message)
        self.down = nn.Linear(in_features, rank, bias=False, device=device, dtype=dtype)
        self.up = nn.Linear(rank, out_features, bias=False, device=device, dtype=dtype)
        self.network_alpha = network_alpha
        self.rank = rank
        self.out_features = out_features
        self.in_features = in_features
        nn.init.normal_(self.down.weight, std=1 / rank)
        nn.init.zeros_(self.up.weight)

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        orig_dtype = hidden_states.dtype
        dtype = self.down.weight.dtype
        down_hidden_states = self.down(hidden_states)
        up_hidden_states = self.up(down_hidden_states)
        if self.network_alpha is not None:
            up_hidden_states *= self.network_alpha / self.rank
        return up_hidden_states


class PatchedLoraProjection(torch.nn.Module):

    def __init__(self, regular_linear_layer, lora_scale=1, network_alpha=None, rank=4, dtype=None):
        deprecation_message = 'Use of `PatchedLoraProjection` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('PatchedLoraProjection', '1.0.0', deprecation_message)
        super().__init__()
        self.regular_linear_layer = regular_linear_layer
        device = self.regular_linear_layer.weight.device
        if dtype is None:
            dtype = self.regular_linear_layer.weight.dtype
        self.lora_linear_layer = LoRALinearLayer(self.regular_linear_layer.in_features, self.regular_linear_layer.out_features, network_alpha=network_alpha, device=device, dtype=dtype, rank=rank)
        self.lora_scale = lora_scale

    def state_dict(self, *args, destination=None, prefix='', keep_vars=False):
        if self.lora_linear_layer is None:
            return self.regular_linear_layer.state_dict(*args, destination=destination, prefix=prefix, keep_vars=keep_vars)
        return super().state_dict(*args, destination=destination, prefix=prefix, keep_vars=keep_vars)

    def _fuse_lora(self, lora_scale=1.0, safe_fusing=False):
        if self.lora_linear_layer is None:
            return
        dtype, device = self.regular_linear_layer.weight.data.dtype, self.regular_linear_layer.weight.data.device
        w_orig = self.regular_linear_layer.weight.data.float()
        w_up = self.lora_linear_layer.up.weight.data.float()
        w_down = self.lora_linear_layer.down.weight.data.float()
        if self.lora_linear_layer.network_alpha is not None:
            w_up = w_up * self.lora_linear_layer.network_alpha / self.lora_linear_layer.rank
        fused_weight = w_orig + lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0]
        if safe_fusing and torch.isnan(fused_weight).any().item():
            raise ValueError(f'This LoRA weight seems to be broken. Encountered NaN values when trying to fuse LoRA weights for {self}.LoRA weights will not be fused.')
        self.regular_linear_layer.weight.data = fused_weight
        self.lora_linear_layer = None
        self.w_up = w_up.cpu()
        self.w_down = w_down.cpu()
        self.lora_scale = lora_scale

    def _unfuse_lora(self):
        if not (getattr(self, 'w_up', None) is not None and getattr(self, 'w_down', None) is not None):
            return
        fused_weight = self.regular_linear_layer.weight.data
        dtype, device = fused_weight.dtype, fused_weight.device
        w_up = self.w_up.float()
        w_down = self.w_down.float()
        unfused_weight = fused_weight.float() - self.lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0]
        self.regular_linear_layer.weight.data = unfused_weight
        self.w_up = None
        self.w_down = None

    def forward(self, input):
        if self.lora_scale is None:
            self.lora_scale = 1.0
        if self.lora_linear_layer is None:
            return self.regular_linear_layer(input)
        return self.regular_linear_layer(input) + self.lora_scale * self.lora_linear_layer(input)


class LoRAConv2dLayer(nn.Module):
    """
    A convolutional layer that is used with LoRA.

    Parameters:
        in_features (`int`):
            Number of input features.
        out_features (`int`):
            Number of output features.
        rank (`int`, `optional`, defaults to 4):
            The rank of the LoRA layer.
        kernel_size (`int` or `tuple` of two `int`, `optional`, defaults to 1):
            The kernel size of the convolution.
        stride (`int` or `tuple` of two `int`, `optional`, defaults to 1):
            The stride of the convolution.
        padding (`int` or `tuple` of two `int` or `str`, `optional`, defaults to 0):
            The padding of the convolution.
        network_alpha (`float`, `optional`, defaults to `None`):
            The value of the network alpha used for stable learning and preventing underflow. This value has the same
            meaning as the `--network_alpha` option in the kohya-ss trainer script. See
            https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning
    """

    def __init__(self, in_features: 'int', out_features: 'int', rank: 'int'=4, kernel_size: 'Union[int, Tuple[int, int]]'=(1, 1), stride: 'Union[int, Tuple[int, int]]'=(1, 1), padding: 'Union[int, Tuple[int, int], str]'=0, network_alpha: 'Optional[float]'=None):
        super().__init__()
        deprecation_message = 'Use of `LoRAConv2dLayer` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('LoRAConv2dLayer', '1.0.0', deprecation_message)
        self.down = nn.Conv2d(in_features, rank, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
        self.up = nn.Conv2d(rank, out_features, kernel_size=(1, 1), stride=(1, 1), bias=False)
        self.network_alpha = network_alpha
        self.rank = rank
        nn.init.normal_(self.down.weight, std=1 / rank)
        nn.init.zeros_(self.up.weight)

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        orig_dtype = hidden_states.dtype
        dtype = self.down.weight.dtype
        down_hidden_states = self.down(hidden_states)
        up_hidden_states = self.up(down_hidden_states)
        if self.network_alpha is not None:
            up_hidden_states *= self.network_alpha / self.rank
        return up_hidden_states


class LoRACompatibleConv(nn.Conv2d):
    """
    A convolutional layer that can be used with LoRA.
    """

    def __init__(self, *args, lora_layer: Optional[LoRAConv2dLayer]=None, **kwargs):
        deprecation_message = 'Use of `LoRACompatibleConv` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('LoRACompatibleConv', '1.0.0', deprecation_message)
        super().__init__(*args, **kwargs)
        self.lora_layer = lora_layer

    def set_lora_layer(self, lora_layer: 'Optional[LoRAConv2dLayer]'):
        deprecation_message = 'Use of `set_lora_layer()` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('set_lora_layer', '1.0.0', deprecation_message)
        self.lora_layer = lora_layer

    def _fuse_lora(self, lora_scale: 'float'=1.0, safe_fusing: 'bool'=False):
        if self.lora_layer is None:
            return
        dtype, device = self.weight.data.dtype, self.weight.data.device
        w_orig = self.weight.data.float()
        w_up = self.lora_layer.up.weight.data.float()
        w_down = self.lora_layer.down.weight.data.float()
        if self.lora_layer.network_alpha is not None:
            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank
        fusion = torch.mm(w_up.flatten(start_dim=1), w_down.flatten(start_dim=1))
        fusion = fusion.reshape(w_orig.shape)
        fused_weight = w_orig + lora_scale * fusion
        if safe_fusing and torch.isnan(fused_weight).any().item():
            raise ValueError(f'This LoRA weight seems to be broken. Encountered NaN values when trying to fuse LoRA weights for {self}.LoRA weights will not be fused.')
        self.weight.data = fused_weight
        self.lora_layer = None
        self.w_up = w_up.cpu()
        self.w_down = w_down.cpu()
        self._lora_scale = lora_scale

    def _unfuse_lora(self):
        if not (getattr(self, 'w_up', None) is not None and getattr(self, 'w_down', None) is not None):
            return
        fused_weight = self.weight.data
        dtype, device = fused_weight.data.dtype, fused_weight.data.device
        self.w_up = self.w_up.float()
        self.w_down = self.w_down.float()
        fusion = torch.mm(self.w_up.flatten(start_dim=1), self.w_down.flatten(start_dim=1))
        fusion = fusion.reshape(fused_weight.shape)
        unfused_weight = fused_weight.float() - self._lora_scale * fusion
        self.weight.data = unfused_weight
        self.w_up = None
        self.w_down = None

    def forward(self, hidden_states: 'torch.Tensor', scale: 'float'=1.0) ->torch.Tensor:
        if self.padding_mode != 'zeros':
            hidden_states = F.pad(hidden_states, self._reversed_padding_repeated_twice, mode=self.padding_mode)
            padding = 0, 0
        else:
            padding = self.padding
        original_outputs = F.conv2d(hidden_states, self.weight, self.bias, self.stride, padding, self.dilation, self.groups)
        if self.lora_layer is None:
            return original_outputs
        else:
            return original_outputs + scale * self.lora_layer(hidden_states)


class LoRACompatibleLinear(nn.Linear):
    """
    A Linear layer that can be used with LoRA.
    """

    def __init__(self, *args, lora_layer: Optional[LoRALinearLayer]=None, **kwargs):
        deprecation_message = 'Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('LoRACompatibleLinear', '1.0.0', deprecation_message)
        super().__init__(*args, **kwargs)
        self.lora_layer = lora_layer

    def set_lora_layer(self, lora_layer: 'Optional[LoRALinearLayer]'):
        deprecation_message = 'Use of `set_lora_layer()` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.'
        deprecate('set_lora_layer', '1.0.0', deprecation_message)
        self.lora_layer = lora_layer

    def _fuse_lora(self, lora_scale: 'float'=1.0, safe_fusing: 'bool'=False):
        if self.lora_layer is None:
            return
        dtype, device = self.weight.data.dtype, self.weight.data.device
        w_orig = self.weight.data.float()
        w_up = self.lora_layer.up.weight.data.float()
        w_down = self.lora_layer.down.weight.data.float()
        if self.lora_layer.network_alpha is not None:
            w_up = w_up * self.lora_layer.network_alpha / self.lora_layer.rank
        fused_weight = w_orig + lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0]
        if safe_fusing and torch.isnan(fused_weight).any().item():
            raise ValueError(f'This LoRA weight seems to be broken. Encountered NaN values when trying to fuse LoRA weights for {self}.LoRA weights will not be fused.')
        self.weight.data = fused_weight
        self.lora_layer = None
        self.w_up = w_up.cpu()
        self.w_down = w_down.cpu()
        self._lora_scale = lora_scale

    def _unfuse_lora(self):
        if not (getattr(self, 'w_up', None) is not None and getattr(self, 'w_down', None) is not None):
            return
        fused_weight = self.weight.data
        dtype, device = fused_weight.dtype, fused_weight.device
        w_up = self.w_up.float()
        w_down = self.w_down.float()
        unfused_weight = fused_weight.float() - self._lora_scale * torch.bmm(w_up[None, :], w_down[None, :])[0]
        self.weight.data = unfused_weight
        self.w_up = None
        self.w_down = None

    def forward(self, hidden_states: 'torch.Tensor', scale: 'float'=1.0) ->torch.Tensor:
        if self.lora_layer is None:
            out = super().forward(hidden_states)
            return out
        else:
            out = super().forward(hidden_states) + scale * self.lora_layer(hidden_states)
            return out


class ModelMixin(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


_CLASS_REMAPPING_DICT = {'Transformer2DModel': {'ada_norm_zero': 'DiTTransformer2DModel', 'ada_norm_single': 'PixArtTransformer2DModel'}}


def _fetch_remapped_cls_from_config(config, old_class):
    previous_class_name = old_class.__name__
    remapped_class_name = _CLASS_REMAPPING_DICT.get(previous_class_name).get(config['norm_type'], None)
    if remapped_class_name:
        diffusers_library = importlib.import_module(__name__.split('.')[0])
        remapped_class = getattr(diffusers_library, remapped_class_name)
        logger.info(f"Changing class object to be of `{remapped_class_name}` type from `{previous_class_name}` type.This is because `{previous_class_name}` is scheduled to be deprecated in a future version. Note that this DOESN'T affect the final results.")
        return remapped_class
    else:
        return old_class


class AdaLayerNormZeroSingle(nn.Module):
    """
    Norm layer adaptive layer norm zero (adaLN-Zero).

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
    """

    def __init__(self, embedding_dim: 'int', norm_type='layer_norm', bias=True):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, 3 * embedding_dim, bias=bias)
        if norm_type == 'layer_norm':
            self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-06)
        else:
            raise ValueError(f"Unsupported `norm_type` ({norm_type}) provided. Supported ones are: 'layer_norm', 'fp32_layer_norm'.")

    def forward(self, x: 'torch.Tensor', emb: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        emb = self.linear(self.silu(emb))
        shift_msa, scale_msa, gate_msa = emb.chunk(3, dim=1)
        x = self.norm(x) * (1 + scale_msa[:, None]) + shift_msa[:, None]
        return x, gate_msa


class LuminaRMSNormZero(nn.Module):
    """
    Norm layer adaptive RMS normalization zero.

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
    """

    def __init__(self, embedding_dim: 'int', norm_eps: 'float', norm_elementwise_affine: 'bool'):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(min(embedding_dim, 1024), 4 * embedding_dim, bias=True)
        self.norm = RMSNorm(embedding_dim, eps=norm_eps, elementwise_affine=norm_elementwise_affine)

    def forward(self, x: 'torch.Tensor', emb: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        emb = self.linear(self.silu(emb))
        scale_msa, gate_msa, scale_mlp, gate_mlp = emb.chunk(4, dim=1)
        x = self.norm(x) * (1 + scale_msa[:, None])
        return x, gate_msa, scale_mlp, gate_mlp


class MochiRMSNormZero(nn.Module):
    """
    Adaptive RMS Norm used in Mochi.

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
    """

    def __init__(self, embedding_dim: 'int', hidden_dim: 'int', eps: 'float'=1e-05, elementwise_affine: 'bool'=False) ->None:
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, hidden_dim)
        self.norm = RMSNorm(embedding_dim, eps=eps, elementwise_affine=elementwise_affine)

    def forward(self, hidden_states: 'torch.Tensor', emb: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        emb = self.linear(self.silu(emb))
        scale_msa, gate_msa, scale_mlp, gate_mlp = emb.chunk(4, dim=1)
        hidden_states = self.norm(hidden_states) * (1 + scale_msa[:, None])
        return hidden_states, gate_msa, scale_mlp, gate_mlp


class AdaLayerNormSingle(nn.Module):
    """
    Norm layer adaptive layer norm single (adaLN-single).

    As proposed in PixArt-Alpha (see: https://arxiv.org/abs/2310.00426; Section 2.3).

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        use_additional_conditions (`bool`): To use additional conditions for normalization or not.
    """

    def __init__(self, embedding_dim: 'int', use_additional_conditions: 'bool'=False):
        super().__init__()
        self.emb = PixArtAlphaCombinedTimestepSizeEmbeddings(embedding_dim, size_emb_dim=embedding_dim // 3, use_additional_conditions=use_additional_conditions)
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=True)

    def forward(self, timestep: 'torch.Tensor', added_cond_kwargs: 'Optional[Dict[str, torch.Tensor]]'=None, batch_size: 'Optional[int]'=None, hidden_dtype: 'Optional[torch.dtype]'=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        added_cond_kwargs = added_cond_kwargs or {'resolution': None, 'aspect_ratio': None}
        embedded_timestep = self.emb(timestep, **added_cond_kwargs, batch_size=batch_size, hidden_dtype=hidden_dtype)
        return self.linear(self.silu(embedded_timestep)), embedded_timestep


class LuminaLayerNormContinuous(nn.Module):

    def __init__(self, embedding_dim: 'int', conditioning_embedding_dim: 'int', elementwise_affine=True, eps=1e-05, bias=True, norm_type='layer_norm', out_dim: 'Optional[int]'=None):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear_1 = nn.Linear(conditioning_embedding_dim, embedding_dim, bias=bias)
        if norm_type == 'layer_norm':
            self.norm = LayerNorm(embedding_dim, eps, elementwise_affine, bias)
        elif norm_type == 'rms_norm':
            self.norm = RMSNorm(embedding_dim, eps=eps, elementwise_affine=elementwise_affine)
        else:
            raise ValueError(f'unknown norm_type {norm_type}')
        self.linear_2 = None
        if out_dim is not None:
            self.linear_2 = nn.Linear(embedding_dim, out_dim, bias=bias)

    def forward(self, x: 'torch.Tensor', conditioning_embedding: 'torch.Tensor') ->torch.Tensor:
        emb = self.linear_1(self.silu(conditioning_embedding))
        scale = emb
        x = self.norm(x) * (1 + scale)[:, None, :]
        if self.linear_2 is not None:
            x = self.linear_2(x)
        return x


class CogView3PlusAdaLayerNormZeroTextImage(nn.Module):
    """
    Norm layer adaptive layer norm zero (adaLN-Zero).

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
    """

    def __init__(self, embedding_dim: 'int', dim: 'int'):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, 12 * dim, bias=True)
        self.norm_x = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-05)
        self.norm_c = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-05)

    def forward(self, x: 'torch.Tensor', context: 'torch.Tensor', emb: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        emb = self.linear(self.silu(emb))
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp, c_shift_msa, c_scale_msa, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = emb.chunk(12, dim=1)
        normed_x = self.norm_x(x)
        normed_context = self.norm_c(context)
        x = normed_x * (1 + scale_msa[:, None]) + shift_msa[:, None]
        context = normed_context * (1 + c_scale_msa[:, None]) + c_shift_msa[:, None]
        return x, gate_msa, shift_mlp, scale_mlp, gate_mlp, context, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp


class CogVideoXLayerNormZero(nn.Module):

    def __init__(self, conditioning_dim: 'int', embedding_dim: 'int', elementwise_affine: 'bool'=True, eps: 'float'=1e-05, bias: 'bool'=True) ->None:
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(conditioning_dim, 6 * embedding_dim, bias=bias)
        self.norm = nn.LayerNorm(embedding_dim, eps=eps, elementwise_affine=elementwise_affine)

    def forward(self, hidden_states: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor', temb: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor]:
        shift, scale, gate, enc_shift, enc_scale, enc_gate = self.linear(self.silu(temb)).chunk(6, dim=1)
        hidden_states = self.norm(hidden_states) * (1 + scale)[:, None, :] + shift[:, None, :]
        encoder_hidden_states = self.norm(encoder_hidden_states) * (1 + enc_scale)[:, None, :] + enc_shift[:, None, :]
        return hidden_states, encoder_hidden_states, gate[:, None, :], enc_gate[:, None, :]


class GlobalResponseNorm(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))
        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))

    def forward(self, x):
        agg_norm = torch.norm(x, p=2, dim=(1, 2), keepdim=True)
        stand_div_norm = agg_norm / (agg_norm.mean(dim=-1, keepdim=True) + 1e-06)
        return self.gamma * (x * stand_div_norm) + self.beta + x


def rearrange_dims(tensor: 'torch.Tensor') ->torch.Tensor:
    if len(tensor.shape) == 2:
        return tensor[:, :, None]
    if len(tensor.shape) == 3:
        return tensor[:, :, None, :]
    elif len(tensor.shape) == 4:
        return tensor[:, :, 0, :]
    else:
        raise ValueError(f'`len(tensor)`: {len(tensor)} has to be 2, 3 or 4.')


class Conv1dBlock(nn.Module):
    """
    Conv1d --> GroupNorm --> Mish

    Parameters:
        inp_channels (`int`): Number of input channels.
        out_channels (`int`): Number of output channels.
        kernel_size (`int` or `tuple`): Size of the convolving kernel.
        n_groups (`int`, default `8`): Number of groups to separate the channels into.
        activation (`str`, defaults to `mish`): Name of the activation function.
    """

    def __init__(self, inp_channels: 'int', out_channels: 'int', kernel_size: 'Union[int, Tuple[int, int]]', n_groups: 'int'=8, activation: 'str'='mish'):
        super().__init__()
        self.conv1d = nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2)
        self.group_norm = nn.GroupNorm(n_groups, out_channels)
        self.mish = get_activation(activation)

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        intermediate_repr = self.conv1d(inputs)
        intermediate_repr = rearrange_dims(intermediate_repr)
        intermediate_repr = self.group_norm(intermediate_repr)
        intermediate_repr = rearrange_dims(intermediate_repr)
        output = self.mish(intermediate_repr)
        return output


class ResidualTemporalBlock1D(nn.Module):
    """
    Residual 1D block with temporal convolutions.

    Parameters:
        inp_channels (`int`): Number of input channels.
        out_channels (`int`): Number of output channels.
        embed_dim (`int`): Embedding dimension.
        kernel_size (`int` or `tuple`): Size of the convolving kernel.
        activation (`str`, defaults `mish`): It is possible to choose the right activation function.
    """

    def __init__(self, inp_channels: 'int', out_channels: 'int', embed_dim: 'int', kernel_size: 'Union[int, Tuple[int, int]]'=5, activation: 'str'='mish'):
        super().__init__()
        self.conv_in = Conv1dBlock(inp_channels, out_channels, kernel_size)
        self.conv_out = Conv1dBlock(out_channels, out_channels, kernel_size)
        self.time_emb_act = get_activation(activation)
        self.time_emb = nn.Linear(embed_dim, out_channels)
        self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) if inp_channels != out_channels else nn.Identity()

    def forward(self, inputs: 'torch.Tensor', t: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            inputs : [ batch_size x inp_channels x horizon ]
            t : [ batch_size x embed_dim ]

        returns:
            out : [ batch_size x out_channels x horizon ]
        """
        t = self.time_emb_act(t)
        t = self.time_emb(t)
        out = self.conv_in(inputs) + rearrange_dims(t)
        out = self.conv_out(out)
        return out + self.residual_conv(inputs)


class TemporalConvLayer(nn.Module):
    """
    Temporal convolutional layer that can be used for video (sequence of images) input Code mostly copied from:
    https://github.com/modelscope/modelscope/blob/1509fdb973e5871f37148a4b5e5964cafd43e64d/modelscope/models/multi_modal/video_synthesis/unet_sd.py#L1016

    Parameters:
        in_dim (`int`): Number of input channels.
        out_dim (`int`): Number of output channels.
        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.
    """

    def __init__(self, in_dim: 'int', out_dim: 'Optional[int]'=None, dropout: 'float'=0.0, norm_num_groups: 'int'=32):
        super().__init__()
        out_dim = out_dim or in_dim
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.conv1 = nn.Sequential(nn.GroupNorm(norm_num_groups, in_dim), nn.SiLU(), nn.Conv3d(in_dim, out_dim, (3, 1, 1), padding=(1, 0, 0)))
        self.conv2 = nn.Sequential(nn.GroupNorm(norm_num_groups, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))
        self.conv3 = nn.Sequential(nn.GroupNorm(norm_num_groups, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))
        self.conv4 = nn.Sequential(nn.GroupNorm(norm_num_groups, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))
        nn.init.zeros_(self.conv4[-1].weight)
        nn.init.zeros_(self.conv4[-1].bias)

    def forward(self, hidden_states: 'torch.Tensor', num_frames: 'int'=1) ->torch.Tensor:
        hidden_states = hidden_states[None, :].reshape((-1, num_frames) + hidden_states.shape[1:]).permute(0, 2, 1, 3, 4)
        identity = hidden_states
        hidden_states = self.conv1(hidden_states)
        hidden_states = self.conv2(hidden_states)
        hidden_states = self.conv3(hidden_states)
        hidden_states = self.conv4(hidden_states)
        hidden_states = identity + hidden_states
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).reshape((hidden_states.shape[0] * hidden_states.shape[2], -1) + hidden_states.shape[3:])
        return hidden_states


class AuraFlowPatchEmbed(nn.Module):

    def __init__(self, height=224, width=224, patch_size=16, in_channels=3, embed_dim=768, pos_embed_max_size=None):
        super().__init__()
        self.num_patches = height // patch_size * (width // patch_size)
        self.pos_embed_max_size = pos_embed_max_size
        self.proj = nn.Linear(patch_size * patch_size * in_channels, embed_dim)
        self.pos_embed = nn.Parameter(torch.randn(1, pos_embed_max_size, embed_dim) * 0.1)
        self.patch_size = patch_size
        self.height, self.width = height // patch_size, width // patch_size
        self.base_size = height // patch_size

    def pe_selection_index_based_on_dim(self, h, w):
        h_p, w_p = h // self.patch_size, w // self.patch_size
        original_pe_indexes = torch.arange(self.pos_embed.shape[1])
        h_max, w_max = int(self.pos_embed_max_size ** 0.5), int(self.pos_embed_max_size ** 0.5)
        original_pe_indexes = original_pe_indexes.view(h_max, w_max)
        starth = h_max // 2 - h_p // 2
        endh = starth + h_p
        startw = w_max // 2 - w_p // 2
        endw = startw + w_p
        original_pe_indexes = original_pe_indexes[starth:endh, startw:endw]
        return original_pe_indexes.flatten()

    def forward(self, latent):
        batch_size, num_channels, height, width = latent.size()
        latent = latent.view(batch_size, num_channels, height // self.patch_size, self.patch_size, width // self.patch_size, self.patch_size)
        latent = latent.permute(0, 2, 4, 1, 3, 5).flatten(-3).flatten(1, 2)
        latent = self.proj(latent)
        pe_index = self.pe_selection_index_based_on_dim(height, width)
        return latent + self.pos_embed[:, pe_index]


def find_multiple(n: 'int', k: 'int') ->int:
    if n % k == 0:
        return n
    return n + k - n % k


class AuraFlowFeedForward(nn.Module):

    def __init__(self, dim, hidden_dim=None) ->None:
        super().__init__()
        if hidden_dim is None:
            hidden_dim = 4 * dim
        final_hidden_dim = int(2 * hidden_dim / 3)
        final_hidden_dim = find_multiple(final_hidden_dim, 256)
        self.linear_1 = nn.Linear(dim, final_hidden_dim, bias=False)
        self.linear_2 = nn.Linear(dim, final_hidden_dim, bias=False)
        self.out_projection = nn.Linear(final_hidden_dim, dim, bias=False)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x = F.silu(self.linear_1(x)) * self.linear_2(x)
        x = self.out_projection(x)
        return x


class AuraFlowPreFinalBlock(nn.Module):

    def __init__(self, embedding_dim: 'int', conditioning_embedding_dim: 'int'):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(conditioning_embedding_dim, embedding_dim * 2, bias=False)

    def forward(self, x: 'torch.Tensor', conditioning_embedding: 'torch.Tensor') ->torch.Tensor:
        emb = self.linear(self.silu(conditioning_embedding))
        scale, shift = torch.chunk(emb, 2, dim=1)
        x = x * (1 + scale)[:, None, :] + shift[:, None, :]
        return x


class AuraFlowAttnProcessor2_0:
    """Attention processor used typically in processing Aura Flow."""

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention') and is_torch_version('<', '2.1'):
            raise ImportError('AuraFlowAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to at least 2.1 or above as we use `scale` in `F.scaled_dot_product_attention()`. ')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'torch.FloatTensor'=None, *args, **kwargs) ->torch.FloatTensor:
        batch_size = hidden_states.shape[0]
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        if encoder_hidden_states is not None:
            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)
            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim)
        key = key.view(batch_size, -1, attn.heads, head_dim)
        value = value.view(batch_size, -1, attn.heads, head_dim)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if encoder_hidden_states is not None:
            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(batch_size, -1, attn.heads, head_dim)
            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(batch_size, -1, attn.heads, head_dim)
            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(batch_size, -1, attn.heads, head_dim)
            if attn.norm_added_q is not None:
                encoder_hidden_states_query_proj = attn.norm_added_q(encoder_hidden_states_query_proj)
            if attn.norm_added_k is not None:
                encoder_hidden_states_key_proj = attn.norm_added_q(encoder_hidden_states_key_proj)
            query = torch.cat([encoder_hidden_states_query_proj, query], dim=1)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        query = query.transpose(1, 2)
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, scale=attn.scale, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        if encoder_hidden_states is not None:
            hidden_states, encoder_hidden_states = hidden_states[:, encoder_hidden_states.shape[1]:], hidden_states[:, :encoder_hidden_states.shape[1]]
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if encoder_hidden_states is not None:
            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
        if encoder_hidden_states is not None:
            return hidden_states, encoder_hidden_states
        else:
            return hidden_states


class AuraFlowTransformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


def apply_rotary_emb(x: 'torch.Tensor', freqs_cis: 'Union[torch.Tensor, Tuple[torch.Tensor]]', use_real: 'bool'=True, use_real_unbind_dim: 'int'=-1) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Apply rotary embeddings to input tensors using the given frequency tensor. This function applies rotary embeddings
    to the given query or key 'x' tensors using the provided frequency tensor 'freqs_cis'. The input tensors are
    reshaped as complex numbers, and the frequency tensor is reshaped for broadcasting compatibility. The resulting
    tensors contain rotary embeddings and are returned as real tensors.

    Args:
        x (`torch.Tensor`):
            Query or key tensor to apply rotary embeddings. [B, H, S, D] xk (torch.Tensor): Key tensor to apply
        freqs_cis (`Tuple[torch.Tensor]`): Precomputed frequency tensor for complex exponentials. ([S, D], [S, D],)

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.
    """
    if use_real:
        cos, sin = freqs_cis
        cos = cos[None, None]
        sin = sin[None, None]
        cos, sin = cos, sin
        if use_real_unbind_dim == -1:
            x_real, x_imag = x.reshape(*x.shape[:-1], -1, 2).unbind(-1)
            x_rotated = torch.stack([-x_imag, x_real], dim=-1).flatten(3)
        elif use_real_unbind_dim == -2:
            x_real, x_imag = x.reshape(*x.shape[:-1], 2, -1).unbind(-2)
            x_rotated = torch.cat([-x_imag, x_real], dim=-1)
        else:
            raise ValueError(f'`use_real_unbind_dim={use_real_unbind_dim}` but should be -1 or -2.')
        out = x.float() * cos + x_rotated.float() * sin
        return out
    else:
        x_rotated = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))
        freqs_cis = freqs_cis.unsqueeze(2)
        x_out = torch.view_as_real(x_rotated * freqs_cis).flatten(3)
        return x_out.type_as(x)


class CogVideoXAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention for the CogVideoX model. It applies a rotary embedding on
    query and key vectors, but does not include spatial normalization.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('CogVideoXAttnProcessor requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor', attention_mask: 'Optional[torch.Tensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        text_seq_length = encoder_hidden_states.size(1)
        hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if image_rotary_emb is not None:
            query[:, :, text_seq_length:] = apply_rotary_emb(query[:, :, text_seq_length:], image_rotary_emb)
            if not attn.is_cross_attention:
                key[:, :, text_seq_length:] = apply_rotary_emb(key[:, :, text_seq_length:], image_rotary_emb)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        encoder_hidden_states, hidden_states = hidden_states.split([text_seq_length, hidden_states.size(1) - text_seq_length], dim=1)
        return hidden_states, encoder_hidden_states


class CogVideoXTransformer3DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class DiTTransformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class AdaLayerNormShift(nn.Module):
    """
    Norm layer modified to incorporate timestep embeddings.

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
    """

    def __init__(self, embedding_dim: 'int', elementwise_affine=True, eps=1e-06):
        super().__init__()
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, embedding_dim)
        self.norm = FP32LayerNorm(embedding_dim, elementwise_affine=elementwise_affine, eps=eps)

    def forward(self, x: 'torch.Tensor', emb: 'torch.Tensor') ->torch.Tensor:
        shift = self.linear(self.silu(emb.to(torch.float32)))
        x = self.norm(x) + shift.unsqueeze(dim=1)
        return x


class HunyuanAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the HunyuanDiT model. It applies a s normalization layer and rotary embedding on query and key vector.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            if not attn.is_cross_attention:
                key = apply_rotary_emb(key, image_rotary_emb)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class HunyuanDiT2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class LatteTransformer3DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class LuminaAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the LuminaNextDiT model. It applies a s normalization layer and rotary embedding on query and key vector.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor', attention_mask: 'Optional[torch.Tensor]'=None, query_rotary_emb: 'Optional[torch.Tensor]'=None, key_rotary_emb: 'Optional[torch.Tensor]'=None, base_sequence_length: 'Optional[int]'=None) ->torch.Tensor:
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        query = attn.to_q(hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query_dim = query.shape[-1]
        inner_dim = key.shape[-1]
        head_dim = query_dim // attn.heads
        dtype = query.dtype
        kv_heads = inner_dim // head_dim
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        query = query.view(batch_size, -1, attn.heads, head_dim)
        key = key.view(batch_size, -1, kv_heads, head_dim)
        value = value.view(batch_size, -1, kv_heads, head_dim)
        if query_rotary_emb is not None:
            query = apply_rotary_emb(query, query_rotary_emb, use_real=False)
        if key_rotary_emb is not None:
            key = apply_rotary_emb(key, key_rotary_emb, use_real=False)
        query, key = query, key
        if key_rotary_emb is None:
            softmax_scale = None
        elif base_sequence_length is not None:
            softmax_scale = math.sqrt(math.log(sequence_length, base_sequence_length)) * attn.scale
        else:
            softmax_scale = attn.scale
        n_rep = attn.heads // kv_heads
        if n_rep >= 1:
            key = key.unsqueeze(3).repeat(1, 1, 1, n_rep, 1).flatten(2, 3)
            value = value.unsqueeze(3).repeat(1, 1, 1, n_rep, 1).flatten(2, 3)
        attention_mask = attention_mask.bool().view(batch_size, 1, 1, -1)
        attention_mask = attention_mask.expand(-1, attn.heads, sequence_length, -1)
        query = query.transpose(1, 2)
        key = key.transpose(1, 2)
        value = value.transpose(1, 2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, scale=softmax_scale)
        hidden_states = hidden_states.transpose(1, 2)
        return hidden_states


class LuminaNextDiTBlock(nn.Module):
    """
    A LuminaNextDiTBlock for LuminaNextDiT2DModel.

    Parameters:
        dim (`int`): Embedding dimension of the input features.
        num_attention_heads (`int`): Number of attention heads.
        num_kv_heads (`int`):
            Number of attention heads in key and value features (if using GQA), or set to None for the same as query.
        multiple_of (`int`): The number of multiple of ffn layer.
        ffn_dim_multiplier (`float`): The multipier factor of ffn layer dimension.
        norm_eps (`float`): The eps for norm layer.
        qk_norm (`bool`): normalization for query and key.
        cross_attention_dim (`int`): Cross attention embedding dimension of the input text prompt hidden_states.
        norm_elementwise_affine (`bool`, *optional*, defaults to True),
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', num_kv_heads: 'int', multiple_of: 'int', ffn_dim_multiplier: 'float', norm_eps: 'float', qk_norm: 'bool', cross_attention_dim: 'int', norm_elementwise_affine: 'bool'=True) ->None:
        super().__init__()
        self.head_dim = dim // num_attention_heads
        self.gate = nn.Parameter(torch.zeros([num_attention_heads]))
        self.attn1 = Attention(query_dim=dim, cross_attention_dim=None, dim_head=dim // num_attention_heads, qk_norm='layer_norm_across_heads' if qk_norm else None, heads=num_attention_heads, kv_heads=num_kv_heads, eps=1e-05, bias=False, out_bias=False, processor=LuminaAttnProcessor2_0())
        self.attn1.to_out = nn.Identity()
        self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim, dim_head=dim // num_attention_heads, qk_norm='layer_norm_across_heads' if qk_norm else None, heads=num_attention_heads, kv_heads=num_kv_heads, eps=1e-05, bias=False, out_bias=False, processor=LuminaAttnProcessor2_0())
        self.feed_forward = LuminaFeedForward(dim=dim, inner_dim=4 * dim, multiple_of=multiple_of, ffn_dim_multiplier=ffn_dim_multiplier)
        self.norm1 = LuminaRMSNormZero(embedding_dim=dim, norm_eps=norm_eps, norm_elementwise_affine=norm_elementwise_affine)
        self.ffn_norm1 = RMSNorm(dim, eps=norm_eps, elementwise_affine=norm_elementwise_affine)
        self.norm2 = RMSNorm(dim, eps=norm_eps, elementwise_affine=norm_elementwise_affine)
        self.ffn_norm2 = RMSNorm(dim, eps=norm_eps, elementwise_affine=norm_elementwise_affine)
        self.norm1_context = RMSNorm(cross_attention_dim, eps=norm_eps, elementwise_affine=norm_elementwise_affine)

    def forward(self, hidden_states: 'torch.Tensor', attention_mask: 'torch.Tensor', image_rotary_emb: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor', encoder_mask: 'torch.Tensor', temb: 'torch.Tensor', cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None):
        """
        Perform a forward pass through the LuminaNextDiTBlock.

        Parameters:
            hidden_states (`torch.Tensor`): The input of hidden_states for LuminaNextDiTBlock.
            attention_mask (`torch.Tensor): The input of hidden_states corresponse attention mask.
            image_rotary_emb (`torch.Tensor`): Precomputed cosine and sine frequencies.
            encoder_hidden_states: (`torch.Tensor`): The hidden_states of text prompt are processed by Gemma encoder.
            encoder_mask (`torch.Tensor`): The hidden_states of text prompt attention mask.
            temb (`torch.Tensor`): Timestep embedding with text prompt embedding.
            cross_attention_kwargs (`Dict[str, Any]`): kwargs for cross attention.
        """
        residual = hidden_states
        norm_hidden_states, gate_msa, scale_mlp, gate_mlp = self.norm1(hidden_states, temb)
        self_attn_output = self.attn1(hidden_states=norm_hidden_states, encoder_hidden_states=norm_hidden_states, attention_mask=attention_mask, query_rotary_emb=image_rotary_emb, key_rotary_emb=image_rotary_emb, **cross_attention_kwargs)
        norm_encoder_hidden_states = self.norm1_context(encoder_hidden_states)
        cross_attn_output = self.attn2(hidden_states=norm_hidden_states, encoder_hidden_states=norm_encoder_hidden_states, attention_mask=encoder_mask, query_rotary_emb=image_rotary_emb, key_rotary_emb=None, **cross_attention_kwargs)
        cross_attn_output = cross_attn_output * self.gate.tanh().view(1, 1, -1, 1)
        mixed_attn_output = self_attn_output + cross_attn_output
        mixed_attn_output = mixed_attn_output.flatten(-2)
        hidden_states = self.attn2.to_out[0](mixed_attn_output)
        hidden_states = residual + gate_msa.unsqueeze(1).tanh() * self.norm2(hidden_states)
        mlp_output = self.feed_forward(self.ffn_norm1(hidden_states) * (1 + scale_mlp.unsqueeze(1)))
        hidden_states = hidden_states + gate_mlp.unsqueeze(1).tanh() * self.ffn_norm2(mlp_output)
        return hidden_states


class LuminaNextDiT2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class PixArtTransformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class PriorTransformer(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class StableAudioGaussianFourierProjection(nn.Module):
    """Gaussian Fourier embeddings for noise levels."""

    def __init__(self, embedding_size: 'int'=256, scale: 'float'=1.0, set_W_to_weight=True, log=True, flip_sin_to_cos=False):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)
        self.log = log
        self.flip_sin_to_cos = flip_sin_to_cos
        if set_W_to_weight:
            del self.weight
            self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)
            self.weight = self.W
            del self.W

    def forward(self, x):
        if self.log:
            x = torch.log(x)
        x_proj = 2 * np.pi * x[:, None] @ self.weight[None, :]
        if self.flip_sin_to_cos:
            out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)
        else:
            out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
        return out


class StableAudioAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the Stable Audio model. It applies rotary embedding on query and key vector, and allows MHA, GQA or MQA.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('StableAudioAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def apply_partial_rotary_emb(self, x: 'torch.Tensor', freqs_cis: 'Tuple[torch.Tensor]') ->torch.Tensor:
        rot_dim = freqs_cis[0].shape[-1]
        x_to_rotate, x_unrotated = x[..., :rot_dim], x[..., rot_dim:]
        x_rotated = apply_rotary_emb(x_to_rotate, freqs_cis, use_real=True, use_real_unbind_dim=-2)
        out = torch.cat((x_rotated, x_unrotated), dim=-1)
        return out

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        head_dim = query.shape[-1] // attn.heads
        kv_heads = key.shape[-1] // head_dim
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, kv_heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, kv_heads, head_dim).transpose(1, 2)
        if kv_heads != attn.heads:
            heads_per_kv_head = attn.heads // kv_heads
            key = torch.repeat_interleave(key, heads_per_kv_head, dim=1)
            value = torch.repeat_interleave(value, heads_per_kv_head, dim=1)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if rotary_emb is not None:
            query_dtype = query.dtype
            key_dtype = key.dtype
            query = query
            key = key
            rot_dim = rotary_emb[0].shape[-1]
            query_to_rotate, query_unrotated = query[..., :rot_dim], query[..., rot_dim:]
            query_rotated = apply_rotary_emb(query_to_rotate, rotary_emb, use_real=True, use_real_unbind_dim=-2)
            query = torch.cat((query_rotated, query_unrotated), dim=-1)
            if not attn.is_cross_attention:
                key_to_rotate, key_unrotated = key[..., :rot_dim], key[..., rot_dim:]
                key_rotated = apply_rotary_emb(key_to_rotate, rotary_emb, use_real=True, use_real_unbind_dim=-2)
                key = torch.cat((key_rotated, key_unrotated), dim=-1)
            query = query
            key = key
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class StableAudioDiTModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class T5FilmDecoder(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class T5LayerNorm(nn.Module):
    """
    T5 style layer normalization module.

    Args:
        hidden_size (`int`):
            Size of the input hidden states.
        eps (`float`, `optional`, defaults to `1e-6`):
            A small value used for numerical stability to avoid dividing by zero.
    """

    def __init__(self, hidden_size: 'int', eps: 'float'=1e-06):
        """
        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        if self.weight.dtype in [torch.float16, torch.bfloat16]:
            hidden_states = hidden_states
        return self.weight * hidden_states


class T5LayerCrossAttention(nn.Module):
    """
    T5 style cross-attention layer.

    Args:
        d_model (`int`):
            Size of the input hidden states.
        d_kv (`int`):
            Size of the key-value projection vectors.
        num_heads (`int`):
            Number of attention heads.
        dropout_rate (`float`):
            Dropout probability.
        layer_norm_epsilon (`float`):
            A small value used for numerical stability to avoid dividing by zero.
    """

    def __init__(self, d_model: 'int', d_kv: 'int', num_heads: 'int', dropout_rate: 'float', layer_norm_epsilon: 'float'):
        super().__init__()
        self.attention = Attention(query_dim=d_model, heads=num_heads, dim_head=d_kv, out_bias=False, scale_qk=False)
        self.layer_norm = T5LayerNorm(d_model, eps=layer_norm_epsilon)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, hidden_states: 'torch.Tensor', key_value_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        normed_hidden_states = self.layer_norm(hidden_states)
        attention_output = self.attention(normed_hidden_states, encoder_hidden_states=key_value_states, attention_mask=attention_mask.squeeze(1))
        layer_output = hidden_states + self.dropout(attention_output)
        return layer_output


class NewGELUActivation(nn.Module):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    """

    def forward(self, input: 'torch.Tensor') ->torch.Tensor:
        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))


class T5DenseGatedActDense(nn.Module):
    """
    T5 style feed-forward layer with gated activations and dropout.

    Args:
        d_model (`int`):
            Size of the input hidden states.
        d_ff (`int`):
            Size of the intermediate feed-forward layer.
        dropout_rate (`float`):
            Dropout probability.
    """

    def __init__(self, d_model: 'int', d_ff: 'int', dropout_rate: 'float'):
        super().__init__()
        self.wi_0 = nn.Linear(d_model, d_ff, bias=False)
        self.wi_1 = nn.Linear(d_model, d_ff, bias=False)
        self.wo = nn.Linear(d_ff, d_model, bias=False)
        self.dropout = nn.Dropout(dropout_rate)
        self.act = NewGELUActivation()

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        hidden_gelu = self.act(self.wi_0(hidden_states))
        hidden_linear = self.wi_1(hidden_states)
        hidden_states = hidden_gelu * hidden_linear
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.wo(hidden_states)
        return hidden_states


class T5FiLMLayer(nn.Module):
    """
    T5 style FiLM Layer.

    Args:
        in_features (`int`):
            Number of input features.
        out_features (`int`):
            Number of output features.
    """

    def __init__(self, in_features: 'int', out_features: 'int'):
        super().__init__()
        self.scale_bias = nn.Linear(in_features, out_features * 2, bias=False)

    def forward(self, x: 'torch.Tensor', conditioning_emb: 'torch.Tensor') ->torch.Tensor:
        emb = self.scale_bias(conditioning_emb)
        scale, shift = torch.chunk(emb, 2, -1)
        x = x * (1 + scale) + shift
        return x


class T5LayerFFCond(nn.Module):
    """
    T5 style feed-forward conditional layer.

    Args:
        d_model (`int`):
            Size of the input hidden states.
        d_ff (`int`):
            Size of the intermediate feed-forward layer.
        dropout_rate (`float`):
            Dropout probability.
        layer_norm_epsilon (`float`):
            A small value used for numerical stability to avoid dividing by zero.
    """

    def __init__(self, d_model: 'int', d_ff: 'int', dropout_rate: 'float', layer_norm_epsilon: 'float'):
        super().__init__()
        self.DenseReluDense = T5DenseGatedActDense(d_model=d_model, d_ff=d_ff, dropout_rate=dropout_rate)
        self.film = T5FiLMLayer(in_features=d_model * 4, out_features=d_model)
        self.layer_norm = T5LayerNorm(d_model, eps=layer_norm_epsilon)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, hidden_states: 'torch.Tensor', conditioning_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        forwarded_states = self.layer_norm(hidden_states)
        if conditioning_emb is not None:
            forwarded_states = self.film(forwarded_states, conditioning_emb)
        forwarded_states = self.DenseReluDense(forwarded_states)
        hidden_states = hidden_states + self.dropout(forwarded_states)
        return hidden_states


class T5LayerSelfAttentionCond(nn.Module):
    """
    T5 style self-attention layer with conditioning.

    Args:
        d_model (`int`):
            Size of the input hidden states.
        d_kv (`int`):
            Size of the key-value projection vectors.
        num_heads (`int`):
            Number of attention heads.
        dropout_rate (`float`):
            Dropout probability.
    """

    def __init__(self, d_model: 'int', d_kv: 'int', num_heads: 'int', dropout_rate: 'float'):
        super().__init__()
        self.layer_norm = T5LayerNorm(d_model)
        self.FiLMLayer = T5FiLMLayer(in_features=d_model * 4, out_features=d_model)
        self.attention = Attention(query_dim=d_model, heads=num_heads, dim_head=d_kv, out_bias=False, scale_qk=False)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, hidden_states: 'torch.Tensor', conditioning_emb: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        normed_hidden_states = self.layer_norm(hidden_states)
        if conditioning_emb is not None:
            normed_hidden_states = self.FiLMLayer(normed_hidden_states, conditioning_emb)
        attention_output = self.attention(normed_hidden_states)
        hidden_states = hidden_states + self.dropout(attention_output)
        return hidden_states


class DecoderLayer(nn.Module):
    """
    T5 decoder layer.

    Args:
        d_model (`int`):
            Size of the input hidden states.
        d_kv (`int`):
            Size of the key-value projection vectors.
        num_heads (`int`):
            Number of attention heads.
        d_ff (`int`):
            Size of the intermediate feed-forward layer.
        dropout_rate (`float`):
            Dropout probability.
        layer_norm_epsilon (`float`, *optional*, defaults to `1e-6`):
            A small value used for numerical stability to avoid dividing by zero.
    """

    def __init__(self, d_model: 'int', d_kv: 'int', num_heads: 'int', d_ff: 'int', dropout_rate: 'float', layer_norm_epsilon: 'float'=1e-06):
        super().__init__()
        self.layer = nn.ModuleList()
        self.layer.append(T5LayerSelfAttentionCond(d_model=d_model, d_kv=d_kv, num_heads=num_heads, dropout_rate=dropout_rate))
        self.layer.append(T5LayerCrossAttention(d_model=d_model, d_kv=d_kv, num_heads=num_heads, dropout_rate=dropout_rate, layer_norm_epsilon=layer_norm_epsilon))
        self.layer.append(T5LayerFFCond(d_model=d_model, d_ff=d_ff, dropout_rate=dropout_rate, layer_norm_epsilon=layer_norm_epsilon))

    def forward(self, hidden_states: 'torch.Tensor', conditioning_emb: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, encoder_decoder_position_bias=None) ->Tuple[torch.Tensor]:
        hidden_states = self.layer[0](hidden_states, conditioning_emb=conditioning_emb, attention_mask=attention_mask)
        if encoder_hidden_states is not None:
            encoder_extended_attention_mask = torch.where(encoder_attention_mask > 0, 0, -10000000000.0)
            hidden_states = self.layer[1](hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_extended_attention_mask)
        hidden_states = self.layer[-1](hidden_states, conditioning_emb)
        return hidden_states,


def apply_rotary_emb_allegro(x: 'torch.Tensor', freqs_cis, positions):

    def apply_1d_rope(tokens, pos, cos, sin):
        cos = F.embedding(pos, cos)[:, None, :, :]
        sin = F.embedding(pos, sin)[:, None, :, :]
        x1, x2 = tokens[..., :tokens.shape[-1] // 2], tokens[..., tokens.shape[-1] // 2:]
        tokens_rotated = torch.cat((-x2, x1), dim=-1)
        return tokens.float() * cos + tokens_rotated.float() * sin
    (t_cos, t_sin), (h_cos, h_sin), (w_cos, w_sin) = freqs_cis
    t, h, w = x.chunk(3, dim=-1)
    t = apply_1d_rope(t, positions[0], t_cos, t_sin)
    h = apply_1d_rope(h, positions[1], h_cos, h_sin)
    w = apply_1d_rope(w, positions[2], w_cos, w_sin)
    x = torch.cat([t, h, w], dim=-1)
    return x


class AllegroAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the Allegro model. It applies a normalization layer and rotary embedding on the query and key vector.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AllegroAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if image_rotary_emb is not None and not attn.is_cross_attention:
            query = apply_rotary_emb_allegro(query, image_rotary_emb[0], image_rotary_emb[1])
            key = apply_rotary_emb_allegro(key, image_rotary_emb[0], image_rotary_emb[1])
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class AllegroTransformer3DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class CogView3PlusTransformerBlock(nn.Module):
    """
    Transformer block used in [CogView](https://github.com/THUDM/CogView3) model.

    Args:
        dim (`int`):
            The number of channels in the input and output.
        num_attention_heads (`int`):
            The number of heads to use for multi-head attention.
        attention_head_dim (`int`):
            The number of channels in each head.
        time_embed_dim (`int`):
            The number of channels in timestep embedding.
    """

    def __init__(self, dim: 'int'=2560, num_attention_heads: 'int'=64, attention_head_dim: 'int'=40, time_embed_dim: 'int'=512):
        super().__init__()
        self.norm1 = CogView3PlusAdaLayerNormZeroTextImage(embedding_dim=time_embed_dim, dim=dim)
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, out_dim=dim, bias=True, qk_norm='layer_norm', elementwise_affine=False, eps=1e-06, processor=CogVideoXAttnProcessor2_0())
        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-05)
        self.norm2_context = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-05)
        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn='gelu-approximate')

    def forward(self, hidden_states: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor', emb: 'torch.Tensor') ->torch.Tensor:
        text_seq_length = encoder_hidden_states.size(1)
        norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp, norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = self.norm1(hidden_states, encoder_hidden_states, emb)
        attn_hidden_states, attn_encoder_hidden_states = self.attn1(hidden_states=norm_hidden_states, encoder_hidden_states=norm_encoder_hidden_states)
        hidden_states = hidden_states + gate_msa.unsqueeze(1) * attn_hidden_states
        encoder_hidden_states = encoder_hidden_states + c_gate_msa.unsqueeze(1) * attn_encoder_hidden_states
        norm_hidden_states = self.norm2(hidden_states)
        norm_hidden_states = norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]
        norm_encoder_hidden_states = self.norm2_context(encoder_hidden_states)
        norm_encoder_hidden_states = norm_encoder_hidden_states * (1 + c_scale_mlp[:, None]) + c_shift_mlp[:, None]
        norm_hidden_states = torch.cat([norm_encoder_hidden_states, norm_hidden_states], dim=1)
        ff_output = self.ff(norm_hidden_states)
        hidden_states = hidden_states + gate_mlp.unsqueeze(1) * ff_output[:, text_seq_length:]
        encoder_hidden_states = encoder_hidden_states + c_gate_mlp.unsqueeze(1) * ff_output[:, :text_seq_length]
        if hidden_states.dtype == torch.float16:
            hidden_states = hidden_states.clip(-65504, 65504)
        if encoder_hidden_states.dtype == torch.float16:
            encoder_hidden_states = encoder_hidden_states.clip(-65504, 65504)
        return hidden_states, encoder_hidden_states


class CogView3PlusTransformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class FluxAttnProcessor2_0:
    """Attention processor used typically in processing the SD3-like self-attention projections."""

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('FluxAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'torch.FloatTensor'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.FloatTensor:
        batch_size, _, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if encoder_hidden_states is not None:
            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)
            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            if attn.norm_added_q is not None:
                encoder_hidden_states_query_proj = attn.norm_added_q(encoder_hidden_states_query_proj)
            if attn.norm_added_k is not None:
                encoder_hidden_states_key_proj = attn.norm_added_k(encoder_hidden_states_key_proj)
            query = torch.cat([encoder_hidden_states_query_proj, query], dim=2)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=2)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=2)
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            key = apply_rotary_emb(key, image_rotary_emb)
        hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        if encoder_hidden_states is not None:
            encoder_hidden_states, hidden_states = hidden_states[:, :encoder_hidden_states.shape[1]], hidden_states[:, encoder_hidden_states.shape[1]:]
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
            return hidden_states, encoder_hidden_states
        else:
            return hidden_states


class FluxAttnProcessor2_0_NPU:
    """Attention processor used typically in processing the SD3-like self-attention projections."""

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('FluxAttnProcessor2_0_NPU requires PyTorch 2.0 and torch NPU, to use it, please upgrade PyTorch to 2.0 and install torch NPU')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'torch.FloatTensor'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.FloatTensor:
        batch_size, _, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if encoder_hidden_states is not None:
            encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)
            encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
            encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
            encoder_hidden_states_query_proj = encoder_hidden_states_query_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            encoder_hidden_states_key_proj = encoder_hidden_states_key_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            encoder_hidden_states_value_proj = encoder_hidden_states_value_proj.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            if attn.norm_added_q is not None:
                encoder_hidden_states_query_proj = attn.norm_added_q(encoder_hidden_states_query_proj)
            if attn.norm_added_k is not None:
                encoder_hidden_states_key_proj = attn.norm_added_k(encoder_hidden_states_key_proj)
            query = torch.cat([encoder_hidden_states_query_proj, query], dim=2)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=2)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=2)
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            key = apply_rotary_emb(key, image_rotary_emb)
        if query.dtype in (torch.float16, torch.bfloat16):
            hidden_states = torch_npu.npu_fusion_attention(query, key, value, attn.heads, input_layout='BNSD', pse=None, scale=1.0 / math.sqrt(query.shape[-1]), pre_tockens=65536, next_tockens=65536, keep_prob=1.0, sync=False, inner_precise=0)[0]
        else:
            hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        if encoder_hidden_states is not None:
            encoder_hidden_states, hidden_states = hidden_states[:, :encoder_hidden_states.shape[1]], hidden_states[:, encoder_hidden_states.shape[1]:]
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
            return hidden_states, encoder_hidden_states
        else:
            return hidden_states


class FluxTransformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class MochiAttnProcessor2_0:
    """Attention processor used in Mochi."""

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('MochiAttnProcessor2_0 requires PyTorch 2.0. To use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'torch.Tensor', attention_mask: 'Optional[torch.Tensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        query = attn.to_q(hidden_states)
        key = attn.to_k(hidden_states)
        value = attn.to_v(hidden_states)
        query = query.unflatten(2, (attn.heads, -1))
        key = key.unflatten(2, (attn.heads, -1))
        value = value.unflatten(2, (attn.heads, -1))
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        encoder_query = attn.add_q_proj(encoder_hidden_states)
        encoder_key = attn.add_k_proj(encoder_hidden_states)
        encoder_value = attn.add_v_proj(encoder_hidden_states)
        encoder_query = encoder_query.unflatten(2, (attn.heads, -1))
        encoder_key = encoder_key.unflatten(2, (attn.heads, -1))
        encoder_value = encoder_value.unflatten(2, (attn.heads, -1))
        if attn.norm_added_q is not None:
            encoder_query = attn.norm_added_q(encoder_query)
        if attn.norm_added_k is not None:
            encoder_key = attn.norm_added_k(encoder_key)
        if image_rotary_emb is not None:

            def apply_rotary_emb(x, freqs_cos, freqs_sin):
                x_even = x[..., 0::2].float()
                x_odd = x[..., 1::2].float()
                cos = x_even * freqs_cos - x_odd * freqs_sin
                sin = x_even * freqs_sin + x_odd * freqs_cos
                return torch.stack([cos, sin], dim=-1).flatten(-2)
            query = apply_rotary_emb(query, *image_rotary_emb)
            key = apply_rotary_emb(key, *image_rotary_emb)
        query, key, value = query.transpose(1, 2), key.transpose(1, 2), value.transpose(1, 2)
        encoder_query, encoder_key, encoder_value = encoder_query.transpose(1, 2), encoder_key.transpose(1, 2), encoder_value.transpose(1, 2)
        sequence_length = query.size(2)
        encoder_sequence_length = encoder_query.size(2)
        query = torch.cat([query, encoder_query], dim=2)
        key = torch.cat([key, encoder_key], dim=2)
        value = torch.cat([value, encoder_value], dim=2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).flatten(2, 3)
        hidden_states = hidden_states
        hidden_states, encoder_hidden_states = hidden_states.split_with_sizes((sequence_length, encoder_sequence_length), dim=1)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if hasattr(attn, 'to_add_out'):
            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)
        return hidden_states, encoder_hidden_states


class MochiRoPE(nn.Module):
    """
    RoPE implementation used in [Mochi](https://huggingface.co/genmo/mochi-1-preview).

    Args:
        base_height (`int`, defaults to `192`):
            Base height used to compute interpolation scale for rotary positional embeddings.
        base_width (`int`, defaults to `192`):
            Base width used to compute interpolation scale for rotary positional embeddings.
    """

    def __init__(self, base_height: 'int'=192, base_width: 'int'=192) ->None:
        super().__init__()
        self.target_area = base_height * base_width

    def _centers(self, start, stop, num, device, dtype) ->torch.Tensor:
        edges = torch.linspace(start, stop, num + 1, device=device, dtype=dtype)
        return (edges[:-1] + edges[1:]) / 2

    def _get_positions(self, num_frames: 'int', height: 'int', width: 'int', device: 'Optional[torch.device]'=None, dtype: 'Optional[torch.dtype]'=None) ->torch.Tensor:
        scale = (self.target_area / (height * width)) ** 0.5
        t = torch.arange(num_frames, device=device, dtype=dtype)
        h = self._centers(-height * scale / 2, height * scale / 2, height, device, dtype)
        w = self._centers(-width * scale / 2, width * scale / 2, width, device, dtype)
        grid_t, grid_h, grid_w = torch.meshgrid(t, h, w, indexing='ij')
        positions = torch.stack([grid_t, grid_h, grid_w], dim=-1).view(-1, 3)
        return positions

    def _create_rope(self, freqs: 'torch.Tensor', pos: 'torch.Tensor') ->torch.Tensor:
        freqs = torch.einsum('nd,dhf->nhf', pos, freqs.float())
        freqs_cos = torch.cos(freqs)
        freqs_sin = torch.sin(freqs)
        return freqs_cos, freqs_sin

    def forward(self, pos_frequencies: 'torch.Tensor', num_frames: 'int', height: 'int', width: 'int', device: 'Optional[torch.device]'=None, dtype: 'Optional[torch.dtype]'=None) ->Tuple[torch.Tensor, torch.Tensor]:
        pos = self._get_positions(num_frames, height, width, device, dtype)
        rope_cos, rope_sin = self._create_rope(pos_frequencies, pos)
        return rope_cos, rope_sin


class MochiTransformer3DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class SD3Transformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class TransformerSpatioTemporalModel(nn.Module):
    """
    A Transformer model for video-like data.

    Parameters:
        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.
        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.
        in_channels (`int`, *optional*):
            The number of channels in the input and output (specify if the input is **continuous**).
        out_channels (`int`, *optional*):
            The number of channels in the output (specify if the input is **continuous**).
        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.
        cross_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.
    """

    def __init__(self, num_attention_heads: 'int'=16, attention_head_dim: 'int'=88, in_channels: 'int'=320, out_channels: 'Optional[int]'=None, num_layers: 'int'=1, cross_attention_dim: 'Optional[int]'=None):
        super().__init__()
        self.num_attention_heads = num_attention_heads
        self.attention_head_dim = attention_head_dim
        inner_dim = num_attention_heads * attention_head_dim
        self.inner_dim = inner_dim
        self.in_channels = in_channels
        self.norm = torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-06)
        self.proj_in = nn.Linear(in_channels, inner_dim)
        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(inner_dim, num_attention_heads, attention_head_dim, cross_attention_dim=cross_attention_dim) for d in range(num_layers)])
        time_mix_inner_dim = inner_dim
        self.temporal_transformer_blocks = nn.ModuleList([TemporalBasicTransformerBlock(inner_dim, time_mix_inner_dim, num_attention_heads, attention_head_dim, cross_attention_dim=cross_attention_dim) for _ in range(num_layers)])
        time_embed_dim = in_channels * 4
        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)
        self.time_proj = Timesteps(in_channels, True, 0)
        self.time_mixer = AlphaBlender(alpha=0.5, merge_strategy='learned_with_images')
        self.out_channels = in_channels if out_channels is None else out_channels
        self.proj_out = nn.Linear(inner_dim, in_channels)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None, return_dict: 'bool'=True):
        """
        Args:
            hidden_states (`torch.Tensor` of shape `(batch size, channel, height, width)`):
                Input hidden_states.
            num_frames (`int`):
                The number of frames to be processed per batch. This is used to reshape the hidden states.
            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):
                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
                self-attention.
            image_only_indicator (`torch.LongTensor` of shape `(batch size, num_frames)`, *optional*):
                A tensor indicating whether the input contains only images. 1 indicates that the input contains only
                images, 0 indicates that the input contains video frames.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~models.transformers.transformer_temporal.TransformerTemporalModelOutput`]
                instead of a plain tuple.

        Returns:
            [`~models.transformers.transformer_temporal.TransformerTemporalModelOutput`] or `tuple`:
                If `return_dict` is True, an
                [`~models.transformers.transformer_temporal.TransformerTemporalModelOutput`] is returned, otherwise a
                `tuple` where the first element is the sample tensor.
        """
        batch_frames, _, height, width = hidden_states.shape
        num_frames = image_only_indicator.shape[-1]
        batch_size = batch_frames // num_frames
        time_context = encoder_hidden_states
        time_context_first_timestep = time_context[None, :].reshape(batch_size, num_frames, -1, time_context.shape[-1])[:, 0]
        time_context = time_context_first_timestep[:, None].broadcast_to(batch_size, height * width, time_context.shape[-2], time_context.shape[-1])
        time_context = time_context.reshape(batch_size * height * width, -1, time_context.shape[-1])
        residual = hidden_states
        hidden_states = self.norm(hidden_states)
        inner_dim = hidden_states.shape[1]
        hidden_states = hidden_states.permute(0, 2, 3, 1).reshape(batch_frames, height * width, inner_dim)
        hidden_states = self.proj_in(hidden_states)
        num_frames_emb = torch.arange(num_frames, device=hidden_states.device)
        num_frames_emb = num_frames_emb.repeat(batch_size, 1)
        num_frames_emb = num_frames_emb.reshape(-1)
        t_emb = self.time_proj(num_frames_emb)
        t_emb = t_emb
        emb = self.time_pos_embed(t_emb)
        emb = emb[:, None, :]
        for block, temporal_block in zip(self.transformer_blocks, self.temporal_transformer_blocks):
            if self.training and self.gradient_checkpointing:
                hidden_states = torch.utils.checkpoint.checkpoint(block, hidden_states, None, encoder_hidden_states, None, use_reentrant=False)
            else:
                hidden_states = block(hidden_states, encoder_hidden_states=encoder_hidden_states)
            hidden_states_mix = hidden_states
            hidden_states_mix = hidden_states_mix + emb
            hidden_states_mix = temporal_block(hidden_states_mix, num_frames=num_frames, encoder_hidden_states=time_context)
            hidden_states = self.time_mixer(x_spatial=hidden_states, x_temporal=hidden_states_mix, image_only_indicator=image_only_indicator)
        hidden_states = self.proj_out(hidden_states)
        hidden_states = hidden_states.reshape(batch_frames, height, width, inner_dim).permute(0, 3, 1, 2).contiguous()
        output = hidden_states + residual
        if not return_dict:
            return output,
        return TransformerTemporalModelOutput(sample=output)


class UNet1DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class DownResnetBlock1D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'Optional[int]'=None, num_layers: 'int'=1, conv_shortcut: 'bool'=False, temb_channels: 'int'=32, groups: 'int'=32, groups_out: 'Optional[int]'=None, non_linearity: 'Optional[str]'=None, time_embedding_norm: 'str'='default', output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut
        self.time_embedding_norm = time_embedding_norm
        self.add_downsample = add_downsample
        self.output_scale_factor = output_scale_factor
        if groups_out is None:
            groups_out = groups
        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=temb_channels)]
        for _ in range(num_layers):
            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))
        self.resnets = nn.ModuleList(resnets)
        if non_linearity is None:
            self.nonlinearity = None
        else:
            self.nonlinearity = get_activation(non_linearity)
        self.downsample = None
        if add_downsample:
            self.downsample = Downsample1D(out_channels, use_conv=True, padding=1)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        output_states = ()
        hidden_states = self.resnets[0](hidden_states, temb)
        for resnet in self.resnets[1:]:
            hidden_states = resnet(hidden_states, temb)
        output_states += hidden_states,
        if self.nonlinearity is not None:
            hidden_states = self.nonlinearity(hidden_states)
        if self.downsample is not None:
            hidden_states = self.downsample(hidden_states)
        return hidden_states, output_states


class Upsample1D(nn.Module):
    """A 1D upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        use_conv_transpose (`bool`, default `False`):
            option to use a convolution transpose.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        name (`str`, default `conv`):
            name of the upsampling 1D layer.
    """

    def __init__(self, channels: 'int', use_conv: 'bool'=False, use_conv_transpose: 'bool'=False, out_channels: 'Optional[int]'=None, name: 'str'='conv'):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.use_conv_transpose = use_conv_transpose
        self.name = name
        self.conv = None
        if use_conv_transpose:
            self.conv = nn.ConvTranspose1d(channels, self.out_channels, 4, 2, 1)
        elif use_conv:
            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, padding=1)

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        assert inputs.shape[1] == self.channels
        if self.use_conv_transpose:
            return self.conv(inputs)
        outputs = F.interpolate(inputs, scale_factor=2.0, mode='nearest')
        if self.use_conv:
            outputs = self.conv(outputs)
        return outputs


class UpResnetBlock1D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'Optional[int]'=None, num_layers: 'int'=1, temb_channels: 'int'=32, groups: 'int'=32, groups_out: 'Optional[int]'=None, non_linearity: 'Optional[str]'=None, time_embedding_norm: 'str'='default', output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.time_embedding_norm = time_embedding_norm
        self.add_upsample = add_upsample
        self.output_scale_factor = output_scale_factor
        if groups_out is None:
            groups_out = groups
        resnets = [ResidualTemporalBlock1D(2 * in_channels, out_channels, embed_dim=temb_channels)]
        for _ in range(num_layers):
            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))
        self.resnets = nn.ModuleList(resnets)
        if non_linearity is None:
            self.nonlinearity = None
        else:
            self.nonlinearity = get_activation(non_linearity)
        self.upsample = None
        if add_upsample:
            self.upsample = Upsample1D(out_channels, use_conv_transpose=True)

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Optional[Tuple[torch.Tensor, ...]]'=None, temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        if res_hidden_states_tuple is not None:
            res_hidden_states = res_hidden_states_tuple[-1]
            hidden_states = torch.cat((hidden_states, res_hidden_states), dim=1)
        hidden_states = self.resnets[0](hidden_states, temb)
        for resnet in self.resnets[1:]:
            hidden_states = resnet(hidden_states, temb)
        if self.nonlinearity is not None:
            hidden_states = self.nonlinearity(hidden_states)
        if self.upsample is not None:
            hidden_states = self.upsample(hidden_states)
        return hidden_states


class ValueFunctionMidBlock1D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', embed_dim: 'int'):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.embed_dim = embed_dim
        self.res1 = ResidualTemporalBlock1D(in_channels, in_channels // 2, embed_dim=embed_dim)
        self.down1 = Downsample1D(out_channels // 2, use_conv=True)
        self.res2 = ResidualTemporalBlock1D(in_channels // 2, in_channels // 4, embed_dim=embed_dim)
        self.down2 = Downsample1D(out_channels // 4, use_conv=True)

    def forward(self, x: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        x = self.res1(x, temb)
        x = self.down1(x)
        x = self.res2(x, temb)
        x = self.down2(x)
        return x


class MidResTemporalBlock1D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', embed_dim: 'int', num_layers: 'int'=1, add_downsample: 'bool'=False, add_upsample: 'bool'=False, non_linearity: 'Optional[str]'=None):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.add_downsample = add_downsample
        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=embed_dim)]
        for _ in range(num_layers):
            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=embed_dim))
        self.resnets = nn.ModuleList(resnets)
        if non_linearity is None:
            self.nonlinearity = None
        else:
            self.nonlinearity = get_activation(non_linearity)
        self.upsample = None
        if add_upsample:
            self.upsample = Upsample1D(out_channels, use_conv=True)
        self.downsample = None
        if add_downsample:
            self.downsample = Downsample1D(out_channels, use_conv=True)
        if self.upsample and self.downsample:
            raise ValueError('Block cannot downsample and upsample')

    def forward(self, hidden_states: 'torch.Tensor', temb: 'torch.Tensor') ->torch.Tensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        for resnet in self.resnets[1:]:
            hidden_states = resnet(hidden_states, temb)
        if self.upsample:
            hidden_states = self.upsample(hidden_states)
        if self.downsample:
            self.downsample = self.downsample(hidden_states)
        return hidden_states


class OutConv1DBlock(nn.Module):

    def __init__(self, num_groups_out: 'int', out_channels: 'int', embed_dim: 'int', act_fn: 'str'):
        super().__init__()
        self.final_conv1d_1 = nn.Conv1d(embed_dim, embed_dim, 5, padding=2)
        self.final_conv1d_gn = nn.GroupNorm(num_groups_out, embed_dim)
        self.final_conv1d_act = get_activation(act_fn)
        self.final_conv1d_2 = nn.Conv1d(embed_dim, out_channels, 1)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.final_conv1d_1(hidden_states)
        hidden_states = rearrange_dims(hidden_states)
        hidden_states = self.final_conv1d_gn(hidden_states)
        hidden_states = rearrange_dims(hidden_states)
        hidden_states = self.final_conv1d_act(hidden_states)
        hidden_states = self.final_conv1d_2(hidden_states)
        return hidden_states


class OutValueFunctionBlock(nn.Module):

    def __init__(self, fc_dim: 'int', embed_dim: 'int', act_fn: 'str'='mish'):
        super().__init__()
        self.final_block = nn.ModuleList([nn.Linear(fc_dim + embed_dim, fc_dim // 2), get_activation(act_fn), nn.Linear(fc_dim // 2, 1)])

    def forward(self, hidden_states: 'torch.Tensor', temb: 'torch.Tensor') ->torch.Tensor:
        hidden_states = hidden_states.view(hidden_states.shape[0], -1)
        hidden_states = torch.cat((hidden_states, temb), dim=-1)
        for layer in self.final_block:
            hidden_states = layer(hidden_states)
        return hidden_states


_kernels = {'linear': [1 / 8, 3 / 8, 3 / 8, 1 / 8], 'cubic': [-0.01171875, -0.03515625, 0.11328125, 0.43359375, 0.43359375, 0.11328125, -0.03515625, -0.01171875], 'lanczos3': [0.003689131001010537, 0.015056144446134567, -0.03399861603975296, -0.066637322306633, 0.13550527393817902, 0.44638532400131226, 0.44638532400131226, 0.13550527393817902, -0.066637322306633, -0.03399861603975296, 0.015056144446134567, 0.003689131001010537]}


class Downsample1d(nn.Module):

    def __init__(self, kernel: 'str'='linear', pad_mode: 'str'='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor(_kernels[kernel])
        self.pad = kernel_1d.shape[0] // 2 - 1
        self.register_buffer('kernel', kernel_1d)

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        hidden_states = F.pad(hidden_states, (self.pad,) * 2, self.pad_mode)
        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])
        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)
        kernel = self.kernel[None, :].expand(hidden_states.shape[1], -1)
        weight[indices, indices] = kernel
        return F.conv1d(hidden_states, weight, stride=2)


class Upsample1d(nn.Module):

    def __init__(self, kernel: 'str'='linear', pad_mode: 'str'='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor(_kernels[kernel]) * 2
        self.pad = kernel_1d.shape[0] // 2 - 1
        self.register_buffer('kernel', kernel_1d)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = F.pad(hidden_states, ((self.pad + 1) // 2,) * 2, self.pad_mode)
        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])
        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)
        kernel = self.kernel[None, :].expand(hidden_states.shape[1], -1)
        weight[indices, indices] = kernel
        return F.conv_transpose1d(hidden_states, weight, stride=2, padding=self.pad * 2 + 1)


class SelfAttention1d(nn.Module):

    def __init__(self, in_channels: 'int', n_head: 'int'=1, dropout_rate: 'float'=0.0):
        super().__init__()
        self.channels = in_channels
        self.group_norm = nn.GroupNorm(1, num_channels=in_channels)
        self.num_heads = n_head
        self.query = nn.Linear(self.channels, self.channels)
        self.key = nn.Linear(self.channels, self.channels)
        self.value = nn.Linear(self.channels, self.channels)
        self.proj_attn = nn.Linear(self.channels, self.channels, bias=True)
        self.dropout = nn.Dropout(dropout_rate, inplace=True)

    def transpose_for_scores(self, projection: 'torch.Tensor') ->torch.Tensor:
        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)
        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)
        return new_projection

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        residual = hidden_states
        batch, channel_dim, seq = hidden_states.shape
        hidden_states = self.group_norm(hidden_states)
        hidden_states = hidden_states.transpose(1, 2)
        query_proj = self.query(hidden_states)
        key_proj = self.key(hidden_states)
        value_proj = self.value(hidden_states)
        query_states = self.transpose_for_scores(query_proj)
        key_states = self.transpose_for_scores(key_proj)
        value_states = self.transpose_for_scores(value_proj)
        scale = 1 / math.sqrt(math.sqrt(key_states.shape[-1]))
        attention_scores = torch.matmul(query_states * scale, key_states.transpose(-1, -2) * scale)
        attention_probs = torch.softmax(attention_scores, dim=-1)
        hidden_states = torch.matmul(attention_probs, value_states)
        hidden_states = hidden_states.permute(0, 2, 1, 3).contiguous()
        new_hidden_states_shape = hidden_states.size()[:-2] + (self.channels,)
        hidden_states = hidden_states.view(new_hidden_states_shape)
        hidden_states = self.proj_attn(hidden_states)
        hidden_states = hidden_states.transpose(1, 2)
        hidden_states = self.dropout(hidden_states)
        output = hidden_states + residual
        return output


class ResConvBlock(nn.Module):

    def __init__(self, in_channels: 'int', mid_channels: 'int', out_channels: 'int', is_last: 'bool'=False):
        super().__init__()
        self.is_last = is_last
        self.has_conv_skip = in_channels != out_channels
        if self.has_conv_skip:
            self.conv_skip = nn.Conv1d(in_channels, out_channels, 1, bias=False)
        self.conv_1 = nn.Conv1d(in_channels, mid_channels, 5, padding=2)
        self.group_norm_1 = nn.GroupNorm(1, mid_channels)
        self.gelu_1 = nn.GELU()
        self.conv_2 = nn.Conv1d(mid_channels, out_channels, 5, padding=2)
        if not self.is_last:
            self.group_norm_2 = nn.GroupNorm(1, out_channels)
            self.gelu_2 = nn.GELU()

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        residual = self.conv_skip(hidden_states) if self.has_conv_skip else hidden_states
        hidden_states = self.conv_1(hidden_states)
        hidden_states = self.group_norm_1(hidden_states)
        hidden_states = self.gelu_1(hidden_states)
        hidden_states = self.conv_2(hidden_states)
        if not self.is_last:
            hidden_states = self.group_norm_2(hidden_states)
            hidden_states = self.gelu_2(hidden_states)
        output = hidden_states + residual
        return output


class UNetMidBlock1D(nn.Module):

    def __init__(self, mid_channels: 'int', in_channels: 'int', out_channels: 'Optional[int]'=None):
        super().__init__()
        out_channels = in_channels if out_channels is None else out_channels
        self.down = Downsample1d('cubic')
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        attentions = [SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(out_channels, out_channels // 32)]
        self.up = Upsample1d(kernel='cubic')
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.down(hidden_states)
        for attn, resnet in zip(self.attentions, self.resnets):
            hidden_states = resnet(hidden_states)
            hidden_states = attn(hidden_states)
        hidden_states = self.up(hidden_states)
        return hidden_states


class AttnDownBlock1D(nn.Module):

    def __init__(self, out_channels: 'int', in_channels: 'int', mid_channels: 'Optional[int]'=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        self.down = Downsample1d('cubic')
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        attentions = [SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(out_channels, out_channels // 32)]
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.down(hidden_states)
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states)
            hidden_states = attn(hidden_states)
        return hidden_states, (hidden_states,)


class DownBlock1D(nn.Module):

    def __init__(self, out_channels: 'int', in_channels: 'int', mid_channels: 'Optional[int]'=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        self.down = Downsample1d('cubic')
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.down(hidden_states)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        return hidden_states, (hidden_states,)


class DownBlock1DNoSkip(nn.Module):

    def __init__(self, out_channels: 'int', in_channels: 'int', mid_channels: 'Optional[int]'=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = torch.cat([hidden_states, temb], dim=1)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        return hidden_states, (hidden_states,)


class AttnUpBlock1D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', mid_channels: 'Optional[int]'=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(2 * in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        attentions = [SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(out_channels, out_channels // 32)]
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.up = Upsample1d(kernel='cubic')

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        res_hidden_states = res_hidden_states_tuple[-1]
        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states)
            hidden_states = attn(hidden_states)
        hidden_states = self.up(hidden_states)
        return hidden_states


class UpBlock1D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', mid_channels: 'Optional[int]'=None):
        super().__init__()
        mid_channels = in_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(2 * in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        self.resnets = nn.ModuleList(resnets)
        self.up = Upsample1d(kernel='cubic')

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        res_hidden_states = res_hidden_states_tuple[-1]
        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        hidden_states = self.up(hidden_states)
        return hidden_states


class UpBlock1DNoSkip(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', mid_channels: 'Optional[int]'=None):
        super().__init__()
        mid_channels = in_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(2 * in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels, is_last=True)]
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        res_hidden_states = res_hidden_states_tuple[-1]
        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        return hidden_states


class UNet2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UNetMidBlock2DSimpleCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0, cross_attention_dim: 'int'=1280, skip_time_act: 'bool'=False, only_cross_attention: 'bool'=False, cross_attention_norm: 'Optional[str]'=None):
        super().__init__()
        self.has_cross_attention = True
        self.attention_head_dim = attention_head_dim
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.num_heads = in_channels // self.attention_head_dim
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act)]
        attentions = []
        for _ in range(num_layers):
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=in_channels, cross_attention_dim=in_channels, heads=self.num_heads, dim_head=self.attention_head_dim, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class AttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0, downsample_padding: 'int'=1, downsample_type: 'str'='conv'):
        super().__init__()
        resnets = []
        attentions = []
        self.downsample_type = downsample_type
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {out_channels}.')
            attention_head_dim = out_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if downsample_type == 'conv':
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        elif downsample_type == 'resnet':
            self.downsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, down=True)])
        else:
            self.downsamplers = None

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = attn(hidden_states, **cross_attention_kwargs)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                if self.downsample_type == 'resnet':
                    hidden_states = downsampler(hidden_states, temb=temb)
                else:
                    hidden_states = downsampler(hidden_states)
            output_states += hidden_states,
        return hidden_states, output_states


class DownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, downsample_padding: 'int'=1):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class DownEncoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, downsample_padding: 'int'=1):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            if resnet_time_scale_shift == 'spatial':
                resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor))
            else:
                resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None

    def forward(self, hidden_states: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb=None)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states


class AttnDownEncoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, downsample_padding: 'int'=1):
        super().__init__()
        resnets = []
        attentions = []
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {out_channels}.')
            attention_head_dim = out_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            if resnet_time_scale_shift == 'spatial':
                resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor))
            else:
                resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None

    def forward(self, hidden_states: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb=None)
            hidden_states = attn(hidden_states)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states


class AttnSkipDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=np.sqrt(2.0), add_downsample: 'bool'=True):
        super().__init__()
        self.attentions = nn.ModuleList([])
        self.resnets = nn.ModuleList([])
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {out_channels}.')
            attention_head_dim = out_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(in_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            self.attentions.append(Attention(out_channels, heads=out_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=32, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        if add_downsample:
            self.resnet_down = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, down=True, kernel='fir')
            self.downsamplers = nn.ModuleList([FirDownsample2D(out_channels, out_channels=out_channels)])
            self.skip_conv = nn.Conv2d(3, out_channels, kernel_size=(1, 1), stride=(1, 1))
        else:
            self.resnet_down = None
            self.downsamplers = None
            self.skip_conv = None

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, skip_sample: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...], torch.Tensor]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = attn(hidden_states)
            output_states += hidden_states,
        if self.downsamplers is not None:
            hidden_states = self.resnet_down(hidden_states, temb)
            for downsampler in self.downsamplers:
                skip_sample = downsampler(skip_sample)
            hidden_states = self.skip_conv(skip_sample) + hidden_states
            output_states += hidden_states,
        return hidden_states, output_states, skip_sample


class SkipDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=np.sqrt(2.0), add_downsample: 'bool'=True, downsample_padding: 'int'=1):
        super().__init__()
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(in_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        if add_downsample:
            self.resnet_down = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, down=True, kernel='fir')
            self.downsamplers = nn.ModuleList([FirDownsample2D(out_channels, out_channels=out_channels)])
            self.skip_conv = nn.Conv2d(3, out_channels, kernel_size=(1, 1), stride=(1, 1))
        else:
            self.resnet_down = None
            self.downsamplers = None
            self.skip_conv = None

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, skip_sample: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...], torch.Tensor]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        output_states = ()
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb)
            output_states += hidden_states,
        if self.downsamplers is not None:
            hidden_states = self.resnet_down(hidden_states, temb)
            for downsampler in self.downsamplers:
                skip_sample = downsampler(skip_sample)
            hidden_states = self.skip_conv(skip_sample) + hidden_states
            output_states += hidden_states,
        return hidden_states, output_states, skip_sample


class ResnetDownsampleBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, skip_time_act: 'bool'=False):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, down=True)])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class SimpleCrossAttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, skip_time_act: 'bool'=False, only_cross_attention: 'bool'=False, cross_attention_norm: 'Optional[str]'=None):
        super().__init__()
        self.has_cross_attention = True
        resnets = []
        attentions = []
        self.attention_head_dim = attention_head_dim
        self.num_heads = out_channels // self.attention_head_dim
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=out_channels, cross_attention_dim=out_channels, heads=self.num_heads, dim_head=attention_head_dim, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, down=True)])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        output_states = ()
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class KDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=4, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu', resnet_group_size: 'int'=32, add_downsample: 'bool'=False):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=out_channels, dropout=dropout, temb_channels=temb_channels, groups=groups, groups_out=groups_out, eps=resnet_eps, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([KDownsample2D()])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states, output_states


class KAttentionBlock(nn.Module):
    """
    A basic Transformer block.

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        attention_bias (`bool`, *optional*, defaults to `False`):
            Configure if the attention layers should contain a bias parameter.
        upcast_attention (`bool`, *optional*, defaults to `False`):
            Set to `True` to upcast the attention computation to `float32`.
        temb_channels (`int`, *optional*, defaults to 768):
            The number of channels in the token embedding.
        add_self_attention (`bool`, *optional*, defaults to `False`):
            Set to `True` to add self-attention to the block.
        cross_attention_norm (`str`, *optional*, defaults to `None`):
            The type of normalization to use for the cross attention. Can be `None`, `layer_norm`, or `group_norm`.
        group_size (`int`, *optional*, defaults to 32):
            The number of groups to separate the channels into for group normalization.
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', dropout: 'float'=0.0, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, upcast_attention: 'bool'=False, temb_channels: 'int'=768, add_self_attention: 'bool'=False, cross_attention_norm: 'Optional[str]'=None, group_size: 'int'=32):
        super().__init__()
        self.add_self_attention = add_self_attention
        if add_self_attention:
            self.norm1 = AdaGroupNorm(temb_channels, dim, max(1, dim // group_size))
            self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=None, cross_attention_norm=None)
        self.norm2 = AdaGroupNorm(temb_channels, dim, max(1, dim // group_size))
        self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, upcast_attention=upcast_attention, cross_attention_norm=cross_attention_norm)

    def _to_3d(self, hidden_states: 'torch.Tensor', height: 'int', weight: 'int') ->torch.Tensor:
        return hidden_states.permute(0, 2, 3, 1).reshape(hidden_states.shape[0], height * weight, -1)

    def _to_4d(self, hidden_states: 'torch.Tensor', height: 'int', weight: 'int') ->torch.Tensor:
        return hidden_states.permute(0, 2, 1).reshape(hidden_states.shape[0], -1, height, weight)

    def forward(self, hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, emb: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        if self.add_self_attention:
            norm_hidden_states = self.norm1(hidden_states, emb)
            height, weight = norm_hidden_states.shape[2:]
            norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)
            attn_output = self.attn1(norm_hidden_states, encoder_hidden_states=None, attention_mask=attention_mask, **cross_attention_kwargs)
            attn_output = self._to_4d(attn_output, height, weight)
            hidden_states = attn_output + hidden_states
        norm_hidden_states = self.norm2(hidden_states, emb)
        height, weight = norm_hidden_states.shape[2:]
        norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)
        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=attention_mask if encoder_hidden_states is None else encoder_attention_mask, **cross_attention_kwargs)
        attn_output = self._to_4d(attn_output, height, weight)
        hidden_states = attn_output + hidden_states
        return hidden_states


class KCrossAttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', cross_attention_dim: 'int', dropout: 'float'=0.0, num_layers: 'int'=4, resnet_group_size: 'int'=32, add_downsample: 'bool'=True, attention_head_dim: 'int'=64, add_self_attention: 'bool'=False, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu'):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=out_channels, dropout=dropout, temb_channels=temb_channels, groups=groups, groups_out=groups_out, eps=resnet_eps, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
            attentions.append(KAttentionBlock(out_channels, out_channels // attention_head_dim, attention_head_dim, cross_attention_dim=cross_attention_dim, temb_channels=temb_channels, attention_bias=True, add_self_attention=add_self_attention, cross_attention_norm='layer_norm', group_size=resnet_group_size))
        self.resnets = nn.ModuleList(resnets)
        self.attentions = nn.ModuleList(attentions)
        if add_downsample:
            self.downsamplers = nn.ModuleList([KDownsample2D()])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, emb=temb, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, emb=temb, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
            if self.downsamplers is None:
                output_states += None,
            else:
                output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states, output_states


class AttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'int'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0, upsample_type: 'str'='conv'):
        super().__init__()
        resnets = []
        attentions = []
        self.upsample_type = upsample_type
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {out_channels}.')
            attention_head_dim = out_channels
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if upsample_type == 'conv':
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        elif upsample_type == 'resnet':
            self.upsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, up=True)])
        else:
            self.upsamplers = None
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
            hidden_states = attn(hidden_states)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                if self.upsample_type == 'resnet':
                    hidden_states = upsampler(hidden_states, temb=temb)
                else:
                    hidden_states = upsampler(hidden_states)
        return hidden_states


class UpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpDecoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, temb_channels: 'Optional[int]'=None):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            if resnet_time_scale_shift == 'spatial':
                resnets.append(ResnetBlockCondNorm2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor))
            else:
                resnets.append(ResnetBlock2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb=temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class AttnUpDecoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, temb_channels: 'Optional[int]'=None):
        super().__init__()
        resnets = []
        attentions = []
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `out_channels`: {out_channels}.')
            attention_head_dim = out_channels
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            if resnet_time_scale_shift == 'spatial':
                resnets.append(ResnetBlockCondNorm2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor))
            else:
                resnets.append(ResnetBlock2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups if resnet_time_scale_shift != 'spatial' else None, spatial_norm_dim=temb_channels if resnet_time_scale_shift == 'spatial' else None, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb=temb)
            hidden_states = attn(hidden_states, temb=temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class FirUpsample2D(nn.Module):
    """A 2D FIR upsampling layer with an optional convolution.

    Parameters:
        channels (`int`, optional):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        fir_kernel (`tuple`, default `(1, 3, 3, 1)`):
            kernel for the FIR filter.
    """

    def __init__(self, channels: 'Optional[int]'=None, out_channels: 'Optional[int]'=None, use_conv: 'bool'=False, fir_kernel: 'Tuple[int, int, int, int]'=(1, 3, 3, 1)):
        super().__init__()
        out_channels = out_channels if out_channels else channels
        if use_conv:
            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.use_conv = use_conv
        self.fir_kernel = fir_kernel
        self.out_channels = out_channels

    def _upsample_2d(self, hidden_states: 'torch.Tensor', weight: 'Optional[torch.Tensor]'=None, kernel: 'Optional[torch.Tensor]'=None, factor: 'int'=2, gain: 'float'=1) ->torch.Tensor:
        """Fused `upsample_2d()` followed by `Conv2d()`.

        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more
        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of
        arbitrary order.

        Args:
            hidden_states (`torch.Tensor`):
                Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
            weight (`torch.Tensor`, *optional*):
                Weight tensor of the shape `[filterH, filterW, inChannels, outChannels]`. Grouped convolution can be
                performed by `inChannels = x.shape[0] // numGroups`.
            kernel (`torch.Tensor`, *optional*):
                FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] * factor`, which
                corresponds to nearest-neighbor upsampling.
            factor (`int`, *optional*): Integer upsampling factor (default: 2).
            gain (`float`, *optional*): Scaling factor for signal magnitude (default: 1.0).

        Returns:
            output (`torch.Tensor`):
                Tensor of the shape `[N, C, H * factor, W * factor]` or `[N, H * factor, W * factor, C]`, and same
                datatype as `hidden_states`.
        """
        assert isinstance(factor, int) and factor >= 1
        if kernel is None:
            kernel = [1] * factor
        kernel = torch.tensor(kernel, dtype=torch.float32)
        if kernel.ndim == 1:
            kernel = torch.outer(kernel, kernel)
        kernel /= torch.sum(kernel)
        kernel = kernel * (gain * factor ** 2)
        if self.use_conv:
            convH = weight.shape[2]
            convW = weight.shape[3]
            inC = weight.shape[1]
            pad_value = kernel.shape[0] - factor - (convW - 1)
            stride = factor, factor
            output_shape = (hidden_states.shape[2] - 1) * factor + convH, (hidden_states.shape[3] - 1) * factor + convW
            output_padding = output_shape[0] - (hidden_states.shape[2] - 1) * stride[0] - convH, output_shape[1] - (hidden_states.shape[3] - 1) * stride[1] - convW
            assert output_padding[0] >= 0 and output_padding[1] >= 0
            num_groups = hidden_states.shape[1] // inC
            weight = torch.reshape(weight, (num_groups, -1, inC, convH, convW))
            weight = torch.flip(weight, dims=[3, 4]).permute(0, 2, 1, 3, 4)
            weight = torch.reshape(weight, (num_groups * inC, -1, convH, convW))
            inverse_conv = F.conv_transpose2d(hidden_states, weight, stride=stride, output_padding=output_padding, padding=0)
            output = upfirdn2d_native(inverse_conv, torch.tensor(kernel, device=inverse_conv.device), pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2 + 1))
        else:
            pad_value = kernel.shape[0] - factor
            output = upfirdn2d_native(hidden_states, torch.tensor(kernel, device=hidden_states.device), up=factor, pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2))
        return output

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        if self.use_conv:
            height = self._upsample_2d(hidden_states, self.Conv2d_0.weight, kernel=self.fir_kernel)
            height = height + self.Conv2d_0.bias.reshape(1, -1, 1, 1)
        else:
            height = self._upsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)
        return height


class AttnSkipUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=np.sqrt(2.0), add_upsample: 'bool'=True):
        super().__init__()
        self.attentions = nn.ModuleList([])
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(resnet_in_channels + res_skip_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `out_channels`: {out_channels}.')
            attention_head_dim = out_channels
        self.attentions.append(Attention(out_channels, heads=out_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=32, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)
        if add_upsample:
            self.resnet_up = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, up=True, kernel='fir')
            self.skip_conv = nn.Conv2d(out_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            self.skip_norm = torch.nn.GroupNorm(num_groups=min(out_channels // 4, 32), num_channels=out_channels, eps=resnet_eps, affine=True)
            self.act = nn.SiLU()
        else:
            self.resnet_up = None
            self.skip_conv = None
            self.skip_norm = None
            self.act = None
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, skip_sample=None, *args, **kwargs) ->Tuple[torch.Tensor, torch.Tensor]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
        hidden_states = self.attentions[0](hidden_states)
        if skip_sample is not None:
            skip_sample = self.upsampler(skip_sample)
        else:
            skip_sample = 0
        if self.resnet_up is not None:
            skip_sample_states = self.skip_norm(hidden_states)
            skip_sample_states = self.act(skip_sample_states)
            skip_sample_states = self.skip_conv(skip_sample_states)
            skip_sample = skip_sample + skip_sample_states
            hidden_states = self.resnet_up(hidden_states, temb)
        return hidden_states, skip_sample


class SkipUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=np.sqrt(2.0), add_upsample: 'bool'=True, upsample_padding: 'int'=1):
        super().__init__()
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min((resnet_in_channels + res_skip_channels) // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)
        if add_upsample:
            self.resnet_up = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, up=True, kernel='fir')
            self.skip_conv = nn.Conv2d(out_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            self.skip_norm = torch.nn.GroupNorm(num_groups=min(out_channels // 4, 32), num_channels=out_channels, eps=resnet_eps, affine=True)
            self.act = nn.SiLU()
        else:
            self.resnet_up = None
            self.skip_conv = None
            self.skip_norm = None
            self.act = None
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, skip_sample=None, *args, **kwargs) ->Tuple[torch.Tensor, torch.Tensor]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
        if skip_sample is not None:
            skip_sample = self.upsampler(skip_sample)
        else:
            skip_sample = 0
        if self.resnet_up is not None:
            skip_sample_states = self.skip_norm(hidden_states)
            skip_sample_states = self.act(skip_sample_states)
            skip_sample_states = self.skip_conv(skip_sample_states)
            skip_sample = skip_sample + skip_sample_states
            hidden_states = self.resnet_up(hidden_states, temb)
        return hidden_states, skip_sample


class ResnetUpsampleBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, skip_time_act: 'bool'=False):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, up=True)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, temb)
        return hidden_states


class SimpleCrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, skip_time_act: 'bool'=False, only_cross_attention: 'bool'=False, cross_attention_norm: 'Optional[str]'=None):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.attention_head_dim = attention_head_dim
        self.num_heads = out_channels // self.attention_head_dim
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=out_channels, cross_attention_dim=out_channels, heads=self.num_heads, dim_head=self.attention_head_dim, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, up=True)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, temb)
        return hidden_states


class KUpsample2D(nn.Module):
    """A 2D K-upsampling layer.

    Parameters:
        pad_mode (`str`, *optional*, default to `"reflect"`): the padding mode to use.
    """

    def __init__(self, pad_mode: 'str'='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]]) * 2
        self.pad = kernel_1d.shape[1] // 2 - 1
        self.register_buffer('kernel', kernel_1d.T @ kernel_1d, persistent=False)

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        inputs = F.pad(inputs, ((self.pad + 1) // 2,) * 4, self.pad_mode)
        weight = inputs.new_zeros([inputs.shape[1], inputs.shape[1], self.kernel.shape[0], self.kernel.shape[1]])
        indices = torch.arange(inputs.shape[1], device=inputs.device)
        kernel = self.kernel[None, :].expand(inputs.shape[1], -1, -1)
        weight[indices, indices] = kernel
        return F.conv_transpose2d(inputs, weight, stride=2, padding=self.pad * 2 + 1)


class KUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'int', dropout: 'float'=0.0, num_layers: 'int'=5, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu', resnet_group_size: 'Optional[int]'=32, add_upsample: 'bool'=True):
        super().__init__()
        resnets = []
        k_in_channels = 2 * out_channels
        k_out_channels = in_channels
        num_layers = num_layers - 1
        for i in range(num_layers):
            in_channels = k_in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=k_out_channels if i == num_layers - 1 else out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=groups, groups_out=groups_out, dropout=dropout, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([KUpsample2D()])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        res_hidden_states_tuple = res_hidden_states_tuple[-1]
        if res_hidden_states_tuple is not None:
            hidden_states = torch.cat([hidden_states, res_hidden_states_tuple], dim=1)
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class KCrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'int', dropout: 'float'=0.0, num_layers: 'int'=4, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu', resnet_group_size: 'int'=32, attention_head_dim: 'int'=1, cross_attention_dim: 'int'=768, add_upsample: 'bool'=True, upcast_attention: 'bool'=False):
        super().__init__()
        resnets = []
        attentions = []
        is_first_block = in_channels == out_channels == temb_channels
        is_middle_block = in_channels != out_channels
        add_self_attention = True if is_first_block else False
        self.has_cross_attention = True
        self.attention_head_dim = attention_head_dim
        k_in_channels = out_channels if is_first_block else 2 * out_channels
        k_out_channels = in_channels
        num_layers = num_layers - 1
        for i in range(num_layers):
            in_channels = k_in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            if is_middle_block and i == num_layers - 1:
                conv_2d_out_channels = k_out_channels
            else:
                conv_2d_out_channels = None
            resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=out_channels, conv_2d_out_channels=conv_2d_out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=groups, groups_out=groups_out, dropout=dropout, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
            attentions.append(KAttentionBlock(k_out_channels if i == num_layers - 1 else out_channels, k_out_channels // attention_head_dim if i == num_layers - 1 else out_channels // attention_head_dim, attention_head_dim, cross_attention_dim=cross_attention_dim, temb_channels=temb_channels, attention_bias=True, add_self_attention=add_self_attention, cross_attention_norm='layer_norm', upcast_attention=upcast_attention))
        self.resnets = nn.ModuleList(resnets)
        self.attentions = nn.ModuleList(attentions)
        if add_upsample:
            self.upsamplers = nn.ModuleList([KUpsample2D()])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        res_hidden_states_tuple = res_hidden_states_tuple[-1]
        if res_hidden_states_tuple is not None:
            hidden_states = torch.cat([hidden_states, res_hidden_states_tuple], dim=1)
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, emb=temb, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, emb=temb, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class UNet2DConditionModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UNetMidBlock3DCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, output_scale_factor: 'float'=1.0, cross_attention_dim: 'int'=1280, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=True, upcast_attention: 'bool'=False):
        super().__init__()
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        temp_convs = [TemporalConvLayer(in_channels, in_channels, dropout=0.1, norm_num_groups=resnet_groups)]
        attentions = []
        temp_attentions = []
        for _ in range(num_layers):
            attentions.append(Transformer2DModel(in_channels // num_attention_heads, num_attention_heads, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention))
            temp_attentions.append(TransformerTemporalModel(in_channels // num_attention_heads, num_attention_heads, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(in_channels, in_channels, dropout=0.1, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)
        self.temp_attentions = nn.ModuleList(temp_attentions)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None) ->torch.Tensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        hidden_states = self.temp_convs[0](hidden_states, num_frames=num_frames)
        for attn, temp_attn, resnet, temp_conv in zip(self.attentions, self.temp_attentions, self.resnets[1:], self.temp_convs[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            hidden_states = temp_attn(hidden_states, num_frames=num_frames, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
        return hidden_states


class CrossAttnDownBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, downsample_padding: 'int'=1, add_downsample: 'bool'=True, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False):
        super().__init__()
        resnets = []
        attentions = []
        temp_attentions = []
        temp_convs = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups))
            attentions.append(Transformer2DModel(out_channels // num_attention_heads, num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            temp_attentions.append(TransformerTemporalModel(out_channels // num_attention_heads, num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)
        self.temp_attentions = nn.ModuleList(temp_attentions)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1, cross_attention_kwargs: 'Dict[str, Any]'=None) ->Union[torch.Tensor, Tuple[torch.Tensor, ...]]:
        output_states = ()
        for resnet, temp_conv, attn, temp_attn in zip(self.resnets, self.temp_convs, self.attentions, self.temp_attentions):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            hidden_states = temp_attn(hidden_states, num_frames=num_frames, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states += hidden_states,
        return hidden_states, output_states


class DownBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, downsample_padding: 'int'=1):
        super().__init__()
        resnets = []
        temp_convs = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1) ->Union[torch.Tensor, Tuple[torch.Tensor, ...]]:
        output_states = ()
        for resnet, temp_conv in zip(self.resnets, self.temp_convs):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states += hidden_states,
        return hidden_states, output_states


class CrossAttnUpBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False, resolution_idx: 'Optional[int]'=None):
        super().__init__()
        resnets = []
        temp_convs = []
        attentions = []
        temp_attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups))
            attentions.append(Transformer2DModel(out_channels // num_attention_heads, num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            temp_attentions.append(TransformerTemporalModel(out_channels // num_attention_heads, num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)
        self.temp_attentions = nn.ModuleList(temp_attentions)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1, cross_attention_kwargs: 'Dict[str, Any]'=None) ->torch.Tensor:
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        for resnet, temp_conv, attn, temp_attn in zip(self.resnets, self.temp_convs, self.attentions, self.temp_attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            hidden_states = temp_attn(hidden_states, num_frames=num_frames, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, resolution_idx: 'Optional[int]'=None):
        super().__init__()
        resnets = []
        temp_convs = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size: 'Optional[int]'=None, num_frames: 'int'=1) ->torch.Tensor:
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        for resnet, temp_conv in zip(self.resnets, self.temp_convs):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UNetMidBlockSpatioTemporal(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280):
        super().__init__()
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        resnets = [SpatioTemporalResBlock(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=1e-05)]
        attentions = []
        for i in range(num_layers):
            attentions.append(TransformerSpatioTemporalModel(num_attention_heads, in_channels // num_attention_heads, in_channels=in_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim))
            resnets.append(SpatioTemporalResBlock(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=1e-05))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.resnets[0](hidden_states, temb, image_only_indicator=image_only_indicator)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, image_only_indicator=image_only_indicator, return_dict=False)[0]
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator, **ckpt_kwargs)
            else:
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, image_only_indicator=image_only_indicator, return_dict=False)[0]
                hidden_states = resnet(hidden_states, temb, image_only_indicator=image_only_indicator)
        return hidden_states


class DownBlockSpatioTemporal(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', num_layers: 'int'=1, add_downsample: 'bool'=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(SpatioTemporalResBlock(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=1e-05))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator)
            else:
                hidden_states = resnet(hidden_states, temb, image_only_indicator=image_only_indicator)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class CrossAttnDownBlockSpatioTemporal(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, add_downsample: 'bool'=True):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(SpatioTemporalResBlock(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=1e-06))
            attentions.append(TransformerSpatioTemporalModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=1, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, ...]]:
        output_states = ()
        blocks = list(zip(self.resnets, self.attentions))
        for resnet, attn in blocks:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator, **ckpt_kwargs)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, image_only_indicator=image_only_indicator, return_dict=False)[0]
            else:
                hidden_states = resnet(hidden_states, temb, image_only_indicator=image_only_indicator)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, image_only_indicator=image_only_indicator, return_dict=False)[0]
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class UpBlockSpatioTemporal(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, add_upsample: 'bool'=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(SpatioTemporalResBlock(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator)
            else:
                hidden_states = resnet(hidden_states, temb, image_only_indicator=image_only_indicator)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class CrossAttnUpBlockSpatioTemporal(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, add_upsample: 'bool'=True):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(SpatioTemporalResBlock(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps))
            attentions.append(TransformerSpatioTemporalModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, image_only_indicator: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, image_only_indicator, **ckpt_kwargs)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, image_only_indicator=image_only_indicator, return_dict=False)[0]
            else:
                hidden_states = resnet(hidden_states, temb, image_only_indicator=image_only_indicator)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, image_only_indicator=image_only_indicator, return_dict=False)[0]
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class UNet3DConditionModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class I2VGenXLTransformerTemporalEncoder(nn.Module):

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', activation_fn: 'str'='geglu', upcast_attention: 'bool'=False, ff_inner_dim: 'Optional[int]'=None, dropout: 'int'=0.0):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim, elementwise_affine=True, eps=1e-05)
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=False, upcast_attention=upcast_attention, out_bias=True)
        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_fn, final_dropout=False, inner_dim=ff_inner_dim, bias=True)

    def forward(self, hidden_states: 'torch.Tensor') ->torch.Tensor:
        norm_hidden_states = self.norm1(hidden_states)
        attn_output = self.attn1(norm_hidden_states, encoder_hidden_states=None)
        hidden_states = attn_output + hidden_states
        if hidden_states.ndim == 4:
            hidden_states = hidden_states.squeeze(1)
        ff_output = self.ff(hidden_states)
        hidden_states = ff_output + hidden_states
        if hidden_states.ndim == 4:
            hidden_states = hidden_states.squeeze(1)
        return hidden_states


class I2VGenXLUNet(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class Kandinsky3EncoderProj(nn.Module):

    def __init__(self, encoder_hid_dim, cross_attention_dim):
        super().__init__()
        self.projection_linear = nn.Linear(encoder_hid_dim, cross_attention_dim, bias=False)
        self.projection_norm = nn.LayerNorm(cross_attention_dim)

    def forward(self, x):
        x = self.projection_linear(x)
        x = self.projection_norm(x)
        return x


class Kandinsky3UNet(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class Kandinsky3ConditionalGroupNorm(nn.Module):

    def __init__(self, groups, normalized_shape, context_dim):
        super().__init__()
        self.norm = nn.GroupNorm(groups, normalized_shape, affine=False)
        self.context_mlp = nn.Sequential(nn.SiLU(), nn.Linear(context_dim, 2 * normalized_shape))
        self.context_mlp[1].weight.data.zero_()
        self.context_mlp[1].bias.data.zero_()

    def forward(self, x, context):
        context = self.context_mlp(context)
        for _ in range(len(x.shape[2:])):
            context = context.unsqueeze(-1)
        scale, shift = context.chunk(2, dim=1)
        x = self.norm(x) * (scale + 1.0) + shift
        return x


class Kandinsky3AttentionBlock(nn.Module):

    def __init__(self, num_channels, time_embed_dim, context_dim=None, norm_groups=32, head_dim=64, expansion_ratio=4):
        super().__init__()
        self.in_norm = Kandinsky3ConditionalGroupNorm(norm_groups, num_channels, time_embed_dim)
        self.attention = Attention(num_channels, context_dim or num_channels, dim_head=head_dim, out_dim=num_channels, out_bias=False)
        hidden_channels = expansion_ratio * num_channels
        self.out_norm = Kandinsky3ConditionalGroupNorm(norm_groups, num_channels, time_embed_dim)
        self.feed_forward = nn.Sequential(nn.Conv2d(num_channels, hidden_channels, kernel_size=1, bias=False), nn.SiLU(), nn.Conv2d(hidden_channels, num_channels, kernel_size=1, bias=False))

    def forward(self, x, time_embed, context=None, context_mask=None, image_mask=None):
        height, width = x.shape[-2:]
        out = self.in_norm(x, time_embed)
        out = out.reshape(x.shape[0], -1, height * width).permute(0, 2, 1)
        context = context if context is not None else out
        if context_mask is not None:
            context_mask = context_mask
        out = self.attention(out, context, context_mask)
        out = out.permute(0, 2, 1).unsqueeze(-1).reshape(out.shape[0], -1, height, width)
        x = x + out
        out = self.out_norm(x, time_embed)
        out = self.feed_forward(out)
        x = x + out
        return x


class Kandinsky3Block(nn.Module):

    def __init__(self, in_channels, out_channels, time_embed_dim, kernel_size=3, norm_groups=32, up_resolution=None):
        super().__init__()
        self.group_norm = Kandinsky3ConditionalGroupNorm(norm_groups, in_channels, time_embed_dim)
        self.activation = nn.SiLU()
        if up_resolution is not None and up_resolution:
            self.up_sample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2)
        else:
            self.up_sample = nn.Identity()
        padding = int(kernel_size > 1)
        self.projection = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)
        if up_resolution is not None and not up_resolution:
            self.down_sample = nn.Conv2d(out_channels, out_channels, kernel_size=2, stride=2)
        else:
            self.down_sample = nn.Identity()

    def forward(self, x, time_embed):
        x = self.group_norm(x, time_embed)
        x = self.activation(x)
        x = self.up_sample(x)
        x = self.projection(x)
        x = self.down_sample(x)
        return x


class Kandinsky3ResNetBlock(nn.Module):

    def __init__(self, in_channels, out_channels, time_embed_dim, norm_groups=32, compression_ratio=2, up_resolutions=4 * [None]):
        super().__init__()
        kernel_sizes = [1, 3, 3, 1]
        hidden_channel = max(in_channels, out_channels) // compression_ratio
        hidden_channels = [(in_channels, hidden_channel)] + [(hidden_channel, hidden_channel)] * 2 + [(hidden_channel, out_channels)]
        self.resnet_blocks = nn.ModuleList([Kandinsky3Block(in_channel, out_channel, time_embed_dim, kernel_size, norm_groups, up_resolution) for (in_channel, out_channel), kernel_size, up_resolution in zip(hidden_channels, kernel_sizes, up_resolutions)])
        self.shortcut_up_sample = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2) if True in up_resolutions else nn.Identity()
        self.shortcut_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()
        self.shortcut_down_sample = nn.Conv2d(out_channels, out_channels, kernel_size=2, stride=2) if False in up_resolutions else nn.Identity()

    def forward(self, x, time_embed):
        out = x
        for resnet_block in self.resnet_blocks:
            out = resnet_block(out, time_embed)
        x = self.shortcut_up_sample(x)
        x = self.shortcut_projection(x)
        x = self.shortcut_down_sample(x)
        x = x + out
        return x


class Kandinsky3UpSampleBlock(nn.Module):

    def __init__(self, in_channels, cat_dim, out_channels, time_embed_dim, context_dim=None, num_blocks=3, groups=32, head_dim=64, expansion_ratio=4, compression_ratio=2, up_sample=True, self_attention=True):
        super().__init__()
        up_resolutions = [[None, True if up_sample else None, None, None]] + [[None] * 4] * (num_blocks - 1)
        hidden_channels = [(in_channels + cat_dim, in_channels)] + [(in_channels, in_channels)] * (num_blocks - 2) + [(in_channels, out_channels)]
        attentions = []
        resnets_in = []
        resnets_out = []
        self.self_attention = self_attention
        self.context_dim = context_dim
        if self_attention:
            attentions.append(Kandinsky3AttentionBlock(out_channels, time_embed_dim, None, groups, head_dim, expansion_ratio))
        else:
            attentions.append(nn.Identity())
        for (in_channel, out_channel), up_resolution in zip(hidden_channels, up_resolutions):
            resnets_in.append(Kandinsky3ResNetBlock(in_channel, in_channel, time_embed_dim, groups, compression_ratio, up_resolution))
            if context_dim is not None:
                attentions.append(Kandinsky3AttentionBlock(in_channel, time_embed_dim, context_dim, groups, head_dim, expansion_ratio))
            else:
                attentions.append(nn.Identity())
            resnets_out.append(Kandinsky3ResNetBlock(in_channel, out_channel, time_embed_dim, groups, compression_ratio))
        self.attentions = nn.ModuleList(attentions)
        self.resnets_in = nn.ModuleList(resnets_in)
        self.resnets_out = nn.ModuleList(resnets_out)

    def forward(self, x, time_embed, context=None, context_mask=None, image_mask=None):
        for attention, resnet_in, resnet_out in zip(self.attentions[1:], self.resnets_in, self.resnets_out):
            x = resnet_in(x, time_embed)
            if self.context_dim is not None:
                x = attention(x, time_embed, context, context_mask, image_mask)
            x = resnet_out(x, time_embed)
        if self.self_attention:
            x = self.attentions[0](x, time_embed, image_mask=image_mask)
        return x


class Kandinsky3DownSampleBlock(nn.Module):

    def __init__(self, in_channels, out_channels, time_embed_dim, context_dim=None, num_blocks=3, groups=32, head_dim=64, expansion_ratio=4, compression_ratio=2, down_sample=True, self_attention=True):
        super().__init__()
        attentions = []
        resnets_in = []
        resnets_out = []
        self.self_attention = self_attention
        self.context_dim = context_dim
        if self_attention:
            attentions.append(Kandinsky3AttentionBlock(in_channels, time_embed_dim, None, groups, head_dim, expansion_ratio))
        else:
            attentions.append(nn.Identity())
        up_resolutions = [[None] * 4] * (num_blocks - 1) + [[None, None, False if down_sample else None, None]]
        hidden_channels = [(in_channels, out_channels)] + [(out_channels, out_channels)] * (num_blocks - 1)
        for (in_channel, out_channel), up_resolution in zip(hidden_channels, up_resolutions):
            resnets_in.append(Kandinsky3ResNetBlock(in_channel, out_channel, time_embed_dim, groups, compression_ratio))
            if context_dim is not None:
                attentions.append(Kandinsky3AttentionBlock(out_channel, time_embed_dim, context_dim, groups, head_dim, expansion_ratio))
            else:
                attentions.append(nn.Identity())
            resnets_out.append(Kandinsky3ResNetBlock(out_channel, out_channel, time_embed_dim, groups, compression_ratio, up_resolution))
        self.attentions = nn.ModuleList(attentions)
        self.resnets_in = nn.ModuleList(resnets_in)
        self.resnets_out = nn.ModuleList(resnets_out)

    def forward(self, x, time_embed, context=None, context_mask=None, image_mask=None):
        if self.self_attention:
            x = self.attentions[0](x, time_embed, image_mask=image_mask)
        for attention, resnet_in, resnet_out in zip(self.attentions[1:], self.resnets_in, self.resnets_out):
            x = resnet_in(x, time_embed)
            if self.context_dim is not None:
                x = attention(x, time_embed, context, context_mask, image_mask)
            x = resnet_out(x, time_embed)
        return x


class Kandinsky3AttentionPooling(nn.Module):

    def __init__(self, num_channels, context_dim, head_dim=64):
        super().__init__()
        self.attention = Attention(context_dim, context_dim, dim_head=head_dim, out_dim=num_channels, out_bias=False)

    def forward(self, x, context, context_mask=None):
        context_mask = context_mask
        context = self.attention(context.mean(dim=1, keepdim=True), context, context_mask)
        return x + context.squeeze(1)


class AnimateDiffTransformer3D(nn.Module):
    """
    A Transformer model for video-like data.

    Parameters:
        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.
        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.
        in_channels (`int`, *optional*):
            The number of channels in the input and output (specify if the input is **continuous**).
        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.
        attention_bias (`bool`, *optional*):
            Configure if the `TransformerBlock` attention should contain a bias parameter.
        sample_size (`int`, *optional*): The width of the latent images (specify if the input is **discrete**).
            This is fixed during training since it is used to learn a number of position embeddings.
        activation_fn (`str`, *optional*, defaults to `"geglu"`):
            Activation function to use in feed-forward. See `diffusers.models.activations.get_activation` for supported
            activation functions.
        norm_elementwise_affine (`bool`, *optional*):
            Configure if the `TransformerBlock` should use learnable elementwise affine parameters for normalization.
        double_self_attention (`bool`, *optional*):
            Configure if each `TransformerBlock` should contain two self-attention layers.
        positional_embeddings: (`str`, *optional*):
            The type of positional embeddings to apply to the sequence input before passing use.
        num_positional_embeddings: (`int`, *optional*):
            The maximum length of the sequence over which to apply positional embeddings.
    """

    def __init__(self, num_attention_heads: 'int'=16, attention_head_dim: 'int'=88, in_channels: 'Optional[int]'=None, out_channels: 'Optional[int]'=None, num_layers: 'int'=1, dropout: 'float'=0.0, norm_num_groups: 'int'=32, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, sample_size: 'Optional[int]'=None, activation_fn: 'str'='geglu', norm_elementwise_affine: 'bool'=True, double_self_attention: 'bool'=True, positional_embeddings: 'Optional[str]'=None, num_positional_embeddings: 'Optional[int]'=None):
        super().__init__()
        self.num_attention_heads = num_attention_heads
        self.attention_head_dim = attention_head_dim
        inner_dim = num_attention_heads * attention_head_dim
        self.in_channels = in_channels
        self.norm = nn.GroupNorm(num_groups=norm_num_groups, num_channels=in_channels, eps=1e-06, affine=True)
        self.proj_in = nn.Linear(in_channels, inner_dim)
        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(inner_dim, num_attention_heads, attention_head_dim, dropout=dropout, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, attention_bias=attention_bias, double_self_attention=double_self_attention, norm_elementwise_affine=norm_elementwise_affine, positional_embeddings=positional_embeddings, num_positional_embeddings=num_positional_embeddings) for _ in range(num_layers)])
        self.proj_out = nn.Linear(inner_dim, in_channels)

    def forward(self, hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.LongTensor]'=None, timestep: 'Optional[torch.LongTensor]'=None, class_labels: 'Optional[torch.LongTensor]'=None, num_frames: 'int'=1, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None) ->torch.Tensor:
        """
        The [`AnimateDiffTransformer3D`] forward method.

        Args:
            hidden_states (`torch.LongTensor` of shape `(batch size, num latent pixels)` if discrete, `torch.Tensor` of shape `(batch size, channel, height, width)` if continuous):
                Input hidden_states.
            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):
                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
                self-attention.
            timestep ( `torch.LongTensor`, *optional*):
                Used to indicate denoising step. Optional timestep to be applied as an embedding in `AdaLayerNorm`.
            class_labels ( `torch.LongTensor` of shape `(batch size, num classes)`, *optional*):
                Used to indicate class labels conditioning. Optional class labels to be applied as an embedding in
                `AdaLayerZeroNorm`.
            num_frames (`int`, *optional*, defaults to 1):
                The number of frames to be processed per batch. This is used to reshape the hidden states.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).

        Returns:
            torch.Tensor:
                The output tensor.
        """
        batch_frames, channel, height, width = hidden_states.shape
        batch_size = batch_frames // num_frames
        residual = hidden_states
        hidden_states = hidden_states[None, :].reshape(batch_size, num_frames, channel, height, width)
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)
        hidden_states = self.norm(hidden_states)
        hidden_states = hidden_states.permute(0, 3, 4, 2, 1).reshape(batch_size * height * width, num_frames, channel)
        hidden_states = self.proj_in(input=hidden_states)
        for block in self.transformer_blocks:
            hidden_states = block(hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, timestep=timestep, cross_attention_kwargs=cross_attention_kwargs, class_labels=class_labels)
        hidden_states = self.proj_out(input=hidden_states)
        hidden_states = hidden_states[None, None, :].reshape(batch_size, height, width, num_frames, channel).permute(0, 3, 4, 1, 2).contiguous()
        hidden_states = hidden_states.reshape(batch_frames, channel, height, width)
        output = hidden_states + residual
        return output


class DownBlockMotion(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_downsample: 'bool'=True, downsample_padding: 'int'=1, temporal_num_attention_heads: 'Union[int, Tuple[int]]'=1, temporal_cross_attention_dim: 'Optional[int]'=None, temporal_max_seq_length: 'int'=32, temporal_transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, temporal_double_self_attention: 'bool'=True):
        super().__init__()
        resnets = []
        motion_modules = []
        if isinstance(temporal_transformer_layers_per_block, int):
            temporal_transformer_layers_per_block = (temporal_transformer_layers_per_block,) * num_layers
        elif len(temporal_transformer_layers_per_block) != num_layers:
            raise ValueError(f'`temporal_transformer_layers_per_block` must be an integer or a tuple of integers of length {num_layers}')
        if isinstance(temporal_num_attention_heads, int):
            temporal_num_attention_heads = (temporal_num_attention_heads,) * num_layers
        elif len(temporal_num_attention_heads) != num_layers:
            raise ValueError(f'`temporal_num_attention_heads` must be an integer or a tuple of integers of length {num_layers}')
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            motion_modules.append(AnimateDiffTransformer3D(num_attention_heads=temporal_num_attention_heads[i], in_channels=out_channels, num_layers=temporal_transformer_layers_per_block[i], norm_num_groups=resnet_groups, cross_attention_dim=temporal_cross_attention_dim, attention_bias=False, activation_fn='geglu', positional_embeddings='sinusoidal', num_positional_embeddings=temporal_max_seq_length, attention_head_dim=out_channels // temporal_num_attention_heads[i], double_self_attention=temporal_double_self_attention))
        self.resnets = nn.ModuleList(resnets)
        self.motion_modules = nn.ModuleList(motion_modules)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1, *args, **kwargs) ->Union[torch.Tensor, Tuple[torch.Tensor, ...]]:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        output_states = ()
        blocks = zip(self.resnets, self.motion_modules)
        for resnet, motion_module in blocks:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(input_tensor=hidden_states, temb=temb)
            hidden_states = motion_module(hidden_states, num_frames=num_frames)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states=hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class CrossAttnDownBlockMotion(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, downsample_padding: 'int'=1, add_downsample: 'bool'=True, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False, attention_type: 'str'='default', temporal_cross_attention_dim: 'Optional[int]'=None, temporal_num_attention_heads: 'int'=8, temporal_max_seq_length: 'int'=32, temporal_transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, temporal_double_self_attention: 'bool'=True):
        super().__init__()
        resnets = []
        attentions = []
        motion_modules = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = (transformer_layers_per_block,) * num_layers
        elif len(transformer_layers_per_block) != num_layers:
            raise ValueError(f'transformer_layers_per_block must be an integer or a list of integers of length {num_layers}')
        if isinstance(temporal_transformer_layers_per_block, int):
            temporal_transformer_layers_per_block = (temporal_transformer_layers_per_block,) * num_layers
        elif len(temporal_transformer_layers_per_block) != num_layers:
            raise ValueError(f'temporal_transformer_layers_per_block must be an integer or a list of integers of length {num_layers}')
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, attention_type=attention_type))
            else:
                attentions.append(DualTransformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            motion_modules.append(AnimateDiffTransformer3D(num_attention_heads=temporal_num_attention_heads, in_channels=out_channels, num_layers=temporal_transformer_layers_per_block[i], norm_num_groups=resnet_groups, cross_attention_dim=temporal_cross_attention_dim, attention_bias=False, activation_fn='geglu', positional_embeddings='sinusoidal', num_positional_embeddings=temporal_max_seq_length, attention_head_dim=out_channels // temporal_num_attention_heads, double_self_attention=temporal_double_self_attention))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.motion_modules = nn.ModuleList(motion_modules)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1, encoder_attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, additional_residuals: 'Optional[torch.Tensor]'=None):
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        output_states = ()
        blocks = list(zip(self.resnets, self.attentions, self.motion_modules))
        for i, (resnet, attn, motion_module) in enumerate(blocks):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
            else:
                hidden_states = resnet(input_tensor=hidden_states, temb=temb)
            hidden_states = attn(hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            hidden_states = motion_module(hidden_states, num_frames=num_frames)
            if i == len(blocks) - 1 and additional_residuals is not None:
                hidden_states = hidden_states + additional_residuals
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states=hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class CrossAttnUpBlockMotion(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, cross_attention_dim: 'int'=1280, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False, attention_type: 'str'='default', temporal_cross_attention_dim: 'Optional[int]'=None, temporal_num_attention_heads: 'int'=8, temporal_max_seq_length: 'int'=32, temporal_transformer_layers_per_block: 'Union[int, Tuple[int]]'=1):
        super().__init__()
        resnets = []
        attentions = []
        motion_modules = []
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = (transformer_layers_per_block,) * num_layers
        elif len(transformer_layers_per_block) != num_layers:
            raise ValueError(f'transformer_layers_per_block must be an integer or a list of integers of length {num_layers}, got {len(transformer_layers_per_block)}')
        if isinstance(temporal_transformer_layers_per_block, int):
            temporal_transformer_layers_per_block = (temporal_transformer_layers_per_block,) * num_layers
        elif len(temporal_transformer_layers_per_block) != num_layers:
            raise ValueError(f'temporal_transformer_layers_per_block must be an integer or a list of integers of length {num_layers}, got {len(temporal_transformer_layers_per_block)}')
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, attention_type=attention_type))
            else:
                attentions.append(DualTransformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            motion_modules.append(AnimateDiffTransformer3D(num_attention_heads=temporal_num_attention_heads, in_channels=out_channels, num_layers=temporal_transformer_layers_per_block[i], norm_num_groups=resnet_groups, cross_attention_dim=temporal_cross_attention_dim, attention_bias=False, activation_fn='geglu', positional_embeddings='sinusoidal', num_positional_embeddings=temporal_max_seq_length, attention_head_dim=out_channels // temporal_num_attention_heads))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.motion_modules = nn.ModuleList(motion_modules)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1) ->torch.Tensor:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        blocks = zip(self.resnets, self.attentions, self.motion_modules)
        for resnet, attn, motion_module in blocks:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
            else:
                hidden_states = resnet(input_tensor=hidden_states, temb=temb)
            hidden_states = attn(hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            hidden_states = motion_module(hidden_states, num_frames=num_frames)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states=hidden_states, output_size=upsample_size)
        return hidden_states


class UpBlockMotion(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', resolution_idx: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor: 'float'=1.0, add_upsample: 'bool'=True, temporal_cross_attention_dim: 'Optional[int]'=None, temporal_num_attention_heads: 'int'=8, temporal_max_seq_length: 'int'=32, temporal_transformer_layers_per_block: 'Union[int, Tuple[int]]'=1):
        super().__init__()
        resnets = []
        motion_modules = []
        if isinstance(temporal_transformer_layers_per_block, int):
            temporal_transformer_layers_per_block = (temporal_transformer_layers_per_block,) * num_layers
        elif len(temporal_transformer_layers_per_block) != num_layers:
            raise ValueError(f'temporal_transformer_layers_per_block must be an integer or a list of integers of length {num_layers}')
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            motion_modules.append(AnimateDiffTransformer3D(num_attention_heads=temporal_num_attention_heads, in_channels=out_channels, num_layers=temporal_transformer_layers_per_block[i], norm_num_groups=resnet_groups, cross_attention_dim=temporal_cross_attention_dim, attention_bias=False, activation_fn='geglu', positional_embeddings='sinusoidal', num_positional_embeddings=temporal_max_seq_length, attention_head_dim=out_channels // temporal_num_attention_heads))
        self.resnets = nn.ModuleList(resnets)
        self.motion_modules = nn.ModuleList(motion_modules)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False
        self.resolution_idx = resolution_idx

    def forward(self, hidden_states: 'torch.Tensor', res_hidden_states_tuple: 'Tuple[torch.Tensor, ...]', temb: 'Optional[torch.Tensor]'=None, upsample_size=None, num_frames: 'int'=1, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        is_freeu_enabled = getattr(self, 's1', None) and getattr(self, 's2', None) and getattr(self, 'b1', None) and getattr(self, 'b2', None)
        blocks = zip(self.resnets, self.motion_modules)
        for resnet, motion_module in blocks:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            if is_freeu_enabled:
                hidden_states, res_hidden_states = apply_freeu(self.resolution_idx, hidden_states, res_hidden_states, s1=self.s1, s2=self.s2, b1=self.b1, b2=self.b2)
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(input_tensor=hidden_states, temb=temb)
            hidden_states = motion_module(hidden_states, num_frames=num_frames)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states=hidden_states, output_size=upsample_size)
        return hidden_states


class UNetMidBlockCrossAttnMotion(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, output_scale_factor: 'float'=1.0, cross_attention_dim: 'int'=1280, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, upcast_attention: 'bool'=False, attention_type: 'str'='default', temporal_num_attention_heads: 'int'=1, temporal_cross_attention_dim: 'Optional[int]'=None, temporal_max_seq_length: 'int'=32, temporal_transformer_layers_per_block: 'Union[int, Tuple[int]]'=1):
        super().__init__()
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = (transformer_layers_per_block,) * num_layers
        elif len(transformer_layers_per_block) != num_layers:
            raise ValueError(f'`transformer_layers_per_block` should be an integer or a list of integers of length {num_layers}.')
        if isinstance(temporal_transformer_layers_per_block, int):
            temporal_transformer_layers_per_block = (temporal_transformer_layers_per_block,) * num_layers
        elif len(temporal_transformer_layers_per_block) != num_layers:
            raise ValueError(f'`temporal_transformer_layers_per_block` should be an integer or a list of integers of length {num_layers}.')
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        motion_modules = []
        for i in range(num_layers):
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(num_attention_heads, in_channels // num_attention_heads, in_channels=in_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention, attention_type=attention_type))
            else:
                attentions.append(DualTransformer2DModel(num_attention_heads, in_channels // num_attention_heads, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            motion_modules.append(AnimateDiffTransformer3D(num_attention_heads=temporal_num_attention_heads, attention_head_dim=in_channels // temporal_num_attention_heads, in_channels=in_channels, num_layers=temporal_transformer_layers_per_block[i], norm_num_groups=resnet_groups, cross_attention_dim=temporal_cross_attention_dim, attention_bias=False, positional_embeddings='sinusoidal', num_positional_embeddings=temporal_max_seq_length, activation_fn='geglu'))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.motion_modules = nn.ModuleList(motion_modules)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, num_frames: 'int'=1) ->torch.Tensor:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        hidden_states = self.resnets[0](input_tensor=hidden_states, temb=temb)
        blocks = zip(self.attentions, self.resnets[1:], self.motion_modules)
        for attn, resnet, motion_module in blocks:
            hidden_states = attn(hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(motion_module), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
            else:
                hidden_states = motion_module(hidden_states, num_frames=num_frames)
                hidden_states = resnet(input_tensor=hidden_states, temb=temb)
        return hidden_states


class MotionModules(nn.Module):

    def __init__(self, in_channels: 'int', layers_per_block: 'int'=2, transformer_layers_per_block: 'Union[int, Tuple[int]]'=8, num_attention_heads: 'Union[int, Tuple[int]]'=8, attention_bias: 'bool'=False, cross_attention_dim: 'Optional[int]'=None, activation_fn: 'str'='geglu', norm_num_groups: 'int'=32, max_seq_length: 'int'=32):
        super().__init__()
        self.motion_modules = nn.ModuleList([])
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = (transformer_layers_per_block,) * layers_per_block
        elif len(transformer_layers_per_block) != layers_per_block:
            raise ValueError(f'The number of transformer layers per block must match the number of layers per block, got {layers_per_block} and {len(transformer_layers_per_block)}')
        for i in range(layers_per_block):
            self.motion_modules.append(AnimateDiffTransformer3D(in_channels=in_channels, num_layers=transformer_layers_per_block[i], norm_num_groups=norm_num_groups, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, attention_bias=attention_bias, num_attention_heads=num_attention_heads, attention_head_dim=in_channels // num_attention_heads, positional_embeddings='sinusoidal', num_positional_embeddings=max_seq_length))


class MotionAdapter(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UNetMotionModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UNetSpatioTemporalConditionModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class SDCascadeLayerNorm(nn.LayerNorm):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, x):
        x = x.permute(0, 2, 3, 1)
        x = super().forward(x)
        return x.permute(0, 3, 1, 2)


class SDCascadeTimestepBlock(nn.Module):

    def __init__(self, c, c_timestep, conds=[]):
        super().__init__()
        self.mapper = nn.Linear(c_timestep, c * 2)
        self.conds = conds
        for cname in conds:
            setattr(self, f'mapper_{cname}', nn.Linear(c_timestep, c * 2))

    def forward(self, x, t):
        t = t.chunk(len(self.conds) + 1, dim=1)
        a, b = self.mapper(t[0])[:, :, None, None].chunk(2, dim=1)
        for i, c in enumerate(self.conds):
            ac, bc = getattr(self, f'mapper_{c}')(t[i + 1])[:, :, None, None].chunk(2, dim=1)
            a, b = a + ac, b + bc
        return x * (1 + a) + b


class SDCascadeResBlock(nn.Module):

    def __init__(self, c, c_skip=0, kernel_size=3, dropout=0.0):
        super().__init__()
        self.depthwise = nn.Conv2d(c, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c)
        self.norm = SDCascadeLayerNorm(c, elementwise_affine=False, eps=1e-06)
        self.channelwise = nn.Sequential(nn.Linear(c + c_skip, c * 4), nn.GELU(), GlobalResponseNorm(c * 4), nn.Dropout(dropout), nn.Linear(c * 4, c))

    def forward(self, x, x_skip=None):
        x_res = x
        x = self.norm(self.depthwise(x))
        if x_skip is not None:
            x = torch.cat([x, x_skip], dim=1)
        x = self.channelwise(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        return x + x_res


class SDCascadeAttnBlock(nn.Module):

    def __init__(self, c, c_cond, nhead, self_attn=True, dropout=0.0):
        super().__init__()
        self.self_attn = self_attn
        self.norm = SDCascadeLayerNorm(c, elementwise_affine=False, eps=1e-06)
        self.attention = Attention(query_dim=c, heads=nhead, dim_head=c // nhead, dropout=dropout, bias=True)
        self.kv_mapper = nn.Sequential(nn.SiLU(), nn.Linear(c_cond, c))

    def forward(self, x, kv):
        kv = self.kv_mapper(kv)
        norm_x = self.norm(x)
        if self.self_attn:
            batch_size, channel, _, _ = x.shape
            kv = torch.cat([norm_x.view(batch_size, channel, -1).transpose(1, 2), kv], dim=1)
        x = x + self.attention(norm_x, encoder_hidden_states=kv)
        return x


class UpDownBlock2d(nn.Module):

    def __init__(self, in_channels, out_channels, mode, enabled=True):
        super().__init__()
        if mode not in ['up', 'down']:
            raise ValueError(f'{mode} not supported')
        interpolation = nn.Upsample(scale_factor=2 if mode == 'up' else 0.5, mode='bilinear', align_corners=True) if enabled else nn.Identity()
        mapping = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.blocks = nn.ModuleList([interpolation, mapping] if mode == 'up' else [mapping, interpolation])

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x


def convert_animatediff_checkpoint_to_diffusers(checkpoint, **kwargs):
    converted_state_dict = {}
    for k, v in checkpoint.items():
        if 'pos_encoder' in k:
            continue
        else:
            converted_state_dict[k.replace('.norms.0', '.norm1').replace('.norms.1', '.norm2').replace('.ff_norm', '.norm3').replace('.attention_blocks.0', '.attn1').replace('.attention_blocks.1', '.attn2').replace('.temporal_transformer', '')] = v
    return converted_state_dict


class ControlNetModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


def assign_to_checkpoint(paths, checkpoint, old_checkpoint, attention_paths_to_split=None, additional_replacements=None, config=None):
    """
    This does the final conversion step: take locally converted weights and apply a global renaming to them. It splits
    attention layers, and takes into account additional replacements that may arise.

    Assigns the weights to the new checkpoint.
    """
    assert isinstance(paths, list), "Paths should be a list of dicts containing 'old' and 'new' keys."
    if attention_paths_to_split is not None:
        for path, path_map in attention_paths_to_split.items():
            old_tensor = old_checkpoint[path]
            channels = old_tensor.shape[0] // 3
            target_shape = (-1, channels) if len(old_tensor.shape) == 3 else -1
            num_heads = old_tensor.shape[0] // config['num_head_channels'] // 3
            old_tensor = old_tensor.reshape((num_heads, 3 * channels // num_heads) + old_tensor.shape[1:])
            query, key, value = old_tensor.split(channels // num_heads, dim=1)
            checkpoint[path_map['query']] = query.reshape(target_shape)
            checkpoint[path_map['key']] = key.reshape(target_shape)
            checkpoint[path_map['value']] = value.reshape(target_shape)
    for path in paths:
        new_path = path['new']
        if attention_paths_to_split is not None and new_path in attention_paths_to_split:
            continue
        new_path = new_path.replace('middle_block.0', 'mid_block.resnets.0')
        new_path = new_path.replace('middle_block.1', 'mid_block.attentions.0')
        new_path = new_path.replace('middle_block.2', 'mid_block.resnets.1')
        if additional_replacements is not None:
            for replacement in additional_replacements:
                new_path = new_path.replace(replacement['old'], replacement['new'])
        is_attn_weight = 'proj_attn.weight' in new_path or 'attentions' in new_path and 'to_' in new_path
        shape = old_checkpoint[path['old']].shape
        if is_attn_weight and len(shape) == 3:
            checkpoint[new_path] = old_checkpoint[path['old']][:, :, 0]
        elif is_attn_weight and len(shape) == 4:
            checkpoint[new_path] = old_checkpoint[path['old']][:, :, 0, 0]
        else:
            checkpoint[new_path] = old_checkpoint[path['old']]


def renew_attention_paths(old_list, n_shave_prefix_segments=0):
    """
    Updates paths inside attentions to the new naming scheme (local renaming)
    """
    mapping = []
    for old_item in old_list:
        new_item = old_item
        mapping.append({'old': old_item, 'new': new_item})
    return mapping


def shave_segments(path, n_shave_prefix_segments=1):
    """
    Removes segments. Positive values shave the first segments, negative shave the last segments.
    """
    if n_shave_prefix_segments >= 0:
        return '.'.join(path.split('.')[n_shave_prefix_segments:])
    else:
        return '.'.join(path.split('.')[:n_shave_prefix_segments])


def renew_resnet_paths(old_list, n_shave_prefix_segments=0):
    """
    Updates paths inside resnets to the new naming scheme (local renaming)
    """
    mapping = []
    for old_item in old_list:
        new_item = old_item.replace('in_layers.0', 'norm1')
        new_item = new_item.replace('in_layers.2', 'conv1')
        new_item = new_item.replace('out_layers.0', 'norm2')
        new_item = new_item.replace('out_layers.3', 'conv2')
        new_item = new_item.replace('emb_layers.1', 'time_emb_proj')
        new_item = new_item.replace('skip_connection', 'conv_shortcut')
        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)
        mapping.append({'old': old_item, 'new': new_item})
    return mapping


def convert_ldm_unet_checkpoint(checkpoint, config, path=None, extract_ema=False, controlnet=False, skip_extract_state_dict=False):
    """
    Takes a state dict and a config, and returns a converted checkpoint.
    """
    if skip_extract_state_dict:
        unet_state_dict = checkpoint
    else:
        unet_state_dict = {}
        keys = list(checkpoint.keys())
        if controlnet:
            unet_key = 'control_model.'
        else:
            unet_key = 'model.diffusion_model.'
        if sum(k.startswith('model_ema') for k in keys) > 100 and extract_ema:
            logger.warning(f'Checkpoint {path} has both EMA and non-EMA weights.')
            logger.warning('In this conversion only the EMA weights are extracted. If you want to instead extract the non-EMA weights (useful to continue fine-tuning), please make sure to remove the `--extract_ema` flag.')
            for key in keys:
                if key.startswith('model.diffusion_model'):
                    flat_ema_key = 'model_ema.' + ''.join(key.split('.')[1:])
                    unet_state_dict[key.replace(unet_key, '')] = checkpoint.pop(flat_ema_key)
        else:
            if sum(k.startswith('model_ema') for k in keys) > 100:
                logger.warning('In this conversion only the non-EMA weights are extracted. If you want to instead extract the EMA weights (usually better for inference), please make sure to add the `--extract_ema` flag.')
            for key in keys:
                if key.startswith(unet_key):
                    unet_state_dict[key.replace(unet_key, '')] = checkpoint.pop(key)
    new_checkpoint = {}
    new_checkpoint['time_embedding.linear_1.weight'] = unet_state_dict['time_embed.0.weight']
    new_checkpoint['time_embedding.linear_1.bias'] = unet_state_dict['time_embed.0.bias']
    new_checkpoint['time_embedding.linear_2.weight'] = unet_state_dict['time_embed.2.weight']
    new_checkpoint['time_embedding.linear_2.bias'] = unet_state_dict['time_embed.2.bias']
    if config['class_embed_type'] is None:
        ...
    elif config['class_embed_type'] == 'timestep' or config['class_embed_type'] == 'projection':
        new_checkpoint['class_embedding.linear_1.weight'] = unet_state_dict['label_emb.0.0.weight']
        new_checkpoint['class_embedding.linear_1.bias'] = unet_state_dict['label_emb.0.0.bias']
        new_checkpoint['class_embedding.linear_2.weight'] = unet_state_dict['label_emb.0.2.weight']
        new_checkpoint['class_embedding.linear_2.bias'] = unet_state_dict['label_emb.0.2.bias']
    else:
        raise NotImplementedError(f"Not implemented `class_embed_type`: {config['class_embed_type']}")
    if config['addition_embed_type'] == 'text_time':
        new_checkpoint['add_embedding.linear_1.weight'] = unet_state_dict['label_emb.0.0.weight']
        new_checkpoint['add_embedding.linear_1.bias'] = unet_state_dict['label_emb.0.0.bias']
        new_checkpoint['add_embedding.linear_2.weight'] = unet_state_dict['label_emb.0.2.weight']
        new_checkpoint['add_embedding.linear_2.bias'] = unet_state_dict['label_emb.0.2.bias']
    if 'num_class_embeds' in config:
        if config['num_class_embeds'] is not None and 'label_emb.weight' in unet_state_dict:
            new_checkpoint['class_embedding.weight'] = unet_state_dict['label_emb.weight']
    new_checkpoint['conv_in.weight'] = unet_state_dict['input_blocks.0.0.weight']
    new_checkpoint['conv_in.bias'] = unet_state_dict['input_blocks.0.0.bias']
    if not controlnet:
        new_checkpoint['conv_norm_out.weight'] = unet_state_dict['out.0.weight']
        new_checkpoint['conv_norm_out.bias'] = unet_state_dict['out.0.bias']
        new_checkpoint['conv_out.weight'] = unet_state_dict['out.2.weight']
        new_checkpoint['conv_out.bias'] = unet_state_dict['out.2.bias']
    num_input_blocks = len({'.'.join(layer.split('.')[:2]) for layer in unet_state_dict if 'input_blocks' in layer})
    input_blocks = {layer_id: [key for key in unet_state_dict if f'input_blocks.{layer_id}' in key] for layer_id in range(num_input_blocks)}
    num_middle_blocks = len({'.'.join(layer.split('.')[:2]) for layer in unet_state_dict if 'middle_block' in layer})
    middle_blocks = {layer_id: [key for key in unet_state_dict if f'middle_block.{layer_id}' in key] for layer_id in range(num_middle_blocks)}
    num_output_blocks = len({'.'.join(layer.split('.')[:2]) for layer in unet_state_dict if 'output_blocks' in layer})
    output_blocks = {layer_id: [key for key in unet_state_dict if f'output_blocks.{layer_id}' in key] for layer_id in range(num_output_blocks)}
    for i in range(1, num_input_blocks):
        block_id = (i - 1) // (config['layers_per_block'] + 1)
        layer_in_block_id = (i - 1) % (config['layers_per_block'] + 1)
        resnets = [key for key in input_blocks[i] if f'input_blocks.{i}.0' in key and f'input_blocks.{i}.0.op' not in key]
        attentions = [key for key in input_blocks[i] if f'input_blocks.{i}.1' in key]
        if f'input_blocks.{i}.0.op.weight' in unet_state_dict:
            new_checkpoint[f'down_blocks.{block_id}.downsamplers.0.conv.weight'] = unet_state_dict.pop(f'input_blocks.{i}.0.op.weight')
            new_checkpoint[f'down_blocks.{block_id}.downsamplers.0.conv.bias'] = unet_state_dict.pop(f'input_blocks.{i}.0.op.bias')
        paths = renew_resnet_paths(resnets)
        meta_path = {'old': f'input_blocks.{i}.0', 'new': f'down_blocks.{block_id}.resnets.{layer_in_block_id}'}
        assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)
        if len(attentions):
            paths = renew_attention_paths(attentions)
            meta_path = {'old': f'input_blocks.{i}.1', 'new': f'down_blocks.{block_id}.attentions.{layer_in_block_id}'}
            assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)
    resnet_0 = middle_blocks[0]
    attentions = middle_blocks[1]
    resnet_1 = middle_blocks[2]
    resnet_0_paths = renew_resnet_paths(resnet_0)
    assign_to_checkpoint(resnet_0_paths, new_checkpoint, unet_state_dict, config=config)
    resnet_1_paths = renew_resnet_paths(resnet_1)
    assign_to_checkpoint(resnet_1_paths, new_checkpoint, unet_state_dict, config=config)
    attentions_paths = renew_attention_paths(attentions)
    meta_path = {'old': 'middle_block.1', 'new': 'mid_block.attentions.0'}
    assign_to_checkpoint(attentions_paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)
    for i in range(num_output_blocks):
        block_id = i // (config['layers_per_block'] + 1)
        layer_in_block_id = i % (config['layers_per_block'] + 1)
        output_block_layers = [shave_segments(name, 2) for name in output_blocks[i]]
        output_block_list = {}
        for layer in output_block_layers:
            layer_id, layer_name = layer.split('.')[0], shave_segments(layer, 1)
            if layer_id in output_block_list:
                output_block_list[layer_id].append(layer_name)
            else:
                output_block_list[layer_id] = [layer_name]
        if len(output_block_list) > 1:
            resnets = [key for key in output_blocks[i] if f'output_blocks.{i}.0' in key]
            attentions = [key for key in output_blocks[i] if f'output_blocks.{i}.1' in key]
            resnet_0_paths = renew_resnet_paths(resnets)
            paths = renew_resnet_paths(resnets)
            meta_path = {'old': f'output_blocks.{i}.0', 'new': f'up_blocks.{block_id}.resnets.{layer_in_block_id}'}
            assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)
            output_block_list = {k: sorted(v) for k, v in sorted(output_block_list.items())}
            if ['conv.bias', 'conv.weight'] in output_block_list.values():
                index = list(output_block_list.values()).index(['conv.bias', 'conv.weight'])
                new_checkpoint[f'up_blocks.{block_id}.upsamplers.0.conv.weight'] = unet_state_dict[f'output_blocks.{i}.{index}.conv.weight']
                new_checkpoint[f'up_blocks.{block_id}.upsamplers.0.conv.bias'] = unet_state_dict[f'output_blocks.{i}.{index}.conv.bias']
                if len(attentions) == 2:
                    attentions = []
            if len(attentions):
                paths = renew_attention_paths(attentions)
                meta_path = {'old': f'output_blocks.{i}.1', 'new': f'up_blocks.{block_id}.attentions.{layer_in_block_id}'}
                assign_to_checkpoint(paths, new_checkpoint, unet_state_dict, additional_replacements=[meta_path], config=config)
        else:
            resnet_0_paths = renew_resnet_paths(output_block_layers, n_shave_prefix_segments=1)
            for path in resnet_0_paths:
                old_path = '.'.join(['output_blocks', str(i), path['old']])
                new_path = '.'.join(['up_blocks', str(block_id), 'resnets', str(layer_in_block_id), path['new']])
                new_checkpoint[new_path] = unet_state_dict[old_path]
    if controlnet:
        orig_index = 0
        new_checkpoint['controlnet_cond_embedding.conv_in.weight'] = unet_state_dict.pop(f'input_hint_block.{orig_index}.weight')
        new_checkpoint['controlnet_cond_embedding.conv_in.bias'] = unet_state_dict.pop(f'input_hint_block.{orig_index}.bias')
        orig_index += 2
        diffusers_index = 0
        while diffusers_index < 6:
            new_checkpoint[f'controlnet_cond_embedding.blocks.{diffusers_index}.weight'] = unet_state_dict.pop(f'input_hint_block.{orig_index}.weight')
            new_checkpoint[f'controlnet_cond_embedding.blocks.{diffusers_index}.bias'] = unet_state_dict.pop(f'input_hint_block.{orig_index}.bias')
            diffusers_index += 1
            orig_index += 2
        new_checkpoint['controlnet_cond_embedding.conv_out.weight'] = unet_state_dict.pop(f'input_hint_block.{orig_index}.weight')
        new_checkpoint['controlnet_cond_embedding.conv_out.bias'] = unet_state_dict.pop(f'input_hint_block.{orig_index}.bias')
        for i in range(num_input_blocks):
            new_checkpoint[f'controlnet_down_blocks.{i}.weight'] = unet_state_dict.pop(f'zero_convs.{i}.0.weight')
            new_checkpoint[f'controlnet_down_blocks.{i}.bias'] = unet_state_dict.pop(f'zero_convs.{i}.0.bias')
        new_checkpoint['controlnet_mid_block.weight'] = unet_state_dict.pop('middle_block_out.0.weight')
        new_checkpoint['controlnet_mid_block.bias'] = unet_state_dict.pop('middle_block_out.0.bias')
    return new_checkpoint


def create_unet_diffusers_config(original_config, image_size: 'int', controlnet=False):
    """
    Creates a config for the diffusers based on the config of the LDM model.
    """
    if controlnet:
        unet_params = original_config['model']['params']['control_stage_config']['params']
    elif 'unet_config' in original_config['model']['params'] and original_config['model']['params']['unet_config'] is not None:
        unet_params = original_config['model']['params']['unet_config']['params']
    else:
        unet_params = original_config['model']['params']['network_config']['params']
    vae_params = original_config['model']['params']['first_stage_config']['params']['ddconfig']
    block_out_channels = [(unet_params['model_channels'] * mult) for mult in unet_params['channel_mult']]
    down_block_types = []
    resolution = 1
    for i in range(len(block_out_channels)):
        block_type = 'CrossAttnDownBlock2D' if resolution in unet_params['attention_resolutions'] else 'DownBlock2D'
        down_block_types.append(block_type)
        if i != len(block_out_channels) - 1:
            resolution *= 2
    up_block_types = []
    for i in range(len(block_out_channels)):
        block_type = 'CrossAttnUpBlock2D' if resolution in unet_params['attention_resolutions'] else 'UpBlock2D'
        up_block_types.append(block_type)
        resolution //= 2
    if unet_params['transformer_depth'] is not None:
        transformer_layers_per_block = unet_params['transformer_depth'] if isinstance(unet_params['transformer_depth'], int) else list(unet_params['transformer_depth'])
    else:
        transformer_layers_per_block = 1
    vae_scale_factor = 2 ** (len(vae_params['ch_mult']) - 1)
    head_dim = unet_params['num_heads'] if 'num_heads' in unet_params else None
    use_linear_projection = unet_params['use_linear_in_transformer'] if 'use_linear_in_transformer' in unet_params else False
    if use_linear_projection:
        if head_dim is None:
            head_dim_mult = unet_params['model_channels'] // unet_params['num_head_channels']
            head_dim = [(head_dim_mult * c) for c in list(unet_params['channel_mult'])]
    class_embed_type = None
    addition_embed_type = None
    addition_time_embed_dim = None
    projection_class_embeddings_input_dim = None
    context_dim = None
    if unet_params['context_dim'] is not None:
        context_dim = unet_params['context_dim'] if isinstance(unet_params['context_dim'], int) else unet_params['context_dim'][0]
    if 'num_classes' in unet_params:
        if unet_params['num_classes'] == 'sequential':
            if context_dim in [2048, 1280]:
                addition_embed_type = 'text_time'
                addition_time_embed_dim = 256
            else:
                class_embed_type = 'projection'
            assert 'adm_in_channels' in unet_params
            projection_class_embeddings_input_dim = unet_params['adm_in_channels']
    config = {'sample_size': image_size // vae_scale_factor, 'in_channels': unet_params['in_channels'], 'down_block_types': tuple(down_block_types), 'block_out_channels': tuple(block_out_channels), 'layers_per_block': unet_params['num_res_blocks'], 'cross_attention_dim': context_dim, 'attention_head_dim': head_dim, 'use_linear_projection': use_linear_projection, 'class_embed_type': class_embed_type, 'addition_embed_type': addition_embed_type, 'addition_time_embed_dim': addition_time_embed_dim, 'projection_class_embeddings_input_dim': projection_class_embeddings_input_dim, 'transformer_layers_per_block': transformer_layers_per_block}
    if 'disable_self_attentions' in unet_params:
        config['only_cross_attention'] = unet_params['disable_self_attentions']
    if 'num_classes' in unet_params and isinstance(unet_params['num_classes'], int):
        config['num_class_embeds'] = unet_params['num_classes']
    if controlnet:
        config['conditioning_channels'] = unet_params['hint_channels']
    else:
        config['out_channels'] = unet_params['out_channels']
        config['up_block_types'] = tuple(up_block_types)
    return config


def is_accelerate_available():
    return _accelerate_available


def convert_controlnet_checkpoint(checkpoint, original_config, checkpoint_path, image_size, upcast_attention, extract_ema, use_linear_projection=None, cross_attention_dim=None):
    ctrlnet_config = create_unet_diffusers_config(original_config, image_size=image_size, controlnet=True)
    ctrlnet_config['upcast_attention'] = upcast_attention
    ctrlnet_config.pop('sample_size')
    if use_linear_projection is not None:
        ctrlnet_config['use_linear_projection'] = use_linear_projection
    if cross_attention_dim is not None:
        ctrlnet_config['cross_attention_dim'] = cross_attention_dim
    ctx = init_empty_weights if is_accelerate_available() else nullcontext
    with ctx():
        controlnet = ControlNetModel(**ctrlnet_config)
    if 'time_embed.0.weight' in checkpoint:
        skip_extract_state_dict = True
    else:
        skip_extract_state_dict = False
    converted_ctrl_checkpoint = convert_ldm_unet_checkpoint(checkpoint, ctrlnet_config, path=checkpoint_path, extract_ema=extract_ema, controlnet=True, skip_extract_state_dict=skip_extract_state_dict)
    if is_accelerate_available():
        for param_name, param in converted_ctrl_checkpoint.items():
            set_module_tensor_to_device(controlnet, param_name, 'cpu', value=param)
    else:
        controlnet.load_state_dict(converted_ctrl_checkpoint)
    return controlnet


def convert_flux_transformer_checkpoint_to_diffusers(checkpoint, **kwargs):
    converted_state_dict = {}
    keys = list(checkpoint.keys())
    for k in keys:
        if 'model.diffusion_model.' in k:
            checkpoint[k.replace('model.diffusion_model.', '')] = checkpoint.pop(k)
    num_layers = list(set(int(k.split('.', 2)[1]) for k in checkpoint if 'double_blocks.' in k))[-1] + 1
    num_single_layers = list(set(int(k.split('.', 2)[1]) for k in checkpoint if 'single_blocks.' in k))[-1] + 1
    mlp_ratio = 4.0
    inner_dim = 3072

    def swap_scale_shift(weight):
        shift, scale = weight.chunk(2, dim=0)
        new_weight = torch.cat([scale, shift], dim=0)
        return new_weight
    converted_state_dict['time_text_embed.timestep_embedder.linear_1.weight'] = checkpoint.pop('time_in.in_layer.weight')
    converted_state_dict['time_text_embed.timestep_embedder.linear_1.bias'] = checkpoint.pop('time_in.in_layer.bias')
    converted_state_dict['time_text_embed.timestep_embedder.linear_2.weight'] = checkpoint.pop('time_in.out_layer.weight')
    converted_state_dict['time_text_embed.timestep_embedder.linear_2.bias'] = checkpoint.pop('time_in.out_layer.bias')
    converted_state_dict['time_text_embed.text_embedder.linear_1.weight'] = checkpoint.pop('vector_in.in_layer.weight')
    converted_state_dict['time_text_embed.text_embedder.linear_1.bias'] = checkpoint.pop('vector_in.in_layer.bias')
    converted_state_dict['time_text_embed.text_embedder.linear_2.weight'] = checkpoint.pop('vector_in.out_layer.weight')
    converted_state_dict['time_text_embed.text_embedder.linear_2.bias'] = checkpoint.pop('vector_in.out_layer.bias')
    has_guidance = any('guidance' in k for k in checkpoint)
    if has_guidance:
        converted_state_dict['time_text_embed.guidance_embedder.linear_1.weight'] = checkpoint.pop('guidance_in.in_layer.weight')
        converted_state_dict['time_text_embed.guidance_embedder.linear_1.bias'] = checkpoint.pop('guidance_in.in_layer.bias')
        converted_state_dict['time_text_embed.guidance_embedder.linear_2.weight'] = checkpoint.pop('guidance_in.out_layer.weight')
        converted_state_dict['time_text_embed.guidance_embedder.linear_2.bias'] = checkpoint.pop('guidance_in.out_layer.bias')
    converted_state_dict['context_embedder.weight'] = checkpoint.pop('txt_in.weight')
    converted_state_dict['context_embedder.bias'] = checkpoint.pop('txt_in.bias')
    converted_state_dict['x_embedder.weight'] = checkpoint.pop('img_in.weight')
    converted_state_dict['x_embedder.bias'] = checkpoint.pop('img_in.bias')
    for i in range(num_layers):
        block_prefix = f'transformer_blocks.{i}.'
        converted_state_dict[f'{block_prefix}norm1.linear.weight'] = checkpoint.pop(f'double_blocks.{i}.img_mod.lin.weight')
        converted_state_dict[f'{block_prefix}norm1.linear.bias'] = checkpoint.pop(f'double_blocks.{i}.img_mod.lin.bias')
        converted_state_dict[f'{block_prefix}norm1_context.linear.weight'] = checkpoint.pop(f'double_blocks.{i}.txt_mod.lin.weight')
        converted_state_dict[f'{block_prefix}norm1_context.linear.bias'] = checkpoint.pop(f'double_blocks.{i}.txt_mod.lin.bias')
        sample_q, sample_k, sample_v = torch.chunk(checkpoint.pop(f'double_blocks.{i}.img_attn.qkv.weight'), 3, dim=0)
        context_q, context_k, context_v = torch.chunk(checkpoint.pop(f'double_blocks.{i}.txt_attn.qkv.weight'), 3, dim=0)
        sample_q_bias, sample_k_bias, sample_v_bias = torch.chunk(checkpoint.pop(f'double_blocks.{i}.img_attn.qkv.bias'), 3, dim=0)
        context_q_bias, context_k_bias, context_v_bias = torch.chunk(checkpoint.pop(f'double_blocks.{i}.txt_attn.qkv.bias'), 3, dim=0)
        converted_state_dict[f'{block_prefix}attn.to_q.weight'] = torch.cat([sample_q])
        converted_state_dict[f'{block_prefix}attn.to_q.bias'] = torch.cat([sample_q_bias])
        converted_state_dict[f'{block_prefix}attn.to_k.weight'] = torch.cat([sample_k])
        converted_state_dict[f'{block_prefix}attn.to_k.bias'] = torch.cat([sample_k_bias])
        converted_state_dict[f'{block_prefix}attn.to_v.weight'] = torch.cat([sample_v])
        converted_state_dict[f'{block_prefix}attn.to_v.bias'] = torch.cat([sample_v_bias])
        converted_state_dict[f'{block_prefix}attn.add_q_proj.weight'] = torch.cat([context_q])
        converted_state_dict[f'{block_prefix}attn.add_q_proj.bias'] = torch.cat([context_q_bias])
        converted_state_dict[f'{block_prefix}attn.add_k_proj.weight'] = torch.cat([context_k])
        converted_state_dict[f'{block_prefix}attn.add_k_proj.bias'] = torch.cat([context_k_bias])
        converted_state_dict[f'{block_prefix}attn.add_v_proj.weight'] = torch.cat([context_v])
        converted_state_dict[f'{block_prefix}attn.add_v_proj.bias'] = torch.cat([context_v_bias])
        converted_state_dict[f'{block_prefix}attn.norm_q.weight'] = checkpoint.pop(f'double_blocks.{i}.img_attn.norm.query_norm.scale')
        converted_state_dict[f'{block_prefix}attn.norm_k.weight'] = checkpoint.pop(f'double_blocks.{i}.img_attn.norm.key_norm.scale')
        converted_state_dict[f'{block_prefix}attn.norm_added_q.weight'] = checkpoint.pop(f'double_blocks.{i}.txt_attn.norm.query_norm.scale')
        converted_state_dict[f'{block_prefix}attn.norm_added_k.weight'] = checkpoint.pop(f'double_blocks.{i}.txt_attn.norm.key_norm.scale')
        converted_state_dict[f'{block_prefix}ff.net.0.proj.weight'] = checkpoint.pop(f'double_blocks.{i}.img_mlp.0.weight')
        converted_state_dict[f'{block_prefix}ff.net.0.proj.bias'] = checkpoint.pop(f'double_blocks.{i}.img_mlp.0.bias')
        converted_state_dict[f'{block_prefix}ff.net.2.weight'] = checkpoint.pop(f'double_blocks.{i}.img_mlp.2.weight')
        converted_state_dict[f'{block_prefix}ff.net.2.bias'] = checkpoint.pop(f'double_blocks.{i}.img_mlp.2.bias')
        converted_state_dict[f'{block_prefix}ff_context.net.0.proj.weight'] = checkpoint.pop(f'double_blocks.{i}.txt_mlp.0.weight')
        converted_state_dict[f'{block_prefix}ff_context.net.0.proj.bias'] = checkpoint.pop(f'double_blocks.{i}.txt_mlp.0.bias')
        converted_state_dict[f'{block_prefix}ff_context.net.2.weight'] = checkpoint.pop(f'double_blocks.{i}.txt_mlp.2.weight')
        converted_state_dict[f'{block_prefix}ff_context.net.2.bias'] = checkpoint.pop(f'double_blocks.{i}.txt_mlp.2.bias')
        converted_state_dict[f'{block_prefix}attn.to_out.0.weight'] = checkpoint.pop(f'double_blocks.{i}.img_attn.proj.weight')
        converted_state_dict[f'{block_prefix}attn.to_out.0.bias'] = checkpoint.pop(f'double_blocks.{i}.img_attn.proj.bias')
        converted_state_dict[f'{block_prefix}attn.to_add_out.weight'] = checkpoint.pop(f'double_blocks.{i}.txt_attn.proj.weight')
        converted_state_dict[f'{block_prefix}attn.to_add_out.bias'] = checkpoint.pop(f'double_blocks.{i}.txt_attn.proj.bias')
    for i in range(num_single_layers):
        block_prefix = f'single_transformer_blocks.{i}.'
        converted_state_dict[f'{block_prefix}norm.linear.weight'] = checkpoint.pop(f'single_blocks.{i}.modulation.lin.weight')
        converted_state_dict[f'{block_prefix}norm.linear.bias'] = checkpoint.pop(f'single_blocks.{i}.modulation.lin.bias')
        mlp_hidden_dim = int(inner_dim * mlp_ratio)
        split_size = inner_dim, inner_dim, inner_dim, mlp_hidden_dim
        q, k, v, mlp = torch.split(checkpoint.pop(f'single_blocks.{i}.linear1.weight'), split_size, dim=0)
        q_bias, k_bias, v_bias, mlp_bias = torch.split(checkpoint.pop(f'single_blocks.{i}.linear1.bias'), split_size, dim=0)
        converted_state_dict[f'{block_prefix}attn.to_q.weight'] = torch.cat([q])
        converted_state_dict[f'{block_prefix}attn.to_q.bias'] = torch.cat([q_bias])
        converted_state_dict[f'{block_prefix}attn.to_k.weight'] = torch.cat([k])
        converted_state_dict[f'{block_prefix}attn.to_k.bias'] = torch.cat([k_bias])
        converted_state_dict[f'{block_prefix}attn.to_v.weight'] = torch.cat([v])
        converted_state_dict[f'{block_prefix}attn.to_v.bias'] = torch.cat([v_bias])
        converted_state_dict[f'{block_prefix}proj_mlp.weight'] = torch.cat([mlp])
        converted_state_dict[f'{block_prefix}proj_mlp.bias'] = torch.cat([mlp_bias])
        converted_state_dict[f'{block_prefix}attn.norm_q.weight'] = checkpoint.pop(f'single_blocks.{i}.norm.query_norm.scale')
        converted_state_dict[f'{block_prefix}attn.norm_k.weight'] = checkpoint.pop(f'single_blocks.{i}.norm.key_norm.scale')
        converted_state_dict[f'{block_prefix}proj_out.weight'] = checkpoint.pop(f'single_blocks.{i}.linear2.weight')
        converted_state_dict[f'{block_prefix}proj_out.bias'] = checkpoint.pop(f'single_blocks.{i}.linear2.bias')
    converted_state_dict['proj_out.weight'] = checkpoint.pop('final_layer.linear.weight')
    converted_state_dict['proj_out.bias'] = checkpoint.pop('final_layer.linear.bias')
    converted_state_dict['norm_out.linear.weight'] = swap_scale_shift(checkpoint.pop('final_layer.adaLN_modulation.1.weight'))
    converted_state_dict['norm_out.linear.bias'] = swap_scale_shift(checkpoint.pop('final_layer.adaLN_modulation.1.bias'))
    return converted_state_dict


def conv_attn_to_linear(checkpoint):
    keys = list(checkpoint.keys())
    attn_keys = ['query.weight', 'key.weight', 'value.weight']
    for key in keys:
        if '.'.join(key.split('.')[-2:]) in attn_keys:
            if checkpoint[key].ndim > 2:
                checkpoint[key] = checkpoint[key][:, :, 0, 0]
        elif 'proj_attn.weight' in key:
            if checkpoint[key].ndim > 2:
                checkpoint[key] = checkpoint[key][:, :, 0]


def renew_vae_attention_paths(old_list, n_shave_prefix_segments=0):
    """
    Updates paths inside attentions to the new naming scheme (local renaming)
    """
    mapping = []
    for old_item in old_list:
        new_item = old_item
        new_item = new_item.replace('norm.weight', 'group_norm.weight')
        new_item = new_item.replace('norm.bias', 'group_norm.bias')
        new_item = new_item.replace('q.weight', 'to_q.weight')
        new_item = new_item.replace('q.bias', 'to_q.bias')
        new_item = new_item.replace('k.weight', 'to_k.weight')
        new_item = new_item.replace('k.bias', 'to_k.bias')
        new_item = new_item.replace('v.weight', 'to_v.weight')
        new_item = new_item.replace('v.bias', 'to_v.bias')
        new_item = new_item.replace('proj_out.weight', 'to_out.0.weight')
        new_item = new_item.replace('proj_out.bias', 'to_out.0.bias')
        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)
        mapping.append({'old': old_item, 'new': new_item})
    return mapping


def renew_vae_resnet_paths(old_list, n_shave_prefix_segments=0):
    """
    Updates paths inside resnets to the new naming scheme (local renaming)
    """
    mapping = []
    for old_item in old_list:
        new_item = old_item
        new_item = new_item.replace('nin_shortcut', 'conv_shortcut')
        new_item = shave_segments(new_item, n_shave_prefix_segments=n_shave_prefix_segments)
        mapping.append({'old': old_item, 'new': new_item})
    return mapping


def convert_ldm_vae_checkpoint(checkpoint, config):
    vae_state_dict = {}
    keys = list(checkpoint.keys())
    vae_key = 'first_stage_model.' if any(k.startswith('first_stage_model.') for k in keys) else ''
    for key in keys:
        if key.startswith(vae_key):
            vae_state_dict[key.replace(vae_key, '')] = checkpoint.get(key)
    new_checkpoint = {}
    new_checkpoint['encoder.conv_in.weight'] = vae_state_dict['encoder.conv_in.weight']
    new_checkpoint['encoder.conv_in.bias'] = vae_state_dict['encoder.conv_in.bias']
    new_checkpoint['encoder.conv_out.weight'] = vae_state_dict['encoder.conv_out.weight']
    new_checkpoint['encoder.conv_out.bias'] = vae_state_dict['encoder.conv_out.bias']
    new_checkpoint['encoder.conv_norm_out.weight'] = vae_state_dict['encoder.norm_out.weight']
    new_checkpoint['encoder.conv_norm_out.bias'] = vae_state_dict['encoder.norm_out.bias']
    new_checkpoint['decoder.conv_in.weight'] = vae_state_dict['decoder.conv_in.weight']
    new_checkpoint['decoder.conv_in.bias'] = vae_state_dict['decoder.conv_in.bias']
    new_checkpoint['decoder.conv_out.weight'] = vae_state_dict['decoder.conv_out.weight']
    new_checkpoint['decoder.conv_out.bias'] = vae_state_dict['decoder.conv_out.bias']
    new_checkpoint['decoder.conv_norm_out.weight'] = vae_state_dict['decoder.norm_out.weight']
    new_checkpoint['decoder.conv_norm_out.bias'] = vae_state_dict['decoder.norm_out.bias']
    new_checkpoint['quant_conv.weight'] = vae_state_dict['quant_conv.weight']
    new_checkpoint['quant_conv.bias'] = vae_state_dict['quant_conv.bias']
    new_checkpoint['post_quant_conv.weight'] = vae_state_dict['post_quant_conv.weight']
    new_checkpoint['post_quant_conv.bias'] = vae_state_dict['post_quant_conv.bias']
    num_down_blocks = len({'.'.join(layer.split('.')[:3]) for layer in vae_state_dict if 'encoder.down' in layer})
    down_blocks = {layer_id: [key for key in vae_state_dict if f'down.{layer_id}' in key] for layer_id in range(num_down_blocks)}
    num_up_blocks = len({'.'.join(layer.split('.')[:3]) for layer in vae_state_dict if 'decoder.up' in layer})
    up_blocks = {layer_id: [key for key in vae_state_dict if f'up.{layer_id}' in key] for layer_id in range(num_up_blocks)}
    for i in range(num_down_blocks):
        resnets = [key for key in down_blocks[i] if f'down.{i}' in key and f'down.{i}.downsample' not in key]
        if f'encoder.down.{i}.downsample.conv.weight' in vae_state_dict:
            new_checkpoint[f'encoder.down_blocks.{i}.downsamplers.0.conv.weight'] = vae_state_dict.pop(f'encoder.down.{i}.downsample.conv.weight')
            new_checkpoint[f'encoder.down_blocks.{i}.downsamplers.0.conv.bias'] = vae_state_dict.pop(f'encoder.down.{i}.downsample.conv.bias')
        paths = renew_vae_resnet_paths(resnets)
        meta_path = {'old': f'down.{i}.block', 'new': f'down_blocks.{i}.resnets'}
        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)
    mid_resnets = [key for key in vae_state_dict if 'encoder.mid.block' in key]
    num_mid_res_blocks = 2
    for i in range(1, num_mid_res_blocks + 1):
        resnets = [key for key in mid_resnets if f'encoder.mid.block_{i}' in key]
        paths = renew_vae_resnet_paths(resnets)
        meta_path = {'old': f'mid.block_{i}', 'new': f'mid_block.resnets.{i - 1}'}
        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)
    mid_attentions = [key for key in vae_state_dict if 'encoder.mid.attn' in key]
    paths = renew_vae_attention_paths(mid_attentions)
    meta_path = {'old': 'mid.attn_1', 'new': 'mid_block.attentions.0'}
    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)
    conv_attn_to_linear(new_checkpoint)
    for i in range(num_up_blocks):
        block_id = num_up_blocks - 1 - i
        resnets = [key for key in up_blocks[block_id] if f'up.{block_id}' in key and f'up.{block_id}.upsample' not in key]
        if f'decoder.up.{block_id}.upsample.conv.weight' in vae_state_dict:
            new_checkpoint[f'decoder.up_blocks.{i}.upsamplers.0.conv.weight'] = vae_state_dict[f'decoder.up.{block_id}.upsample.conv.weight']
            new_checkpoint[f'decoder.up_blocks.{i}.upsamplers.0.conv.bias'] = vae_state_dict[f'decoder.up.{block_id}.upsample.conv.bias']
        paths = renew_vae_resnet_paths(resnets)
        meta_path = {'old': f'up.{block_id}.block', 'new': f'up_blocks.{i}.resnets'}
        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)
    mid_resnets = [key for key in vae_state_dict if 'decoder.mid.block' in key]
    num_mid_res_blocks = 2
    for i in range(1, num_mid_res_blocks + 1):
        resnets = [key for key in mid_resnets if f'decoder.mid.block_{i}' in key]
        paths = renew_vae_resnet_paths(resnets)
        meta_path = {'old': f'mid.block_{i}', 'new': f'mid_block.resnets.{i - 1}'}
        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)
    mid_attentions = [key for key in vae_state_dict if 'decoder.mid.attn' in key]
    paths = renew_vae_attention_paths(mid_attentions)
    meta_path = {'old': 'mid.attn_1', 'new': 'mid_block.attentions.0'}
    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)
    conv_attn_to_linear(new_checkpoint)
    return new_checkpoint


def get_attn2_layers(state_dict):
    attn2_layers = []
    for key in state_dict.keys():
        if 'attn2.' in key:
            layer_num = int(key.split('.')[1])
            attn2_layers.append(layer_num)
    return tuple(sorted(set(attn2_layers)))


def get_caption_projection_dim(state_dict):
    caption_projection_dim = state_dict['context_embedder.weight'].shape[0]
    return caption_projection_dim


def swap_scale_shift(weight, dim):
    shift, scale = weight.chunk(2, dim=0)
    new_weight = torch.cat([scale, shift], dim=0)
    return new_weight


def convert_sd3_transformer_checkpoint_to_diffusers(checkpoint, **kwargs):
    converted_state_dict = {}
    keys = list(checkpoint.keys())
    for k in keys:
        if 'model.diffusion_model.' in k:
            checkpoint[k.replace('model.diffusion_model.', '')] = checkpoint.pop(k)
    num_layers = list(set(int(k.split('.', 2)[1]) for k in checkpoint if 'joint_blocks' in k))[-1] + 1
    dual_attention_layers = get_attn2_layers(checkpoint)
    caption_projection_dim = get_caption_projection_dim(checkpoint)
    has_qk_norm = any('ln_q' in key for key in checkpoint.keys())
    converted_state_dict['pos_embed.pos_embed'] = checkpoint.pop('pos_embed')
    converted_state_dict['pos_embed.proj.weight'] = checkpoint.pop('x_embedder.proj.weight')
    converted_state_dict['pos_embed.proj.bias'] = checkpoint.pop('x_embedder.proj.bias')
    converted_state_dict['time_text_embed.timestep_embedder.linear_1.weight'] = checkpoint.pop('t_embedder.mlp.0.weight')
    converted_state_dict['time_text_embed.timestep_embedder.linear_1.bias'] = checkpoint.pop('t_embedder.mlp.0.bias')
    converted_state_dict['time_text_embed.timestep_embedder.linear_2.weight'] = checkpoint.pop('t_embedder.mlp.2.weight')
    converted_state_dict['time_text_embed.timestep_embedder.linear_2.bias'] = checkpoint.pop('t_embedder.mlp.2.bias')
    converted_state_dict['context_embedder.weight'] = checkpoint.pop('context_embedder.weight')
    converted_state_dict['context_embedder.bias'] = checkpoint.pop('context_embedder.bias')
    converted_state_dict['time_text_embed.text_embedder.linear_1.weight'] = checkpoint.pop('y_embedder.mlp.0.weight')
    converted_state_dict['time_text_embed.text_embedder.linear_1.bias'] = checkpoint.pop('y_embedder.mlp.0.bias')
    converted_state_dict['time_text_embed.text_embedder.linear_2.weight'] = checkpoint.pop('y_embedder.mlp.2.weight')
    converted_state_dict['time_text_embed.text_embedder.linear_2.bias'] = checkpoint.pop('y_embedder.mlp.2.bias')
    for i in range(num_layers):
        sample_q, sample_k, sample_v = torch.chunk(checkpoint.pop(f'joint_blocks.{i}.x_block.attn.qkv.weight'), 3, dim=0)
        context_q, context_k, context_v = torch.chunk(checkpoint.pop(f'joint_blocks.{i}.context_block.attn.qkv.weight'), 3, dim=0)
        sample_q_bias, sample_k_bias, sample_v_bias = torch.chunk(checkpoint.pop(f'joint_blocks.{i}.x_block.attn.qkv.bias'), 3, dim=0)
        context_q_bias, context_k_bias, context_v_bias = torch.chunk(checkpoint.pop(f'joint_blocks.{i}.context_block.attn.qkv.bias'), 3, dim=0)
        converted_state_dict[f'transformer_blocks.{i}.attn.to_q.weight'] = torch.cat([sample_q])
        converted_state_dict[f'transformer_blocks.{i}.attn.to_q.bias'] = torch.cat([sample_q_bias])
        converted_state_dict[f'transformer_blocks.{i}.attn.to_k.weight'] = torch.cat([sample_k])
        converted_state_dict[f'transformer_blocks.{i}.attn.to_k.bias'] = torch.cat([sample_k_bias])
        converted_state_dict[f'transformer_blocks.{i}.attn.to_v.weight'] = torch.cat([sample_v])
        converted_state_dict[f'transformer_blocks.{i}.attn.to_v.bias'] = torch.cat([sample_v_bias])
        converted_state_dict[f'transformer_blocks.{i}.attn.add_q_proj.weight'] = torch.cat([context_q])
        converted_state_dict[f'transformer_blocks.{i}.attn.add_q_proj.bias'] = torch.cat([context_q_bias])
        converted_state_dict[f'transformer_blocks.{i}.attn.add_k_proj.weight'] = torch.cat([context_k])
        converted_state_dict[f'transformer_blocks.{i}.attn.add_k_proj.bias'] = torch.cat([context_k_bias])
        converted_state_dict[f'transformer_blocks.{i}.attn.add_v_proj.weight'] = torch.cat([context_v])
        converted_state_dict[f'transformer_blocks.{i}.attn.add_v_proj.bias'] = torch.cat([context_v_bias])
        if has_qk_norm:
            converted_state_dict[f'transformer_blocks.{i}.attn.norm_q.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn.ln_q.weight')
            converted_state_dict[f'transformer_blocks.{i}.attn.norm_k.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn.ln_k.weight')
            converted_state_dict[f'transformer_blocks.{i}.attn.norm_added_q.weight'] = checkpoint.pop(f'joint_blocks.{i}.context_block.attn.ln_q.weight')
            converted_state_dict[f'transformer_blocks.{i}.attn.norm_added_k.weight'] = checkpoint.pop(f'joint_blocks.{i}.context_block.attn.ln_k.weight')
        converted_state_dict[f'transformer_blocks.{i}.attn.to_out.0.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn.proj.weight')
        converted_state_dict[f'transformer_blocks.{i}.attn.to_out.0.bias'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn.proj.bias')
        if not i == num_layers - 1:
            converted_state_dict[f'transformer_blocks.{i}.attn.to_add_out.weight'] = checkpoint.pop(f'joint_blocks.{i}.context_block.attn.proj.weight')
            converted_state_dict[f'transformer_blocks.{i}.attn.to_add_out.bias'] = checkpoint.pop(f'joint_blocks.{i}.context_block.attn.proj.bias')
        if i in dual_attention_layers:
            sample_q2, sample_k2, sample_v2 = torch.chunk(checkpoint.pop(f'joint_blocks.{i}.x_block.attn2.qkv.weight'), 3, dim=0)
            sample_q2_bias, sample_k2_bias, sample_v2_bias = torch.chunk(checkpoint.pop(f'joint_blocks.{i}.x_block.attn2.qkv.bias'), 3, dim=0)
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_q.weight'] = torch.cat([sample_q2])
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_q.bias'] = torch.cat([sample_q2_bias])
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_k.weight'] = torch.cat([sample_k2])
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_k.bias'] = torch.cat([sample_k2_bias])
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_v.weight'] = torch.cat([sample_v2])
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_v.bias'] = torch.cat([sample_v2_bias])
            if has_qk_norm:
                converted_state_dict[f'transformer_blocks.{i}.attn2.norm_q.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn2.ln_q.weight')
                converted_state_dict[f'transformer_blocks.{i}.attn2.norm_k.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn2.ln_k.weight')
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_out.0.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn2.proj.weight')
            converted_state_dict[f'transformer_blocks.{i}.attn2.to_out.0.bias'] = checkpoint.pop(f'joint_blocks.{i}.x_block.attn2.proj.bias')
        converted_state_dict[f'transformer_blocks.{i}.norm1.linear.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.adaLN_modulation.1.weight')
        converted_state_dict[f'transformer_blocks.{i}.norm1.linear.bias'] = checkpoint.pop(f'joint_blocks.{i}.x_block.adaLN_modulation.1.bias')
        if not i == num_layers - 1:
            converted_state_dict[f'transformer_blocks.{i}.norm1_context.linear.weight'] = checkpoint.pop(f'joint_blocks.{i}.context_block.adaLN_modulation.1.weight')
            converted_state_dict[f'transformer_blocks.{i}.norm1_context.linear.bias'] = checkpoint.pop(f'joint_blocks.{i}.context_block.adaLN_modulation.1.bias')
        else:
            converted_state_dict[f'transformer_blocks.{i}.norm1_context.linear.weight'] = swap_scale_shift(checkpoint.pop(f'joint_blocks.{i}.context_block.adaLN_modulation.1.weight'), dim=caption_projection_dim)
            converted_state_dict[f'transformer_blocks.{i}.norm1_context.linear.bias'] = swap_scale_shift(checkpoint.pop(f'joint_blocks.{i}.context_block.adaLN_modulation.1.bias'), dim=caption_projection_dim)
        converted_state_dict[f'transformer_blocks.{i}.ff.net.0.proj.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.mlp.fc1.weight')
        converted_state_dict[f'transformer_blocks.{i}.ff.net.0.proj.bias'] = checkpoint.pop(f'joint_blocks.{i}.x_block.mlp.fc1.bias')
        converted_state_dict[f'transformer_blocks.{i}.ff.net.2.weight'] = checkpoint.pop(f'joint_blocks.{i}.x_block.mlp.fc2.weight')
        converted_state_dict[f'transformer_blocks.{i}.ff.net.2.bias'] = checkpoint.pop(f'joint_blocks.{i}.x_block.mlp.fc2.bias')
        if not i == num_layers - 1:
            converted_state_dict[f'transformer_blocks.{i}.ff_context.net.0.proj.weight'] = checkpoint.pop(f'joint_blocks.{i}.context_block.mlp.fc1.weight')
            converted_state_dict[f'transformer_blocks.{i}.ff_context.net.0.proj.bias'] = checkpoint.pop(f'joint_blocks.{i}.context_block.mlp.fc1.bias')
            converted_state_dict[f'transformer_blocks.{i}.ff_context.net.2.weight'] = checkpoint.pop(f'joint_blocks.{i}.context_block.mlp.fc2.weight')
            converted_state_dict[f'transformer_blocks.{i}.ff_context.net.2.bias'] = checkpoint.pop(f'joint_blocks.{i}.context_block.mlp.fc2.bias')
    converted_state_dict['proj_out.weight'] = checkpoint.pop('final_layer.linear.weight')
    converted_state_dict['proj_out.bias'] = checkpoint.pop('final_layer.linear.bias')
    converted_state_dict['norm_out.linear.weight'] = swap_scale_shift(checkpoint.pop('final_layer.adaLN_modulation.1.weight'), dim=caption_projection_dim)
    converted_state_dict['norm_out.linear.bias'] = swap_scale_shift(checkpoint.pop('final_layer.adaLN_modulation.1.bias'), dim=caption_projection_dim)
    return converted_state_dict


def convert_stable_cascade_unet_single_file_to_diffusers(checkpoint, **kwargs):
    is_stage_c = 'clip_txt_mapper.weight' in checkpoint
    if is_stage_c:
        state_dict = {}
        for key in checkpoint.keys():
            if key.endswith('in_proj_weight'):
                weights = checkpoint[key].chunk(3, 0)
                state_dict[key.replace('attn.in_proj_weight', 'to_q.weight')] = weights[0]
                state_dict[key.replace('attn.in_proj_weight', 'to_k.weight')] = weights[1]
                state_dict[key.replace('attn.in_proj_weight', 'to_v.weight')] = weights[2]
            elif key.endswith('in_proj_bias'):
                weights = checkpoint[key].chunk(3, 0)
                state_dict[key.replace('attn.in_proj_bias', 'to_q.bias')] = weights[0]
                state_dict[key.replace('attn.in_proj_bias', 'to_k.bias')] = weights[1]
                state_dict[key.replace('attn.in_proj_bias', 'to_v.bias')] = weights[2]
            elif key.endswith('out_proj.weight'):
                weights = checkpoint[key]
                state_dict[key.replace('attn.out_proj.weight', 'to_out.0.weight')] = weights
            elif key.endswith('out_proj.bias'):
                weights = checkpoint[key]
                state_dict[key.replace('attn.out_proj.bias', 'to_out.0.bias')] = weights
            else:
                state_dict[key] = checkpoint[key]
    else:
        state_dict = {}
        for key in checkpoint.keys():
            if key.endswith('in_proj_weight'):
                weights = checkpoint[key].chunk(3, 0)
                state_dict[key.replace('attn.in_proj_weight', 'to_q.weight')] = weights[0]
                state_dict[key.replace('attn.in_proj_weight', 'to_k.weight')] = weights[1]
                state_dict[key.replace('attn.in_proj_weight', 'to_v.weight')] = weights[2]
            elif key.endswith('in_proj_bias'):
                weights = checkpoint[key].chunk(3, 0)
                state_dict[key.replace('attn.in_proj_bias', 'to_q.bias')] = weights[0]
                state_dict[key.replace('attn.in_proj_bias', 'to_k.bias')] = weights[1]
                state_dict[key.replace('attn.in_proj_bias', 'to_v.bias')] = weights[2]
            elif key.endswith('out_proj.weight'):
                weights = checkpoint[key]
                state_dict[key.replace('attn.out_proj.weight', 'to_out.0.weight')] = weights
            elif key.endswith('out_proj.bias'):
                weights = checkpoint[key]
                state_dict[key.replace('attn.out_proj.bias', 'to_out.0.bias')] = weights
            elif key.endswith('clip_mapper.weight'):
                weights = checkpoint[key]
                state_dict[key.replace('clip_mapper.weight', 'clip_txt_pooled_mapper.weight')] = weights
            elif key.endswith('clip_mapper.bias'):
                weights = checkpoint[key]
                state_dict[key.replace('clip_mapper.bias', 'clip_txt_pooled_mapper.bias')] = weights
            else:
                state_dict[key] = checkpoint[key]
    return state_dict


DIFFUSERS_TO_LDM_DEFAULT_IMAGE_SIZE_MAP = {'xl_base': 1024, 'xl_refiner': 1024, 'xl_inpaint': 1024, 'playground-v2-5': 1024, 'upscale': 512, 'inpainting': 512, 'inpainting_v2': 512, 'controlnet': 512, 'v2': 768, 'v1': 512}


CHECKPOINT_KEY_NAMES = {'v2': 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'xl_base': 'conditioner.embedders.1.model.transformer.resblocks.9.mlp.c_proj.bias', 'xl_refiner': 'conditioner.embedders.0.model.transformer.resblocks.9.mlp.c_proj.bias', 'upscale': 'model.diffusion_model.input_blocks.10.0.skip_connection.bias', 'controlnet': 'control_model.time_embed.0.weight', 'playground-v2-5': 'edm_mean', 'inpainting': 'model.diffusion_model.input_blocks.0.0.weight', 'clip': 'cond_stage_model.transformer.text_model.embeddings.position_embedding.weight', 'clip_sdxl': 'conditioner.embedders.0.transformer.text_model.embeddings.position_embedding.weight', 'clip_sd3': 'text_encoders.clip_l.transformer.text_model.embeddings.position_embedding.weight', 'open_clip': 'cond_stage_model.model.token_embedding.weight', 'open_clip_sdxl': 'conditioner.embedders.1.model.positional_embedding', 'open_clip_sdxl_refiner': 'conditioner.embedders.0.model.text_projection', 'open_clip_sd3': 'text_encoders.clip_g.transformer.text_model.embeddings.position_embedding.weight', 'stable_cascade_stage_b': 'down_blocks.1.0.channelwise.0.weight', 'stable_cascade_stage_c': 'clip_txt_mapper.weight', 'sd3': 'model.diffusion_model.joint_blocks.0.context_block.adaLN_modulation.1.bias', 'sd35_large': 'model.diffusion_model.joint_blocks.37.x_block.mlp.fc1.weight', 'animatediff': 'down_blocks.0.motion_modules.0.temporal_transformer.transformer_blocks.0.attention_blocks.0.pos_encoder.pe', 'animatediff_v2': 'mid_block.motion_modules.0.temporal_transformer.norm.bias', 'animatediff_sdxl_beta': 'up_blocks.2.motion_modules.0.temporal_transformer.norm.weight', 'animatediff_scribble': 'controlnet_cond_embedding.conv_in.weight', 'animatediff_rgb': 'controlnet_cond_embedding.weight', 'flux': ['double_blocks.0.img_attn.norm.key_norm.scale', 'model.diffusion_model.double_blocks.0.img_attn.norm.key_norm.scale']}


def infer_diffusers_model_type(checkpoint):
    if CHECKPOINT_KEY_NAMES['inpainting'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['inpainting']].shape[1] == 9:
        if CHECKPOINT_KEY_NAMES['v2'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['v2']].shape[-1] == 1024:
            model_type = 'inpainting_v2'
        elif CHECKPOINT_KEY_NAMES['xl_base'] in checkpoint:
            model_type = 'xl_inpaint'
        else:
            model_type = 'inpainting'
    elif CHECKPOINT_KEY_NAMES['v2'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['v2']].shape[-1] == 1024:
        model_type = 'v2'
    elif CHECKPOINT_KEY_NAMES['playground-v2-5'] in checkpoint:
        model_type = 'playground-v2-5'
    elif CHECKPOINT_KEY_NAMES['xl_base'] in checkpoint:
        model_type = 'xl_base'
    elif CHECKPOINT_KEY_NAMES['xl_refiner'] in checkpoint:
        model_type = 'xl_refiner'
    elif CHECKPOINT_KEY_NAMES['upscale'] in checkpoint:
        model_type = 'upscale'
    elif CHECKPOINT_KEY_NAMES['controlnet'] in checkpoint:
        model_type = 'controlnet'
    elif CHECKPOINT_KEY_NAMES['stable_cascade_stage_c'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['stable_cascade_stage_c']].shape[0] == 1536:
        model_type = 'stable_cascade_stage_c_lite'
    elif CHECKPOINT_KEY_NAMES['stable_cascade_stage_c'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['stable_cascade_stage_c']].shape[0] == 2048:
        model_type = 'stable_cascade_stage_c'
    elif CHECKPOINT_KEY_NAMES['stable_cascade_stage_b'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['stable_cascade_stage_b']].shape[-1] == 576:
        model_type = 'stable_cascade_stage_b_lite'
    elif CHECKPOINT_KEY_NAMES['stable_cascade_stage_b'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['stable_cascade_stage_b']].shape[-1] == 640:
        model_type = 'stable_cascade_stage_b'
    elif CHECKPOINT_KEY_NAMES['sd3'] in checkpoint and checkpoint[CHECKPOINT_KEY_NAMES['sd3']].shape[-1] == 9216:
        model_type = 'sd3'
    elif CHECKPOINT_KEY_NAMES['sd35_large'] in checkpoint:
        model_type = 'sd35_large'
    elif CHECKPOINT_KEY_NAMES['animatediff'] in checkpoint:
        if CHECKPOINT_KEY_NAMES['animatediff_scribble'] in checkpoint:
            model_type = 'animatediff_scribble'
        elif CHECKPOINT_KEY_NAMES['animatediff_rgb'] in checkpoint:
            model_type = 'animatediff_rgb'
        elif CHECKPOINT_KEY_NAMES['animatediff_v2'] in checkpoint:
            model_type = 'animatediff_v2'
        elif checkpoint[CHECKPOINT_KEY_NAMES['animatediff_sdxl_beta']].shape[-1] == 320:
            model_type = 'animatediff_sdxl_beta'
        elif checkpoint[CHECKPOINT_KEY_NAMES['animatediff']].shape[1] == 24:
            model_type = 'animatediff_v1'
        else:
            model_type = 'animatediff_v3'
    elif any(key in checkpoint for key in CHECKPOINT_KEY_NAMES['flux']):
        if any(g in checkpoint for g in ['guidance_in.in_layer.bias', 'model.diffusion_model.guidance_in.in_layer.bias']):
            model_type = 'flux-dev'
        else:
            model_type = 'flux-schnell'
    else:
        model_type = 'v1'
    return model_type


def set_image_size(checkpoint, image_size=None):
    if image_size:
        return image_size
    model_type = infer_diffusers_model_type(checkpoint)
    image_size = DIFFUSERS_TO_LDM_DEFAULT_IMAGE_SIZE_MAP[model_type]
    return image_size


def create_unet_diffusers_config_from_ldm(original_config, checkpoint, image_size=None, upcast_attention=None, num_in_channels=None):
    """
    Creates a config for the diffusers based on the config of the LDM model.
    """
    if image_size is not None:
        deprecation_message = 'Configuring UNet2DConditionModel with the `image_size` argument to `from_single_file`is deprecated and will be ignored in future versions.'
        deprecate('image_size', '1.0.0', deprecation_message)
    image_size = set_image_size(checkpoint, image_size=image_size)
    if 'unet_config' in original_config['model']['params'] and original_config['model']['params']['unet_config'] is not None:
        unet_params = original_config['model']['params']['unet_config']['params']
    else:
        unet_params = original_config['model']['params']['network_config']['params']
    if num_in_channels is not None:
        deprecation_message = 'Configuring UNet2DConditionModel with the `num_in_channels` argument to `from_single_file`is deprecated and will be ignored in future versions.'
        deprecate('image_size', '1.0.0', deprecation_message)
        in_channels = num_in_channels
    else:
        in_channels = unet_params['in_channels']
    vae_params = original_config['model']['params']['first_stage_config']['params']['ddconfig']
    block_out_channels = [(unet_params['model_channels'] * mult) for mult in unet_params['channel_mult']]
    down_block_types = []
    resolution = 1
    for i in range(len(block_out_channels)):
        block_type = 'CrossAttnDownBlock2D' if resolution in unet_params['attention_resolutions'] else 'DownBlock2D'
        down_block_types.append(block_type)
        if i != len(block_out_channels) - 1:
            resolution *= 2
    up_block_types = []
    for i in range(len(block_out_channels)):
        block_type = 'CrossAttnUpBlock2D' if resolution in unet_params['attention_resolutions'] else 'UpBlock2D'
        up_block_types.append(block_type)
        resolution //= 2
    if unet_params['transformer_depth'] is not None:
        transformer_layers_per_block = unet_params['transformer_depth'] if isinstance(unet_params['transformer_depth'], int) else list(unet_params['transformer_depth'])
    else:
        transformer_layers_per_block = 1
    vae_scale_factor = 2 ** (len(vae_params['ch_mult']) - 1)
    head_dim = unet_params['num_heads'] if 'num_heads' in unet_params else None
    use_linear_projection = unet_params['use_linear_in_transformer'] if 'use_linear_in_transformer' in unet_params else False
    if use_linear_projection:
        if head_dim is None:
            head_dim_mult = unet_params['model_channels'] // unet_params['num_head_channels']
            head_dim = [(head_dim_mult * c) for c in list(unet_params['channel_mult'])]
    class_embed_type = None
    addition_embed_type = None
    addition_time_embed_dim = None
    projection_class_embeddings_input_dim = None
    context_dim = None
    if unet_params['context_dim'] is not None:
        context_dim = unet_params['context_dim'] if isinstance(unet_params['context_dim'], int) else unet_params['context_dim'][0]
    if 'num_classes' in unet_params:
        if unet_params['num_classes'] == 'sequential':
            if context_dim in [2048, 1280]:
                addition_embed_type = 'text_time'
                addition_time_embed_dim = 256
            else:
                class_embed_type = 'projection'
            assert 'adm_in_channels' in unet_params
            projection_class_embeddings_input_dim = unet_params['adm_in_channels']
    config = {'sample_size': image_size // vae_scale_factor, 'in_channels': in_channels, 'down_block_types': down_block_types, 'block_out_channels': block_out_channels, 'layers_per_block': unet_params['num_res_blocks'], 'cross_attention_dim': context_dim, 'attention_head_dim': head_dim, 'use_linear_projection': use_linear_projection, 'class_embed_type': class_embed_type, 'addition_embed_type': addition_embed_type, 'addition_time_embed_dim': addition_time_embed_dim, 'projection_class_embeddings_input_dim': projection_class_embeddings_input_dim, 'transformer_layers_per_block': transformer_layers_per_block}
    if upcast_attention is not None:
        deprecation_message = 'Configuring UNet2DConditionModel with the `upcast_attention` argument to `from_single_file`is deprecated and will be ignored in future versions.'
        deprecate('image_size', '1.0.0', deprecation_message)
        config['upcast_attention'] = upcast_attention
    if 'disable_self_attentions' in unet_params:
        config['only_cross_attention'] = unet_params['disable_self_attentions']
    if 'num_classes' in unet_params and isinstance(unet_params['num_classes'], int):
        config['num_class_embeds'] = unet_params['num_classes']
    config['out_channels'] = unet_params['out_channels']
    config['up_block_types'] = up_block_types
    return config


def create_controlnet_diffusers_config_from_ldm(original_config, checkpoint, image_size=None, **kwargs):
    if image_size is not None:
        deprecation_message = 'Configuring ControlNetModel with the `image_size` argumentis deprecated and will be ignored in future versions.'
        deprecate('image_size', '1.0.0', deprecation_message)
    image_size = set_image_size(checkpoint, image_size=image_size)
    unet_params = original_config['model']['params']['control_stage_config']['params']
    diffusers_unet_config = create_unet_diffusers_config_from_ldm(original_config, image_size=image_size)
    controlnet_config = {'conditioning_channels': unet_params['hint_channels'], 'in_channels': diffusers_unet_config['in_channels'], 'down_block_types': diffusers_unet_config['down_block_types'], 'block_out_channels': diffusers_unet_config['block_out_channels'], 'layers_per_block': diffusers_unet_config['layers_per_block'], 'cross_attention_dim': diffusers_unet_config['cross_attention_dim'], 'attention_head_dim': diffusers_unet_config['attention_head_dim'], 'use_linear_projection': diffusers_unet_config['use_linear_projection'], 'class_embed_type': diffusers_unet_config['class_embed_type'], 'addition_embed_type': diffusers_unet_config['addition_embed_type'], 'addition_time_embed_dim': diffusers_unet_config['addition_time_embed_dim'], 'projection_class_embeddings_input_dim': diffusers_unet_config['projection_class_embeddings_input_dim'], 'transformer_layers_per_block': diffusers_unet_config['transformer_layers_per_block']}
    return controlnet_config


LDM_VAE_DEFAULT_SCALING_FACTOR = 0.18215


PLAYGROUND_VAE_SCALING_FACTOR = 0.5


def create_vae_diffusers_config_from_ldm(original_config, checkpoint, image_size=None, scaling_factor=None):
    """
    Creates a config for the diffusers based on the config of the LDM model.
    """
    if image_size is not None:
        deprecation_message = 'Configuring AutoencoderKL with the `image_size` argumentis deprecated and will be ignored in future versions.'
        deprecate('image_size', '1.0.0', deprecation_message)
    image_size = set_image_size(checkpoint, image_size=image_size)
    if 'edm_mean' in checkpoint and 'edm_std' in checkpoint:
        latents_mean = checkpoint['edm_mean']
        latents_std = checkpoint['edm_std']
    else:
        latents_mean = None
        latents_std = None
    vae_params = original_config['model']['params']['first_stage_config']['params']['ddconfig']
    if scaling_factor is None and latents_mean is not None and latents_std is not None:
        scaling_factor = PLAYGROUND_VAE_SCALING_FACTOR
    elif scaling_factor is None and 'scale_factor' in original_config['model']['params']:
        scaling_factor = original_config['model']['params']['scale_factor']
    elif scaling_factor is None:
        scaling_factor = LDM_VAE_DEFAULT_SCALING_FACTOR
    block_out_channels = [(vae_params['ch'] * mult) for mult in vae_params['ch_mult']]
    down_block_types = ['DownEncoderBlock2D'] * len(block_out_channels)
    up_block_types = ['UpDecoderBlock2D'] * len(block_out_channels)
    config = {'sample_size': image_size, 'in_channels': vae_params['in_channels'], 'out_channels': vae_params['out_ch'], 'down_block_types': down_block_types, 'up_block_types': up_block_types, 'block_out_channels': block_out_channels, 'latent_channels': vae_params['z_channels'], 'layers_per_block': vae_params['num_res_blocks'], 'scaling_factor': scaling_factor}
    if latents_mean is not None and latents_std is not None:
        config.update({'latents_mean': latents_mean, 'latents_std': latents_std})
    return config


SINGLE_FILE_LOADABLE_CLASSES = {'StableCascadeUNet': {'checkpoint_mapping_fn': convert_stable_cascade_unet_single_file_to_diffusers}, 'UNet2DConditionModel': {'checkpoint_mapping_fn': convert_ldm_unet_checkpoint, 'config_mapping_fn': create_unet_diffusers_config_from_ldm, 'default_subfolder': 'unet', 'legacy_kwargs': {'num_in_channels': 'in_channels'}}, 'AutoencoderKL': {'checkpoint_mapping_fn': convert_ldm_vae_checkpoint, 'config_mapping_fn': create_vae_diffusers_config_from_ldm, 'default_subfolder': 'vae'}, 'ControlNetModel': {'checkpoint_mapping_fn': convert_controlnet_checkpoint, 'config_mapping_fn': create_controlnet_diffusers_config_from_ldm}, 'SD3Transformer2DModel': {'checkpoint_mapping_fn': convert_sd3_transformer_checkpoint_to_diffusers, 'default_subfolder': 'transformer'}, 'MotionAdapter': {'checkpoint_mapping_fn': convert_animatediff_checkpoint_to_diffusers}, 'SparseControlNetModel': {'checkpoint_mapping_fn': convert_animatediff_checkpoint_to_diffusers}, 'FluxTransformer2DModel': {'checkpoint_mapping_fn': convert_flux_transformer_checkpoint_to_diffusers, 'default_subfolder': 'transformer'}}


class SingleFileComponentError(Exception):

    def __init__(self, message=None):
        self.message = message
        super().__init__(self.message)


def _get_mapping_function_kwargs(mapping_fn, **kwargs):
    parameters = inspect.signature(mapping_fn).parameters
    mapping_kwargs = {}
    for parameter in parameters:
        if parameter in kwargs:
            mapping_kwargs[parameter] = kwargs[parameter]
    return mapping_kwargs


def _get_single_file_loadable_mapping_class(cls):
    diffusers_module = importlib.import_module(__name__.split('.')[0])
    for loadable_class_str in SINGLE_FILE_LOADABLE_CLASSES:
        loadable_class = getattr(diffusers_module, loadable_class_str)
        if issubclass(cls, loadable_class):
            return loadable_class_str
    return None


DIFFUSERS_DEFAULT_PIPELINE_PATHS = {'xl_base': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-xl-base-1.0'}, 'xl_refiner': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-xl-refiner-1.0'}, 'xl_inpaint': {'pretrained_model_name_or_path': 'diffusers/stable-diffusion-xl-1.0-inpainting-0.1'}, 'playground-v2-5': {'pretrained_model_name_or_path': 'playgroundai/playground-v2.5-1024px-aesthetic'}, 'upscale': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-x4-upscaler'}, 'inpainting': {'pretrained_model_name_or_path': 'stable-diffusion-v1-5/stable-diffusion-inpainting'}, 'inpainting_v2': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-2-inpainting'}, 'controlnet': {'pretrained_model_name_or_path': 'lllyasviel/control_v11p_sd15_canny'}, 'v2': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-2-1'}, 'v1': {'pretrained_model_name_or_path': 'stable-diffusion-v1-5/stable-diffusion-v1-5'}, 'stable_cascade_stage_b': {'pretrained_model_name_or_path': 'stabilityai/stable-cascade', 'subfolder': 'decoder'}, 'stable_cascade_stage_b_lite': {'pretrained_model_name_or_path': 'stabilityai/stable-cascade', 'subfolder': 'decoder_lite'}, 'stable_cascade_stage_c': {'pretrained_model_name_or_path': 'stabilityai/stable-cascade-prior', 'subfolder': 'prior'}, 'stable_cascade_stage_c_lite': {'pretrained_model_name_or_path': 'stabilityai/stable-cascade-prior', 'subfolder': 'prior_lite'}, 'sd3': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-3-medium-diffusers'}, 'sd35_large': {'pretrained_model_name_or_path': 'stabilityai/stable-diffusion-3.5-large'}, 'animatediff_v1': {'pretrained_model_name_or_path': 'guoyww/animatediff-motion-adapter-v1-5'}, 'animatediff_v2': {'pretrained_model_name_or_path': 'guoyww/animatediff-motion-adapter-v1-5-2'}, 'animatediff_v3': {'pretrained_model_name_or_path': 'guoyww/animatediff-motion-adapter-v1-5-3'}, 'animatediff_sdxl_beta': {'pretrained_model_name_or_path': 'guoyww/animatediff-motion-adapter-sdxl-beta'}, 'animatediff_scribble': {'pretrained_model_name_or_path': 'guoyww/animatediff-sparsectrl-scribble'}, 'animatediff_rgb': {'pretrained_model_name_or_path': 'guoyww/animatediff-sparsectrl-rgb'}, 'flux-dev': {'pretrained_model_name_or_path': 'black-forest-labs/FLUX.1-dev'}, 'flux-schnell': {'pretrained_model_name_or_path': 'black-forest-labs/FLUX.1-schnell'}}


def fetch_diffusers_config(checkpoint):
    model_type = infer_diffusers_model_type(checkpoint)
    model_path = DIFFUSERS_DEFAULT_PIPELINE_PATHS[model_type]
    model_path = copy.deepcopy(model_path)
    return model_path


def is_valid_url(url):
    result = urlparse(url)
    if result.scheme and result.netloc:
        return True
    return False


def fetch_original_config(original_config_file, local_files_only=False):
    if os.path.isfile(original_config_file):
        with open(original_config_file, 'r') as fp:
            original_config_file = fp.read()
    elif is_valid_url(original_config_file):
        if local_files_only:
            raise ValueError('`local_files_only` is set to True, but a URL was provided as `original_config_file`. Please provide a valid local file path.')
        original_config_file = BytesIO(requests.get(original_config_file).content)
    else:
        raise ValueError('Invalid `original_config_file` provided. Please set it to a valid file path or URL.')
    original_config = yaml.safe_load(original_config_file)
    return original_config


class QuantizationMethod(str, Enum):
    BITS_AND_BYTES = 'bitsandbytes'


def load_model_dict_into_meta(model, state_dict: 'OrderedDict', device: 'Optional[Union[str, torch.device]]'=None, dtype: 'Optional[Union[str, torch.dtype]]'=None, model_name_or_path: 'Optional[str]'=None, hf_quantizer=None, keep_in_fp32_modules=None) ->List[str]:
    if hf_quantizer is None:
        device = device or torch.device('cpu')
    dtype = dtype or torch.float32
    is_quantized = hf_quantizer is not None
    is_quant_method_bnb = getattr(model, 'quantization_method', None) == QuantizationMethod.BITS_AND_BYTES
    accepts_dtype = 'dtype' in set(inspect.signature(set_module_tensor_to_device).parameters.keys())
    empty_state_dict = model.state_dict()
    unexpected_keys = [param_name for param_name in state_dict if param_name not in empty_state_dict]
    for param_name, param in state_dict.items():
        if param_name not in empty_state_dict:
            continue
        set_module_kwargs = {}
        if torch.is_floating_point(param):
            if keep_in_fp32_modules is not None and any(module_to_keep_in_fp32 in param_name.split('.') for module_to_keep_in_fp32 in keep_in_fp32_modules) and dtype == torch.float16:
                param = param
                if accepts_dtype:
                    set_module_kwargs['dtype'] = torch.float32
            else:
                param = param
                if accepts_dtype:
                    set_module_kwargs['dtype'] = dtype
        if empty_state_dict[param_name].shape != param.shape:
            if is_quant_method_bnb and hf_quantizer.pre_quantized and hf_quantizer.check_if_quantized_param(model, param, param_name, state_dict, param_device=device):
                hf_quantizer.check_quantized_param_shape(param_name, empty_state_dict[param_name].shape, param.shape)
            elif not is_quant_method_bnb:
                model_name_or_path_str = f'{model_name_or_path} ' if model_name_or_path is not None else ''
                raise ValueError(f'Cannot load {model_name_or_path_str} because {param_name} expected shape {empty_state_dict[param_name]}, but got {param.shape}. If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example.')
        if is_quantized and hf_quantizer.check_if_quantized_param(model, param, param_name, state_dict, param_device=device):
            hf_quantizer.create_quantized_param(model, param, param_name, device, state_dict, unexpected_keys)
        elif accepts_dtype:
            set_module_tensor_to_device(model, param_name, device, value=param, **set_module_kwargs)
        else:
            set_module_tensor_to_device(model, param_name, device, value=param)
    return unexpected_keys


VALID_URL_PREFIXES = ['https://huggingface.co/', 'huggingface.co/', 'hf.co/', 'https://hf.co/']


def _extract_repo_id_and_weights_name(pretrained_model_name_or_path):
    if not is_valid_url(pretrained_model_name_or_path):
        raise ValueError('Invalid `pretrained_model_name_or_path` provided. Please set it to a valid URL.')
    pattern = '([^/]+)/([^/]+)/(?:blob/main/)?(.+)'
    weights_name = None
    repo_id = None,
    for prefix in VALID_URL_PREFIXES:
        pretrained_model_name_or_path = pretrained_model_name_or_path.replace(prefix, '')
    match = re.match(pattern, pretrained_model_name_or_path)
    if not match:
        logger.warning('Unable to identify the repo_id and weights_name from the provided URL.')
        return repo_id, weights_name
    repo_id = f'{match.group(1)}/{match.group(2)}'
    weights_name = match.group(3)
    return repo_id, weights_name


DEPRECATED_REVISION_ARGS = ['fp16', 'non-ema']


SAFETENSORS_WEIGHTS_NAME = 'diffusion_pytorch_model.safetensors'


def _add_variant(weights_name: 'str', variant: 'Optional[str]'=None) ->str:
    if variant is not None:
        splits = weights_name.split('.')
        splits = splits[:-1] + [variant] + splits[-1:]
        weights_name = '.'.join(splits)
    return weights_name


def load_single_file_checkpoint(pretrained_model_link_or_path, force_download=False, proxies=None, token=None, cache_dir=None, local_files_only=None, revision=None):
    if os.path.isfile(pretrained_model_link_or_path):
        pretrained_model_link_or_path = pretrained_model_link_or_path
    else:
        repo_id, weights_name = _extract_repo_id_and_weights_name(pretrained_model_link_or_path)
        pretrained_model_link_or_path = _get_model_file(repo_id, weights_name=weights_name, force_download=force_download, cache_dir=cache_dir, proxies=proxies, local_files_only=local_files_only, token=token, revision=revision)
    checkpoint = load_state_dict(pretrained_model_link_or_path)
    while 'state_dict' in checkpoint:
        checkpoint = checkpoint['state_dict']
    return checkpoint


class UVit2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UVit2DConvEmbed(nn.Module):

    def __init__(self, in_channels, block_out_channels, vocab_size, elementwise_affine, eps, bias):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, in_channels)
        self.layer_norm = RMSNorm(in_channels, eps, elementwise_affine)
        self.conv = nn.Conv2d(in_channels, block_out_channels, kernel_size=1, bias=bias)

    def forward(self, input_ids):
        embeddings = self.embeddings(input_ids)
        embeddings = self.layer_norm(embeddings)
        embeddings = embeddings.permute(0, 3, 1, 2)
        embeddings = self.conv(embeddings)
        return embeddings


class ConvNextBlock(nn.Module):

    def __init__(self, channels, layer_norm_eps, ln_elementwise_affine, use_bias, hidden_dropout, hidden_size, res_ffn_factor=4):
        super().__init__()
        self.depthwise = nn.Conv2d(channels, channels, kernel_size=3, padding=1, groups=channels, bias=use_bias)
        self.norm = RMSNorm(channels, layer_norm_eps, ln_elementwise_affine)
        self.channelwise_linear_1 = nn.Linear(channels, int(channels * res_ffn_factor), bias=use_bias)
        self.channelwise_act = nn.GELU()
        self.channelwise_norm = GlobalResponseNorm(int(channels * res_ffn_factor))
        self.channelwise_linear_2 = nn.Linear(int(channels * res_ffn_factor), channels, bias=use_bias)
        self.channelwise_dropout = nn.Dropout(hidden_dropout)
        self.cond_embeds_mapper = nn.Linear(hidden_size, channels * 2, use_bias)

    def forward(self, x, cond_embeds):
        x_res = x
        x = self.depthwise(x)
        x = x.permute(0, 2, 3, 1)
        x = self.norm(x)
        x = self.channelwise_linear_1(x)
        x = self.channelwise_act(x)
        x = self.channelwise_norm(x)
        x = self.channelwise_linear_2(x)
        x = self.channelwise_dropout(x)
        x = x.permute(0, 3, 1, 2)
        x = x + x_res
        scale, shift = self.cond_embeds_mapper(F.silu(cond_embeds)).chunk(2, dim=1)
        x = x * (1 + scale[:, :, None, None]) + shift[:, :, None, None]
        return x


class UVitBlock(nn.Module):

    def __init__(self, channels, num_res_blocks: 'int', hidden_size, hidden_dropout, ln_elementwise_affine, layer_norm_eps, use_bias, block_num_heads, attention_dropout, downsample: 'bool', upsample: 'bool'):
        super().__init__()
        if downsample:
            self.downsample = Downsample2D(channels, use_conv=True, padding=0, name='Conv2d_0', kernel_size=2, norm_type='rms_norm', eps=layer_norm_eps, elementwise_affine=ln_elementwise_affine, bias=use_bias)
        else:
            self.downsample = None
        self.res_blocks = nn.ModuleList([ConvNextBlock(channels, layer_norm_eps, ln_elementwise_affine, use_bias, hidden_dropout, hidden_size) for i in range(num_res_blocks)])
        self.attention_blocks = nn.ModuleList([SkipFFTransformerBlock(channels, block_num_heads, channels // block_num_heads, hidden_size, use_bias, attention_dropout, channels, attention_bias=use_bias, attention_out_bias=use_bias) for _ in range(num_res_blocks)])
        if upsample:
            self.upsample = Upsample2D(channels, use_conv_transpose=True, kernel_size=2, padding=0, name='conv', norm_type='rms_norm', eps=layer_norm_eps, elementwise_affine=ln_elementwise_affine, bias=use_bias, interpolate=False)
        else:
            self.upsample = None

    def forward(self, x, pooled_text_emb, encoder_hidden_states, cross_attention_kwargs):
        if self.downsample is not None:
            x = self.downsample(x)
        for res_block, attention_block in zip(self.res_blocks, self.attention_blocks):
            x = res_block(x, pooled_text_emb)
            batch_size, channels, height, width = x.shape
            x = x.view(batch_size, channels, height * width).permute(0, 2, 1)
            x = attention_block(x, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs)
            x = x.permute(0, 2, 1).view(batch_size, channels, height, width)
        if self.upsample is not None:
            x = self.upsample(x)
        return x


class ConvMlmLayer(nn.Module):

    def __init__(self, block_out_channels: 'int', in_channels: 'int', use_bias: 'bool', ln_elementwise_affine: 'bool', layer_norm_eps: 'float', codebook_size: 'int'):
        super().__init__()
        self.conv1 = nn.Conv2d(block_out_channels, in_channels, kernel_size=1, bias=use_bias)
        self.layer_norm = RMSNorm(in_channels, layer_norm_eps, ln_elementwise_affine)
        self.conv2 = nn.Conv2d(in_channels, codebook_size, kernel_size=1, bias=use_bias)

    def forward(self, hidden_states):
        hidden_states = self.conv1(hidden_states)
        hidden_states = self.layer_norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        logits = self.conv2(hidden_states)
        return logits


class FlaxDiagonalGaussianDistribution(object):

    def __init__(self, parameters, deterministic=False):
        self.mean, self.logvar = jnp.split(parameters, 2, axis=-1)
        self.logvar = jnp.clip(self.logvar, -30.0, 20.0)
        self.deterministic = deterministic
        self.std = jnp.exp(0.5 * self.logvar)
        self.var = jnp.exp(self.logvar)
        if self.deterministic:
            self.var = self.std = jnp.zeros_like(self.mean)

    def sample(self, key):
        return self.mean + self.std * jax.random.normal(key, self.mean.shape)

    def kl(self, other=None):
        if self.deterministic:
            return jnp.array([0.0])
        if other is None:
            return 0.5 * jnp.sum(self.mean ** 2 + self.var - 1.0 - self.logvar, axis=[1, 2, 3])
        return 0.5 * jnp.sum(jnp.square(self.mean - other.mean) / other.var + self.var / other.var - 1.0 - self.logvar + other.logvar, axis=[1, 2, 3])

    def nll(self, sample, axis=[1, 2, 3]):
        if self.deterministic:
            return jnp.array([0.0])
        logtwopi = jnp.log(2.0 * jnp.pi)
        return 0.5 * jnp.sum(logtwopi + self.logvar + jnp.square(sample - self.mean) / self.var, axis=axis)

    def mode(self):
        return self.mean


class AudioLDM2ProjectionModel(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class AudioLDM2UNet2DConditionModel(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class Blip2TextEmbeddings(nn.Module):
    """Construct the embeddings from word and position embeddings."""

    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand((1, -1)))
        self.position_embedding_type = getattr(config, 'position_embedding_type', 'absolute')
        self.config = config

    def forward(self, input_ids=None, position_ids=None, query_embeds=None, past_key_values_length=0):
        if input_ids is not None:
            seq_length = input_ids.size()[1]
        else:
            seq_length = 0
        if position_ids is None:
            position_ids = self.position_ids[:, past_key_values_length:seq_length + past_key_values_length].clone()
        if input_ids is not None:
            embeddings = self.word_embeddings(input_ids)
            if self.position_embedding_type == 'absolute':
                position_embeddings = self.position_embeddings(position_ids)
                embeddings = embeddings + position_embeddings
            if query_embeds is not None:
                batch_size = embeddings.shape[0]
                query_embeds = query_embeds.repeat(batch_size, 1, 1)
                embeddings = torch.cat((query_embeds, embeddings), dim=1)
        else:
            embeddings = query_embeds
        embeddings = embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class Blip2VisionEmbeddings(nn.Module):

    def __init__(self, config: 'Blip2VisionConfig'):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.image_size = config.image_size
        self.patch_size = config.patch_size
        self.class_embedding = nn.Parameter(torch.randn(1, 1, self.embed_dim))
        self.patch_embedding = nn.Conv2d(in_channels=3, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size, bias=False)
        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.num_positions = self.num_patches + 1
        self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))

    def forward(self, pixel_values: 'torch.Tensor') ->torch.Tensor:
        batch_size = pixel_values.shape[0]
        target_dtype = self.patch_embedding.weight.dtype
        patch_embeds = self.patch_embedding(pixel_values)
        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)
        class_embeds = self.class_embedding.expand(batch_size, 1, -1)
        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
        embeddings = embeddings + self.position_embedding[:, :embeddings.size(1), :]
        return embeddings


class Blip2QFormerLayer(nn.Module):

    def __init__(self, config, layer_idx):
        super().__init__()
        self.chunk_size_feed_forward = config.chunk_size_feed_forward
        self.seq_len_dim = 1
        self.attention = Blip2QFormerAttention(config)
        self.layer_idx = layer_idx
        if layer_idx % config.cross_attention_frequency == 0:
            self.crossattention = Blip2QFormerAttention(config, is_cross_attention=True)
            self.has_cross_attention = True
        else:
            self.has_cross_attention = False
        self.intermediate = Blip2QFormerIntermediate(config)
        self.intermediate_query = Blip2QFormerIntermediate(config)
        self.output_query = Blip2QFormerOutput(config)
        self.output = Blip2QFormerOutput(config)

    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, query_length=0):
        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None
        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions, past_key_value=self_attn_past_key_value)
        attention_output = self_attention_outputs[0]
        outputs = self_attention_outputs[1:-1]
        present_key_value = self_attention_outputs[-1]
        if query_length > 0:
            query_attention_output = attention_output[:, :query_length, :]
            if self.has_cross_attention:
                if encoder_hidden_states is None:
                    raise ValueError('encoder_hidden_states must be given for cross-attention layers')
                cross_attention_outputs = self.crossattention(query_attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions=output_attentions)
                query_attention_output = cross_attention_outputs[0]
                outputs = outputs + cross_attention_outputs[1:-1]
            layer_output = apply_chunking_to_forward(self.feed_forward_chunk_query, self.chunk_size_feed_forward, self.seq_len_dim, query_attention_output)
            if attention_output.shape[1] > query_length:
                layer_output_text = apply_chunking_to_forward(self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output[:, query_length:, :])
                layer_output = torch.cat([layer_output, layer_output_text], dim=1)
        else:
            layer_output = apply_chunking_to_forward(self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output)
        outputs = (layer_output,) + outputs
        outputs = outputs + (present_key_value,)
        return outputs

    def feed_forward_chunk(self, attention_output):
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output

    def feed_forward_chunk_query(self, attention_output):
        intermediate_output = self.intermediate_query(attention_output)
        layer_output = self.output_query(intermediate_output, attention_output)
        return layer_output


class Blip2QFormerEncoder(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList([Blip2QFormerLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])
        self.gradient_checkpointing = False

    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=False, output_hidden_states=False, return_dict=True, query_length=0):
        all_hidden_states = () if output_hidden_states else None
        all_self_attentions = () if output_attentions else None
        all_cross_attentions = () if output_attentions else None
        next_decoder_cache = () if use_cache else None
        for i in range(self.config.num_hidden_layers):
            layer_module = self.layer[i]
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)
            layer_head_mask = head_mask[i] if head_mask is not None else None
            past_key_value = past_key_values[i] if past_key_values is not None else None
            if getattr(self.config, 'gradient_checkpointing', False) and self.training:
                if use_cache:
                    logger.warning('`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...')
                    use_cache = False

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs, past_key_value, output_attentions, query_length)
                    return custom_forward
                layer_outputs = torch.utils.checkpoint.checkpoint(create_custom_forward(layer_module), hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask)
            else:
                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, query_length)
            hidden_states = layer_outputs[0]
            if use_cache:
                next_decoder_cache += layer_outputs[-1],
            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)
                if layer_module.has_cross_attention:
                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        if not return_dict:
            return tuple(v for v in [hidden_states, next_decoder_cache, all_hidden_states, all_self_attentions, all_cross_attentions] if v is not None)
        return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_states, past_key_values=next_decoder_cache, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions)


class ProjLayer(nn.Module):

    def __init__(self, in_dim, out_dim, hidden_dim, drop_p=0.1, eps=1e-12):
        super().__init__()
        self.dense1 = nn.Linear(in_dim, hidden_dim)
        self.act_fn = QuickGELU()
        self.dense2 = nn.Linear(hidden_dim, out_dim)
        self.dropout = nn.Dropout(drop_p)
        self.LayerNorm = nn.LayerNorm(out_dim, eps=eps)

    def forward(self, x):
        x_in = x
        x = self.LayerNorm(x)
        x = self.dropout(self.dense2(self.act_fn(self.dense1(x)))) + x_in
        return x


class ContextCLIPTextEmbeddings(nn.Module):

    def __init__(self, config: 'CLIPTextConfig'):
        super().__init__()
        embed_dim = config.hidden_size
        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)
        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)
        self.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand((1, -1)))

    def forward(self, ctx_embeddings: 'torch.Tensor', ctx_begin_pos: 'list', input_ids: 'Optional[torch.LongTensor]'=None, position_ids: 'Optional[torch.LongTensor]'=None, inputs_embeds: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        if ctx_embeddings is None:
            ctx_len = 0
        else:
            ctx_len = ctx_embeddings.shape[1]
        seq_length = (input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]) + ctx_len
        if position_ids is None:
            position_ids = self.position_ids[:, :seq_length]
        if inputs_embeds is None:
            inputs_embeds = self.token_embedding(input_ids)
            input_embeds_ctx = []
            bsz = inputs_embeds.shape[0]
            if ctx_embeddings is not None:
                for i in range(bsz):
                    cbp = ctx_begin_pos[i]
                    prefix = inputs_embeds[i, :cbp]
                    suffix = inputs_embeds[i, cbp:]
                    input_embeds_ctx.append(torch.cat([prefix, ctx_embeddings[i], suffix], dim=0))
                inputs_embeds = torch.stack(input_embeds_ctx, dim=0)
        position_embeddings = self.position_embedding(position_ids)
        embeddings = inputs_embeds + position_embeddings
        return embeddings


def _expand_mask(mask: 'torch.Tensor', dtype: 'torch.dtype', tgt_len: 'Optional[int]'=None):
    """
    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
    """
    bsz, src_len = mask.size()
    tgt_len = tgt_len if tgt_len is not None else src_len
    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len)
    inverted_mask = 1.0 - expanded_mask
    return inverted_mask.masked_fill(inverted_mask, torch.finfo(dtype).min)


ADDED_KV_ATTENTION_PROCESSORS = AttnAddedKVProcessor, SlicedAttnAddedKVProcessor, AttnAddedKVProcessor2_0, XFormersAttnAddedKVProcessor


class FusedAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). It uses
    fused projection layers. For self-attention modules, all projection matrices (i.e., query, key, value) are fused.
    For cross-attention modules, key and value projection matrices are fused.

    <Tip warning={true}>

    This API is currently 🧪 experimental in nature and can change in future.

    </Tip>
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('FusedAttnProcessor2_0 requires at least PyTorch 2.0, to use it. Please upgrade PyTorch to > 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, *args, **kwargs) ->torch.Tensor:
        if len(args) > 0 or kwargs.get('scale', None) is not None:
            deprecation_message = 'The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.'
            deprecate('scale', '1.0.0', deprecation_message)
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        if encoder_hidden_states is None:
            qkv = attn.to_qkv(hidden_states)
            split_size = qkv.shape[-1] // 3
            query, key, value = torch.split(qkv, split_size, dim=-1)
        else:
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
            query = attn.to_q(hidden_states)
            kv = attn.to_kv(encoder_hidden_states)
            split_size = kv.shape[-1] // 2
            key, value = torch.split(kv, split_size, dim=-1)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class PAGCFGHunyuanAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the HunyuanDiT model. It applies a normalization layer and rotary embedding on query and key vector. This
    variant of the processor employs [Pertubed Attention Guidance](https://arxiv.org/abs/2403.17377).
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('PAGCFGHunyuanAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        hidden_states_uncond, hidden_states_org, hidden_states_ptb = hidden_states.chunk(3)
        hidden_states_org = torch.cat([hidden_states_uncond, hidden_states_org])
        batch_size, sequence_length, _ = hidden_states_org.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states_org = attn.group_norm(hidden_states_org.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states_org)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states_org
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            if not attn.is_cross_attention:
                key = apply_rotary_emb(key, image_rotary_emb)
        hidden_states_org = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states_org = hidden_states_org.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states_org = hidden_states_org
        hidden_states_org = attn.to_out[0](hidden_states_org)
        hidden_states_org = attn.to_out[1](hidden_states_org)
        if input_ndim == 4:
            hidden_states_org = hidden_states_org.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.group_norm is not None:
            hidden_states_ptb = attn.group_norm(hidden_states_ptb.transpose(1, 2)).transpose(1, 2)
        hidden_states_ptb = attn.to_v(hidden_states_ptb)
        hidden_states_ptb = hidden_states_ptb
        hidden_states_ptb = attn.to_out[0](hidden_states_ptb)
        hidden_states_ptb = attn.to_out[1](hidden_states_ptb)
        if input_ndim == 4:
            hidden_states_ptb = hidden_states_ptb.transpose(-1, -2).reshape(batch_size, channel, height, width)
        hidden_states = torch.cat([hidden_states_org, hidden_states_ptb])
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class PAGCFGIdentitySelfAttnProcessor2_0:
    """
    Processor for implementing PAG using scaled dot-product attention (enabled by default if you're using PyTorch 2.0).
    PAG reference: https://arxiv.org/abs/2403.17377
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('PAGCFGIdentitySelfAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, temb: 'Optional[torch.FloatTensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        hidden_states_uncond, hidden_states_org, hidden_states_ptb = hidden_states.chunk(3)
        hidden_states_org = torch.cat([hidden_states_uncond, hidden_states_org])
        batch_size, sequence_length, _ = hidden_states_org.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states_org = attn.group_norm(hidden_states_org.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states_org)
        key = attn.to_k(hidden_states_org)
        value = attn.to_v(hidden_states_org)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        hidden_states_org = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states_org = hidden_states_org.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states_org = hidden_states_org
        hidden_states_org = attn.to_out[0](hidden_states_org)
        hidden_states_org = attn.to_out[1](hidden_states_org)
        if input_ndim == 4:
            hidden_states_org = hidden_states_org.transpose(-1, -2).reshape(batch_size, channel, height, width)
        batch_size, sequence_length, _ = hidden_states_ptb.shape
        if attn.group_norm is not None:
            hidden_states_ptb = attn.group_norm(hidden_states_ptb.transpose(1, 2)).transpose(1, 2)
        value = attn.to_v(hidden_states_ptb)
        hidden_states_ptb = value
        hidden_states_ptb = hidden_states_ptb
        hidden_states_ptb = attn.to_out[0](hidden_states_ptb)
        hidden_states_ptb = attn.to_out[1](hidden_states_ptb)
        if input_ndim == 4:
            hidden_states_ptb = hidden_states_ptb.transpose(-1, -2).reshape(batch_size, channel, height, width)
        hidden_states = torch.cat([hidden_states_org, hidden_states_ptb])
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class PAGHunyuanAttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0). This is
    used in the HunyuanDiT model. It applies a normalization layer and rotary embedding on query and key vector. This
    variant of the processor employs [Pertubed Attention Guidance](https://arxiv.org/abs/2403.17377).
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('PAGHunyuanAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.Tensor', encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, temb: 'Optional[torch.Tensor]'=None, image_rotary_emb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        hidden_states_org, hidden_states_ptb = hidden_states.chunk(2)
        batch_size, sequence_length, _ = hidden_states_org.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states_org = attn.group_norm(hidden_states_org.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states_org)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states_org
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        if attn.norm_q is not None:
            query = attn.norm_q(query)
        if attn.norm_k is not None:
            key = attn.norm_k(key)
        if image_rotary_emb is not None:
            query = apply_rotary_emb(query, image_rotary_emb)
            if not attn.is_cross_attention:
                key = apply_rotary_emb(key, image_rotary_emb)
        hidden_states_org = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states_org = hidden_states_org.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states_org = hidden_states_org
        hidden_states_org = attn.to_out[0](hidden_states_org)
        hidden_states_org = attn.to_out[1](hidden_states_org)
        if input_ndim == 4:
            hidden_states_org = hidden_states_org.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.group_norm is not None:
            hidden_states_ptb = attn.group_norm(hidden_states_ptb.transpose(1, 2)).transpose(1, 2)
        hidden_states_ptb = attn.to_v(hidden_states_ptb)
        hidden_states_ptb = hidden_states_ptb
        hidden_states_ptb = attn.to_out[0](hidden_states_ptb)
        hidden_states_ptb = attn.to_out[1](hidden_states_ptb)
        if input_ndim == 4:
            hidden_states_ptb = hidden_states_ptb.transpose(-1, -2).reshape(batch_size, channel, height, width)
        hidden_states = torch.cat([hidden_states_org, hidden_states_ptb])
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class PAGIdentitySelfAttnProcessor2_0:
    """
    Processor for implementing PAG using scaled dot-product attention (enabled by default if you're using PyTorch 2.0).
    PAG reference: https://arxiv.org/abs/2403.17377
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('PAGIdentitySelfAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, temb: 'Optional[torch.FloatTensor]'=None) ->torch.Tensor:
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        hidden_states_org, hidden_states_ptb = hidden_states.chunk(2)
        batch_size, sequence_length, _ = hidden_states_org.shape
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states_org = attn.group_norm(hidden_states_org.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states_org)
        key = attn.to_k(hidden_states_org)
        value = attn.to_v(hidden_states_org)
        inner_dim = key.shape[-1]
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        hidden_states_org = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states_org = hidden_states_org.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states_org = hidden_states_org
        hidden_states_org = attn.to_out[0](hidden_states_org)
        hidden_states_org = attn.to_out[1](hidden_states_org)
        if input_ndim == 4:
            hidden_states_org = hidden_states_org.transpose(-1, -2).reshape(batch_size, channel, height, width)
        batch_size, sequence_length, _ = hidden_states_ptb.shape
        if attn.group_norm is not None:
            hidden_states_ptb = attn.group_norm(hidden_states_ptb.transpose(1, 2)).transpose(1, 2)
        hidden_states_ptb = attn.to_v(hidden_states_ptb)
        hidden_states_ptb = hidden_states_ptb
        hidden_states_ptb = attn.to_out[0](hidden_states_ptb)
        hidden_states_ptb = attn.to_out[1](hidden_states_ptb)
        if input_ndim == 4:
            hidden_states_ptb = hidden_states_ptb.transpose(-1, -2).reshape(batch_size, channel, height, width)
        hidden_states = torch.cat([hidden_states_org, hidden_states_ptb])
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


AttentionProcessor = Union[AttnProcessor, AttnProcessor2_0, FusedAttnProcessor2_0, XFormersAttnProcessor, SlicedAttnProcessor, AttnAddedKVProcessor, SlicedAttnAddedKVProcessor, AttnAddedKVProcessor2_0, XFormersAttnAddedKVProcessor, CustomDiffusionAttnProcessor, CustomDiffusionXFormersAttnProcessor, CustomDiffusionAttnProcessor2_0, PAGCFGIdentitySelfAttnProcessor2_0, PAGIdentitySelfAttnProcessor2_0, PAGCFGHunyuanAttnProcessor2_0, PAGHunyuanAttnProcessor2_0]


CROSS_ATTENTION_PROCESSORS = AttnProcessor, AttnProcessor2_0, XFormersAttnProcessor, SlicedAttnProcessor, IPAdapterAttnProcessor, IPAdapterAttnProcessor2_0


class UNetMidBlockFlat(nn.Module):
    """
    A 2D UNet mid-block [`UNetMidBlockFlat`] with multiple residual blocks and optional attention blocks.

    Args:
        in_channels (`int`): The number of input channels.
        temb_channels (`int`): The number of temporal embedding channels.
        dropout (`float`, *optional*, defaults to 0.0): The dropout rate.
        num_layers (`int`, *optional*, defaults to 1): The number of residual blocks.
        resnet_eps (`float`, *optional*, 1e-6 ): The epsilon value for the resnet blocks.
        resnet_time_scale_shift (`str`, *optional*, defaults to `default`):
            The type of normalization to apply to the time embeddings. This can help to improve the performance of the
            model on tasks with long-range temporal dependencies.
        resnet_act_fn (`str`, *optional*, defaults to `swish`): The activation function for the resnet blocks.
        resnet_groups (`int`, *optional*, defaults to 32):
            The number of groups to use in the group normalization layers of the resnet blocks.
        attn_groups (`Optional[int]`, *optional*, defaults to None): The number of groups for the attention blocks.
        resnet_pre_norm (`bool`, *optional*, defaults to `True`):
            Whether to use pre-normalization for the resnet blocks.
        add_attention (`bool`, *optional*, defaults to `True`): Whether to add attention blocks.
        attention_head_dim (`int`, *optional*, defaults to 1):
            Dimension of a single attention head. The number of attention heads is determined based on this value and
            the number of input channels.
        output_scale_factor (`float`, *optional*, defaults to 1.0): The output scale factor.

    Returns:
        `torch.Tensor`: The output of the last residual block, which is a tensor of shape `(batch_size, in_channels,
        height, width)`.

    """

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, attn_groups: 'Optional[int]'=None, resnet_pre_norm: 'bool'=True, add_attention: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0):
        super().__init__()
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.add_attention = add_attention
        if attn_groups is None:
            attn_groups = resnet_groups if resnet_time_scale_shift == 'default' else None
        if resnet_time_scale_shift == 'spatial':
            resnets = [ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor)]
        else:
            resnets = [ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        if attention_head_dim is None:
            logger.warning(f'It is not recommend to pass `attention_head_dim=None`. Defaulting `attention_head_dim` to `in_channels`: {in_channels}.')
            attention_head_dim = in_channels
        for _ in range(num_layers):
            if self.add_attention:
                attentions.append(Attention(in_channels, heads=in_channels // attention_head_dim, dim_head=attention_head_dim, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=attn_groups, spatial_norm_dim=temb_channels if resnet_time_scale_shift == 'spatial' else None, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
            else:
                attentions.append(None)
            if resnet_time_scale_shift == 'spatial':
                resnets.append(ResnetBlockCondNorm2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm='spatial', non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor))
            else:
                resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            if attn is not None:
                hidden_states = attn(hidden_states, temb=temb)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class UNetMidBlockFlatCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', out_channels: 'Optional[int]'=None, dropout: 'float'=0.0, num_layers: 'int'=1, transformer_layers_per_block: 'Union[int, Tuple[int]]'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_groups_out: 'Optional[int]'=None, resnet_pre_norm: 'bool'=True, num_attention_heads: 'int'=1, output_scale_factor: 'float'=1.0, cross_attention_dim: 'int'=1280, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, upcast_attention: 'bool'=False, attention_type: 'str'='default'):
        super().__init__()
        out_channels = out_channels or in_channels
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.has_cross_attention = True
        self.num_attention_heads = num_attention_heads
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        if isinstance(transformer_layers_per_block, int):
            transformer_layers_per_block = [transformer_layers_per_block] * num_layers
        resnet_groups_out = resnet_groups_out or resnet_groups
        resnets = [ResnetBlockFlat(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, groups_out=resnet_groups_out, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        for i in range(num_layers):
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=transformer_layers_per_block[i], cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups_out, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention, attention_type=attention_type))
            else:
                attentions.append(DualTransformer2DModel(num_attention_heads, out_channels // num_attention_heads, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            resnets.append(ResnetBlockFlat(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups_out, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        if cross_attention_kwargs is not None:
            if cross_attention_kwargs.get('scale', None) is not None:
                logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
            else:
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
                hidden_states = resnet(hidden_states, temb)
        return hidden_states


class UNetMidBlockFlatSimpleCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attention_head_dim: 'int'=1, output_scale_factor: 'float'=1.0, cross_attention_dim: 'int'=1280, skip_time_act: 'bool'=False, only_cross_attention: 'bool'=False, cross_attention_norm: 'Optional[str]'=None):
        super().__init__()
        self.has_cross_attention = True
        self.attention_head_dim = attention_head_dim
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.num_heads = in_channels // self.attention_head_dim
        resnets = [ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act)]
        attentions = []
        for _ in range(num_layers):
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=in_channels, cross_attention_dim=in_channels, heads=self.num_heads, dim_head=self.attention_head_dim, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.Tensor', temb: 'Optional[torch.Tensor]'=None, encoder_hidden_states: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if cross_attention_kwargs.get('scale', None) is not None:
            logger.warning('Passing `scale` to `cross_attention_kwargs` is deprecated. `scale` will be ignored.')
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


def scale_lora_layers(model, weight):
    """
    Adjust the weightage given to the LoRA layers of the model.

    Args:
        model (`torch.nn.Module`):
            The model to scale.
        weight (`float`):
            The weight to be given to the LoRA layers.
    """
    if weight == 1.0:
        return
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            module.scale_layer(weight)


def unscale_lora_layers(model, weight: 'Optional[float]'=None):
    """
    Removes the previously passed weight given to the LoRA layers of the model.

    Args:
        model (`torch.nn.Module`):
            The model to scale.
        weight (`float`, *optional*):
            The weight to be given to the LoRA layers. If no scale is passed the scale of the lora layer will be
            re-initialized to the correct value. If 0.0 is passed, we will re-initialize the scale with the correct
            value.
    """
    if weight is None or weight == 1.0:
        return
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            if weight != 0:
                module.unscale_layer(weight)
            else:
                for adapter_name in module.active_adapters:
                    module.set_scale(adapter_name, 1.0)


class SplitInferenceModule(nn.Module):
    """
    A wrapper module class that splits inputs along a specified dimension before performing a forward pass.

    This module is useful when you need to perform inference on large tensors in a memory-efficient way by breaking
    them into smaller chunks, processing each chunk separately, and then reassembling the results.

    Args:
        module (`nn.Module`):
            The underlying PyTorch module that will be applied to each chunk of split inputs.
        split_size (`int`, defaults to `1`):
            The size of each chunk after splitting the input tensor.
        split_dim (`int`, defaults to `0`):
            The dimension along which the input tensors are split.
        input_kwargs_to_split (`List[str]`, defaults to `["hidden_states"]`):
            A list of keyword arguments (strings) that represent the input tensors to be split.

    Workflow:
        1. The keyword arguments specified in `input_kwargs_to_split` are split into smaller chunks using
        `torch.split()` along the dimension `split_dim` and with a chunk size of `split_size`.
        2. The `module` is invoked once for each split with both the split inputs and any unchanged arguments
        that were passed.
        3. The output tensors from each split are concatenated back together along `split_dim` before returning.

    Example:
        ```python
        >>> import torch
        >>> import torch.nn as nn

        >>> model = nn.Linear(1000, 1000)
        >>> split_module = SplitInferenceModule(model, split_size=2, split_dim=0, input_kwargs_to_split=["input"])

        >>> input_tensor = torch.randn(42, 1000)
        >>> # Will split the tensor into 21 slices of shape [2, 1000].
        >>> output = split_module(input=input_tensor)
        ```

    It is also possible to nest `SplitInferenceModule` across different split dimensions for more complex
    multi-dimensional splitting.
    """

    def __init__(self, module: 'nn.Module', split_size: 'int'=1, split_dim: 'int'=0, input_kwargs_to_split: 'List[str]'=['hidden_states']) ->None:
        super().__init__()
        self.module = module
        self.split_size = split_size
        self.split_dim = split_dim
        self.input_kwargs_to_split = set(input_kwargs_to_split)

    def forward(self, *args, **kwargs) ->Union[torch.Tensor, Tuple[torch.Tensor]]:
        """Forward method for the `SplitInferenceModule`.

        This method processes the input by splitting specified keyword arguments along a given dimension, running the
        underlying module on each split, and then concatenating the results. The splitting is controlled by the
        `split_size` and `split_dim` parameters specified during initialization.

        Args:
            *args (`Any`):
                Positional arguments that are passed directly to the `module` without modification.
            **kwargs (`Dict[str, torch.Tensor]`):
                Keyword arguments passed to the underlying `module`. Only keyword arguments whose names match the
                entries in `input_kwargs_to_split` and are of type `torch.Tensor` will be split. The remaining keyword
                arguments are passed unchanged.

        Returns:
            `Union[torch.Tensor, Tuple[torch.Tensor]]`:
                The outputs obtained from `SplitInferenceModule` are the same as if the underlying module was inferred
                without it.
                - If the underlying module returns a single tensor, the result will be a single concatenated tensor
                along the same `split_dim` after processing all splits.
                - If the underlying module returns a tuple of tensors, each element of the tuple will be concatenated
                along the `split_dim` across all splits, and the final result will be a tuple of concatenated tensors.
        """
        split_inputs = {}
        for key in list(kwargs.keys()):
            if key not in self.input_kwargs_to_split or not torch.is_tensor(kwargs[key]):
                continue
            split_inputs[key] = torch.split(kwargs[key], self.split_size, self.split_dim)
            kwargs.pop(key)
        results = []
        for split_input in zip(*split_inputs.values()):
            inputs = dict(zip(split_inputs.keys(), split_input))
            inputs.update(kwargs)
            intermediate_tensor_or_tensor_tuple = self.module(*args, **inputs)
            results.append(intermediate_tensor_or_tensor_tuple)
        if isinstance(results[0], torch.Tensor):
            return torch.cat(results, dim=self.split_dim)
        elif isinstance(results[0], tuple):
            return tuple([torch.cat(x, dim=self.split_dim) for x in zip(*results)])
        else:
            raise ValueError("In order to use the SplitInferenceModule, it is necessary for the underlying `module` to either return a torch.Tensor or a tuple of torch.Tensor's.")


class CoreAttention(torch.nn.Module):

    def __init__(self, config: 'ChatGLMConfig', layer_number):
        super(CoreAttention, self).__init__()
        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling
        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32
        if self.apply_query_key_layer_scaling:
            self.attention_softmax_in_fp32 = True
        self.layer_number = max(1, layer_number)
        projection_size = config.kv_channels * config.num_attention_heads
        self.hidden_size_per_partition = projection_size
        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads
        self.num_attention_heads_per_partition = config.num_attention_heads
        coeff = None
        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
        if self.apply_query_key_layer_scaling:
            coeff = self.layer_number
            self.norm_factor *= coeff
        self.coeff = coeff
        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)

    def forward(self, query_layer, key_layer, value_layer, attention_mask):
        pytorch_major_version = int(torch.__version__.split('.')[0])
        if pytorch_major_version >= 2:
            query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]
            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:
                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer, is_causal=True)
            else:
                if attention_mask is not None:
                    attention_mask = ~attention_mask
                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer, attention_mask)
            context_layer = context_layer.permute(2, 0, 1, 3)
            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
            context_layer = context_layer.reshape(*new_context_layer_shape)
        else:
            output_size = query_layer.size(1), query_layer.size(2), query_layer.size(0), key_layer.size(0)
            query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            matmul_input_buffer = torch.empty(output_size[0] * output_size[1], output_size[2], output_size[3], dtype=query_layer.dtype, device=query_layer.device)
            matmul_result = torch.baddbmm(matmul_input_buffer, query_layer.transpose(0, 1), key_layer.transpose(0, 1).transpose(1, 2), beta=0.0, alpha=1.0 / self.norm_factor)
            attention_scores = matmul_result.view(*output_size)
            if self.attention_softmax_in_fp32:
                attention_scores = attention_scores.float()
            if self.coeff is not None:
                attention_scores = attention_scores * self.coeff
            if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:
                attention_mask = torch.ones(output_size[0], 1, output_size[2], output_size[3], device=attention_scores.device, dtype=torch.bool)
                attention_mask.tril_()
                attention_mask = ~attention_mask
            if attention_mask is not None:
                attention_scores = attention_scores.masked_fill(attention_mask, float('-inf'))
            attention_probs = F.softmax(attention_scores, dim=-1)
            attention_probs = attention_probs.type_as(value_layer)
            attention_probs = self.attention_dropout(attention_probs)
            output_size = value_layer.size(1), value_layer.size(2), query_layer.size(0), value_layer.size(3)
            value_layer = value_layer.view(value_layer.size(0), output_size[0] * output_size[1], -1)
            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)
            context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))
            context_layer = context_layer.view(*output_size)
            context_layer = context_layer.permute(2, 0, 1, 3).contiguous()
            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
            context_layer = context_layer.view(*new_context_layer_shape)
        return context_layer


def _config_to_kwargs(args):
    common_kwargs = {'dtype': args.torch_dtype}
    return common_kwargs


@torch.jit.script
def apply_rotary_pos_emb(x: 'torch.Tensor', rope_cache: 'torch.Tensor') ->torch.Tensor:
    sq, _b, np, _hn = x.size(0), x.size(1), x.size(2), x.size(3)
    rot_dim = rope_cache.shape[-2] * 2
    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]
    rope_cache = rope_cache[:sq]
    xshaped = x.reshape(sq, -1, np, rot_dim // 2, 2)
    rope_cache = rope_cache.view(sq, -1, 1, xshaped.size(3), 2)
    x_out2 = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1]], -1)
    x_out2 = x_out2.flatten(3)
    return torch.cat((x_out2, x_pass), dim=-1)


def split_tensor_along_last_dim(tensor: 'torch.Tensor', num_partitions: 'int', contiguous_split_chunks: 'bool'=False) ->List[torch.Tensor]:
    """Split a tensor along its last dimension.

    Arguments:
        tensor: input tensor.
        num_partitions: number of partitions to split the tensor
        contiguous_split_chunks: If True, make each chunk contiguous
                                 in memory.

    Returns:
        A list of Tensors
    """
    last_dim = tensor.dim() - 1
    last_dim_size = tensor.size()[last_dim] // num_partitions
    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
    if contiguous_split_chunks:
        return tuple(chunk.contiguous() for chunk in tensor_list)
    return tensor_list


class SelfAttention(torch.nn.Module):
    """Parallel self-attention layer abstract class.

    Self-attention layer takes input with size [s, b, h] and returns output of the same size.
    """

    def __init__(self, config: 'ChatGLMConfig', layer_number, device=None):
        super(SelfAttention, self).__init__()
        self.layer_number = max(1, layer_number)
        self.projection_size = config.kv_channels * config.num_attention_heads
        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads
        self.num_attention_heads_per_partition = config.num_attention_heads
        self.multi_query_attention = config.multi_query_attention
        self.qkv_hidden_size = 3 * self.projection_size
        if self.multi_query_attention:
            self.num_multi_query_groups_per_partition = config.multi_query_group_num
            self.qkv_hidden_size = self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num
        self.query_key_value = nn.Linear(config.hidden_size, self.qkv_hidden_size, bias=config.add_bias_linear or config.add_qkv_bias, device=device, **_config_to_kwargs(config))
        self.core_attention = CoreAttention(config, self.layer_number)
        self.dense = nn.Linear(self.projection_size, config.hidden_size, bias=config.add_bias_linear, device=device, **_config_to_kwargs(config))

    def _allocate_memory(self, inference_max_sequence_len, batch_size, device=None, dtype=None):
        if self.multi_query_attention:
            num_attention_heads = self.num_multi_query_groups_per_partition
        else:
            num_attention_heads = self.num_attention_heads_per_partition
        return torch.empty(inference_max_sequence_len, batch_size, num_attention_heads, self.hidden_size_per_attention_head, dtype=dtype, device=device)

    def forward(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True):
        mixed_x_layer = self.query_key_value(hidden_states)
        if self.multi_query_attention:
            query_layer, key_layer, value_layer = mixed_x_layer.split([self.num_attention_heads_per_partition * self.hidden_size_per_attention_head, self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head, self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head], dim=-1)
            query_layer = query_layer.view(query_layer.size()[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head))
            key_layer = key_layer.view(key_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head))
            value_layer = value_layer.view(value_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head))
        else:
            new_tensor_shape = mixed_x_layer.size()[:-1] + (self.num_attention_heads_per_partition, 3 * self.hidden_size_per_attention_head)
            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)
            query_layer, key_layer, value_layer = split_tensor_along_last_dim(mixed_x_layer, 3)
        if rotary_pos_emb is not None:
            query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)
            key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)
        if kv_cache is not None:
            cache_k, cache_v = kv_cache
            key_layer = torch.cat((cache_k, key_layer), dim=0)
            value_layer = torch.cat((cache_v, value_layer), dim=0)
        if use_cache:
            kv_cache = key_layer, value_layer
        else:
            kv_cache = None
        if self.multi_query_attention:
            key_layer = key_layer.unsqueeze(-2)
            key_layer = key_layer.expand(-1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1)
            key_layer = key_layer.contiguous().view(key_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head))
            value_layer = value_layer.unsqueeze(-2)
            value_layer = value_layer.expand(-1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1)
            value_layer = value_layer.contiguous().view(value_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head))
        context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)
        output = self.dense(context_layer)
        return output, kv_cache


class MLP(torch.nn.Module):
    """MLP.

    MLP will take the input with h hidden state, project it to 4*h hidden dimension, perform nonlinear transformation,
    and project the state back into h hidden dimension.
    """

    def __init__(self, config: 'ChatGLMConfig', device=None):
        super(MLP, self).__init__()
        self.add_bias = config.add_bias_linear
        self.dense_h_to_4h = nn.Linear(config.hidden_size, config.ffn_hidden_size * 2, bias=self.add_bias, device=device, **_config_to_kwargs(config))

        def swiglu(x):
            x = torch.chunk(x, 2, dim=-1)
            return F.silu(x[0]) * x[1]
        self.activation_func = swiglu
        self.dense_4h_to_h = nn.Linear(config.ffn_hidden_size, config.hidden_size, bias=self.add_bias, device=device, **_config_to_kwargs(config))

    def forward(self, hidden_states):
        intermediate_parallel = self.dense_h_to_4h(hidden_states)
        intermediate_parallel = self.activation_func(intermediate_parallel)
        output = self.dense_4h_to_h(intermediate_parallel)
        return output


class GLMBlock(torch.nn.Module):
    """A single transformer layer.

    Transformer layer takes input with size [s, b, h] and returns an output of the same size.
    """

    def __init__(self, config: 'ChatGLMConfig', layer_number, device=None):
        super(GLMBlock, self).__init__()
        self.layer_number = layer_number
        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm
        self.fp32_residual_connection = config.fp32_residual_connection
        LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm
        self.input_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device, dtype=config.torch_dtype)
        self.self_attention = SelfAttention(config, layer_number, device=device)
        self.hidden_dropout = config.hidden_dropout
        self.post_attention_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device, dtype=config.torch_dtype)
        self.mlp = MLP(config, device=device)

    def forward(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True):
        layernorm_output = self.input_layernorm(hidden_states)
        attention_output, kv_cache = self.self_attention(layernorm_output, attention_mask, rotary_pos_emb, kv_cache=kv_cache, use_cache=use_cache)
        if self.apply_residual_connection_post_layernorm:
            residual = layernorm_output
        else:
            residual = hidden_states
        layernorm_input = torch.nn.functional.dropout(attention_output, p=self.hidden_dropout, training=self.training)
        layernorm_input = residual + layernorm_input
        layernorm_output = self.post_attention_layernorm(layernorm_input)
        mlp_output = self.mlp(layernorm_output)
        if self.apply_residual_connection_post_layernorm:
            residual = layernorm_output
        else:
            residual = layernorm_input
        output = torch.nn.functional.dropout(mlp_output, p=self.hidden_dropout, training=self.training)
        output = residual + output
        return output, kv_cache


class GLMTransformer(torch.nn.Module):
    """Transformer class."""

    def __init__(self, config: 'ChatGLMConfig', device=None):
        super(GLMTransformer, self).__init__()
        self.fp32_residual_connection = config.fp32_residual_connection
        self.post_layer_norm = config.post_layer_norm
        self.num_layers = config.num_layers

        def build_layer(layer_number):
            return GLMBlock(config, layer_number, device=device)
        self.layers = torch.nn.ModuleList([build_layer(i + 1) for i in range(self.num_layers)])
        if self.post_layer_norm:
            LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm
            self.final_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device, dtype=config.torch_dtype)
        self.gradient_checkpointing = False

    def _get_layer(self, layer_number):
        return self.layers[layer_number]

    def forward(self, hidden_states, attention_mask, rotary_pos_emb, kv_caches=None, use_cache: 'Optional[bool]'=True, output_hidden_states: 'Optional[bool]'=False):
        if not kv_caches:
            kv_caches = [None for _ in range(self.num_layers)]
        presents = () if use_cache else None
        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once('`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...')
                use_cache = False
        all_self_attentions = None
        all_hidden_states = () if output_hidden_states else None
        for index in range(self.num_layers):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)
            layer = self._get_layer(index)
            if self.gradient_checkpointing and self.training:
                layer_ret = torch.utils.checkpoint.checkpoint(layer, hidden_states, attention_mask, rotary_pos_emb, kv_caches[index], use_cache)
            else:
                layer_ret = layer(hidden_states, attention_mask, rotary_pos_emb, kv_cache=kv_caches[index], use_cache=use_cache)
            hidden_states, kv_cache = layer_ret
            if use_cache:
                presents = presents + (kv_cache,)
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        if self.post_layer_norm:
            hidden_states = self.final_layernorm(hidden_states)
        return hidden_states, presents, all_hidden_states, all_self_attentions


class Embedding(torch.nn.Module):
    """Language model embeddings."""

    def __init__(self, config: 'ChatGLMConfig', device=None):
        super(Embedding, self).__init__()
        self.hidden_size = config.hidden_size
        self.word_embeddings = nn.Embedding(config.padded_vocab_size, self.hidden_size, dtype=config.torch_dtype, device=device)
        self.fp32_residual_connection = config.fp32_residual_connection

    def forward(self, input_ids):
        words_embeddings = self.word_embeddings(input_ids)
        embeddings = words_embeddings
        embeddings = embeddings.transpose(0, 1).contiguous()
        if self.fp32_residual_connection:
            embeddings = embeddings.float()
        return embeddings


class RotaryEmbedding(nn.Module):

    def __init__(self, dim, original_impl=False, device=None, dtype=None):
        super().__init__()
        inv_freq = 1.0 / 10000 ** (torch.arange(0, dim, 2, device=device) / dim)
        self.register_buffer('inv_freq', inv_freq)
        self.dim = dim
        self.original_impl = original_impl

    def forward_impl(self, seq_len: 'int', n_elem: 'int', dtype: 'torch.dtype', device: 'torch.device', base: 'int'=10000):
        """Enhanced Transformer with Rotary Position Embedding.

        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/
        transformers/rope/__init__.py. MIT License:
        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.
        """
        theta = 1.0 / base ** (torch.arange(0, n_elem, 2, dtype=torch.float, device=device) / n_elem)
        seq_idx = torch.arange(seq_len, dtype=torch.float, device=device)
        idx_theta = torch.outer(seq_idx, theta).float()
        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)
        if dtype in (torch.float16, torch.bfloat16, torch.int8):
            cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()
        return cache

    def forward(self, max_seq_len, offset=0):
        return self.forward_impl(max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device)


class PrefixEncoder(torch.nn.Module):
    """
    The torch.nn model to encode the prefix Input shape: (batch-size, prefix-length) Output shape: (batch-size,
    prefix-length, 2*layers*hidden)
    """

    def __init__(self, config: 'ChatGLMConfig'):
        super().__init__()
        self.prefix_projection = config.prefix_projection
        if self.prefix_projection:
            kv_size = config.num_layers * config.kv_channels * config.multi_query_group_num * 2
            self.embedding = torch.nn.Embedding(config.pre_seq_len, kv_size)
            self.trans = torch.nn.Sequential(torch.nn.Linear(kv_size, config.hidden_size), torch.nn.Tanh(), torch.nn.Linear(config.hidden_size, kv_size))
        else:
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_layers * config.kv_channels * config.multi_query_group_num * 2)

    def forward(self, prefix: 'torch.Tensor'):
        if self.prefix_projection:
            prefix_tokens = self.embedding(prefix)
            past_key_values = self.trans(prefix_tokens)
        else:
            past_key_values = self.embedding(prefix)
        return past_key_values


class LDMBertAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, embed_dim: 'int', num_heads: 'int', head_dim: 'int', dropout: 'float'=0.0, is_decoder: 'bool'=False, bias: 'bool'=False):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = head_dim
        self.inner_dim = head_dim * num_heads
        self.scaling = self.head_dim ** -0.5
        self.is_decoder = is_decoder
        self.k_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)
        self.q_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)
        self.out_proj = nn.Linear(self.inner_dim, embed_dim)

    def _shape(self, tensor: 'torch.Tensor', seq_len: 'int', bsz: 'int'):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

    def forward(self, hidden_states: 'torch.Tensor', key_value_states: 'Optional[torch.Tensor]'=None, past_key_value: 'Optional[Tuple[torch.Tensor]]'=None, attention_mask: 'Optional[torch.Tensor]'=None, layer_head_mask: 'Optional[torch.Tensor]'=None, output_attentions: 'bool'=False) ->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """Input shape: Batch x Time x Channel"""
        is_cross_attention = key_value_states is not None
        bsz, tgt_len, _ = hidden_states.size()
        query_states = self.q_proj(hidden_states) * self.scaling
        if is_cross_attention and past_key_value is not None:
            key_states = past_key_value[0]
            value_states = past_key_value[1]
        elif is_cross_attention:
            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
        elif past_key_value is not None:
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        else:
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
        if self.is_decoder:
            past_key_value = key_states, value_states
        proj_shape = bsz * self.num_heads, -1, self.head_dim
        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
        key_states = key_states.view(*proj_shape)
        value_states = value_states.view(*proj_shape)
        src_len = key_states.size(1)
        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))
        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
            raise ValueError(f'Attention weights should be of size {bsz * self.num_heads, tgt_len, src_len}, but is {attn_weights.size()}')
        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, tgt_len, src_len):
                raise ValueError(f'Attention mask should be of size {bsz, 1, tgt_len, src_len}, but is {attention_mask.size()}')
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
        if layer_head_mask is not None:
            if layer_head_mask.size() != (self.num_heads,):
                raise ValueError(f'Head mask for a single layer should be of size {self.num_heads,}, but is {layer_head_mask.size()}')
            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        if output_attentions:
            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
        else:
            attn_weights_reshaped = None
        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
        attn_output = torch.bmm(attn_probs, value_states)
        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
            raise ValueError(f'`attn_output` should be of size {bsz, self.num_heads, tgt_len, self.head_dim}, but is {attn_output.size()}')
        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
        attn_output = attn_output.transpose(1, 2)
        attn_output = attn_output.reshape(bsz, tgt_len, self.inner_dim)
        attn_output = self.out_proj(attn_output)
        return attn_output, attn_weights_reshaped, past_key_value


class LDMBertEncoderLayer(nn.Module):

    def __init__(self, config: 'LDMBertConfig'):
        super().__init__()
        self.embed_dim = config.d_model
        self.self_attn = LDMBertAttention(embed_dim=self.embed_dim, num_heads=config.encoder_attention_heads, head_dim=config.head_dim, dropout=config.attention_dropout)
        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function]
        self.activation_dropout = config.activation_dropout
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)

    def forward(self, hidden_states: 'torch.Tensor', attention_mask: 'torch.Tensor', layer_head_mask: 'torch.Tensor', output_attentions: 'Optional[bool]'=False) ->Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Args:
            hidden_states (`torch.Tensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
            attention_mask (`torch.Tensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.Tensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        residual = hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)
        hidden_states, attn_weights, _ = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        residual = hidden_states
        hidden_states = self.final_layer_norm(hidden_states)
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):
            clamp_value = torch.finfo(hidden_states.dtype).max - 1000
            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)
        outputs = hidden_states,
        if output_attentions:
            outputs += attn_weights,
        return outputs


class PaintByExampleMapper(nn.Module):

    def __init__(self, config):
        super().__init__()
        num_layers = (config.num_hidden_layers + 1) // 5
        hid_size = config.hidden_size
        num_heads = 1
        self.blocks = nn.ModuleList([BasicTransformerBlock(hid_size, num_heads, hid_size, activation_fn='gelu', attention_bias=True) for _ in range(num_layers)])

    def forward(self, hidden_states):
        for block in self.blocks:
            hidden_states = block(hidden_states)
        return hidden_states


class VoidNeRFModel(nn.Module):
    """
    Implements the default empty space model where all queries are rendered as background.
    """

    def __init__(self, background, channel_scale=255.0):
        super().__init__()
        background = nn.Parameter(torch.from_numpy(np.array(background)) / channel_scale)
        self.register_buffer('background', background)

    def forward(self, position):
        background = self.background[None]
        shape = position.shape[:-1]
        ones = [1] * (len(shape) - 1)
        n_channels = background.shape[-1]
        background = torch.broadcast_to(background.view(background.shape[0], *ones, n_channels), [*shape, n_channels])
        return background


@dataclass
class VolumeRange:
    t0: 'torch.Tensor'
    t1: 'torch.Tensor'
    intersected: 'torch.Tensor'

    def __post_init__(self):
        assert self.t0.shape == self.t1.shape == self.intersected.shape

    def partition(self, ts):
        """
        Partitions t0 and t1 into n_samples intervals.

        Args:
            ts: [batch_size, *shape, n_samples, 1]

        Return:

            lower: [batch_size, *shape, n_samples, 1] upper: [batch_size, *shape, n_samples, 1] delta: [batch_size,
            *shape, n_samples, 1]

        where
            ts \\in [lower, upper] deltas = upper - lower
        """
        mids = (ts[..., 1:, :] + ts[..., :-1, :]) * 0.5
        lower = torch.cat([self.t0[..., None, :], mids], dim=-2)
        upper = torch.cat([mids, self.t1[..., None, :]], dim=-2)
        delta = upper - lower
        assert lower.shape == upper.shape == delta.shape == ts.shape
        return lower, upper, delta


class BoundingBoxVolume(nn.Module):
    """
    Axis-aligned bounding box defined by the two opposite corners.
    """

    def __init__(self, *, bbox_min, bbox_max, min_dist: float=0.0, min_t_range: float=0.001):
        """
        Args:
            bbox_min: the left/bottommost corner of the bounding box
            bbox_max: the other corner of the bounding box
            min_dist: all rays should start at least this distance away from the origin.
        """
        super().__init__()
        self.min_dist = min_dist
        self.min_t_range = min_t_range
        self.bbox_min = torch.tensor(bbox_min)
        self.bbox_max = torch.tensor(bbox_max)
        self.bbox = torch.stack([self.bbox_min, self.bbox_max])
        assert self.bbox.shape == (2, 3)
        assert min_dist >= 0.0
        assert min_t_range > 0.0

    def intersect(self, origin: 'torch.Tensor', direction: 'torch.Tensor', t0_lower: 'Optional[torch.Tensor]'=None, epsilon=1e-06):
        """
        Args:
            origin: [batch_size, *shape, 3]
            direction: [batch_size, *shape, 3]
            t0_lower: Optional [batch_size, *shape, 1] lower bound of t0 when intersecting this volume.
            params: Optional meta parameters in case Volume is parametric
            epsilon: to stabilize calculations

        Return:
            A tuple of (t0, t1, intersected) where each has a shape [batch_size, *shape, 1]. If a ray intersects with
            the volume, `o + td` is in the volume for all t in [t0, t1]. If the volume is bounded, t1 is guaranteed to
            be on the boundary of the volume.
        """
        batch_size, *shape, _ = origin.shape
        ones = [1] * len(shape)
        bbox = self.bbox.view(1, *ones, 2, 3)

        def _safe_divide(a, b, epsilon=1e-06):
            return a / torch.where(b < 0, b - epsilon, b + epsilon)
        ts = _safe_divide(bbox - origin[..., None, :], direction[..., None, :], epsilon=epsilon)
        t0 = ts.min(dim=-2).values.max(dim=-1, keepdim=True).values.clamp(self.min_dist)
        t1 = ts.max(dim=-2).values.min(dim=-1, keepdim=True).values
        assert t0.shape == t1.shape == (batch_size, *shape, 1)
        if t0_lower is not None:
            assert t0.shape == t0_lower.shape
            t0 = torch.maximum(t0, t0_lower)
        intersected = t0 + self.min_t_range < t1
        t0 = torch.where(intersected, t0, torch.zeros_like(t0))
        t1 = torch.where(intersected, t1, torch.ones_like(t1))
        return VolumeRange(t0=t0, t1=t1, intersected=intersected)


class StratifiedRaySampler(nn.Module):
    """
    Instead of fixed intervals, a sample is drawn uniformly at random from each interval.
    """

    def __init__(self, depth_mode: 'str'='linear'):
        """
        :param depth_mode: linear samples ts linearly in depth. harmonic ensures
            closer points are sampled more densely.
        """
        self.depth_mode = depth_mode
        assert self.depth_mode in ('linear', 'geometric', 'harmonic')

    def sample(self, t0: 'torch.Tensor', t1: 'torch.Tensor', n_samples: 'int', epsilon: 'float'=0.001) ->torch.Tensor:
        """
        Args:
            t0: start time has shape [batch_size, *shape, 1]
            t1: finish time has shape [batch_size, *shape, 1]
            n_samples: number of ts to sample
        Return:
            sampled ts of shape [batch_size, *shape, n_samples, 1]
        """
        ones = [1] * (len(t0.shape) - 1)
        ts = torch.linspace(0, 1, n_samples).view(*ones, n_samples).to(t0.dtype)
        if self.depth_mode == 'linear':
            ts = t0 * (1.0 - ts) + t1 * ts
        elif self.depth_mode == 'geometric':
            ts = (t0.clamp(epsilon).log() * (1.0 - ts) + t1.clamp(epsilon).log() * ts).exp()
        elif self.depth_mode == 'harmonic':
            ts = 1.0 / (1.0 / t0.clamp(epsilon) * (1.0 - ts) + 1.0 / t1.clamp(epsilon) * ts)
        mids = 0.5 * (ts[..., 1:] + ts[..., :-1])
        upper = torch.cat([mids, t1], dim=-1)
        lower = torch.cat([t0, mids], dim=-1)
        torch.manual_seed(0)
        t_rand = torch.rand_like(ts)
        ts = lower + (upper - lower) * t_rand
        return ts.unsqueeze(-1)


def sample_pmf(pmf: 'torch.Tensor', n_samples: 'int') ->torch.Tensor:
    """
    Sample from the given discrete probability distribution with replacement.

    The i-th bin is assumed to have mass pmf[i].

    Args:
        pmf: [batch_size, *shape, n_samples, 1] where (pmf.sum(dim=-2) == 1).all()
        n_samples: number of samples

    Return:
        indices sampled with replacement
    """
    *shape, support_size, last_dim = pmf.shape
    assert last_dim == 1
    cdf = torch.cumsum(pmf.view(-1, support_size), dim=1)
    inds = torch.searchsorted(cdf, torch.rand(cdf.shape[0], n_samples, device=cdf.device))
    return inds.view(*shape, n_samples, 1).clamp(0, support_size - 1)


class ImportanceRaySampler(nn.Module):
    """
    Given the initial estimate of densities, this samples more from regions/bins expected to have objects.
    """

    def __init__(self, volume_range: 'VolumeRange', ts: 'torch.Tensor', weights: 'torch.Tensor', blur_pool: 'bool'=False, alpha: 'float'=1e-05):
        """
        Args:
            volume_range: the range in which a ray intersects the given volume.
            ts: earlier samples from the coarse rendering step
            weights: discretized version of density * transmittance
            blur_pool: if true, use 2-tap max + 2-tap blur filter from mip-NeRF.
            alpha: small value to add to weights.
        """
        self.volume_range = volume_range
        self.ts = ts.clone().detach()
        self.weights = weights.clone().detach()
        self.blur_pool = blur_pool
        self.alpha = alpha

    @torch.no_grad()
    def sample(self, t0: 'torch.Tensor', t1: 'torch.Tensor', n_samples: 'int') ->torch.Tensor:
        """
        Args:
            t0: start time has shape [batch_size, *shape, 1]
            t1: finish time has shape [batch_size, *shape, 1]
            n_samples: number of ts to sample
        Return:
            sampled ts of shape [batch_size, *shape, n_samples, 1]
        """
        lower, upper, _ = self.volume_range.partition(self.ts)
        batch_size, *shape, n_coarse_samples, _ = self.ts.shape
        weights = self.weights
        if self.blur_pool:
            padded = torch.cat([weights[..., :1, :], weights, weights[..., -1:, :]], dim=-2)
            maxes = torch.maximum(padded[..., :-1, :], padded[..., 1:, :])
            weights = 0.5 * (maxes[..., :-1, :] + maxes[..., 1:, :])
        weights = weights + self.alpha
        pmf = weights / weights.sum(dim=-2, keepdim=True)
        inds = sample_pmf(pmf, n_samples)
        assert inds.shape == (batch_size, *shape, n_samples, 1)
        assert (inds >= 0).all() and (inds < n_coarse_samples).all()
        t_rand = torch.rand(inds.shape, device=inds.device)
        lower_ = torch.gather(lower, -2, inds)
        upper_ = torch.gather(upper, -2, inds)
        ts = lower_ + (upper_ - lower_) * t_rand
        ts = torch.sort(ts, dim=-2).values
        return ts


def _create_flat_edge_indices(flat_cube_indices: 'torch.Tensor', grid_size: 'Tuple[int, int, int]'):
    num_xs = (grid_size[0] - 1) * grid_size[1] * grid_size[2]
    y_offset = num_xs
    num_ys = grid_size[0] * (grid_size[1] - 1) * grid_size[2]
    z_offset = num_xs + num_ys
    return torch.stack([flat_cube_indices[:, 0] * grid_size[1] * grid_size[2] + flat_cube_indices[:, 1] * grid_size[2] + flat_cube_indices[:, 2], flat_cube_indices[:, 0] * grid_size[1] * grid_size[2] + (flat_cube_indices[:, 1] + 1) * grid_size[2] + flat_cube_indices[:, 2], flat_cube_indices[:, 0] * grid_size[1] * grid_size[2] + flat_cube_indices[:, 1] * grid_size[2] + flat_cube_indices[:, 2] + 1, flat_cube_indices[:, 0] * grid_size[1] * grid_size[2] + (flat_cube_indices[:, 1] + 1) * grid_size[2] + flat_cube_indices[:, 2] + 1, y_offset + flat_cube_indices[:, 0] * (grid_size[1] - 1) * grid_size[2] + flat_cube_indices[:, 1] * grid_size[2] + flat_cube_indices[:, 2], y_offset + (flat_cube_indices[:, 0] + 1) * (grid_size[1] - 1) * grid_size[2] + flat_cube_indices[:, 1] * grid_size[2] + flat_cube_indices[:, 2], y_offset + flat_cube_indices[:, 0] * (grid_size[1] - 1) * grid_size[2] + flat_cube_indices[:, 1] * grid_size[2] + flat_cube_indices[:, 2] + 1, y_offset + (flat_cube_indices[:, 0] + 1) * (grid_size[1] - 1) * grid_size[2] + flat_cube_indices[:, 1] * grid_size[2] + flat_cube_indices[:, 2] + 1, z_offset + flat_cube_indices[:, 0] * grid_size[1] * (grid_size[2] - 1) + flat_cube_indices[:, 1] * (grid_size[2] - 1) + flat_cube_indices[:, 2], z_offset + (flat_cube_indices[:, 0] + 1) * grid_size[1] * (grid_size[2] - 1) + flat_cube_indices[:, 1] * (grid_size[2] - 1) + flat_cube_indices[:, 2], z_offset + flat_cube_indices[:, 0] * grid_size[1] * (grid_size[2] - 1) + (flat_cube_indices[:, 1] + 1) * (grid_size[2] - 1) + flat_cube_indices[:, 2], z_offset + (flat_cube_indices[:, 0] + 1) * grid_size[1] * (grid_size[2] - 1) + (flat_cube_indices[:, 1] + 1) * (grid_size[2] - 1) + flat_cube_indices[:, 2]], dim=-1)


class MeshDecoder(nn.Module):
    """
    Construct meshes from Signed distance functions (SDFs) using marching cubes method
    """

    def __init__(self):
        super().__init__()
        cases = torch.zeros(256, 5, 3, dtype=torch.long)
        masks = torch.zeros(256, 5, dtype=torch.bool)
        self.register_buffer('cases', cases)
        self.register_buffer('masks', masks)

    def forward(self, field: 'torch.Tensor', min_point: 'torch.Tensor', size: 'torch.Tensor'):
        """
        For a signed distance field, produce a mesh using marching cubes.

        :param field: a 3D tensor of field values, where negative values correspond
                    to the outside of the shape. The dimensions correspond to the x, y, and z directions, respectively.
        :param min_point: a tensor of shape [3] containing the point corresponding
                        to (0, 0, 0) in the field.
        :param size: a tensor of shape [3] containing the per-axis distance from the
                    (0, 0, 0) field corner and the (-1, -1, -1) field corner.
        """
        assert len(field.shape) == 3, 'input must be a 3D scalar field'
        dev = field.device
        cases = self.cases
        masks = self.masks
        min_point = min_point
        size = size
        grid_size = field.shape
        grid_size_tensor = torch.tensor(grid_size)
        bitmasks = field > 0
        bitmasks = bitmasks[:-1, :, :] | bitmasks[1:, :, :] << 1
        bitmasks = bitmasks[:, :-1, :] | bitmasks[:, 1:, :] << 2
        bitmasks = bitmasks[:, :, :-1] | bitmasks[:, :, 1:] << 4
        corner_coords = torch.empty(*grid_size, 3, device=dev, dtype=field.dtype)
        corner_coords[range(grid_size[0]), :, :, 0] = torch.arange(grid_size[0], device=dev, dtype=field.dtype)[:, None, None]
        corner_coords[:, range(grid_size[1]), :, 1] = torch.arange(grid_size[1], device=dev, dtype=field.dtype)[:, None]
        corner_coords[:, :, range(grid_size[2]), 2] = torch.arange(grid_size[2], device=dev, dtype=field.dtype)
        edge_midpoints = torch.cat([((corner_coords[:-1] + corner_coords[1:]) / 2).reshape(-1, 3), ((corner_coords[:, :-1] + corner_coords[:, 1:]) / 2).reshape(-1, 3), ((corner_coords[:, :, :-1] + corner_coords[:, :, 1:]) / 2).reshape(-1, 3)], dim=0)
        cube_indices = torch.zeros(grid_size[0] - 1, grid_size[1] - 1, grid_size[2] - 1, 3, device=dev, dtype=torch.long)
        cube_indices[range(grid_size[0] - 1), :, :, 0] = torch.arange(grid_size[0] - 1, device=dev)[:, None, None]
        cube_indices[:, range(grid_size[1] - 1), :, 1] = torch.arange(grid_size[1] - 1, device=dev)[:, None]
        cube_indices[:, :, range(grid_size[2] - 1), 2] = torch.arange(grid_size[2] - 1, device=dev)
        flat_cube_indices = cube_indices.reshape(-1, 3)
        edge_indices = _create_flat_edge_indices(flat_cube_indices, grid_size)
        flat_bitmasks = bitmasks.reshape(-1).long()
        local_tris = cases[flat_bitmasks]
        local_masks = masks[flat_bitmasks]
        global_tris = torch.gather(edge_indices, 1, local_tris.reshape(local_tris.shape[0], -1)).reshape(local_tris.shape)
        selected_tris = global_tris.reshape(-1, 3)[local_masks.reshape(-1)]
        used_vertex_indices = torch.unique(selected_tris.view(-1))
        used_edge_midpoints = edge_midpoints[used_vertex_indices]
        old_index_to_new_index = torch.zeros(len(edge_midpoints), device=dev, dtype=torch.long)
        old_index_to_new_index[used_vertex_indices] = torch.arange(len(used_vertex_indices), device=dev, dtype=torch.long)
        faces = torch.gather(old_index_to_new_index, 0, selected_tris.view(-1)).reshape(selected_tris.shape)
        v1 = torch.floor(used_edge_midpoints)
        v2 = torch.ceil(used_edge_midpoints)
        s1 = field[v1[:, 0], v1[:, 1], v1[:, 2]]
        s2 = field[v2[:, 0], v2[:, 1], v2[:, 2]]
        p1 = v1.float() / (grid_size_tensor - 1) * size + min_point
        p2 = v2.float() / (grid_size_tensor - 1) * size + min_point
        t = (s1 / (s1 - s2))[:, None]
        verts = t * p2 + (1 - t) * p1
        return MeshDecoderOutput(verts=verts, faces=faces, vertex_channels=None)


def posenc_nerf(x: 'torch.Tensor', min_deg: 'int'=0, max_deg: 'int'=15) ->torch.Tensor:
    """
    Concatenate x and its positional encodings, following NeRF.

    Reference: https://arxiv.org/pdf/2210.04628.pdf
    """
    if min_deg == max_deg:
        return x
    scales = 2.0 ** torch.arange(min_deg, max_deg, dtype=x.dtype, device=x.device)
    *shape, dim = x.shape
    xb = (x.reshape(-1, 1, dim) * scales.view(1, -1, 1)).reshape(*shape, -1)
    assert xb.shape[-1] == dim * (max_deg - min_deg)
    emb = torch.cat([xb, xb + math.pi / 2.0], axis=-1).sin()
    return torch.cat([x, emb], dim=-1)


def encode_direction(position, direction=None):
    if direction is None:
        return torch.zeros_like(posenc_nerf(position, min_deg=0, max_deg=8))
    else:
        return posenc_nerf(direction, min_deg=0, max_deg=8)


def encode_position(position):
    return posenc_nerf(position, min_deg=0, max_deg=15)


class ChannelsProj(nn.Module):

    def __init__(self, *, vectors: int, channels: int, d_latent: int):
        super().__init__()
        self.proj = nn.Linear(d_latent, vectors * channels)
        self.norm = nn.LayerNorm(channels)
        self.d_latent = d_latent
        self.vectors = vectors
        self.channels = channels

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x_bvd = x
        w_vcd = self.proj.weight.view(self.vectors, self.channels, self.d_latent)
        b_vc = self.proj.bias.view(1, self.vectors, self.channels)
        h = torch.einsum('bvd,vcd->bvc', x_bvd, w_vcd)
        h = self.norm(h)
        h = h + b_vc
        return h


def _sanitize_name(x: 'str') ->str:
    return x.replace('.', '__')


def _convert_srgb_to_linear(u: 'torch.Tensor'):
    return torch.where(u <= 0.04045, u / 12.92, ((u + 0.055) / 1.055) ** 2.4)


def integrate_samples(volume_range, ts, density, channels):
    """
    Function integrating the model output.

    Args:
        volume_range: Specifies the integral range [t0, t1]
        ts: timesteps
        density: torch.Tensor [batch_size, *shape, n_samples, 1]
        channels: torch.Tensor [batch_size, *shape, n_samples, n_channels]
    returns:
        channels: integrated rgb output weights: torch.Tensor [batch_size, *shape, n_samples, 1] (density
        *transmittance)[i] weight for each rgb output at [..., i, :]. transmittance: transmittance of this volume
    )
    """
    _, _, dt = volume_range.partition(ts)
    ddensity = density * dt
    mass = torch.cumsum(ddensity, dim=-2)
    transmittance = torch.exp(-mass[..., -1, :])
    alphas = 1.0 - torch.exp(-ddensity)
    Ts = torch.exp(torch.cat([torch.zeros_like(mass[..., :1, :]), -mass[..., :-1, :]], dim=-2))
    weights = alphas * Ts
    channels = torch.sum(channels * weights, dim=-2)
    return channels, weights, transmittance


def volume_query_points(volume, grid_size):
    indices = torch.arange(grid_size ** 3, device=volume.bbox_min.device)
    zs = indices % grid_size
    ys = torch.div(indices, grid_size, rounding_mode='trunc') % grid_size
    xs = torch.div(indices, grid_size ** 2, rounding_mode='trunc') % grid_size
    combined = torch.stack([xs, ys, zs], dim=1)
    return combined.float() / (grid_size - 1) * (volume.bbox_max - volume.bbox_min) + volume.bbox_min


class StableAudioPositionalEmbedding(nn.Module):
    """Used for continuous time"""

    def __init__(self, dim: 'int'):
        super().__init__()
        assert dim % 2 == 0
        half_dim = dim // 2
        self.weights = nn.Parameter(torch.randn(half_dim))

    def forward(self, times: 'torch.Tensor') ->torch.Tensor:
        times = times[..., None]
        freqs = times * self.weights[None] * 2 * pi
        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-1)
        fouriered = torch.cat((times, fouriered), dim=-1)
        return fouriered


class StableAudioNumberConditioner(nn.Module):
    """
    A simple linear projection model to map numbers to a latent space.

    Args:
        number_embedding_dim (`int`):
            Dimensionality of the number embeddings.
        min_value (`int`):
            The minimum value of the seconds number conditioning modules.
        max_value (`int`):
            The maximum value of the seconds number conditioning modules
        internal_dim (`int`):
            Dimensionality of the intermediate number hidden states.
    """

    def __init__(self, number_embedding_dim, min_value, max_value, internal_dim: 'Optional[int]'=256):
        super().__init__()
        self.time_positional_embedding = nn.Sequential(StableAudioPositionalEmbedding(internal_dim), nn.Linear(in_features=internal_dim + 1, out_features=number_embedding_dim))
        self.number_embedding_dim = number_embedding_dim
        self.min_value = min_value
        self.max_value = max_value

    def forward(self, floats: 'torch.Tensor'):
        floats = floats.clamp(self.min_value, self.max_value)
        normalized_floats = (floats - self.min_value) / (self.max_value - self.min_value)
        embedder_dtype = next(self.time_positional_embedding.parameters()).dtype
        normalized_floats = normalized_floats
        embedding = self.time_positional_embedding(normalized_floats)
        float_embeds = embedding.view(-1, 1, self.number_embedding_dim)
        return float_embeds


class StableAudioProjectionModel(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class CLIPImageProjection(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


def jax_cosine_distance(emb_1, emb_2, eps=1e-12):
    norm_emb_1 = jnp.divide(emb_1.T, jnp.clip(jnp.linalg.norm(emb_1, axis=1), a_min=eps)).T
    norm_emb_2 = jnp.divide(emb_2.T, jnp.clip(jnp.linalg.norm(emb_2, axis=1), a_min=eps)).T
    return jnp.matmul(norm_emb_1, norm_emb_2.T)


class UniDiffuserTextDecoder(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class SkipBlock(nn.Module):

    def __init__(self, dim: 'int'):
        super().__init__()
        self.skip_linear = nn.Linear(2 * dim, dim)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x, skip):
        x = self.skip_linear(torch.cat([x, skip], dim=-1))
        x = self.norm(x)
        return x


class UTransformerBlock(nn.Module):
    """
    A modification of BasicTransformerBlock which supports pre-LayerNorm and post-LayerNorm configurations.

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        activation_fn (`str`, *optional*, defaults to `"geglu"`):
            Activation function to be used in feed-forward.
        num_embeds_ada_norm (:obj: `int`, *optional*):
            The number of diffusion steps used during training. See `Transformer2DModel`.
        attention_bias (:obj: `bool`, *optional*, defaults to `False`):
            Configure if the attentions should contain a bias parameter.
        only_cross_attention (`bool`, *optional*):
            Whether to use only cross-attention layers. In this case two cross attention layers are used.
        double_self_attention (`bool`, *optional*):
            Whether to use two self-attention layers. In this case no cross attention layers are used.
        upcast_attention (`bool`, *optional*):
            Whether to upcast the query and key to float32 when performing the attention calculation.
        norm_elementwise_affine (`bool`, *optional*):
            Whether to use learnable per-element affine parameters during layer normalization.
        norm_type (`str`, defaults to `"layer_norm"`):
            The layer norm implementation to use.
        pre_layer_norm (`bool`, *optional*):
            Whether to perform layer normalization before the attention and feedforward operations ("pre-LayerNorm"),
            as opposed to after ("post-LayerNorm"). Note that `BasicTransformerBlock` uses pre-LayerNorm, e.g.
            `pre_layer_norm = True`.
        final_dropout (`bool`, *optional*):
            Whether to use a final Dropout layer after the feedforward network.
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', dropout=0.0, cross_attention_dim: 'Optional[int]'=None, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None, attention_bias: 'bool'=False, only_cross_attention: 'bool'=False, double_self_attention: 'bool'=False, upcast_attention: 'bool'=False, norm_elementwise_affine: 'bool'=True, norm_type: 'str'='layer_norm', pre_layer_norm: 'bool'=True, final_dropout: 'bool'=False):
        super().__init__()
        self.only_cross_attention = only_cross_attention
        self.use_ada_layer_norm = num_embeds_ada_norm is not None and norm_type == 'ada_norm'
        self.pre_layer_norm = pre_layer_norm
        if norm_type in ('ada_norm', 'ada_norm_zero') and num_embeds_ada_norm is None:
            raise ValueError(f'`norm_type` is set to {norm_type}, but `num_embeds_ada_norm` is not defined. Please make sure to define `num_embeds_ada_norm` if setting `norm_type` to {norm_type}.')
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=cross_attention_dim if only_cross_attention else None, upcast_attention=upcast_attention)
        if cross_attention_dim is not None or double_self_attention:
            self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim if not double_self_attention else None, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, upcast_attention=upcast_attention)
        else:
            self.attn2 = None
        if self.use_ada_layer_norm:
            self.norm1 = AdaLayerNorm(dim, num_embeds_ada_norm)
        else:
            self.norm1 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        if cross_attention_dim is not None or double_self_attention:
            self.norm2 = AdaLayerNorm(dim, num_embeds_ada_norm) if self.use_ada_layer_norm else nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        else:
            self.norm2 = None
        self.norm3 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_fn, final_dropout=final_dropout)

    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, timestep=None, cross_attention_kwargs=None, class_labels=None):
        if self.pre_layer_norm:
            if self.use_ada_layer_norm:
                norm_hidden_states = self.norm1(hidden_states, timestep)
            else:
                norm_hidden_states = self.norm1(hidden_states)
        else:
            norm_hidden_states = hidden_states
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        attn_output = self.attn1(norm_hidden_states, encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None, attention_mask=attention_mask, **cross_attention_kwargs)
        if not self.pre_layer_norm:
            if self.use_ada_layer_norm:
                attn_output = self.norm1(attn_output, timestep)
            else:
                attn_output = self.norm1(attn_output)
        hidden_states = attn_output + hidden_states
        if self.attn2 is not None:
            if self.pre_layer_norm:
                norm_hidden_states = self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
            else:
                norm_hidden_states = hidden_states
            attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=encoder_attention_mask, **cross_attention_kwargs)
            if not self.pre_layer_norm:
                attn_output = self.norm2(attn_output, timestep) if self.use_ada_layer_norm else self.norm2(attn_output)
            hidden_states = attn_output + hidden_states
        if self.pre_layer_norm:
            norm_hidden_states = self.norm3(hidden_states)
        else:
            norm_hidden_states = hidden_states
        ff_output = self.ff(norm_hidden_states)
        if not self.pre_layer_norm:
            ff_output = self.norm3(ff_output)
        hidden_states = ff_output + hidden_states
        return hidden_states


class UniDiffuserBlock(nn.Module):
    """
    A modification of BasicTransformerBlock which supports pre-LayerNorm and post-LayerNorm configurations and puts the
    LayerNorms on the residual backbone of the block. This matches the transformer block in the [original UniDiffuser
    implementation](https://github.com/thu-ml/unidiffuser/blob/main/libs/uvit_multi_post_ln_v1.py#L104).

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        activation_fn (`str`, *optional*, defaults to `"geglu"`):
            Activation function to be used in feed-forward.
        num_embeds_ada_norm (:obj: `int`, *optional*):
            The number of diffusion steps used during training. See `Transformer2DModel`.
        attention_bias (:obj: `bool`, *optional*, defaults to `False`):
            Configure if the attentions should contain a bias parameter.
        only_cross_attention (`bool`, *optional*):
            Whether to use only cross-attention layers. In this case two cross attention layers are used.
        double_self_attention (`bool`, *optional*):
            Whether to use two self-attention layers. In this case no cross attention layers are used.
        upcast_attention (`bool`, *optional*):
            Whether to upcast the query and key to float() when performing the attention calculation.
        norm_elementwise_affine (`bool`, *optional*):
            Whether to use learnable per-element affine parameters during layer normalization.
        norm_type (`str`, defaults to `"layer_norm"`):
            The layer norm implementation to use.
        pre_layer_norm (`bool`, *optional*):
            Whether to perform layer normalization before the attention and feedforward operations ("pre-LayerNorm"),
            as opposed to after ("post-LayerNorm"). The original UniDiffuser implementation is post-LayerNorm
            (`pre_layer_norm = False`).
        final_dropout (`bool`, *optional*):
            Whether to use a final Dropout layer after the feedforward network.
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', dropout=0.0, cross_attention_dim: 'Optional[int]'=None, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None, attention_bias: 'bool'=False, only_cross_attention: 'bool'=False, double_self_attention: 'bool'=False, upcast_attention: 'bool'=False, norm_elementwise_affine: 'bool'=True, norm_type: 'str'='layer_norm', pre_layer_norm: 'bool'=False, final_dropout: 'bool'=True):
        super().__init__()
        self.only_cross_attention = only_cross_attention
        self.use_ada_layer_norm = num_embeds_ada_norm is not None and norm_type == 'ada_norm'
        self.pre_layer_norm = pre_layer_norm
        if norm_type in ('ada_norm', 'ada_norm_zero') and num_embeds_ada_norm is None:
            raise ValueError(f'`norm_type` is set to {norm_type}, but `num_embeds_ada_norm` is not defined. Please make sure to define `num_embeds_ada_norm` if setting `norm_type` to {norm_type}.')
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=cross_attention_dim if only_cross_attention else None, upcast_attention=upcast_attention)
        if cross_attention_dim is not None or double_self_attention:
            self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim if not double_self_attention else None, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, upcast_attention=upcast_attention)
        else:
            self.attn2 = None
        if self.use_ada_layer_norm:
            self.norm1 = AdaLayerNorm(dim, num_embeds_ada_norm)
        else:
            self.norm1 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        if cross_attention_dim is not None or double_self_attention:
            self.norm2 = AdaLayerNorm(dim, num_embeds_ada_norm) if self.use_ada_layer_norm else nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        else:
            self.norm2 = None
        self.norm3 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_fn, final_dropout=final_dropout)

    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, timestep=None, cross_attention_kwargs=None, class_labels=None):
        if self.pre_layer_norm:
            if self.use_ada_layer_norm:
                hidden_states = self.norm1(hidden_states, timestep)
            else:
                hidden_states = self.norm1(hidden_states)
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        attn_output = self.attn1(hidden_states, encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None, attention_mask=attention_mask, **cross_attention_kwargs)
        hidden_states = attn_output + hidden_states
        if not self.pre_layer_norm:
            if self.use_ada_layer_norm:
                hidden_states = self.norm1(hidden_states, timestep)
            else:
                hidden_states = self.norm1(hidden_states)
        if self.attn2 is not None:
            if self.pre_layer_norm:
                hidden_states = self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
            attn_output = self.attn2(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=encoder_attention_mask, **cross_attention_kwargs)
            hidden_states = attn_output + hidden_states
            if not self.pre_layer_norm:
                hidden_states = self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
        if self.pre_layer_norm:
            hidden_states = self.norm3(hidden_states)
        ff_output = self.ff(hidden_states)
        hidden_states = ff_output + hidden_states
        if not self.pre_layer_norm:
            hidden_states = self.norm3(hidden_states)
        return hidden_states


class UniDiffuserModel(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class MixingResidualBlock(nn.Module):
    """
    Residual block with mixing used by Paella's VQ-VAE.
    """

    def __init__(self, inp_channels, embed_dim):
        super().__init__()
        self.norm1 = nn.LayerNorm(inp_channels, elementwise_affine=False, eps=1e-06)
        self.depthwise = nn.Sequential(nn.ReplicationPad2d(1), nn.Conv2d(inp_channels, inp_channels, kernel_size=3, groups=inp_channels))
        self.norm2 = nn.LayerNorm(inp_channels, elementwise_affine=False, eps=1e-06)
        self.channelwise = nn.Sequential(nn.Linear(inp_channels, embed_dim), nn.GELU(), nn.Linear(embed_dim, inp_channels))
        self.gammas = nn.Parameter(torch.zeros(6), requires_grad=True)

    def forward(self, x):
        mods = self.gammas
        x_temp = self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) * (1 + mods[0]) + mods[1]
        x = x + self.depthwise(x_temp) * mods[2]
        x_temp = self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) * (1 + mods[3]) + mods[4]
        x = x + self.channelwise(x_temp.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) * mods[5]
        return x


def apply_forward_hook(method):
    """
    Decorator that applies a registered CpuOffload hook to an arbitrary function rather than `forward`. This is useful
    for cases where a PyTorch module provides functions other than `forward` that should trigger a move to the
    appropriate acceleration device. This is the case for `encode` and `decode` in [`AutoencoderKL`].

    This decorator looks inside the internal `_hf_hook` property to find a registered offload hook.

    :param method: The method to decorate. This method should be a method of a PyTorch module.
    """
    if not is_accelerate_available():
        return method
    accelerate_version = version.parse(accelerate.__version__).base_version
    if version.parse(accelerate_version) < version.parse('0.17.0'):
        return method

    def wrapper(self, *args, **kwargs):
        if hasattr(self, '_hf_hook') and hasattr(self._hf_hook, 'pre_forward'):
            self._hf_hook.pre_forward(self)
        return method(self, *args, **kwargs)
    return wrapper


class WuerstchenLayerNorm(nn.LayerNorm):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, x):
        x = x.permute(0, 2, 3, 1)
        x = super().forward(x)
        return x.permute(0, 3, 1, 2)


class TimestepBlock(nn.Module):

    def __init__(self, c, c_timestep):
        super().__init__()
        self.mapper = nn.Linear(c_timestep, c * 2)

    def forward(self, x, t):
        a, b = self.mapper(t)[:, :, None, None].chunk(2, dim=1)
        return x * (1 + a) + b


class ResBlock(nn.Module):

    def __init__(self, c, c_skip=0, kernel_size=3, dropout=0.0):
        super().__init__()
        self.depthwise = nn.Conv2d(c + c_skip, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c)
        self.norm = WuerstchenLayerNorm(c, elementwise_affine=False, eps=1e-06)
        self.channelwise = nn.Sequential(nn.Linear(c, c * 4), nn.GELU(), GlobalResponseNorm(c * 4), nn.Dropout(dropout), nn.Linear(c * 4, c))

    def forward(self, x, x_skip=None):
        x_res = x
        if x_skip is not None:
            x = torch.cat([x, x_skip], dim=1)
        x = self.norm(self.depthwise(x)).permute(0, 2, 3, 1)
        x = self.channelwise(x).permute(0, 3, 1, 2)
        return x + x_res


class AttnBlock(nn.Module):

    def __init__(self, c, c_cond, nhead, self_attn=True, dropout=0.0):
        super().__init__()
        self.self_attn = self_attn
        self.norm = WuerstchenLayerNorm(c, elementwise_affine=False, eps=1e-06)
        self.attention = Attention(query_dim=c, heads=nhead, dim_head=c // nhead, dropout=dropout, bias=True)
        self.kv_mapper = nn.Sequential(nn.SiLU(), nn.Linear(c_cond, c))

    def forward(self, x, kv):
        kv = self.kv_mapper(kv)
        norm_x = self.norm(x)
        if self.self_attn:
            batch_size, channel, _, _ = x.shape
            kv = torch.cat([norm_x.view(batch_size, channel, -1).transpose(1, 2), kv], dim=1)
        x = x + self.attention(norm_x, encoder_hidden_states=kv)
        return x


class ResBlockStageB(nn.Module):

    def __init__(self, c, c_skip=0, kernel_size=3, dropout=0.0):
        super().__init__()
        self.depthwise = nn.Conv2d(c, c, kernel_size=kernel_size, padding=kernel_size // 2, groups=c)
        self.norm = WuerstchenLayerNorm(c, elementwise_affine=False, eps=1e-06)
        self.channelwise = nn.Sequential(nn.Linear(c + c_skip, c * 4), nn.GELU(), GlobalResponseNorm(c * 4), nn.Dropout(dropout), nn.Linear(c * 4, c))

    def forward(self, x, x_skip=None):
        x_res = x
        x = self.norm(self.depthwise(x))
        if x_skip is not None:
            x = torch.cat([x, x_skip], dim=1)
        x = self.channelwise(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        return x + x_res


MIN_PEFT_VERSION = '0.6.0'


def _translate_into_actual_layer_name(name):
    """Translate user-friendly name (e.g. 'mid') into actual layer name (e.g. 'mid_block.attentions.0')"""
    if name == 'mid':
        return 'mid_block.attentions.0'
    updown, block, attn = name.split('.')
    updown = updown.replace('down', 'down_blocks').replace('up', 'up_blocks')
    block = block.replace('block_', '')
    attn = 'attentions.' + attn
    return '.'.join((updown, block, attn))


def _maybe_expand_lora_scales_for_one_adapter(scales: 'Union[float, Dict]', blocks_with_transformer: 'Dict[str, int]', transformer_per_block: 'Dict[str, int]', state_dict: 'None', default_scale: 'float'=1.0):
    """
    Expands the inputs into a more granular dictionary. See the example below for more details.

    Parameters:
        scales (`Union[float, Dict]`):
            Scales dict to expand.
        blocks_with_transformer (`Dict[str, int]`):
            Dict with keys 'up' and 'down', showing which blocks have transformer layers
        transformer_per_block (`Dict[str, int]`):
            Dict with keys 'up' and 'down', showing how many transformer layers each block has

    E.g. turns
    ```python
    scales = {"down": 2, "mid": 3, "up": {"block_0": 4, "block_1": [5, 6, 7]}}
    blocks_with_transformer = {"down": [1, 2], "up": [0, 1]}
    transformer_per_block = {"down": 2, "up": 3}
    ```
    into
    ```python
    {
        "down.block_1.0": 2,
        "down.block_1.1": 2,
        "down.block_2.0": 2,
        "down.block_2.1": 2,
        "mid": 3,
        "up.block_0.0": 4,
        "up.block_0.1": 4,
        "up.block_0.2": 4,
        "up.block_1.0": 5,
        "up.block_1.1": 6,
        "up.block_1.2": 7,
    }
    ```
    """
    if sorted(blocks_with_transformer.keys()) != ['down', 'up']:
        raise ValueError("blocks_with_transformer needs to be a dict with keys `'down' and `'up'`")
    if sorted(transformer_per_block.keys()) != ['down', 'up']:
        raise ValueError("transformer_per_block needs to be a dict with keys `'down' and `'up'`")
    if not isinstance(scales, dict):
        return scales
    scales = copy.deepcopy(scales)
    if 'mid' not in scales:
        scales['mid'] = default_scale
    elif isinstance(scales['mid'], list):
        if len(scales['mid']) == 1:
            scales['mid'] = scales['mid'][0]
        else:
            raise ValueError(f"Expected 1 scales for mid, got {len(scales['mid'])}.")
    for updown in ['up', 'down']:
        if updown not in scales:
            scales[updown] = default_scale
        if not isinstance(scales[updown], dict):
            scales[updown] = {f'block_{i}': copy.deepcopy(scales[updown]) for i in blocks_with_transformer[updown]}
        for i in blocks_with_transformer[updown]:
            block = f'block_{i}'
            if block not in scales[updown]:
                scales[updown][block] = default_scale
            if not isinstance(scales[updown][block], list):
                scales[updown][block] = [scales[updown][block] for _ in range(transformer_per_block[updown])]
            elif len(scales[updown][block]) == 1:
                scales[updown][block] = scales[updown][block] * transformer_per_block[updown]
            elif len(scales[updown][block]) != transformer_per_block[updown]:
                raise ValueError(f'Expected {transformer_per_block[updown]} scales for {updown}.{block}, got {len(scales[updown][block])}.')
        for i in blocks_with_transformer[updown]:
            block = f'block_{i}'
            for tf_idx, value in enumerate(scales[updown][block]):
                scales[f'{updown}.{block}.{tf_idx}'] = value
        del scales[updown]
    for layer in scales.keys():
        if not any(_translate_into_actual_layer_name(layer) in module for module in state_dict.keys()):
            raise ValueError(f"Can't set lora scale for layer {layer}. It either doesn't exist in this unet or it has no attentions.")
    return {_translate_into_actual_layer_name(name): weight for name, weight in scales.items()}


def _maybe_expand_lora_scales(unet: "'UNet2DConditionModel'", weight_scales: 'List[Union[float, Dict]]', default_scale=1.0):
    blocks_with_transformer = {'down': [i for i, block in enumerate(unet.down_blocks) if hasattr(block, 'attentions')], 'up': [i for i, block in enumerate(unet.up_blocks) if hasattr(block, 'attentions')]}
    transformer_per_block = {'down': unet.config.layers_per_block, 'up': unet.config.layers_per_block + 1}
    expanded_weight_scales = [_maybe_expand_lora_scales_for_one_adapter(weight_for_adapter, blocks_with_transformer, transformer_per_block, unet.state_dict(), default_scale=default_scale) for weight_for_adapter in weight_scales]
    return expanded_weight_scales


_SET_ADAPTER_SCALE_FN_MAPPING = {'UNet2DConditionModel': _maybe_expand_lora_scales, 'UNetMotionModel': _maybe_expand_lora_scales, 'SD3Transformer2DModel': lambda model_cls, weights: weights, 'FluxTransformer2DModel': lambda model_cls, weights: weights, 'CogVideoXTransformer3DModel': lambda model_cls, weights: weights}


LORA_WEIGHT_NAME = 'pytorch_lora_weights.bin'


LORA_WEIGHT_NAME_SAFE = 'pytorch_lora_weights.safetensors'


def _best_guess_weight_name(pretrained_model_name_or_path_or_dict, file_extension='.safetensors', local_files_only=False):
    if local_files_only or HF_HUB_OFFLINE:
        raise ValueError('When using the offline mode, you must specify a `weight_name`.')
    targeted_files = []
    if os.path.isfile(pretrained_model_name_or_path_or_dict):
        return
    elif os.path.isdir(pretrained_model_name_or_path_or_dict):
        targeted_files = [f for f in os.listdir(pretrained_model_name_or_path_or_dict) if f.endswith(file_extension)]
    else:
        files_in_repo = model_info(pretrained_model_name_or_path_or_dict).siblings
        targeted_files = [f.rfilename for f in files_in_repo if f.rfilename.endswith(file_extension)]
    if len(targeted_files) == 0:
        return
    unallowed_substrings = {'scheduler', 'optimizer', 'checkpoint'}
    targeted_files = list(filter(lambda x: all(substring not in x for substring in unallowed_substrings), targeted_files))
    if any(f.endswith(LORA_WEIGHT_NAME) for f in targeted_files):
        targeted_files = list(filter(lambda x: x.endswith(LORA_WEIGHT_NAME), targeted_files))
    elif any(f.endswith(LORA_WEIGHT_NAME_SAFE) for f in targeted_files):
        targeted_files = list(filter(lambda x: x.endswith(LORA_WEIGHT_NAME_SAFE), targeted_files))
    if len(targeted_files) > 1:
        raise ValueError(f"Provided path contains more than one weights file in the {file_extension} format. Either specify `weight_name` in `load_lora_weights` or make sure there's only one  `.safetensors` or `.bin` file in  {pretrained_model_name_or_path_or_dict}.")
    weight_name = targeted_files[0]
    return weight_name


def _fetch_state_dict(pretrained_model_name_or_path_or_dict, weight_name, use_safetensors, local_files_only, cache_dir, force_download, proxies, token, revision, subfolder, user_agent, allow_pickle):
    model_file = None
    if not isinstance(pretrained_model_name_or_path_or_dict, dict):
        if use_safetensors and weight_name is None or weight_name is not None and weight_name.endswith('.safetensors'):
            try:
                if weight_name is None:
                    weight_name = _best_guess_weight_name(pretrained_model_name_or_path_or_dict, file_extension='.safetensors', local_files_only=local_files_only)
                model_file = _get_model_file(pretrained_model_name_or_path_or_dict, weights_name=weight_name or LORA_WEIGHT_NAME_SAFE, cache_dir=cache_dir, force_download=force_download, proxies=proxies, local_files_only=local_files_only, token=token, revision=revision, subfolder=subfolder, user_agent=user_agent)
                state_dict = safetensors.torch.load_file(model_file, device='cpu')
            except (IOError, safetensors.SafetensorError) as e:
                if not allow_pickle:
                    raise e
                model_file = None
                pass
        if model_file is None:
            if weight_name is None:
                weight_name = _best_guess_weight_name(pretrained_model_name_or_path_or_dict, file_extension='.bin', local_files_only=local_files_only)
            model_file = _get_model_file(pretrained_model_name_or_path_or_dict, weights_name=weight_name or LORA_WEIGHT_NAME, cache_dir=cache_dir, force_download=force_download, proxies=proxies, local_files_only=local_files_only, token=token, revision=revision, subfolder=subfolder, user_agent=user_agent)
            state_dict = load_state_dict(model_file)
    else:
        state_dict = pretrained_model_name_or_path_or_dict
    return state_dict


def check_peft_version(min_version: 'str') ->None:
    """
    Checks if the version of PEFT is compatible.

    Args:
        version (`str`):
            The version of PEFT to check against.
    """
    if not is_peft_available():
        raise ValueError('PEFT is not installed. Please install it with `pip install peft`')
    is_peft_version_compatible = version.parse(importlib.metadata.version('peft')) > version.parse(min_version)
    if not is_peft_version_compatible:
        raise ValueError(f'The version of PEFT you are using is not compatible, please use a version that is greater than {min_version}')


UNET_TO_DIFFUSERS = {'.to_out_lora.up': '.to_out.0.lora_B', '.to_out_lora.down': '.to_out.0.lora_A', '.to_q_lora.down': '.to_q.lora_A', '.to_q_lora.up': '.to_q.lora_B', '.to_k_lora.down': '.to_k.lora_A', '.to_k_lora.up': '.to_k.lora_B', '.to_v_lora.down': '.to_v.lora_A', '.to_v_lora.up': '.to_v.lora_B', '.lora.up': '.lora_B', '.lora.down': '.lora_A', '.to_out.lora_magnitude_vector': '.to_out.0.lora_magnitude_vector'}


KEYS_TO_ALWAYS_REPLACE = {'.processor.': '.'}


def convert_state_dict(state_dict, mapping):
    """
    Simply iterates over the state dict and replaces the patterns in `mapping` with the corresponding values.

    Args:
        state_dict (`dict[str, torch.Tensor]`):
            The state dict to convert.
        mapping (`dict[str, str]`):
            The mapping to use for conversion, the mapping should be a dictionary with the following structure:
                - key: the pattern to replace
                - value: the pattern to replace with

    Returns:
        converted_state_dict (`dict`)
            The converted state dict.
    """
    converted_state_dict = {}
    for k, v in state_dict.items():
        for pattern in KEYS_TO_ALWAYS_REPLACE.keys():
            if pattern in k:
                new_pattern = KEYS_TO_ALWAYS_REPLACE[pattern]
                k = k.replace(pattern, new_pattern)
        for pattern in mapping.keys():
            if pattern in k:
                new_pattern = mapping[pattern]
                k = k.replace(pattern, new_pattern)
                break
        converted_state_dict[k] = v
    return converted_state_dict


def convert_unet_state_dict_to_peft(state_dict):
    """
    Converts a state dict from UNet format to diffusers format - i.e. by removing some keys
    """
    mapping = UNET_TO_DIFFUSERS
    return convert_state_dict(state_dict, mapping)


def delete_adapter_layers(model, adapter_name):
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            if hasattr(module, 'delete_adapter'):
                module.delete_adapter(adapter_name)
            else:
                raise ValueError('The version of PEFT you are using is not compatible, please use a version that is greater than 0.6.1')
    if getattr(model, '_hf_peft_config_loaded', False) and hasattr(model, 'peft_config'):
        model.peft_config.pop(adapter_name, None)
        if len(model.peft_config) == 0:
            del model.peft_config
            model._hf_peft_config_loaded = None


def get_adapter_name(model):
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            return f'default_{len(module.r)}'
    return 'default_0'


def get_peft_kwargs(rank_dict, network_alpha_dict, peft_state_dict, is_unet=True):
    rank_pattern = {}
    alpha_pattern = {}
    r = lora_alpha = list(rank_dict.values())[0]
    if len(set(rank_dict.values())) > 1:
        r = collections.Counter(rank_dict.values()).most_common()[0][0]
        rank_pattern = dict(filter(lambda x: x[1] != r, rank_dict.items()))
        rank_pattern = {k.split('.lora_B.')[0]: v for k, v in rank_pattern.items()}
    if network_alpha_dict is not None and len(network_alpha_dict) > 0:
        if len(set(network_alpha_dict.values())) > 1:
            lora_alpha = collections.Counter(network_alpha_dict.values()).most_common()[0][0]
            alpha_pattern = dict(filter(lambda x: x[1] != lora_alpha, network_alpha_dict.items()))
            if is_unet:
                alpha_pattern = {'.'.join(k.split('.lora_A.')[0].split('.')).replace('.alpha', ''): v for k, v in alpha_pattern.items()}
            else:
                alpha_pattern = {'.'.join(k.split('.down.')[0].split('.')[:-1]): v for k, v in alpha_pattern.items()}
        else:
            lora_alpha = set(network_alpha_dict.values()).pop()
    target_modules = list({name.split('.lora')[0] for name in peft_state_dict.keys()})
    use_dora = any('lora_magnitude_vector' in k for k in peft_state_dict)
    lora_config_kwargs = {'r': r, 'lora_alpha': lora_alpha, 'rank_pattern': rank_pattern, 'alpha_pattern': alpha_pattern, 'target_modules': target_modules, 'use_dora': use_dora}
    return lora_config_kwargs


def is_peft_version(operation: 'str', version: 'str'):
    """
    Compares the current PEFT version to a given reference with an operation.

    Args:
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`
        version (`str`):
            A version string
    """
    if not _peft_version:
        return False
    return compare_versions(parse(_peft_version), operation, version)


def recurse_remove_peft_layers(model):
    """
    Recursively replace all instances of `LoraLayer` with corresponding new layers in `model`.
    """
    has_base_layer_pattern = False
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            has_base_layer_pattern = hasattr(module, 'base_layer')
            break
    if has_base_layer_pattern:
        key_list = [key for key, _ in model.named_modules() if 'lora' not in key]
        for key in key_list:
            try:
                parent, target, target_name = _get_submodules(model, key)
            except AttributeError:
                continue
            if hasattr(target, 'base_layer'):
                setattr(parent, target_name, target.get_base_layer())
    else:
        for name, module in model.named_children():
            if len(list(module.children())) > 0:
                recurse_remove_peft_layers(module)
            module_replaced = False
            if isinstance(module, LoraLayer) and isinstance(module, torch.nn.Linear):
                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=module.bias is not None)
                new_module.weight = module.weight
                if module.bias is not None:
                    new_module.bias = module.bias
                module_replaced = True
            elif isinstance(module, LoraLayer) and isinstance(module, torch.nn.Conv2d):
                new_module = torch.nn.Conv2d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups)
                new_module.weight = module.weight
                if module.bias is not None:
                    new_module.bias = module.bias
                module_replaced = True
            if module_replaced:
                setattr(model, name, new_module)
                del module
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    return model


def set_adapter_layers(model, enabled=True):
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            if hasattr(module, 'enable_adapters'):
                module.enable_adapters(enabled=enabled)
            else:
                module.disable_adapters = not enabled


def set_weights_and_activate_adapters(model, adapter_names, weights):

    def get_module_weight(weight_for_adapter, module_name):
        if not isinstance(weight_for_adapter, dict):
            return weight_for_adapter
        for layer_name, weight_ in weight_for_adapter.items():
            if layer_name in module_name:
                return weight_
        parts = module_name.split('.')
        key = f'{parts[0]}.{parts[1]}.attentions.{parts[3]}'
        block_weight = weight_for_adapter.get(key, 1.0)
        return block_weight
    for adapter_name, weight in zip(adapter_names, weights):
        for module_name, module in model.named_modules():
            if isinstance(module, BaseTunerLayer):
                if hasattr(module, 'set_adapter'):
                    module.set_adapter(adapter_name)
                else:
                    module.active_adapter = adapter_name
                module.set_scale(adapter_name, get_module_weight(weight, module_name))
    for module in model.modules():
        if isinstance(module, BaseTunerLayer):
            if hasattr(module, 'set_adapter'):
                module.set_adapter(adapter_names)
            else:
                module.active_adapter = adapter_names


class PeftAdapterMixin:
    """
    A class containing all functions for loading and using adapters weights that are supported in PEFT library. For
    more details about adapters and injecting them in a base model, check out the PEFT
    [documentation](https://huggingface.co/docs/peft/index).

    Install the latest version of PEFT, and use this mixin to:

    - Attach new adapters in the model.
    - Attach multiple adapters and iteratively activate/deactivate them.
    - Activate/deactivate all adapters from the model.
    - Get a list of the active adapters.
    """
    _hf_peft_config_loaded = False

    @classmethod
    def _optionally_disable_offloading(cls, _pipeline):
        """
        Optionally removes offloading in case the pipeline has been already sequentially offloaded to CPU.

        Args:
            _pipeline (`DiffusionPipeline`):
                The pipeline to disable offloading for.

        Returns:
            tuple:
                A tuple indicating if `is_model_cpu_offload` or `is_sequential_cpu_offload` is True.
        """
        is_model_cpu_offload = False
        is_sequential_cpu_offload = False
        if _pipeline is not None and _pipeline.hf_device_map is None:
            for _, component in _pipeline.components.items():
                if isinstance(component, nn.Module) and hasattr(component, '_hf_hook'):
                    if not is_model_cpu_offload:
                        is_model_cpu_offload = isinstance(component._hf_hook, CpuOffload)
                    if not is_sequential_cpu_offload:
                        is_sequential_cpu_offload = isinstance(component._hf_hook, AlignDevicesHook) or hasattr(component._hf_hook, 'hooks') and isinstance(component._hf_hook.hooks[0], AlignDevicesHook)
                    logger.info('Accelerate hooks detected. Since you have called `load_lora_weights()`, the previous hooks will be first removed. Then the LoRA parameters will be loaded and the hooks will be applied again.')
                    remove_hook_from_module(component, recurse=is_sequential_cpu_offload)
        return is_model_cpu_offload, is_sequential_cpu_offload

    def load_lora_adapter(self, pretrained_model_name_or_path_or_dict, prefix='transformer', **kwargs):
        """
        Loads a LoRA adapter into the underlying model.

        Parameters:
            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):
                Can be either:

                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
                      the Hub.
                    - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved
                      with [`ModelMixin.save_pretrained`].
                    - A [torch state
                      dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).

            prefix (`str`, *optional*): Prefix to filter the state dict.

            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
                is not used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            proxies (`Dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            local_files_only (`bool`, *optional*, defaults to `False`):
                Whether to only load local model weights and configuration files or not. If set to `True`, the model
                won't be downloaded from the Hub.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from
                `diffusers-cli login` (stored in `~/.huggingface`) is used.
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
                allowed by Git.
            subfolder (`str`, *optional*, defaults to `""`):
                The subfolder location of a model file within a larger model repository on the Hub or locally.
            network_alphas (`Dict[str, float]`):
                The value of the network alpha used for stable learning and preventing underflow. This value has the
                same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this
                link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).
            low_cpu_mem_usage (`bool`, *optional*):
                Speed up model loading by only loading the pretrained LoRA weights and not initializing the random
                weights.
        """
        cache_dir = kwargs.pop('cache_dir', None)
        force_download = kwargs.pop('force_download', False)
        proxies = kwargs.pop('proxies', None)
        local_files_only = kwargs.pop('local_files_only', None)
        token = kwargs.pop('token', None)
        revision = kwargs.pop('revision', None)
        subfolder = kwargs.pop('subfolder', None)
        weight_name = kwargs.pop('weight_name', None)
        use_safetensors = kwargs.pop('use_safetensors', None)
        adapter_name = kwargs.pop('adapter_name', None)
        network_alphas = kwargs.pop('network_alphas', None)
        _pipeline = kwargs.pop('_pipeline', None)
        low_cpu_mem_usage = kwargs.pop('low_cpu_mem_usage', False)
        allow_pickle = False
        if low_cpu_mem_usage and is_peft_version('<=', '0.13.0'):
            raise ValueError('`low_cpu_mem_usage=True` is not compatible with this `peft` version. Please update it with `pip install -U peft`.')
        user_agent = {'file_type': 'attn_procs_weights', 'framework': 'pytorch'}
        state_dict = _fetch_state_dict(pretrained_model_name_or_path_or_dict=pretrained_model_name_or_path_or_dict, weight_name=weight_name, use_safetensors=use_safetensors, local_files_only=local_files_only, cache_dir=cache_dir, force_download=force_download, proxies=proxies, token=token, revision=revision, subfolder=subfolder, user_agent=user_agent, allow_pickle=allow_pickle)
        keys = list(state_dict.keys())
        transformer_keys = [k for k in keys if k.startswith(prefix)]
        if len(transformer_keys) > 0:
            state_dict = {k.replace(f'{prefix}.', ''): v for k, v in state_dict.items() if k in transformer_keys}
        if len(state_dict.keys()) > 0:
            first_key = next(iter(state_dict.keys()))
            if 'lora_A' not in first_key:
                state_dict = convert_unet_state_dict_to_peft(state_dict)
            if adapter_name in getattr(self, 'peft_config', {}):
                raise ValueError(f'Adapter name {adapter_name} already in use in the transformer - please select a new adapter name.')
            rank = {}
            for key, val in state_dict.items():
                if 'lora_B' in key:
                    rank[key] = val.shape[1]
            if network_alphas is not None and len(network_alphas) >= 1:
                alpha_keys = [k for k in network_alphas.keys() if k.startswith(prefix) and k.split('.')[0] == prefix]
                network_alphas = {k.replace(f'{prefix}.', ''): v for k, v in network_alphas.items() if k in alpha_keys}
            lora_config_kwargs = get_peft_kwargs(rank, network_alpha_dict=network_alphas, peft_state_dict=state_dict)
            if 'use_dora' in lora_config_kwargs:
                if lora_config_kwargs['use_dora'] and is_peft_version('<', '0.9.0'):
                    raise ValueError('You need `peft` 0.9.0 at least to use DoRA-enabled LoRAs. Please upgrade your installation of `peft`.')
                else:
                    lora_config_kwargs.pop('use_dora')
            lora_config = LoraConfig(**lora_config_kwargs)
            if adapter_name is None:
                adapter_name = get_adapter_name(self)
            is_model_cpu_offload, is_sequential_cpu_offload = self._optionally_disable_offloading(_pipeline)
            peft_kwargs = {}
            if is_peft_version('>=', '0.13.1'):
                peft_kwargs['low_cpu_mem_usage'] = low_cpu_mem_usage
            inject_adapter_in_model(lora_config, self, adapter_name=adapter_name, **peft_kwargs)
            incompatible_keys = set_peft_model_state_dict(self, state_dict, adapter_name, **peft_kwargs)
            warn_msg = ''
            if incompatible_keys is not None:
                unexpected_keys = getattr(incompatible_keys, 'unexpected_keys', None)
                if unexpected_keys:
                    lora_unexpected_keys = [k for k in unexpected_keys if 'lora_' in k and adapter_name in k]
                    if lora_unexpected_keys:
                        warn_msg = f"Loading adapter weights from state_dict led to unexpected keys found in the model: {', '.join(lora_unexpected_keys)}. "
                missing_keys = getattr(incompatible_keys, 'missing_keys', None)
                if missing_keys:
                    lora_missing_keys = [k for k in missing_keys if 'lora_' in k and adapter_name in k]
                    if lora_missing_keys:
                        warn_msg += f"Loading adapter weights from state_dict led to missing keys in the model: {', '.join(lora_missing_keys)}."
            if warn_msg:
                logger.warning(warn_msg)
            if is_model_cpu_offload:
                _pipeline.enable_model_cpu_offload()
            elif is_sequential_cpu_offload:
                _pipeline.enable_sequential_cpu_offload()

    def set_adapters(self, adapter_names: 'Union[List[str], str]', weights: 'Optional[Union[float, Dict, List[float], List[Dict], List[None]]]'=None):
        """
        Set the currently active adapters for use in the UNet.

        Args:
            adapter_names (`List[str]` or `str`):
                The names of the adapters to use.
            adapter_weights (`Union[List[float], float]`, *optional*):
                The adapter(s) weights to use with the UNet. If `None`, the weights are set to `1.0` for all the
                adapters.

        Example:

        ```py
        from diffusers import AutoPipelineForText2Image
        import torch

        pipeline = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        pipeline.load_lora_weights(
            "jbilcke-hf/sdxl-cinematic-1", weight_name="pytorch_lora_weights.safetensors", adapter_name="cinematic"
        )
        pipeline.load_lora_weights("nerijs/pixel-art-xl", weight_name="pixel-art-xl.safetensors", adapter_name="pixel")
        pipeline.set_adapters(["cinematic", "pixel"], adapter_weights=[0.5, 0.5])
        ```
        """
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for `set_adapters()`.')
        adapter_names = [adapter_names] if isinstance(adapter_names, str) else adapter_names
        if not isinstance(weights, list):
            weights = [weights] * len(adapter_names)
        if len(adapter_names) != len(weights):
            raise ValueError(f'Length of adapter names {len(adapter_names)} is not equal to the length of their weights {len(weights)}.')
        weights = [(w if w is not None else 1.0) for w in weights]
        scale_expansion_fn = _SET_ADAPTER_SCALE_FN_MAPPING[self.__class__.__name__]
        weights = scale_expansion_fn(self, weights)
        set_weights_and_activate_adapters(self, adapter_names, weights)

    def add_adapter(self, adapter_config, adapter_name: 'str'='default') ->None:
        """
        Adds a new adapter to the current model for training. If no adapter name is passed, a default name is assigned
        to the adapter to follow the convention of the PEFT library.

        If you are not familiar with adapters and PEFT methods, we invite you to read more about them in the PEFT
        [documentation](https://huggingface.co/docs/peft).

        Args:
            adapter_config (`[~peft.PeftConfig]`):
                The configuration of the adapter to add; supported adapters are non-prefix tuning and adaption prompt
                methods.
            adapter_name (`str`, *optional*, defaults to `"default"`):
                The name of the adapter to add. If no name is passed, a default name is assigned to the adapter.
        """
        check_peft_version(min_version=MIN_PEFT_VERSION)
        if not is_peft_available():
            raise ImportError('PEFT is not available. Please install PEFT to use this function: `pip install peft`.')
        if not self._hf_peft_config_loaded:
            self._hf_peft_config_loaded = True
        elif adapter_name in self.peft_config:
            raise ValueError(f'Adapter with name {adapter_name} already exists. Please use a different name.')
        if not isinstance(adapter_config, PeftConfig):
            raise ValueError(f'adapter_config should be an instance of PeftConfig. Got {type(adapter_config)} instead.')
        adapter_config.base_model_name_or_path = None
        inject_adapter_in_model(adapter_config, self, adapter_name)
        self.set_adapter(adapter_name)

    def set_adapter(self, adapter_name: 'Union[str, List[str]]') ->None:
        """
        Sets a specific adapter by forcing the model to only use that adapter and disables the other adapters.

        If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
        [documentation](https://huggingface.co/docs/peft).

        Args:
            adapter_name (Union[str, List[str]])):
                The list of adapters to set or the adapter name in the case of a single adapter.
        """
        check_peft_version(min_version=MIN_PEFT_VERSION)
        if not self._hf_peft_config_loaded:
            raise ValueError('No adapter loaded. Please load an adapter first.')
        if isinstance(adapter_name, str):
            adapter_name = [adapter_name]
        missing = set(adapter_name) - set(self.peft_config)
        if len(missing) > 0:
            raise ValueError(f"Following adapter(s) could not be found: {', '.join(missing)}. Make sure you are passing the correct adapter name(s). current loaded adapters are: {list(self.peft_config.keys())}")
        _adapters_has_been_set = False
        for _, module in self.named_modules():
            if isinstance(module, BaseTunerLayer):
                if hasattr(module, 'set_adapter'):
                    module.set_adapter(adapter_name)
                elif not hasattr(module, 'set_adapter') and len(adapter_name) != 1:
                    raise ValueError('You are trying to set multiple adapters and you have a PEFT version that does not support multi-adapter inference. Please upgrade to the latest version of PEFT. `pip install -U peft` or `pip install -U git+https://github.com/huggingface/peft.git`')
                else:
                    module.active_adapter = adapter_name
                _adapters_has_been_set = True
        if not _adapters_has_been_set:
            raise ValueError('Did not succeeded in setting the adapter. Please make sure you are using a model that supports adapters.')

    def disable_adapters(self) ->None:
        """
        Disable all adapters attached to the model and fallback to inference with the base model only.

        If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
        [documentation](https://huggingface.co/docs/peft).
        """
        check_peft_version(min_version=MIN_PEFT_VERSION)
        if not self._hf_peft_config_loaded:
            raise ValueError('No adapter loaded. Please load an adapter first.')
        for _, module in self.named_modules():
            if isinstance(module, BaseTunerLayer):
                if hasattr(module, 'enable_adapters'):
                    module.enable_adapters(enabled=False)
                else:
                    module.disable_adapters = True

    def enable_adapters(self) ->None:
        """
        Enable adapters that are attached to the model. The model uses `self.active_adapters()` to retrieve the list of
        adapters to enable.

        If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
        [documentation](https://huggingface.co/docs/peft).
        """
        check_peft_version(min_version=MIN_PEFT_VERSION)
        if not self._hf_peft_config_loaded:
            raise ValueError('No adapter loaded. Please load an adapter first.')
        for _, module in self.named_modules():
            if isinstance(module, BaseTunerLayer):
                if hasattr(module, 'enable_adapters'):
                    module.enable_adapters(enabled=True)
                else:
                    module.disable_adapters = False

    def active_adapters(self) ->List[str]:
        """
        Gets the current list of active adapters of the model.

        If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT
        [documentation](https://huggingface.co/docs/peft).
        """
        check_peft_version(min_version=MIN_PEFT_VERSION)
        if not is_peft_available():
            raise ImportError('PEFT is not available. Please install PEFT to use this function: `pip install peft`.')
        if not self._hf_peft_config_loaded:
            raise ValueError('No adapter loaded. Please load an adapter first.')
        for _, module in self.named_modules():
            if isinstance(module, BaseTunerLayer):
                return module.active_adapter

    def fuse_lora(self, lora_scale=1.0, safe_fusing=False, adapter_names=None):
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for `fuse_lora()`.')
        self.lora_scale = lora_scale
        self._safe_fusing = safe_fusing
        self.apply(partial(self._fuse_lora_apply, adapter_names=adapter_names))

    def _fuse_lora_apply(self, module, adapter_names=None):
        merge_kwargs = {'safe_merge': self._safe_fusing}
        if isinstance(module, BaseTunerLayer):
            if self.lora_scale != 1.0:
                module.scale_layer(self.lora_scale)
            supported_merge_kwargs = list(inspect.signature(module.merge).parameters)
            if 'adapter_names' in supported_merge_kwargs:
                merge_kwargs['adapter_names'] = adapter_names
            elif 'adapter_names' not in supported_merge_kwargs and adapter_names is not None:
                raise ValueError('The `adapter_names` argument is not supported with your PEFT version. Please upgrade to the latest version of PEFT. `pip install -U peft`')
            module.merge(**merge_kwargs)

    def unfuse_lora(self):
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for `unfuse_lora()`.')
        self.apply(self._unfuse_lora_apply)

    def _unfuse_lora_apply(self, module):
        if isinstance(module, BaseTunerLayer):
            module.unmerge()

    def unload_lora(self):
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for `unload_lora()`.')
        recurse_remove_peft_layers(self)
        if hasattr(self, 'peft_config'):
            del self.peft_config

    def disable_lora(self):
        """
        Disables the active LoRA layers of the underlying model.

        Example:

        ```py
        from diffusers import AutoPipelineForText2Image
        import torch

        pipeline = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        pipeline.load_lora_weights(
            "jbilcke-hf/sdxl-cinematic-1", weight_name="pytorch_lora_weights.safetensors", adapter_name="cinematic"
        )
        pipeline.disable_lora()
        ```
        """
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for this method.')
        set_adapter_layers(self, enabled=False)

    def enable_lora(self):
        """
        Enables the active LoRA layers of the underlying model.

        Example:

        ```py
        from diffusers import AutoPipelineForText2Image
        import torch

        pipeline = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        pipeline.load_lora_weights(
            "jbilcke-hf/sdxl-cinematic-1", weight_name="pytorch_lora_weights.safetensors", adapter_name="cinematic"
        )
        pipeline.enable_lora()
        ```
        """
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for this method.')
        set_adapter_layers(self, enabled=True)

    def delete_adapters(self, adapter_names: 'Union[List[str], str]'):
        """
        Delete an adapter's LoRA layers from the underlying model.

        Args:
            adapter_names (`Union[List[str], str]`):
                The names (single string or list of strings) of the adapter to delete.

        Example:

        ```py
        from diffusers import AutoPipelineForText2Image
        import torch

        pipeline = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16
        ).to("cuda")
        pipeline.load_lora_weights(
            "jbilcke-hf/sdxl-cinematic-1", weight_name="pytorch_lora_weights.safetensors", adapter_names="cinematic"
        )
        pipeline.delete_adapters("cinematic")
        ```
        """
        if not USE_PEFT_BACKEND:
            raise ValueError('PEFT backend is required for this method.')
        if isinstance(adapter_names, str):
            adapter_names = [adapter_names]
        for adapter_name in adapter_names:
            delete_adapter_layers(self, adapter_name)
            if hasattr(self, 'peft_config'):
                self.peft_config.pop(adapter_name, None)


CUSTOM_DIFFUSION_WEIGHT_NAME = 'pytorch_custom_diffusion_weights.bin'


CUSTOM_DIFFUSION_WEIGHT_NAME_SAFE = 'pytorch_custom_diffusion_weights.safetensors'


TEXT_ENCODER_NAME = 'text_encoder'


UNET_NAME = 'unet'


import torch
from torch.nn import MSELoss, ReLU
from types import SimpleNamespace


TESTCASES = [
    # (nn.Module, init_args, forward_args)
    (AdaGroupNorm,
     lambda: ([], {'embedding_dim': 4, 'out_dim': 4, 'num_groups': 1}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (AdaLayerNormContinuous,
     lambda: ([], {'embedding_dim': 4, 'conditioning_embedding_dim': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (AdaLayerNormShift,
     lambda: ([], {'embedding_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (AdapterBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'num_res_blocks': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (AdapterResnetBlock,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ApproximateGELU,
     lambda: ([], {'dim_in': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (AttentionPooling,
     lambda: ([], {'num_heads': 4, 'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (AuraFlowFeedForward,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (AuraFlowPreFinalBlock,
     lambda: ([], {'embedding_dim': 4, 'conditioning_embedding_dim': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (ChannelsProj,
     lambda: ([], {'vectors': 4, 'channels': 4, 'd_latent': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (CogVideoXDownsample3D,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {})),
    (CogVideoXLayerNormZero,
     lambda: ([], {'conditioning_dim': 4, 'embedding_dim': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (CogVideoXUpsample3D,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {})),
    (ControlNetConditioningEmbedding,
     lambda: ([], {'conditioning_embedding_channels': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {})),
    (DownBlock1D,
     lambda: ([], {'out_channels': 4, 'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (Downsample1D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (Downsample1d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (Downsample2D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (FP32LayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (FP32SiLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (FirDownsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (FirUpsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (FourierFeatures,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {})),
    (GELU,
     lambda: ([], {'dim_in': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GaussianSmoothing,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {})),
    (GlobalResponseNorm,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (HunyuanDiTAttentionPool,
     lambda: ([], {'spacial_dim': 4, 'embed_dim': 4, 'num_heads': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (ImageEmbedding,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 7, 64, 64])], {})),
    (KDownsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (KUpsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Kandinsky3ConditionalGroupNorm,
     lambda: ([], {'groups': 1, 'normalized_shape': 4, 'context_dim': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (Kandinsky3EncoderProj,
     lambda: ([], {'encoder_hid_dim': 4, 'cross_attention_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (LDMBertAttention,
     lambda: ([], {'embed_dim': 4, 'num_heads': 4, 'head_dim': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (LightAdapterBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'num_res_blocks': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (LightAdapterResnetBlock,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (LinearMultiDim,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([16])], {})),
    (LpNorm,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (LuminaFeedForward,
     lambda: ([], {'dim': 4, 'inner_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (LuminaLayerNormContinuous,
     lambda: ([], {'embedding_dim': 4, 'conditioning_embedding_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (LuminaPatchEmbed,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (MLP,
     lambda: ([], {'config': SimpleNamespace(add_bias_linear=4, hidden_size=4, ffn_hidden_size=4, torch_dtype=torch.float32)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (MaskConditionEncoder,
     lambda: ([], {'in_ch': 4}),
     lambda: ([torch.rand([4, 4, 64, 64])], {})),
    (MixingResidualBlock,
     lambda: ([], {'inp_channels': 4, 'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (MochiAttentionPool,
     lambda: ([], {'num_attention_heads': 4, 'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4])], {})),
    (MochiRMSNormZero,
     lambda: ([], {'embedding_dim': 4, 'hidden_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (NewGELUActivation,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ObservationEncoder,
     lambda: ([], {'state_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (OobleckDecoderBlock,
     lambda: ([], {'input_dim': 4, 'output_dim': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (OobleckEncoderBlock,
     lambda: ([], {'input_dim': 4, 'output_dim': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (OobleckResidualUnit,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 16, 4])], {})),
    (OutValueFunctionBlock,
     lambda: ([], {'fc_dim': 4, 'embed_dim': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (PaintByExampleMapper,
     lambda: ([], {'config': SimpleNamespace(num_hidden_layers=1, hidden_size=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (PerceiverAttention,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {})),
    (PixArtAlphaTextProjection,
     lambda: ([], {'in_features': 4, 'hidden_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (RMSNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ResBlock,
     lambda: ([], {'c': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ResBlockStageB,
     lambda: ([], {'c': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ResConvBlock,
     lambda: ([], {'in_channels': 4, 'mid_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (SDCascadeLayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (SDCascadeResBlock,
     lambda: ([], {'c': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (SDCascadeTimestepBlock,
     lambda: ([], {'c': 4, 'c_timestep': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (SelfAttention1d,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (SinusoidalPositionalEmbedding,
     lambda: ([], {'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (SkipBlock,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (Snake1d,
     lambda: ([], {'hidden_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (SparseControlNetConditioningEmbedding,
     lambda: ([], {'conditioning_embedding_channels': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {})),
    (StableAudioNumberConditioner,
     lambda: ([], {'number_embedding_dim': 4, 'min_value': 4, 'max_value': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (StableAudioPositionalEmbedding,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (SwiGLU,
     lambda: ([], {'dim_in': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (T5DenseGatedActDense,
     lambda: ([], {'d_model': 4, 'd_ff': 4, 'dropout_rate': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (T5FiLMLayer,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (T5LayerFFCond,
     lambda: ([], {'d_model': 4, 'd_ff': 4, 'dropout_rate': 0.5, 'layer_norm_epsilon': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (T5LayerNorm,
     lambda: ([], {'hidden_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TimestepBlock,
     lambda: ([], {'c': 4, 'c_timestep': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (TimestepEmbedding,
     lambda: ([], {'in_channels': 4, 'time_embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TranslatorBase,
     lambda: ([], {'num_tok': 4, 'dim': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TranslatorBaseNoLN,
     lambda: ([], {'num_tok': 4, 'dim': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TranslatorNoLN,
     lambda: ([], {'num_tok': 4, 'dim': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (UpBlock1D,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (UpBlock1DNoSkip,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (UpSample,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Upsample1D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Upsample1d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (Upsample2D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (VectorQuantizer,
     lambda: ([], {'n_e': 4, 'vq_embed_dim': 4, 'beta': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (VoidNeRFModel,
     lambda: ([], {'background': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (WuerstchenLayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
]


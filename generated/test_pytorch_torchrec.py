import sys
_module = sys.modules[__name__]
del sys
ebc_benchmarks = _module
ebc_benchmarks_utils = _module
setup = _module
src = _module
torchrec_dynamic_embedding = _module
dataloader = _module
distributed = _module
comm = _module
id_transformer = _module
id_transformer_collection = _module
id_transformer_group = _module
ps = _module
tensor_list = _module
utils = _module
tests = _module
test_id_transformer = _module
test_id_transformer_collection = _module
test_integral_precision = _module
test_ps = _module
test_ps_collection = _module
utils = _module
conf = _module
examples = _module
bert4rec_main = _module
bert4rec_metrics = _module
bert4rec_movielens_datasets = _module
test_bert4rec_movielens_datasets = _module
bert4rec_movielens_dataloader = _module
test_bert4rec_movielens_dataloader = _module
bert4rec = _module
test_bert4rec = _module
test_bert4rec_main = _module
golden_training = _module
test_train_dlrm = _module
train_dlrm = _module
dlrm_client = _module
dlrm_packager = _module
dlrm_predict = _module
dlrm_predict_single_gpu = _module
aws_component = _module
nvt_binary_dataloader = _module
train_torchrec = _module
ray = _module
compute_world_size = _module
train_torchrec = _module
retrieval = _module
data = _module
dataloader = _module
knn_index = _module
modules = _module
test_two_tower = _module
two_tower = _module
test_two_tower_retrieval = _module
test_two_tower_train = _module
two_tower_retrieval = _module
two_tower_train = _module
torcharrow = _module
dataloader = _module
run = _module
transfer_learning = _module
train_from_pretrained_embedding = _module
test_installation = _module
black_linter = _module
pip_init = _module
usort_linter = _module
torchrec = _module
datasets = _module
criteo = _module
movielens = _module
random = _module
scripts = _module
contiguous_preproc_criteo = _module
npy_preproc_criteo = _module
convert_parquet_to_binary = _module
convert_tsv_to_parquet = _module
process_criteo_parquet = _module
split_binary_dataset = _module
criteo_constant = _module
dask = _module
shuffle_preproc_criteo = _module
test_npy_preproc_criteo = _module
test_utils = _module
criteo_test_utils = _module
test_criteo = _module
test_movielens = _module
test_random = _module
test_utils = _module
utils = _module
batched_embedding_kernel = _module
collective_utils = _module
comm = _module
comm_ops = _module
composable = _module
table_batched_embedding_slice = _module
test_ddp = _module
test_embedding = _module
test_embeddingbag = _module
test_fsdp = _module
test_fused_optim = _module
test_table_batched_embedding_slice = _module
dist_data = _module
embedding = _module
embedding_kernel = _module
embedding_lookup = _module
embedding_sharding = _module
embedding_tower_sharding = _module
embedding_types = _module
embeddingbag = _module
fbgemm_qcomm_codec = _module
fused_embedding = _module
fused_embeddingbag = _module
grouped_position_weighted = _module
model_parallel = _module
planner = _module
sparsenn_planner_model = _module
constants = _module
enumerators = _module
parallelized_planners = _module
partitioners = _module
perf_models = _module
planners = _module
proposers = _module
shard_estimators = _module
stats = _module
storage_reservations = _module
test_enumerators = _module
test_parallelized_planners = _module
test_partitioners = _module
test_planners = _module
test_proposers = _module
test_shard_estimators = _module
test_storage_reservations = _module
types = _module
utils = _module
quant_embedding = _module
quant_embedding_kernel = _module
quant_embeddingbag = _module
shard = _module
sharding = _module
cw_sequence_sharding = _module
cw_sharding = _module
dp_sequence_sharding = _module
dp_sharding = _module
rw_sequence_sharding = _module
rw_sharding = _module
sequence_sharding = _module
tw_sequence_sharding = _module
tw_sharding = _module
twcw_sharding = _module
twrw_sharding = _module
sharding_plan = _module
multi_process = _module
test_model = _module
test_model_parallel = _module
test_model_parallel_base = _module
test_sharding = _module
collective_utils_test = _module
test_apply_optim_per_param = _module
test_awaitable = _module
test_comm = _module
test_dist_data = _module
test_fbgemm_qcomm_codec = _module
test_fused_embedding_bag_collection = _module
test_fused_embedding_collection = _module
test_lazy_awaitable = _module
test_model_parallel_gloo = _module
test_model_parallel_hierarchical = _module
test_model_parallel_nccl = _module
test_model_parallel_nccl_single_rank = _module
test_qcomms_embedding_modules = _module
test_quant_model_parallel = _module
test_quant_sequence_model_parallel = _module
test_quantize = _module
test_sequence_model = _module
test_sequence_model_parallel = _module
test_sequence_model_parallel_hierarchical = _module
test_sharding_plan = _module
test_train_pipeline = _module
test_utils = _module
train_pipeline = _module
types = _module
utils = _module
fx = _module
test_tracer = _module
tracer = _module
inference = _module
client = _module
model_packager = _module
modules = _module
state_dict_transform = _module
generate_test_packages = _module
model_packager_tests = _module
predict_module_tests = _module
test_modules = _module
module_linter = _module
test_module_linter = _module
metrics = _module
auc = _module
calibration = _module
ctr = _module
mae = _module
metric_module = _module
metrics_config = _module
metrics_namespace = _module
model_utils = _module
mse = _module
multiclass_recall = _module
ne = _module
rec_metric = _module
test_utils = _module
test_auc = _module
test_calibration = _module
test_ctr = _module
test_gpu = _module
test_mae = _module
test_metric_module = _module
test_metrics_namespace = _module
test_mse = _module
test_multiclass_recall = _module
test_ne = _module
test_recmetric = _module
test_throughput = _module
throughput = _module
models = _module
deepfm = _module
dlrm = _module
experimental = _module
test_transformerdlrm = _module
transformerdlrm = _module
test_deepfm = _module
test_dlrm = _module
activation = _module
crossnet = _module
deepfm = _module
embedding_configs = _module
embedding_modules = _module
embedding_tower = _module
feature_processor = _module
fused_embedding_modules = _module
lazy_extension = _module
mlp = _module
test_activation = _module
test_code_quality = _module
test_crossnet = _module
test_deepfm = _module
test_embedding_modules = _module
test_feature_processor = _module
test_fused_embedding_modules = _module
test_lazy_extension = _module
test_mlp = _module
utils = _module
optim = _module
apply_optimizer_in_backward = _module
clipping = _module
fused = _module
keyed = _module
optimizers = _module
rowwise_adagrad = _module
test_apply_optimizer_in_backward = _module
test_clipping = _module
test_keyed = _module
test_optim = _module
test_rowwise_adagrad = _module
test_warmup = _module
warmup = _module
quant = _module
embedding_modules = _module
test_embedding_modules = _module
test_quant_utils = _module
utils = _module
sparse = _module
jagged_tensor = _module
test_utils = _module
test_jagged_tensor = _module
streamable = _module
test_utils = _module
types = _module
version = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import List


from typing import Tuple


import torch


import time


from typing import Dict


from typing import Optional


import numpy as np


from torch.utils.data.dataset import IterableDataset


import queue


from typing import Union


from torch.utils.data._utils import MP_STATUS_CHECK_INTERVAL


import torch.distributed as dist


from time import time


from typing import Callable


import torch.nn as nn


from torch.distributed._shard.sharded_tensor import ShardedTensor


from typing import Any


from typing import cast


import torch.optim as optim


import torch.utils.data as data_utils


from torch import distributed as dist


from torch.nn.parallel import DistributedDataParallel as DDP


import pandas as pd


from torch.utils.data.distributed import DistributedSampler


import copy


import math


import uuid


from torch.distributed.launcher.api import elastic_launch


from torch.distributed.launcher.api import LaunchConfig


from torch.distributed.elastic.multiprocessing.errors import record


from torch.utils.data import IterableDataset


import logging


from torch.utils.data import DataLoader


from torch.package import PackageExporter


from torch.utils import data as data_utils


from torch.utils.data import Dataset


from typing import Iterator


import torch.nn.functional as F


from torch.distributed import all_reduce


from torch.distributed import get_rank


from torch.distributed import get_world_size


from torch.distributed import init_process_group


from torch import nn


from collections import OrderedDict


from typing import Mapping


from typing import OrderedDict


import torch.distributed.launcher as pet


from torch.multiprocessing.reductions import reduce_storage


from torch.multiprocessing.reductions import reduce_typed_storage


from torch.multiprocessing.reductions import reduce_typed_storage_child


from typing import Iterable


import torch.utils.data.datapipes as dp


from torch.utils.data import IterDataPipe


import random


from functools import partial


from typing import Sequence


from typing import TypeVar


from torch.utils.data import functional_datapipe


from torch.utils.data import get_worker_info


import abc


import itertools


from functools import wraps


from torch import Tensor


from torch.autograd import Function


from torch.autograd.profiler import record_function


from torch.distributed._composable import replicate


from torch.distributed._shard.api import ShardedTensor


from torch.distributed.checkpoint import FileSystemReader


from torch.distributed.checkpoint import FileSystemWriter


from torch.distributed.checkpoint import load_state_dict


from torch.distributed.checkpoint import save_state_dict


from torch.distributed._composable import fully_shard


from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel


from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType


from torch.distributed.fsdp.wrap import ModuleWrapPolicy


from collections import defaultdict


from collections import deque


from typing import MutableMapping


from typing import Type


from torch.nn.parallel import DistributedDataParallel


from abc import ABC


from torch.nn.modules.module import _IncompatibleKeys


from typing import Generic


from typing import Set


from enum import Enum


from enum import unique


from torch import fx


from torch.nn.modules.module import _addindent


from torch.distributed.fsdp import FullyShardedDataParallel


from functools import reduce


from time import perf_counter


import numpy


from copy import deepcopy


from torch.distributed._composable.contract import contract


from torch._utils_internal import TEST_MASTER_ADDR as MASTER_ADDR


from typing import Generator


from numpy.testing import assert_array_equal


import torch.fx


from torch import quantization as quant


import torch.quantization as quant


from torch import optim


from torch.fx.node import Node


from torch.distributed._shard.sharded_tensor import Shard


from torch.distributed._shard.sharded_tensor import ShardedTensorMetadata


from torch.distributed._shard.sharded_tensor import TensorProperties


from torch.distributed._shard.sharding_spec import EnumerableShardingSpec


from torch.distributed._shard.sharding_spec import ShardingSpec


from torch.distributed._shard.sharding_spec import ShardMetadata


from torch.testing import FileCheck


from torch.fx._compatibility import compatibility


from torch.fx.graph import Graph


from torch.fx.node import Argument


from typing import BinaryIO


from torch.distributed import ProcessGroup


from torch.package import PackageImporter


from typing import Deque


from functools import update_wrapper


from torch.fx import wrap


from math import sqrt


import functools


import inspect


import torch.utils.hooks as hooks


from torch.nn.modules.lazy import _LazyProtocol


from torch.nn.modules.lazy import LazyModuleMixin


from torch.nn.modules.module import _global_backward_hooks


from torch.nn.modules.module import _global_forward_hooks


from torch.nn.modules.module import _global_forward_pre_hooks


from torch.fx import GraphModule


from torch.fx import Tracer


import re


from typing import Collection


from torch.optim.optimizer import Optimizer


from torch.autograd import Variable


from torch.distributed._shard import sharded_tensor


from torch.distributed._shard import sharding_spec


from abc import abstractmethod


@unique
class DataType(Enum):
    """
    Our fusion implementation supports only certain types of data
    so it makes sense to retrict in a non-fused version as well.
    """
    FP32 = 'FP32'
    FP16 = 'FP16'
    INT64 = 'INT64'
    INT32 = 'INT32'
    INT8 = 'INT8'
    INT4 = 'INT4'
    INT2 = 'INT2'

    def __str__(self) ->str:
        return self.value


class JaggedTensorMeta(abc.ABCMeta, torch.fx.ProxyableClassMeta):
    pass


class Multistreamable(abc.ABC):
    """
    Objects implementing this interface are allowed to be transferred
    from one CUDA stream to another.
    torch.Tensor and (Keyed)JaggedTensor implement this interface.
    """

    @abc.abstractmethod
    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        """
        See https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html
        """
        ...


class Pipelineable(Multistreamable):
    """
    This interface contains two methods, one for moving an input across devices,
    the other one for marking streams that operate the input.

    torch.Tensor implements this interface and we can used it in many applications.
    Another example is torchrec.(Keyed)JaggedTensor, which we use as the input to
    torchrec.EmbeddingBagCollection, which in turn is often the first layer of many models.
    Some models take compound inputs, which should implement this interface.
    """

    @abc.abstractmethod
    def to(self, device: torch.device, non_blocking: bool) ->'Pipelineable':
        """
        Please be aware that according to https://pytorch.org/docs/stable/generated/torch.Tensor.to.html,
        `to` might return self or a copy of self.  So please remember to use `to` with the assignment operator,
        for example, `in = in.to(new_device)`.
        """
        ...


@torch.fx.wrap
def _arange(*args, **kwargs) ->torch.Tensor:
    return torch.arange(*args, **kwargs)


def _assert_offsets_or_lengths_is_provided(offsets: Optional[torch.Tensor], lengths: Optional[torch.Tensor]) ->None:
    assert offsets is not None or lengths is not None, 'Must provide lengths or offsets'


def _assert_tensor_has_no_elements_or_has_integers(tensor: torch.Tensor, tensor_name: str) ->None:
    assert tensor.numel() == 0 or tensor.dtype in [torch.long, torch.int, torch.short, torch.int8, torch.uint8], '{} must be of integer type, but got {}'.format(tensor_name, tensor.dtype)


def _get_weights_or_throw(weights: Optional[torch.Tensor]) ->torch.Tensor:
    assert weights is not None, "This (Keyed)JaggedTensor doesn't have weights."
    return weights


def _values_string(values: torch.Tensor, start: int, end: int) ->str:
    size = values.size()
    if len(size) == 1:
        return '[' + ', '.join([str(value.item()) for value in values[start:end]]) + ']'
    elif len(size) == 2:
        values_list: List[str] = []
        for value in values[start:end]:
            values_list.append('[' + ', '.join([str(s.item()) for s in value]) + ']')
        return '[' + ', '.join(values_list) + ']'
    else:
        raise ValueError("the values dimension is larger than 2, we don't support printing")


def _jagged_values_string(values: torch.Tensor, offsets: torch.Tensor, offset_start: int, offset_end: int) ->str:
    return '[' + ', '.join([_values_string(values, offsets[index], offsets[index + 1]) for index in range(offset_start, offset_end)]) + ']'


def _to_lengths(offsets: torch.Tensor) ->torch.Tensor:
    return offsets[1:] - offsets[:-1]


def _maybe_compute_lengths(lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]) ->torch.Tensor:
    if lengths is None:
        assert offsets is not None
        lengths = _to_lengths(offsets)
    return lengths


def _to_offsets(lengths: torch.Tensor) ->torch.Tensor:
    return torch.ops.fbgemm.asynchronous_complete_cumsum(lengths)


def _maybe_compute_offsets(lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]) ->torch.Tensor:
    if offsets is None:
        assert lengths is not None
        offsets = _to_offsets(lengths)
    return offsets


@torch.fx.wrap
def _optional_mask(tensor: Optional[torch.Tensor], mask: torch.Tensor) ->Optional[torch.Tensor]:
    return tensor[mask] if tensor is not None else None


class JaggedTensor(Pipelineable, metaclass=JaggedTensorMeta):
    """
    Represents an (optionally weighted) jagged tensor.

    A `JaggedTensor` is a tensor with a *jagged dimension* which is dimension whose
    slices may be of different lengths. See `KeyedJaggedTensor` for full example.

    Implementation is torch.jit.script-able.

    NOTE:
        We will NOT do input validation as it's expensive, you should always pass in the
        valid lengths, offsets, etc.

    Args:
        values (torch.Tensor): values tensor in dense representation.
        weights (Optional[torch.Tensor]): if values have weights. Tensor with same shape
            as values.
        lengths (Optional[torch.Tensor]): jagged slices, represented as lengths.
        offsets (Optional[torch.Tensor]): jagged slices, represented as cumulative
            offsets.
    """

    def __init__(self, values: torch.Tensor, weights: Optional[torch.Tensor]=None, lengths: Optional[torch.Tensor]=None, offsets: Optional[torch.Tensor]=None) ->None:
        self._values: torch.Tensor = values
        self._weights: Optional[torch.Tensor] = weights
        _assert_offsets_or_lengths_is_provided(offsets, lengths)
        if offsets is not None:
            _assert_tensor_has_no_elements_or_has_integers(offsets, 'offsets')
        if lengths is not None:
            _assert_tensor_has_no_elements_or_has_integers(lengths, 'lengths')
        self._lengths: Optional[torch.Tensor] = lengths
        self._offsets: Optional[torch.Tensor] = offsets

    @staticmethod
    def empty(is_weighted: bool=False) ->'JaggedTensor':
        weights = torch.tensor([]) if is_weighted else None
        return JaggedTensor(values=torch.tensor([]), offsets=torch.tensor([]), lengths=torch.tensor([]), weights=weights)

    @staticmethod
    def from_dense_lengths(values: torch.Tensor, lengths: torch.Tensor, weights: Optional[torch.Tensor]=None) ->'JaggedTensor':
        """
        Constructs `JaggedTensor` from dense values/weights of shape (B, N,).

        Note that `lengths` is still of shape (B,).
        """
        mask2d = _arange(end=values.size(1), device=values.device).expand(values.size(0), -1) < lengths.unsqueeze(-1)
        return JaggedTensor(values=values[mask2d], weights=_optional_mask(weights, mask2d), lengths=lengths)

    @staticmethod
    def from_dense(values: List[torch.Tensor], weights: Optional[List[torch.Tensor]]=None) ->'JaggedTensor':
        """
        Constructs `JaggedTensor` from dense values/weights of shape (B, N,).

        Note that `lengths` and `offsets` are still of shape (B,).

        Args:
            values (List[torch.Tensor]): a list of tensors for dense representation
            weights (Optional[List[torch.Tensor]]): if values have weights, tensor with
                the same shape as values.

        Returns:
            JaggedTensor: JaggedTensor created from 2D dense tensor.

        Example::

            values = [
                torch.Tensor([1.0]),
                torch.Tensor(),
                torch.Tensor([7.0, 8.0]),
                torch.Tensor([10.0, 11.0, 12.0]),
            ]
            weights = [
                torch.Tensor([1.0]),
                torch.Tensor(),
                torch.Tensor([7.0, 8.0]),
                torch.Tensor([10.0, 11.0, 12.0]),
            ]
            j1 = JaggedTensor.from_dense(
                values=values,
                weights=weights,
            )

            # j1 = [[1.0], [], [7.0], [8.0], [10.0, 11.0, 12.0]]
        """
        lengths = torch.IntTensor([value.size(0) for value in values])
        values = torch.cat(values, dim=0)
        weights = torch.cat(weights, dim=0) if weights is not None else None
        return JaggedTensor(values=values, weights=weights, lengths=lengths)

    def to_dense(self) ->List[torch.Tensor]:
        """
        Constructs dense-reprensentation tensor from JT.

        Returns:
            List[torch.Tensor]: list of tensors.

        Example::

            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])
            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])
            jt = JaggedTensor(values=values, offsets=offsets)

            torch_list = jt.to_dense()

            # torch_list = [
            #     torch.tensor([1.0, 2.0]),
            #     torch.tensor([]),
            #     torch.tensor([3.0]),
            #     torch.tensor([4.0]),
            #     torch.tensor([5.0]),
            #     torch.tensor([6.0, 7.0, 8.0]),
            # ]
        """
        tensor_list = []
        for index in range(self.offsets().size(0) - 1):
            offset = self.offsets()[index].item()
            next_offset = self.offsets()[index + 1].item()
            tensor_list.append(self.values()[offset:next_offset])
        return tensor_list

    def to_padded_dense(self, desired_length: Optional[int]=None, padding_value: float=0.0) ->torch.Tensor:
        """
        Constructs 2D dense Tensor from JT to shape (B, N,).

        Note that `B` is the length of self.lengths() and `N` is the longest feature
        length or `desired_length`.

        If `desired_length` > `length` we will pad with `padding_value`, otherwise we
        will select the last value at `desired_length`.

        Args:
            desired_length (int): the length of the tensor.
            padding_value (float): padding value if we need to pad.

        Returns:
            torch.Tensor: 2d dense tensor.

        Example::

            values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])
            offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8])
            jt = JaggedTensor(values=values, offsets=offsets)

            dt = jt.to_padded_dense(
                desired_length=2,
                padding_value=10.0,
            )

            # dt = [
            #     [1.0, 2.0],
            #     [10.0, 10.0],
            #     [3.0, 10.0],
            #     [4.0, 10.0],
            #     [5.0, 10.0],
            #     [7.0, 8.0],
            # ]
        """
        lengths_list: List[int] = self.lengths().tolist()
        N = max(lengths_list) if desired_length is None else desired_length
        return torch.ops.fbgemm.jagged_to_padded_dense(self.values(), [self.offsets()], [N], padding_value)

    def lengths(self) ->torch.Tensor:
        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)
        self._lengths = _lengths
        return _lengths

    def lengths_or_none(self) ->Optional[torch.Tensor]:
        return self._lengths

    def offsets(self) ->torch.Tensor:
        _offsets = _maybe_compute_offsets(self._lengths, self._offsets)
        self._offsets = _offsets
        return _offsets

    def offsets_or_none(self) ->Optional[torch.Tensor]:
        return self._offsets

    def values(self) ->torch.Tensor:
        return self._values

    def weights(self) ->torch.Tensor:
        return _get_weights_or_throw(self._weights)

    def weights_or_none(self) ->Optional[torch.Tensor]:
        return self._weights

    def to(self, device: torch.device, non_blocking: bool=False) ->'JaggedTensor':
        weights = self._weights
        lengths = self._lengths
        offsets = self._offsets
        return JaggedTensor(values=self._values, weights=weights if weights is not None else None, lengths=lengths if lengths is not None else None, offsets=offsets if offsets is not None else None)

    @torch.jit.unused
    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        self._values.record_stream(stream)
        weights = self._weights
        lengths = self._lengths
        offsets = self._offsets
        if weights is not None:
            weights.record_stream(stream)
        if lengths is not None:
            lengths.record_stream(stream)
        if offsets is not None:
            offsets.record_stream(stream)

    def __str__(self) ->str:
        offsets = self.offsets()
        if self._weights is None:
            return 'JaggedTensor({\n    ' + _jagged_values_string(self._values, offsets, 0, len(offsets) - 1) + '\n})\n'
        return 'JaggedTensor({\n' + '    "values": ' + _jagged_values_string(self._values, offsets, 0, len(offsets) - 1) + ',\n    "weights": ' + _jagged_values_string(_get_weights_or_throw(self._weights), offsets, 0, len(offsets) - 1) + '\n})\n'


def _jagged_tensor_string(key: str, values: torch.Tensor, weights: Optional[torch.Tensor], offsets: torch.Tensor, offset_start: int, offset_end: int) ->str:
    if weights is None:
        return '"{}": '.format(key) + _jagged_values_string(values, offsets, offset_start, offset_end)
    return '"{}"'.format(key) + """: {
        "values": """ + _jagged_values_string(values, offsets, offset_start, offset_end) + """,
        "weights": """ + _jagged_values_string(_get_weights_or_throw(weights), offsets, offset_start, offset_end) + '\n    }'


def _maybe_compute_index_per_key(keys: List[str], index_per_key: Optional[Dict[str, int]]) ->Dict[str, int]:
    if index_per_key is None:
        index_per_key = {key: i for i, key in enumerate(keys)}
    return index_per_key


def _batched_lengths_to_offsets(lengths: torch.Tensor) ->torch.Tensor:
    f, b = lengths.shape
    offsets_0 = lengths.new_zeros((f, 1))
    offsets_1 = torch.cumsum(lengths, dim=-1)
    offsets = torch.cat([offsets_0, offsets_1], dim=-1)
    return offsets


def _maybe_compute_kjt_to_jt_dict(stride: int, keys: List[str], length_per_key: List[int], values: torch.Tensor, lengths: torch.Tensor, weights: Optional[torch.Tensor], jt_dict: Optional[Dict[str, JaggedTensor]]) ->Dict[str, JaggedTensor]:
    if not length_per_key:
        return {}
    if jt_dict is None:
        _jt_dict: Dict[str, JaggedTensor] = {}
        values_list = torch.split(values, length_per_key)
        lengths_tuple = torch.unbind(lengths.view(-1, stride) if lengths.numel() != 0 else lengths, dim=0)
        offsets_tuple = torch.unbind(_batched_lengths_to_offsets(lengths.view(-1, stride)) if lengths.numel() != 0 else lengths, dim=0)
        if weights is not None:
            weights_list = torch.split(weights, length_per_key)
            for idx, key in enumerate(keys):
                length = lengths_tuple[idx]
                offset = offsets_tuple[idx]
                _jt_dict[key] = JaggedTensor(lengths=length, offsets=offset, values=values_list[idx], weights=weights_list[idx])
        else:
            for idx, key in enumerate(keys):
                length = lengths_tuple[idx]
                offset = offsets_tuple[idx]
                _jt_dict[key] = JaggedTensor(lengths=length, offsets=offset, values=values_list[idx])
        jt_dict = _jt_dict
    return jt_dict


def _maybe_compute_length_per_key(keys: List[str], stride: int, length_per_key: Optional[List[int]], lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]) ->List[int]:
    if length_per_key is None:
        if len(keys) and offsets is not None and len(offsets) > 0:
            _length: List[int] = torch.sum(torch.diff(offsets).view(-1, stride), dim=1).tolist()
        elif len(keys) and lengths is not None:
            _length: List[int] = torch.sum(lengths.view(-1, stride), dim=1).tolist() if lengths.numel() != 0 else [0] * len(keys)
        else:
            _length: List[int] = []
        length_per_key = _length
    return length_per_key


def _cumsum(o: List[int]) ->List[int]:
    ret = [0] * (len(o) + 1)
    for i in range(len(o)):
        ret[i + 1] = ret[i] + o[i]
    return ret


def _maybe_compute_offset_per_key(keys: List[str], stride: int, length_per_key: Optional[List[int]], offset_per_key: Optional[List[int]], lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]) ->Tuple[List[int], List[int]]:
    if length_per_key is None:
        _length_per_key: List[int] = _maybe_compute_length_per_key(keys, stride, length_per_key, lengths, offsets)
        return _length_per_key, _cumsum(_length_per_key)
    elif offset_per_key is None:
        return length_per_key, _cumsum(length_per_key)
    else:
        return length_per_key, offset_per_key


def _maybe_compute_stride_kjt(keys: List[str], stride: Optional[int], lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]) ->int:
    if stride is None:
        if len(keys) == 0:
            stride = 0
        elif offsets is not None and offsets.numel() > 0:
            stride = (offsets.numel() - 1) // len(keys)
        elif lengths is not None:
            stride = lengths.numel() // len(keys)
        else:
            stride = 0
    return stride


@torch.jit.script
def _maybe_compute_stride_kjt_scripted(keys: List[str], stride: Optional[int], lengths: Optional[torch.Tensor], offsets: Optional[torch.Tensor]) ->torch.Tensor:
    return torch.tensor([_maybe_compute_stride_kjt(keys, stride, lengths, offsets)])


def _sum_by_splits(input_list: List[int], splits: List[int]) ->List[int]:
    return [sum(input_list[sum(splits[:i]):sum(splits[:i]) + n]) for i, n in enumerate(splits)]


class KeyedJaggedTensor(Pipelineable, metaclass=JaggedTensorMeta):
    """Represents an (optionally weighted) keyed jagged tensor.

    A `KeyedJaggedTensor` is a tensor with a *jagged dimension* which is dimension whose
    slices may be of different lengths. Keyed on first dimension and jagged on the last
    dimension.

    Implementation is torch.jit.script-able.

    Args:
        keys (List[str]): keys to the jagged Tensor.
        values (torch.Tensor): values tensor in dense representation.
        weights (Optional[torch.Tensor]): if the values have weights. Tensor with the
            same shape as values.
        lengths (Optional[torch.Tensor]): jagged slices, represented as lengths.
        offsets (Optional[torch.Tensor]): jagged slices, represented as cumulative
            offsets.
        stride (Optional[int]): number of examples per batch.
        length_per_key (Optional[List[int]]): start length for each key.
        offset_per_key (Optional[List[int]]): start offset for each key and final
            offset.
        index_per_key (Optional[Dict[str, int]]): index for each key.
        jt_dict (Optional[Dict[str, JaggedTensor]]):

    Example::

        #              0       1        2  <-- dim_1
        # "Feature0"   [V0,V1] None    [V2]
        # "Feature1"   [V3]    [V4]    [V5,V6,V7]
        #   ^
        #  dim_0

        dim_0: keyed dimension (ie. `Feature0`, `Feature1`)
        dim_1: optional second dimension (ie. batch size)
        dim_2: The jagged dimension which has slice lengths between 0-3 in the above example

        # We represent this data with following inputs:

        values: torch.Tensor = [V0, V1, V2, V3, V4, V5, V6, V7]  # V == any tensor datatype
        weights: torch.Tensor = [W0, W1, W2, W3, W4, W5, W6, W7]  # W == any tensor datatype
        lengths: torch.Tensor = [2, 0, 1, 1, 1, 3]  # representing the jagged slice
        offsets: torch.Tensor = [0, 2, 2, 3, 4, 5, 8]  # offsets from 0 for each jagged slice
        keys: List[str] = ["Feature0", "Feature1"]  # correspond to each value of dim_0
        index_per_key: Dict[str, int] = {"Feature0": 0, "Feature1": 1}  # index for each key
        offset_per_key: List[int] = [0, 3, 8]  # start offset for each key and final offset
    """

    def __init__(self, keys: List[str], values: torch.Tensor, weights: Optional[torch.Tensor]=None, lengths: Optional[torch.Tensor]=None, offsets: Optional[torch.Tensor]=None, stride: Optional[int]=None, length_per_key: Optional[List[int]]=None, offset_per_key: Optional[List[int]]=None, index_per_key: Optional[Dict[str, int]]=None, jt_dict: Optional[Dict[str, JaggedTensor]]=None) ->None:
        self._keys: List[str] = keys
        self._values: torch.Tensor = values
        self._weights: Optional[torch.Tensor] = weights
        if offsets is not None:
            _assert_tensor_has_no_elements_or_has_integers(offsets, 'offsets')
        if lengths is not None:
            _assert_tensor_has_no_elements_or_has_integers(lengths, 'lengths')
        self._lengths: Optional[torch.Tensor] = lengths
        self._offsets: Optional[torch.Tensor] = offsets
        if torch.jit.is_tracing():
            stride = _maybe_compute_stride_kjt_scripted(keys, stride, lengths, offsets)[0]
        else:
            stride = _maybe_compute_stride_kjt(keys, stride, lengths, offsets)
        self._stride: int = stride
        self._length_per_key: Optional[List[int]] = length_per_key
        self._offset_per_key: Optional[List[int]] = offset_per_key
        self._index_per_key: Optional[Dict[str, int]] = index_per_key
        self._jt_dict: Optional[Dict[str, JaggedTensor]] = jt_dict

    @staticmethod
    def from_offsets_sync(keys: List[str], values: torch.Tensor, offsets: torch.Tensor, weights: Optional[torch.Tensor]=None, stride: Optional[int]=None) ->'KeyedJaggedTensor':
        kjt = KeyedJaggedTensor(keys=keys, values=values, weights=weights, offsets=offsets, stride=stride)
        return kjt.sync()

    @staticmethod
    def from_lengths_sync(keys: List[str], values: torch.Tensor, lengths: torch.Tensor, weights: Optional[torch.Tensor]=None, stride: Optional[int]=None) ->'KeyedJaggedTensor':
        kjt = KeyedJaggedTensor(keys=keys, values=values, weights=weights, lengths=lengths, stride=stride)
        return kjt.sync()

    @staticmethod
    def concat(kjt_list: List['KeyedJaggedTensor']) ->'KeyedJaggedTensor':
        if len(kjt_list) == 0:
            raise ValueError("Can't concat empty KJT list")
        stride: int = kjt_list[0].stride()
        is_weighted: bool = kjt_list[0].weights_or_none() is not None
        has_length_per_key: bool = True
        length_per_key: List[int] = []
        keys: List[str] = []
        value_list: List[torch.Tensor] = []
        weight_list: List[torch.Tensor] = []
        length_list: List[torch.Tensor] = []
        for kjt in kjt_list:
            if kjt.stride() != stride:
                raise ValueError(f'Can only merge KJTs of the same stride ({stride} != kjt.stride())')
            curr_is_weighted: bool = kjt.weights_or_none() is not None
            if is_weighted != curr_is_weighted:
                raise ValueError("Can't merge weighted KJT with unweighted KJT")
            _length_per_key: Optional[List[int]] = None
            if kjt._length_per_key is None:
                has_length_per_key = False
            else:
                _length_per_key = kjt._length_per_key
            if has_length_per_key and _length_per_key is not None:
                length_per_key += _length_per_key
            keys += kjt.keys()
            value_list.append(kjt.values())
            if is_weighted:
                weight_list.append(kjt.weights())
            length_list.append(kjt.lengths())
        return KeyedJaggedTensor(keys=keys, values=torch.cat(value_list, dim=0), weights=torch.cat(weight_list, dim=0) if is_weighted else None, lengths=torch.cat(length_list, dim=0), stride=stride, length_per_key=length_per_key if has_length_per_key else None)

    @staticmethod
    def empty(is_weighted: bool=False, device: Optional[torch.device]=None) ->'KeyedJaggedTensor':
        weights = None
        if is_weighted is True:
            weights = torch.tensor([], device=device) if device else torch.tensor([])
        return KeyedJaggedTensor(keys=[], values=torch.tensor([], device=device) if device else torch.tensor([]), weights=weights, lengths=torch.tensor([], device=device) if device else torch.tensor([]), stride=0)

    @staticmethod
    def empty_like(kjt: 'KeyedJaggedTensor') ->'KeyedJaggedTensor':
        return KeyedJaggedTensor(keys=[], values=torch.tensor([], device=kjt.device(), dtype=kjt.values().dtype), weights=None if kjt.weights_or_none() is None else torch.tensor([], device=kjt.device(), dtype=kjt.weights().dtype), lengths=torch.tensor([], device=kjt.device(), dtype=kjt.lengths().dtype), stride=kjt.stride())

    def sync(self) ->'KeyedJaggedTensor':
        self.length_per_key()
        self.offset_per_key()
        return self

    def device(self) ->torch.device:
        return self._values.device

    def lengths(self) ->torch.Tensor:
        _lengths = _maybe_compute_lengths(self._lengths, self._offsets)
        self._lengths = _lengths
        return _lengths

    def lengths_or_none(self) ->Optional[torch.Tensor]:
        return self._lengths

    def offsets(self) ->torch.Tensor:
        _offsets = _maybe_compute_offsets(self._lengths, self._offsets)
        self._offsets = _offsets
        return _offsets

    def offsets_or_none(self) ->Optional[torch.Tensor]:
        return self._offsets

    def keys(self) ->List[str]:
        return self._keys

    def values(self) ->torch.Tensor:
        return self._values

    def weights(self) ->torch.Tensor:
        return _get_weights_or_throw(self._weights)

    def weights_or_none(self) ->Optional[torch.Tensor]:
        return self._weights

    def stride(self) ->int:
        return self._stride

    def _key_indices(self) ->Dict[str, int]:
        _index_per_key: Dict[str, int] = _maybe_compute_index_per_key(self._keys, self._index_per_key)
        self._index_per_key = _index_per_key
        return _index_per_key

    def length_per_key(self) ->List[int]:
        _length_per_key = _maybe_compute_length_per_key(self._keys, self.stride(), self._length_per_key, self._lengths, self._offsets)
        self._length_per_key = _length_per_key
        return _length_per_key

    def length_per_key_or_none(self) ->Optional[List[int]]:
        return self._length_per_key

    def offset_per_key(self) ->List[int]:
        _length_per_key, _offset_per_key = _maybe_compute_offset_per_key(self._keys, self.stride(), self._length_per_key, self._offset_per_key, self._lengths, self._offsets)
        self._length_per_key = _length_per_key
        self._offset_per_key = _offset_per_key
        return _offset_per_key

    def offset_per_key_or_none(self) ->Optional[List[int]]:
        return self._offset_per_key

    def split(self, segments: List[int]) ->List['KeyedJaggedTensor']:
        split_list: List[KeyedJaggedTensor] = []
        start = 0
        start_offset = 0
        _length_per_key = self.length_per_key()
        _offset_per_key = self.offset_per_key()
        for segment in segments:
            end = start + segment
            end_offset = _offset_per_key[end]
            keys: List[str] = self._keys[start:end]
            if segment == len(self._keys):
                split_list.append(KeyedJaggedTensor(keys=self._keys, values=self._values, weights=self.weights_or_none(), lengths=self._lengths, offsets=self._offsets, stride=self._stride, length_per_key=self._length_per_key, offset_per_key=self._offset_per_key, index_per_key=self._index_per_key, jt_dict=self._jt_dict))
            elif segment == 0:
                split_list.append(KeyedJaggedTensor(keys=keys, values=torch.tensor([], device=self.device(), dtype=self._values.dtype), weights=None if self.weights_or_none() is None else torch.tensor([], device=self.device(), dtype=self.weights().dtype), lengths=torch.tensor([], device=self.device(), dtype=torch.int), offsets=torch.tensor([], device=self.device(), dtype=torch.int), stride=self._stride, length_per_key=None, offset_per_key=None, index_per_key=None, jt_dict=None))
            else:
                split_length_per_key = _length_per_key[start:end]
                split_list.append(KeyedJaggedTensor(keys=keys, values=self._values[start_offset:end_offset], weights=None if self.weights_or_none() is None else self.weights()[start_offset:end_offset], lengths=self.lengths()[start * self._stride:end * self._stride], offsets=None, stride=self._stride, length_per_key=split_length_per_key, offset_per_key=None, index_per_key=None, jt_dict=None))
            start = end
            start_offset = end_offset
        return split_list

    def permute(self, indices: List[int], indices_tensor: Optional[torch.Tensor]=None) ->'KeyedJaggedTensor':
        if indices_tensor is None:
            indices_tensor = torch.tensor(indices, dtype=torch.int, device=self.device())
        length_per_key = self.length_per_key()
        permuted_keys: List[str] = []
        permuted_length_per_key: List[int] = []
        permuted_lengths_sum = 0
        for index in indices:
            key = self._keys[index]
            permuted_keys.append(key)
            permuted_lengths_sum += length_per_key[index]
            permuted_length_per_key.append(length_per_key[index])
        permuted_lengths, permuted_values, permuted_weights = torch.ops.fbgemm.permute_2D_sparse_data(indices_tensor, self.lengths().view(len(self._keys), -1), self.values(), self.weights_or_none(), permuted_lengths_sum)
        kjt = KeyedJaggedTensor(keys=permuted_keys, values=permuted_values, weights=permuted_weights, lengths=permuted_lengths.view(-1), offsets=None, stride=self._stride, length_per_key=permuted_length_per_key if len(permuted_keys) > 0 else None, offset_per_key=None, index_per_key=None, jt_dict=None)
        return kjt

    def __getitem__(self, key: str) ->JaggedTensor:
        offset_per_key = self.offset_per_key()
        index = self._key_indices()[key]
        start_offset = offset_per_key[index]
        end_offset = offset_per_key[index + 1] if index + 1 < len(offset_per_key) else start_offset
        return JaggedTensor(values=self._values[start_offset:end_offset], weights=None if self.weights_or_none() is None else self.weights()[start_offset:end_offset], lengths=self.lengths()[index * self._stride:(index + 1) * self._stride], offsets=None)

    def to_dict(self) ->Dict[str, JaggedTensor]:
        _jt_dict = _maybe_compute_kjt_to_jt_dict(self.stride(), self.keys(), self.length_per_key(), self.values(), self.lengths(), self.weights_or_none(), self._jt_dict)
        self._jt_dict = _jt_dict
        return _jt_dict

    @torch.jit.unused
    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        self._values.record_stream(stream)
        weights = self._weights
        lengths = self._lengths
        offsets = self._offsets
        if weights is not None:
            weights.record_stream(stream)
        if lengths is not None:
            lengths.record_stream(stream)
        if offsets is not None:
            offsets.record_stream(stream)

    def to(self, device: torch.device, non_blocking: bool=False) ->'KeyedJaggedTensor':
        weights = self._weights
        lengths = self._lengths
        offsets = self._offsets
        length_per_key = self._length_per_key
        offset_per_key = self._offset_per_key
        index_per_key = self._index_per_key
        jt_dict = self._jt_dict
        return KeyedJaggedTensor(keys=self._keys, values=self._values, weights=weights if weights is not None else None, lengths=lengths if lengths is not None else None, offsets=offsets if offsets is not None else None, stride=self._stride, length_per_key=length_per_key, offset_per_key=offset_per_key, index_per_key=index_per_key, jt_dict=jt_dict)

    def __str__(self) ->str:
        if len(self._keys) == 0 or self._offsets is None and self._lengths is None:
            return 'KeyedJaggedTensor()\n'
        offsets = self.offsets()
        step = (len(offsets) - 1) // len(self._keys)
        return 'KeyedJaggedTensor({\n' + ',\n'.join([('    ' + _jagged_tensor_string(self._keys[index], self._values, self._weights, offsets, index * step, (index + 1) * step)) for index in range(len(self._keys))]) + '\n})\n'

    def pin_memory(self) ->'KeyedJaggedTensor':
        weights = self._weights
        lengths = self._lengths
        offsets = self._offsets
        return KeyedJaggedTensor(keys=self._keys, values=self._values.pin_memory(), weights=weights.pin_memory() if weights is not None else None, lengths=lengths.pin_memory() if lengths is not None else None, offsets=offsets.pin_memory() if offsets is not None else None, stride=self._stride, length_per_key=self._length_per_key, offset_per_key=self._offset_per_key, index_per_key=self._index_per_key, jt_dict=None)

    def dist_labels(self) ->List[str]:
        labels = ['lengths', 'values']
        if self.weights_or_none() is not None:
            labels.append('weights')
        return labels

    def dist_splits(self, key_splits: List[int]) ->List[List[int]]:
        batch_size_per_split = _sum_by_splits([self.stride()] * len(self.keys()), key_splits)
        length_per_split = _sum_by_splits(self.length_per_key(), key_splits)
        splits = [batch_size_per_split, length_per_split]
        if self.weights_or_none() is not None:
            splits.append(length_per_split)
        return splits

    def dist_tensors(self) ->List[torch.Tensor]:
        tensors = [self.lengths(), self.values()]
        if self.weights_or_none() is not None:
            tensors.append(self.weights())
        return tensors

    @staticmethod
    def dist_init(keys: List[str], tensors: List[torch.Tensor], batch_size_per_rank: List[int], recat: torch.Tensor) ->'KeyedJaggedTensor':
        assert len(tensors) in [2, 3]
        lengths = tensors[0]
        values = tensors[1]
        weights = tensors[2] if len(tensors) == 3 else None
        with record_function('## all2all_data:recat_values ##'):
            if recat.numel() > 0:
                if all(bs == batch_size_per_rank[0] for bs in batch_size_per_rank):
                    lengths, values, weights = torch.ops.fbgemm.permute_2D_sparse_data(recat, lengths.view(-1, batch_size_per_rank[0]), values, weights, values.numel())
                    lengths = lengths.view(-1)
                else:
                    lengths, values, weights = torch.ops.fbgemm.permute_1D_sparse_data(recat, lengths.view(-1), values, weights, values.numel())
        kjt = KeyedJaggedTensor(keys=keys, values=values, weights=weights, lengths=lengths, stride=sum(batch_size_per_rank))
        return kjt.sync()


class CopyMixIn:

    @abstractmethod
    def copy(self, device: torch.device) ->nn.Module:
        ...


class ModuleNoCopyMixin(CopyMixIn):
    """
    A mixin to allow modules to override copy behaviors in DMP.
    """

    def copy(self, device: torch.device) ->nn.Module:
        return self


def dtype_to_data_type(dtype: torch.dtype) ->DataType:
    if dtype == torch.float:
        return DataType.FP32
    elif dtype == torch.float16 or dtype == torch.half:
        return DataType.FP16
    elif dtype in {torch.int, torch.int32}:
        return DataType.INT32
    elif dtype in {torch.long, torch.int64}:
        return DataType.INT64
    elif dtype in {torch.quint8, torch.qint8, torch.int8, torch.uint8}:
        return DataType.INT8
    elif dtype == torch.quint4x2:
        return DataType.INT4
    elif dtype == torch.quint2x4:
        return DataType.INT2
    else:
        raise Exception(f'Invalid data type {dtype}')


@unique
class PoolingType(Enum):
    SUM = 'SUM'
    MEAN = 'MEAN'
    NONE = 'NONE'


def quantize_state_dict(module: nn.Module, table_name_to_quantized_weights: Dict[str, Tuple[Tensor, Tensor]], data_type: DataType) ->torch.device:
    device = torch.device('cpu')
    for key, tensor in module.state_dict().items():
        splits = key.split('.')
        assert splits[-1] == 'weight'
        table_name = splits[-2]
        device = tensor.device
        num_bits = DATA_TYPE_NUM_BITS[data_type]
        if tensor.is_meta:
            quant_weight = torch.empty((tensor.shape[0], tensor.shape[1] * num_bits // 8), device='meta', dtype=torch.uint8)
            if data_type == DataType.INT8 or data_type == DataType.INT4 or data_type == DataType.INT2:
                scale_shift = torch.empty((tensor.shape[0], 4), device='meta', dtype=torch.uint8)
            else:
                scale_shift = None
        else:
            if tensor.dtype == torch.float or tensor.dtype == torch.float16:
                if data_type == DataType.FP16:
                    if tensor.dtype == torch.float:
                        tensor = tensor.half()
                    quant_res = tensor.view(torch.uint8)
                else:
                    quant_res = torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(tensor, num_bits)
            else:
                raise Exception('Unsupported dtype: {tensor.dtype}')
            if data_type == DataType.INT8 or data_type == DataType.INT4 or data_type == DataType.INT2:
                quant_weight, scale_shift = quant_res[:, :-4], quant_res[:, -4:]
            else:
                quant_weight, scale_shift = quant_res, None
        table_name_to_quantized_weights[table_name] = quant_weight, scale_shift
    return device


class Model(nn.Module):

    def __init__(self, num_embeddings, init_max, init_min, batch_size):
        super().__init__()
        self.embedding_dim = 16
        self.batch_size = batch_size
        self.config = EmbeddingConfig(name='id', embedding_dim=self.embedding_dim, num_embeddings=num_embeddings, weight_init_max=init_max, weight_init_min=init_min)
        self.emb = EmbeddingCollection(tables=[self.config], device=torch.device('meta'))
        self.dense = nn.Linear(16, 1)

    def forward(self, x):
        embeddings = self.emb(x)['id'].values().reshape((self.batch_size, -1, self.embedding_dim))
        fused = embeddings.sum(dim=1)
        output = self.dense(fused)
        pred = torch.sigmoid(output)
        return pred


class Attention(nn.Module):
    """
    Compute 'Scaled Dot Product Attention

    Args:
        query (torch.Tensor): the query tensor
        key (torch.Tensor): the key tensor
        value (torch.Tensor): the value tensor
        mask (torch.Tensor): the mask tensor
        dropout (nn.Dropout): the dropout layer

    Example::

        self.attention = Attention()
        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)
    """

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor]=None, dropout: Optional[nn.Dropout]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        forward function

        Args:
            query (torch.Tensor): the query tensor
            key (torch.Tensor): the key tensor
            value (torch.Tensor): the value tensor
            mask (torch.Tensor): the mask tensor
            dropout (nn.Dropout): the dropout layer

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
        """
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1000000000.0)
        p_attn = nn.functional.softmax(scores, dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn


class MultiHeadedAttention(nn.Module):
    """
    Multi-headed attention block.

    Args:
        num_heads (int): number of attention heads
        dim_model (int): input/output dimensionality
        dropout (float): the dropout probability
        mask (torch.Tensor): the mask tensor
        device: (Optional[torch.device]).

    Example::

        self.attention = MultiHeadedAttention(
            num_heads=attn_heads, dim_model=hidden, dropout=dropout, device=device
        )
        self.attention.forward(query, key, value, mask=mask)
    """

    def __init__(self, num_heads: int, dim_model: int, dropout: float=0.1, device: Optional[torch.device]=None) ->None:
        super().__init__()
        assert dim_model % num_heads == 0
        self.d_k: int = dim_model // num_heads
        self.num_heads = num_heads
        self.linear_layers = nn.ModuleList([nn.Linear(dim_model, dim_model, device=device) for _ in range(3)])
        self.output_linear = nn.Linear(dim_model, dim_model, device=device)
        self.attention = Attention()
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor]=None) ->torch.Tensor:
        """
        forward function

        Args:
            query (torch.Tensor): the query tensor
            key (torch.Tensor): the key tensor
            value (torch.Tensor): the value tensor
            mask (torch.Tensor): the mask tensor

        Returns:
            torch.Tensor.
        """
        batch_size = query.size(0)
        query, key, value = [linearLayer(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for linearLayer, x in zip(self.linear_layers, (query, key, value))]
        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)
        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)
        return self.output_linear(x)


class PositionwiseFeedForward(nn.Module):
    """
    Feed-forward block.

    Args:
        dim_model (int): input/output dimensionality
        d_ff (int): hidden dimensionality
        dropout (float): the dropout probability
        device: (Optional[torch.device]).

    Example::

        self.feed_forward = PositionwiseFeedForward(
            dim_model=hidden, d_ff=feed_forward_hidden, dropout=dropout, device=device
        )
    """

    def __init__(self, dim_model: int, d_ff: int, dropout: float=0.1, device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.w_1 = nn.Linear(dim_model, d_ff, device=device)
        self.w_2 = nn.Linear(d_ff, dim_model, device=device)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        forward function including the first Linear layer, ReLu, Dropout
        and the final Linear layer

        Args:
            x (torch.Tensor): the input tensor

        Returns:
            torch.Tensor.
        """
        return self.w_2(self.dropout(nn.functional.relu(self.w_1(x))))


class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.

    Args:
        size (int): layerNorm size
        dropout (float): the dropout probability
        device: (Optional[torch.device]).

    Example::

        self.input_sublayer = SublayerConnection(
            size=hidden, dropout=dropout, device=device
        )
    """

    def __init__(self, size: int, dropout: float, device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.norm = nn.LayerNorm(size, device=device)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, sublayer: Callable[[torch.Tensor], torch.Tensor]) ->torch.Tensor:
        """
        forward function including the norm layer, sublayer and dropout, finally
        add up with the input tensor

        Args:
            x (torch.Tensor): the input tensor
            sublayer (Callable[[torch.Tensor): callable layer

        Returns:
            torch.Tensor.
        """
        return x + self.dropout(sublayer(self.norm(x)))


class TransformerBlock(nn.Module):
    """
    Bidirectional Encoder = Transformer (self-attention)
    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection

    Args:
        hidden (int): hidden size of transformer
        attn_heads (int): head sizes of multi-head attention
        feed_forward_hidden (int): feed_forward_hidden, usually 4*hidden_size
        dropout (float): the dropout probability
        device: (Optional[torch.device]).

    Example::

        self.transformer_blocks = nn.ModuleList(
            [
                TransformerBlock(emb_dim, nhead, emb_dim * 4, dropout, device=device)
                for _ in range(num_layers)
            ]
        )
    """

    def __init__(self, hidden: int, attn_heads: int, feed_forward_hidden: int, dropout: float, device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.attention = MultiHeadedAttention(num_heads=attn_heads, dim_model=hidden, dropout=dropout, device=device)
        self.feed_forward = PositionwiseFeedForward(dim_model=hidden, d_ff=feed_forward_hidden, dropout=dropout, device=device)
        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout, device=device)
        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout, device=device)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        """
        forward function

        Args:
            x (torch.Tensor): the input tensor
            mask (torch.BoolTensor): determine which position has been masked

        Returns:
            torch.Tensor.
        """
        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))
        x = self.output_sublayer(x, self.feed_forward)
        return self.dropout(x)


class HistoryArch(torch.nn.Module):
    """
    embedding.HistoryArch is the input embedding layer the BERT4Rec model
    as described in Section 3.4 of the paper.  It consits of an item
    embedding table and a positional embedding table.

    The item embedding table consists of vocab_size vectors.  The
    positional embedding table has history_len vectors.  Both
    kinds of embedding vectors have length emb_dim.  As mentioned
    in Section 3.7, BERT4Rec differs from BERT lacking the
    sentence embedding.

    Note that for the item embedding table, we have applied TorchRec
    EmbeddingCollection which supports sharding

    Args:
        vocab_size (int): the item count including mask and padding
        history_len (int): the max length
        emb_dim (int): embedding dimension
        dropout (float): the dropout probability
        device: (Optional[torch.device]).

    Example::

        self.history = HistoryArch(
            vocab_size, max_len, emb_dim, dropout=dropout, device=device
        )
        x = self.history(input)
    """

    def __init__(self, vocab_size: int, history_len: int, emb_dim: int, dropout: float=0.1, device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.emb_dim = emb_dim
        self.history_len = history_len
        self.positional = nn.Parameter(torch.randn(history_len, emb_dim, device=device))
        self.layernorm = torch.nn.LayerNorm([history_len, emb_dim], device=device)
        self.dropout = torch.nn.Dropout(p=dropout)
        item_embedding_config = EmbeddingConfig(name='item_embedding', embedding_dim=emb_dim, num_embeddings=vocab_size, feature_names=['item'], weight_init_max=1.0, weight_init_min=-1.0)
        self.ec = EmbeddingCollection(tables=[item_embedding_config], device=device)

    def forward(self, id_list_features: KeyedJaggedTensor) ->torch.Tensor:
        """
        forward function: first query the item embedding and do the padding
        then add up with positional parameters and do the norm and dropout

        Args:
            id_list_features (KeyedJaggedTensor): the input KeyedJaggedTensor

        Returns:
            torch.Tensor.
        """
        jt_dict = self.ec(id_list_features)
        padded_embeddings = [torch.ops.fbgemm.jagged_2d_to_dense(values=jt_dict[e].values(), offsets=jt_dict[e].offsets(), max_sequence_length=self.history_len).view(-1, self.history_len, self.emb_dim) for e in id_list_features.keys()]
        item_output = torch.cat(padded_embeddings, dim=1)
        batch_size = id_list_features.stride()
        positional_output = self.positional.unsqueeze(0).repeat(batch_size, 1, 1)
        x = item_output + positional_output
        return self.dropout(self.layernorm(x))


class BERT4Rec(nn.Module):
    """
    The overall arch described in the BERT4Rec paper: (https://arxiv.org/abs/1904.06690)
    the encoder_layer was described in the section of 3.3, the output_layer was described in the
    section of 3.5

    Args:
        vocab_size (int): the item count including mask and padding
        max_len (int): the max length
        emb_dim (int): embedding dimension
        nhead (int): number of the transformation headers
        num_layers (int): number of the transformation layers
        dropout (float): the dropout probability
        device: (Optional[torch.device]).

    Example::

        input_kjt = KeyedJaggedTensor.from_lengths_sync(
            keys=["item"],
            values=torch.tensor([2, 4, 3, 4, 5]),
            lengths=torch.tensor([2, 3]),
        )
        device = (
            torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        )
        input_kjt = input_kjt.to(device)
        bert4rec = BERT4Rec(
            vocab_size=6, max_len=3, emb_dim=4, nhead=4, num_layers=4, device=device
        )
        logits = bert4rec(input_kjt)
    """

    def __init__(self, vocab_size: int, max_len: int, emb_dim: int, nhead: int, num_layers: int, dropout: float=0.1, device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.vocab_size = vocab_size
        self.emb_dim = emb_dim
        self.max_len = max_len
        self.history = HistoryArch(vocab_size, max_len, emb_dim, dropout=dropout, device=device)
        self.transformer_blocks = nn.ModuleList([TransformerBlock(emb_dim, nhead, emb_dim * 4, dropout, device=device) for _ in range(num_layers)])
        self.out = nn.Linear(self.emb_dim, self.vocab_size, device=device)

    def forward(self, input: KeyedJaggedTensor) ->torch.Tensor:
        """
        forward function: first get the item embedding result and
        fit into transformer blocks and fit into the last linaer
        layer to get the final output

        Args:
            input (KeyedJaggedTensor): the input KeyedJaggedTensor

        Returns:
            torch.Tensor.
        """
        dense_tensor = input['item'].to_padded_dense(desired_length=self.max_len)
        mask = (dense_tensor > 0).unsqueeze(1).repeat(1, dense_tensor.size(1), 1).unsqueeze(1)
        x = self.history(input)
        for transformer in self.transformer_blocks:
            x = transformer.forward(x, mask)
        return self.out(x)


def _keyed_values_string(values: torch.Tensor) ->str:
    return '[' + ', '.join([_values_string(value, 0, len(value)) for value in values]) + ']'


def _maybe_compute_offset_per_key_kt(length_per_key: List[int], offset_per_key: Optional[List[int]]) ->List[int]:
    if offset_per_key is None:
        offset_per_key = _cumsum(length_per_key)
    return offset_per_key


@torch.fx.wrap
def _regroup_keyed_tensors(keyed_tensors: List['KeyedTensor'], groups: List[List[str]]) ->List[torch.Tensor]:
    if len(keyed_tensors) == len(groups):
        match = True
        for kt, group in zip(keyed_tensors, groups):
            if kt.keys() != group:
                match = False
                break
        if match:
            return [kt.values() for kt in keyed_tensors]
    embedding_dicts = [keyed_tensor.to_dict() for keyed_tensor in keyed_tensors]
    lengths = [keyed_tensor.length_per_key() for keyed_tensor in keyed_tensors]
    indices = [keyed_tensor._key_indices() for keyed_tensor in keyed_tensors]
    key_dim = keyed_tensors[0].key_dim()
    key_to_idx: dict[str, int] = {}
    for i, keyed_tensor in enumerate(keyed_tensors):
        for key in keyed_tensor.keys():
            key_to_idx[key] = i
    split_lengths: List[int] = []
    cat_input: List[torch.Tensor] = []
    for group in groups:
        group_length = 0
        for name in group:
            cat_input.append(embedding_dicts[key_to_idx[name]][name])
            group_length += lengths[key_to_idx[name]][indices[key_to_idx[name]][name]]
        split_lengths.append(group_length)
    rearranged_values = torch.cat(cat_input, key_dim)
    return list(rearranged_values.split(split_lengths, dim=key_dim))


class KeyedTensor(Pipelineable, metaclass=JaggedTensorMeta):
    """
    KeyedTensor holds a concatenated list of dense tensors, each of which can be
    accessed by a key.

    The keyed dimension can be of variable length (length_per_key).
    Common use cases uses include storage of pooled embeddings of different dimensions.

    Implementation is torch.jit.script-able.

    Args:
        keys (List[str]): list of keys.
        length_per_key (List[int]): length of each key along key dimension.
        values (torch.Tensor): dense tensor, concatenated typically along key dimension.
        key_dim (int): key dimension, zero indexed - defaults to 1
            (typically B is 0-dimension).

    Example::

        # kt is KeyedTensor holding

        #                         0           1           2
        #     "Embedding A"    [1,1]       [1,1]        [1,1]
        #     "Embedding B"    [2,1,2]     [2,1,2]      [2,1,2]
        #     "Embedding C"    [3,1,2,3]   [3,1,2,3]    [3,1,2,3]

        tensor_list = [
            torch.tensor([[1,1]] * 3),
            torch.tensor([[2,1,2]] * 3),
            torch.tensor([[3,1,2,3]] * 3),
        ]

        keys = ["Embedding A", "Embedding B", "Embedding C"]

        kt = KeyedTensor.from_tensor_list(keys, tensor_list)

        kt.values()
            # tensor(
            #     [
            #         [1, 1, 2, 1, 2, 3, 1, 2, 3],
            #         [1, 1, 2, 1, 2, 3, 1, 2, 3],
            #         [1, 1, 2, 1, 2, 3, 1, 2, 3],
            #     ]
            # )

        kt["Embedding B"]
            # tensor([[2, 1, 2], [2, 1, 2], [2, 1, 2]])
    """

    def __init__(self, keys: List[str], length_per_key: List[int], values: torch.Tensor, key_dim: int=1, offset_per_key: Optional[List[int]]=None, index_per_key: Optional[Dict[str, int]]=None) ->None:
        self._keys = keys
        self._length_per_key = length_per_key
        self._values = values
        self._key_dim = key_dim
        self._offset_per_key: Optional[List[int]] = offset_per_key
        self._index_per_key: Optional[Dict[str, int]] = index_per_key

    @staticmethod
    def from_tensor_list(keys: List[str], tensors: List[torch.Tensor], key_dim: int=1, cat_dim: int=1) ->'KeyedTensor':
        length_per_key = [tensor.shape[key_dim] for tensor in tensors]
        return KeyedTensor(keys=keys, length_per_key=length_per_key, values=torch.cat(tensors, dim=cat_dim), key_dim=key_dim)

    def keys(self) ->List[str]:
        return self._keys

    def values(self) ->torch.Tensor:
        return self._values

    def key_dim(self) ->int:
        return self._key_dim

    def offset_per_key(self) ->List[int]:
        _offset_per_key = _maybe_compute_offset_per_key_kt(self._length_per_key, self._offset_per_key)
        self._offset_per_key = _offset_per_key
        return _offset_per_key

    def length_per_key(self) ->List[int]:
        return self._length_per_key

    def _key_indices(self) ->Dict[str, int]:
        _index_per_key = _maybe_compute_index_per_key(self._keys, self._index_per_key)
        self._index_per_key = _index_per_key
        return _index_per_key

    def __getitem__(self, key: str) ->torch.Tensor:
        index = self._key_indices()[key]
        start = self.offset_per_key()[index]
        length = self._length_per_key[index]
        return self._values.narrow(dim=self._key_dim, start=start, length=length)

    def to_dict(self) ->Dict[str, torch.Tensor]:
        indices = self._key_indices()
        lengths = self._length_per_key
        split_values = self._values.split(lengths, dim=self._key_dim)
        return {key: split_values[index] for key, index in indices.items()}

    @staticmethod
    def regroup(keyed_tensors: List['KeyedTensor'], groups: List[List[str]]) ->List[torch.Tensor]:
        return _regroup_keyed_tensors(keyed_tensors, groups)

    @staticmethod
    def regroup_as_dict(keyed_tensors: List['KeyedTensor'], groups: List[List[str]], keys: List[str]) ->Dict[str, torch.Tensor]:
        assert len(groups) == len(keys), 'Groups and keys should have same length'
        embeddings_list = _regroup_keyed_tensors(keyed_tensors, groups)
        embeddings_dict: Dict[str, torch.Tensor] = {}
        for i, key in enumerate(keys):
            embeddings_dict[key] = embeddings_list[i]
        return embeddings_dict

    @torch.jit.unused
    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        self._values.record_stream(stream)

    def to(self, device: torch.device, non_blocking: bool=False) ->'KeyedTensor':
        return KeyedTensor(keys=self._keys, length_per_key=self._length_per_key, values=self._values, key_dim=self._key_dim, offset_per_key=self._offset_per_key, index_per_key=self._index_per_key)

    def __str__(self) ->str:
        if len(self._keys) == 0:
            return 'KeyedTensor()\n'
        return 'KeyedTensor({\n' + ',\n'.join([('    "{}": '.format(key) + _keyed_values_string(self[key])) for key in self._keys]) + '\n})\n'


class Perceptron(torch.nn.Module):
    """
    Applies a linear transformation and activation.

    Args:
        in_size (int): number of elements in each input sample.
        out_size (int): number of elements in each output sample.
        bias (bool): if set to ``False``, the layer will not learn an additive bias.
            Default: ``True``.
        activation (Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]):
            the activation function to apply to the output of linear transformation.
            Default: torch.relu.
        device (Optional[torch.device]): default compute device.

    Example::

        batch_size = 3
        in_size = 40
        input = torch.randn(batch_size, in_size)

        out_size = 16
        perceptron = Perceptron(in_size, out_size, bias=True)

        output = perceptron(input)
        assert list(output) == [batch_size, out_size]
    """

    def __init__(self, in_size: int, out_size: int, bias: bool=True, activation: Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]=torch.relu, device: Optional[torch.device]=None) ->None:
        super().__init__()
        torch._C._log_api_usage_once(f'torchrec.modules.{self.__class__.__name__}')
        self._out_size = out_size
        self._in_size = in_size
        self._linear: nn.Linear = nn.Linear(self._in_size, self._out_size, bias=bias, device=device)
        self._activation_fn: Callable[[torch.Tensor], torch.Tensor] = activation

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): tensor of shape (B, I) where I is number of elements
                in each input sample.

        Returns:
            torch.Tensor: tensor of shape (B, O) where O is number of elements per
                channel in each output sample (i.e. `out_size`).
        """
        return self._activation_fn(self._linear(input))


class SwishLayerNorm(nn.Module):
    """
    Applies the Swish function with layer normalization: `Y = X * Sigmoid(LayerNorm(X))`.

    Args:
        input_dims (Union[int, List[int], torch.Size]): dimensions to normalize over.
            If an input tensor has shape [batch_size, d1, d2, d3], setting
            input_dim=[d2, d3] will do the layer normalization on last two dimensions.
        device (Optional[torch.device]): default compute device.

    Example::

        sln = SwishLayerNorm(100)
    """

    def __init__(self, input_dims: Union[int, List[int], torch.Size], device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.norm: torch.nn.modules.Sequential = nn.Sequential(nn.LayerNorm(input_dims, device=device), nn.Sigmoid())

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): an input tensor.

        Returns:
            torch.Tensor: an output tensor.
        """
        return input * self.norm(input)


def extract_module_or_tensor_callable(module_or_callable: Union[Callable[[], torch.nn.Module], torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]) ->Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]:
    try:
        module = module_or_callable()
        if isinstance(module, torch.nn.Module):
            return module
        else:
            raise ValueError('Expected callable that takes no input to return a torch.nn.Module, but got: {}'.format(type(module)))
    except TypeError as e:
        if 'required positional argument' in str(e):
            return module_or_callable
        raise


class MLP(torch.nn.Module):
    """
    Applies a stack of Perceptron modules sequentially (i.e. Multi-Layer Perceptron).

    Args:
        in_size (int): `in_size` of the input.
        layer_sizes (List[int]): `out_size` of each Perceptron module.
        bias (bool): if set to False, the layer will not learn an additive bias.
            Default: True.
        activation (str, Union[Callable[[], torch.nn.Module], torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]):
            the activation function to apply to the output of linear transformation of
            each Perceptron module.
            If `activation` is a `str`, we currently only support the follow strings, as
            "relu", "sigmoid", and "swish_layernorm".
            If `activation` is a `Callable[[], torch.nn.Module]`, `activation()` will be
            called once per Perceptron module to generate the activation module for that
            Perceptron module, and the parameters won't be shared between those activation
            modules.
            One use case is when all the activation modules share the same constructor
            arguments, but don't share the actual module parameters.
            Default: torch.relu.
        device (Optional[torch.device]): default compute device.

    Example::

        batch_size = 3
        in_size = 40
        input = torch.randn(batch_size, in_size)

        layer_sizes = [16, 8, 4]
        mlp_module = MLP(in_size, layer_sizes, bias=True)
        output = mlp_module(input)
        assert list(output.shape) == [batch_size, layer_sizes[-1]]
    """

    def __init__(self, in_size: int, layer_sizes: List[int], bias: bool=True, activation: Union[str, Callable[[], torch.nn.Module], torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]=torch.relu, device: Optional[torch.device]=None) ->None:
        super().__init__()
        if activation == 'relu':
            activation = torch.relu
        elif activation == 'sigmoid':
            activation = torch.sigmoid
        if not isinstance(activation, str):
            self._mlp: torch.nn.Module = torch.nn.Sequential(*[Perceptron(layer_sizes[i - 1] if i > 0 else in_size, layer_sizes[i], bias=bias, activation=extract_module_or_tensor_callable(activation), device=device) for i in range(len(layer_sizes))])
        elif activation == 'swish_layernorm':
            self._mlp: torch.nn.Module = torch.nn.Sequential(*[Perceptron(layer_sizes[i - 1] if i > 0 else in_size, layer_sizes[i], bias=bias, activation=SwishLayerNorm(layer_sizes[i], device=device), device=device) for i in range(len(layer_sizes))])
        else:
            assert ValueError, 'This MLP only support str version activation function of relu, sigmoid, and swish_layernorm'

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): tensor of shape (B, I) where I is number of elements
                in each input sample.

        Returns:
            torch.Tensor: tensor of shape (B, O) where O is `out_size` of the last Perceptron module.
        """
        return self._mlp(input)


W = TypeVar('W')


class Awaitable(abc.ABC, Generic[W]):

    def __init__(self) ->None:
        self._callbacks: List[Callable[[W], W]] = []

    @abc.abstractmethod
    def _wait_impl(self) ->W:
        pass

    def wait(self) ->W:
        with record_function(f'## {self.__class__.__name__} wait() ##'):
            ret: W = self._wait_impl()
            for callback in self.callbacks:
                ret = callback(ret)
        return ret

    @property
    def callbacks(self) ->List[Callable[[W], W]]:
        return self._callbacks


def _get_recat(local_split: int, num_splits: int, stagger: int=1, device: Optional[torch.device]=None, batch_size_per_rank: Optional[List[int]]=None) ->torch.Tensor:
    """
    Calculates relevant recat indices required to reorder AlltoAll collective.

    Args:
        local_split (int): how many features in local split.
        num_splits (int): how many splits (typically WORLD_SIZE).
        stagger (int): secondary reordering, (typically 1, but
            `WORLD_SIZE/LOCAL_WORLD_SIZE` for TWRW).
        device (Optional[torch.device]): device on which buffer will be allocated.
        batch_size_per_rank (Optional[List[int]]): batch size per rank, needed for
            variable batch size.

    Returns:
        torch.Tensor: recat tensor.

    Example::

        _recat(2, 4, 1)
            # [0, 2, 4, 6, 1, 3, 5, 7]
        _recat(2, 4, 2)
            # [0, 4, 2, 6, 1, 5, 3, 7]
    """
    with record_function('## all2all_data:recat_permute_gen ##'):
        recat: List[int] = []
        if local_split == 0:
            return torch.tensor(recat, device=device, dtype=torch.int32)
        feature_order: List[int] = [(x + num_splits // stagger * y) for x in range(num_splits // stagger) for y in range(stagger)]
        for i in range(local_split):
            for j in feature_order:
                recat.append(i + j * local_split)
        if batch_size_per_rank is not None:
            batch_size_per_feature = list(itertools.chain.from_iterable(itertools.repeat(x, local_split) for x in batch_size_per_rank))
            permuted_batch_size_per_feature = [batch_size_per_feature[r] for r in recat]
            input_offset = [0] + list(itertools.accumulate(batch_size_per_feature))
            output_offset = [0] + list(itertools.accumulate(permuted_batch_size_per_feature))
            recat_tensor = torch.tensor(recat, device=device, dtype=torch.int32)
            input_offset_tensor = torch.tensor(input_offset, device=device, dtype=torch.int32)
            output_offset_tensor = torch.tensor(output_offset, device=device, dtype=torch.int32)
            recat = torch.ops.fbgemm.expand_into_jagged_permute(recat_tensor, input_offset_tensor, output_offset_tensor, output_offset[-1])
            return recat
        else:
            return torch.tensor(recat, device=device, dtype=torch.int32)


class KJTAllToAllTensorsAwaitable(Awaitable[KeyedJaggedTensor]):
    """
    Awaitable for KJT tensors AlltoAll.

    Args:
        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.
        input (KeyedJaggedTensor): input KJT.
        splits (List[int]): list of len(pg.size()) which indicates how many features to
            send to each pg.rank(). It is assumed the `KeyedJaggedTensor` is ordered by
            destination rank. Same for all ranks.
        input_splits (List[List[int]]): input splits (number of values each rank will
            get) for each tensor in AlltoAll.
        output_splits (List[List[int]]): output splits (number of values per rank in
            output) for each tensor in AlltoAll.
        input_tensors (List[torch.Tensor]): provided KJT tensors (ie. lengths, values)
            to redistribute according to splits.
        labels (List[str]): labels for each provided tensor.
        batch_size_per_rank (List[int]): batch size per rank, to support variable batch
            size.
        keys (List[str]): KJT keys after AlltoAll.
        device (torch.device): device on which buffers will be allocated.
        stagger (int): stagger value to apply to recat tensor.
    """

    def __init__(self, pg: dist.ProcessGroup, input: KeyedJaggedTensor, splits: List[int], input_splits: List[List[int]], output_splits: List[List[int]], input_tensors: List[torch.Tensor], labels: List[str], batch_size_per_rank: List[int], keys: List[str], device: torch.device, stagger: int) ->None:
        super().__init__()
        self._workers: int = pg.size()
        self._pg: dist.ProcessGroup = pg
        self._device: torch.device = device
        self._input = input
        self._splits = splits
        self._input_splits: Dict[str, List[int]] = dict(zip(labels, input_splits))
        self._output_splits: Dict[str, List[int]] = dict(zip(labels, output_splits))
        self._batch_size_per_rank = batch_size_per_rank
        self._keys = keys
        self._stagger = stagger
        self._recat: torch.Tensor = _get_recat(local_split=splits[pg.rank()], num_splits=len(splits), stagger=stagger, device=device, batch_size_per_rank=batch_size_per_rank if len(set(batch_size_per_rank)) > 1 else None)
        if self._workers == 1:
            return
        self._output_tensors: List[torch.Tensor] = []
        self._awaitables: List[dist.Work] = []
        for input_split, output_split, input_tensor, label in zip(input_splits, output_splits, input_tensors, labels):
            output_tensor = torch.empty(sum(output_split), device=self._device, dtype=input_tensor.dtype)
            with record_function(f'## all2all_data:kjt {label} ##'):
                awaitable = dist.all_to_all_single(output=output_tensor, input=input_tensor, output_split_sizes=output_split, input_split_sizes=input_split, group=self._pg, async_op=True)
            self._output_tensors.append(output_tensor)
            self._awaitables.append(awaitable)

    def _wait_impl(self) ->KeyedJaggedTensor:
        """
        Overwrites wait function as we don't handle callbacks here.

        Returns:
            KeyedJaggedTensor: Synced KJT after AlltoAll.
        """
        if self._workers == 1:
            self._input.sync()
            return self._input
        for awaitable in self._awaitables:
            awaitable.wait()
        return type(self._input).dist_init(keys=self._keys, tensors=self._output_tensors, batch_size_per_rank=self._batch_size_per_rank, recat=self._recat)


class SplitsAllToAllAwaitable(Awaitable[List[List[int]]]):
    """
    Awaitable for splits AlltoAll.

    Args:
        input_tensors (List[torch.Tensor]): tensor of splits to redistribute.
        num_workers (int): total ranks.
        device (torch.device): device on which buffers are allocated.
        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.
    """

    def __init__(self, input_tensors: List[torch.Tensor], num_workers: int, device: torch.device, pg: dist.ProcessGroup) ->None:
        super().__init__()
        self.num_workers: int = num_workers
        with record_function('## all2all_data:kjt splits ##'):
            self._output_tensor: torch.Tensor = torch.empty([num_workers * len(input_tensors)], device=device, dtype=input_tensors[0].dtype)
            input_tensor = torch.stack(input_tensors, dim=1).flatten()
            self._splits_awaitable: dist.Work = dist.all_to_all_single(output=self._output_tensor, input=input_tensor, group=pg, async_op=True)

    def _wait_impl(self) ->List[List[int]]:
        self._splits_awaitable.wait()
        return self._output_tensor.view(self.num_workers, -1).T.tolist()


class KJTAllToAllSplitsAwaitable(Awaitable[KJTAllToAllTensorsAwaitable]):
    """
    Awaitable for KJT tensors splits AlltoAll.

    Args:
        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.
        input (KeyedJaggedTensor): input KJT.
        splits (List[int]): list of len(pg.size()) which indicates how many features to
            send to each pg.rank(). It is assumed the `KeyedJaggedTensor` is ordered by
            destination rank. Same for all ranks.
        tensor_splits (Dict[str, List[int]]): tensor splits provided by input KJT.
        input_tensors (List[torch.Tensor]): provided KJT tensors (ie. lengths, values)
            to redistribute according to splits.
        keys (List[str]): KJT keys after AlltoAll.
        device (torch.device): device on which buffers will be allocated.
        stagger (int): stagger value to apply to recat tensor.
    """

    def __init__(self, pg: dist.ProcessGroup, input: KeyedJaggedTensor, splits: List[int], labels: List[str], tensor_splits: List[List[int]], input_tensors: List[torch.Tensor], keys: List[str], device: torch.device, stagger: int) ->None:
        super().__init__()
        self._workers: int = pg.size()
        self._pg: dist.ProcessGroup = pg
        self._device: torch.device = device
        self._input = input
        self._splits = splits
        self._labels = labels
        self._input_splits = tensor_splits
        self._input_tensors = input_tensors
        self._keys = keys
        self._stagger = stagger
        self._output_splits: List[List[int]] = self._input_splits
        self._batch_size_per_rank: List[int] = [input.stride()]
        if self._workers == 1:
            return
        input_tensors = [torch.tensor(split, device=device) for split in self._input_splits]
        batch_size_tensor = torch.tensor([input.stride()] * self._workers, device=device)
        input_tensors.append(batch_size_tensor)
        self._splits_awaitable = SplitsAllToAllAwaitable(input_tensors, self._workers, self._device, self._pg)

    def _wait_impl(self) ->KJTAllToAllTensorsAwaitable:
        """
        Overwrites wait function as we don't handle callbacks here.

        Returns:
            KJTAllToAllTensorsAwaitable.
        """
        if self._workers > 1:
            output_list = self._splits_awaitable.wait()
            self._output_splits = output_list[:-1]
            self._batch_size_per_rank = output_list[-1]
        return KJTAllToAllTensorsAwaitable(pg=self._pg, input=self._input, splits=self._splits, input_splits=self._input_splits, output_splits=self._output_splits, input_tensors=self._input_tensors, labels=self._labels, batch_size_per_rank=self._batch_size_per_rank, keys=self._keys, device=self._device, stagger=self._stagger)


class KJTAllToAll(nn.Module):
    """
    Redistributes `KeyedJaggedTensor` to a `ProcessGroup` according to splits.

    Implementation utilizes AlltoAll collective as part of torch.distributed.

    The input provides the necessary tensors and input splits to distribute.
    The first collective call in `KJTAllToAllSplitsAwaitable` will transmit output
    splits (to allocate correct space for tensors) and batch size per rank. The
    following collective calls in `KJTAllToAllTensorsAwaitable` will transmit the actual
    tensors asynchronously.

    Args:
        pg (dist.ProcessGroup): ProcessGroup for AlltoAll communication.
        splits (List[int]): List of len(pg.size()) which indicates how many features to
            send to each pg.rank(). It is assumed the `KeyedJaggedTensor` is ordered by
            destination rank. Same for all ranks.
        device (Optional[torch.device]): device on which buffers will be allocated.
        stagger (int): stagger value to apply to recat tensor, see `_get_recat` function
            for more detail.

    Example::

        keys=['A','B','C']
        splits=[2,1]
        kjtA2A = KJTAllToAll(pg, splits, device)
        awaitable = kjtA2A(rank0_input)

        # where:
        # rank0_input is KeyedJaggedTensor holding

        #         0           1           2
        # 'A'    [A.V0]       None        [A.V1, A.V2]
        # 'B'    None         [B.V0]      [B.V1]
        # 'C'    [C.V0]       [C.V1]      None

        # rank1_input is KeyedJaggedTensor holding

        #         0           1           2
        # 'A'     [A.V3]      [A.V4]      None
        # 'B'     None        [B.V2]      [B.V3, B.V4]
        # 'C'     [C.V2]      [C.V3]      None

        rank0_output = awaitable.wait()

        # where:
        # rank0_output is KeyedJaggedTensor holding

        #         0           1           2           3           4           5
        # 'A'     [A.V0]      None      [A.V1, A.V2]  [A.V3]      [A.V4]      None
        # 'B'     None        [B.V0]    [B.V1]        None        [B.V2]      [B.V3, B.V4]

        # rank1_output is KeyedJaggedTensor holding
        #         0           1           2           3           4           5
        # 'C'     [C.V0]      [C.V1]      None        [C.V2]      [C.V3]      None
    """

    def __init__(self, pg: dist.ProcessGroup, splits: List[int], device: Optional[torch.device]=None, stagger: int=1) ->None:
        super().__init__()
        assert len(splits) == pg.size()
        self._pg: dist.ProcessGroup = pg
        self._workers: int = pg.size()
        self._splits = splits
        self._no_dist: bool = all(s == 0 for s in splits)
        self._splits_cumsum: List[int] = [0] + list(itertools.accumulate(splits))
        self._stagger = stagger

    def forward(self, input: KeyedJaggedTensor) ->Awaitable[KJTAllToAllTensorsAwaitable]:
        """
        Sends input to relevant `ProcessGroup` ranks.

        The first wait will get the output splits for the provided tensors and issue
        tensors AlltoAll. The second wait will get the tensors.

        Args:
            input (KeyedJaggedTensor): `KeyedJaggedTensor` of values to distribute.

        Returns:
            Awaitable[KJTAllToAllTensorsAwaitable]: awaitable of a `KJTAllToAllTensorsAwaitable`.
        """
        device = input.values().device
        with torch.no_grad():
            assert len(input.keys()) == sum(self._splits)
            rank = dist.get_rank(self._pg)
            local_keys = input.keys()[self._splits_cumsum[rank]:self._splits_cumsum[rank + 1]]
            return KJTAllToAllSplitsAwaitable(pg=self._pg, input=input, splits=self._splits, labels=input.dist_labels(), tensor_splits=input.dist_splits(self._splits), input_tensors=input.dist_tensors(), keys=local_keys, device=device, stagger=self._stagger)


class KJTList(Multistreamable):

    def __init__(self, features: List[KeyedJaggedTensor]) ->None:
        self.features = features

    def __len__(self) ->int:
        return len(self.features)

    def __setitem__(self, key: int, item: KeyedJaggedTensor) ->None:
        self.features[key] = item

    def __getitem__(self, key: int) ->KeyedJaggedTensor:
        return self.features[key]

    def __iter__(self) ->Iterator[KeyedJaggedTensor]:
        return iter(self.features)

    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        for feature in self.features:
            feature.record_stream(stream)

    def __fx_create_arg__(self, tracer: torch.fx.Tracer) ->fx.node.Argument:
        return tracer.create_node('call_function', KJTList, args=(tracer.create_arg(self.features),), kwargs={})


class NoWait(Awaitable[W]):

    def __init__(self, obj: W) ->None:
        super().__init__()
        self._obj = obj

    def _wait_impl(self) ->W:
        return self._obj


class KJTOneToAll(nn.Module):
    """
    Redistributes `KeyedJaggedTensor` to all devices.

    Implementation utilizes OnetoAll function, which essentially P2P copies the feature
    to the devices.

    Args:
        splits (List[int]): lengths of features to split the `KeyJaggedTensor` features
            into before copying them.
        world_size (int): number of devices in the topology.
    """

    def __init__(self, splits: List[int], world_size: int) ->None:
        super().__init__()
        self._splits = splits
        self._world_size = world_size
        assert self._world_size == len(splits)

    def forward(self, kjt: KeyedJaggedTensor) ->Awaitable[KJTList]:
        """
        Splits features first and then sends the slices to the corresponding devices.

        Args:
            kjt (KeyedJaggedTensor): the input features.

        Returns:
            Awaitable[List[KeyedJaggedTensor]]: awaitable of `KeyedJaggedTensor` splits.
        """
        kjts: List[KeyedJaggedTensor] = kjt.split(self._splits)
        dist_kjts = [split_kjt for rank, split_kjt in enumerate(kjts)]
        return NoWait(KJTList(dist_kjts))


class PooledEmbeddingsAwaitable(Awaitable[torch.Tensor]):
    """
    Awaitable for pooled embeddings after collective operation.

    Args:
        tensor_awaitable (Awaitable[torch.Tensor]): awaitable of concatenated tensors
            from all the processes in the group after collective.
    """

    def __init__(self, tensor_awaitable: Awaitable[torch.Tensor]) ->None:
        super().__init__()
        self._tensor_awaitable = tensor_awaitable

    def _wait_impl(self) ->torch.Tensor:
        """
        Syncs pooled embeddings after collective operation.

        Returns:
            torch.Tensor: synced pooled embeddings.
        """
        ret = self._tensor_awaitable.wait()
        return ret

    @property
    def callbacks(self) ->List[Callable[[torch.Tensor], torch.Tensor]]:
        return self._callbacks


QuantizationContext = TypeVar('QuantizationContext')


class NoOpQuantizedCommCodec(Generic[QuantizationContext]):
    """
    Default No-Op implementation of QuantizedCommCodec
    """

    def encode(self, input_tensor: torch.Tensor, ctx: Optional[QuantizationContext]=None) ->torch.Tensor:
        return input_tensor

    def decode(self, input_grad: torch.Tensor, ctx: Optional[QuantizationContext]=None) ->torch.Tensor:
        return input_grad

    def quantized_dtype(self) ->torch.dtype:
        return torch.float

    def calc_quantized_size(self, input_len: int, ctx: Optional[QuantizationContext]=None) ->int:
        return input_len

    def create_context(self) ->Optional[QuantizationContext]:
        return None


class QuantizedCommCodec(Generic[QuantizationContext]):
    """
    Provide an implementation to quantized, or apply mixed precision, to the tensors used in collective calls (pooled_all_to_all, reduce_scatter, etc).
    The dtype is the dtype of the tensor called from encode.

    This makes the assumption that the input tensor has type torch.float32

    >>>
        quantized_tensor = quantized_comm_codec.encode(input_tensor)
        quantized_tensor.dtype == quantized_comm_codec.quantized_dtype
        collective_call(output_tensors, input_tensors=tensor)
        output_tensor = decode(output_tensors)

        torch.assert_close(input_tensors, output_tensor)

    """

    def encode(self, input_tensor: torch.Tensor, ctx: Optional[QuantizationContext]=None) ->torch.Tensor:
        ...

    def decode(self, input_grad: torch.Tensor, ctx: Optional[QuantizationContext]=None) ->torch.Tensor:
        ...

    def quantized_dtype(self) ->torch.dtype:
        """
        tensor.dtype of the resultant encode(input_tensor)
        """
        ...

    def calc_quantized_size(self, input_len: int, ctx: Optional[QuantizationContext]=None) ->int:
        """
        Given the length of input tensor, returns the length of tensor after
        quantization. Used by INT8 codecs where the quantized tensor have
        some additional parameters. For other cases, the quantized tensor should
        have the same length with input.
        """
        ...

    def create_context(self) ->Optional[QuantizationContext]:
        """
        Create a context object that can be used to carry session-based
        parameters between encoder and decoder.
        """
        ...


class Request(Awaitable[W]):
    """
    Defines a collective operation request for a process group on a tensor.

    Args:
        pg (dist.ProcessGroup): The process group the request is for.
    """

    def __init__(self, pg: dist.ProcessGroup, device: torch.device) ->None:
        super().__init__()
        self.pg: dist.ProcessGroup = pg
        self.req: Optional[dist.Work] = None
        self.tensor: Optional[W] = None
        self.a2ai = None
        self.qcomm_ctx = None
        self.rsi = None
        self.agi = None
        self.wait_function = None
        self.dummy_tensor: torch.Tensor = torch.empty(1, requires_grad=True, device=device)

    def _wait_impl(self) ->W:
        """
        Calls the wait function for this request.
        """
        ret = self.wait_function.apply(self.pg, self, self.dummy_tensor)
        self.req = None
        self.tensor = None
        return ret


def _recat_pooled_embedding_grad_out(grad_output: Tensor, num_features_per_rank: List[int]) ->Tensor:
    grad_outputs_by_rank = grad_output.split(num_features_per_rank, dim=1)
    return torch.cat([grad_output_by_rank.contiguous().view(-1) for grad_output_by_rank in grad_outputs_by_rank], dim=0)


_T = TypeVar('_T')


def none_throws(optional: Optional[_T], message: str='Unexpected `None`') ->_T:
    """Convert an optional to its value. Raises an `AssertionError` if the
    value is `None`"""
    if optional is None:
        raise AssertionError(message)
    return optional


class All2All_Pooled_Wait(Function):

    @staticmethod
    def forward(ctx, pg: dist.ProcessGroup, myreq: Request[Tensor], *dummy_tensor: Tensor) ->Tensor:
        my_rank = dist.get_rank(pg)
        a2ai = myreq.a2ai
        ctx.a2ai = a2ai
        assert myreq.req is not None
        myreq.req.wait()
        sharded_output_embeddings = myreq.tensor
        myreq.req = None
        myreq.tensor = None
        ctx.pg = pg
        ctx.myreq = myreq
        dim_sum_per_rank = a2ai.dim_sum_per_rank
        batch_size_per_rank = a2ai.batch_size_per_rank
        B_local = batch_size_per_rank[my_rank]
        if a2ai.codecs is not None:
            codecs = none_throws(a2ai.codecs)
            sharded_output_embeddings = codecs.forward.decode(sharded_output_embeddings, myreq.qcomm_ctx)
        outputs_by_rank = sharded_output_embeddings.split([(B_local * D_rank_sum) for D_rank_sum in dim_sum_per_rank])
        result = torch.cat([output.view(B_local, -1) for output in outputs_by_rank], dim=1)
        return result

    @staticmethod
    def backward(ctx, grad_output: Tensor) ->Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        a2ai = ctx.a2ai
        pg = ctx.pg
        my_rank = dist.get_rank(pg)
        dim_sum_per_rank = a2ai.dim_sum_per_rank
        batch_size_per_rank = a2ai.batch_size_per_rank
        D_local_sum = dim_sum_per_rank[my_rank]
        B_local, D_global_sum = grad_output.shape
        assert sum(dim_sum_per_rank) == D_global_sum
        sharded_grad_output = _recat_pooled_embedding_grad_out(grad_output.contiguous(), dim_sum_per_rank)
        if a2ai.codecs is not None:
            codecs = none_throws(a2ai.codecs)
            qcomm_ctx = codecs.backward.create_context()
            sharded_grad_output = codecs.backward.encode(sharded_grad_output, qcomm_ctx)
            input_split_sizes = [codecs.backward.calc_quantized_size(B_local * D_rank_sum, qcomm_ctx) for D_rank_sum in dim_sum_per_rank]
            output_split_sizes = [codecs.backward.calc_quantized_size(D_local_sum * B_rank, qcomm_ctx) for B_rank in batch_size_per_rank]
        else:
            qcomm_ctx = None
            input_split_sizes = [(B_local * D_rank_sum) for D_rank_sum in dim_sum_per_rank]
            output_split_sizes = [(D_local_sum * B_rank) for B_rank in batch_size_per_rank]
        sharded_grad_input = torch.empty(sum(output_split_sizes), device=sharded_grad_output.device, dtype=sharded_grad_output.dtype)
        with record_function('## alltoall_bwd_single ##'):
            req = dist.all_to_all_single(output=sharded_grad_input, input=sharded_grad_output, output_split_sizes=output_split_sizes, input_split_sizes=input_split_sizes, group=pg, async_op=True)
        myreq.req = req
        myreq.tensor = sharded_grad_input
        myreq.qcomm_ctx = qcomm_ctx
        return None, None, myreq.dummy_tensor


class EmbeddingsAllToOne(nn.Module):
    """
    Merges the pooled/sequence embedding tensor on each device into single tensor.

    Args:
        device (torch.device): device on which buffer will be allocated.
        world_size (int): number of devices in the topology.
        cat_dim (int): which dimension you would like to concatenate on.
            For pooled embedding it is 1; for sequence embedding it is 0.
    """

    def __init__(self, device: torch.device, world_size: int, cat_dim: int) ->None:
        super().__init__()
        self._device = device
        self._world_size = world_size
        self._cat_dim = cat_dim

    def forward(self, tensors: List[torch.Tensor]) ->Awaitable[torch.Tensor]:
        """
        Performs AlltoOne operation on pooled/sequence embeddings tensors.

        Args:
            tensors (List[torch.Tensor]): list of embedding tensors.

        Returns:
            Awaitable[torch.Tensor]: awaitable of the merged embeddings.
        """
        assert len(tensors) == self._world_size
        non_cat_size = tensors[0].size(1 - self._cat_dim)
        return NoWait(torch.ops.fbgemm.merge_pooled_embeddings(tensors, non_cat_size, str(self._device), self._cat_dim))


class SeqEmbeddingsAllToOne(nn.Module):
    """
    Merges the pooled/sequence embedding tensor on each device into single tensor.

    Args:
        device (torch.device): device on which buffer will be allocated
        world_size (int): number of devices in the topology.
        cat_dim (int): which dimension you like to concate on.
            For pooled embedding it is 1; for sequence embedding it is 0.
    """

    def __init__(self, device: torch.device, world_size: int) ->None:
        super().__init__()
        self._device = device
        self._world_size = world_size

    def forward(self, tensors: List[torch.Tensor]) ->Awaitable[List[torch.Tensor]]:
        """
        Performs AlltoOne operation on pooled embeddings tensors.

        Args:
            tensors (List[torch.Tensor]): list of pooled embedding tensors.

        Returns:
            Awaitable[torch.Tensor]: awaitable of the merged pooled embeddings.
        """
        assert len(tensors) == self._world_size
        return NoWait(torch.ops.fbgemm.all_to_one_device(tensors, self._device))


class ReduceScatterBase_Wait(Function):

    @staticmethod
    def forward(ctx, pg: dist.ProcessGroup, myreq: Request[Tensor], *dummy_Tensor: Tensor) ->Tensor:
        assert myreq.req is not None
        myreq.req.wait()
        myreq.req = None
        output = myreq.tensor
        myreq.tensor = None
        ctx.myreq = myreq
        ctx.pg = pg
        rsi = myreq.rsi
        if rsi.codecs is not None:
            output = rsi.codecs.forward.decode(output)
        return output

    @staticmethod
    def backward(ctx, grad_output: Tensor) ->Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        rsi = myreq.rsi
        if rsi.codecs is not None:
            grad_output = rsi.codecs.backward.encode(grad_output)
        grad_inputs = grad_output.new_empty(rsi.input_sizes)
        with record_function('## reduce_scatter_base_bw (all_gather) ##'):
            req = dist._all_gather_base(grad_inputs, grad_output.contiguous(), group=ctx.pg, async_op=True)
        myreq.req = req
        myreq.tensor = grad_inputs
        return None, None, myreq.dummy_tensor


class ReduceScatterV_Wait(Function):

    @staticmethod
    def forward(ctx, pg: dist.ProcessGroup, myreq: Request[Tensor], *dummy_tensor: Tensor) ->Tensor:
        assert myreq.req is not None
        myreq.req.wait()
        myreq.req = None
        output: torch.Tensor = myreq.tensor
        myreq.tensor = None
        ctx.myreq = myreq
        ctx.pg = pg
        rsi = myreq.rsi
        if rsi.codecs is not None:
            output = rsi.codecs.forward.decode(output)
        return output

    @staticmethod
    def backward(ctx, grad_output: Tensor) ->Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        rsi = myreq.rsi
        if rsi.codecs is not None:
            grad_output = rsi.codecs.backward.encode(grad_output)
        grad_input = grad_output.new_empty(rsi.total_input_size)
        if rsi.equal_splits:
            with record_function('## reduce_scatter_base_bw (all_gather) ##'):
                req = dist._all_gather_base(grad_input, grad_output.contiguous(), group=ctx.pg, async_op=True)
        else:
            with record_function('## reduce_scatter_v_bw (all_gather_v) ##'):
                req = dist.all_gather(list(torch.split(grad_input, rsi.input_splits)), grad_output.contiguous(), group=ctx.pg, async_op=True)
        myreq.req = req
        myreq.tensor = grad_input
        return None, None, myreq.dummy_tensor


class AllGatherBase_Wait(Function):

    @staticmethod
    def forward(ctx, pg: dist.ProcessGroup, myreq: Request[Tensor], *dummy_tensor: Tensor) ->Tensor:
        assert myreq.req is not None
        myreq.req.wait()
        myreq.req = None
        outputs = myreq.tensor
        myreq.tensor = None
        ctx.myreq = myreq
        ctx.pg = pg
        agi = myreq.agi
        if agi.codecs is not None:
            outputs = agi.codecs.forward.decode(outputs)
        return outputs

    @staticmethod
    def backward(ctx, grad_outputs: Tensor) ->Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        agi = myreq.agi
        if agi.codecs is not None:
            grad_outputs = agi.codecs.backward.encode(grad_outputs)
        grad_input = grad_outputs.new_empty(agi.input_size)
        with record_function('## all_gather_base_bw (reduce_scatter) ##'):
            req = dist._reduce_scatter_base(grad_input, grad_outputs.contiguous(), group=ctx.pg, async_op=True)
        myreq.req = req
        myreq.tensor = grad_input
        return None, None, myreq.dummy_tensor


class SequenceEmbeddingsAwaitable(Awaitable[torch.Tensor]):
    """
    Awaitable for sequence embeddings after collective operation.

    Args:
        tensor_awaitable (Awaitable[torch.Tensor]): awaitable of concatenated tensors
            from all the processes in the group after collective.
        unbucketize_permute_tensor (Optional[torch.Tensor]): stores the permute order of
            KJT bucketize (for row-wise sharding only).
        embedding_dim (int): embedding dimension.
    """

    def __init__(self, tensor_awaitable: Awaitable[torch.Tensor], unbucketize_permute_tensor: Optional[torch.Tensor], embedding_dim: int) ->None:
        super().__init__()
        self._tensor_awaitable = tensor_awaitable
        self._unbucketize_permute_tensor = unbucketize_permute_tensor
        self._embedding_dim = embedding_dim
        if self._unbucketize_permute_tensor is not None:
            self.callbacks.append(lambda ret: torch.index_select(ret.view(-1, self._embedding_dim), 0, self._unbucketize_permute_tensor))

    def _wait_impl(self) ->torch.Tensor:
        """
        Syncs sequence embeddings after collective operation.

        Returns:
            torch.Tensor: synced sequence embeddings.
        """
        ret = self._tensor_awaitable.wait()
        return ret


class All2All_Seq_Req_Wait(Function):

    @staticmethod
    def forward(ctx, pg: dist.ProcessGroup, myreq: Request[Tensor], *dummy_tensor: torch.Tensor) ->Tensor:
        a2ai = myreq.a2ai
        D = a2ai.embedding_dim
        ctx.a2ai = a2ai
        assert myreq.req is not None
        myreq.req.wait()
        myreq.req = None
        sharded_output_embeddings = myreq.tensor
        myreq.tensor = None
        ctx.pg = pg
        ctx.myreq = myreq
        if a2ai.codecs is not None:
            codecs = none_throws(a2ai.codecs)
            sharded_output_embeddings = codecs.forward.decode(sharded_output_embeddings, myreq.qcomm_ctx)
        return sharded_output_embeddings.view(-1, D)

    @staticmethod
    def backward(ctx, sharded_grad_output: Tensor) ->Tuple[None, None, Tensor]:
        myreq = ctx.myreq
        a2ai = ctx.a2ai
        pg = ctx.pg
        input_splits = a2ai.output_splits
        output_splits = a2ai.input_splits
        if a2ai.codecs is not None:
            codecs = none_throws(a2ai.codecs)
            qcomm_ctx = codecs.backward.create_context()
            sharded_grad_output = a2ai.codecs.backward.encode(sharded_grad_output, qcomm_ctx)
            output_splits = [a2ai.codecs.backward.calc_quantized_size(x, qcomm_ctx) for x in output_splits]
            input_splits = [a2ai.codecs.backward.calc_quantized_size(x, qcomm_ctx) for x in input_splits]
        else:
            qcomm_ctx = None
        sharded_grad_input = torch.empty(sum(output_splits), device=sharded_grad_output.device, dtype=sharded_grad_output.dtype)
        with record_function('## alltoall_seq_embedding_bwd_single ##'):
            req = dist.all_to_all_single(output=sharded_grad_input, input=sharded_grad_output.view(-1), output_split_sizes=output_splits, input_split_sizes=input_splits, group=pg, async_op=True)
        myreq.req = req
        myreq.tensor = sharded_grad_input
        myreq.qcomm_ctx = qcomm_ctx
        return None, None, myreq.dummy_tensor


@unique
class EmbeddingComputeKernel(Enum):
    DENSE = 'dense'
    FUSED = 'fused'
    FUSED_UVM = 'fused_uvm'
    FUSED_UVM_CACHING = 'fused_uvm_caching'
    QUANT = 'quant'
    QUANT_UVM = 'quant_uvm'
    QUANT_UVM_CACHING = 'quant_uvm_caching'


C = TypeVar('C', bound=Multistreamable)


T = TypeVar('T')


class BaseEmbeddingDist(abc.ABC, nn.Module, Generic[C, T, W]):
    """
    Converts output of EmbeddingLookup from model-parallel to data-parallel.
    """

    @abc.abstractmethod
    def forward(self, local_embs: T, sharding_ctx: Optional[C]=None) ->Awaitable[W]:
        pass


class BaseGroupedFeatureProcessor(nn.Module):
    """
    Abstract base class for grouped feature processor
    """

    @abc.abstractmethod
    def forward(self, features: KeyedJaggedTensor) ->KeyedJaggedTensor:
        pass


def append_prefix(prefix: str, name: str) ->str:
    """
    Appends provided prefix to provided name.
    """
    if prefix != '' and name != '':
        return prefix + '.' + name
    else:
        return prefix + name


class GroupedPositionWeightedModule(BaseGroupedFeatureProcessor):

    def __init__(self, max_feature_lengths: Dict[str, int], device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.max_feature_lengths = max_feature_lengths
        for length in self.max_feature_lengths.values():
            if length <= 0:
                raise
        self.position_weights: nn.ParameterDict = nn.ParameterDict()
        for key, length in max_feature_lengths.items():
            self.position_weights[key] = nn.Parameter(torch.empty([length], device=device).fill_(1.0))
        self.register_buffer('_dummy_weights', torch.tensor(max(self.max_feature_lengths.values()), device=device).fill_(1.0))

    def forward(self, features: KeyedJaggedTensor) ->KeyedJaggedTensor:
        if features.weights_or_none() is None:
            cat_seq = torch.ops.fbgemm.offsets_range(features.offsets().long(), torch.numel(features.values()))
        else:
            cat_seq = features.weights().long()
        seqs = torch.split(cat_seq, features.length_per_key())
        weights_list = []
        for key, seq in zip(features.keys(), seqs):
            if key in self.max_feature_lengths:
                weights_list.append(torch.gather(self.position_weights[key], dim=0, index=seq))
            else:
                weights_list.append(self._dummy_weights[:self.max_feature_lengths[key]])
        weights = torch.cat(weights_list)
        return KeyedJaggedTensor(keys=features.keys(), values=features.values(), weights=weights, lengths=features.lengths(), offsets=features.offsets(), stride=features.stride(), length_per_key=features.length_per_key())

    def named_parameters(self, prefix: str='', recurse: bool=True, remove_duplicate: bool=True) ->Iterator[Tuple[str, nn.Parameter]]:
        for name, param in self.position_weights.items():
            yield append_prefix(prefix, f'position_weights.{name}'), param

    def named_buffers(self, prefix: str='', recurse: bool=True, remove_duplicate: bool=True) ->Iterator[Tuple[str, torch.Tensor]]:
        yield from ()

    def state_dict(self, destination: Optional[Dict[str, Any]]=None, prefix: str='', keep_vars: bool=False) ->Dict[str, Any]:
        if destination is None:
            destination = OrderedDict()
            destination._metadata = OrderedDict()
        for name, param in self.position_weights.items():
            destination[prefix + f'position_weights.{name}'] = param if keep_vars else param.detach()
        return destination


class KeyedOptimizer(optim.Optimizer):
    """
    Takes a dict of parameters and exposes state_dict by parameter key.

    This implementation is much stricter than the one in torch.Optimizer:
    it requires implementations to fully initialize their state during first optimization iteration,
    and it prohibits loading an empty state into already initialized KeyedOptimizer and vise versa.

    It also doesn't expose param_groups in state_dict() by default
    Old behavior can be switch on by setting save_param_groups flag.
    The reason is that during distributed training not all parameters are present on all ranks
    and we identify param_group by its parameters.
    In addition to that, param_groups are typically re-set during training initialization,
    so it makes little sense to save them as a part of the state to begin with.
    """

    def __init__(self, params: Mapping[str, Union[torch.Tensor, ShardedTensor]], state: Mapping[Any, Any], param_groups: Collection[Mapping[str, Any]]) ->None:
        torch._C._log_api_usage_once(f'torchrec.optim.{self.__class__.__name__}')
        self._optimizer_step_pre_hooks: Dict[int, Callable] = OrderedDict()
        self._optimizer_step_post_hooks: Dict[int, Callable] = OrderedDict()
        self.state: Mapping[Any, Any] = state
        self.param_groups: Collection[Mapping[str, Any]] = param_groups
        self.params = params
        self.defaults: Dict[str, Any] = {'_save_param_groups': False}
        params_set = set(params.values())
        non_param_state_keys = [key for key in self.state if key not in params_set]
        if len(non_param_state_keys) > 0:
            raise ValueError('All state keys must be params. The following keys are not: {}.'.format(non_param_state_keys))

    def state_dict(self) ->Dict[str, Any]:
        """
        Returned state and param_groups will contain parameter keys
        instead of parameter indices in torch.Optimizer.
        This allows for advanced functionality like optimizer re-sharding to be implemented.

        Can also handle classes and supported data structures that follow the PyTorch stateful
        protocol.
        """
        state = self.state
        param_groups = self.param_groups
        params = self.params
        param_to_key = {param: key for key, param in params.items()}
        ret_state = {}
        for param, state_val in state.items():
            if isinstance(state_val, dict):
                ret_state[param_to_key[param]] = {}
                for k, v in state_val.items():
                    if hasattr(v, 'state_dict') and callable(v.state_dict):
                        ret_state[param_to_key[param]][k] = v.state_dict()
                    else:
                        ret_state[param_to_key[param]][k] = v
            else:
                ret_state[param_to_key[param]] = state_val
        ret_groups = []
        for group in param_groups:
            param_keys = []
            for param in group['params']:
                param_keys.append(param_to_key[param])
            ret_group = {'params': sorted(param_keys)}
            for k, v in group.items():
                if k != 'params':
                    ret_group[k] = deepcopy(v)
            ret_groups.append(ret_group)
        ret: Dict[str, object] = {'state': ret_state}
        if self.defaults['_save_param_groups']:
            ret['param_groups'] = ret_groups
        return ret

    def post_load_state_dict(self) ->None:
        pass

    def load_state_dict(self, state_dict: Mapping[str, Any]) ->None:
        """
        This implementation is much stricter than the one in torch.Optimizer:
        it requires implementations to fully initialize their state during first optimization iteration,
        and it prohibits loading an empty state into already initialized KeyedOptimizer and vise versa.

        Because of introduced strictness it allows us to:
            * do compatibility checks for state and param_groups, which improves usability
            * avoid state duplication by directly copying into state tensors, e.g.
              optimizer.step()  # make sure optimizer is initialized
              sd = optimizer.state_dict()
              load_checkpoint(sd)  # copy state directly into tensors, re-shard if needed
              optimizer.load_state_dict(sd)  # replace param_groups
        """
        new_state = state_dict['state']
        state = self.state
        params = self.params
        if len(state) != len(new_state):
            raise ValueError(f'Different parameter count: {len(state)} vs {len(new_state)}')
        for param_key, param in params.items():
            if param not in state:
                continue
            if param_key not in new_state:
                raise ValueError(f'Parameter {param_key} not found')
            if len(state[param]) != len(new_state[param_key]):
                raise ValueError(f'Different state size: {len(state[param])} vs {len(new_state[param_key])}')
            for state_key, state_val in state[param].items():
                if state_key not in new_state[param_key]:
                    raise ValueError(f'State key {state_key} not found for param {param_key}')
                new_state_val = new_state[param_key][state_key]
                if isinstance(state_val, ShardedTensor):
                    assert isinstance(new_state_val, ShardedTensor)
                    num_shards = len(state_val.local_shards())
                    num_new_shards = len(new_state_val.local_shards())
                    if num_shards != num_new_shards:
                        raise ValueError(f'Different number of shards {num_shards} vs {num_new_shards} for {param_key}/{state_key}')
                    for shard, new_shard in zip(state_val.local_shards(), new_state_val.local_shards()):
                        shard.tensor.detach().copy_(new_shard.tensor)
                elif isinstance(state_val, torch.Tensor):
                    assert isinstance(new_state_val, torch.Tensor)
                    state_val.detach().copy_(new_state_val)
                elif hasattr(state_val, 'load_state_dict') and callable(state_val.load_state_dict):
                    state_val.load_state_dict(new_state_val)
                else:
                    state[param][state_key] = deepcopy(new_state_val)
        if self.defaults['_save_param_groups']:
            new_param_groups = state_dict['param_groups']
            param_groups = self.param_groups
            if len(param_groups) != len(new_param_groups):
                raise ValueError(f'Different param_groups count: {len(param_groups)} vs {len(new_param_groups)}')
            param_to_key = {param: key for key, param in params.items()}
            group_map = {}
            for group in param_groups:
                param_keys = []
                for param in group['params']:
                    param_keys.append(param_to_key[param])
                group_map['/'.join(sorted(param_keys))] = group
            new_group_map = {}
            for new_group in new_param_groups:
                param_keys = []
                for param_key in new_group['params']:
                    param_keys.append(param_key)
                new_group_map['/'.join(sorted(param_keys))] = new_group
            for group_key, group in group_map.items():
                if group_key not in new_group_map:
                    raise ValueError(f'Group {group_key} not found')
                new_group = new_group_map[group_key]
                if len(group) != len(new_group):
                    raise ValueError(f'Different param_group size: {len(group)} vs {len(new_group)}')
                for k in group:
                    if k not in new_group:
                        raise ValueError(f'Group key {k} not found for group {group_key}')
                    if k != 'params':
                        group[k] = deepcopy(new_group[k])
        self.post_load_state_dict()

    def add_param_group(self, param_group: Any) ->None:
        raise NotImplementedError()

    def init_state(self, sparse_grad_parameter_names: Optional[Set[str]]=None) ->None:
        """
        Runs a dummy optimizer step, which allows to initialize
        optimizer state, which is typically lazy.
        This allows us to do in-place loading of optimizer state from a checkpoint.
        """
        for key, param in self.params.items():
            if param.requires_grad:
                t = torch.zeros_like(param)
                if sparse_grad_parameter_names is not None and key in sparse_grad_parameter_names:
                    t = t.to_sparse()
                param.grad = torch.autograd.Variable(t)
        self.step(closure=None)

    def save_param_groups(self, save: bool) ->None:
        self.defaults['_save_param_groups'] = save

    def __getstate__(self) ->Dict[str, Any]:
        return self.__dict__


class CombinedOptimizer(KeyedOptimizer):
    """
    Combines multiple KeyedOptimizers into one.

    Meant to combine different optimizers for different submodules
    """

    def __init__(self, optims: List[Union[KeyedOptimizer, Tuple[str, KeyedOptimizer]]]) ->None:
        self.defaults: Dict[str, Any] = {}
        self._optims: List[Tuple[str, KeyedOptimizer]] = []
        for key_value in optims:
            if isinstance(key_value, KeyedOptimizer):
                key_value = '', key_value
            self._optims.append(key_value)
        all_keys: Set[str] = set()
        self.defaults['_save_param_groups'] = False if len(self._optims) == 0 else self._optims[0][1].defaults['_save_param_groups']
        for opt_key, opt in self._optims:
            assert self.defaults['_save_param_groups'] == opt.defaults['_save_param_groups']
            for param_key in opt.params.keys():
                new_param = CombinedOptimizer.prepend_opt_key(param_key, opt_key)
                if new_param in all_keys:
                    raise ValueError(f'Duplicate param key {new_param}')
                all_keys.add(new_param)

    def __repr__(self) ->str:
        ret = []
        for key, opt in self._optims:
            ret.append(f'{key}: {opt.__repr__()}')
        return ','.join(ret)

    def zero_grad(self, set_to_none: bool=False) ->None:
        for _, opt in self._optims:
            opt.zero_grad(set_to_none=set_to_none)

    def step(self, closure: Any=None) ->None:
        for _, opt in self._optims:
            opt.step(closure=closure)

    @property
    def optimizers(self) ->List[Tuple[str, KeyedOptimizer]]:
        return self._optims

    @staticmethod
    def prepend_opt_key(name: str, opt_key: str) ->str:
        if not name:
            return opt_key
        return opt_key + ('.' if opt_key else '') + name

    @property
    def param_groups(self) ->Collection[Mapping[str, Any]]:
        return [param_group for _, opt in self._optims for param_group in opt.param_groups]

    @property
    def params(self) ->Mapping[str, Union[torch.Tensor, ShardedTensor]]:
        ret = {}
        for opt_key, opt in self._optims:
            for param_key, param in opt.params.items():
                ret[CombinedOptimizer.prepend_opt_key(param_key, opt_key)] = param
        return ret

    @property
    def state(self) ->Mapping[torch.Tensor, Any]:
        ret = {}
        for _, opt in self._optims:
            for param, state in opt.state.items():
                ret[param] = state
        return ret

    def post_load_state_dict(self) ->None:
        for _, opt in self._optims:
            opt.post_load_state_dict()

    def save_param_groups(self, save: bool) ->None:
        self.defaults['_save_param_groups'] = save
        for _, opt in self._optims:
            opt.save_param_groups(save)


class ShardingEnv:
    """
    Provides an abstraction over `torch.distributed.ProcessGroup`, which practically
    enables `DistributedModelParallel` to be used during inference.
    """

    def __init__(self, world_size: int, rank: int, pg: Optional[dist.ProcessGroup]=None) ->None:
        self.world_size = world_size
        self.rank = rank
        self.process_group: Optional[dist.ProcessGroup] = pg

    @classmethod
    def from_process_group(cls, pg: dist.ProcessGroup) ->'ShardingEnv':
        """
        Creates ProcessGroup-based sharding environment.

        NOTE:
            Typically used during training.
        """
        return cls(dist.get_world_size(pg), dist.get_rank(pg), pg)

    @classmethod
    def from_local(cls, world_size: int, rank: int) ->'ShardingEnv':
        """
        Creates a local host-based sharding environment.

        NOTE:
            Typically used during single host inference.
        """
        return cls(world_size, rank, None)


class DataParallelWrapper(abc.ABC):
    """
    Interface implemented by custom data parallel wrappers.
    """

    @abc.abstractmethod
    def wrap(self, dmp: 'DistributedModelParallel', env: ShardingEnv, device: torch.device) ->None:
        pass


class DefaultDataParallelWrapper(DataParallelWrapper):
    """
    Default data parallel wrapper, which applies data parallel to all unsharded modules.
    """

    def wrap(self, dmp: 'DistributedModelParallel', env: ShardingEnv, device: torch.device) ->None:
        if isinstance(dmp._dmp_wrapped_module, DistributedDataParallel) or isinstance(dmp._dmp_wrapped_module, FullyShardedDataParallel):
            return
        pg = env.process_group
        if pg is None:
            raise RuntimeError('Can only init DDP for ProcessGroup-based ShardingEnv')
        sharded_parameter_names = set(DistributedModelParallel._sharded_parameter_names(dmp._dmp_wrapped_module))
        all_parameter_names = set(dict(dmp.named_parameters()).keys())
        if len(all_parameter_names - sharded_parameter_names) == 0:
            return
        DistributedDataParallel._set_params_and_buffers_to_ignore_for_model(module=dmp._dmp_wrapped_module, params_and_buffers_to_ignore=sharded_parameter_names)
        dmp._dmp_wrapped_module = cast(nn.Module, DistributedDataParallel(module=dmp._dmp_wrapped_module, device_ids=None if device.type == 'cpu' else [device], process_group=pg, gradient_as_bucket_view=True, broadcast_buffers=False, static_graph=True))


BATCH_SIZE = 32


@unique
class ComputeKernel(Enum):
    DEFAULT = 'default'


M = TypeVar('M', bound=nn.Module)


class ParameterStorage(Enum):
    """
    Well-known physical resources, which can be used as constraints by ShardingPlanner.
    """
    HBM = 'hbm'
    DDR = 'ddr'


CompIn = TypeVar('CompIn', bound=Multistreamable)


DistOut = TypeVar('DistOut')


Out = TypeVar('Out')


ShrdCtx = TypeVar('ShrdCtx', bound=Multistreamable)


class ShardingType(Enum):
    """
    Well-known sharding types, used by inter-module optimizations.
    """
    DATA_PARALLEL = 'data_parallel'
    TABLE_WISE = 'table_wise'
    COLUMN_WISE = 'column_wise'
    ROW_WISE = 'row_wise'
    TABLE_ROW_WISE = 'table_row_wise'
    TABLE_COLUMN_WISE = 'table_column_wise'


class PlannerErrorType(Enum):
    """
    Classify PlannerError based on the following cases.
    """
    INSUFFICIENT_STORAGE = 'insufficient_storage'
    STRICT_CONSTRAINTS = 'strict_constraints'
    PARTITION = 'partition'
    OTHER = 'other'


class PlannerError(Exception):

    def __init__(self, message: str, error_type: PlannerErrorType=PlannerErrorType.OTHER) ->None:
        self.error_type = error_type
        super().__init__(message)


def _get_dp_sharding_perf(batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], grad_num_elem: int, emb_dim: int, input_data_type_size: float, output_data_type_size: float, num_poolings: List[float], device_bw: float, bw_inter_host: float, is_pooled: bool, is_weighted: bool=False, has_feature_processor: bool=False) ->float:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_read_size = math.ceil(batch_inputs * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size = batch_inputs * emb_dim * output_data_type_size
    output_write_size = batch_outputs * emb_dim * output_data_type_size
    table_size = grad_num_elem * output_data_type_size
    fwd_compute = (input_read_size + embedding_lookup_size + output_write_size) / device_bw
    num_nodes = min(world_size / local_world_size, 2)
    all_reduce = table_size * (2 * num_nodes - 1) / num_nodes / (bw_inter_host * local_world_size)
    if world_size > 2 * local_world_size:
        all_reduce *= 2
    optimizer_kernels = table_size * DP_ELEMENTWISE_KERNELS_PERF_FACTOR / device_bw
    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER
    bwd_grad_indice_weights_kernel = fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted or has_feature_processor else 0
    return all_reduce + optimizer_kernels + bwd_grad_indice_weights_kernel + bwd_compute + fwd_compute


def _get_rw_sharding_perf(batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], emb_dim: int, input_data_type_size: float, output_data_type_size: float, num_poolings: List[float], device_bw: float, bw_inter_host: float, bw_intra_host: float, is_pooled: bool, is_weighted: bool=False, has_feature_processor: bool=False) ->float:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]) / world_size
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size = batch_inputs * world_size * emb_dim * output_data_type_size
    output_write_size = batch_outputs * world_size * emb_dim * output_data_type_size
    comms_bw = bw_inter_host if world_size > local_world_size else bw_intra_host
    fwd_comms = output_write_size / comms_bw
    fwd_compute = (input_read_size + embedding_lookup_size + output_write_size) / device_bw
    bwd_comms = fwd_comms
    bwd_batched_copy = output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw
    bwd_grad_indice_weights_kernel = fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted or has_feature_processor else 0
    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER
    return bwd_comms + bwd_batched_copy + bwd_grad_indice_weights_kernel + bwd_compute + fwd_compute + fwd_comms


def _get_tw_sharding_perf(batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], emb_dim: int, input_data_type_size: float, output_data_type_size: float, num_poolings: List[float], device_bw: float, bw_inter_host: float, bw_intra_host: float, is_pooled: bool, is_weighted: bool=False, is_inference: bool=False, has_feature_processor: bool=False) ->float:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size = batch_inputs * world_size * max(emb_dim, 32) * output_data_type_size
    output_write_size = batch_outputs * world_size * emb_dim * output_data_type_size
    block_usage_penalty = 1
    if emb_dim < FULL_BLOCK_EMB_DIM:
        if emb_dim >= 64:
            block_usage_penalty = HALF_BLOCK_PENALTY
        else:
            block_usage_penalty = QUARTER_BLOCK_PENALTY
    comms_bw = bw_inter_host if world_size > local_world_size else bw_intra_host
    fwd_comms = output_write_size / comms_bw
    fwd_compute = (input_read_size + embedding_lookup_size + output_write_size) * block_usage_penalty / device_bw
    if is_inference:
        return fwd_compute + fwd_comms
    bwd_comms = fwd_comms
    bwd_grad_indice_weights_kernel = fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted or has_feature_processor else 0
    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER
    return bwd_comms + bwd_grad_indice_weights_kernel + bwd_compute + fwd_compute + fwd_comms


def _get_twrw_sharding_perf(batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], emb_dim: int, input_data_type_size: float, output_data_type_size: float, num_poolings: List[float], device_bw: float, bw_inter_host: float, bw_intra_host: float, is_pooled: bool, is_weighted: bool=False, has_feature_processor: bool=False) ->float:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]) / local_world_size
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)
    if is_weighted or has_feature_processor:
        input_read_size *= 2
    embedding_lookup_size = batch_inputs * world_size * emb_dim * output_data_type_size
    output_write_size = batch_outputs * world_size * emb_dim * output_data_type_size
    fwd_comms = output_write_size / bw_intra_host
    if world_size > local_world_size:
        fwd_comms += output_write_size * (local_world_size / world_size) / bw_inter_host
    fwd_compute = (input_read_size + embedding_lookup_size + output_write_size) / device_bw
    bwd_comms = fwd_comms
    bwd_grad_indice_weights_kernel = fwd_compute * WEIGHTED_KERNEL_MULTIPLIER if is_weighted or has_feature_processor else 0
    bwd_batched_copy = output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw
    bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER
    return bwd_comms + bwd_batched_copy + bwd_grad_indice_weights_kernel + bwd_compute + fwd_compute + fwd_comms


def kernel_bw_lookup(compute_device: str, compute_kernel: str, caching_ratio: Optional[float]=None) ->Optional[float]:
    """
    Calculates the device bandwidth based on given compute device, compute kernel, and
    caching ratio.

    Args:
        compute_kernel (str): compute kernel.
        compute_device (str): compute device.
        caching_ratio (Optional[float]): caching ratio used to determine device bandwidth
            if UVM caching is enabled.

    Returns:
        float: the device bandwidth.
    """
    caching_ratio = caching_ratio if caching_ratio else UVM_CACHING_RATIO
    lookup = {('cpu', EmbeddingComputeKernel.DENSE.value): 0.5 * DDR_MEM_BW, ('cpu', EmbeddingComputeKernel.FUSED.value): 1 * DDR_MEM_BW, ('cpu', EmbeddingComputeKernel.QUANT.value): 1 * DDR_MEM_BW, ('cuda', EmbeddingComputeKernel.DENSE.value): 0.5 * HBM_MEM_BW, ('cuda', EmbeddingComputeKernel.FUSED.value): 1 * HBM_MEM_BW, ('cuda', EmbeddingComputeKernel.FUSED_UVM.value): DDR_MEM_BW / 10, ('cuda', EmbeddingComputeKernel.FUSED_UVM_CACHING.value): (caching_ratio * HBM_MEM_BW + (1 - caching_ratio) * DDR_MEM_BW) / 10, ('cuda', EmbeddingComputeKernel.QUANT.value): 1 * HBM_MEM_BW, ('cuda', EmbeddingComputeKernel.QUANT_UVM.value): DDR_MEM_BW / 10, ('cuda', EmbeddingComputeKernel.QUANT_UVM_CACHING.value): (caching_ratio * HBM_MEM_BW + (1 - caching_ratio) * DDR_MEM_BW) / 10}
    return lookup.get((compute_device, compute_kernel))


def perf_func_emb_wall_time(shard_sizes: List[List[int]], compute_kernel: str, compute_device: str, sharding_type: str, batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], input_data_type_size: float, output_data_type_size: float, num_poolings: List[float], bw_intra_host: float, bw_inter_host: float, is_pooled: bool, is_weighted: bool=False, has_feature_processor: bool=False, caching_ratio: Optional[float]=None, is_inference: bool=False) ->List[float]:
    """
    Attempts to model perfs as a function of relative wall times.

    Args:
        shard_sizes (List[List[int]]): the list of (local_rows, local_cols) of each
            shard.
        compute_kernel (str): compute kernel.
        compute_device (str): compute device.
        sharding_type (str): tw, rw, cw, twrw, dp.
        batch_sizes (List[int]): batch size for each input feature.
        world_size (int): the number of devices for all hosts.
        local_world_size (int): the number of the device for each host.
        input_lengths (List[float]): the list of the average number of lookups of each
            input query feature.
        input_data_type_size (float): the data type size of the distributed
            data_parallel input.
        output_data_type_size (float): the data type size of the distributed
            data_parallel output.
        num_poolings (List[float]): number of poolings per sample, typically 1.0.
        bw_intra_host (float): the bandwidth within a single host like multiple threads.
        bw_inter_host (float): the bandwidth between two hosts like multiple machines.
        is_pooled (bool): True if embedding output is pooled (ie. EmbeddingBag), False
            if unpooled/sequential (ie. Embedding).
        is_weighted (bool = False): if the module is an EBC and is weighted, typically
            signifying an id score list feature.
        is_inference (bool = False): if planning for inference.
        has_feature_processor (bool = False): if the module has a feature processor.
        caching_ratio (Optional[float] = None): cache ratio to determine the bandwidth
            of device.

    Returns:
        List[float]: the list of perf for each shard.
    """
    shard_perfs = []
    device_bw = kernel_bw_lookup(compute_device, compute_kernel, caching_ratio)
    if device_bw is None:
        raise PlannerError(f'No kernel bandwidth exists for this combo of compute device: {compute_device}, compute kernel: {compute_kernel}')
    for hash_size, emb_dim in shard_sizes:
        if sharding_type == ShardingType.TABLE_WISE.value or sharding_type == ShardingType.COLUMN_WISE.value or sharding_type == ShardingType.TABLE_COLUMN_WISE.value:
            shard_perf = _get_tw_sharding_perf(batch_sizes=batch_sizes, world_size=world_size, local_world_size=local_world_size, input_lengths=input_lengths, emb_dim=emb_dim, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, device_bw=device_bw, bw_inter_host=bw_inter_host, bw_intra_host=bw_intra_host, is_pooled=is_pooled, is_weighted=is_weighted, is_inference=is_inference, has_feature_processor=has_feature_processor)
        elif sharding_type == ShardingType.ROW_WISE.value:
            shard_perf = _get_rw_sharding_perf(batch_sizes=batch_sizes, world_size=world_size, local_world_size=local_world_size, input_lengths=input_lengths, emb_dim=emb_dim, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, device_bw=device_bw, bw_inter_host=bw_inter_host, bw_intra_host=bw_intra_host, is_pooled=is_pooled, is_weighted=is_weighted, has_feature_processor=has_feature_processor)
        elif sharding_type == ShardingType.TABLE_ROW_WISE.value:
            shard_perf = _get_twrw_sharding_perf(batch_sizes=batch_sizes, world_size=world_size, local_world_size=local_world_size, input_lengths=input_lengths, emb_dim=emb_dim, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, device_bw=device_bw, bw_inter_host=bw_inter_host, bw_intra_host=bw_intra_host, is_pooled=is_pooled, is_weighted=is_weighted, has_feature_processor=has_feature_processor)
        elif sharding_type == ShardingType.DATA_PARALLEL.value:
            shard_perf = _get_dp_sharding_perf(batch_sizes=batch_sizes, world_size=world_size, local_world_size=local_world_size, input_lengths=input_lengths, grad_num_elem=hash_size * emb_dim, emb_dim=emb_dim, input_data_type_size=output_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, device_bw=device_bw, bw_inter_host=bw_inter_host, is_pooled=is_pooled, is_weighted=is_weighted, has_feature_processor=has_feature_processor)
        else:
            raise ValueError(f'Unrecognized or unsupported sharding type provided: {sharding_type}')
        shard_perfs.append(shard_perf)
    return shard_perfs


def sharder_name(t: Type[Any]) ->str:
    return t.__module__ + '.' + t.__name__


def _calculate_cw_shard_io_sizes(batch_sizes: List[int], world_size: int, input_lengths: List[float], shard_sizes: List[List[int]], input_data_type_size: int, output_data_type_size: int, num_poolings: List[float], is_pooled: bool) ->Tuple[List[int], List[int]]:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_sizes = [math.ceil(batch_inputs * world_size * input_data_type_size)] * len(shard_sizes)
    output_sizes = [math.ceil(batch_outputs * world_size * shard_sizes[i][1] * output_data_type_size) for i in range(len(shard_sizes))]
    return input_sizes, output_sizes


def _calculate_dp_shard_io_sizes(batch_sizes: List[int], input_lengths: List[float], emb_dim: int, num_shards: int, input_data_type_size: int, output_data_type_size: int, num_poolings: List[float], is_pooled: bool) ->Tuple[List[int], List[int]]:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_sizes = [math.ceil(batch_inputs * input_data_type_size)] * num_shards
    output_sizes = [math.ceil(batch_outputs * emb_dim * output_data_type_size)] * num_shards
    return input_sizes, output_sizes


def prod(iterable: Iterable[int]) ->int:
    return reduce(operator.mul, iterable, 1)


def _calculate_rw_shard_io_sizes(batch_sizes: List[int], world_size: int, input_lengths: List[float], shard_sizes: List[List[int]], input_data_type_size: int, output_data_type_size: int, num_poolings: List[float], is_pooled: bool) ->Tuple[List[int], List[int]]:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]) / world_size
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_sizes = [(math.ceil(batch_inputs * world_size * input_data_type_size) if prod(shard) != 0 else 0) for shard in shard_sizes]
    output_sizes = [(math.ceil(batch_outputs * world_size * shard_sizes[i][1] * output_data_type_size) if prod(shard) != 0 else 0) for i, shard in enumerate(shard_sizes)]
    return input_sizes, output_sizes


def _calculate_tw_shard_io_sizes(batch_sizes: List[int], world_size: int, input_lengths: List[float], emb_dim: int, input_data_type_size: int, output_data_type_size: int, num_poolings: List[float], is_pooled: bool) ->Tuple[List[int], List[int]]:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)])
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_sizes = [math.ceil(batch_inputs * world_size * input_data_type_size)]
    output_sizes = [math.ceil(batch_outputs * world_size * emb_dim * output_data_type_size)]
    return input_sizes, output_sizes


def _calculate_twrw_shard_io_sizes(batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], shard_sizes: List[List[int]], input_data_type_size: int, output_data_type_size: int, num_poolings: List[float], is_pooled: bool) ->Tuple[List[int], List[int]]:
    batch_inputs = sum([(x * y * z) for x, y, z in zip(input_lengths, num_poolings, batch_sizes)]) / local_world_size
    batch_outputs = sum([(x * y) for x, y in zip(num_poolings, batch_sizes)]) if is_pooled else batch_inputs
    input_sizes = [(math.ceil(batch_inputs * world_size * input_data_type_size) if prod(shard) != 0 else 0) for shard in shard_sizes]
    output_sizes = [(math.ceil(batch_outputs * world_size * shard_sizes[i][1] * output_data_type_size) if prod(shard) != 0 else 0) for i, shard in enumerate(shard_sizes)]
    return input_sizes, output_sizes


def _calculate_shard_io_sizes(sharding_type: str, batch_sizes: List[int], world_size: int, local_world_size: int, input_lengths: List[float], emb_dim: int, shard_sizes: List[List[int]], input_data_type_size: int, output_data_type_size: int, num_poolings: List[float], is_pooled: bool) ->Tuple[List[int], List[int]]:
    if sharding_type == ShardingType.DATA_PARALLEL.value:
        return _calculate_dp_shard_io_sizes(batch_sizes=batch_sizes, input_lengths=input_lengths, emb_dim=emb_dim, num_shards=len(shard_sizes), input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, is_pooled=is_pooled)
    elif sharding_type == ShardingType.TABLE_WISE.value:
        return _calculate_tw_shard_io_sizes(batch_sizes=batch_sizes, world_size=world_size, input_lengths=input_lengths, emb_dim=emb_dim, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, is_pooled=is_pooled)
    elif sharding_type in {ShardingType.COLUMN_WISE.value, ShardingType.TABLE_COLUMN_WISE.value}:
        return _calculate_cw_shard_io_sizes(batch_sizes=batch_sizes, world_size=world_size, input_lengths=input_lengths, shard_sizes=shard_sizes, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, is_pooled=is_pooled)
    elif sharding_type == ShardingType.ROW_WISE.value:
        return _calculate_rw_shard_io_sizes(batch_sizes=batch_sizes, world_size=world_size, input_lengths=input_lengths, shard_sizes=shard_sizes, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, is_pooled=is_pooled)
    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:
        return _calculate_twrw_shard_io_sizes(batch_sizes=batch_sizes, world_size=world_size, local_world_size=local_world_size, input_lengths=input_lengths, shard_sizes=shard_sizes, input_data_type_size=input_data_type_size, output_data_type_size=output_data_type_size, num_poolings=num_poolings, is_pooled=is_pooled)
    else:
        raise ValueError(f'Unrecognized or unsupported sharding type provided: {sharding_type}')


def _get_optimizer_multipler(optimizer_class: Optional[Type[torch.optim.Optimizer]], shape: torch.Size) ->float:
    if not optimizer_class:
        return 0.0
    if optimizer_class in [torch.optim.SGD, trec_optim.SGD]:
        return 0
    elif optimizer_class in [torch.optim.Adam, trec_optim.Adam]:
        return 2
    elif optimizer_class == trec_optim.RowWiseAdagrad:
        return 1 / shape[-1]
    else:
        return 1


def _calculate_storage_specific_sizes(storage: int, shape: torch.Size, shard_sizes: List[List[int]], sharding_type: str, optimizer_class: Optional[Type[torch.optim.Optimizer]]=None) ->List[int]:
    tensor_sizes: List[int] = [(math.ceil(storage * prod(size) / prod(shape)) if sharding_type != ShardingType.DATA_PARALLEL.value else storage) for size in shard_sizes]
    optimizer_multipler: float = _get_optimizer_multipler(optimizer_class, shape)
    optimizer_sizes: List[int] = [math.ceil(tensor_size * optimizer_multipler) for tensor_size in tensor_sizes]
    return [(tensor_size + optimizer_size) for tensor_size, optimizer_size in zip(tensor_sizes, optimizer_sizes)]


class EmbeddingTower(nn.Module):
    """
    Logical "Tower" of embeddings directly passed to provided interaction.
    All TorchRec shardable embedding modules are supported.

    Args:
        embedding_module (nn.Module):
        interaction_module (nn.Module):
        device (Optional[torch.device]):

    Example::

        ebc, interaction = EmbeddingBagCollection(), MyInteractionModule()
        tower = EmbeddingTower(ebc, interaction, device)
        kjt = KeyedJaggedTensor()
        output = tower(kjt)
    """

    def __init__(self, embedding_module: nn.Module, interaction_module: nn.Module, device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.embedding = embedding_module
        self.interaction = interaction_module

    def forward(self, *args, **kwargs) ->torch.Tensor:
        """
        Executes the embedding module and interaction module.

        Args:
            *args (Any): user provided positional arguments.
            **kwargs (Any): user provided keyword arguments.

        Returns:
            torch.Tensor: 2-D tensor of shape of `N X B`, where `B` is local batch size.
        """
        embeddings = self.embedding(*args, **kwargs)
        return self.interaction(embeddings)


def tower_input_params(module: nn.Module) ->Tuple[bool, bool]:
    """
    Utilty to compute the mapping of tower KJT args to pass to the embedding modules.

    Args:
        module (nn.Module):
    Returns:
        Tuple[bool, bool]: tuple of 2 booleans representing if KJT and weighted KJT are required, respectively.
    """
    if isinstance(module, EmbeddingCollection):
        return True, False
    elif isinstance(module, EmbeddingBagCollection):
        return not module.is_weighted(), module.is_weighted()
    return True, True


class EmbeddingTowerCollection(nn.Module):
    """
    Collection of EmbeddingTowers.

    Args:
        towers (List[EmbeddingTower]): list of embedding towers that make up the
            collection.
        device (Optional[torch.device]): default compute device.

    Example::

        ebc, ebc_interaction = EmbeddingBagCollection(), MyEBCInteractionModule()
        eb, eb_interaction = EmbeddingCollection(), MyECInteractionModule()
        tower_0 = EmbeddingTower(ebc, ebc_interaction, device)
        tower_1 = EmbeddingTower(eb, eb_interaction, device)
        tower_collection = EmbeddingTowerCollection([tower_0, tower_1])
        kjt = KeyedJaggedTensor()
        output = tower_collection(kjt)
    """

    def __init__(self, towers: List[EmbeddingTower], device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.towers = nn.ModuleList(towers)
        self._input_params: List[Tuple[bool, bool]] = []
        for tower in towers:
            self._input_params.append(tower_input_params(tower.embedding))

    def forward(self, features: Optional[KeyedJaggedTensor]=None, weighted_features: Optional[KeyedJaggedTensor]=None) ->torch.Tensor:
        """
        Executes the collection of towers.

        Features and/or weighted features must be provided as required by the
        underlying embedding modules.

        Args:
            features (Optional[KeyedJaggedTensor]):
            weighted_features (Optional[KeyedJaggedTensor]):

        Returns:
            torch.Tensor: 2-D tensor of shape `M X B`, where `M = sum(N_i)` for tower output i, and `B` is local batch size.
        """
        tower_outputs = []
        for tower, input_params in zip(self.towers, self._input_params):
            has_kjt_param, has_wkjt_param = input_params
            if has_kjt_param and has_wkjt_param:
                assert features is not None
                assert weighted_features is not None
                tower_outputs.append(tower(features, weighted_features))
            elif has_wkjt_param:
                assert weighted_features is not None
                tower_outputs.append(tower(weighted_features))
            else:
                assert features is not None
                tower_outputs.append(tower(features))
        return torch.cat(tower_outputs, dim=1)


def _get_tower_index(name: str, child_module: EmbeddingTowerCollection) ->int:
    for i, tower in enumerate(child_module.towers):
        for n, m in tower.named_modules():
            if isinstance(m, nn.Embedding) or isinstance(m, nn.EmbeddingBag):
                table_name = n.split('.')[-1]
                if name == table_name:
                    return i
    raise RuntimeError(f"couldn't get the tower index for table {name}, tower collection: {child_module}")


def _calculate_cw_shard_sizes_and_offsets(hash_size: int, rows: int, col_wise_shard_dim: Optional[int]=None) ->Tuple[List[List[int]], List[List[int]]]:
    block_size: int = min(col_wise_shard_dim if col_wise_shard_dim else MIN_CW_DIM, hash_size)
    num_col_wise_shards, residual = divmod(hash_size, block_size)
    shard_sizes: List[List[int]] = [[rows, block_size]] * (num_col_wise_shards - 1)
    shard_sizes.append([rows, block_size + residual])
    shard_offsets: List[List[int]] = [[0, block_size * rank] for rank in range(num_col_wise_shards)]
    return shard_sizes, shard_offsets


def _calculate_rw_shard_sizes_and_offsets(hash_size: int, num_devices: int, columns: int) ->Tuple[List[List[int]], List[List[int]]]:
    """
    Sets prefix of shard_sizes to be ceil(hash_size/num_devices).

    For example if hash_size = 10, num_devices = 3, we will allocate the rows as 3,3,3,1
    (rather than 3,3,2,2).
    This is due to implementation in RW sharding that sets block_size_lists to be ceil.
    The balanced way is harder to support on GPU.
    For more details see https://fb.quip.com/xbgbAchCTOL0

    Also consider the example of hash_size = 5, num_devices = 4. The expected rows per
    rank is [2,2,1,0].
    """
    block_size: int = math.ceil(hash_size / num_devices)
    last_rank: int = hash_size // block_size
    last_block_size: int = hash_size - block_size * last_rank
    shard_sizes: List[List[int]] = []
    for rank in range(num_devices):
        if rank < last_rank:
            local_row: int = block_size
        elif rank == last_rank:
            local_row: int = last_block_size
        else:
            local_row: int = 0
        shard_sizes.append([local_row, columns])
    shard_offsets = [[0, 0]]
    for i in range(num_devices - 1):
        shard_offsets.append([shard_sizes[i][0] + shard_offsets[i][0], 0])
    return shard_sizes, shard_offsets


def calculate_shard_sizes_and_offsets(tensor: torch.Tensor, world_size: int, local_world_size: int, sharding_type: str, col_wise_shard_dim: Optional[int]=None) ->Tuple[List[List[int]], List[List[int]]]:
    """
    Calculates sizes and offsets for tensor sharded according to provided sharding type.

    Args:
        tensor (torch.Tensor): tensor to be sharded.
        world_size (int): total number of devices in topology.
        local_world_size (int): total number of devices in host group topology.
        sharding_type (str): provided ShardingType value.
        col_wise_shard_dim (Optional[int]): dimension for column wise sharding split.

    Returns:
        Tuple[List[List[int]], List[List[int]]]: shard sizes, represented as a list of the dimensions of the sharded tensor on each device, and shard offsets, represented as a list of coordinates of placement on each device.

    Raises:
        ValueError: If `sharding_type` is not a valid ShardingType.
    """
    rows, columns = tensor.shape
    if sharding_type == ShardingType.DATA_PARALLEL.value:
        return [[rows, columns]] * world_size, [[0, 0]] * world_size
    elif sharding_type == ShardingType.TABLE_WISE.value:
        return [[rows, columns]], [[0, 0]]
    elif sharding_type == ShardingType.ROW_WISE.value:
        return _calculate_rw_shard_sizes_and_offsets(rows, world_size, columns)
    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:
        return _calculate_rw_shard_sizes_and_offsets(rows, local_world_size, columns)
    elif sharding_type == ShardingType.COLUMN_WISE.value or sharding_type == ShardingType.TABLE_COLUMN_WISE.value:
        return _calculate_cw_shard_sizes_and_offsets(columns, rows, col_wise_shard_dim)
    raise ValueError(f'Unrecognized or unsupported sharding type provided: {sharding_type}')


class PartitionByType(Enum):
    """
    Well-known partition types.
    """
    DEVICE = 'device'
    HOST = 'host'
    UNIFORM = 'uniform'


def get_partition_by_type(sharding_type: str) ->str:
    """
    Gets corresponding partition by type for provided sharding type.

    Args:
        sharding_type (str): sharding type string.

    Returns:
        str: the corresponding `PartitionByType` value.
    """
    device_sharding_types = {ShardingType.TABLE_WISE.value, ShardingType.COLUMN_WISE.value}
    host_sharding_types = {ShardingType.TABLE_ROW_WISE.value, ShardingType.TABLE_COLUMN_WISE.value}
    uniform_sharding_types = {ShardingType.ROW_WISE.value, ShardingType.DATA_PARALLEL.value}
    if sharding_type in device_sharding_types:
        return PartitionByType.DEVICE.value
    elif sharding_type in host_sharding_types:
        return PartitionByType.HOST.value
    elif sharding_type in uniform_sharding_types:
        return PartitionByType.UNIFORM.value
    raise ValueError(f'Unrecognized or unsupported sharding type provided: {sharding_type}')


def _get_module_size(module: nn.Module, multiplier: int) ->int:
    parameters_size = sum([(multiplier * parameter.element_size() * parameter.nelement()) for parameter in module.parameters()])
    buffers_size = sum([(buffer.element_size() * buffer.nelement()) for buffer in module.buffers()])
    return parameters_size + buffers_size


MIN_WIDTH = 90


def _collapse_consecutive_ranks(ranks: List[int]) ->List[str]:
    if len(ranks) > 1 and ranks == list(range(min(ranks), max(ranks) + 1)):
        return [f'{min(ranks)}-{max(ranks)}']
    else:
        return [str(rank) for rank in ranks]


def _format_table(table: List[List[Union[str, int]]]) ->List[str]:
    longest_cols = [(max([len(str(row[i])) for row in table]) + 3) for i in range(len(table[0]))]
    row_format = ''.join([('{:>' + str(longest_col) + '}') for longest_col in longest_cols])
    return [row_format.format(*row) for row in table]


def _get_sharding_type_abbr(sharding_type: str) ->str:
    if sharding_type == ShardingType.DATA_PARALLEL.value:
        return 'DP'
    elif sharding_type == ShardingType.TABLE_WISE.value:
        return 'TW'
    elif sharding_type == ShardingType.COLUMN_WISE.value:
        return 'CW'
    elif sharding_type == ShardingType.ROW_WISE.value:
        return 'RW'
    elif sharding_type == ShardingType.TABLE_ROW_WISE.value:
        return 'TWRW'
    elif sharding_type == ShardingType.TABLE_COLUMN_WISE.value:
        return 'TWCW'
    else:
        raise ValueError(f'Unrecognized or unsupported sharding type provided: {sharding_type}')


def bytes_to_gb(num_bytes: int) ->float:
    return float(num_bytes / (1024 * 1024 * 1024))


def bytes_to_mb(num_bytes: Union[float, int]) ->float:
    return float(num_bytes / (1024 * 1024))


def placement(compute_device: str, rank: int, local_size: int) ->str:
    param_device = compute_device
    if compute_device == 'cuda':
        param_device = torch.device('cuda', rank % local_size)
    return f'rank:{rank}/{param_device}'


class FeatureShardingMixIn:
    """
    Feature Sharding Interface to provide sharding-aware feature metadata.
    """

    def feature_names(self) ->List[str]:
        raise NotImplementedError

    def feature_names_per_rank(self) ->List[List[str]]:
        raise NotImplementedError

    def features_per_rank(self) ->List[int]:
        raise NotImplementedError


class TableBatchedEmbeddingSlice(nn.Parameter):
    """
    Parameter to represent a slice of a table batched embedding. The slice will be
    a view of the TBE of shape (num_embeddings, embedding_dim) and contain consistent .grad
    """
    __slots__ = ['_original_tensor', '_start_offset', '_end_offset', '_num_embeddings', '_embedding_dim']

    def __init__(self, original_tensor: torch.Tensor, start_offset: int, end_offset: int, num_embeddings: int, embedding_dim: int) ->None:
        self._original_tensor: torch.Tensor = original_tensor
        self._start_offset: int = start_offset
        self._end_offset: int = end_offset
        self._num_embeddings: int = num_embeddings
        self._embedding_dim: int = embedding_dim
        if original_tensor.requires_grad:
            self.retain_grad()

    def __new__(cls, original_tensor: torch.Tensor, start_offset: int, end_offset: int, num_embeddings: int, embedding_dim: int) ->'TableBatchedEmbeddingSlice':
        _slice = original_tensor[start_offset:end_offset].view(num_embeddings, embedding_dim)
        return _slice.as_subclass(cls)

    @property
    def grad(self) ->Optional[torch.Tensor]:
        if self._original_tensor.grad is None:
            return None
        return self._original_tensor.grad[self._start_offset:self._end_offset].view(self._num_embeddings, self._embedding_dim)

    @property
    def grad_fn(self) ->None:
        return None


class FusedOptimizer(KeyedOptimizer, abc.ABC):
    """
    Assumes that weight update is done during backward pass,
    thus step() is a no-op.
    """

    @abc.abstractmethod
    def step(self, closure: Any=None) ->None:
        ...

    @abc.abstractmethod
    def zero_grad(self, set_to_none: bool=False) ->None:
        ...

    def __repr__(self) ->str:
        return optim.Optimizer.__repr__(self)


class EmptyFusedOptimizer(FusedOptimizer):
    """
    Fused Optimizer class with no-op step and no parameters to optimize over
    """

    def __init__(self) ->None:
        super().__init__({}, {}, {})

    def step(self, closure: Any=None) ->None:
        pass

    def zero_grad(self, set_to_none: bool=False) ->None:
        pass


class FusedOptimizerModule(abc.ABC):
    """
    Module, which does weight update during backward pass.
    """

    @property
    @abc.abstractmethod
    def fused_optimizer(self) ->KeyedOptimizer:
        ...


def _load_state_dict(emb_modules: 'nn.ModuleList', state_dict: 'OrderedDict[str, Union[torch.Tensor, ShardedTensor]]') ->Tuple[List[str], List[str]]:
    missing_keys = []
    unexpected_keys = list(state_dict.keys())
    for emb_module in emb_modules:
        for key, dst_param in emb_module.state_dict().items():
            if key in state_dict:
                src_param = state_dict[key]
                if isinstance(dst_param, ShardedTensor):
                    assert isinstance(src_param, ShardedTensor)
                    assert len(dst_param.local_shards()) == len(src_param.local_shards())
                    for dst_local_shard, src_local_shard in zip(dst_param.local_shards(), src_param.local_shards()):
                        assert dst_local_shard.metadata.shard_offsets == src_local_shard.metadata.shard_offsets
                        assert dst_local_shard.metadata.shard_sizes == src_local_shard.metadata.shard_sizes
                        dst_local_shard.tensor.detach().copy_(src_local_shard.tensor)
                else:
                    assert isinstance(src_param, torch.Tensor) and isinstance(dst_param, torch.Tensor)
                    dst_param.detach().copy_(src_param)
                unexpected_keys.remove(key)
            else:
                missing_keys.append(cast(str, key))
    return missing_keys, unexpected_keys


class KJTListAwaitable(Awaitable[KJTList]):
    """
    Awaitable of KJTList.

    Args:
        awaitables (List[Awaitable[KeyedJaggedTensor]]): list of `Awaitable` of sparse
            features.
    """

    def __init__(self, awaitables: List[Awaitable[KeyedJaggedTensor]]) ->None:
        super().__init__()
        self.awaitables = awaitables

    def _wait_impl(self) ->KJTList:
        """
        Syncs KJTs in `KJTList`.

        Returns:
            KJTList: synced `KJTList`.
        """
        return KJTList([w.wait() for w in self.awaitables])


class KJTListSplitsAwaitable(Awaitable[Awaitable[KJTList]], Generic[C]):
    """
    Awaitable of Awaitable of KJTList.

    Args:
        awaitables (List[Awaitable[Awaitable[KeyedJaggedTensor]]]): result from calling
            forward on `KJTAllToAll` with sparse features to redistribute.
        ctx (C): sharding context to save the metadata from the input dist to for the
            embedding AlltoAll.
    """

    def __init__(self, awaitables: List[Awaitable[Awaitable[KeyedJaggedTensor]]], ctx: C) ->None:
        super().__init__()
        self.awaitables = awaitables
        self.ctx = ctx

    def _wait_impl(self) ->KJTListAwaitable:
        """
        Calls first wait on the awaitable of awaitable of sparse features and updates
        the context with metadata from the tensors awaitable.

        The first wait gets the result of splits AlltoAll and returns the tensors
        awaitable.

        Returns:
            KJTListAwaitable: awaitables for tensors of the sparse features.
        """
        tensors_awaitables = [w.wait() for w in self.awaitables]
        for awaitable, sharding_context in zip(tensors_awaitables, getattr(self.ctx, 'sharding_contexts', [])):
            if isinstance(awaitable, KJTAllToAllTensorsAwaitable):
                if hasattr(sharding_context, 'batch_size_per_rank'):
                    sharding_context.batch_size_per_rank = awaitable._batch_size_per_rank
                if hasattr(sharding_context, 'input_splits'):
                    sharding_context.input_splits = awaitable._input_splits['values']
                if hasattr(sharding_context, 'output_splits'):
                    sharding_context.output_splits = awaitable._output_splits['values']
                if hasattr(sharding_context, 'sparse_features_recat'):
                    sharding_context.sparse_features_recat = awaitable._recat
        return KJTListAwaitable(tensors_awaitables)


class ModuleShardingMixIn:
    """
    The interface to access a sharded module's sharding scheme.
    """

    @property
    def shardings(self) ->Dict[str, FeatureShardingMixIn]:
        raise NotImplementedError


class CommOp(Enum):
    POOLED_EMBEDDINGS_ALL_TO_ALL = 'pooled_embeddings_all_to_all'
    POOLED_EMBEDDINGS_REDUCE_SCATTER = 'pooled_embeddings_reduce_scatter'
    SEQUENCE_EMBEDDINGS_ALL_TO_ALL = 'sequence_embeddings_all_to_all'


def bucketize_kjt_before_all2all(kjt: KeyedJaggedTensor, num_buckets: int, block_sizes: torch.Tensor, output_permute: bool=False, bucketize_pos: bool=False) ->Tuple[KeyedJaggedTensor, Optional[torch.Tensor]]:
    """
    Bucketizes the `values` in KeyedJaggedTensor into `num_buckets` buckets,
    `lengths` are readjusted based on the bucketization results.

    Note: This function should be used only for row-wise sharding before calling
    `KJTAllToAll`.

    Args:
        num_buckets (int): number of buckets to bucketize the values into.
        block_sizes: (torch.Tensor): bucket sizes for the keyed dimension.
        output_permute (bool): output the memory location mapping from the unbucketized
            values to bucketized values or not.
        bucketize_pos (bool): output the changed position of the bucketized values or
            not.

    Returns:
        Tuple[KeyedJaggedTensor, Optional[torch.Tensor]]: the bucketized `KeyedJaggedTensor` and the optional permute mapping from the unbucketized values to bucketized value.
    """
    num_features = len(kjt.keys())
    assert block_sizes.numel() == num_features, f'Expecting block sizes for {num_features} features, but {block_sizes.numel()} received.'
    block_sizes_new_type = block_sizes.type(kjt.values().type())
    bucketized_lengths, bucketized_indices, bucketized_weights, pos, unbucketize_permute = torch.ops.fbgemm.block_bucketize_sparse_features(kjt.lengths().view(-1), kjt.values(), bucketize_pos=bucketize_pos, sequence=output_permute, block_sizes=block_sizes_new_type, my_size=num_buckets, weights=kjt.weights_or_none())
    return KeyedJaggedTensor(keys=kjt.keys() * num_buckets, values=bucketized_indices, weights=pos if bucketize_pos else bucketized_weights, lengths=bucketized_lengths.view(-1), offsets=None, stride=kjt.stride(), length_per_key=None, offset_per_key=None, index_per_key=None), unbucketize_permute


def _env2int(env_list: List[str], default: int=-1) ->int:
    for e in env_list:
        val = int(os.environ.get(e, -1))
        if val >= 0:
            return val
    return default


def get_local_size(world_size: Optional[int]=None) ->int:
    if world_size is None:
        world_size = dist.get_world_size()
    """
    Gets the local world size (see https://pytorch.org/docs/stable/elastic/run.html)
    This is usually the size of workers on each node, or nproc_per_node
    """
    local_size = _env2int(['LOCAL_WORLD_SIZE', 'MPI_LOCALNRANKS', 'OMPI_COMM_WORLD_LOCAL_SIZE', 'MV2_COMM_WORLD_LOCAL_SIZE'], 8)
    if local_size == -1 or world_size % local_size != 0:
        logging.warning('Could not determine LOCAL_WORLD_SIZE from environment, falling back to WORLD_SIZE.')
        local_size = world_size
    return local_size


def get_group_rank(world_size: Optional[int]=None, rank: Optional[int]=None) ->int:
    """
    Gets the group rank of the worker group. Also available with GROUP_RANK environment varible
    A number between 0 and get_num_groups() (See https://pytorch.org/docs/stable/elastic/run.html)
    """
    if rank is None:
        rank = dist.get_rank()
    return rank // get_local_size(world_size)


def get_local_rank(world_size: Optional[int]=None, rank: Optional[int]=None) ->int:
    """
    Gets the local rank of the local processes (see https://pytorch.org/docs/stable/elastic/run.html)
    This is usually the rank of the worker on its node
    """
    my_local_rank = _env2int(['LOCAL_RANK', 'MPI_LOCALRANKID', 'OMPI_COMM_WORLD_LOCAL_RANK', 'MV2_COMM_WORLD_LOCAL_RANK'], -1)
    local_size = get_local_size(world_size)
    if my_local_rank == -1 or my_local_rank >= local_size:
        logging.warning('Could not determine LOCAL_RANK from environment, falling back to GLOBAL_RANK % LOCAL_SIZE.')
        if rank is None:
            rank = dist.get_rank()
        my_local_rank = rank % local_size
    return my_local_rank


def get_num_groups(world_size: Optional[int]=None) ->int:
    """
    Gets the number of worker groups.
    Usually equivalent to max_nnodes (See https://pytorch.org/docs/stable/elastic/run.html)
    """
    if world_size is None:
        world_size = dist.get_world_size()
    return world_size // get_local_size(world_size)


def intra_and_cross_node_pg(device: Optional[torch.device]=None) ->Tuple[Optional[dist.ProcessGroup], Optional[dist.ProcessGroup]]:
    """
    Creates sub process groups (intra and cross node)
    """
    if device is not None and device.type == 'meta':
        return None, None
    global _INTRA_PG
    global _CROSS_PG
    my_size = dist.get_world_size()
    my_rank = dist.get_rank()
    my_local_rank = get_local_rank(my_size, my_rank)
    local_size = get_local_size(my_size)
    my_group_rank = get_group_rank(my_size, my_rank)
    group_count = get_num_groups(my_size)
    backend = dist.get_backend()
    logger.info(f'[{my_rank}] my_local_rank = {my_local_rank}, local_size = {local_size},my_group_rank = {my_group_rank}, group_count = {group_count}, backend = {backend}')
    if _INTRA_PG is None:
        for group_rank in range(group_count):
            peers = [(group_rank * local_size + r) for r in range(local_size)]
            curr_intra_group_pg = dist.new_group(backend=backend, ranks=peers)
            if my_group_rank == group_rank:
                logger.warning('[Connection] intra_group: [%d] -> [%s]' % (my_rank, peers))
                _INTRA_PG = curr_intra_group_pg
    dist.barrier()
    if _CROSS_PG is None:
        for l_rank in range(local_size):
            peers = [(l_rank + g * local_size) for g in range(group_count)]
            curr_cross_group_pg = dist.new_group(backend=backend, ranks=peers)
            if l_rank == my_local_rank:
                logger.warning('[Connection] cross_group: [%d] -> [%s]' % (my_rank, peers))
                _CROSS_PG = curr_cross_group_pg
    dist.barrier()
    return _INTRA_PG, _CROSS_PG


def merge_fused_params(fused_params: Optional[Dict[str, Any]]=None, param_fused_params: Optional[Dict[str, Any]]=None) ->Dict[str, Any]:
    """
    Configure the fused_params including cache_precision if the value is not preset.

    Values set in table_level_fused_params take precidence over the global fused_params

    Args:
        fused_params (Optional[Dict[str, Any]]): the original fused_params
        grouped_fused_params

    Returns:
        [Dict[str, Any]]: a non-null configured fused_params dictionary to be
        used to configure the embedding lookup kernel
    """
    if fused_params is None:
        fused_params = {}
    if param_fused_params is None:
        param_fused_params = {}
    if 'lr' in param_fused_params:
        param_fused_params['learning_rate'] = param_fused_params.pop('lr')
    _fused_params = copy.deepcopy(fused_params)
    _fused_params.update(param_fused_params)
    return _fused_params


def _construct_jagged_tensors(embeddings: torch.Tensor, features: KeyedJaggedTensor, need_indices: bool=False) ->Dict[str, JaggedTensor]:
    ret: Dict[str, JaggedTensor] = {}
    lengths = features.lengths().view(-1, features.stride())
    values = features.values()
    length_per_key = features.length_per_key()
    values_list = torch.split(values, length_per_key) if need_indices else None
    embeddings_list = torch.split(embeddings, length_per_key, dim=0)
    stride = features.stride()
    lengths_tuple = torch.unbind(lengths.view(-1, stride), dim=0)
    for i, key in enumerate(features.keys()):
        ret[key] = JaggedTensor(lengths=lengths_tuple[i], values=embeddings_list[i], weights=values_list[i] if need_indices else None)
    return ret


class ListOfKJTList(Multistreamable):

    def __init__(self, features: List[KJTList]) ->None:
        self.features_list = features

    def __len__(self) ->int:
        return len(self.features_list)

    def __setitem__(self, key: int, item: KJTList) ->None:
        self.features_list[key] = item

    def __getitem__(self, key: int) ->KJTList:
        return self.features_list[key]

    def __iter__(self) ->Iterator[KJTList]:
        return iter(self.features_list)

    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        for feature in self.features_list:
            feature.record_stream(stream)

    def __fx_create_arg__(self, tracer: torch.fx.Tracer) ->fx.node.Argument:
        return tracer.create_node('call_function', ListOfKJTList, args=(tracer.create_arg(self.features_list),), kwargs={})


class ListOfKJTListAwaitable(Awaitable[ListOfKJTList]):
    """
    This module handles the tables-wise sharding input features distribution for
    inference.

    Args:
        awaitables (List[Awaitable[KJTList]]): list of `Awaitable` of `KJTList`.
    """

    def __init__(self, awaitables: List[Awaitable[KJTList]]) ->None:
        super().__init__()
        self.awaitables = awaitables

    def _wait_impl(self) ->ListOfKJTList:
        """
        Syncs sparse features in list of KJTList.

        Returns:
            ListOfKJTList: synced `ListOfKJTList`.

        """
        return ListOfKJTList([w.wait() for w in self.awaitables])


class ListOfKJTListSplitsAwaitable(Awaitable[Awaitable[ListOfKJTList]]):
    """
    Awaitable of Awaitable of ListOfKJTList.

    Args:
        awaitables (List[Awaitable[Awaitable[KJTList]]]): list of `Awaitable`
            of `Awaitable` of sparse features list.
    """

    def __init__(self, awaitables: List[Awaitable[Awaitable[KJTList]]]) ->None:
        super().__init__()
        self.awaitables = awaitables

    def _wait_impl(self) ->Awaitable[ListOfKJTList]:
        """
        Calls first wait on the awaitable of awaitable of ListOfKJTList.

        Returns:
            Awaitable[ListOfKJTList]: awaitable of `ListOfKJTList`.

        """
        return ListOfKJTListAwaitable([w.wait() for w in self.awaitables])


class NullShardedModuleContext(Multistreamable):

    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        pass

    def __setattr__(self, key: str, value: Any) ->None:
        raise NotImplementedError()


class NullShardingContext(Multistreamable):

    def record_stream(self, stream: torch.cuda.streams.Stream) ->None:
        pass


class InferGroupedLookupMixin(ABC):

    def forward(self, sparse_features: KJTList) ->List[torch.Tensor]:
        embeddings: List[torch.Tensor] = []
        for i, embedding_lookup in enumerate(self._embedding_lookups_per_rank):
            sparse_features_rank = sparse_features[i]
            embeddings.append(embedding_lookup(sparse_features_rank))
        return embeddings

    def state_dict(self, destination: Optional[Dict[str, Any]]=None, prefix: str='', keep_vars: bool=False) ->Dict[str, Any]:
        if destination is None:
            destination = OrderedDict()
            destination._metadata = OrderedDict()
        for rank_modules in self._embedding_lookups_per_rank:
            rank_modules.state_dict(destination, prefix, keep_vars)
        return destination

    def load_state_dict(self, state_dict: 'OrderedDict[str, torch.Tensor]', strict: bool=True) ->_IncompatibleKeys:
        missing_keys = []
        unexpected_keys = []
        for rank_modules in self._embedding_lookups_per_rank:
            incompatible_keys = rank_modules.load_state_dict(state_dict)
            missing_keys.extend(incompatible_keys.missing_keys)
            unexpected_keys.extend(incompatible_keys.unexpected_keys)
        return _IncompatibleKeys(missing_keys=missing_keys, unexpected_keys=unexpected_keys)

    def named_parameters(self, prefix: str='', recurse: bool=True) ->Iterator[Tuple[str, nn.Parameter]]:
        for rank_modules in self._embedding_lookups_per_rank:
            yield from rank_modules.named_parameters(prefix, recurse)

    def named_buffers(self, prefix: str='', recurse: bool=True) ->Iterator[Tuple[str, torch.Tensor]]:
        for rank_modules in self._embedding_lookups_per_rank:
            yield from rank_modules.named_buffers(prefix, recurse)


def _quantize_weight(state_dict: Dict[str, torch.Tensor], data_type: DataType) ->List[Tuple[torch.Tensor, Optional[torch.Tensor]]]:
    quant_weight_list = []
    for weight in state_dict.values():
        if weight.dtype == torch.float or weight.dtype == torch.float16:
            quantized_weights = torch.ops.fbgemm.FloatOrHalfToFusedNBitRowwiseQuantizedSBHalf(weight, DATA_TYPE_NUM_BITS[data_type])
        else:
            raise Exception('Unsupported dtype: {weight.dtype}')
        quant_weight = quantized_weights[:, :-4]
        scale_shift = quantized_weights[:, -4:]
        quant_weight_list.append((quant_weight, scale_shift))
    return quant_weight_list


class InferTwPooledEmbeddingDist(BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]):
    """
    Merges pooled embedding tensor from each device for inference.

    Args:
        device (Optional[torch.device]): device on which buffer will be allocated.
        world_size (int): number of devices in the topology.
    """

    def __init__(self, device: torch.device, world_size: int) ->None:
        super().__init__()
        self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(device, world_size, 1)

    def forward(self, local_embs: List[torch.Tensor], sharding_ctx: Optional[NullShardingContext]=None) ->Awaitable[torch.Tensor]:
        """
        Performs AlltoOne operation on pooled embedding tensors.

        Args:
            local_embs (List[torch.Tensor]): pooled embedding tensors with
                `len(local_embs) == world_size`.

        Returns:
            Awaitable[torch.Tensor]: awaitable of merged pooled embedding tensor.
        """
        return self._dist.forward(local_embs)


def invoke_on_rank_and_broadcast_result(pg: dist.ProcessGroup, rank: int, func: Callable[..., T], *args: Any, **kwargs: Any) ->T:
    """
    Invokes a function on the designated rank and broadcasts the result to all
    members within the group.

    Example::

        id = invoke_on_rank_and_broadcast_result(pg, 0, allocate_id)
    """
    if pg.rank() == rank:
        res = func(*args, **kwargs)
        object_list = [res]
    else:
        object_list = [None]
    if pg.size() > 1:
        dist.broadcast_object_list(object_list, rank, group=pg)
    return cast(T, object_list[0])


_DDP_STATE_DICT_PREFIX = 'module.'


def add_prefix_to_state_dict(state_dict: Dict[str, Any], prefix: str) ->None:
    """
    Adds prefix to all keys in state dict, in place.

    Args:
        state_dict (Dict[str, Any]): input state dict to update.
        prefix (str): name to filter from state dict keys.

    Returns:
        None.
    """
    keys = sorted(state_dict.keys())
    for key in keys:
        state_dict[prefix + key] = state_dict.pop(key)
    if '_metadata' in state_dict:
        metadata = state_dict['_metadata']
        for key in list(metadata.keys()):
            if len(key) == 0:
                continue
            metadata[prefix + key] = metadata.pop(key)


class sharded_model_copy:
    """
    Allows copying of DistributedModelParallel module to a target device.

    Example::

        # Copying model to CPU.

        m = DistributedModelParallel(m)
        with sharded_model_copy("cpu"):
            m_cpu = copy.deepcopy(m)
    """

    def __init__(self, device: Optional[Union[str, int, torch.device]]) ->None:
        self.device = device

    def __enter__(self) ->None:
        self.t_copy_save_ = torch.Tensor.__deepcopy__
        self.p_copy_save_ = torch.nn.Parameter.__deepcopy__
        device = self.device

        def _tensor_copy(tensor, memo):
            if tensor.device != device:
                return tensor.detach()
            else:
                return tensor.detach().clone()

        def _no_copy(obj, memo):
            return obj
        _copy_or_not = _tensor_copy if self.device is not None else _no_copy

        def _param_copy(param, memo):
            return torch.nn.Parameter(_copy_or_not(param, memo), requires_grad=param.requires_grad)
        torch.Tensor.__deepcopy__ = _copy_or_not
        torch.nn.Parameter.__deepcopy__ = _param_copy
        torch._C._distributed_c10d.ProcessGroupNCCL.__deepcopy__ = _no_copy
        torch._C._distributed_c10d.ProcessGroupGloo.__deepcopy__ = _no_copy
        torch._C._distributed_c10d.Work.__deepcopy__ = _no_copy
        torch.cuda.streams.Stream.__deepcopy__ = _no_copy

    def __exit__(self, exc_type, exc_val, exc_tb) ->None:
        torch.Tensor.__deepcopy__ = self.t_copy_save_
        torch.nn.Parameter.__deepcopy__ = self.p_copy_save_
        torch._C._distributed_c10d.ProcessGroupNCCL.__deepcopy__ = None
        torch._C._distributed_c10d.ProcessGroupGloo.__deepcopy__ = None
        torch._C._distributed_c10d.Work.__deepcopy__ = None
        torch.cuda.streams.Stream.__deepcopy__ = None


def copy_to_device(module: nn.Module, current_device: torch.device, to_device: torch.device) ->nn.Module:
    with sharded_model_copy(device=None):
        copy_module = copy.deepcopy(module)

    def _copy_if_device_match(tensor: torch.Tensor) ->torch.Tensor:
        if tensor.device == current_device:
            return tensor
        return tensor
    if isinstance(copy_module, CopyMixIn):
        return copy_module.copy(to_device)
    for child_name, child in copy_module.named_children():
        if not any([isinstance(submodule, CopyMixIn) for submodule in child.modules()]):
            child_copy = child._apply(_copy_if_device_match)
        else:
            child_copy = copy_to_device(child, current_device, to_device)
        copy_module.register_module(child_name, child_copy)
    return copy_module


def filter_state_dict(state_dict: 'OrderedDict[str, torch.Tensor]', name: str) ->'OrderedDict[str, torch.Tensor]':
    """
    Filters state dict for keys that start with provided name.
    Strips provided name from beginning of key in the resulting state dict.

    Args:
        state_dict (OrderedDict[str, torch.Tensor]): input state dict to filter.
        name (str): name to filter from state dict keys.

    Returns:
        OrderedDict[str, torch.Tensor]: filtered state dict.
    """
    filtered_state_dict = OrderedDict()
    for key, value in state_dict.items():
        if key.startswith(name):
            filtered_state_dict[key[len(name) + 1:]] = value
    return filtered_state_dict


def get_module(module: nn.Module) ->nn.Module:
    """
    Unwraps DMP module.

    Does not unwrap data parallel wrappers (i.e. DDP/FSDP), so overriding
    implementations by the wrappers can be used.
    """
    while isinstance(module, DistributedModelParallel):
        module = module._dmp_wrapped_module
    return module


def get_unwrapped_module(module: nn.Module) ->nn.Module:
    """
    Unwraps module wrapped by DMP, DDP, or FSDP.
    """
    while isinstance(module, DistributedModelParallel) or isinstance(module, DistributedDataParallel) or isinstance(module, FullyShardedDataParallel):
        if isinstance(module, DistributedModelParallel):
            module = module._dmp_wrapped_module
        elif isinstance(module, FullyShardedDataParallel):
            module = module._fsdp_wrapped_module
        else:
            module = module.module
    return module


class TestDenseArch(nn.Module):
    """
    Basic nn.Module for testing

    Args:
        device

    Call Args:
        dense_input: torch.Tensor

    Returns:
        KeyedTensor

    Example::

        TestDenseArch()
    """

    def __init__(self, num_float_features: int=10, device: Optional[torch.device]=None) ->None:
        super().__init__()
        if device is None:
            device = torch.device('cpu')
        self.linear: nn.modules.Linear = nn.Linear(in_features=num_float_features, out_features=8, device=device)
        self.dummy_param = torch.nn.Parameter(torch.empty(2, device=device))
        self.register_buffer('dummy_buffer', torch.nn.Parameter(torch.empty(1, device=device)))

    def forward(self, dense_input: torch.Tensor) ->torch.Tensor:
        return self.linear(dense_input)


class CopyableMixin(nn.Module):

    def copy(self, device: torch.device) ->nn.Module:
        return copy_to_device(self, current_device=torch.device('cpu'), to_device=device)


class CopyModule(nn.Module, CopyMixIn):

    def __init__(self) ->None:
        super().__init__()
        self.tensor: torch.Tensor = torch.empty(10, device='cpu')

    def copy(self, device: torch.device) ->nn.Module:
        self.tensor = self.tensor
        return self


class NoCopyModule(nn.Module, CopyMixIn):

    def __init__(self) ->None:
        super().__init__()
        self.tensor: torch.Tensor = torch.empty(10, device='cpu')

    def copy(self, device: torch.device) ->nn.Module:
        return self


@torch.fx.wrap
def _post_sparsenn_forward(padded_embeddings: List[torch.Tensor], batch_size: Optional[int]=None) ->torch.Tensor:
    if batch_size is None or padded_embeddings[0].size(0) == batch_size:
        return torch.cat(padded_embeddings, dim=1)
    else:
        seq_emb = torch.cat(padded_embeddings, dim=1)
        ec_values = torch.zeros(batch_size, seq_emb.size(1), dtype=seq_emb.dtype, device=seq_emb.device)
        ec_values[:seq_emb.size(0), :] = seq_emb
        return ec_values


class TestSequenceTowerInteraction(nn.Module):

    def __init__(self, embedding_names: List[str], embedding_dim: int, device: Optional[torch.device]=None) ->None:
        super().__init__()
        if device is None:
            device = torch.device('cpu')
        self.embedding_names = embedding_names
        self.embedding_dim: int = embedding_dim
        self.max_sequence_length = 20
        self.linear = nn.Linear(in_features=self.max_sequence_length * self.embedding_dim * len(embedding_names), out_features=8, device=device)

    def forward(self, sequence_emb: Dict[str, JaggedTensor]) ->torch.Tensor:
        padded_embeddings = [torch.ops.fbgemm.jagged_2d_to_dense(values=sequence_emb[e].values(), offsets=sequence_emb[e].offsets(), max_sequence_length=self.max_sequence_length).view(-1, self.max_sequence_length * self.embedding_dim) for e in self.embedding_names]
        cat_embeddings = torch.cat(padded_embeddings, dim=1)
        return self.linear(cat_embeddings)


class TestModule(nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.linear0 = nn.Linear(10, 1)
        self.linear1 = nn.Linear(1, 1)


class PredictModule(nn.Module):
    """
    Interface for modules to work in a torch.deploy based backend. Users should
    override predict_forward to convert batch input format to module input format.

    Call Args:
        batch: a dict of input tensors

    Returns:
        output: a dict of output tensors

    Args:
        module: the actual predict module
        device: the primary device for this module that will be used in forward calls.

    Example::

        module = PredictModule(torch.device("cuda", torch.cuda.current_device()))
    """

    def __init__(self, module: nn.Module) ->None:
        super().__init__()
        self._module: nn.Module = module
        self._device: Optional[torch.device] = None
        self._module.eval()

    @property
    def predict_module(self) ->nn.Module:
        return self._module

    @abc.abstractmethod
    def predict_forward(self, batch: Dict[str, torch.Tensor]) ->Any:
        pass

    def forward(self, batch: Dict[str, torch.Tensor]) ->Any:
        if self._device is None:
            self._device = torch.device('cuda', torch.cuda.current_device())
        with torch.device(self._device), torch.inference_mode():
            return self.predict_forward(batch)

    def state_dict(self, destination: Optional[Dict[str, Any]]=None, prefix: str='', keep_vars: bool=False) ->Dict[str, Any]:
        return self._module.state_dict(destination, prefix, keep_vars)


class TestPredictModule(PredictModule):

    def predict_forward(self, batch: Dict[str, torch.Tensor]) ->Dict[str, torch.Tensor]:
        return self.predict_module(*batch)


class Simple(torch.nn.Module):

    def __init__(self, N: int, M: int) ->None:
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(N, M))

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        output = self.weight + input
        return output

    def set_weight(self, weight: torch.Tensor) ->None:
        self.weight[:] = torch.nn.Parameter(weight)


class Nested(torch.nn.Module):

    def __init__(self, N: int, M: int) ->None:
        super().__init__()
        self.simple = Simple(N, M)

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        return self.simple(input)


MEMORY_AVG_WARNING_PERCENTAGE = 20


MEMORY_AVG_WARNING_WARMUP = 100


MetricValue = Union[torch.Tensor, float]


class StrValueMixin:

    def __str__(self) ->str:
        return self.value


class MetricNameBase(StrValueMixin, Enum):
    pass


class MetricPrefix(StrValueMixin, Enum):
    DEFAULT = ''
    LIFETIME = 'lifetime_'
    WINDOW = 'window_'


class MetricNamespaceBase(StrValueMixin, Enum):
    pass


class RecComputeMode(Enum):
    """This Enum lists the supported computation modes for RecMetrics.

    FUSED_TASKS_COMPUTATION indicates that RecMetrics will fuse the computation
    for multiple tasks of the same metric. This can be used by modules where the
    outputs of all the tasks are vectorized.
    """
    FUSED_TASKS_COMPUTATION = 1
    UNFUSED_TASKS_COMPUTATION = 2


DefaultValueT = TypeVar('DefaultValueT')


MAX_BUFFER_COUNT = 1000


class WindowBuffer:

    def __init__(self, max_size: int, max_buffer_count: int) ->None:
        self._max_size: int = max_size
        self._max_buffer_count: int = max_buffer_count
        self._buffers: Deque[torch.Tensor] = deque(maxlen=max_buffer_count)
        self._used_sizes: Deque[int] = deque(maxlen=max_buffer_count)
        self._window_used_size = 0

    def aggregate_state(self, window_state: torch.Tensor, curr_state: torch.Tensor, size: int) ->None:

        def remove(window_state: torch.Tensor) ->None:
            window_state -= self._buffers.popleft()
            self._window_used_size -= self._used_sizes.popleft()
        if len(self._buffers) == self._buffers.maxlen:
            remove(window_state)
        self._buffers.append(curr_state)
        self._used_sizes.append(size)
        window_state += curr_state
        self._window_used_size += size
        while self._window_used_size > self._max_size:
            remove(window_state)

    @property
    def buffers(self) ->Deque[torch.Tensor]:
        return self._buffers


RecModelOutput = Union[torch.Tensor, Dict[str, torch.Tensor]]


def compose_customized_metric_key(namespace: str, metric_name: str, description: Optional[str]=None) ->str:
    """Get the metric key. The input are unrestricted (string) namespace and
    metric_name. This API should only be used by compose_metric_key() and
    state metrics as the keys of state metrics are unknown.
    """
    return f"{namespace}|{metric_name}{description or ''}"


def compose_metric_namespace(namespace: MetricNamespaceBase, task_name: str) ->str:
    """Get the full namespace of a metric based on the input parameters"""
    return f'{namespace}-{task_name}'


def compose_metric_key(namespace: MetricNamespaceBase, task_name: str, metric_name: MetricNameBase, metric_prefix: MetricPrefix=MetricPrefix.DEFAULT, description: Optional[str]=None) ->str:
    """Get the metric key based on the input parameters"""
    return compose_customized_metric_key(compose_metric_namespace(namespace, task_name), f'{metric_prefix}{metric_name}', description)


class StateMetric(abc.ABC):
    """
    The interface of state metrics for a component (e.g., optimizer, qat).
    """

    @abc.abstractmethod
    def get_metrics(self) ->Dict[str, MetricValue]:
        pass


class MetricName(MetricNameBase):
    DEFAULT = ''
    NE = 'ne'
    THROUGHPUT = 'throughput'
    TOTAL_EXAMPLES = 'total_examples'
    CTR = 'ctr'
    CALIBRATION = 'calibration'
    MSE = 'mse'
    MAE = 'mae'
    RMSE = 'rmse'
    AUC = 'auc'
    MULTICLASS_RECALL = 'multiclass_recall'


class MetricNamespace(MetricNamespaceBase):
    DEFAULT = ''
    NE = 'ne'
    THROUGHPUT = 'throughput'
    CTR = 'ctr'
    CALIBRATION = 'calibration'
    MSE = 'mse'
    AUC = 'auc'
    MAE = 'mae'
    OPTIMIZERS = 'optimizers'
    MODEL_CONFIGURATOR = 'model_configurator'
    MULTICLASS_RECALL = 'multiclass_recall'


class ThroughputMetric(nn.Module):
    """
    The module to calculate throughput. Throughput is defined as the trained examples
    across all ranks per second. For example, if the batch size on each rank is 512
    and there are 32 ranks, throughput is 512 * 32 / time_to_train_one_step.

    Args:
        batch_size (int): batch size for the trainer
        world_size (int): the number of trainers
        window_seconds (int): Throughput use time-based window for window_throughput. This
                              argument specify the window size in seconds.
        warmup_steps (int): the number of warmup batches. No Throughput will be calculated
                            before the warmup batches count reached.

    Call Args:
        Not supported.

    Returns:
        Not supported.

    Example::

        throughput = ThroughputMetric(
                      batch_size=128,
                      world_size=4,
                      window_seconds=100,
                      warmup_steps=100
                  )
    """
    _namespace: MetricNamespace = MetricNamespace.THROUGHPUT
    _metric_name: MetricName = MetricName.THROUGHPUT
    _batch_examples: int
    _window_seconds: int
    _warmup_steps: int
    _window_time_lapse_buffer: Deque[float]
    _window_time_lapse: float
    _previous_ts: float
    _lifetime_throughput_key: str
    _window_throughput_key: str
    _total_examples_key: str
    _steps: int

    def __init__(self, *, batch_size: int, world_size: int, window_seconds: int, warmup_steps: int=100) ->None:
        super().__init__()
        if window_seconds < 1:
            raise ValueError('window_seconds must be at least 1 to give window throughput the minimum time window')
        if warmup_steps < 1:
            raise ValueError('warmup_steps must be at least 1 to give throughput a reasonable begin time.')
        if window_seconds > MAX_WINDOW_TS:
            logger.warn(f'window_seconds is greater than {MAX_WINDOW_TS}, capping to {MAX_WINDOW_TS} to make sure window_qps is not staled')
            window_seconds = MAX_WINDOW_TS
        self._batch_examples = batch_size * world_size
        self._window_seconds = window_seconds
        self._warmup_steps = warmup_steps
        self.register_buffer('total_examples', torch.tensor(0, dtype=torch.long))
        self.register_buffer('warmup_examples', torch.tensor(0, dtype=torch.long))
        self.register_buffer('time_lapse_after_warmup', torch.tensor(0, dtype=torch.double))
        self._window_time_lapse_buffer = deque(maxlen=MAX_WINDOW_TS)
        self._window_time_lapse = 0
        self._previous_ts = 0
        self._lifetime_throughput_key = compose_metric_key(self._namespace, str(self._namespace), self._metric_name, MetricPrefix.LIFETIME)
        self._window_throughput_key = compose_metric_key(self._namespace, str(self._namespace), self._metric_name, MetricPrefix.WINDOW)
        self._total_examples_key = compose_metric_key(self._namespace, str(self._namespace), MetricName.TOTAL_EXAMPLES)
        self._steps = 0

    def _check_window(self) ->None:
        while self._window_time_lapse > self._window_seconds:
            self._window_time_lapse -= self._window_time_lapse_buffer.popleft()

    def update(self) ->None:
        ts = time.monotonic()
        self._steps += 1
        self.total_examples += self._batch_examples
        if self._steps <= self._warmup_steps:
            self.warmup_examples += self._batch_examples
            if self._steps == self._warmup_steps:
                self._previous_ts = ts
        else:
            time_lapse = ts - self._previous_ts
            self.time_lapse_after_warmup += time_lapse
            self._window_time_lapse += time_lapse
            self._window_time_lapse_buffer.append(time_lapse)
            self._check_window()
            self._previous_ts = ts

    def compute(self) ->Dict[str, torch.Tensor]:
        ret = {self._total_examples_key: self.total_examples}
        if self._steps > self._warmup_steps and not math.isclose(self.time_lapse_after_warmup.item(), 0):
            lifetime_throughput = (self.total_examples - self.warmup_examples) / self.time_lapse_after_warmup
            if not math.isclose(self._window_time_lapse, 0):
                window_throughput = len(self._window_time_lapse_buffer) * self._batch_examples / self._window_time_lapse
            else:
                window_throughput = 0.0
            if not math.isclose(lifetime_throughput.item(), 0):
                ret.update({self._lifetime_throughput_key: torch.tensor(lifetime_throughput, dtype=torch.double), self._window_throughput_key: torch.tensor(window_throughput, dtype=torch.double)})
        return ret


def is_empty_signals(labels: torch.Tensor, predictions: torch.Tensor, weights: torch.Tensor) ->bool:
    return torch.numel(labels) <= 0 and torch.numel(predictions) <= 0 and torch.numel(weights) <= 0


def parse_model_outputs(label_name: str, prediction_name: str, weight_name: str, model_out: Dict[str, torch.Tensor]) ->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
    labels = model_out[label_name].squeeze()
    if not prediction_name:
        assert not weight_name, 'weight name must be empty if prediction name is empty'
        return labels, None, None
    assert isinstance(labels, torch.Tensor)
    predictions = model_out[prediction_name].squeeze()
    assert isinstance(predictions, torch.Tensor)
    weights = model_out[weight_name].squeeze()
    assert isinstance(weights, torch.Tensor)
    if not is_empty_signals(labels, predictions, weights):
        if labels.dim() == predictions.dim():
            assert torch.numel(labels) == torch.numel(predictions) and torch.numel(labels) == torch.numel(weights), f'Expect the same number of elements in labels, predictions, and weights. Instead got {torch.numel(labels)}, {torch.numel(predictions)}, {torch.numel(weights)}'
        else:
            assert torch.numel(labels) == torch.numel(predictions) / predictions.size()[-1] and torch.numel(labels) == torch.numel(weights)
        if len(labels.size()) == 0:
            labels = labels.unsqueeze(0)
            predictions = predictions.unsqueeze(0)
            weights = weights.unsqueeze(0)
    return labels, predictions, weights


class DenseArch(nn.Module):
    """
    Processes the dense features of DLRM model.

    Args:
        in_features (int): dimensionality of the dense input features.
        layer_sizes (List[int]): list of layer sizes.
        device (Optional[torch.device]): default compute device.

    Example::

        B = 20
        D = 3
        dense_arch = DenseArch(10, layer_sizes=[15, D])
        dense_embedded = dense_arch(torch.rand((B, 10)))
    """

    def __init__(self, in_features: int, layer_sizes: List[int], device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.model: nn.Module = MLP(in_features, layer_sizes, bias=True, activation='relu', device=device)

    def forward(self, features: torch.Tensor) ->torch.Tensor:
        """
        Args:
            features (torch.Tensor): an input tensor of dense features.

        Returns:
            torch.Tensor: an output tensor of size B X D.
        """
        return self.model(features)


@wrap
def _get_flatten_input(inputs: List[torch.Tensor]) ->torch.Tensor:
    return torch.cat([input.flatten(1) for input in inputs], dim=1)


class DeepFM(nn.Module):
    """
    This is the `DeepFM module <https://arxiv.org/pdf/1703.04247.pdf>`_

    This module does not cover the end-end functionality of the published paper.
    Instead, it covers only the deep component of the publication. It is used to learn
    high-order feature interactions. If low-order feature interactions should
    be learnt, please use `FactorizationMachine` module instead, which will share
    the same embedding input of this module.

    To support modeling flexibility, we customize the key components as:

    * Different from the public paper, we change the input from raw sparse features to
      embeddings of the features. It allows flexibility in embedding dimensions and the
      number of embeddings, as long as all embedding tensors have the same batch size.

    * On top of the public paper, we allow users to customize the hidden layer to be any
      module, not limited to just MLP.

    The general architecture of the module is like::

                                1 x 10                  output
                                 /|\\
                                  |                     pass into `dense_module`
                                  |
                                1 x 90
                                 /|\\
                                  |                     concat
                                  |
                        1 x 20, 1 x 30, 1 x 40          list of embeddings

    Args:
        dense_module (nn.Module):
            any customized module that can be used (such as MLP) in DeepFM. The
            `in_features` of this module must be equal to the element counts. For
            example, if the input embedding is `[randn(3, 2, 3), randn(3, 4, 5)]`, the
            `in_features` should be: 2*3+4*5.

    Example::

        import torch
        from torchrec.fb.modules.deepfm import DeepFM
        from torchrec.fb.modules.mlp import LazyMLP
        batch_size = 3
        output_dim = 30
        # the input embedding are a torch.Tensor of [batch_size, num_embeddings, embedding_dim]
        input_embeddings = [
            torch.randn(batch_size, 2, 64),
            torch.randn(batch_size, 2, 32),
        ]
        dense_module = nn.Linear(192, output_dim)
        deepfm = DeepFM(dense_module=dense_module)
        deep_fm_output = deepfm(embeddings=input_embeddings)
    """

    def __init__(self, dense_module: nn.Module) ->None:
        super().__init__()
        self.dense_module = dense_module

    def forward(self, embeddings: List[torch.Tensor]) ->torch.Tensor:
        """
        Args:
            embeddings (List[torch.Tensor]):
                The list of all embeddings (e.g. dense, common_sparse,
                specialized_sparse,
                embedding_features, raw_embedding_features) in the shape of::

                    (batch_size, num_embeddings, embedding_dim)

                For the ease of operation, embeddings that have the same embedding
                dimension have the option to be stacked into a single tensor. For
                example, when we have 1 trained embedding with dimension=32, 5 native
                embeddings with dimension=64, and 3 dense features with dimension=16, we
                can prepare the embeddings list to be the list of::

                    tensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)
                    tensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)
                    tensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32)

                .. note::
                    `batch_size` of all input tensors need to be identical.

        Returns:
            torch.Tensor: output of `dense_module` with flattened and concatenated `embeddings` as input.
        """
        deepfm_input = _get_flatten_input(embeddings)
        deepfm_output = self.dense_module(deepfm_input)
        return deepfm_output


class FactorizationMachine(nn.Module):
    """
    This is the Factorization Machine module, mentioned in the `DeepFM paper
    <https://arxiv.org/pdf/1703.04247.pdf>`_:

    This module does not cover the end-end functionality of the published paper.
    Instead, it covers only the FM part of the publication, and is used to learn
    2nd-order feature interactions.

    To support modeling flexibility, we customize the key components as different from
    the public paper:
        We change the input from raw sparse features to embeddings of the features.
        This allows flexibility in embedding dimensions and the number of embeddings,
        as long as all embedding tensors have the same batch size.

    The general architecture of the module is like::

                                1 x 10                  output
                                 /|\\
                                  |                     pass into `dense_module`
                                  |
                                1 x 90
                                 /|\\
                                  |                     concat
                                  |
                        1 x 20, 1 x 30, 1 x 40          list of embeddings

    Example::

        batch_size = 3
        # the input embedding are in torch.Tensor of [batch_size, num_embeddings, embedding_dim]
        input_embeddings = [
            torch.randn(batch_size, 2, 64),
            torch.randn(batch_size, 2, 32),
        ]
        fm = FactorizationMachine()
        output = fm(embeddings=input_embeddings)
    """

    def __init__(self) ->None:
        super().__init__()

    def forward(self, embeddings: List[torch.Tensor]) ->torch.Tensor:
        """
        Args:
            embeddings (List[torch.Tensor]):
                The list of all embeddings (e.g. dense, common_sparse,
                specialized_sparse, embedding_features, raw_embedding_features) in the
                shape of::

                    (batch_size, num_embeddings, embedding_dim)

                For the ease of operation, embeddings that have the same embedding
                dimension have the option to be stacked into a single tensor. For
                example, when we have 1 trained embedding with dimension=32, 5 native
                embeddings with dimension=64, and 3 dense features with dimension=16, we
                can prepare the embeddings list to be the list of::

                    tensor(B, 1, 32) (trained_embedding with num_embeddings=1, embedding_dim=32)
                    tensor(B, 5, 64) (native_embedding with num_embeddings=5, embedding_dim=64)
                    tensor(B, 3, 16) (dense_features with num_embeddings=3, embedding_dim=32)

                NOTE:
                    `batch_size` of all input tensors need to be identical.

        Returns:
            torch.Tensor: output of fm with flattened and concatenated `embeddings` as input. Expected to be [B, 1].
        """
        fm_input = _get_flatten_input(embeddings)
        sum_of_input = torch.sum(fm_input, dim=1, keepdim=True)
        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)
        square_of_sum = sum_of_input * sum_of_input
        cross_term = square_of_sum - sum_of_square
        cross_term = torch.sum(cross_term, dim=1, keepdim=True) * 0.5
        return cross_term


class FMInteractionArch(nn.Module):
    """
    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`
    (dense_features) and apply the general DeepFM interaction according to the
    external source of DeepFM paper: https://arxiv.org/pdf/1703.04247.pdf

    The output dimension is expected to be a cat of `dense_features`, D.

    Args:
        fm_in_features (int): the input dimension of `dense_module` in DeepFM. For
            example, if the input embeddings is [randn(3, 2, 3), randn(3, 4, 5)], then
            the `fm_in_features` should be: 2 * 3 + 4 * 5.
        sparse_feature_names (List[str]): length of F.
        deep_fm_dimension (int): output of the deep interaction (DI) in the DeepFM arch.

    Example::

        D = 3
        B = 10
        keys = ["f1", "f2"]
        F = len(keys)
        fm_inter_arch = FMInteractionArch(sparse_feature_names=keys)
        dense_features = torch.rand((B, D))
        sparse_features = KeyedTensor(
            keys=keys,
            length_per_key=[D, D],
            values=torch.rand((B, D * F)),
        )
        cat_fm_output = fm_inter_arch(dense_features, sparse_features)
    """

    def __init__(self, fm_in_features: int, sparse_feature_names: List[str], deep_fm_dimension: int) ->None:
        super().__init__()
        self.sparse_feature_names: List[str] = sparse_feature_names
        self.deep_fm = DeepFM(dense_module=nn.Sequential(nn.Linear(fm_in_features, deep_fm_dimension), nn.ReLU()))
        self.fm = FactorizationMachine()

    def forward(self, dense_features: torch.Tensor, sparse_features: KeyedTensor) ->torch.Tensor:
        """
        Args:
            dense_features (torch.Tensor): tensor of size B X D.
            sparse_features (KeyedJaggedTensor): KJT of size F * D X B.

        Returns:
            torch.Tensor: an output tensor of size B X (D + DI + 1).
        """
        if len(self.sparse_feature_names) == 0:
            return dense_features
        tensor_list: List[torch.Tensor] = [dense_features]
        for feature_name in self.sparse_feature_names:
            tensor_list.append(sparse_features[feature_name])
        deep_interaction = self.deep_fm(tensor_list)
        fm_interaction = self.fm(tensor_list)
        return torch.cat([dense_features, deep_interaction, fm_interaction], dim=1)


class OverArch(nn.Module):
    """
    Final Arch of DLRM - simple MLP over OverArch.

    Args:
        in_features (int): size of the input.
        layer_sizes (List[int]): sizes of the layers of the `OverArch`.
        device (Optional[torch.device]): default compute device.

    Example::

        B = 20
        D = 3
        over_arch = OverArch(10, [5, 1])
        logits = over_arch(torch.rand((B, 10)))
    """

    def __init__(self, in_features: int, layer_sizes: List[int], device: Optional[torch.device]=None) ->None:
        super().__init__()
        if len(layer_sizes) <= 1:
            raise ValueError('OverArch must have multiple layers.')
        self.model: nn.Module = nn.Sequential(MLP(in_features, layer_sizes[:-1], bias=True, activation='relu', device=device), nn.Linear(layer_sizes[-2], layer_sizes[-1], bias=True, device=device))

    def forward(self, features: torch.Tensor) ->torch.Tensor:
        """
        Args:
            features (torch.Tensor):

        Returns:
            torch.Tensor: size B X layer_sizes[-1]
        """
        return self.model(features)


class InteractionArch(nn.Module):
    """
    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`
    (dense_features). Returns the pairwise dot product of each sparse feature pair,
    the dot product of each sparse features with the output of the dense layer,
    and the dense layer itself (all concatenated).

    .. note::
        The dimensionality of the `dense_features` (D) is expected to match the
        dimensionality of the `sparse_features` so that the dot products between them
        can be computed.


    Args:
        num_sparse_features (int): F.

    Example::

        D = 3
        B = 10
        keys = ["f1", "f2"]
        F = len(keys)
        inter_arch = InteractionArch(num_sparse_features=len(keys))

        dense_features = torch.rand((B, D))
        sparse_features = torch.rand((B, F, D))

        #  B X (D + F + F choose 2)
        concat_dense = inter_arch(dense_features, sparse_features)
    """

    def __init__(self, num_sparse_features: int) ->None:
        super().__init__()
        self.F: int = num_sparse_features
        self.triu_indices: torch.Tensor = torch.triu_indices(self.F + 1, self.F + 1, offset=1)

    def forward(self, dense_features: torch.Tensor, sparse_features: torch.Tensor) ->torch.Tensor:
        """
        Args:
            dense_features (torch.Tensor): an input tensor of size B X D.
            sparse_features (torch.Tensor): an input tensor of size B X F X D.

        Returns:
            torch.Tensor: an output tensor of size B X (D + F + F choose 2).
        """
        if self.F <= 0:
            return dense_features
        B, D = dense_features.shape
        combined_values = torch.cat((dense_features.unsqueeze(1), sparse_features), dim=1)
        interactions = torch.bmm(combined_values, torch.transpose(combined_values, 1, 2))
        interactions_flat = interactions[:, self.triu_indices[0], self.triu_indices[1]]
        return torch.cat((dense_features, interactions_flat), dim=1)


class InteractionDCNArch(nn.Module):
    """
    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`
    (dense_features). Returns the output of a Deep Cross Net v2
    https://arxiv.org/pdf/2008.13535.pdf with a low rank approximation for the
    weight matrix. The input and output sizes are the same for this
    interaction layer (F*D + D).

    .. note::
        The dimensionality of the `dense_features` (D) is expected to match the
        dimensionality of the `sparse_features` so that the dot products between them
        can be computed.


    Args:
        num_sparse_features (int): F.

    Example::

        D = 3
        B = 10
        keys = ["f1", "f2"]
        F = len(keys)
        DCN = LowRankCrossNet(
            in_features = F*D+D,
            dcn_num_layers = 2,
            dnc_low_rank_dim = 4,
        )
        inter_arch = InteractionDCNArch(
            num_sparse_features=len(keys),
            crossnet=DCN,
        )

        dense_features = torch.rand((B, D))
        sparse_features = torch.rand((B, F, D))

        #  B X (F*D + D)
        concat_dense = inter_arch(dense_features, sparse_features)
    """

    def __init__(self, num_sparse_features: int, crossnet: nn.Module) ->None:
        super().__init__()
        self.F: int = num_sparse_features
        self.crossnet = crossnet

    def forward(self, dense_features: torch.Tensor, sparse_features: torch.Tensor) ->torch.Tensor:
        """
        Args:
            dense_features (torch.Tensor): an input tensor of size B X D.
            sparse_features (torch.Tensor): an input tensor of size B X F X D.

        Returns:
            torch.Tensor: an output tensor of size B X (F*D + D).
        """
        if self.F <= 0:
            return dense_features
        B, D = dense_features.shape
        combined_values = torch.cat((dense_features.unsqueeze(1), sparse_features), dim=1)
        return self.crossnet(combined_values.reshape([B, -1]))


class InteractionProjectionArch(nn.Module):
    """
    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`
    (dense_features). Return Y*Z and the dense layer itself (all concatenated)
    where Y is the output of interaction branch 1 and Z is the output of interaction
    branch 2. Y and Z are of size Bx(F1xD) and Bx(DxF2) respectively for some F1 and F2.

    .. note::

        The dimensionality of the `dense_features` (D) is expected to match the
        dimensionality of the `sparse_features` so that the dot products between them
        can be computed.
        The output dimension of the 2 interaction branches should be a multiple
        of D.


    Args:
        num_sparse_features (int): F.
        interaction_branch1 (nn.Module): MLP module for the first branch of
            interaction layer
        interaction_branch2 (nn.Module): MLP module for the second branch of
            interaction layer

    Example::

        D = 3
        B = 10
        keys = ["f1", "f2"]
        F = len(keys)
        # Assume last layer of
        I1 = DenseArch(
            in_features= 3 * D + D,
            layer_sizes=[4*D, 4*D], # F1 = 4
            device=dense_device,
        )
        I2 = DenseArch(
            in_features= 3 * D + D,
            layer_sizes=[4*D, 4*D], # F2 = 4
            device=dense_device,
        )
        inter_arch = InteractionProjectionArch(
                        num_sparse_features=len(keys),
                        interaction_branch1 = I1,
                        interaction_branch2 = I2,
                    )

        dense_features = torch.rand((B, D))
        sparse_features = torch.rand((B, F, D))

        #  B X (D + F1 * F2)
        concat_dense = inter_arch(dense_features, sparse_features)
    """

    def __init__(self, num_sparse_features: int, interaction_branch1: nn.Module, interaction_branch2: nn.Module) ->None:
        super().__init__()
        self.F: int = num_sparse_features
        self.interaction_branch1 = interaction_branch1
        self.interaction_branch2 = interaction_branch2

    def forward(self, dense_features: torch.Tensor, sparse_features: torch.Tensor) ->torch.Tensor:
        """
        Args:
            dense_features (torch.Tensor): an input tensor of size B X D.
            sparse_features (torch.Tensor): an input tensor of size B X F X D.

        Returns:
            torch.Tensor: an output tensor of size B X (D + F1 * F2)) where
            F1*D and F2*D are the output dimensions of the 2 interaction MLPs.
        """
        if self.F <= 0:
            return dense_features
        B, D = dense_features.shape
        combined_values = torch.cat((dense_features.unsqueeze(1), sparse_features), dim=1)
        interaction_branch1_out = self.interaction_branch1(torch.reshape(combined_values, (B, -1)))
        interaction_branch2_out = self.interaction_branch2(torch.reshape(combined_values, (B, -1)))
        interactions = torch.bmm(interaction_branch1_out.reshape([B, -1, D]), interaction_branch2_out.reshape([B, D, -1]))
        interactions_flat = torch.reshape(interactions, (B, -1))
        return torch.cat((dense_features, interactions_flat), dim=1)


def choose(n: int, k: int) ->int:
    """
    Simple implementation of math.comb for Python 3.7 compatibility.
    """
    if 0 <= k <= n:
        ntok = 1
        ktok = 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0


class LowRankCrossNet(torch.nn.Module):
    """
    Low Rank Cross Net is a highly efficient cross net. Instead of using full rank cross
    matrices (NxN) at each layer, it will use two kernels :math:`W (N x r)` and
    :math:`V (r x N)`, where `r << N`, to simplify the matrix multiplication.

    On each layer l, the tensor is transformed into:

    .. math::    x_{l+1} = x_0 * (W_l \\cdot (V_l \\cdot x_l) + b_l) + x_l

    where :math:`W_l` is either a vector, :math:`*` means element-wise multiplication,
    and :math:`\\cdot` means matrix multiplication.

    NOTE:
        Rank `r` should be chosen smartly. Usually, we  expect `r < N/2` to have
        computational savings; we should expect :math:`r ~= N/4` to preserve the
        accuracy of the full rank cross net.

    Args:
        in_features (int): the dimension of the input.
        num_layers (int): the number of layers in the module.
        low_rank (int): the rank setup of the cross matrix (default = 1).
            Value must be always >= 1.

    Example::

        batch_size = 3
        num_layers = 2
        in_features = 10
        input = torch.randn(batch_size, in_features)
        dcn = LowRankCrossNet(num_layers=num_layers, low_rank=3)
        output = dcn(input)
    """

    def __init__(self, in_features: int, num_layers: int, low_rank: int=1) ->None:
        super().__init__()
        assert low_rank >= 1, 'Low rank must be larger or equal to 1'
        self._num_layers = num_layers
        self._low_rank = low_rank
        self.W_kernels: torch.nn.ParameterList = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(in_features, self._low_rank))) for i in range(self._num_layers)])
        self.V_kernels: torch.nn.ParameterList = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(self._low_rank, in_features))) for i in range(self._num_layers)])
        self.bias: torch.nn.ParameterList = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features))) for i in range(self._num_layers)])

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): tensor with shape [batch_size, in_features].

        Returns:
            torch.Tensor: tensor with shape [batch_size, in_features].
        """
        x_0 = input
        x_l = x_0
        for layer in range(self._num_layers):
            x_l_v = torch.nn.functional.linear(x_l, self.V_kernels[layer])
            x_l_w = torch.nn.functional.linear(x_l_v, self.W_kernels[layer])
            x_l = x_0 * (x_l_w + self.bias[layer]) + x_l
        return x_l


class InteractionTransformerArch(nn.Module):
    """
    Processes the output of both `SparseArch` (sparse_features) and `DenseArch`
    (dense_features). Returns the output of the nn.transformerencoder,
    that takes the combined values of both sparse features and the output of the dense layer,
    and the dense layer itself (i.e. concat(dense layer output, transformer encoder output).
    Note: This model is for benchmarking purposes only, i.e. to measure the performance of transformer + embeddings using the dlrm models.
    It is not intended to increase model convergence metrics.
    Implemented TE as described here:
    https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html?highlight=transformer+encoder#torch.nn.TransformerEncoder
    BERT Transformer Paper: https://arxiv.org/abs/1810.04805
    Attention is All you Need: https://arxiv.org/abs/1706.03762


    .. note::
        The dimensionality of the `dense_features` (D) is expected to match the
        dimensionality of the `sparse_features` so that the dot products between them
        can be computed.
    Args:
        num_sparse_features (int): F.
        embedding_dim: int,
        nhead: int, #number of attention heads
        ntransformer_layers: int, #number of transformer layers.
    Example::
        D = 8   #must divisible by number of transformer heads
        B = 10
        keys = ["f1", "f2"]
        F = len(keys)
        inter_arch = InteractionTransormerArch(num_sparse_features=len(keys))
        dense_features = torch.rand((B, D))
        sparse_features = torch.rand((B, F, D))
        #  B X (D * (F + 1))
        concat_dense = inter_arch(dense_features, sparse_features)
    """

    def __init__(self, num_sparse_features: int, embedding_dim: int, nhead: int=8, ntransformer_layers: int=4) ->None:
        super().__init__()
        self.F: int = num_sparse_features
        self.nhead = nhead
        self.ntransformer_layers = ntransformer_layers
        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=self.nhead)
        self.interarch_TE = nn.TransformerEncoder(transformer_encoder_layer, num_layers=self.ntransformer_layers)

    def forward(self, dense_features: torch.Tensor, sparse_features: torch.Tensor) ->torch.Tensor:
        """
        Args:
            dense_features (torch.Tensor): an input tensor of size B X D.
            sparse_features (torch.Tensor): an input tensor of size B X F X D.
        Returns:
            torch.Tensor: an output tensor of size B X (D + F + F choose 2).
        """
        if self.F <= 0:
            return dense_features
        B, D = dense_features.shape
        combined_values = torch.cat((dense_features.unsqueeze(1), sparse_features), dim=1)
        transformer_interactions = self.interarch_TE(combined_values)
        interactions_flat = torch.reshape(transformer_interactions, (B, -1))
        return interactions_flat


class CrossNet(torch.nn.Module):
    """
    `Cross Network <https://arxiv.org/abs/1708.05123>`_:

    Cross Net is a stack of "crossing" operations on a tensor of shape :math:`(*, N)`
    to the same shape, effectively creating :math:`N` learnable polynomical functions
    over the input tensor.

    In this module, the crossing operations are defined based on a full rank matrix
    (NxN), such that the crossing effect can cover all bits on each layer. On each layer
    l, the tensor is transformed into:

    .. math ::    x_{l+1} = x_0 * (W_l \\cdot x_l + b_l) + x_l

    where :math:`W_l` is a square matrix :math:`(NxN)`, :math:`*` means element-wise
    multiplication, :math:`\\cdot` means matrix multiplication.

    Args:
        in_features (int): the dimension of the input.
        num_layers (int): the number of layers in the module.

    Example::

        batch_size = 3
        num_layers = 2
        in_features = 10
        input = torch.randn(batch_size, in_features)
        dcn = CrossNet(num_layers=num_layers)
        output = dcn(input)
    """

    def __init__(self, in_features: int, num_layers: int) ->None:
        super().__init__()
        self._num_layers = num_layers
        self.kernels: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(in_features, in_features))) for i in range(self._num_layers)])
        self.bias: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features, 1))) for i in range(self._num_layers)])

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): tensor with shape [batch_size, in_features].

        Returns:
            torch.Tensor: tensor with shape [batch_size, in_features].
        """
        x_0 = input.unsqueeze(2)
        x_l = x_0
        for layer in range(self._num_layers):
            xl_w = torch.matmul(self.kernels[layer], x_l)
            x_l = x_0 * (xl_w + self.bias[layer]) + x_l
        return torch.squeeze(x_l, dim=2)


class VectorCrossNet(torch.nn.Module):
    """
    Vector Cross Network can be refered as
    `DCN-V1 <https://arxiv.org/pdf/1708.05123.pdf>`_.

    It is also a specialized low rank cross net, where rank=1. In this version, on each
    layer, instead of keeping two kernels W and V, we only keep one vector kernel W
    (Nx1). We use the dot operation to compute the "crossing" effect of the features,
    thus saving two matrix multiplications to further reduce computational cost and cut
    the number of learnable parameters.

    On each layer l, the tensor is transformed into

    .. math::    x_{l+1} = x_0 * (W_l . x_l + b_l) + x_l

    where :math:`W_l` is either a vector, :math:`*` means element-wise multiplication;
    :math:`.` means dot operations.

    Args:
        in_features (int): the dimension of the input.
        num_layers (int): the number of layers in the module.

    Example::

        batch_size = 3
        num_layers = 2
        in_features = 10
        input = torch.randn(batch_size, in_features)
        dcn = VectorCrossNet(num_layers=num_layers)
        output = dcn(input)
    """

    def __init__(self, in_features: int, num_layers: int) ->None:
        super().__init__()
        self._num_layers = num_layers
        self.kernels: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(in_features, 1))) for i in range(self._num_layers)])
        self.bias: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(in_features, 1))) for i in range(self._num_layers)])

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): tensor with shape [batch_size, in_features].

        Returns:
            torch.Tensor: tensor with shape [batch_size, in_features].
        """
        x_0 = input.unsqueeze(2)
        x_l = x_0
        for layer in range(self._num_layers):
            xl_w = torch.tensordot(x_l, self.kernels[layer], dims=([1], [0]))
            x_l = torch.matmul(x_0, xl_w) + self.bias[layer] + x_l
        return torch.squeeze(x_l, dim=2)


class LowRankMixtureCrossNet(torch.nn.Module):
    """
    Low Rank Mixture Cross Net is a DCN V2 implementation from the `paper
    <https://arxiv.org/pdf/2008.13535.pdf>`_:

    `LowRankMixtureCrossNet` defines the learnable crossing parameter per layer as a
    low-rank matrix :math:`(N*r)` together with mixture of experts. Compared to
    `LowRankCrossNet`, instead of relying on one single expert to learn feature crosses,
    this module leverages such :math:`K` experts; each learning feature interactions in
    different subspaces, and adaptively combining the learned crosses using a gating
    mechanism that depends on input :math:`x`..

    On each layer l, the tensor is transformed into:

    .. math::    x_{l+1} = MoE({expert_i : i \\in K_{experts}}) + x_l

    and each :math:`expert_i` is defined as:

    .. math::   expert_i = x_0 * (U_{li} \\cdot g(C_{li} \\cdot g(V_{li} \\cdot x_l)) + b_l)

    where :math:`U_{li} (N, r)`, :math:`C_{li} (r, r)` and :math:`V_{li} (r, N)` are
    low-rank matrices, :math:`*` means element-wise multiplication, :math:`x` means
    matrix multiplication, and :math:`g()` is the non-linear activation function.

    When num_expert is 1, the gate evaluation and MOE will be skipped to save
    computation.

    Args:
        in_features (int): the dimension of the input.
        num_layers (int): the number of layers in the module.
        low_rank (int): the rank setup of the cross matrix (default = 1).
            Value must be always >= 1
        activation (Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]):
            the non-linear activation function, used in defining experts.
            Default is relu.

    Example::

        batch_size = 3
        num_layers = 2
        in_features = 10
        input = torch.randn(batch_size, in_features)
        dcn = LowRankCrossNet(num_layers=num_layers, num_experts=5, low_rank=3)
        output = dcn(input)
    """

    def __init__(self, in_features: int, num_layers: int, num_experts: int=1, low_rank: int=1, activation: Union[torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]]=torch.relu) ->None:
        super().__init__()
        assert num_experts >= 1, 'num_experts must be larger or equal to 1'
        assert low_rank >= 1, 'Low rank must be larger or equal to 1'
        self._num_layers = num_layers
        self._num_experts = num_experts
        self._low_rank = low_rank
        self._in_features = in_features
        self.U_kernels: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(self._num_experts, self._in_features, self._low_rank))) for i in range(self._num_layers)])
        self.V_kernels: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(self._num_experts, self._low_rank, self._in_features))) for i in range(self._num_layers)])
        self.bias: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.zeros_(torch.empty(self._in_features, 1))) for i in range(self._num_layers)])
        self.gates: Optional[torch.nn.Module] = torch.nn.ModuleList([torch.nn.Linear(self._in_features, 1, bias=False) for i in range(self._num_experts)]) if self._num_experts > 1 else None
        self._activation = activation
        self.C_kernels: torch.nn.Module = torch.nn.ParameterList([torch.nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(self._num_experts, self._low_rank, self._low_rank))) for i in range(self._num_layers)])

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): tensor with shape [batch_size, in_features].

        Returns:
            torch.Tensor: tensor with shape [batch_size, in_features].
        """
        x_0 = input.unsqueeze(2)
        x_l = x_0
        for layer in range(self._num_layers):
            if self._num_experts > 1:
                gating = []
                for i in range(self._num_experts):
                    gating.append(self.gates[i](x_l.squeeze(2)))
                gating = torch.stack(gating, 1)
            experts = []
            for i in range(self._num_experts):
                expert = torch.matmul(self.V_kernels[layer][i], x_l)
                expert = torch.matmul(self.C_kernels[layer][i], self._activation(expert))
                expert = torch.matmul(self.U_kernels[layer][i], self._activation(expert))
                expert = x_0 * (expert + self.bias[layer])
                experts.append(expert.squeeze(2))
            experts = torch.stack(experts, 2)
            if self._num_experts > 1:
                moe = torch.matmul(experts, torch.nn.functional.softmax(gating, 1))
                x_l = moe + x_l
            else:
                x_l = experts + x_l
        return torch.squeeze(x_l, dim=2)


class BaseFeatureProcessor(nn.Module):
    """
    Abstract base class for feature processor.
    """

    @abc.abstractmethod
    def forward(self, features: Dict[str, JaggedTensor]) ->Dict[str, JaggedTensor]:
        pass


class PositionWeightedModule(BaseFeatureProcessor):
    """
    Adds position weights to id list features.

    Args:
        max_feature_lengths (Dict[str, int]): feature name to `max_length` mapping.
            `max_length`, a.k.a truncation size, specifies the maximum number of ids
            each sample has. For each feature, its position weight parameter size is
            `max_length`.
    """

    def __init__(self, max_feature_lengths: Dict[str, int]) ->None:
        super().__init__()
        self.max_feature_lengths = max_feature_lengths
        self.position_weights: nn.ParameterDict = nn.ParameterDict()
        for key, length in max_feature_lengths.items():
            self.position_weights[key] = nn.Parameter(torch.empty([length]).fill_(1.0))

    def forward(self, features: Dict[str, JaggedTensor]) ->Dict[str, JaggedTensor]:
        """
        Args:
            features (Dict[str, JaggedTensor]): dictionary of keys to `JaggedTensor`,
                representing the features.

        Returns:
            Dict[str, JaggedTensor]: same as input features with `weights` field being populated.
        """
        weighted_features: Dict[str, JaggedTensor] = {}
        for key, position_weight in self.position_weights.items():
            seq = torch.ops.fbgemm.offsets_range(features[key].offsets().long(), torch.numel(features[key].values()))
            weighted_features[key] = JaggedTensor(values=features[key].values(), lengths=features[key].lengths(), offsets=features[key].offsets(), weights=torch.gather(position_weight, dim=0, index=seq))
        features.update(weighted_features)
        return features


_is_fx_tracing_flag = False


def is_fx_tracing() ->bool:
    return _is_fx_tracing_flag


class PositionWeightedProcessor(BaseGroupedFeatureProcessor):
    """
    PositionWeightedProcessor represents a processor to apply position weight to a KeyedJaggedTensor.

    It can handle both unsharded and sharded input and output corresponding output

    Args:
        max_feature_lengths (Dict[str, int]): Dict of feature_lengths, the key is the feature_name and value is length.
        device (Optional[torch.device]): default compute device.

    Example::

        keys=["Feature0", "Feature1", "Feature2"]
        values=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7])
        lengths=torch.tensor([2, 0, 1, 1, 1, 3, 2, 3, 0])
        features = KeyedJaggedTensor.from_lengths_sync(keys=keys, values=values, lengths=lengths)
        pw = FeatureProcessorCollection(
            feature_processor_modules={key: PositionWeightedFeatureProcessor(max_feature_length=100) for key in keys}
        )
        result = pw(features)
        # result is
        # KeyedJaggedTensor({
        #     "Feature0": {
        #         "values": [[0, 1], [], [2]],
        #         "weights": [[1.0, 1.0], [], [1.0]]
        #     },
        #     "Feature1": {
        #         "values": [[3], [4], [5, 6, 7]],
        #         "weights": [[1.0], [1.0], [1.0, 1.0, 1.0]]
        #     },
        #     "Feature2": {
        #         "values": [[3, 4], [5, 6, 7], []],
        #         "weights": [[1.0, 1.0], [1.0, 1.0, 1.0], []]
        #     }
        # })
    """

    def __init__(self, max_feature_lengths: Dict[str, int], device: Optional[torch.device]=None) ->None:
        super().__init__()
        self.max_feature_lengths = max_feature_lengths
        for length in self.max_feature_lengths.values():
            if length <= 0:
                raise
        self.position_weights: nn.ParameterDict = nn.ParameterDict()
        for key, length in max_feature_lengths.items():
            self.position_weights[key] = nn.Parameter(torch.empty([length], device=device).fill_(1.0))

    def forward(self, features: KeyedJaggedTensor) ->KeyedJaggedTensor:
        """
        In unsharded or non-pipelined model, the input features both contain fp_feature
        and non_fp_features, and the output will filter out non_fp features
        In sharded pipelining model, the input features can only contain either none
        or all feature_processed features, since the input feature comes from the
        input_dist() of ebc which will filter out the keys not in the ebc. And the
        input size is same as output size

        Args:
            features (KeyedJaggedTensor): input features

        Returns:
            KeyedJaggedTensor
        """
        if is_fx_tracing():
            features_dict = features.to_dict()
            weighted_features_names: List[str] = []
            weighted_features_values: List[torch.Tensor] = []
            weighted_features_lengths: List[torch.Tensor] = []
            weighted_features_weights: List[torch.Tensor] = []
            for key, position_weight in self.position_weights.items():
                seq = torch.ops.fbgemm.offsets_range(features_dict[key].offsets().long(), torch.numel(features_dict[key].values()))
                weighted_features_names.append(key)
                weighted_features_values.append(features_dict[key].values())
                weighted_features_lengths.append(features_dict[key].lengths())
                weighted_features_weights.append(torch.gather(position_weight, dim=0, index=seq))
            return KeyedJaggedTensor.from_lengths_sync(keys=weighted_features_names, values=torch.cat(weighted_features_values), lengths=torch.cat(weighted_features_lengths), weights=torch.cat(weighted_features_weights))
        else:
            feature_names = features.keys()
            lengths = features.lengths()
            offsets = features.offsets()
            values = features.values()
            length_per_key = features.length_per_key()
            weights = features.weights_or_none()
            batch_size = features.stride()
            has_fp_id_list_feature = False
            has_normal_id_list_feature = False
            if weights is None:
                cat_seq = torch.ops.fbgemm.offsets_range(offsets.long(), torch.numel(values))
            else:
                cat_seq = weights.long()
            seqs = torch.split(cat_seq, features.length_per_key())
            for feature_name in feature_names:
                if feature_name in self.max_feature_lengths:
                    has_fp_id_list_feature = True
                else:
                    has_normal_id_list_feature = True
            if has_fp_id_list_feature:
                if not has_normal_id_list_feature:
                    processed_features_weights: List[torch.Tensor] = []
                    for feature_index, feature_name in enumerate(feature_names):
                        processed_weight = torch.gather(self.position_weights[feature_name], dim=0, index=seqs[feature_index])
                        processed_features_weights.append(processed_weight)
                    fp_features = KeyedJaggedTensor(keys=feature_names, values=values, weights=torch.cat(processed_features_weights), lengths=lengths, offsets=offsets, stride=batch_size, length_per_key=length_per_key, offset_per_key=features.offset_per_key(), index_per_key=features._key_indices())
                else:
                    feature_values = values.split(length_per_key)
                    feature_lengths = lengths.split(batch_size)
                    processed_features_names: List[str] = []
                    processed_features_lengths: List[torch.Tensor] = []
                    processed_features_values: List[torch.Tensor] = []
                    processed_features_weights: List[torch.Tensor] = []
                    for feature_index, feature_name in enumerate(feature_names):
                        if feature_name in self.max_feature_lengths:
                            feature_value = feature_values[feature_index]
                            feature_length = feature_lengths[feature_index]
                            processed_weight = torch.gather(self.position_weights[feature_name], dim=0, index=seqs[feature_index])
                            processed_features_names.append(feature_name)
                            processed_features_lengths.append(feature_length)
                            processed_features_values.append(feature_value)
                            processed_features_weights.append(processed_weight)
                    fp_features = KeyedJaggedTensor.from_lengths_sync(keys=processed_features_names, values=torch.cat(processed_features_values), lengths=torch.cat(processed_features_lengths), weights=torch.cat(processed_features_weights))
                return fp_features
            else:
                return features

    def named_buffers(self, prefix: str='', recurse: bool=True, remove_duplicate: bool=True) ->Iterator[Tuple[str, torch.Tensor]]:
        yield from ()

    def state_dict(self, destination: Optional[Dict[str, Any]]=None, prefix: str='', keep_vars: bool=False) ->Dict[str, Any]:
        if destination is None:
            destination = OrderedDict()
            destination._metadata = OrderedDict()
        for name, param in self.position_weights.items():
            destination[prefix + f'position_weights.{name}'] = param if keep_vars else param.detach()
        return destination


class ComputeKJTToJTDict(torch.nn.Module):
    """Converts a KeyedJaggedTensor to a dict of JaggedTensors.

    Example::
        #              0       1        2  <-- dim_1
        # "Feature0"   [V0,V1] None    [V2]
        # "Feature1"   [V3]    [V4]    [V5,V6,V7]
        #   ^
        #  dim_0

        would return

        {
            "Feature0": JaggedTensor([[V0,V1],None,V2]),
            "Feature1": JaggedTensor([V3,V4,[V5,V6,V7]]),
        }
    """

    def forward(self, keyed_jagged_tensor: 'KeyedJaggedTensor') ->Dict[str, JaggedTensor]:
        """
        Converts a KeyedJaggedTensor into a dict of JaggedTensors.
        Args:
            keyed_jagged_tensor (KeyedJaggedTensor): tensor to convert
        Returns:
            Dict[str, JaggedTensor]
        """
        return _maybe_compute_kjt_to_jt_dict(stride=keyed_jagged_tensor.stride(), keys=keyed_jagged_tensor.keys(), length_per_key=keyed_jagged_tensor.length_per_key(), values=keyed_jagged_tensor.values(), lengths=keyed_jagged_tensor.lengths(), weights=keyed_jagged_tensor.weights_or_none(), jt_dict=keyed_jagged_tensor._jt_dict)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (Attention,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (BaseFeatureProcessor,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BaseGroupedFeatureProcessor,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (CrossNet,
     lambda: ([], {'in_features': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DeepFM,
     lambda: ([], {'dense_module': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DenseArch,
     lambda: ([], {'in_features': 4, 'layer_sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (EmbeddingTower,
     lambda: ([], {'embedding_module': _mock_layer(), 'interaction_module': _mock_layer()}),
     lambda: ([], {'input': torch.rand([4, 4])}),
     False),
    (FactorizationMachine,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (InteractionArch,
     lambda: ([], {'num_sparse_features': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4, 4])], {}),
     True),
    (InteractionDCNArch,
     lambda: ([], {'num_sparse_features': 4, 'crossnet': _mock_layer()}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4, 4])], {}),
     True),
    (InteractionProjectionArch,
     lambda: ([], {'num_sparse_features': 4, 'interaction_branch1': _mock_layer(), 'interaction_branch2': _mock_layer()}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4, 4])], {}),
     True),
    (LowRankCrossNet,
     lambda: ([], {'in_features': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LowRankMixtureCrossNet,
     lambda: ([], {'in_features': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MultiHeadedAttention,
     lambda: ([], {'num_heads': 4, 'dim_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (Nested,
     lambda: ([], {'N': 4, 'M': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (OverArch,
     lambda: ([], {'in_features': 4, 'layer_sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Perceptron,
     lambda: ([], {'in_size': 4, 'out_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PositionwiseFeedForward,
     lambda: ([], {'dim_model': 4, 'd_ff': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Simple,
     lambda: ([], {'N': 4, 'M': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SublayerConnection,
     lambda: ([], {'size': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4]), _mock_layer()], {}),
     False),
    (SwishLayerNorm,
     lambda: ([], {'input_dims': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TransformerBlock,
     lambda: ([], {'hidden': 4, 'attn_heads': 4, 'feed_forward_hidden': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (VectorCrossNet,
     lambda: ([], {'in_features': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
]

class Test_pytorch_torchrec(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])


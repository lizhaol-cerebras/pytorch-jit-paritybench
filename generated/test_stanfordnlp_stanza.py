import sys
_module = sys.modules[__name__]
del sys
corenlp = _module
pipeline_demo = _module
scenegraph = _module
semgrex = _module
setup = _module
stanza = _module
_version = _module
models = _module
_training_logging = _module
charlm = _module
classifier = _module
classifiers = _module
base_classifier = _module
cnn_classifier = _module
constituency_classifier = _module
data = _module
iterate_test = _module
trainer = _module
utils = _module
common = _module
beam = _module
bert_embedding = _module
biaffine = _module
build_short_name_to_treebank = _module
char_model = _module
chuliu_edmonds = _module
constant = _module
convert_pretrain = _module
count_ner_coverage = _module
count_pretrain_coverage = _module
crf = _module
data = _module
doc = _module
dropout = _module
exceptions = _module
foundation_cache = _module
hlstm = _module
large_margin_loss = _module
loss = _module
maxout_linear = _module
packed_lstm = _module
peft_config = _module
pretrain = _module
seq2seq_constant = _module
seq2seq_model = _module
seq2seq_modules = _module
seq2seq_utils = _module
short_name_to_treebank = _module
stanza_object = _module
trainer = _module
utils = _module
vocab = _module
constituency = _module
base_model = _module
base_trainer = _module
dynamic_oracle = _module
ensemble = _module
evaluate_treebanks = _module
in_order_compound_oracle = _module
in_order_oracle = _module
label_attention = _module
lstm_model = _module
lstm_tree_stack = _module
parse_transitions = _module
parse_tree = _module
parser_training = _module
partitioned_transformer = _module
positional_encoding = _module
retagging = _module
score_converted_dependencies = _module
state = _module
text_processing = _module
top_down_oracle = _module
trainer = _module
transformer_tree_stack = _module
transition_sequence = _module
tree_embedding = _module
tree_reader = _module
tree_stack = _module
utils = _module
constituency_parser = _module
coref = _module
anaphoricity_scorer = _module
bert = _module
cluster_checker = _module
config = _module
conll = _module
const = _module
coref_chain = _module
dataset = _module
loss = _module
model = _module
pairwise_encoder = _module
predict = _module
rough_scorer = _module
span_predictor = _module
tokenizer_customization = _module
utils = _module
word_encoder = _module
depparse = _module
data = _module
model = _module
scorer = _module
trainer = _module
identity_lemmatizer = _module
lang_identifier = _module
langid = _module
create_ud_data = _module
data = _module
model = _module
trainer = _module
lemma = _module
data = _module
edit = _module
trainer = _module
lemmatizer = _module
mwt = _module
character_classifier = _module
data = _module
trainer = _module
mwt_expander = _module
ner = _module
data = _module
model = _module
trainer = _module
ner_tagger = _module
parser = _module
pos = _module
build_xpos_vocab_factory = _module
data = _module
model = _module
trainer = _module
xpos_vocab_factory = _module
xpos_vocab_utils = _module
tagger = _module
tokenization = _module
data = _module
model = _module
tokenize_files = _module
trainer = _module
utils = _module
tokenizer = _module
wl_coref = _module
pipeline = _module
_constants = _module
constituency_processor = _module
core = _module
coref_processor = _module
demo_server = _module
depparse_processor = _module
external = _module
corenlp_converter_depparse = _module
jieba = _module
pythainlp = _module
spacy = _module
sudachipy = _module
langid_processor = _module
lemma_processor = _module
multilingual = _module
mwt_processor = _module
ner_processor = _module
pos_processor = _module
processor = _module
registry = _module
sentiment_processor = _module
tokenize_processor = _module
CoreNLP_pb2 = _module
protobuf = _module
resources = _module
default_packages = _module
installation = _module
prepare_resources = _module
print_charlm_depparse = _module
server = _module
annotator = _module
client = _module
dependency_converter = _module
java_protobuf_requests = _module
main = _module
morphology = _module
parser_eval = _module
ssurgeon = _module
tokensregex = _module
tsurgeon = _module
ud_enhancer = _module
tests = _module
test_classifier = _module
test_constituency_classifier = _module
test_data = _module
test_process_utils = _module
test_bert_embedding = _module
test_char_model = _module
test_chuliu_edmonds = _module
test_common_data = _module
test_confusion = _module
test_constant = _module
test_data_conversion = _module
test_data_objects = _module
test_doc = _module
test_dropout = _module
test_foundation_cache = _module
test_pretrain = _module
test_short_name_to_treebank = _module
test_utils = _module
test_convert_arboretum = _module
test_convert_it_vit = _module
test_convert_starlang = _module
test_ensemble = _module
test_in_order_compound_oracle = _module
test_in_order_oracle = _module
test_lstm_model = _module
test_parse_transitions = _module
test_parse_tree = _module
test_positional_encoding = _module
test_selftrain_vi_quad = _module
test_text_processing = _module
test_top_down_oracle = _module
test_trainer = _module
test_transformer_tree_stack = _module
test_transition_sequence = _module
test_tree_reader = _module
test_tree_stack = _module
test_vietnamese = _module
datasets = _module
test_prepare_ner_file = _module
test_common = _module
test_vietnamese_renormalization = _module
test_depparse_data = _module
test_parser = _module
test_langid = _module
test_multilingual = _module
test_lemma_trainer = _module
test_lowercase = _module
test_character_classifier = _module
test_english_corner_cases = _module
test_prepare_mwt = _module
test_bsf_2_beios = _module
test_bsf_2_iob = _module
test_combine_ner_datasets = _module
test_convert_amt = _module
test_convert_nkjp = _module
test_convert_starlang_ner = _module
test_models_ner_scorer = _module
test_ner_tagger = _module
test_ner_trainer = _module
test_ner_training = _module
test_ner_utils = _module
test_pay_amt_annotators = _module
test_split_wikiner = _module
test_suc3 = _module
pipeline_device_tests = _module
test_arabic_pipeline = _module
test_core = _module
test_decorators = _module
test_depparse = _module
test_english_pipeline = _module
test_french_pipeline = _module
test_lemmatizer = _module
test_pipeline_constituency_processor = _module
test_pipeline_depparse_processor = _module
test_pipeline_mwt_expander = _module
test_pipeline_ner_processor = _module
test_pipeline_pos_processor = _module
test_pipeline_sentiment_processor = _module
test_requirements = _module
test_tokenizer = _module
test_tagger = _module
test_xpos_vocab_factory = _module
test_charlm_depparse = _module
test_default_packages = _module
test_installation = _module
test_prepare_resources = _module
test_client = _module
test_java_protobuf_requests = _module
test_morphology = _module
test_parser_eval = _module
test_protobuf = _module
test_semgrex = _module
test_server_misc = _module
test_server_pretokenized = _module
test_server_request = _module
test_server_start = _module
test_ssurgeon = _module
test_tokensregex = _module
test_tsurgeon = _module
test_ud_enhancer = _module
test_prepare_tokenizer_treebank = _module
test_replace_long_tokens = _module
test_spaces = _module
test_tokenization_lst20 = _module
test_tokenization_orchid = _module
test_tokenize_data = _module
test_tokenize_files = _module
test_tokenize_utils = _module
avg_sent_len = _module
conll17_to_text = _module
dump_oscar = _module
make_lm_data = _module
oscar_to_text = _module
confusion = _module
conll18_ud_eval = _module
check_transitions = _module
grep_dev_logs = _module
grep_test_logs = _module
list_tensors = _module
build_silver_dataset = _module
common_trees = _module
convert_alt = _module
convert_arboretum = _module
convert_cintil = _module
convert_ctb = _module
convert_icepahc = _module
convert_it_turin = _module
convert_it_vit = _module
convert_spmrl = _module
convert_starlang = _module
count_common_words = _module
extract_all_silver_dataset = _module
extract_silver_dataset = _module
prepare_con_dataset = _module
relabel_tags = _module
selftrain = _module
selftrain_it = _module
selftrain_single_file = _module
selftrain_vi_quad = _module
selftrain_wiki = _module
silver_variance = _module
split_holdout = _module
split_weighted_ensemble = _module
tokenize_wiki = _module
treebank_to_labeled_brackets = _module
vtb_convert = _module
vtb_split = _module
contract_mwt = _module
balance_languages = _module
convert_hindi = _module
convert_ontonotes = _module
convert_udcoref = _module
corenlp_segmenter_dataset = _module
build_en_combined = _module
check_for_duplicates = _module
combine_ner_datasets = _module
compare_entities = _module
conll_to_iob = _module
convert_amt = _module
convert_ar_aqmar = _module
convert_bn_daffodil = _module
convert_bsf_to_beios = _module
convert_bsnlp = _module
convert_en_conll03 = _module
convert_fire_2013 = _module
convert_he_iahlt = _module
convert_hy_armtdp = _module
convert_ijc = _module
convert_kk_kazNERD = _module
convert_lst20 = _module
convert_mr_l3cube = _module
convert_my_ucsy = _module
convert_nkjp = _module
convert_nner22 = _module
convert_nytk = _module
convert_rgai = _module
convert_sindhi_siner = _module
convert_starlang_ner = _module
count_entities = _module
json_to_bio = _module
misc_to_date = _module
ontonotes_multitag = _module
prepare_ner_dataset = _module
prepare_ner_file = _module
preprocess_wikiner = _module
simplify_en_worldwide = _module
simplify_ontonotes_to_worldwide = _module
split_wikiner = _module
suc_conll_to_iob = _module
suc_to_iob = _module
convert_trees_to_pos = _module
prepare_depparse_treebank = _module
prepare_lemma_treebank = _module
prepare_mwt_treebank = _module
prepare_pos_treebank = _module
prepare_tokenizer_data = _module
prepare_tokenizer_treebank = _module
word_in_pretrain = _module
random_split_conllu = _module
sentiment = _module
add_constituency = _module
convert_italian_poetry_classification = _module
convert_italian_sentence_classification = _module
prepare_sentiment_dataset = _module
process_MELD = _module
process_airline = _module
process_arguana_xml = _module
process_corona = _module
process_es_tass2020 = _module
process_it_sentipolc16 = _module
process_ren_chinese = _module
process_sb10k = _module
process_scare = _module
process_slsd = _module
process_sst = _module
process_usage_german = _module
process_utils = _module
process_vsfc_vietnamese = _module
thai_syllable_dict_generator = _module
convert_ml_cochin = _module
convert_my_alt = _module
convert_text_files = _module
convert_th_best = _module
convert_th_lst20 = _module
convert_th_orchid = _module
convert_vi_vlsp = _module
process_thai_tokenization = _module
vietnamese = _module
renormalize = _module
default_paths = _module
get_tqdm = _module
helper_func = _module
languages = _module
kazakh_transliteration = _module
count_ambiguous_lemmas = _module
max_mwt_length = _module
flair_ner_tag_dataset = _module
paying_annotators = _module
spacy_ner_tag_dataset = _module
compare_pretrains = _module
select_backoff = _module
training = _module
compose_ete_results = _module
remove_constituency_optimizer = _module
run_charlm = _module
run_constituency = _module
run_depparse = _module
run_ete = _module
run_lemma = _module
run_mwt = _module
run_ner = _module
run_pos = _module
run_sentiment = _module
run_tokenizer = _module
separate_ner_pretrain = _module
visualization = _module
conll_deprel_visualization = _module
constants = _module
dependency_visualization = _module
ner_visualization = _module
semgrex_app = _module
semgrex_visualizer = _module
ssurgeon_visualizer = _module

from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import re


from copy import copy


import logging


import math


import random


import time


from types import GeneratorType


import numpy as np


import torch


from enum import Enum


import torch.nn as nn


from abc import ABC


from abc import abstractmethod


from types import SimpleNamespace


import torch.nn.functional as F


import torch.optim as optim


from torch import nn


from torch.nn.utils.rnn import pad_packed_sequence


from torch.nn.utils.rnn import pack_padded_sequence


from torch.nn.utils.rnn import pack_sequence


from torch.nn.utils.rnn import PackedSequence


from collections import Counter


from numbers import Number


import torch.nn.init as init


from collections import defaultdict


import copy


import functools


from torch.autograd import Variable


from collections import namedtuple


import warnings


from torch import optim


from typing import Any


from typing import Dict


from typing import List


from typing import Tuple


from torch.utils.data import Dataset


from typing import Optional


from typing import Set


from torch.nn.utils.rnn import pad_sequence


from torch.utils.data import DataLoader as DL


from torch.utils.data.sampler import Sampler


from torch.utils.data import DataLoader as TorchDataLoader


from itertools import compress


logger = logging.getLogger('stanza')


def sort_with_indices(data, key=None, reverse=False):
    """
    Sort data and return both the data and the original indices.

    One useful application is to sort by length, which can be done with key=len
    Returns the data as a sorted list, then the indices of the original list.
    """
    if not data:
        return [], []
    if key:
        ordered = sorted(enumerate(data), key=lambda x: key(x[1]), reverse=reverse)
    else:
        ordered = sorted(enumerate(data), key=lambda x: x[1], reverse=reverse)
    result = tuple(zip(*ordered))
    return result[1], result[0]


def split_into_batches(data, batch_size):
    """
    Returns a list of intervals so that each interval is either <= batch_size or one element long.

    Long elements are not dropped from the intervals.
    data is a list of lists
    batch_size is how long to make each batch
    return value is a list of pairs, start_idx end_idx
    """
    intervals = []
    interval_start = 0
    interval_size = 0
    for idx, line in enumerate(data):
        if len(line) > batch_size:
            if interval_size > 0:
                intervals.append((interval_start, idx))
            intervals.append((idx, idx + 1))
            interval_start = idx + 1
            interval_size = 0
        elif len(line) + interval_size > batch_size:
            intervals.append((interval_start, idx))
            interval_start = idx
            interval_size = len(line)
        else:
            interval_size = interval_size + len(line)
    if interval_size > 0:
        intervals.append((interval_start, len(data)))
    return intervals


def unsort(sorted_list, oidx):
    """
    Unsort a sorted list, based on the original idx.
    """
    assert len(sorted_list) == len(oidx), 'Number of list elements must match with original indices.'
    if len(sorted_list) == 0:
        return []
    _, unsorted = [list(t) for t in zip(*sorted(zip(oidx, sorted_list)))]
    return unsorted


class BaseClassifier(ABC, nn.Module):

    @abstractmethod
    def extract_sentences(self, doc):
        """
        Extract the sentences or the relevant information in the sentences from a document
        """

    def preprocess_sentences(self, sentences):
        """
        By default, don't do anything
        """
        return sentences

    def label_sentences(self, sentences, batch_size=None):
        """
        Given a list of sentences, return the model's results on that text.
        """
        self.eval()
        sentences = self.preprocess_sentences(sentences)
        if batch_size is None:
            intervals = [(0, len(sentences))]
            orig_idx = None
        else:
            sentences, orig_idx = sort_with_indices(sentences, key=len, reverse=True)
            intervals = split_into_batches(sentences, batch_size)
        labels = []
        for interval in intervals:
            if interval[1] - interval[0] == 0:
                continue
            output = self(sentences[interval[0]:interval[1]])
            predicted = torch.argmax(output, dim=1)
            labels.extend(predicted.tolist())
        if orig_idx:
            sentences = unsort(sentences, orig_idx)
            labels = unsort(labels, orig_idx)
        logger.debug('Found labels')
        for label, sentence in zip(labels, sentences):
            logger.debug((label, sentence))
        return labels


class ExtraVectors(Enum):
    NONE = 1
    CONCAT = 2
    SUM = 3


class ModelType(Enum):
    LSTM = 1
    ENSEMBLE = 2


PAD_ID = 0


class SentimentDatum:

    def __init__(self, sentiment, text, constituency=None):
        self.sentiment = sentiment
        self.text = text
        self.constituency = constituency

    def __eq__(self, other):
        if self is other:
            return True
        if not isinstance(other, SentimentDatum):
            return False
        return self.sentiment == other.sentiment and self.text == other.text and self.constituency == other.constituency

    def __str__(self):
        return str(self._asdict())

    def _asdict(self):
        if self.constituency is None:
            return {'sentiment': self.sentiment, 'text': self.text}
        else:
            return {'sentiment': self.sentiment, 'text': self.text, 'constituency': str(self.constituency)}


UNK_ID = 1


def attach_bert_model(model, bert_model, bert_tokenizer, use_peft, force_bert_saved):
    if use_peft:
        model.add_unsaved_module('bert_model', bert_model)
        model.bert_model.train()
    elif force_bert_saved:
        model.bert_model = bert_model
    elif bert_model is not None:
        model.add_unsaved_module('bert_model', bert_model)
        for _, parameter in bert_model.named_parameters():
            parameter.requires_grad = False
    else:
        model.bert_model = None
    model.add_unsaved_module('bert_tokenizer', bert_tokenizer)


def build_output_layers(fc_input_size, fc_shapes, num_classes):
    """
    Build a sequence of fully connected layers to go from the final conv layer to num_classes

    Returns an nn.ModuleList
    """
    fc_layers = []
    previous_layer_size = fc_input_size
    for shape in fc_shapes:
        fc_layers.append(nn.Linear(previous_layer_size, shape))
        previous_layer_size = shape
    fc_layers.append(nn.Linear(previous_layer_size, num_classes))
    return nn.ModuleList(fc_layers)


tlogger = logging.getLogger('stanza.constituency.trainer')


class CNNClassifier(BaseClassifier):

    def __init__(self, pretrain, extra_vocab, labels, charmodel_forward, charmodel_backward, elmo_model, bert_model, bert_tokenizer, force_bert_saved, peft_name, args):
        """
        pretrain is a pretrained word embedding.  should have .emb and .vocab

        extra_vocab is a collection of words in the training data to
        be used for the delta word embedding, if used.  can be set to
        None if delta word embedding is not used.

        labels is the list of labels we expect in the training data.
        Used to derive the number of classes.  Saving it in the model
        will let us check that test data has the same labels

        args is either the complete arguments when training, or the
        subset of arguments stored in the model save file
        """
        super(CNNClassifier, self).__init__()
        self.labels = labels
        bert_finetune = getattr(args, 'bert_finetune', False)
        use_peft = getattr(args, 'use_peft', False)
        force_bert_saved = force_bert_saved or bert_finetune
        logger.debug('bert_finetune %s / force_bert_saved %s', bert_finetune, force_bert_saved)
        self.peft_name = peft_name
        self.config = SimpleNamespace(filter_channels=args.filter_channels, filter_sizes=args.filter_sizes, fc_shapes=args.fc_shapes, dropout=args.dropout, num_classes=len(labels), wordvec_type=args.wordvec_type, extra_wordvec_method=args.extra_wordvec_method, extra_wordvec_dim=args.extra_wordvec_dim, extra_wordvec_max_norm=args.extra_wordvec_max_norm, char_lowercase=args.char_lowercase, charlm_projection=args.charlm_projection, has_charlm_forward=charmodel_forward is not None, has_charlm_backward=charmodel_backward is not None, use_elmo=args.use_elmo, elmo_projection=args.elmo_projection, bert_model=args.bert_model, bert_finetune=bert_finetune, bert_hidden_layers=getattr(args, 'bert_hidden_layers', None), force_bert_saved=force_bert_saved, use_peft=use_peft, lora_rank=getattr(args, 'lora_rank', None), lora_alpha=getattr(args, 'lora_alpha', None), lora_dropout=getattr(args, 'lora_dropout', None), lora_modules_to_save=getattr(args, 'lora_modules_to_save', None), lora_target_modules=getattr(args, 'lora_target_modules', None), bilstm=args.bilstm, bilstm_hidden_dim=args.bilstm_hidden_dim, maxpool_width=args.maxpool_width, model_type=ModelType.CNN)
        self.char_lowercase = args.char_lowercase
        self.unsaved_modules = []
        emb_matrix = pretrain.emb
        self.add_unsaved_module('embedding', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))
        self.add_unsaved_module('elmo_model', elmo_model)
        self.vocab_size = emb_matrix.shape[0]
        self.embedding_dim = emb_matrix.shape[1]
        self.add_unsaved_module('forward_charlm', charmodel_forward)
        if charmodel_forward is not None:
            tlogger.debug('Got forward char model of dimension {}'.format(charmodel_forward.hidden_dim()))
            if not charmodel_forward.is_forward_lm:
                raise ValueError('Got a backward charlm as a forward charlm!')
        self.add_unsaved_module('backward_charlm', charmodel_backward)
        if charmodel_backward is not None:
            tlogger.debug('Got backward char model of dimension {}'.format(charmodel_backward.hidden_dim()))
            if charmodel_backward.is_forward_lm:
                raise ValueError('Got a forward charlm as a backward charlm!')
        attach_bert_model(self, bert_model, bert_tokenizer, self.config.use_peft, force_bert_saved)
        self.unk = nn.Parameter(torch.randn(self.embedding_dim) / np.sqrt(self.embedding_dim) / 10.0)
        self.vocab_map = {word.replace('\xa0', ' '): i for i, word in enumerate(pretrain.vocab)}
        if self.config.extra_wordvec_method is not ExtraVectors.NONE:
            if not extra_vocab:
                raise ValueError('Should have had extra_vocab set for extra_wordvec_method {}'.format(self.config.extra_wordvec_method))
            if not args.extra_wordvec_dim:
                self.config.extra_wordvec_dim = self.embedding_dim
            if self.config.extra_wordvec_method is ExtraVectors.SUM:
                if self.config.extra_wordvec_dim != self.embedding_dim:
                    raise ValueError('extra_wordvec_dim must equal embedding_dim for {}'.format(self.config.extra_wordvec_method))
            self.extra_vocab = list(extra_vocab)
            self.extra_vocab_map = {word: i for i, word in enumerate(self.extra_vocab)}
            self.extra_embedding = nn.Embedding(num_embeddings=len(extra_vocab), embedding_dim=self.config.extra_wordvec_dim, max_norm=self.config.extra_wordvec_max_norm, padding_idx=0)
            tlogger.debug('Extra embedding size: {}'.format(self.extra_embedding.weight.shape))
        else:
            self.extra_vocab = None
            self.extra_vocab_map = None
            self.config.extra_wordvec_dim = 0
            self.extra_embedding = None
        if self.config.extra_wordvec_method is ExtraVectors.NONE:
            total_embedding_dim = self.embedding_dim
        elif self.config.extra_wordvec_method is ExtraVectors.SUM:
            total_embedding_dim = self.embedding_dim
        elif self.config.extra_wordvec_method is ExtraVectors.CONCAT:
            total_embedding_dim = self.embedding_dim + self.config.extra_wordvec_dim
        else:
            raise ValueError('unable to handle {}'.format(self.config.extra_wordvec_method))
        if charmodel_forward is not None:
            if args.charlm_projection:
                self.charmodel_forward_projection = nn.Linear(charmodel_forward.hidden_dim(), args.charlm_projection)
                total_embedding_dim += args.charlm_projection
            else:
                self.charmodel_forward_projection = None
                total_embedding_dim += charmodel_forward.hidden_dim()
        if charmodel_backward is not None:
            if args.charlm_projection:
                self.charmodel_backward_projection = nn.Linear(charmodel_backward.hidden_dim(), args.charlm_projection)
                total_embedding_dim += args.charlm_projection
            else:
                self.charmodel_backward_projection = None
                total_embedding_dim += charmodel_backward.hidden_dim()
        if self.config.use_elmo:
            if elmo_model is None:
                raise ValueError('Model requires elmo, but elmo_model not passed in')
            elmo_dim = elmo_model.sents2elmo([['Test']])[0].shape[1]
            self.elmo_combine_layers = nn.Linear(in_features=3, out_features=1, bias=False)
            if self.config.elmo_projection:
                self.elmo_projection = nn.Linear(in_features=elmo_dim, out_features=self.config.elmo_projection)
                total_embedding_dim = total_embedding_dim + self.config.elmo_projection
            else:
                total_embedding_dim = total_embedding_dim + elmo_dim
        if bert_model is not None:
            if self.config.bert_hidden_layers:
                if self.config.bert_hidden_layers > bert_model.config.num_hidden_layers:
                    self.config.bert_hidden_layers = bert_model.config.num_hidden_layers + 1
                self.bert_layer_mix = nn.Linear(self.config.bert_hidden_layers, 1, bias=False)
                nn.init.zeros_(self.bert_layer_mix.weight)
            else:
                self.bert_layer_mix = None
            if bert_tokenizer is None:
                raise ValueError('Cannot have a bert model without a tokenizer')
            self.bert_dim = self.bert_model.config.hidden_size
            total_embedding_dim += self.bert_dim
        if self.config.bilstm:
            conv_input_dim = self.config.bilstm_hidden_dim * 2
            self.bilstm = nn.LSTM(batch_first=True, input_size=total_embedding_dim, hidden_size=self.config.bilstm_hidden_dim, num_layers=2, bidirectional=True, dropout=0.2)
        else:
            conv_input_dim = total_embedding_dim
            self.bilstm = None
        self.fc_input_size = 0
        self.conv_layers = nn.ModuleList()
        self.max_window = 0
        for filter_idx, filter_size in enumerate(self.config.filter_sizes):
            if isinstance(filter_size, int):
                self.max_window = max(self.max_window, filter_size)
                if isinstance(self.config.filter_channels, int):
                    filter_channels = self.config.filter_channels
                else:
                    filter_channels = self.config.filter_channels[filter_idx]
                fc_delta = filter_channels // self.config.maxpool_width
                tlogger.debug('Adding full width filter %d.  Output channels: %d -> %d', filter_size, filter_channels, fc_delta)
                self.fc_input_size += fc_delta
                self.conv_layers.append(nn.Conv2d(in_channels=1, out_channels=filter_channels, kernel_size=(filter_size, conv_input_dim)))
            elif isinstance(filter_size, tuple) and len(filter_size) == 2:
                filter_height, filter_width = filter_size
                self.max_window = max(self.max_window, filter_width)
                if isinstance(self.config.filter_channels, int):
                    filter_channels = max(1, self.config.filter_channels // (conv_input_dim // filter_width))
                else:
                    filter_channels = self.config.filter_channels[filter_idx]
                fc_delta = filter_channels * (conv_input_dim // filter_width) // self.config.maxpool_width
                tlogger.debug('Adding filter %s.  Output channels: %d -> %d', filter_size, filter_channels, fc_delta)
                self.fc_input_size += fc_delta
                self.conv_layers.append(nn.Conv2d(in_channels=1, out_channels=filter_channels, stride=(1, filter_width), kernel_size=(filter_height, filter_width)))
            else:
                raise ValueError('Expected int or 2d tuple for conv size')
        tlogger.debug('Input dim to FC layers: %d', self.fc_input_size)
        self.fc_layers = build_output_layers(self.fc_input_size, self.config.fc_shapes, self.config.num_classes)
        self.dropout = nn.Dropout(self.config.dropout)

    def add_unsaved_module(self, name, module):
        self.unsaved_modules += [name]
        setattr(self, name, module)
        if module is not None and (name in ('forward_charlm', 'backward_charlm') or name == 'bert_model' and not self.config.use_peft):
            for _, parameter in module.named_parameters():
                parameter.requires_grad = False

    def is_unsaved_module(self, name):
        return name.split('.')[0] in self.unsaved_modules

    def log_configuration(self):
        """
        Log some essential information about the model configuration to the training logger
        """
        tlogger.info('Filter sizes: %s' % str(self.config.filter_sizes))
        tlogger.info('Filter channels: %s' % str(self.config.filter_channels))
        tlogger.info('Intermediate layers: %s' % str(self.config.fc_shapes))

    def log_norms(self):
        lines = ['NORMS FOR MODEL PARAMTERS']
        for name, param in self.named_parameters():
            if param.requires_grad and name.split('.')[0] not in ('forward_charlm', 'backward_charlm'):
                lines.append('%s %.6g' % (name, torch.norm(param).item()))
        logger.info('\n'.join(lines))

    def build_char_reps(self, inputs, max_phrase_len, charlm, projection, begin_paddings, device):
        char_reps = charlm.build_char_representation(inputs)
        if projection is not None:
            char_reps = [projection(x) for x in char_reps]
        char_inputs = torch.zeros((len(inputs), max_phrase_len, char_reps[0].shape[-1]), device=device)
        for idx, rep in enumerate(char_reps):
            start = begin_paddings[idx]
            end = start + rep.shape[0]
            char_inputs[idx, start:end, :] = rep
        return char_inputs

    def extract_bert_embeddings(self, inputs, max_phrase_len, begin_paddings, device):
        bert_embeddings = extract_bert_embeddings(self.config.bert_model, self.bert_tokenizer, self.bert_model, inputs, device, keep_endpoints=False, num_layers=self.bert_layer_mix.in_features if self.bert_layer_mix is not None else None, detach=not self.config.bert_finetune, peft_name=self.peft_name)
        if self.bert_layer_mix is not None:
            bert_embeddings = [(self.bert_layer_mix(feature).squeeze(2) + feature.sum(axis=2) / self.bert_layer_mix.in_features) for feature in bert_embeddings]
        bert_inputs = torch.zeros((len(inputs), max_phrase_len, bert_embeddings[0].shape[-1]), device=device)
        for idx, rep in enumerate(bert_embeddings):
            start = begin_paddings[idx]
            end = start + rep.shape[0]
            bert_inputs[idx, start:end, :] = rep
        return bert_inputs

    def forward(self, inputs):
        device = next(self.parameters()).device
        vocab_map = self.vocab_map

        def map_word(word):
            idx = vocab_map.get(word, None)
            if idx is not None:
                return idx
            if word[-1] == "'":
                idx = vocab_map.get(word[:-1], None)
                if idx is not None:
                    return idx
            return vocab_map.get(word.lower(), UNK_ID)
        inputs = [(x.text if isinstance(x, SentimentDatum) else x) for x in inputs]
        max_phrase_len = max(len(x) for x in inputs)
        if self.max_window > max_phrase_len:
            max_phrase_len = self.max_window
        batch_indices = []
        batch_unknowns = []
        extra_batch_indices = []
        begin_paddings = []
        end_paddings = []
        elmo_batch_words = []
        for phrase in inputs:
            if self.training:
                begin_pad_width = random.randint(0, max_phrase_len - len(phrase))
            else:
                begin_pad_width = 0
            end_pad_width = max_phrase_len - begin_pad_width - len(phrase)
            begin_paddings.append(begin_pad_width)
            end_paddings.append(end_pad_width)
            sentence_indices = [PAD_ID] * begin_pad_width
            sentence_indices.extend([map_word(x) for x in phrase])
            sentence_indices.extend([PAD_ID] * end_pad_width)
            sentence_unknowns = [idx for idx, word in enumerate(sentence_indices) if word == UNK_ID]
            batch_indices.append(sentence_indices)
            batch_unknowns.append(sentence_unknowns)
            if self.extra_vocab:
                extra_sentence_indices = [PAD_ID] * begin_pad_width
                for word in phrase:
                    if word in self.extra_vocab_map:
                        if self.training and random.random() < 0.01:
                            extra_sentence_indices.append(UNK_ID)
                        else:
                            extra_sentence_indices.append(self.extra_vocab_map[word])
                    else:
                        extra_sentence_indices.append(UNK_ID)
                extra_sentence_indices.extend([PAD_ID] * end_pad_width)
                extra_batch_indices.append(extra_sentence_indices)
            if self.config.use_elmo:
                elmo_phrase_words = [''] * begin_pad_width
                for word in phrase:
                    elmo_phrase_words.append(word)
                elmo_phrase_words.extend([''] * end_pad_width)
                elmo_batch_words.append(elmo_phrase_words)
        batch_indices = torch.tensor(batch_indices, requires_grad=False, device=device)
        input_vectors = self.embedding(batch_indices)
        for phrase_num, sentence_unknowns in enumerate(batch_unknowns):
            input_vectors[phrase_num][sentence_unknowns] = self.unk
        if self.extra_vocab:
            extra_batch_indices = torch.tensor(extra_batch_indices, requires_grad=False, device=device)
            extra_input_vectors = self.extra_embedding(extra_batch_indices)
            if self.config.extra_wordvec_method is ExtraVectors.CONCAT:
                all_inputs = [input_vectors, extra_input_vectors]
            elif self.config.extra_wordvec_method is ExtraVectors.SUM:
                all_inputs = [input_vectors + extra_input_vectors]
            else:
                raise ValueError('unable to handle {}'.format(self.config.extra_wordvec_method))
        else:
            all_inputs = [input_vectors]
        if self.forward_charlm is not None:
            char_reps_forward = self.build_char_reps(inputs, max_phrase_len, self.forward_charlm, self.charmodel_forward_projection, begin_paddings, device)
            all_inputs.append(char_reps_forward)
        if self.backward_charlm is not None:
            char_reps_backward = self.build_char_reps(inputs, max_phrase_len, self.backward_charlm, self.charmodel_backward_projection, begin_paddings, device)
            all_inputs.append(char_reps_backward)
        if self.config.use_elmo:
            elmo_arrays = self.elmo_model.sents2elmo(elmo_batch_words, output_layer=-2)
            elmo_tensors = [torch.tensor(x) for x in elmo_arrays]
            elmo_tensor = torch.stack(elmo_tensors)
            elmo_tensor = torch.transpose(elmo_tensor, 1, 3)
            elmo_tensor = torch.transpose(elmo_tensor, 1, 2)
            elmo_tensor = self.elmo_combine_layers(elmo_tensor)
            elmo_tensor = elmo_tensor.squeeze(3)
            if self.config.elmo_projection:
                elmo_tensor = self.elmo_projection(elmo_tensor)
            all_inputs.append(elmo_tensor)
        if self.bert_model is not None:
            bert_embeddings = self.extract_bert_embeddings(inputs, max_phrase_len, begin_paddings, device)
            all_inputs.append(bert_embeddings)
        input_vectors = torch.cat(all_inputs, dim=2)
        if self.config.bilstm:
            input_vectors, _ = self.bilstm(self.dropout(input_vectors))
        x = input_vectors.unsqueeze(1)
        conv_outs = []
        for conv, filter_size in zip(self.conv_layers, self.config.filter_sizes):
            if isinstance(filter_size, int):
                conv_out = self.dropout(F.relu(conv(x).squeeze(3)))
                conv_outs.append(conv_out)
            else:
                conv_out = conv(x).transpose(2, 3).flatten(1, 2)
                conv_out = self.dropout(F.relu(conv_out))
                conv_outs.append(conv_out)
        pool_outs = [F.max_pool2d(out, (self.config.maxpool_width, out.shape[2])).squeeze(2) for out in conv_outs]
        pooled = torch.cat(pool_outs, dim=1)
        previous_layer = pooled
        for fc in self.fc_layers[:-1]:
            previous_layer = self.dropout(F.relu(fc(previous_layer)))
        out = self.fc_layers[-1](previous_layer)
        return out

    def get_params(self, skip_modules=True):
        model_state = self.state_dict()
        if skip_modules:
            skipped = [k for k in model_state.keys() if self.is_unsaved_module(k)]
            for k in skipped:
                del model_state[k]
        params = {'model': model_state, 'config': self.config, 'labels': self.labels, 'extra_vocab': self.extra_vocab}
        if self.config.use_peft:
            params['bert_lora'] = get_peft_model_state_dict(self.bert_model, adapter_name=self.peft_name)
        return params

    def preprocess_data(self, sentences):
        sentences = [data.update_text(s, self.config.wordvec_type) for s in sentences]
        return sentences

    def extract_sentences(self, doc):
        return [[token.text for token in sentence.tokens] for sentence in doc.sentences]


class ConstituencyClassifier(BaseClassifier):

    def __init__(self, tree_embedding, labels, args):
        super(ConstituencyClassifier, self).__init__()
        self.labels = labels
        self.config = SimpleNamespace(fc_shapes=args.fc_shapes, dropout=args.dropout, num_classes=len(labels), constituency_backprop=args.constituency_backprop, constituency_batch_norm=args.constituency_batch_norm, constituency_node_attn=args.constituency_node_attn, constituency_top_layer=args.constituency_top_layer, constituency_all_words=args.constituency_all_words, model_type=ModelType.CONSTITUENCY)
        self.tree_embedding = tree_embedding
        self.fc_layers = build_output_layers(self.tree_embedding.output_size, self.config.fc_shapes, self.config.num_classes)
        self.dropout = nn.Dropout(self.config.dropout)

    def is_unsaved_module(self, name):
        return False

    def log_configuration(self):
        tlogger.info('Backprop into parser: %s', self.config.constituency_backprop)
        tlogger.info('Batch norm: %s', self.config.constituency_batch_norm)
        tlogger.info('Word positions used: %s', 'all words' if self.config.constituency_all_words else 'start and end words')
        tlogger.info('Attention over nodes: %s', self.config.constituency_node_attn)
        tlogger.info('Intermediate layers: %s', self.config.fc_shapes)

    def log_norms(self):
        lines = ['NORMS FOR MODEL PARAMTERS']
        lines.extend([('tree_embedding.' + x) for x in self.tree_embedding.get_norms()])
        for name, param in self.named_parameters():
            if param.requires_grad and not name.startswith('tree_embedding.'):
                lines.append('%s %.6g' % (name, torch.norm(param).item()))
        logger.info('\n'.join(lines))

    def forward(self, inputs):
        inputs = [(x.constituency if isinstance(x, SentimentDatum) else x) for x in inputs]
        embedding = self.tree_embedding.embed_trees(inputs)
        previous_layer = torch.stack([torch.max(x, dim=0)[0] for x in embedding], dim=0)
        previous_layer = self.dropout(previous_layer)
        for fc in self.fc_layers[:-1]:
            previous_layer = self.dropout(F.gelu(fc(previous_layer)))
        out = self.fc_layers[-1](previous_layer)
        return out

    def get_params(self, skip_modules=True):
        model_state = self.state_dict()
        skipped = [k for k in model_state.keys() if k.startswith('tree_embedding.')]
        for k in skipped:
            del model_state[k]
        tree_embedding = self.tree_embedding.get_params(skip_modules)
        params = {'model': model_state, 'tree_embedding': tree_embedding, 'config': self.config, 'labels': self.labels}
        return params

    def extract_sentences(self, doc):
        return [sentence.constituency for sentence in doc.sentences]


class PairwiseBilinear(nn.Module):
    """ A bilinear module that deals with broadcasting for efficient memory usage.
    Input: tensors of sizes (N x L1 x D1) and (N x L2 x D2)
    Output: tensor of size (N x L1 x L2 x O)"""

    def __init__(self, input1_size, input2_size, output_size, bias=True):
        super().__init__()
        self.input1_size = input1_size
        self.input2_size = input2_size
        self.output_size = output_size
        self.weight = nn.Parameter(torch.Tensor(input1_size, input2_size, output_size))
        self.bias = nn.Parameter(torch.Tensor(output_size)) if bias else 0

    def forward(self, input1, input2):
        input1_size = list(input1.size())
        input2_size = list(input2.size())
        output_size = [input1_size[0], input1_size[1], input2_size[1], self.output_size]
        intermediate = torch.mm(input1.view(-1, input1_size[-1]), self.weight.view(-1, self.input2_size * self.output_size))
        input2 = input2.transpose(1, 2)
        output = intermediate.view(input1_size[0], input1_size[1] * self.output_size, input2_size[2]).bmm(input2)
        output = output.view(input1_size[0], input1_size[1], self.output_size, input2_size[1]).transpose(2, 3)
        return output


class BiaffineScorer(nn.Module):

    def __init__(self, input1_size, input2_size, output_size):
        super().__init__()
        self.W_bilin = nn.Bilinear(input1_size + 1, input2_size + 1, output_size)
        self.W_bilin.weight.data.zero_()
        self.W_bilin.bias.data.zero_()

    def forward(self, input1, input2):
        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size()) - 1)
        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size()) - 1)
        return self.W_bilin(input1, input2)


class PairwiseBiaffineScorer(nn.Module):

    def __init__(self, input1_size, input2_size, output_size):
        super().__init__()
        self.W_bilin = PairwiseBilinear(input1_size + 1, input2_size + 1, output_size)
        self.W_bilin.weight.data.zero_()
        self.W_bilin.bias.data.zero_()

    def forward(self, input1, input2):
        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size()) - 1)
        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size()) - 1)
        return self.W_bilin(input1, input2)


class DeepBiaffineScorer(nn.Module):

    def __init__(self, input1_size, input2_size, hidden_size, output_size, hidden_func=F.relu, dropout=0, pairwise=True):
        super().__init__()
        self.W1 = nn.Linear(input1_size, hidden_size)
        self.W2 = nn.Linear(input2_size, hidden_size)
        self.hidden_func = hidden_func
        if pairwise:
            self.scorer = PairwiseBiaffineScorer(hidden_size, hidden_size, output_size)
        else:
            self.scorer = BiaffineScorer(hidden_size, hidden_size, output_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input1, input2):
        return self.scorer(self.dropout(self.hidden_func(self.W1(input1))), self.dropout(self.hidden_func(self.W2(input2))))


class LSTMwRecDropout(nn.Module):
    """ An LSTM implementation that supports recurrent dropout """

    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):
        super().__init__()
        self.batch_first = batch_first
        self.pad = pad
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.dropout = dropout
        self.drop = nn.Dropout(dropout, inplace=True)
        self.rec_drop = nn.Dropout(rec_dropout, inplace=True)
        self.num_directions = 2 if bidirectional else 1
        self.cells = nn.ModuleList()
        for l in range(num_layers):
            in_size = input_size if l == 0 else self.num_directions * hidden_size
            for d in range(self.num_directions):
                self.cells.append(nn.LSTMCell(in_size, hidden_size, bias=bias))

    def forward(self, input, hx=None):

        def rnn_loop(x, batch_sizes, cell, inits, reverse=False):
            batch_size = batch_sizes[0].item()
            states = [list(init.split([1] * batch_size)) for init in inits]
            h_drop_mask = x.new_ones(batch_size, self.hidden_size)
            h_drop_mask = self.rec_drop(h_drop_mask)
            resh = []
            if not reverse:
                st = 0
                for bs in batch_sizes:
                    s1 = cell(x[st:st + bs], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))
                    resh.append(s1[0])
                    for j in range(bs):
                        states[0][j] = s1[0][j].unsqueeze(0)
                        states[1][j] = s1[1][j].unsqueeze(0)
                    st += bs
            else:
                en = x.size(0)
                for i in range(batch_sizes.size(0) - 1, -1, -1):
                    bs = batch_sizes[i]
                    s1 = cell(x[en - bs:en], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))
                    resh.append(s1[0])
                    for j in range(bs):
                        states[0][j] = s1[0][j].unsqueeze(0)
                        states[1][j] = s1[1][j].unsqueeze(0)
                    en -= bs
                resh = list(reversed(resh))
            return torch.cat(resh, 0), tuple(torch.cat(s, 0) for s in states)
        all_states = [[], []]
        inputdata, batch_sizes = input.data, input.batch_sizes
        for l in range(self.num_layers):
            new_input = []
            if self.dropout > 0 and l > 0:
                inputdata = self.drop(inputdata)
            for d in range(self.num_directions):
                idx = l * self.num_directions + d
                cell = self.cells[idx]
                out, states = rnn_loop(inputdata, batch_sizes, cell, (hx[i][idx] for i in range(2)) if hx is not None else (input.data.new_zeros(input.batch_sizes[0].item(), self.hidden_size, requires_grad=False) for _ in range(2)), reverse=d == 1)
                new_input.append(out)
                all_states[0].append(states[0].unsqueeze(0))
                all_states[1].append(states[1].unsqueeze(0))
            if self.num_directions > 1:
                inputdata = torch.cat(new_input, 1)
            else:
                inputdata = new_input[0]
        input = PackedSequence(inputdata, batch_sizes)
        return input, tuple(torch.cat(x, 0) for x in all_states)


class PackedLSTM(nn.Module):

    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):
        super().__init__()
        self.batch_first = batch_first
        self.pad = pad
        if rec_dropout == 0:
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)
        else:
            self.lstm = LSTMwRecDropout(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, rec_dropout=rec_dropout)

    def forward(self, input, lengths, hx=None):
        if not isinstance(input, PackedSequence):
            input = pack_padded_sequence(input, lengths, batch_first=self.batch_first)
        res = self.lstm(input, hx)
        if self.pad:
            res = pad_packed_sequence(res[0], batch_first=self.batch_first)[0], res[1]
        return res


def tensor_unsort(sorted_tensor, oidx):
    """
    Unsort a sorted tensor on its 0-th dimension, based on the original idx.
    """
    assert sorted_tensor.size(0) == len(oidx), 'Number of list elements must match with original indices.'
    backidx = [x[0] for x in sorted(enumerate(oidx), key=lambda x: x[1])]
    return sorted_tensor[backidx]


class CharacterModel(nn.Module):

    def __init__(self, args, vocab, pad=False, bidirectional=False, attention=True):
        super().__init__()
        self.args = args
        self.pad = pad
        self.num_dir = 2 if bidirectional else 1
        self.attn = attention
        self.char_emb = nn.Embedding(len(vocab['char']), self.args['char_emb_dim'], padding_idx=0)
        if self.attn:
            self.char_attn = nn.Linear(self.num_dir * self.args['char_hidden_dim'], 1, bias=False)
            self.char_attn.weight.data.zero_()
        self.charlstm = PackedLSTM(self.args['char_emb_dim'], self.args['char_hidden_dim'], self.args['char_num_layers'], batch_first=True, dropout=0 if self.args['char_num_layers'] == 1 else args['dropout'], rec_dropout=self.args['char_rec_dropout'], bidirectional=bidirectional)
        self.charlstm_h_init = nn.Parameter(torch.zeros(self.num_dir * self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
        self.charlstm_c_init = nn.Parameter(torch.zeros(self.num_dir * self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
        self.dropout = nn.Dropout(args['dropout'])

    def forward(self, chars, chars_mask, word_orig_idx, sentlens, wordlens):
        embs = self.dropout(self.char_emb(chars))
        batch_size = embs.size(0)
        embs = pack_padded_sequence(embs, wordlens, batch_first=True)
        output = self.charlstm(embs, wordlens, hx=(self.charlstm_h_init.expand(self.num_dir * self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous(), self.charlstm_c_init.expand(self.num_dir * self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous()))
        if self.attn:
            char_reps = output[0]
            weights = torch.sigmoid(self.char_attn(self.dropout(char_reps.data)))
            char_reps = PackedSequence(char_reps.data * weights, char_reps.batch_sizes)
            char_reps, _ = pad_packed_sequence(char_reps, batch_first=True)
            res = char_reps.sum(1)
        else:
            h, c = output[1]
            res = h[-2:].transpose(0, 1).contiguous().view(batch_size, -1)
        res = tensor_unsort(res, word_orig_idx)
        res = pack_sequence(res.split(sentlens))
        if self.pad:
            res = pad_packed_sequence(res, batch_first=True)[0]
        return res


CHARLM_END = ' '


CHARLM_START = '\n'


UNK = '<UNK>'


class BaseVocab:
    """ A base class for common vocabulary operations. Each subclass should at least 
    implement its own build_vocab() function."""

    def __init__(self, data=None, lang='', idx=0, cutoff=0, lower=False):
        self.data = data
        self.lang = lang
        self.idx = idx
        self.cutoff = cutoff
        self.lower = lower
        if data is not None:
            self.build_vocab()
        self.state_attrs = ['lang', 'idx', 'cutoff', 'lower', '_unit2id', '_id2unit']

    def build_vocab(self):
        raise NotImplementedError()

    def state_dict(self):
        """ Returns a dictionary containing all states that are necessary to recover
        this vocab. Useful for serialization."""
        state = OrderedDict()
        for attr in self.state_attrs:
            if hasattr(self, attr):
                state[attr] = getattr(self, attr)
        return state

    @classmethod
    def load_state_dict(cls, state_dict):
        """ Returns a new Vocab instance constructed from a state dict. """
        new = cls()
        for attr, value in state_dict.items():
            setattr(new, attr, value)
        return new

    def normalize_unit(self, unit):
        if unit is None:
            return unit
        if self.lower:
            return unit.lower()
        return unit

    def unit2id(self, unit):
        unit = self.normalize_unit(unit)
        if unit in self._unit2id:
            return self._unit2id[unit]
        else:
            return self._unit2id[UNK]

    def id2unit(self, id):
        return self._id2unit[id]

    def map(self, units):
        return [self.unit2id(x) for x in units]

    def unmap(self, ids):
        return [self.id2unit(x) for x in ids]

    def __str__(self):
        lang_str = '(%s)' % self.lang if self.lang else ''
        name = str(type(self)) + lang_str
        return '<%s: %s>' % (name, self._id2unit)

    def __len__(self):
        return len(self._id2unit)

    def __getitem__(self, key):
        if isinstance(key, str):
            return self.unit2id(key)
        elif isinstance(key, int) or isinstance(key, list):
            return self.id2unit(key)
        else:
            raise TypeError('Vocab key must be one of str, list, or int')

    def __contains__(self, key):
        return self.normalize_unit(key) in self._unit2id

    @property
    def size(self):
        return len(self)


EOS = '<EOS>'


PAD = '<PAD>'


SOS = '<SOS>'


VOCAB_PREFIX = [PAD, UNK, SOS, EOS]


class CharVocab(BaseVocab):

    def build_vocab(self):
        if isinstance(self.data[0][0], (list, tuple)):
            counter = Counter([c for sent in self.data for w in sent for c in w[self.idx]])
            for k in list(counter.keys()):
                if counter[k] < self.cutoff:
                    del counter[k]
        else:
            counter = Counter([c for sent in self.data for c in sent])
        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))
        self._unit2id = {w: i for i, w in enumerate(self._id2unit)}


class SequenceUnitDropout(nn.Module):
    """ A unit dropout layer that's designed for input of sequence units (e.g., word sequence, char sequence, etc.).
    Given a sequence of unit indices, this layer randomly set some of them to be a replacement id (usually set to be <UNK>).
    """

    def __init__(self, dropprob, replacement_id):
        super().__init__()
        self.dropprob = dropprob
        self.replacement_id = replacement_id

    def forward(self, x):
        """ :param: x must be a LongTensor of unit indices. """
        if not self.training or self.dropprob == 0:
            return x
        masksize = [y for y in x.size()]
        dropmask = torch.rand(*masksize, device=x.device) < self.dropprob
        res = x.masked_fill(dropmask, self.replacement_id)
        return res

    def extra_repr(self):
        return 'p={}, replacement_id={}'.format(self.dropprob, self.replacement_id)


def get_long_tensor(tokens_list, batch_size, pad_id=constant.PAD_ID):
    """ Convert (list of )+ tokens to a padded LongTensor. """
    sizes = []
    x = tokens_list
    while isinstance(x[0], list):
        sizes.append(max(len(y) for y in x))
        x = [z for y in x for z in y]
    tokens = torch.LongTensor(batch_size, *sizes).fill_(pad_id)
    for i, s in enumerate(tokens_list):
        tokens[i, :len(s)] = torch.LongTensor(s)
    return tokens


class CharacterLanguageModel(nn.Module):

    def __init__(self, args, vocab, pad=False, is_forward_lm=True):
        super().__init__()
        self.args = args
        self.vocab = vocab
        self.is_forward_lm = is_forward_lm
        self.pad = pad
        self.finetune = True
        self.char_emb = nn.Embedding(len(self.vocab['char']), self.args['char_emb_dim'], padding_idx=None)
        self.charlstm = PackedLSTM(self.args['char_emb_dim'], self.args['char_hidden_dim'], self.args['char_num_layers'], batch_first=True, dropout=0 if self.args['char_num_layers'] == 1 else args['char_dropout'], rec_dropout=self.args['char_rec_dropout'], bidirectional=False)
        self.charlstm_h_init = nn.Parameter(torch.zeros(self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
        self.charlstm_c_init = nn.Parameter(torch.zeros(self.args['char_num_layers'], 1, self.args['char_hidden_dim']))
        self.decoder = nn.Linear(self.args['char_hidden_dim'], len(self.vocab['char']))
        self.dropout = nn.Dropout(args['char_dropout'])
        self.char_dropout = SequenceUnitDropout(args.get('char_unit_dropout', 0), UNK_ID)

    def forward(self, chars, charlens, hidden=None):
        chars = self.char_dropout(chars)
        embs = self.dropout(self.char_emb(chars))
        batch_size = embs.size(0)
        embs = pack_padded_sequence(embs, charlens, batch_first=True)
        if hidden is None:
            hidden = self.charlstm_h_init.expand(self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous(), self.charlstm_c_init.expand(self.args['char_num_layers'], batch_size, self.args['char_hidden_dim']).contiguous()
        output, hidden = self.charlstm(embs, charlens, hx=hidden)
        output = self.dropout(pad_packed_sequence(output, batch_first=True)[0])
        decoded = self.decoder(output)
        return output, hidden, decoded

    def get_representation(self, chars, charoffsets, charlens, char_orig_idx):
        with torch.no_grad():
            output, _, _ = self.forward(chars, charlens)
            res = [output[i, offsets] for i, offsets in enumerate(charoffsets)]
            res = unsort(res, char_orig_idx)
            res = pack_sequence(res)
            if self.pad:
                res = pad_packed_sequence(res, batch_first=True)[0]
        return res

    def per_char_representation(self, words):
        device = next(self.parameters()).device
        vocab = self.char_vocab()
        all_data = [(vocab.map(word), len(word), idx) for idx, word in enumerate(words)]
        all_data.sort(key=itemgetter(1), reverse=True)
        chars = [x[0] for x in all_data]
        char_lens = [x[1] for x in all_data]
        char_tensor = get_long_tensor(chars, len(chars), pad_id=vocab.unit2id(CHARLM_END))
        with torch.no_grad():
            output, _, _ = self.forward(char_tensor, char_lens)
            output = [x[:y, :] for x, y in zip(output, char_lens)]
            output = unsort(output, [x[2] for x in all_data])
        return output

    def build_char_representation(self, sentences):
        """
        Return values from this charlm for a list of list of words

        input: [[str]]
          K sentences, each of length Ki (can be different for each sentence)
        output: [tensor(Ki x dim)]
          list of tensors, each one with shape Ki by the dim of the character model

        Values are taken from the last character in a word for each word.
        The words are effectively treated as if they are whitespace separated
        (which may actually be somewhat inaccurate for languages such as Chinese or for MWT)
        """
        forward = self.is_forward_lm
        vocab = self.char_vocab()
        device = next(self.parameters()).device
        all_data = []
        for idx, words in enumerate(sentences):
            if not forward:
                words = [x[::-1] for x in reversed(words)]
            chars = [CHARLM_START]
            offsets = []
            for w in words:
                chars.extend(w)
                chars.append(CHARLM_END)
                offsets.append(len(chars) - 1)
            if not forward:
                offsets.reverse()
            chars = vocab.map(chars)
            all_data.append((chars, offsets, len(chars), len(all_data)))
        all_data.sort(key=itemgetter(2), reverse=True)
        chars, char_offsets, char_lens, orig_idx = tuple(zip(*all_data))
        chars = get_long_tensor(chars, len(all_data), pad_id=vocab.unit2id(CHARLM_END))
        with torch.no_grad():
            output, _, _ = self.forward(chars, char_lens)
            res = [output[i, offsets] for i, offsets in enumerate(char_offsets)]
            res = unsort(res, orig_idx)
        return res

    def hidden_dim(self):
        return self.args['char_hidden_dim']

    def char_vocab(self):
        return self.vocab['char']

    def train(self, mode=True):
        """
        Override the default train() function, so that when self.finetune == False, the training mode 
        won't be impacted by the parent models' status change.
        """
        if not mode:
            super().train(mode)
        elif self.finetune:
            super().train(mode)

    def full_state(self):
        state = {'vocab': self.vocab['char'].state_dict(), 'args': self.args, 'state_dict': self.state_dict(), 'pad': self.pad, 'is_forward_lm': self.is_forward_lm}
        return state

    def save(self, filename):
        os.makedirs(os.path.split(filename)[0], exist_ok=True)
        state = self.full_state()
        torch.save(state, filename, _use_new_zipfile_serialization=False)

    @classmethod
    def from_full_state(cls, state, finetune=False):
        vocab = {'char': CharVocab.load_state_dict(state['vocab'])}
        model = cls(state['args'], vocab, state['pad'], state['is_forward_lm'])
        model.load_state_dict(state['state_dict'])
        model.eval()
        model.finetune = finetune
        return model

    @classmethod
    def load(cls, filename, finetune=False):
        state = torch.load(filename, lambda storage, loc: storage)
        if 'state_dict' in state:
            return cls.from_full_state(state, finetune)
        return cls.from_full_state(state['model'], finetune)


class CharacterLanguageModelWordAdapter(nn.Module):
    """
    Adapts a character model to return embeddings for each character in a word
    """

    def __init__(self, charlms):
        super().__init__()
        self.charlms = charlms

    def forward(self, words):
        words = [(CHARLM_START + x + CHARLM_END) for x in words]
        padded_reps = []
        for charlm in self.charlms:
            rep = charlm.per_char_representation(words)
            padded_rep = torch.zeros(len(rep), max(x.shape[0] for x in rep), rep[0].shape[1], dtype=rep[0].dtype, device=rep[0].device)
            for idx, row in enumerate(rep):
                padded_rep[idx, :row.shape[0], :] = row
            padded_reps.append(padded_rep)
        padded_rep = torch.cat(padded_reps, dim=2)
        return padded_rep

    def hidden_dim(self):
        return sum(charlm.hidden_dim() for charlm in self.charlms)


def log_sum_exp(value, dim=None, keepdim=False):
    """Numerically stable implementation of the operation
    value.exp().sum(dim, keepdim).log()
    """
    if dim is not None:
        m, _ = torch.max(value, dim=dim, keepdim=True)
        value0 = value - m
        if keepdim is False:
            m = m.squeeze(dim)
        return m + torch.log(torch.sum(torch.exp(value0), dim=dim, keepdim=keepdim))
    else:
        m = torch.max(value)
        sum_exp = torch.sum(torch.exp(value - m))
        if isinstance(sum_exp, Number):
            return m + math.log(sum_exp)
        else:
            return m + torch.log(sum_exp)


class CRFLoss(nn.Module):
    """
    Calculate log-space crf loss, given unary potentials, a transition matrix
    and gold tag sequences.
    """

    def __init__(self, num_tag, batch_average=True):
        super().__init__()
        self._transitions = nn.Parameter(torch.zeros(num_tag, num_tag))
        self._batch_average = batch_average

    def forward(self, inputs, masks, tag_indices):
        """
        inputs: batch_size x seq_len x num_tags
        masks: batch_size x seq_len
        tag_indices: batch_size x seq_len
        
        @return:
            loss: CRF negative log likelihood on all instances.
            transitions: the transition matrix
        """
        input_bs, input_sl, input_nc = inputs.size()
        unary_scores = self.crf_unary_score(inputs, masks, tag_indices, input_bs, input_sl, input_nc)
        binary_scores = self.crf_binary_score(inputs, masks, tag_indices, input_bs, input_sl, input_nc)
        log_norm = self.crf_log_norm(inputs, masks, tag_indices)
        log_likelihood = unary_scores + binary_scores - log_norm
        loss = torch.sum(-log_likelihood)
        if self._batch_average:
            loss = loss / input_bs
        else:
            total = masks.eq(0).sum()
            loss = loss / (total + 1e-08)
        return loss, self._transitions

    def crf_unary_score(self, inputs, masks, tag_indices, input_bs, input_sl, input_nc):
        """
        @return:
            unary_scores: batch_size
        """
        flat_inputs = inputs.view(input_bs, -1)
        flat_tag_indices = tag_indices + torch.arange(input_sl, device=tag_indices.device).long().unsqueeze(0) * input_nc
        unary_scores = torch.gather(flat_inputs, 1, flat_tag_indices).view(input_bs, -1)
        unary_scores.masked_fill_(masks, 0)
        return unary_scores.sum(dim=1)

    def crf_binary_score(self, inputs, masks, tag_indices, input_bs, input_sl, input_nc):
        """
        @return:
            binary_scores: batch_size
        """
        nt = tag_indices.size(-1) - 1
        start_indices = tag_indices[:, :nt]
        end_indices = tag_indices[:, 1:]
        flat_transition_indices = start_indices * input_nc + end_indices
        flat_transition_indices = flat_transition_indices.view(-1)
        flat_transition_matrix = self._transitions.view(-1)
        binary_scores = torch.gather(flat_transition_matrix, 0, flat_transition_indices).view(input_bs, -1)
        score_masks = masks[:, 1:]
        binary_scores.masked_fill_(score_masks, 0)
        return binary_scores.sum(dim=1)

    def crf_log_norm(self, inputs, masks, tag_indices):
        """
        Calculate the CRF partition in log space for each instance, following:
            http://www.cs.columbia.edu/~mcollins/fb.pdf
        @return:
            log_norm: batch_size
        """
        start_inputs = inputs[:, 0, :]
        rest_inputs = inputs[:, 1:, :]
        rest_masks = masks[:, 1:]
        alphas = start_inputs
        trans = self._transitions.unsqueeze(0)
        for i in range(rest_inputs.size(1)):
            transition_scores = alphas.unsqueeze(2) + trans
            new_alphas = rest_inputs[:, i, :] + log_sum_exp(transition_scores, dim=1)
            m = rest_masks[:, i].unsqueeze(1).expand_as(new_alphas)
            new_alphas.masked_scatter_(m, alphas.masked_select(m))
            alphas = new_alphas
        log_norm = log_sum_exp(alphas, dim=1)
        all_masked = torch.all(masks, dim=1)
        log_norm = log_norm * torch.logical_not(all_masked)
        return log_norm


class WordDropout(nn.Module):
    """ A word dropout layer that's designed for embedded inputs (e.g., any inputs to an LSTM layer).
    Given a batch of embedded inputs, this layer randomly set some of them to be a replacement state.
    Note that this layer assumes the last dimension of the input to be the hidden dimension of a unit.
    """

    def __init__(self, dropprob):
        super().__init__()
        self.dropprob = dropprob

    def forward(self, x, replacement=None):
        if not self.training or self.dropprob == 0:
            return x
        masksize = [y for y in x.size()]
        masksize[-1] = 1
        dropmask = torch.rand(*masksize, device=x.device) < self.dropprob
        res = x.masked_fill(dropmask, 0)
        if replacement is not None:
            res = res + dropmask.float() * replacement
        return res

    def extra_repr(self):
        return 'p={}'.format(self.dropprob)


class LockedDropout(nn.Module):
    """
    A variant of dropout layer that consistently drops out the same parameters over time. Also known as the variational dropout. 
    This implementation was modified from the LockedDropout implementation in the flair library (https://github.com/zalandoresearch/flair).
    """

    def __init__(self, dropprob, batch_first=True):
        super().__init__()
        self.dropprob = dropprob
        self.batch_first = batch_first

    def forward(self, x):
        if not self.training or self.dropprob == 0:
            return x
        if not self.batch_first:
            m = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.dropprob)
        else:
            m = x.new_empty(x.size(0), 1, x.size(2), requires_grad=False).bernoulli_(1 - self.dropprob)
        mask = m.div(1 - self.dropprob).expand_as(x)
        return mask * x

    def extra_repr(self):
        return 'p={}'.format(self.dropprob)


class HLSTMCell(nn.modules.rnn.RNNCellBase):
    """
    A Highway LSTM Cell as proposed in Zhang et al. (2018) Highway Long Short-Term Memory RNNs for 
    Distant Speech Recognition.
    """

    def __init__(self, input_size, hidden_size, bias=True):
        super(HLSTMCell, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.Wi = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
        self.Wf = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
        self.Wo = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
        self.Wg = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)
        self.gate = nn.Linear(input_size + 2 * hidden_size, hidden_size, bias=bias)

    def forward(self, input, c_l_minus_one=None, hx=None):
        self.check_forward_input(input)
        if hx is None:
            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
            hx = hx, hx
        if c_l_minus_one is None:
            c_l_minus_one = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)
        self.check_forward_hidden(input, hx[0], '[0]')
        self.check_forward_hidden(input, hx[1], '[1]')
        self.check_forward_hidden(input, c_l_minus_one, 'c_l_minus_one')
        rec_input = torch.cat([input, hx[0]], 1)
        i = F.sigmoid(self.Wi(rec_input))
        f = F.sigmoid(self.Wf(rec_input))
        o = F.sigmoid(self.Wo(rec_input))
        g = F.tanh(self.Wg(rec_input))
        gate = F.sigmoid(self.gate(torch.cat([c_l_minus_one, hx[1], input], 1)))
        c = gate * c_l_minus_one + f * hx[1] + i * g
        h = o * F.tanh(c)
        return h, c


class HighwayLSTM(nn.Module):
    """
    A Highway LSTM network, as used in the original Tensorflow version of the Dozat parser. Note that this
    is independent from the HLSTMCell above.
    """

    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0, bidirectional=False, rec_dropout=0, highway_func=None, pad=False):
        super(HighwayLSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.dropout_state = {}
        self.bidirectional = bidirectional
        self.num_directions = 2 if bidirectional else 1
        self.highway_func = highway_func
        self.pad = pad
        self.lstm = nn.ModuleList()
        self.highway = nn.ModuleList()
        self.gate = nn.ModuleList()
        self.drop = nn.Dropout(dropout, inplace=True)
        in_size = input_size
        for l in range(num_layers):
            self.lstm.append(PackedLSTM(in_size, hidden_size, num_layers=1, bias=bias, batch_first=batch_first, dropout=0, bidirectional=bidirectional, rec_dropout=rec_dropout))
            self.highway.append(nn.Linear(in_size, hidden_size * self.num_directions))
            self.gate.append(nn.Linear(in_size, hidden_size * self.num_directions))
            self.highway[-1].bias.data.zero_()
            self.gate[-1].bias.data.zero_()
            in_size = hidden_size * self.num_directions

    def forward(self, input, seqlens, hx=None):
        highway_func = (lambda x: x) if self.highway_func is None else self.highway_func
        hs = []
        cs = []
        if not isinstance(input, PackedSequence):
            input = pack_padded_sequence(input, seqlens, batch_first=self.batch_first)
        for l in range(self.num_layers):
            if l > 0:
                input = PackedSequence(self.drop(input.data), input.batch_sizes, input.sorted_indices, input.unsorted_indices)
            layer_hx = (hx[0][l * self.num_directions:(l + 1) * self.num_directions], hx[1][l * self.num_directions:(l + 1) * self.num_directions]) if hx is not None else None
            h, (ht, ct) = self.lstm[l](input, seqlens, layer_hx)
            hs.append(ht)
            cs.append(ct)
            input = PackedSequence(h.data + torch.sigmoid(self.gate[l](input.data)) * highway_func(self.highway[l](input.data)), input.batch_sizes, input.sorted_indices, input.unsorted_indices)
        if self.pad:
            input = pad_packed_sequence(input, batch_first=self.batch_first)[0]
        return input, (torch.cat(hs, 0), torch.cat(cs, 0))


class LargeMarginInSoftmaxLoss(nn.CrossEntropyLoss):
    """
    This combines the Softmax Cross-Entropy Loss (nn.CrossEntropyLoss) and the large-margin inducing
    regularization proposed in
       T. Kobayashi, "Large-Margin In Softmax Cross-Entropy Loss." In BMVC2019.

    This loss function inherits the parameters from nn.CrossEntropyLoss except for `reg_lambda` and `deg_logit`.
    Args:
         reg_lambda (float, optional): a regularization parameter. (default: 0.3)
         deg_logit (bool, optional): underestimate (degrade) the target logit by -1 or not. (default: False)
                                     If True, it realizes the method that incorporates the modified loss into ours
                                     as described in the above paper (Table 4).
    """

    def __init__(self, reg_lambda=0.3, deg_logit=None, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean'):
        super(LargeMarginInSoftmaxLoss, self).__init__(weight=weight, size_average=size_average, ignore_index=ignore_index, reduce=reduce, reduction=reduction)
        self.reg_lambda = reg_lambda
        self.deg_logit = deg_logit

    def forward(self, input, target):
        N = input.size(0)
        C = input.size(1)
        Mask = torch.zeros_like(input, requires_grad=False)
        Mask[range(N), target] = 1
        if self.deg_logit is not None:
            input = input - self.deg_logit * Mask
        loss = F.cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
        X = input - 1000000.0 * Mask
        reg = 0.5 * ((F.softmax(X, dim=1) - 1.0 / (C - 1)) * F.log_softmax(X, dim=1) * (1.0 - Mask)).sum(dim=1)
        if self.reduction == 'sum':
            reg = reg.sum()
        elif self.reduction == 'mean':
            reg = reg.mean()
        elif self.reduction == 'none':
            reg = reg
        return loss + self.reg_lambda * reg


class FocalLoss(nn.Module):
    """
    Uses the model's assessment of how likely the correct answer is
    to weight the loss for a each error

    multi-category focal loss, in other words

    from "Focal Loss for Dense Object Detection"

    https://arxiv.org/abs/1708.02002
    """

    def __init__(self, reduction='mean', gamma=2.0):
        super().__init__()
        if reduction not in ('sum', 'none', 'mean'):
            raise ValueError('Unknown reduction: %s' % reduction)
        self.reduction = reduction
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
        self.gamma = gamma

    def forward(self, inputs, targets):
        """
        Weight the loss using the models assessment of the correct answer

        inputs: [N, C]
        targets: [N]
        """
        if len(inputs.shape) == 2 and len(targets.shape) == 1:
            if inputs.shape[0] != targets.shape[0]:
                raise ValueError('Expected inputs N,C and targets N, but got {} and {}'.format(inputs.shape, targets.shape))
        elif len(inputs.shape) == 1 and len(targets.shape) == 0:
            raise NotImplementedError("This would be a reasonable thing to implement, but we haven't done it yet")
        else:
            raise ValueError('Expected inputs N,C and targets N, but got {} and {}'.format(inputs.shape, targets.shape))
        raw_loss = self.ce_loss(inputs, targets)
        assert len(raw_loss.shape) == 1 and raw_loss.shape[0] == inputs.shape[0]
        final_loss = raw_loss * (1 - torch.exp(-raw_loss)) ** self.gamma
        assert len(final_loss.shape) == 1 and final_loss.shape[0] == inputs.shape[0]
        if self.reduction == 'sum':
            return final_loss.sum()
        elif self.reduction == 'mean':
            return final_loss.mean()
        elif self.reduction == 'none':
            return final_loss
        raise AssertionError('unknown reduction!  how did this happen??')


def SequenceLoss(vocab_size):
    weight = torch.ones(vocab_size)
    weight[constant.PAD_ID] = 0
    crit = nn.NLLLoss(weight)
    return crit


class MixLoss(nn.Module):
    """
    A mixture of SequenceLoss and CrossEntropyLoss.
    Loss = SequenceLoss + alpha * CELoss
    """

    def __init__(self, vocab_size, alpha):
        super().__init__()
        self.seq_loss = SequenceLoss(vocab_size)
        self.ce_loss = nn.CrossEntropyLoss()
        assert alpha >= 0
        self.alpha = alpha

    def forward(self, seq_inputs, seq_targets, class_inputs, class_targets):
        sl = self.seq_loss(seq_inputs, seq_targets)
        cel = self.ce_loss(class_inputs, class_targets)
        loss = sl + self.alpha * cel
        return loss


class MaxEntropySequenceLoss(nn.Module):
    """
    A max entropy loss that encourage the model to have large entropy,
    therefore giving more diverse outputs.

    Loss = NLLLoss + alpha * EntropyLoss
    """

    def __init__(self, vocab_size, alpha):
        super().__init__()
        weight = torch.ones(vocab_size)
        weight[constant.PAD_ID] = 0
        self.nll = nn.NLLLoss(weight)
        self.alpha = alpha

    def forward(self, inputs, targets):
        """
        inputs: [N, C]
        targets: [N]
        """
        assert inputs.size(0) == targets.size(0)
        nll_loss = self.nll(inputs, targets)
        mask = targets.eq(constant.PAD_ID).unsqueeze(1).expand_as(inputs)
        masked_inputs = inputs.clone().masked_fill_(mask, 0.0)
        p = torch.exp(masked_inputs)
        ent_loss = p.mul(masked_inputs).sum() / inputs.size(0)
        loss = nll_loss + self.alpha * ent_loss
        return loss


class MaxoutLinear(nn.Module):

    def __init__(self, in_channels, out_channels, maxout_k):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.maxout_k = maxout_k
        self.linear = nn.Linear(in_channels, out_channels * maxout_k)

    def forward(self, inputs):
        """
        Use the oversized linear as the repeated linear, then take the max

        One large linear map makes the implementation simpler and easier for pytorch to make parallel
        """
        outputs = self.linear(inputs)
        outputs = outputs.view(*outputs.shape[:-1], self.maxout_k, self.out_channels)
        outputs = torch.max(outputs, dim=-2)[0]
        return outputs


class Beam(object):

    def __init__(self, size, device=None):
        self.size = size
        self.done = False
        self.scores = torch.zeros(size, dtype=torch.float32, device=device)
        self.allScores = []
        self.prevKs = []
        self.nextYs = [torch.zeros(size, dtype=torch.int64, device=device).fill_(constant.PAD_ID)]
        self.nextYs[0][0] = constant.SOS_ID
        self.copy = []

    def get_current_state(self):
        """Get the outputs for the current timestep."""
        return self.nextYs[-1]

    def get_current_origin(self):
        """Get the backpointers for the current timestep."""
        return self.prevKs[-1]

    def advance(self, wordLk, copy_indices=None):
        """
        Given prob over words for every last beam `wordLk` and attention
        `attnOut`: Compute and update the beam search.

        Parameters:

        * `wordLk`- probs of advancing from the last step (K x words)
        * `copy_indices` - copy indices (K x ctx_len)

        Returns: True if beam search is complete.
        """
        if self.done:
            return True
        numWords = wordLk.size(1)
        if len(self.prevKs) > 0:
            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)
        else:
            beamLk = wordLk[0]
        flatBeamLk = beamLk.view(-1)
        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)
        self.allScores.append(self.scores)
        self.scores = bestScores
        prevK = trunc_division(bestScoresId, numWords)
        self.prevKs.append(prevK)
        self.nextYs.append(bestScoresId - prevK * numWords)
        if copy_indices is not None:
            self.copy.append(copy_indices.index_select(0, prevK))
        if self.nextYs[-1][0] == constant.EOS_ID:
            self.done = True
            self.allScores.append(self.scores)
        return self.done

    def sort_best(self):
        return torch.sort(self.scores, 0, True)

    def get_best(self):
        """Get the score of the best in the beam."""
        scores, ids = self.sortBest()
        return scores[1], ids[1]

    def get_hyp(self, k):
        """
        Walk back to construct the full hypothesis.

        Parameters:

             * `k` - the position in the beam to construct.

         Returns: The hypothesis
        """
        hyp = []
        cpy = []
        for j in range(len(self.prevKs) - 1, -1, -1):
            hyp.append(self.nextYs[j + 1][k])
            if len(self.copy) > 0:
                cpy.append(self.copy[j][k])
            k = self.prevKs[j][k]
        hyp = hyp[::-1]
        cpy = cpy[::-1]
        for i, cidx in enumerate(cpy):
            if cidx >= 0:
                hyp[i] = -(cidx + 1)
        return hyp


class BasicAttention(nn.Module):
    """
    A basic MLP attention layer.
    """

    def __init__(self, dim):
        super(BasicAttention, self).__init__()
        self.linear_in = nn.Linear(dim, dim, bias=False)
        self.linear_c = nn.Linear(dim, dim)
        self.linear_v = nn.Linear(dim, 1, bias=False)
        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
        self.tanh = nn.Tanh()
        self.sm = nn.Softmax(dim=1)

    def forward(self, input, context, mask=None, attn_only=False):
        """
        input: batch x dim
        context: batch x sourceL x dim
        """
        batch_size = context.size(0)
        source_len = context.size(1)
        dim = context.size(2)
        target = self.linear_in(input)
        source = self.linear_c(context.contiguous().view(-1, dim)).view(batch_size, source_len, dim)
        attn = target.unsqueeze(1).expand_as(context) + source
        attn = self.tanh(attn)
        attn = self.linear_v(attn.view(-1, dim)).view(batch_size, source_len)
        if mask is not None:
            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
        attn = self.sm(attn)
        if attn_only:
            return attn
        weighted_context = torch.bmm(attn.unsqueeze(1), context).squeeze(1)
        h_tilde = torch.cat((weighted_context, input), 1)
        h_tilde = self.tanh(self.linear_out(h_tilde))
        return h_tilde, attn


class DeepAttention(nn.Module):
    """ A deep attention form, invented by Robert:
        u = ReLU(Wx)
        v = ReLU(Wy)
        a = V.(u o v)
    """

    def __init__(self, dim):
        super(DeepAttention, self).__init__()
        self.linear_in = nn.Linear(dim, dim, bias=False)
        self.linear_v = nn.Linear(dim, 1, bias=False)
        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
        self.relu = nn.ReLU()
        self.sm = nn.Softmax(dim=1)
        self.tanh = nn.Tanh()
        self.mask = None

    def forward(self, input, context, mask=None, attn_only=False):
        """
        input: batch x dim
        context: batch x sourceL x dim
        """
        batch_size = context.size(0)
        source_len = context.size(1)
        dim = context.size(2)
        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)
        u = self.relu(self.linear_in(u))
        v = self.relu(self.linear_in(context.contiguous().view(-1, dim)))
        attn = self.linear_v(u.mul(v)).view(batch_size, source_len)
        if mask is not None:
            assert mask.size() == attn.size(), 'Mask size must match the attention size!'
            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
        attn = self.sm(attn)
        if attn_only:
            return attn
        attn3 = attn.view(batch_size, 1, source_len)
        weighted_context = torch.bmm(attn3, context).squeeze(1)
        h_tilde = torch.cat((weighted_context, input), 1)
        h_tilde = self.tanh(self.linear_out(h_tilde))
        return h_tilde, attn


class LinearAttention(nn.Module):
    """ A linear attention form, inspired by BiDAF:
        a = W (u; v; u o v)
    """

    def __init__(self, dim):
        super(LinearAttention, self).__init__()
        self.linear = nn.Linear(dim * 3, 1, bias=False)
        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
        self.sm = nn.Softmax(dim=1)
        self.tanh = nn.Tanh()
        self.mask = None

    def forward(self, input, context, mask=None, attn_only=False):
        """
        input: batch x dim
        context: batch x sourceL x dim
        """
        batch_size = context.size(0)
        source_len = context.size(1)
        dim = context.size(2)
        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)
        v = context.contiguous().view(-1, dim)
        attn_in = torch.cat((u, v, u.mul(v)), 1)
        attn = self.linear(attn_in).view(batch_size, source_len)
        if mask is not None:
            assert mask.size() == attn.size(), 'Mask size must match the attention size!'
            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
        attn = self.sm(attn)
        if attn_only:
            return attn
        attn3 = attn.view(batch_size, 1, source_len)
        weighted_context = torch.bmm(attn3, context).squeeze(1)
        h_tilde = torch.cat((weighted_context, input), 1)
        h_tilde = self.tanh(self.linear_out(h_tilde))
        return h_tilde, attn


class SoftDotAttention(nn.Module):
    """Soft Dot Attention.

    Ref: http://www.aclweb.org/anthology/D15-1166
    Adapted from PyTorch OPEN NMT.
    """

    def __init__(self, dim):
        """Initialize layer."""
        super(SoftDotAttention, self).__init__()
        self.linear_in = nn.Linear(dim, dim, bias=False)
        self.sm = nn.Softmax(dim=1)
        self.linear_out = nn.Linear(dim * 2, dim, bias=False)
        self.tanh = nn.Tanh()
        self.mask = None

    def forward(self, input, context, mask=None, attn_only=False, return_logattn=False):
        """Propagate input through the network.

        input: batch x dim
        context: batch x sourceL x dim
        """
        target = self.linear_in(input).unsqueeze(2)
        attn = torch.bmm(context, target).squeeze(2)
        if mask is not None:
            assert mask.size() == attn.size(), 'Mask size must match the attention size!'
            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)
        if return_logattn:
            attn = torch.log_softmax(attn, 1)
            attn_w = torch.exp(attn)
        else:
            attn = self.sm(attn)
            attn_w = attn
        if attn_only:
            return attn
        attn3 = attn_w.view(attn_w.size(0), 1, attn_w.size(1))
        weighted_context = torch.bmm(attn3, context).squeeze(1)
        h_tilde = torch.cat((weighted_context, input), 1)
        h_tilde = self.tanh(self.linear_out(h_tilde))
        return h_tilde, attn


class LSTMAttention(nn.Module):
    """A long short-term memory (LSTM) cell with attention."""

    def __init__(self, input_size, hidden_size, batch_first=True, attn_type='soft'):
        """Initialize params."""
        super(LSTMAttention, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.batch_first = batch_first
        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)
        if attn_type == 'soft':
            self.attention_layer = SoftDotAttention(hidden_size)
        elif attn_type == 'mlp':
            self.attention_layer = BasicAttention(hidden_size)
        elif attn_type == 'linear':
            self.attention_layer = LinearAttention(hidden_size)
        elif attn_type == 'deep':
            self.attention_layer = DeepAttention(hidden_size)
        else:
            raise Exception('Unsupported LSTM attention type: {}'.format(attn_type))
        logger.debug('Using {} attention for LSTM.'.format(attn_type))

    def forward(self, input, hidden, ctx, ctx_mask=None, return_logattn=False):
        """Propagate input through the network."""
        if self.batch_first:
            input = input.transpose(0, 1)
        output = []
        attn = []
        steps = range(input.size(0))
        for i in steps:
            hidden = self.lstm_cell(input[i], hidden)
            hy, cy = hidden
            h_tilde, alpha = self.attention_layer(hy, ctx, mask=ctx_mask, return_logattn=return_logattn)
            output.append(h_tilde)
            attn.append(alpha)
        output = torch.cat(output, 0).view(input.size(0), *output[0].size())
        if self.batch_first:
            output = output.transpose(0, 1)
        if return_logattn:
            attn = torch.stack(attn, 0)
            if self.batch_first:
                attn = attn.transpose(0, 1)
            return output, hidden, attn
        return output, hidden


class Seq2SeqModel(nn.Module):
    """
    A complete encoder-decoder model, with optional attention.

    A parent class which makes use of the contextual_embedding (such as a charlm)
    can make use of unsaved_modules when saving.
    """

    def __init__(self, args, emb_matrix=None, contextual_embedding=None):
        super().__init__()
        self.unsaved_modules = []
        self.vocab_size = args['vocab_size']
        self.emb_dim = args['emb_dim']
        self.hidden_dim = args['hidden_dim']
        self.nlayers = args['num_layers']
        self.emb_dropout = args.get('emb_dropout', 0.0)
        self.dropout = args['dropout']
        self.pad_token = constant.PAD_ID
        self.max_dec_len = args['max_dec_len']
        self.top = args.get('top', 10000000000.0)
        self.args = args
        self.emb_matrix = emb_matrix
        self.add_unsaved_module('contextual_embedding', contextual_embedding)
        logger.debug('Building an attentional Seq2Seq model...')
        logger.debug('Using a Bi-LSTM encoder')
        self.num_directions = 2
        self.enc_hidden_dim = self.hidden_dim // 2
        self.dec_hidden_dim = self.hidden_dim
        self.use_pos = args.get('pos', False)
        self.pos_dim = args.get('pos_dim', 0)
        self.pos_vocab_size = args.get('pos_vocab_size', 0)
        self.pos_dropout = args.get('pos_dropout', 0)
        self.edit = args.get('edit', False)
        self.num_edit = args.get('num_edit', 0)
        self.copy = args.get('copy', False)
        self.emb_drop = nn.Dropout(self.emb_dropout)
        self.drop = nn.Dropout(self.dropout)
        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim, self.pad_token)
        self.input_dim = self.emb_dim
        if self.contextual_embedding is not None:
            self.input_dim += self.contextual_embedding.hidden_dim()
        self.encoder = nn.LSTM(self.input_dim, self.enc_hidden_dim, self.nlayers, bidirectional=True, batch_first=True, dropout=self.dropout if self.nlayers > 1 else 0)
        self.decoder = LSTMAttention(self.emb_dim, self.dec_hidden_dim, batch_first=True, attn_type=self.args['attn_type'])
        self.dec2vocab = nn.Linear(self.dec_hidden_dim, self.vocab_size)
        if self.use_pos and self.pos_dim > 0:
            logger.debug('Using POS in encoder')
            self.pos_embedding = nn.Embedding(self.pos_vocab_size, self.pos_dim, self.pad_token)
            self.pos_drop = nn.Dropout(self.pos_dropout)
        if self.edit:
            edit_hidden = self.hidden_dim // 2
            self.edit_clf = nn.Sequential(nn.Linear(self.hidden_dim, edit_hidden), nn.ReLU(), nn.Linear(edit_hidden, self.num_edit))
        if self.copy:
            self.copy_gate = nn.Linear(self.dec_hidden_dim, 1)
        SOS_tensor = torch.LongTensor([constant.SOS_ID])
        self.register_buffer('SOS_tensor', SOS_tensor)
        self.init_weights()

    def add_unsaved_module(self, name, module):
        self.unsaved_modules += [name]
        setattr(self, name, module)

    def init_weights(self):
        init_range = constant.EMB_INIT_RANGE
        if self.emb_matrix is not None:
            if isinstance(self.emb_matrix, np.ndarray):
                self.emb_matrix = torch.from_numpy(self.emb_matrix)
            assert self.emb_matrix.size() == (self.vocab_size, self.emb_dim), 'Input embedding matrix must match size: {} x {}'.format(self.vocab_size, self.emb_dim)
            self.embedding.weight.data.copy_(self.emb_matrix)
        else:
            self.embedding.weight.data.uniform_(-init_range, init_range)
        if self.top <= 0:
            logger.debug('Do not finetune embedding layer.')
            self.embedding.weight.requires_grad = False
        elif self.top < self.vocab_size:
            logger.debug('Finetune top {} embeddings.'.format(self.top))
            self.embedding.weight.register_hook(lambda x: utils.keep_partial_grad(x, self.top))
        else:
            logger.debug('Finetune all embeddings.')
        if self.use_pos:
            self.pos_embedding.weight.data.uniform_(-init_range, init_range)

    def zero_state(self, inputs):
        batch_size = inputs.size(0)
        device = self.SOS_tensor.device
        h0 = torch.zeros(self.encoder.num_layers * 2, batch_size, self.enc_hidden_dim, requires_grad=False, device=device)
        c0 = torch.zeros(self.encoder.num_layers * 2, batch_size, self.enc_hidden_dim, requires_grad=False, device=device)
        return h0, c0

    def encode(self, enc_inputs, lens):
        """ Encode source sequence. """
        h0, c0 = self.zero_state(enc_inputs)
        packed_inputs = nn.utils.rnn.pack_padded_sequence(enc_inputs, lens, batch_first=True)
        packed_h_in, (hn, cn) = self.encoder(packed_inputs, (h0, c0))
        h_in, _ = nn.utils.rnn.pad_packed_sequence(packed_h_in, batch_first=True)
        hn = torch.cat((hn[-1], hn[-2]), 1)
        cn = torch.cat((cn[-1], cn[-2]), 1)
        return h_in, (hn, cn)

    def decode(self, dec_inputs, hn, cn, ctx, ctx_mask=None, src=None, never_decode_unk=False):
        """ Decode a step, based on context encoding and source context states."""
        dec_hidden = hn, cn
        decoder_output = self.decoder(dec_inputs, dec_hidden, ctx, ctx_mask, return_logattn=self.copy)
        if self.copy:
            h_out, dec_hidden, log_attn = decoder_output
        else:
            h_out, dec_hidden = decoder_output
        h_out_reshape = h_out.contiguous().view(h_out.size(0) * h_out.size(1), -1)
        decoder_logits = self.dec2vocab(h_out_reshape)
        decoder_logits = decoder_logits.view(h_out.size(0), h_out.size(1), -1)
        log_probs = self.get_log_prob(decoder_logits)
        if self.copy:
            copy_logit = self.copy_gate(h_out)
            if self.use_pos:
                log_attn = log_attn[:, :, 1:]
            log_attn = torch.log_softmax(log_attn, -1)
            log_copy_prob = torch.nn.functional.logsigmoid(copy_logit) + log_attn
            mx = log_copy_prob.max(-1, keepdim=True)[0]
            log_copy_prob = log_copy_prob - mx
            copy_prob = torch.exp(log_copy_prob)
            copied_vocab_shape = list(log_probs.size())
            if torch.max(src) >= copied_vocab_shape[-1]:
                copied_vocab_shape[-1] = torch.max(src) + 1
            copied_vocab_prob = log_probs.new_zeros(copied_vocab_shape)
            scattered_copy = src.unsqueeze(1).expand(src.size(0), copy_prob.size(1), src.size(1))
            copied_vocab_prob = copied_vocab_prob.scatter_add(-1, scattered_copy, copy_prob)
            zero_mask = copied_vocab_prob == 0
            log_copied_vocab_prob = torch.log(copied_vocab_prob.masked_fill(zero_mask, 1e-12)) + mx
            log_copied_vocab_prob = log_copied_vocab_prob.masked_fill(zero_mask, -1000000000000.0)
            log_nocopy_prob = -torch.log(1 + torch.exp(copy_logit))
            if log_probs.shape[-1] < copied_vocab_shape[-1]:
                new_log_probs = log_probs.new_zeros(copied_vocab_shape)
                new_log_probs[:, :, :log_probs.shape[-1]] = log_probs
                new_log_probs[:, :, log_probs.shape[-1]:] = new_log_probs[:, :, UNK_ID].unsqueeze(2)
                log_probs = new_log_probs
            log_probs = log_probs + log_nocopy_prob
            log_probs = torch.logsumexp(torch.stack([log_copied_vocab_prob, log_probs]), 0)
        if never_decode_unk:
            log_probs[:, :, UNK_ID] = float('-inf')
        return log_probs, dec_hidden

    def embed(self, src, src_mask, pos, raw):
        embed_src = src.clone()
        embed_src[embed_src >= self.vocab_size] = UNK_ID
        enc_inputs = self.emb_drop(self.embedding(embed_src))
        batch_size = enc_inputs.size(0)
        if self.use_pos:
            assert pos is not None, 'Missing POS input for seq2seq lemmatizer.'
            pos_inputs = self.pos_drop(self.pos_embedding(pos))
            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)
            pos_src_mask = src_mask.new_zeros([batch_size, 1])
            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)
        if raw is not None and self.contextual_embedding is not None:
            raw_inputs = self.contextual_embedding(raw)
            if self.use_pos:
                raw_zeros = raw_inputs.new_zeros((raw_inputs.shape[0], 1, raw_inputs.shape[2]))
                raw_inputs = torch.cat([raw_inputs, raw_zeros], dim=1)
            enc_inputs = torch.cat([enc_inputs, raw_inputs], dim=2)
        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))
        return enc_inputs, batch_size, src_lens, src_mask

    def forward(self, src, src_mask, tgt_in, pos=None, raw=None):
        enc_inputs, batch_size, src_lens, src_mask = self.embed(src, src_mask, pos, raw)
        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
        if self.edit:
            edit_logits = self.edit_clf(hn)
        else:
            edit_logits = None
        dec_inputs = self.emb_drop(self.embedding(tgt_in))
        log_probs, _ = self.decode(dec_inputs, hn, cn, h_in, src_mask, src=src)
        return log_probs, edit_logits

    def get_log_prob(self, logits):
        logits_reshape = logits.view(-1, self.vocab_size)
        log_probs = F.log_softmax(logits_reshape, dim=1)
        if logits.dim() == 2:
            return log_probs
        return log_probs.view(logits.size(0), logits.size(1), logits.size(2))

    def predict_greedy(self, src, src_mask, pos=None, raw=None, never_decode_unk=False):
        """ Predict with greedy decoding. """
        enc_inputs, batch_size, src_lens, src_mask = self.embed(src, src_mask, pos, raw)
        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
        if self.edit:
            edit_logits = self.edit_clf(hn)
        else:
            edit_logits = None
        dec_inputs = self.embedding(self.SOS_tensor)
        dec_inputs = dec_inputs.expand(batch_size, dec_inputs.size(0), dec_inputs.size(1))
        done = [(False) for _ in range(batch_size)]
        total_done = 0
        max_len = 0
        output_seqs = [[] for _ in range(batch_size)]
        while total_done < batch_size and max_len < self.max_dec_len:
            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask, src=src, never_decode_unk=never_decode_unk)
            assert log_probs.size(1) == 1, 'Output must have 1-step of output.'
            _, preds = log_probs.squeeze(1).max(1, keepdim=True)
            dec_inputs = preds.clone()
            dec_inputs[dec_inputs >= self.vocab_size] = UNK_ID
            dec_inputs = self.embedding(dec_inputs)
            max_len += 1
            for i in range(batch_size):
                if not done[i]:
                    token = preds.data[i][0].item()
                    if token == constant.EOS_ID:
                        done[i] = True
                        total_done += 1
                    else:
                        output_seqs[i].append(token)
        return output_seqs, edit_logits

    def predict(self, src, src_mask, pos=None, beam_size=5, raw=None, never_decode_unk=False):
        """ Predict with beam search. """
        if beam_size == 1:
            return self.predict_greedy(src, src_mask, pos, raw, never_decode_unk=never_decode_unk)
        enc_inputs, batch_size, src_lens, src_mask = self.embed(src, src_mask, pos, raw)
        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)
        if self.edit:
            edit_logits = self.edit_clf(hn)
        else:
            edit_logits = None
        with torch.no_grad():
            h_in = h_in.data.repeat(beam_size, 1, 1)
            src_mask = src_mask.repeat(beam_size, 1)
            hn = hn.data.repeat(beam_size, 1)
            cn = cn.data.repeat(beam_size, 1)
        device = self.SOS_tensor.device
        beam = [Beam(beam_size, device) for _ in range(batch_size)]

        def update_state(states, idx, positions, beam_size):
            """ Select the states according to back pointers. """
            for e in states:
                br, d = e.size()
                s = e.contiguous().view(beam_size, br // beam_size, d)[:, idx]
                s.data.copy_(s.data.index_select(0, positions))
        for i in range(self.max_dec_len):
            dec_inputs = torch.stack([b.get_current_state() for b in beam]).t().contiguous().view(-1, 1)
            dec_inputs[dec_inputs >= self.vocab_size] = UNK_ID
            dec_inputs = self.embedding(dec_inputs)
            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask, src=src, never_decode_unk=never_decode_unk)
            log_probs = log_probs.view(beam_size, batch_size, -1).transpose(0, 1).contiguous()
            done = []
            for b in range(batch_size):
                is_done = beam[b].advance(log_probs.data[b])
                if is_done:
                    done += [b]
                update_state((hn, cn), b, beam[b].get_current_origin(), beam_size)
            if len(done) == batch_size:
                break
        all_hyp, all_scores = [], []
        for b in range(batch_size):
            scores, ks = beam[b].sort_best()
            all_scores += [scores[0]]
            k = ks[0]
            hyp = beam[b].get_hyp(k)
            hyp = utils.prune_hyp(hyp)
            hyp = [i.item() for i in hyp]
            all_hyp += [hyp]
        return all_hyp, edit_logits


BertRecord = namedtuple('BertRecord', ['model', 'tokenizer', 'peft_ids'])


class PretrainedWordVocab(BaseVocab):

    def build_vocab(self):
        self._id2unit = VOCAB_PREFIX + self.data
        self._unit2id = {w: i for i, w in enumerate(self._id2unit)}

    def normalize_unit(self, unit):
        unit = super().normalize_unit(unit)
        if unit:
            unit = unit.replace(' ', '\xa0')
        return unit


class Pretrain:
    """ A loader and saver for pretrained embeddings. """

    def __init__(self, filename=None, vec_filename=None, max_vocab=-1, save_to_file=True, csv_filename=None):
        self.filename = filename
        self._vec_filename = vec_filename
        self._csv_filename = csv_filename
        self._max_vocab = max_vocab
        self._save_to_file = save_to_file

    @property
    def vocab(self):
        if not hasattr(self, '_vocab'):
            self.load()
        return self._vocab

    @property
    def emb(self):
        if not hasattr(self, '_emb'):
            self.load()
        return self._emb

    def load(self):
        if self.filename is not None and os.path.exists(self.filename):
            try:
                data = torch.load(self.filename, lambda storage, loc: storage)
                logger.debug('Loaded pretrain from {}'.format(self.filename))
                if not isinstance(data, dict):
                    raise RuntimeError("File {} exists but is not a stanza pretrain file.  It is not a dict, whereas a Stanza pretrain should have a dict with 'emb' and 'vocab'".format(self.filename))
                if 'emb' not in data or 'vocab' not in data:
                    raise RuntimeError("File {} exists but is not a stanza pretrain file.  A Stanza pretrain file should have 'emb' and 'vocab' fields in its state dict".format(self.filename))
                self._vocab, self._emb = PretrainedWordVocab.load_state_dict(data['vocab']), data['emb']
                return
            except (KeyboardInterrupt, SystemExit):
                raise
            except BaseException as e:
                if not self._vec_filename and not self._csv_filename:
                    raise
                logger.warning('Pretrained file exists but cannot be loaded from {}, due to the following exception:\n\t{}'.format(self.filename, e))
                vocab, emb = self.read_pretrain()
        else:
            if not self._vec_filename and not self._csv_filename:
                raise FileNotFoundError('Pretrained file {} does not exist, and no text/xz file was provided'.format(self.filename))
            if self.filename is not None:
                logger.info('Pretrained filename %s specified, but file does not exist.  Attempting to load from text file' % self.filename)
            vocab, emb = self.read_pretrain()
        self._vocab = vocab
        self._emb = emb
        if self._save_to_file:
            assert self.filename is not None, 'Filename must be provided to save pretrained vector to file.'
            self.save(self.filename)

    def save(self, filename):
        directory, _ = os.path.split(filename)
        if directory:
            os.makedirs(directory, exist_ok=True)
        data = {'vocab': self.vocab.state_dict(), 'emb': self.emb}
        try:
            torch.save(data, filename, _use_new_zipfile_serialization=False, pickle_protocol=3)
            logger.info('Saved pretrained vocab and vectors to {}'.format(filename))
        except (KeyboardInterrupt, SystemExit):
            raise
        except BaseException as e:
            logger.warning('Saving pretrained data failed due to the following exception... continuing anyway.\n\t{}'.format(e))

    def write_text(self, filename):
        """
        Write the vocab & values to a text file
        """
        with open(filename, 'w') as fout:
            for i in range(len(self.vocab)):
                row = self.emb[i]
                fout.write(self.vocab[i])
                fout.write('\t')
                fout.write('\t'.join(map(str, row)))
                fout.write('\n')

    def read_pretrain(self):
        if self._vec_filename is not None:
            words, emb, failed = self.read_from_file(self._vec_filename, self._max_vocab)
        elif self._csv_filename is not None:
            words, emb = self.read_from_csv(self._csv_filename)
        else:
            raise RuntimeError('Vector file is not provided.')
        if len(emb) - len(VOCAB_PREFIX) != len(words):
            raise RuntimeError('Loaded number of vectors does not match number of words.')
        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words) + len(VOCAB_PREFIX):
            words = words[:self._max_vocab - len(VOCAB_PREFIX)]
            emb = emb[:self._max_vocab]
        vocab = PretrainedWordVocab(words)
        return vocab, emb

    @staticmethod
    def read_from_csv(filename):
        """
        Read vectors from CSV

        Skips the first row
        """
        logger.info('Reading pretrained vectors from csv file %s ...', filename)
        with open_read_text(filename) as fin:
            csv_reader = csv.reader(fin)
            for line in csv_reader:
                break
            lines = [line for line in csv_reader]
        rows = len(lines)
        cols = len(lines[0]) - 1
        emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)
        for i, line in enumerate(lines):
            emb[i + len(VOCAB_PREFIX)] = [float(x) for x in line[-cols:]]
        words = [line[0].replace(' ', '\xa0') for line in lines]
        return words, emb

    @staticmethod
    def read_from_file(filename, max_vocab=None):
        """
        Open a vector file using the provided function and read from it.
        """
        logger.info('Reading pretrained vectors from %s ...', filename)
        tab_space_pattern = re.compile('[ \\t]+')
        first = True
        cols = None
        lines = []
        failed = 0
        unk_line = None
        with open_read_binary(filename) as f:
            for i, line in enumerate(f):
                try:
                    line = line.decode()
                except UnicodeDecodeError:
                    failed += 1
                    continue
                line = line.rstrip()
                if not line:
                    continue
                pieces = tab_space_pattern.split(line)
                if first:
                    first = False
                    if len(pieces) == 2:
                        cols = int(pieces[1])
                        continue
                if pieces[0] == '<unk>':
                    if unk_line is not None:
                        logger.error('More than one <unk> line in the pretrain!  Keeping the most recent one')
                    else:
                        logger.debug('Found an unk line while reading the pretrain')
                    unk_line = pieces
                elif not max_vocab or max_vocab < 0 or len(lines) < max_vocab:
                    lines.append(pieces)
        if cols is None:
            cols = min(len(x) for x in lines) - 1
        rows = len(lines)
        emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)
        if unk_line is not None:
            emb[UNK_ID] = [float(x) for x in unk_line[-cols:]]
        for i, line in enumerate(lines):
            emb[i + len(VOCAB_PREFIX)] = [float(x) for x in line[-cols:]]
        words = ['\xa0'.join(line[:-cols]) for line in lines]
        if failed > 0:
            logger.info('Failed to read %d lines from embedding', failed)
        return words, emb, failed


class FoundationCache:

    def __init__(self, other=None, local_files_only=False):
        if other is None:
            self.bert = {}
            self.charlms = {}
            self.pretrains = {}
            self.lock = threading.Lock()
        else:
            self.bert = other.bert
            self.charlms = other.charlms
            self.pretrains = other.pretrains
            self.lock = other.lock
        self.local_files_only = local_files_only

    def load_bert(self, transformer_name, local_files_only=None):
        m, t, _ = self.load_bert_with_peft(transformer_name, None, local_files_only=local_files_only)
        return m, t

    def load_bert_with_peft(self, transformer_name, peft_name, local_files_only=None):
        """
        Load a transformer only once

        Uses a lock for thread safety
        """
        if transformer_name is None:
            return None, None, None
        with self.lock:
            if transformer_name not in self.bert:
                if local_files_only is None:
                    local_files_only = self.local_files_only
                model, tokenizer = bert_embedding.load_bert(transformer_name, local_files_only=local_files_only)
                self.bert[transformer_name] = BertRecord(model, tokenizer, {})
            else:
                logger.debug('Reusing bert %s', transformer_name)
            bert_record = self.bert[transformer_name]
            if not peft_name:
                return bert_record.model, bert_record.tokenizer, None
            if peft_name not in bert_record.peft_ids:
                bert_record.peft_ids[peft_name] = 0
            else:
                bert_record.peft_ids[peft_name] = bert_record.peft_ids[peft_name] + 1
            peft_name = '%s_%d' % (peft_name, bert_record.peft_ids[peft_name])
            return bert_record.model, bert_record.tokenizer, peft_name

    def load_charlm(self, filename):
        if not filename:
            return None
        with self.lock:
            if filename not in self.charlms:
                logger.debug('Loading charlm from %s', filename)
                self.charlms[filename] = CharacterLanguageModel.load(filename, finetune=False)
            else:
                logger.debug('Reusing charlm from %s', filename)
            return self.charlms[filename]

    def load_pretrain(self, filename):
        """
        Load a pretrained word embedding only once

        Uses a lock for thread safety
        """
        if filename is None:
            return None
        with self.lock:
            if filename not in self.pretrains:
                logger.debug('Loading pretrain %s', filename)
                self.pretrains[filename] = Pretrain(filename)
            else:
                logger.debug('Reusing pretrain %s', filename)
            return self.pretrains[filename]


class MultiState(namedtuple('MultiState', ['states', 'gold_tree', 'gold_sequence', 'score'])):

    def finished(self, ensemble):
        return self.states[0].finished(ensemble.models[0])

    def get_tree(self, ensemble):
        return self.states[0].get_tree(ensemble.models[0])

    @property
    def empty_constituents(self):
        return self.states[0].empty_constituents

    def num_constituents(self):
        return len(self.states[0].constituents) - 1

    @property
    def num_transitions(self):
        return len(self.states[0].transitions) - 1

    @property
    def num_opens(self):
        return self.states[0].num_opens

    @property
    def sentence_length(self):
        return self.states[0].sentence_length

    def empty_word_queue(self):
        return self.states[0].empty_word_queue()

    def empty_transitions(self):
        return self.states[0].empty_transitions()

    @property
    def constituents(self):
        return self.states[0].constituents

    @property
    def transitions(self):
        return self.states[0].transitions


ParseResult = namedtuple('ParseResult', ['gold', 'predictions', 'state', 'constituents'])


ScoredTree = namedtuple('ScoredTree', ['tree', 'score'])


import sys
_module = sys.modules[__name__]
del sys
conf = _module
load_model_from_checkpoint = _module
python_create_dataset_azure_example = _module
python_create_dataset_gcs_example = _module
python_create_dataset_s3_example = _module
python_create_frame_predictions = _module
python_run_datapool_example = _module
python_run_datapool_example_2 = _module
python_run_object_level = _module
python_run_object_level_pretagging = _module
python_run_pretagging = _module
python_run_sequence_selection = _module
semantic_segmentation_inference = _module
create_dataset = _module
trigger_job = _module
python_run_active_learning = _module
cifar10_benchmark = _module
imagenet100_benchmark = _module
imagenette_benchmark = _module
plot_image_augmentations = _module
tutorial_custom_augmentations = _module
tutorial_moco_memory_bank = _module
tutorial_pretrain_detectron2 = _module
tutorial_simclr_clothing = _module
tutorial_simsiam_esa = _module
tutorial_active_learning = _module
tutorial_active_learning_detectron2 = _module
tutorial_aquarium_custom_metadata = _module
tutorial_cropped_objects_metadata = _module
tutorial_label_studio_export = _module
tutorial_pizza_filter = _module
tutorial_sunflowers = _module
barlowtwins = _module
byol = _module
dcl = _module
dino = _module
mae = _module
moco = _module
msn = _module
nnclr = _module
simclr = _module
simmim = _module
simsiam = _module
smog = _module
swav = _module
swav_queue = _module
vicreg = _module
barlowtwins = _module
byol = _module
dcl = _module
dino = _module
mae = _module
moco = _module
msn = _module
nnclr = _module
simclr = _module
simmim = _module
simsiam = _module
smog = _module
swav = _module
vicreg = _module
barlowtwins = _module
byol = _module
dcl = _module
dino = _module
mae = _module
moco = _module
msn = _module
nnclr = _module
simclr = _module
simmim = _module
simsiam = _module
swav = _module
vicreg = _module
lightly = _module
active_learning = _module
agents = _module
agent = _module
config = _module
selection_config = _module
scorers = _module
classification = _module
detection = _module
keypoint_detection = _module
scorer = _module
semantic_segmentation = _module
utils = _module
bounding_box = _module
keypoint_predictions = _module
object_detection_output = _module
api = _module
api_workflow_client = _module
api_workflow_collaboration = _module
api_workflow_compute_worker = _module
api_workflow_datasets = _module
api_workflow_datasources = _module
api_workflow_download_dataset = _module
api_workflow_predictions = _module
api_workflow_selection = _module
api_workflow_tags = _module
api_workflow_upload_dataset = _module
api_workflow_upload_embeddings = _module
api_workflow_upload_metadata = _module
bitmask = _module
download = _module
patch_rest_client = _module
prediction_singletons = _module
version_checking = _module
cli = _module
_cli_simclr = _module
_helpers = _module
crop_cli = _module
download_cli = _module
embed_cli = _module
lightly_cli = _module
train_cli = _module
upload_cli = _module
version_cli = _module
core = _module
data = _module
_image = _module
_image_loaders = _module
_utils = _module
_video = _module
collate = _module
dataset = _module
lightly_subset = _module
embedding = _module
_base = _module
callbacks = _module
embedding = _module
loss = _module
barlow_twins_loss = _module
dcl_loss = _module
dino_loss = _module
hypersphere_loss = _module
memory_bank = _module
msn_loss = _module
negative_cosine_similarity = _module
ntx_ent_loss = _module
regularizer = _module
co2 = _module
swav_loss = _module
sym_neg_cos_sim_loss = _module
vicreg_loss = _module
models = _module
_momentum = _module
barlowtwins = _module
batchnorm = _module
byol = _module
moco = _module
modules = _module
heads = _module
masked_autoencoder = _module
nn_memory_bank = _module
nnclr = _module
resnet = _module
simclr = _module
simsiam = _module
utils = _module
zoo = _module
swagger_client = _module
collaboration_api = _module
datasets_api = _module
datasources_api = _module
docker_api = _module
embeddings2d_api = _module
embeddings_api = _module
jobs_api = _module
mappings_api = _module
meta_data_configurations_api = _module
predictions_api = _module
quota_api = _module
samples_api = _module
samplings_api = _module
scores_api = _module
tags_api = _module
versioning_api = _module
api_client = _module
configuration = _module
access_role = _module
active_learning_score_create_request = _module
active_learning_score_data = _module
active_learning_score_type = _module
active_learning_scores = _module
api_error_code = _module
api_error_response = _module
async_task_data = _module
category_id = _module
category_name = _module
configuration_data = _module
configuration_entry = _module
configuration_set_request = _module
configuration_value_data_type = _module
create_docker_worker_registry_entry_request = _module
create_entity_response = _module
crop_data = _module
custom_sample_meta_data = _module
dataset_create_request = _module
dataset_data = _module
dataset_data_enriched = _module
dataset_embedding_data = _module
dataset_name = _module
dataset_type = _module
dataset_update_request = _module
datasource_config = _module
datasource_config_azure = _module
datasource_config_base = _module
datasource_config_gcs = _module
datasource_config_lightly = _module
datasource_config_local = _module
datasource_config_obs = _module
datasource_config_s3 = _module
datasource_config_s3_delegated_access = _module
datasource_config_verify_data = _module
datasource_config_verify_data_errors = _module
datasource_processed_until_timestamp_request = _module
datasource_processed_until_timestamp_response = _module
datasource_purpose = _module
datasource_raw_samples_data = _module
datasource_raw_samples_data_row = _module
datasource_raw_samples_metadata_data = _module
datasource_raw_samples_metadata_data_row = _module
datasource_raw_samples_predictions_data = _module
datasource_raw_samples_predictions_data_row = _module
dimensionality_reduction_method = _module
docker_license_information = _module
docker_run_artifact_create_request = _module
docker_run_artifact_created_data = _module
docker_run_artifact_data = _module
docker_run_artifact_type = _module
docker_run_create_request = _module
docker_run_data = _module
docker_run_log_data = _module
docker_run_log_entry_data = _module
docker_run_log_level = _module
docker_run_scheduled_create_request = _module
docker_run_scheduled_data = _module
docker_run_scheduled_priority = _module
docker_run_scheduled_state = _module
docker_run_scheduled_update_request = _module
docker_run_state = _module
docker_run_update_request = _module
docker_task_description = _module
docker_user_stats = _module
docker_worker_config = _module
docker_worker_config_create_request = _module
docker_worker_config_data = _module
docker_worker_labels = _module
docker_worker_name = _module
docker_worker_registry_entry_data = _module
docker_worker_state = _module
docker_worker_type = _module
embedding2d_coordinates = _module
embedding2d_create_request = _module
embedding2d_data = _module
embedding_data = _module
embedding_id_is_processed_body = _module
file_name_format = _module
file_output_format = _module
filename_and_read_url = _module
filename_and_read_urls = _module
general_job_result = _module
image_type = _module
initial_tag_create_request = _module
job_result_type = _module
job_state = _module
job_status_data = _module
job_status_data_result = _module
job_status_meta = _module
job_status_upload_method = _module
jobs_data = _module
label_box_data_row = _module
label_box_data_rows = _module
label_studio_task = _module
label_studio_task_data = _module
label_studio_tasks = _module
mongo_object_id = _module
object_id = _module
path_safe_name = _module
prediction_singleton = _module
prediction_singleton_base = _module
prediction_singleton_classification = _module
prediction_singleton_instance_segmentation = _module
prediction_singleton_keypoint_detection = _module
prediction_singleton_object_detection = _module
prediction_singleton_semantic_segmentation = _module
prediction_task_schema = _module
prediction_task_schema_category = _module
probabilities = _module
read_url = _module
redirected_read_url = _module
s3_region = _module
s3_server_side_encryption_kms_key = _module
sama_task = _module
sama_task_data = _module
sama_tasks = _module
sample_create_request = _module
sample_data = _module
sample_data_modes = _module
sample_meta_data = _module
sample_partial_mode = _module
sample_sort_by = _module
sample_type = _module
sample_update_request = _module
sample_write_urls = _module
sampling_config = _module
sampling_config_stopping_condition = _module
sampling_create_request = _module
sampling_method = _module
score = _module
selection_config_entry = _module
selection_config_entry_input = _module
selection_config_entry_strategy = _module
selection_input_predictions_name = _module
selection_input_type = _module
selection_strategy_threshold_operation = _module
selection_strategy_type = _module
shared_access_config_create_request = _module
shared_access_config_data = _module
shared_access_type = _module
tag_active_learning_scores_data = _module
tag_arithmetics_operation = _module
tag_arithmetics_request = _module
tag_arithmetics_response = _module
tag_bit_mask_data = _module
tag_bit_mask_response = _module
tag_change_data = _module
tag_create_request = _module
tag_creator = _module
tag_data = _module
tag_filenames_data = _module
tag_name = _module
tag_update_request = _module
tag_upsize_request = _module
task_name = _module
task_type = _module
team_basic_data = _module
timestamp = _module
timestamp_seconds = _module
trigger2d_embedding_job_request = _module
update_docker_worker_registry_entry_request = _module
version_number = _module
video_frame_data = _module
write_csv_url_data = _module
rest = _module
transforms = _module
gaussian_blur = _module
jigsaw = _module
rotation = _module
solarize = _module
benchmarking = _module
crop_image_by_bounding_boxes = _module
read_yolo_label_file = _module
debug = _module
dist = _module
embeddings_2d = _module
io = _module
reordering = _module
version_compare = _module
setup = _module
create_custom_metadata_from_input_dir = _module
delete_datasets_test_unmocked_cli = _module
test_api_latency = _module
test_api = _module
test_api_append = _module
test_download_large_files = _module
tests = _module
test_BoundingBox = _module
test_ObjectDetectionOutput = _module
test_ScorerClassification = _module
test_ScorerKeypointDetection = _module
test_ScorerObjectDetection = _module
test_ScorerSemanticSegmentation = _module
test_active_learning_agent = _module
benchmark_video_download = _module
test_BitMask = _module
test_download = _module
test_rest_parser = _module
test_utils = _module
test_version_checking = _module
mocked_api_workflow_client = _module
test_api_workflow = _module
test_api_workflow_client = _module
test_api_workflow_collaboration = _module
test_api_workflow_compute_worker = _module
test_api_workflow_datasets = _module
test_api_workflow_datasources = _module
test_api_workflow_download_dataset = _module
test_api_workflow_predictions = _module
test_api_workflow_selection = _module
test_api_workflow_tags = _module
test_api_workflow_upload_custom_metadata = _module
test_api_workflow_upload_dataset = _module
test_api_workflow_upload_embeddings = _module
test_cli_crop = _module
test_cli_download = _module
test_cli_embed = _module
test_cli_magic = _module
test_cli_train = _module
test_cli_upload = _module
conftest = _module
test_Core = _module
test_LightlyDataset = _module
test_LightlySubset = _module
test_VideoDataset = _module
test_data_collate = _module
test_callbacks = _module
test_embedding = _module
test_from_imports = _module
test_nested_imports = _module
test_seminested_imports = _module
test_CO2Regularizer = _module
test_DCLLoss = _module
test_DINOLoss = _module
test_HyperSphere = _module
test_MSNLoss = _module
test_MemoryBank = _module
test_NTXentLoss = _module
test_NegativeCosineSimilarity = _module
test_SwaVLoss = _module
test_SymNegCosineSimilarityLoss = _module
test_VICRegLoss = _module
test_masked_autoencoder = _module
test_ModelUtils = _module
test_ModelsBYOL = _module
test_ModelsMoCo = _module
test_ModelsNNCLR = _module
test_ModelsSimCLR = _module
test_ModelsSimSiam = _module
test_ProjectionHeads = _module
test_GaussianBlur = _module
test_Jigsaw = _module
test_Rotation = _module
test_Solarize = _module
test_debug = _module
test_dist = _module
test_io = _module
test_version_compare = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from collections import OrderedDict


import torch


import copy


import time


import numpy as np


import torch.nn as nn


import torchvision


from sklearn.cluster import KMeans


from torch.optim.lr_scheduler import LambdaLR


import math


import matplotlib.pyplot as plt


from sklearn.neighbors import NearestNeighbors


from sklearn.preprocessing import normalize


import pandas


import matplotlib.offsetbox as osb


from matplotlib import rcParams as rcp


import torchvision.transforms.functional as functional


from sklearn import random_projection


from torchvision.datasets import ImageFolder


from torch.utils.data import DataLoader


from torch.utils.data import random_split


from torchvision import transforms


from torchvision.models import resnet18


from torch import nn


from typing import Callable


from typing import Union


from typing import Generator


from typing import List


from typing import Dict


from typing import Optional


import warnings


from torch.utils.hipify.hipify_python import bcolors


from typing import Type


from torch import nn as nn


from typing import Tuple


from typing import Any


from torch.utils.data import Dataset


from torchvision import datasets


from torchvision import io


import torchvision.transforms as T


from torch._C import Value


import torchvision.datasets as datasets


import torch.distributed as dist


from functools import partial


from torch import Tensor


import torch.nn.functional as F


import functools


from torch.nn.functional import cosine_similarity


from torchvision.models import vision_transformer


from torchvision.models.vision_transformer import ConvStemConfig


import re


import random


import torchvision.transforms as transforms


from torch import manual_seed


import itertools


from torch.optim import SGD


class SelectStage(torch.nn.Module):
    """Selects features from a given stage."""

    def __init__(self, stage: str='res5'):
        super().__init__()
        self.stage = stage

    def forward(self, x):
        return x[self.stage]


class ProjectionHead(nn.Module):
    """Base class for all projection and prediction heads.

    Args:
        blocks:
            List of tuples, each denoting one block of the projection head MLP.
            Each tuple reads (in_features, out_features, batch_norm_layer,
            non_linearity_layer).

    Examples:
        >>> # the following projection head has two blocks
        >>> # the first block uses batch norm an a ReLU non-linearity
        >>> # the second block is a simple linear layer
        >>> projection_head = ProjectionHead([
        >>>     (256, 256, nn.BatchNorm1d(256), nn.ReLU()),
        >>>     (256, 128, None, None)
        >>> ])

    """

    def __init__(self, blocks: List[Tuple[int, int, Optional[nn.Module], Optional[nn.Module]]]):
        super(ProjectionHead, self).__init__()
        layers = []
        for input_dim, output_dim, batch_norm, non_linearity in blocks:
            use_bias = not bool(batch_norm)
            layers.append(nn.Linear(input_dim, output_dim, bias=use_bias))
            if batch_norm:
                layers.append(batch_norm)
            if non_linearity:
                layers.append(non_linearity)
        self.layers = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor):
        """Computes one forward pass through the projection head.

        Args:
            x:
                Input of shape bsz x num_ftrs.

        """
        return self.layers(x)


class SimSiamPredictionHead(ProjectionHead):
    """Prediction head used for SimSiam.

    "The prediction MLP (h) has BN applied to its hidden fc layers. Its output
    fc does not have BN (...) or ReLU. This MLP has 2 layers." [0]

    [0]: SimSiam, 2020, https://arxiv.org/abs/2011.10566

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=512, output_dim: int=2048):
        super(SimSiamPredictionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


class SimSiamProjectionHead(ProjectionHead):
    """Projection head used for SimSiam.

    "The projection MLP (in f) has BN applied to each fully-connected (fc)
    layer, including its output fc. Its output fc has no ReLU. The hidden fc is
    2048-d. This MLP has 3 layers." [0]

    [0]: SimSiam, 2020, https://arxiv.org/abs/2011.10566

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, output_dim: int=2048):
        super(SimSiamProjectionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, nn.BatchNorm1d(output_dim, affine=False), None)])


class SimSiam(nn.Module):
    """Implementation of SimSiam[0] network

    Recommended loss: :py:class:`lightly.loss.sym_neg_cos_sim_loss.SymNegCosineSimilarityLoss`

    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        proj_hidden_dim:
            Dimension of the hidden layer of the projection head. This should
            be the same size as `num_ftrs`.
        pred_hidden_dim:
            Dimension of the hidden layer of the predicion head. This should
            be `num_ftrs` / 4.
        out_dim:
            Dimension of the output (after the projection head).

    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=2048, proj_hidden_dim: int=2048, pred_hidden_dim: int=512, out_dim: int=2048):
        super(SimSiam, self).__init__()
        self.backbone = backbone
        self.num_ftrs = num_ftrs
        self.proj_hidden_dim = proj_hidden_dim
        self.pred_hidden_dim = pred_hidden_dim
        self.out_dim = out_dim
        self.projection_mlp = SimSiamProjectionHead(num_ftrs, proj_hidden_dim, out_dim)
        self.prediction_mlp = SimSiamPredictionHead(out_dim, pred_hidden_dim, out_dim)
        warnings.warn(Warning('The high-level building block SimSiam will be deprecated in version 1.3.0. ' + 'Use low-level building blocks instead. ' + 'See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information'), PendingDeprecationWarning)

    def forward(self, x0: torch.Tensor, x1: torch.Tensor=None, return_features: bool=False):
        """Forward pass through SimSiam.

        Extracts features with the backbone and applies the projection
        head and prediction head to the output space. If both x0 and x1 are not
        None, both will be passed through the backbone, projection, and
        prediction head. If x1 is None, only x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output prediction and projection of x0 and (if x1 is not None)
            the output prediction and projection of x1. If return_features is
            True, the output for each x is a tuple (out, f) where f are the
            features before the projection head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)
        """
        f0 = self.backbone(x0).flatten(start_dim=1)
        z0 = self.projection_mlp(f0)
        p0 = self.prediction_mlp(z0)
        out0 = z0, p0
        if return_features:
            out0 = out0, f0
        if x1 is None:
            return out0
        f1 = self.backbone(x1).flatten(start_dim=1)
        z1 = self.projection_mlp(f1)
        p1 = self.prediction_mlp(z1)
        out1 = z1, p1
        if return_features:
            out1 = out1, f1
        return out0, out1


class BarlowTwinsProjectionHead(ProjectionHead):
    """Projection head used for Barlow Twins.

    "The projector network has three linear layers, each with 8192 output
    units. The first two layers of the projector are followed by a batch
    normalization layer and rectified linear units." [0]

    [0]: 2021, Barlow Twins, https://arxiv.org/abs/2103.03230

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=8192, output_dim: int=8192):
        super(BarlowTwinsProjectionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


class BarlowTwins(nn.Module):
    """Implementation of BarlowTwins[0] network.

    Recommended loss: :py:class:`lightly.loss.barlow_twins_loss.BarlowTwinsLoss`

    Default params are the ones explained in the original paper [0].
    [0] Zbontar,J. et.al. 2021. Barlow Twins... https://arxiv.org/abs/2103.03230

    Attributes:
        backbone:
            Backbone model to extract features from images.
            ResNet-50 in original paper [0].
        num_ftrs:
            Dimension of the embedding (before the projection head).
        proj_hidden_dim:
            Dimension of the hidden layer of the projection head. This should
            be the same size as `num_ftrs`.
        out_dim:
            Dimension of the output (after the projection head).

    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=2048, proj_hidden_dim: int=8192, out_dim: int=8192):
        super(BarlowTwins, self).__init__()
        self.backbone = backbone
        self.num_ftrs = num_ftrs
        self.proj_hidden_dim = proj_hidden_dim
        self.out_dim = out_dim
        self.projection_mlp = BarlowTwinsProjectionHead(num_ftrs, proj_hidden_dim, out_dim)
        warnings.warn(Warning('The high-level building block BarlowTwins will be deprecated in version 1.3.0. ' + 'Use low-level building blocks instead. ' + 'See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information'), PendingDeprecationWarning)

    def forward(self, x0: torch.Tensor, x1: torch.Tensor=None, return_features: bool=False):
        """Forward pass through BarlowTwins.

        Extracts features with the backbone and applies the projection
        head to the output space. If both x0 and x1 are not None, both will be
        passed through the backbone and projection. If x1 is None, only x0 will
        be forwarded.
        Barlow Twins only implement a projection head unlike SimSiam.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None)
            the output projection of x1. If return_features is
            True, the output for each x is a tuple (out, f) where f are the
            features before the projection head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)
        """
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_mlp(f0)
        if return_features:
            out0 = out0, f0
        if x1 is None:
            return out0
        f1 = self.backbone(x1).flatten(start_dim=1)
        out1 = self.projection_mlp(f1)
        if return_features:
            out1 = out1, f1
        return out0, out1


class BYOLProjectionHead(ProjectionHead):
    """Projection head used for BYOL.

    "This MLP consists in a linear layer with output size 4096 followed by
    batch normalization, rectified linear units (ReLU), and a final
    linear layer with output dimension 256." [0]

    [0]: BYOL, 2020, https://arxiv.org/abs/2006.07733

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=4096, output_dim: int=256):
        super(BYOLProjectionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


def _deactivate_requires_grad(params):
    """Deactivates the requires_grad flag for all parameters.

    """
    for param in params:
        param.requires_grad = False


def _do_momentum_update(prev_params, params, m):
    """Updates the weights of the previous parameters.

    """
    for prev_param, param in zip(prev_params, params):
        prev_param.data = prev_param.data * m + param.data * (1.0 - m)


class _MomentumEncoderMixin:
    """Mixin to provide momentum encoder functionalities.

    Provides the following functionalities:
        - Momentum encoder initialization.
        - Momentum updates.
        - Batch shuffling and unshuffling.

    To make use of the mixin, simply inherit from it:

    >>> class MyMoCo(nn.Module, _MomentumEncoderMixin):
    >>>
    >>>     def __init__(self, backbone):
    >>>         super(MyMoCo, self).__init__()
    >>>
    >>>         self.backbone = backbone
    >>>         self.projection_head = get_projection_head()
    >>>
    >>>         # initialize momentum_backbone and momentum_projection_head
    >>>         self._init_momentum_encoder()
    >>>
    >>>     def forward(self, x: torch.Tensor):
    >>>
    >>>         # do the momentum update
    >>>         self._momentum_update(0.999)
    >>>
    >>>         # use momentum backbone
    >>>         y = self.momentum_backbone(x)
    >>>         y = self.momentum_projection_head(y)

    """
    m: float
    backbone: nn.Module
    projection_head: nn.Module
    momentum_backbone: nn.Module
    momentum_projection_head: nn.Module

    def _init_momentum_encoder(self):
        """Initializes momentum backbone and a momentum projection head.

        """
        assert self.backbone is not None
        assert self.projection_head is not None
        self.momentum_backbone = copy.deepcopy(self.backbone)
        self.momentum_projection_head = copy.deepcopy(self.projection_head)
        _deactivate_requires_grad(self.momentum_backbone.parameters())
        _deactivate_requires_grad(self.momentum_projection_head.parameters())

    @torch.no_grad()
    def _momentum_update(self, m: float=0.999):
        """Performs the momentum update for the backbone and projection head.

        """
        _do_momentum_update(self.momentum_backbone.parameters(), self.backbone.parameters(), m=m)
        _do_momentum_update(self.momentum_projection_head.parameters(), self.projection_head.parameters(), m=m)

    @torch.no_grad()
    def _batch_shuffle(self, batch: torch.Tensor):
        """Returns the shuffled batch and the indices to undo.

        """
        batch_size = batch.shape[0]
        shuffle = torch.randperm(batch_size, device=batch.device)
        return batch[shuffle], shuffle

    @torch.no_grad()
    def _batch_unshuffle(self, batch: torch.Tensor, shuffle: torch.Tensor):
        """Returns the unshuffled batch.

        """
        unshuffle = torch.argsort(shuffle)
        return batch[unshuffle]


class BYOL(nn.Module, _MomentumEncoderMixin):
    """Implementation of the BYOL architecture.

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection mlp).
        hidden_dim:
            Dimension of the hidden layer in the projection and prediction mlp.
        out_dim:
            Dimension of the output (after the projection/prediction mlp).
        m:
            Momentum for the momentum update of encoder.
    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=2048, hidden_dim: int=4096, out_dim: int=256, m: float=0.9):
        super(BYOL, self).__init__()
        self.backbone = backbone
        self.projection_head = BYOLProjectionHead(num_ftrs, hidden_dim, out_dim)
        self.prediction_head = BYOLProjectionHead(out_dim, hidden_dim, out_dim)
        self.momentum_backbone = None
        self.momentum_projection_head = None
        self._init_momentum_encoder()
        self.m = m
        warnings.warn(Warning('The high-level building block BYOL will be deprecated in version 1.3.0. ' + 'Use low-level building blocks instead. ' + 'See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information'), PendingDeprecationWarning)

    def _forward(self, x0: torch.Tensor, x1: torch.Tensor=None):
        """Forward pass through the encoder and the momentum encoder.

        Performs the momentum update, extracts features with the backbone and
        applies the projection (and prediciton) head to the output space. If
        x1 is None, only x0 will be processed otherwise, x0 is processed with
        the encoder and x1 with the momentum encoder.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.

        Returns:
            The output proejction of x0 and (if x1 is not None) the output
            projection of x1.

        Examples:
            >>> # single input, single output
            >>> out = model._forward(x)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model._forward(x0, x1)

        """
        self._momentum_update(self.m)
        f0 = self.backbone(x0).flatten(start_dim=1)
        z0 = self.projection_head(f0)
        out0 = self.prediction_head(z0)
        if x1 is None:
            return out0
        with torch.no_grad():
            f1 = self.momentum_backbone(x1).flatten(start_dim=1)
            out1 = self.momentum_projection_head(f1)
        return out0, out1

    def forward(self, x0: torch.Tensor, x1: torch.Tensor, return_features: bool=False):
        """Symmetrizes the forward pass (see _forward).

        Performs two forward passes, once where x0 is passed through the encoder
        and x1 through the momentum encoder and once the other way around.

        Note that this model currently requires two inputs for the forward pass
        (x0 and x1) which correspond to the two augmentations.
        Furthermore, `the return_features` argument does not work yet.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.

        Returns:
            A tuple out0, out1, where out0 and out1 are tuples containing the
            predictions and projections of x0 and x1: out0 = (z0, p0) and
            out1 = (z1, p1).

        Examples:
            >>> # initialize the model and the loss function
            >>> model = BYOL()
            >>> criterion = SymNegCosineSimilarityLoss()
            >>>
            >>> # forward pass for two batches of transformed images x1 and x2
            >>> out0, out1 = model(x0, x1)
            >>> loss = criterion(out0, out1)

        """
        if x0 is None:
            raise ValueError('x0 must not be None!')
        if x1 is None:
            raise ValueError('x1 must not be None!')
        if not all([(s0 == s1) for s0, s1 in zip(x0.shape, x1.shape)]):
            raise ValueError(f'x0 and x1 must have same shape but got shapes {x0.shape} and {x1.shape}!')
        p0, z1 = self._forward(x0, x1)
        p1, z0 = self._forward(x1, x0)
        return (z0, p0), (z1, p1)


class DCLLoss(nn.Module):
    """Implementation of the Decoupled Contrastive Learning Loss from
    Decoupled Contrastive Learning [0].
    
    This code implements Equation 6 in [0], including the sum over all images `i`
    and views `k`. The loss is reduced to a mean loss over the mini-batch.
    The implementation was inspired by [1].

    - [0] Chun-Hsiao Y. et. al., 2021, Decoupled Contrastive Learning https://arxiv.org/abs/2110.06848
    - [1] https://github.com/raminnakhli/Decoupled-Contrastive-Learning

    Attributes:
        temperature:
            Similarities are scaled by inverse temperature.
        weight_fn:
            Weighting function `w` from the paper. Scales the loss between the
            positive views (views from the same image). No weighting is performed 
            if weight_fn is None. The function must take the two input tensors
            passed to the forward call as input and return a weight tensor. The
            returned weight tensor must have the same length as the input tensors.
        gather_distributed:
            If True then negatives from all gpus are gathered before the 
            loss calculation.
    
    Examples:

        >>> loss_fn = DCLLoss(temperature=0.07)
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # embed images using some model, for example SimCLR
        >>> out0 = model(t0)
        >>> out1 = model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
        >>>
        >>> # you can also add a custom weighting function
        >>> weight_fn = lambda out0, out1: torch.sum((out0 - out1) ** 2, dim=1)
        >>> loss_fn = DCLLoss(weight_fn=weight_fn)
        
    """

    def __init__(self, temperature: float=0.1, weight_fn: Optional[Callable[[Tensor, Tensor], Tensor]]=None, gather_distributed: bool=False):
        super().__init__()
        self.temperature = temperature
        self.weight_fn = weight_fn
        self.gather_distributed = gather_distributed

    def forward(self, out0: Tensor, out1: Tensor) ->Tensor:
        """Forward pass of the DCL loss.
        
        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)
        
        Returns:
            Mean loss over the mini-batch.
        """
        out0 = nn.functional.normalize(out0, dim=1)
        out1 = nn.functional.normalize(out1, dim=1)
        if self.gather_distributed and dist.world_size() > 1:
            out0_all = torch.cat(dist.gather(out0), 0)
            out1_all = torch.cat(dist.gather(out1), 0)
        else:
            out0_all = out0
            out1_all = out1
        loss0 = self._loss(out0, out1, out0_all, out1_all)
        loss1 = self._loss(out1, out0, out1_all, out0_all)
        return 0.5 * (loss0 + loss1)

    def _loss(self, out0, out1, out0_all, out1_all):
        """Calculates DCL loss for out0 with respect to its positives in out1
        and the negatives in out1, out0_all, and out1_all.
        
        This code implements Equation 6 in [0], including the sum over all images `i`
        but with `k` fixed at 0.
        
        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)
            out0_all:
                Output projections of the first set of transformed images from
                all distributed processes/gpus. Should be equal to out0 in an 
                undistributed setting.
                Shape (batch_size * world_size, embedding_size)
            out1_all:
                Output projections of the second set of transformed images from
                all distributed processes/gpus. Should be equal to out1 in an 
                undistributed setting.
                Shape (batch_size * world_size, embedding_size)

        Returns:
            Mean loss over the mini-batch.
        """
        batch_size = out0.shape[0]
        if self.gather_distributed and dist.world_size() > 1:
            diag_mask = dist.eye_rank(batch_size, device=out0.device)
        else:
            diag_mask = torch.eye(batch_size, device=out0.device, dtype=torch.bool)
        sim_00 = torch.einsum('nc,mc->nm', out0, out0_all) / self.temperature
        sim_01 = torch.einsum('nc,mc->nm', out0, out1_all) / self.temperature
        positive_loss = -sim_01[diag_mask]
        if self.weight_fn:
            positive_loss = positive_loss * self.weight_fn(out0, out1)
        sim_00 = sim_00[~diag_mask].view(batch_size, -1)
        sim_01 = sim_01[~diag_mask].view(batch_size, -1)
        negative_loss_00 = torch.logsumexp(sim_00, dim=1)
        negative_loss_01 = torch.logsumexp(sim_01, dim=1)
        return (positive_loss + negative_loss_00 + negative_loss_01).mean()


class SimCLRProjectionHead(ProjectionHead):
    """Projection head used for SimCLR.

    "We use a MLP with one hidden layer to obtain zi = g(h) = W_2 * σ(W_1 * h)
    where σ is a ReLU non-linearity." [0]

    [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, output_dim: int=128):
        super(SimCLRProjectionHead, self).__init__([(input_dim, hidden_dim, None, nn.ReLU()), (hidden_dim, output_dim, None, None)])


class DINOLoss(nn.Module):
    """
    Implementation of the loss described in 'Emerging Properties in 
    Self-Supervised Vision Transformers'. [0]

    This implementation follows the code published by the authors. [1]
    It supports global and local image crops. A linear warmup schedule for the
    teacher temperature is implemented to stabilize training at the beginning.
    Centering is applied to the teacher output to avoid model collapse.

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: https://github.com/facebookresearch/dino

    Attributes:
        output_dim:
            Dimension of the model output.
        warmup_teacher_temp:
            Initial value of the teacher temperature. Should be decreased if the
            training loss does not decrease.
        teacher_temp:
            Final value of the teacher temperature after linear warmup. Values
            above 0.07 result in unstable behavior in most cases. Can be
            slightly increased to improve performance during finetuning.
        warmup_teacher_temp_epochs:
            Number of epochs for the teacher temperature warmup.
        student_temp:
            Temperature of the student.
        center_momentum:
            Momentum term for the center calculation.

    Examples:

        >>> # initialize loss function
        >>> loss_fn = DINOLoss(128)
        >>>
        >>> # generate a view of the images with a random transform
        >>> view = transform(images)
        >>>
        >>> # embed the view with a student and teacher model
        >>> teacher_out = teacher(view)
        >>> student_out = student(view)
        >>> 
        >>> # calculate loss
        >>> loss = loss_fn([teacher_out], [student_out], epoch=0)

    """

    def __init__(self, output_dim: int, warmup_teacher_temp: float=0.04, teacher_temp: float=0.04, warmup_teacher_temp_epochs: int=30, student_temp: float=0.1, center_momentum: float=0.9):
        super().__init__()
        self.warmup_teacher_temp_epochs = warmup_teacher_temp_epochs
        self.teacher_temp = teacher_temp
        self.student_temp = student_temp
        self.center_momentum = center_momentum
        self.register_buffer('center', torch.zeros(1, 1, output_dim))
        self.teacher_temp_schedule = torch.linspace(start=warmup_teacher_temp, end=teacher_temp, steps=warmup_teacher_temp_epochs)

    def forward(self, teacher_out: List[torch.Tensor], student_out: List[torch.Tensor], epoch: int) ->torch.Tensor:
        """Cross-entropy between softmax outputs of the teacher and student 
        networks.

        Args:
            teacher_out:
                List of view feature tensors from the teacher model. Each
                tensor is assumed to contain features from one view of the batch
                and have length batch_size.
            student_out:
                List of view feature tensors from the student model. Each tensor 
                is assumed to contain features from one view of the batch and 
                have length batch_size.
            epoch:
                The current training epoch.

        Returns:
            The average cross-entropy loss.

        """
        if epoch < self.warmup_teacher_temp_epochs:
            teacher_temp = self.teacher_temp_schedule[epoch]
        else:
            teacher_temp = self.teacher_temp
        teacher_out = torch.stack(teacher_out)
        t_out = F.softmax((teacher_out - self.center) / teacher_temp, dim=-1)
        student_out = torch.stack(student_out)
        s_out = F.log_softmax(student_out / self.student_temp, dim=-1)
        loss = -torch.einsum('tbd,sbd->ts', t_out, s_out)
        loss.fill_diagonal_(0)
        n_terms = loss.numel() - loss.diagonal().numel()
        batch_size = teacher_out.shape[1]
        loss = loss.sum() / (n_terms * batch_size)
        self.update_center(teacher_out)
        return loss

    @torch.no_grad()
    def update_center(self, teacher_out: torch.Tensor) ->None:
        """Moving average update of the center used for the teacher output.

        Args:
            teacher_out:
                Stacked output from the teacher model.

        """
        batch_center = torch.mean(teacher_out, dim=(0, 1), keepdim=True)
        if dist.is_initialized():
            dist.all_reduce(batch_center)
            batch_center = batch_center / dist.get_world_size()
        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)


class DINOProjectionHead(ProjectionHead):
    """Projection head used in DINO.

    "The projection head consists of a 3-layer multi-layer perceptron (MLP) 
    with hidden dimension 2048 followed by l2 normalization and a weight
    normalized fully connected layer with K dimensions, which is similar to the
    design from SwAV [1]." [0]

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: SwAV, 2020, https://arxiv.org/abs/2006.09882

    Attributes:
        input_dim:
            The input dimension of the head.
        hidden_dim:
            The hidden dimension.
        bottleneck_dim:
            Dimension of the bottleneck in the last layer of the head.
        output_dim:
            The output dimension of the head.
        batch_norm:
            Whether to use batch norm or not. Should be set to False when using
            a vision transformer backbone.
        freeze_last_layer:
            Number of epochs during which we keep the output layer fixed. 
            Typically doing so during the first epoch helps training. Try 
            increasing this value if the loss does not decrease.
        norm_last_layer:
            Whether or not to weight normalize the last layer of the DINO head.
            Not normalizing leads to better performance but can make the 
            training unstable.
    
    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, bottleneck_dim: int=256, output_dim: int=65536, batch_norm: bool=False, freeze_last_layer: int=-1, norm_last_layer: bool=True):
        bn = nn.BatchNorm1d(hidden_dim) if batch_norm else None
        super().__init__([(input_dim, hidden_dim, bn, nn.GELU()), (hidden_dim, hidden_dim, bn, nn.GELU()), (hidden_dim, bottleneck_dim, None, None)])
        self.apply(self._init_weights)
        self.freeze_last_layer = freeze_last_layer
        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, output_dim, bias=False))
        self.last_layer.weight_g.data.fill_(1)
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def cancel_last_layer_gradients(self, current_epoch: int):
        """Cancel last layer gradients to stabilize the training.
        
        """
        if current_epoch >= self.freeze_last_layer:
            return
        for param in self.last_layer.parameters():
            param.grad = None

    def _init_weights(self, module):
        """Initializes layers with a truncated normal distribution.
        
        """
        if isinstance(module, nn.Linear):
            utils._no_grad_trunc_normal(module.weight, mean=0, std=0.2, a=-2, b=2)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Computes one forward pass through the head.
        
        """
        x = self.layers(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x


def deactivate_requires_grad(model: nn.Module):
    """Deactivates the requires_grad flag for all parameters of a model.
    
    This has the same effect as permanently executing the model within a `torch.no_grad()`
    context. Use this method to disable gradient computation and therefore
    training for a model.

    Examples:
        >>> backbone = resnet18()
        >>> deactivate_requires_grad(backbone)
    """
    for param in model.parameters():
        param.requires_grad = False


def update_momentum(model: nn.Module, model_ema: nn.Module, m: float):
    """Updates parameters of `model_ema` with Exponential Moving Average of `model`

    Momentum encoders are a crucial component fo models such as MoCo or BYOL. 

    Examples:
        >>> backbone = resnet18()
        >>> projection_head = MoCoProjectionHead()
        >>> backbone_momentum = copy.deepcopy(moco)
        >>> projection_head_momentum = copy.deepcopy(projection_head)
        >>>
        >>> # update momentum
        >>> update_momentum(moco, moco_momentum, m=0.999)
        >>> update_momentum(projection_head, projection_head_momentum, m=0.999)
    """
    for model_ema, model in zip(model_ema.parameters(), model.parameters()):
        model_ema.data = model_ema.data * m + model.data * (1.0 - m)


class MoCoProjectionHead(ProjectionHead):
    """Projection head used for MoCo.

    "(...) we replace the fc head in MoCo with a 2-layer MLP head (hidden layer
    2048-d, with ReLU)" [0]

    [0]: MoCo, 2020, https://arxiv.org/abs/1911.05722

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, output_dim: int=128):
        super(MoCoProjectionHead, self).__init__([(input_dim, hidden_dim, None, nn.ReLU()), (hidden_dim, output_dim, None, None)])


class MoCo(nn.Module, _MomentumEncoderMixin):
    """Implementation of the MoCo (Momentum Contrast)[0] architecture.

    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss` with
    a memory bank.

    [0] MoCo, 2020, https://arxiv.org/abs/1911.05722

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        out_dim:
            Dimension of the output (after the projection head).
        m:
            Momentum for momentum update of the key-encoder.

    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=32, out_dim: int=128, m: float=0.999, batch_shuffle: bool=False):
        super(MoCo, self).__init__()
        self.backbone = backbone
        self.projection_head = MoCoProjectionHead(num_ftrs, num_ftrs, out_dim)
        self.momentum_features = None
        self.momentum_projection_head = None
        self.m = m
        self.batch_shuffle = batch_shuffle
        self._init_momentum_encoder()
        warnings.warn(Warning('The high-level building block MoCo will be deprecated in version 1.3.0. ' + 'Use low-level building blocks instead. ' + 'See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information'), PendingDeprecationWarning)

    def forward(self, x0: torch.Tensor, x1: torch.Tensor=None, return_features: bool=False):
        """Embeds and projects the input image.

        Performs the momentum update, extracts features with the backbone and
        applies the projection head to the output space. If both x0 and x1 are
        not None, both will be passed through the backbone and projection head.
        If x1 is None, only x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None) the output
            projection of x1. If return_features is True, the output for each x
            is a tuple (out, f) where f are the features before the projection
            head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)

        """
        self._momentum_update(self.m)
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_head(f0)
        if return_features:
            out0 = out0, f0
        if x1 is None:
            return out0
        with torch.no_grad():
            if self.batch_shuffle:
                x1, shuffle = self._batch_shuffle(x1)
            f1 = self.momentum_backbone(x1).flatten(start_dim=1)
            out1 = self.momentum_projection_head(f1).detach()
            if self.batch_shuffle:
                f1 = self._batch_unshuffle(f1, shuffle)
                out1 = self._batch_unshuffle(out1, shuffle)
            if return_features:
                out1 = out1, f1
        return out0, out1


def prototype_probabilities(queries: torch.Tensor, prototypes: torch.Tensor, temperature: float) ->torch.Tensor:
    """Returns probability for each query to belong to each prototype.

    Args:
        queries:
            Tensor with shape (batch_size, dim)
        prototypes:
            Tensor with shape (num_prototypes, dim)
        temperature:
            Inverse scaling factor for the similarity.

    Returns:
        Probability tensor with shape (batch_size, num_prototypes) which sums to 1 along
        the num_prototypes dimension.

    """
    return F.softmax(torch.matmul(queries, prototypes.T) / temperature, dim=1)


def sharpen(probabilities: torch.Tensor, temperature: float) ->torch.Tensor:
    """Sharpens the probabilities with the given temperature.

    Args:
        probabilities:
            Tensor with shape (batch_size, dim)
        temperature:
            Temperature in (0, 1]. Lower temperature results in stronger sharpening (
            output probabilities are less uniform).
    Returns:
        Probabilities tensor with shape (batch_size, dim).

    """
    probabilities = probabilities ** (1.0 / temperature)
    probabilities /= torch.sum(probabilities, dim=1, keepdim=True)
    return probabilities


@torch.no_grad()
def sinkhorn(out: torch.Tensor, iterations: int=3, epsilon: float=0.05, gather_distributed: bool=False) ->torch.Tensor:
    """Distributed sinkhorn algorithm.

    As outlined in [0] and implemented in [1].
    
    [0]: SwaV, 2020, https://arxiv.org/abs/2006.09882
    [1]: https://github.com/facebookresearch/swav/ 

    Args:
        out:
            Similarity of the features and the SwaV prototypes.
        iterations:
            Number of sinkhorn iterations.
        epsilon:
            Temperature parameter.
        gather_distributed:
            If True then features from all gpus are gathered to calculate the
            soft codes Q. 

    Returns:
        Soft codes Q assigning each feature to a prototype.
    
    """
    world_size = 1
    if gather_distributed and dist.is_initialized():
        world_size = dist.get_world_size()
    Q = torch.exp(out / epsilon).t()
    sum_Q = torch.sum(Q)
    if world_size > 1:
        dist.all_reduce(sum_Q)
    Q /= sum_Q
    B = Q.shape[1] * world_size
    for _ in range(iterations):
        sum_of_rows = torch.sum(Q, dim=1, keepdim=True)
        if world_size > 1:
            dist.all_reduce(sum_of_rows)
        Q /= sum_of_rows
        Q /= torch.sum(Q, dim=0, keepdim=True)
        Q /= B
    Q *= B
    return Q.t()


class MSNLoss(nn.Module):
    """Implementation of the loss function from MSN [0].

    Code inspired by [1].

    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141
    - [1]: https://github.com/facebookresearch/msn

    Attributes:
        temperature:
            Similarities between anchors and targets are scaled by the inverse of
            the temperature. Must be in (0, 1].
        sinkhorn_iterations:
            Number of sinkhorn normalization iterations on the targets.
        me_max_weight:
            Weight factor lambda by which the mean entropy maximization regularization
            loss is scaled. Set to 0 to disable the reguliarization.

     Examples:

        >>> # initialize loss function
        >>> loss_fn = MSNLoss()
        >>>
        >>> # generate anchors and targets of images
        >>> anchors = transforms(images)
        >>> targets = transforms(images)
        >>>
        >>> # feed through MSN model
        >>> anchors_out = model(anchors)
        >>> targets_out = model.target(targets)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(anchors_out, targets_out, prototypes=model.prototypes)

    """

    def __init__(self, temperature: float=0.1, sinkhorn_iterations: int=3, me_max_weight: float=1.0, gather_distributed: bool=False):
        super().__init__()
        self.temperature = temperature
        self.sinkhorn_iterations = sinkhorn_iterations
        self.me_max_weight = me_max_weight
        self.gather_distributed = gather_distributed

    def forward(self, anchors: torch.Tensor, targets: torch.Tensor, prototypes: torch.Tensor, target_sharpen_temperature: float=0.25) ->torch.Tensor:
        """Computes the MSN loss for a set of anchors, targets and prototypes.

        Args:
            anchors:
                Tensor with shape (batch_size * anchor_views, dim).
            targets:
                Tensor with shape (batch_size, dim).
            prototypes:
                Tensor with shape (num_prototypes, dim).
            target_sharpen_temperature:
                Temperature used to sharpen the target probabilities.

        Returns:
            Mean loss over all anchors.

        """
        num_views = anchors.shape[0] // targets.shape[0]
        anchors = F.normalize(anchors, dim=1)
        targets = F.normalize(targets, dim=1)
        prototypes = F.normalize(prototypes, dim=1)
        anchor_probs = prototype_probabilities(anchors, prototypes, temperature=self.temperature)
        with torch.no_grad():
            target_probs = prototype_probabilities(targets, prototypes, temperature=self.temperature)
            target_probs = sharpen(target_probs, temperature=target_sharpen_temperature)
            if self.sinkhorn_iterations > 0:
                target_probs = sinkhorn(probabilities=target_probs, iterations=self.sinkhorn_iterations, gather_distributed=self.gather_distributed)
            target_probs = target_probs.repeat((num_views, 1))
        loss = torch.mean(torch.sum(torch.log(anchor_probs ** -target_probs), dim=1))
        if self.me_max_weight > 0:
            mean_anchor_probs = torch.mean(anchor_probs, dim=0)
            me_max_loss = torch.sum(torch.log(mean_anchor_probs ** -mean_anchor_probs))
            me_max_loss += math.log(float(len(mean_anchor_probs)))
            loss -= self.me_max_weight * me_max_loss
        return loss


class MSNProjectionHead(ProjectionHead):
    """Projection head for MSN [0].

    "We train with a 3-layer projection head with output dimension 256 and 
    batch-normalization at the input and hidden layers.." [0]
    Code inspired by [1].

    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141
    - [1]: https://github.com/facebookresearch/msn

    Attributes:
        input_dim: 
            Input dimension, default value 768 is for a ViT base model.
        hidden_dim: 
            Hidden dimension.
        output_dim: 
            Output dimension.
    """

    def __init__(self, input_dim: int=768, hidden_dim: int=2048, output_dim: int=256):
        super().__init__(blocks=[(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.GELU()), (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.GELU()), (hidden_dim, output_dim, None, None)])


class NNCLRPredictionHead(ProjectionHead):
    """Prediction head used for NNCLR.

    "The architecture of the prediction MLP g is 2 fully-connected layers
    of size [4096,d]. The hidden layer of the prediction MLP is followed by
    batch-norm and ReLU. The last layer has no batch-norm or activation." [0]

    [0]: NNCLR, 2021, https://arxiv.org/abs/2104.14548

    """

    def __init__(self, input_dim: int=256, hidden_dim: int=4096, output_dim: int=256):
        super(NNCLRPredictionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


class NNCLRProjectionHead(ProjectionHead):
    """Projection head used for NNCLR.

    "The architectureof the projection MLP is 3 fully connected layers of sizes
    [2048,2048,d] where d is the embedding size used to apply the loss. We use
    d = 256 in the experiments unless otherwise stated. All fully-connected
    layers are followed by batch-normalization [36]. All the batch-norm layers
    except the last layer are followed by ReLU activation." [0]

    [0]: NNCLR, 2021, https://arxiv.org/abs/2104.14548

    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, output_dim: int=256):
        super(NNCLRProjectionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, nn.BatchNorm1d(output_dim), None)])


class NNCLR(nn.Module):
    """Implementation of the NNCLR[0] architecture

    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss`
    Recommended module: :py:class:`lightly.models.modules.nn_memory_bank.NNmemoryBankModule`

    [0] NNCLR, 2021, https://arxiv.org/abs/2104.14548

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        proj_hidden_dim:
            Dimension of the hidden layer of the projection head.
        pred_hidden_dim:
            Dimension of the hidden layer of the predicion head.
        out_dim:
            Dimension of the output (after the projection head).
        num_mlp_layers:
            Number of linear layers for MLP.

    Examples:
        >>> model = NNCLR(backbone)
        >>> criterion = NTXentLoss(temperature=0.1)
        >>>
        >>> nn_replacer = NNmemoryBankModule(size=2 ** 16)
        >>>
        >>> # forward pass
        >>> (z0, p0), (z1, p1) = model(x0, x1)
        >>> z0 = nn_replacer(z0.detach(), update=False)
        >>> z1 = nn_replacer(z1.detach(), update=True)
        >>>
        >>> loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))

    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=512, proj_hidden_dim: int=2048, pred_hidden_dim: int=4096, out_dim: int=256):
        super(NNCLR, self).__init__()
        self.backbone = backbone
        self.num_ftrs = num_ftrs
        self.proj_hidden_dim = proj_hidden_dim
        self.pred_hidden_dim = pred_hidden_dim
        self.out_dim = out_dim
        self.projection_mlp = NNCLRProjectionHead(num_ftrs, proj_hidden_dim, out_dim)
        self.prediction_mlp = NNCLRPredictionHead(out_dim, pred_hidden_dim, out_dim)
        warnings.warn(Warning('The high-level building block NNCLR will be deprecated in version 1.3.0. ' + 'Use low-level building blocks instead. ' + 'See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information'), PendingDeprecationWarning)

    def forward(self, x0: torch.Tensor, x1: torch.Tensor=None, return_features: bool=False):
        """Embeds and projects the input images.

        Extracts features with the backbone and applies the projection
        head to the output space. If both x0 and x1 are not None, both will be
        passed through the backbone and projection head. If x1 is None, only
        x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None) the output
            projection of x1. If return_features is True, the output for each x
            is a tuple (out, f) where f are the features before the projection
            head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)

        """
        f0 = self.backbone(x0).flatten(start_dim=1)
        z0 = self.projection_mlp(f0)
        p0 = self.prediction_mlp(z0)
        out0 = z0, p0
        if return_features:
            out0 = out0, f0
        if x1 is None:
            return out0
        f1 = self.backbone(x1).flatten(start_dim=1)
        z1 = self.projection_mlp(f1)
        p1 = self.prediction_mlp(z1)
        out1 = z1, p1
        if return_features:
            out1 = out1, f1
        return out0, out1


class SimCLR(nn.Module):
    """Implementation of the SimCLR[0] architecture

    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss`

    [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        out_dim:
            Dimension of the output (after the projection head).

    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=32, out_dim: int=128):
        super(SimCLR, self).__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(num_ftrs, num_ftrs, out_dim)
        warnings.warn(Warning('The high-level building block SimCLR will be deprecated in version 1.3.0. ' + 'Use low-level building blocks instead. ' + 'See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information'), PendingDeprecationWarning)

    def forward(self, x0: torch.Tensor, x1: torch.Tensor=None, return_features: bool=False):
        """Embeds and projects the input images.

        Extracts features with the backbone and applies the projection
        head to the output space. If both x0 and x1 are not None, both will be
        passed through the backbone and projection head. If x1 is None, only
        x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None) the output
            projection of x1. If return_features is True, the output for each x
            is a tuple (out, f) where f are the features before the projection
            head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)

        """
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_head(f0)
        if return_features:
            out0 = out0, f0
        if x1 is None:
            return out0
        f1 = self.backbone(x1).flatten(start_dim=1)
        out1 = self.projection_head(f1)
        if return_features:
            out1 = out1, f1
        return out0, out1


def knn_predict(feature: torch.Tensor, feature_bank: torch.Tensor, feature_labels: torch.Tensor, num_classes: int, knn_k: int=200, knn_t: float=0.1) ->torch.Tensor:
    """Run kNN predictions on features based on a feature bank

    This method is commonly used to monitor performance of self-supervised
    learning methods.

    The default parameters are the ones
    used in https://arxiv.org/pdf/1805.01978v1.pdf.

    Args:
        feature: 
            Tensor of shape [N, D] for which you want predictions
        feature_bank: 
            Tensor of a database of features used for kNN
        feature_labels: 
            Labels for the features in our feature_bank
        num_classes: 
            Number of classes (e.g. `10` for CIFAR-10)
        knn_k: 
            Number of k neighbors used for kNN
        knn_t: 
            Temperature parameter to reweights similarities for kNN

    Returns:
        A tensor containing the kNN predictions

    Examples:
        >>> images, targets, _ = batch
        >>> feature = backbone(images).squeeze()
        >>> # we recommend to normalize the features
        >>> feature = F.normalize(feature, dim=1)
        >>> pred_labels = knn_predict(
        >>>     feature,
        >>>     feature_bank,
        >>>     targets_bank,
        >>>     num_classes=10,
        >>> )
    """
    sim_matrix = torch.mm(feature, feature_bank)
    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)
    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)
    sim_weight = (sim_weight / knn_t).exp()
    one_hot_label = torch.zeros(feature.size(0) * knn_k, num_classes, device=sim_labels.device)
    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)
    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, num_classes) * sim_weight.unsqueeze(dim=-1), dim=1)
    pred_labels = pred_scores.argsort(dim=-1, descending=True)
    return pred_labels


batch_size = 256


lr_factor = batch_size / 256


max_epochs = 20


gather_distributed = False


@torch.no_grad()
def concat_all_gather(x: torch.Tensor) ->torch.Tensor:
    """Returns concatenated instances of x gathered from all gpus.

    This code was taken and adapted from here:
    https://github.com/facebookresearch/moco.

    """
    output = [torch.empty_like(x) for _ in range(dist.get_world_size())]
    dist.all_gather(output, x, async_op=False)
    output = torch.cat(output, dim=0)
    return output


@torch.no_grad()
def batch_shuffle_distributed(batch: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
    """Shuffles batch over multiple gpus.

    This code was taken and adapted from here:
    https://github.com/facebookresearch/moco.

    Args:
        batch:
            The tensor to shuffle.

    Returns:
        A (batch, shuffle) tuple where batch is the shuffled version of the 
        input batch and shuffle is an index to restore the original order.
    
    """
    batch_size_this = batch.shape[0]
    batch_gather = concat_all_gather(batch)
    batch_size_all = batch_gather.shape[0]
    num_gpus = batch_size_all // batch_size_this
    idx_shuffle = torch.randperm(batch_size_all)
    dist.broadcast(idx_shuffle, src=0)
    shuffle = torch.argsort(idx_shuffle)
    gpu_idx = dist.get_rank()
    idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]
    return batch_gather[idx_this], shuffle


@torch.no_grad()
def batch_shuffle(batch: torch.Tensor, distributed: bool=False) ->Tuple[torch.Tensor, torch.Tensor]:
    """Randomly shuffles all tensors in the batch.

    Args:
        batch:
            The batch to shuffle.
        distributed:
            If True then batches are shuffles across multiple gpus.

    Returns:
        A (batch, shuffle) tuple where batch is the shuffled version of the 
        input batch and shuffle is an index to restore the original order.

    Examples:
        >>> # forward pass through the momentum model with batch shuffling
        >>> x1_shuffled, shuffle = batch_shuffle(x1)
        >>> f1 = moco_momentum(x1)
        >>> out0 = projection_head_momentum(f0)
        >>> out1 = batch_unshuffle(out1, shuffle)
    """
    if distributed:
        return batch_shuffle_distributed(batch)
    batch_size = batch.shape[0]
    shuffle = torch.randperm(batch_size, device=batch.device)
    return batch[shuffle], shuffle


@torch.no_grad()
def batch_unshuffle_distributed(batch: torch.Tensor, shuffle: torch.Tensor) ->torch.Tensor:
    """Undo batch shuffle over multiple gpus.

    This code was taken and adapted from here:
    https://github.com/facebookresearch/moco.

    Args:
        batch:
            The tensor to unshuffle.
        shuffle:
            Index to restore the original tensor.

    Returns:
        The unshuffled tensor.

    """
    batch_size_this = batch.shape[0]
    batch_gather = concat_all_gather(batch)
    batch_size_all = batch_gather.shape[0]
    num_gpus = batch_size_all // batch_size_this
    gpu_idx = dist.get_rank()
    idx_this = shuffle.view(num_gpus, -1)[gpu_idx]
    return batch_gather[idx_this]


@torch.no_grad()
def batch_unshuffle(batch: torch.Tensor, shuffle: torch.Tensor, distributed: bool=False) ->torch.Tensor:
    """Unshuffles a batch. 

    Args:
        batch:
            The batch to unshuffle.
        shuffle:
            Index to unshuffle the batch.
        distributed:
            If True then the batch is unshuffled across multiple gpus.
    
    Returns:
        The unshuffled batch.

    Examples:
        >>> # forward pass through the momentum model with batch shuffling
        >>> x1_shuffled, shuffle = batch_shuffle(x1)
        >>> f1 = moco_momentum(x1)
        >>> out0 = projection_head_momentum(f0)
        >>> out1 = batch_unshuffle(out1, shuffle)
    """
    if distributed:
        return batch_unshuffle_distributed(batch, shuffle)
    unshuffle = torch.argsort(shuffle)
    return batch[unshuffle]


memory_bank_size = 300 * batch_size


class MemoryBankModule(torch.nn.Module):
    """Memory bank implementation

    This is a parent class to all loss functions implemented by the lightly
    Python package. This way, any loss can be used with a memory bank if 
    desired.

    Attributes:
        size:
            Number of keys the memory bank can store. If set to 0,
            memory bank is not used.

    Examples:
        >>> class MyLossFunction(MemoryBankModule):
        >>>
        >>>     def __init__(self, memory_bank_size: int = 2 ** 16):
        >>>         super(MyLossFunction, self).__init__(memory_bank_size)
        >>>
        >>>     def forward(self, output: torch.Tensor,
        >>>                 labels: torch.Tensor = None):
        >>>
        >>>         output, negatives = super(
        >>>             MyLossFunction, self).forward(output)
        >>>
        >>>         if negatives is not None:
        >>>             # evaluate loss with negative samples
        >>>         else:
        >>>             # evaluate loss without negative samples

    """

    def __init__(self, size: int=2 ** 16):
        super(MemoryBankModule, self).__init__()
        if size < 0:
            msg = f'Illegal memory bank size {size}, must be non-negative.'
            raise ValueError(msg)
        self.size = size
        self.register_buffer('bank', tensor=torch.empty(0, dtype=torch.float), persistent=False)
        self.register_buffer('bank_ptr', tensor=torch.empty(0, dtype=torch.long), persistent=False)

    @torch.no_grad()
    def _init_memory_bank(self, dim: int):
        """Initialize the memory bank if it's empty

        Args:
            dim:
                The dimension of the which are stored in the bank.

        """
        self.bank = torch.randn(dim, self.size).type_as(self.bank)
        self.bank = torch.nn.functional.normalize(self.bank, dim=0)
        self.bank_ptr = torch.zeros(1).type_as(self.bank_ptr)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, batch: torch.Tensor):
        """Dequeue the oldest batch and add the latest one

        Args:
            batch:
                The latest batch of keys to add to the memory bank.

        """
        batch_size = batch.shape[0]
        ptr = int(self.bank_ptr)
        if ptr + batch_size >= self.size:
            self.bank[:, ptr:] = batch[:self.size - ptr].T.detach()
            self.bank_ptr[0] = 0
        else:
            self.bank[:, ptr:ptr + batch_size] = batch.T.detach()
            self.bank_ptr[0] = ptr + batch_size

    def forward(self, output: torch.Tensor, labels: torch.Tensor=None, update: bool=False):
        """Query memory bank for additional negative samples

        Args:
            output:
                The output of the model.
            labels:
                Should always be None, will be ignored.

        Returns:
            The output if the memory bank is of size 0, otherwise the output
            and the entries from the memory bank.

        """
        if self.size == 0:
            return output, None
        _, dim = output.shape
        if self.bank.nelement() == 0:
            self._init_memory_bank(dim)
        bank = self.bank.clone().detach()
        if update:
            self._dequeue_and_enqueue(output)
        return output, bank


class NTXentLoss(MemoryBankModule):
    """Implementation of the Contrastive Cross Entropy Loss.

    This implementation follows the SimCLR[0] paper. If you enable the memory
    bank by setting the `memory_bank_size` value > 0 the loss behaves like 
    the one described in the MoCo[1] paper.

    - [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709
    - [1] MoCo, 2020, https://arxiv.org/abs/1911.05722
    
    Attributes:
        temperature:
            Scale logits by the inverse of the temperature.
        memory_bank_size:
            Number of negative samples to store in the memory bank. 
            Use 0 for SimCLR. For MoCo we typically use numbers like 4096 or 65536.
        gather_distributed:
            If True then negatives from all gpus are gathered before the 
            loss calculation. This flag has no effect if memory_bank_size > 0.

    Raises:
        ValueError: If abs(temperature) < 1e-8 to prevent divide by zero.

    Examples:

        >>> # initialize loss function without memory bank
        >>> loss_fn = NTXentLoss(memory_bank_size=0)
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimCLR or MoCo model
        >>> batch = torch.cat((t0, t1), dim=0)
        >>> output = model(batch)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(output)

    """

    def __init__(self, temperature: float=0.5, memory_bank_size: int=0, gather_distributed: bool=False):
        super(NTXentLoss, self).__init__(size=memory_bank_size)
        self.temperature = temperature
        self.gather_distributed = gather_distributed
        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean')
        self.eps = 1e-08
        if abs(self.temperature) < self.eps:
            raise ValueError('Illegal temperature: abs({}) < 1e-8'.format(self.temperature))

    def forward(self, out0: torch.Tensor, out1: torch.Tensor):
        """Forward pass through Contrastive Cross-Entropy Loss.

        If used with a memory bank, the samples from the memory bank are used
        as negative examples. Otherwise, within-batch samples are used as 
        negative samples.

        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)

        Returns:
            Contrastive Cross Entropy Loss value.

        """
        device = out0.device
        batch_size, _ = out0.shape
        out0 = nn.functional.normalize(out0, dim=1)
        out1 = nn.functional.normalize(out1, dim=1)
        out1, negatives = super(NTXentLoss, self).forward(out1, update=out0.requires_grad)
        if negatives is not None:
            negatives = negatives
            sim_pos = torch.einsum('nc,nc->n', out0, out1).unsqueeze(-1)
            sim_neg = torch.einsum('nc,ck->nk', out0, negatives)
            logits = torch.cat([sim_pos, sim_neg], dim=1) / self.temperature
            labels = torch.zeros(logits.shape[0], device=device, dtype=torch.long)
        else:
            if self.gather_distributed and dist.world_size() > 1:
                out0_large = torch.cat(dist.gather(out0), 0)
                out1_large = torch.cat(dist.gather(out1), 0)
                diag_mask = dist.eye_rank(batch_size, device=out0.device)
            else:
                out0_large = out0
                out1_large = out1
                diag_mask = torch.eye(batch_size, device=out0.device, dtype=torch.bool)
            logits_00 = torch.einsum('nc,mc->nm', out0, out0_large) / self.temperature
            logits_01 = torch.einsum('nc,mc->nm', out0, out1_large) / self.temperature
            logits_10 = torch.einsum('nc,mc->nm', out1, out0_large) / self.temperature
            logits_11 = torch.einsum('nc,mc->nm', out1, out1_large) / self.temperature
            logits_00 = logits_00[~diag_mask].view(batch_size, -1)
            logits_11 = logits_11[~diag_mask].view(batch_size, -1)
            logits_0100 = torch.cat([logits_01, logits_00], dim=1)
            logits_1011 = torch.cat([logits_10, logits_11], dim=1)
            logits = torch.cat([logits_0100, logits_1011], dim=0)
            labels = torch.arange(batch_size, device=device, dtype=torch.long)
            labels = labels + dist.rank() * batch_size
            labels = labels.repeat(2)
        loss = self.cross_entropy(logits, labels)
        return loss


class SwaVLoss(nn.Module):
    """Implementation of the SwaV loss.

    Attributes:
        temperature:
            Temperature parameter used for cross entropy calculations.
        sinkhorn_iterations:
            Number of iterations of the sinkhorn algorithm.
        sinkhorn_epsilon:
            Temperature parameter used in the sinkhorn algorithm.
        sinkhorn_gather_distributed:
            If True then features from all gpus are gathered to calculate the
            soft codes in the sinkhorn algorithm. 
    
    """

    def __init__(self, temperature: float=0.1, sinkhorn_iterations: int=3, sinkhorn_epsilon: float=0.05, sinkhorn_gather_distributed: bool=False):
        super(SwaVLoss, self).__init__()
        self.temperature = temperature
        self.sinkhorn_iterations = sinkhorn_iterations
        self.sinkhorn_epsilon = sinkhorn_epsilon
        self.sinkhorn_gather_distributed = sinkhorn_gather_distributed

    def subloss(self, z: torch.Tensor, q: torch.Tensor):
        """Calculates the cross entropy for the SwaV prediction problem.

        Args:
            z:
                Similarity of the features and the SwaV prototypes.
            q:
                Codes obtained from Sinkhorn iterations.

        Returns:
            Cross entropy between predictions z and codes q.

        """
        return -torch.mean(torch.sum(q * F.log_softmax(z / self.temperature, dim=1), dim=1))

    def forward(self, high_resolution_outputs: List[torch.Tensor], low_resolution_outputs: List[torch.Tensor], queue_outputs: List[torch.Tensor]=None):
        """Computes the SwaV loss for a set of high and low resolution outputs.

        Args:
            high_resolution_outputs:
                List of similarities of features and SwaV prototypes for the
                high resolution crops.
            low_resolution_outputs:
                List of similarities of features and SwaV prototypes for the
                low resolution crops.
            queue_outputs:
                List of similarities of features and SwaV prototypes for the
                queue of high resolution crops from previous batches.

        Returns:
            Swapping assignments between views loss (SwaV) as described in [0].

        [0]: SwaV, 2020, https://arxiv.org/abs/2006.09882

        """
        n_crops = len(high_resolution_outputs) + len(low_resolution_outputs)
        loss = 0.0
        for i in range(len(high_resolution_outputs)):
            with torch.no_grad():
                outputs = high_resolution_outputs[i].detach()
                if queue_outputs is not None:
                    outputs = torch.cat((outputs, queue_outputs[i].detach()))
                q = sinkhorn(outputs, iterations=self.sinkhorn_iterations, epsilon=self.sinkhorn_epsilon, gather_distributed=self.sinkhorn_gather_distributed)
                if queue_outputs is not None:
                    q = q[:len(high_resolution_outputs[i])]
            subloss = 0.0
            for v in range(len(high_resolution_outputs)):
                if v != i:
                    subloss += self.subloss(high_resolution_outputs[v], q)
            for v in range(len(low_resolution_outputs)):
                subloss += self.subloss(low_resolution_outputs[v], q)
            loss += subloss / (n_crops - 1)
        return loss / len(high_resolution_outputs)


class SwaVProjectionHead(ProjectionHead):
    """Projection head used for SwaV.

    [0]: SwAV, 2020, https://arxiv.org/abs/2006.09882
    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, output_dim: int=128):
        super(SwaVProjectionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


class SwaVPrototypes(nn.Module):
    """Multihead Prototypes used for SwaV.

    Each output feature is assigned to a prototype, SwaV solves the swapped
    prediction problem where the features of one augmentation are used to
    predict the assigned prototypes of the other augmentation.

    Examples:
        >>> # use features with 128 dimensions and 512 prototypes
        >>> prototypes = SwaVPrototypes(128, 512)
        >>>
        >>> # pass batch through backbone and projection head.
        >>> features = model(x)
        >>> features = nn.functional.normalize(features, dim=1, p=2)
        >>>
        >>> # logits has shape bsz x 512
        >>> logits = prototypes(features)

    """

    def __init__(self, input_dim: int=128, n_prototypes: Union[List[int], int]=3000):
        super(SwaVPrototypes, self).__init__()
        self.n_prototypes = n_prototypes if isinstance(n_prototypes, list) else [n_prototypes]
        self._is_single_prototype = True if isinstance(n_prototypes, int) else False
        self.heads = nn.ModuleList([nn.Linear(input_dim, prototypes) for prototypes in self.n_prototypes])

    def forward(self, x) ->Union[torch.Tensor, List[torch.Tensor]]:
        out = []
        for layer in self.heads:
            out.append(layer(x))
        return out[0] if self._is_single_prototype else out

    def normalize(self):
        """Normalizes the prototypes so that they are on the unit sphere."""
        for layer in self.heads:
            utils.normalize_weight(layer.weight)


class GatherLayer(torch.autograd.Function):
    """Gather tensors from all processes, supporting backward propagation.
    
    This code was taken and adapted from here:
    https://github.com/Spijkervet/SimCLR
    
    """

    @staticmethod
    def forward(ctx, input: torch.Tensor) ->Tuple[torch.Tensor, ...]:
        ctx.save_for_backward(input)
        output = [torch.empty_like(input) for _ in range(dist.get_world_size())]
        dist.all_gather(output, input)
        return tuple(output)

    @staticmethod
    def backward(ctx, *grads: torch.Tensor) ->torch.Tensor:
        input, = ctx.saved_tensors
        grad_out = torch.empty_like(input)
        grad_out[:] = grads[dist.get_rank()]
        return grad_out


def gather(input: torch.Tensor) ->Tuple[torch.Tensor]:
    """Gathers this tensor from all processes. Supports backprop."""
    return GatherLayer.apply(input)


class VICRegLoss(torch.nn.Module):
    """Implementation of the VICReg Loss from VICReg[0] paper.
    This implementation follows the code published by the authors. [1]

    [0] Bardes, A. et. al, 2022, VICReg... https://arxiv.org/abs/2105.04906
    [1] https://github.com/facebookresearch/vicreg/
        
    Examples:
    
        >>> # initialize loss function
        >>> loss_fn = VICRegLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(self, lambda_param: float=25.0, mu_param: float=25.0, nu_param: float=1.0, gather_distributed: bool=False, eps=0.0001):
        """Lambda, mu and nu params configuration with default value like in [0]
        Args:
            lambda_param:
                Coefficient for the invariance term of the loss
                Defaults to 25.0 [0].
            mu_param:
                Coefficient for the variance term of the loss
                Defaults to 25.0 [0].
            nu_param:
                Coefficient for the covariance term of the loss
                Defaults to 1.0 [0].
            gather_distributed:
                If True then the cross-correlation matrices from all gpus are
                gathered and summed before the loss calculation.
            eps:
                Numerical epsilon
                Defaults to 0.0001 [1].
        """
        super(VICRegLoss, self).__init__()
        self.lambda_param = lambda_param
        self.mu_param = mu_param
        self.nu_param = nu_param
        self.gather_distributed = gather_distributed
        self.eps = eps

    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) ->torch.Tensor:
        assert z_a.shape[0] > 1 and z_b.shape[0] > 1, f'z_a and z_b must have batch size > 1 but found {z_a.shape[0]} and  {z_b.shape[0]}'
        assert z_a.shape == z_b.shape, f'z_a and z_b must have same shape but found {z_a.shape} and {z_b.shape}.'
        repr_loss = F.mse_loss(z_a, z_b)
        if self.gather_distributed and dist.is_initialized():
            world_size = dist.get_world_size()
            if world_size > 1:
                z_a = torch.cat(gather(z_a), dim=0)
                z_b = torch.cat(gather(z_b), dim=0)
        z_a = z_a - z_a.mean(0)
        z_b = z_b - z_b.mean(0)
        N = z_a.size(0)
        D = z_a.size(1)
        std_x = torch.sqrt(z_a.var(dim=0) + self.eps)
        std_y = torch.sqrt(z_b.var(dim=0) + self.eps)
        std_loss = 0.5 * (torch.mean(F.relu(1 - std_x)) + torch.mean(F.relu(1 - std_y)))
        cov_x = z_a.T @ z_a / (N - 1)
        cov_y = z_b.T @ z_b / (N - 1)
        n, _ = cov_x.shape
        off_diag_cov_x = cov_x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()
        off_diag_cov_y = cov_y.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()
        cov_loss = off_diag_cov_x.pow_(2).sum().div(D) + off_diag_cov_y.pow_(2).sum().div(D)
        loss = self.lambda_param * repr_loss + self.mu_param * std_loss + self.nu_param * cov_loss
        return loss


class _SimCLR(nn.Module):
    """Implementation of SimCLR used by the command-line interface.

        Provides backwards compatability with old checkpoints.
    """

    def __init__(self, backbone: nn.Module, num_ftrs: int=32, out_dim: int=128):
        super(_SimCLR, self).__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(num_ftrs, num_ftrs, out_dim)

    def forward(self, x0: torch.Tensor, x1: torch.Tensor=None):
        """Embeds and projects the input images.

        """
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_head(f0)
        if x1 is None:
            return out0
        f1 = self.backbone(x1).flatten(start_dim=1)
        out1 = self.projection_head(f1)
        return out0, out1


class BaseCollateFunction(nn.Module):
    """Base class for other collate implementations.

    Takes a batch of images as input and transforms each image into two 
    different augmentations with the help of random transforms. The images are
    then concatenated such that the output batch is exactly twice the length 
    of the input batch.

    Attributes:
        transform:
            A set of torchvision transforms which are randomly applied to
            each image.

    """

    def __init__(self, transform: torchvision.transforms.Compose):
        super(BaseCollateFunction, self).__init__()
        self.transform = transform

    def forward(self, batch: List[tuple]):
        """Turns a batch of tuples into a tuple of batches.

            Args:
                batch:
                    A batch of tuples of images, labels, and filenames which
                    is automatically provided if the dataloader is built from 
                    a LightlyDataset.

            Returns:
                A tuple of images, labels, and filenames. The images consist of 
                two batches corresponding to the two transformations of the
                input images.

            Examples:
                >>> # define a random transformation and the collate function
                >>> transform = ... # some random augmentations
                >>> collate_fn = BaseCollateFunction(transform)
                >>>
                >>> # input is a batch of tuples (here, batch_size = 1)
                >>> input = [(img, 0, 'my-image.png')]
                >>> output = collate_fn(input)
                >>>
                >>> # output consists of two random transforms of the images,
                >>> # the labels, and the filenames in the batch
                >>> (img_t0, img_t1), label, filename = output

        """
        batch_size = len(batch)
        transforms = [self.transform(batch[i % batch_size][0]).unsqueeze_(0) for i in range(2 * batch_size)]
        labels = torch.LongTensor([item[1] for item in batch])
        fnames = [item[2] for item in batch]
        transforms = torch.cat(transforms[:batch_size], 0), torch.cat(transforms[batch_size:], 0)
        return transforms, labels, fnames


class GaussianBlur(object):
    """Implementation of random Gaussian blur.

    Utilizes the built-in ImageFilter method from PIL to apply a Gaussian 
    blur to the input image with a certain probability. The blur is further
    randomized as the kernel size is chosen randomly around a mean specified
    by the user.

    Attributes:
        kernel_size:
            Mean kernel size for the Gaussian blur.
        prob:
            Probability with which the blur is applied.
        scale:
            Fraction of the kernel size which is used for upper and lower
            limits of the randomized kernel size.

    """

    def __init__(self, kernel_size: float, prob: float=0.5, scale: float=0.2):
        self.prob = prob
        self.scale = scale
        self.min_size = (1 - scale) * kernel_size
        self.max_size = (1 + scale) * kernel_size
        self.kernel_size = kernel_size

    def __call__(self, sample):
        """Blurs the image with a given probability.

        Args:
            sample:
                PIL image to which blur will be applied.
        
        Returns:
            Blurred image or original image.

        """
        prob = np.random.random_sample()
        if prob < self.prob:
            kernel_size = np.random.normal(self.kernel_size, self.scale * self.kernel_size)
            kernel_size = max(self.min_size, kernel_size)
            kernel_size = min(self.max_size, kernel_size)
            radius = int(kernel_size / 2)
            return sample.filter(ImageFilter.GaussianBlur(radius=radius))
        return sample


class RandomRotate(object):
    """Implementation of random rotation.

    Randomly rotates an input image by a fixed angle. By default, we rotate
    the image by 90 degrees with a probability of 50%.

    This augmentation can be very useful for rotation invariant images such as
    in medical imaging or satellite imaginary.

    Attributes:
        prob:
            Probability with which image is rotated.
        angle:
            Angle by which the image is rotated. We recommend multiples of 90
            to prevent rasterization artifacts. If you pick numbers like
            90, 180, 270 the tensor will be rotated without introducing 
            any artifacts.
    
    """

    def __init__(self, prob: float=0.5, angle: int=90):
        self.prob = prob
        self.angle = angle

    def __call__(self, sample):
        """Rotates the images with a given probability.

        Args:
            sample:
                PIL image which will be rotated.
        
        Returns:
            Rotated image or original image.

        """
        prob = np.random.random_sample()
        if prob < self.prob:
            sample = TF.rotate(sample, self.angle)
        return sample


def _random_rotation_transform(rr_prob: float, rr_degrees: Union[None, float, Tuple[float, float]]) ->Union[RandomRotate, T.RandomApply]:
    if rr_degrees is None:
        return RandomRotate(prob=rr_prob, angle=90)
    else:
        return T.RandomApply([T.RandomRotation(degrees=rr_degrees)], p=rr_prob)


imagenet_normalize = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}


class ImageCollateFunction(BaseCollateFunction):
    """Implementation of a collate function for images.

    This is an implementation of the BaseCollateFunction with a concrete
    set of transforms.

    The set of transforms is inspired by the SimCLR paper as it has shown
    to produce powerful embeddings. 

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Sigma of gaussian blur is kernel_size * input_size.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None, 
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple, 
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in 
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(self, input_size: int=64, cj_prob: float=0.8, cj_bright: float=0.7, cj_contrast: float=0.7, cj_sat: float=0.7, cj_hue: float=0.2, min_scale: float=0.15, random_gray_scale: float=0.2, gaussian_blur: float=0.5, kernel_size: float=0.1, vf_prob: float=0.0, hf_prob: float=0.5, rr_prob: float=0.0, rr_degrees: Union[None, float, Tuple[float, float]]=None, normalize: dict=imagenet_normalize):
        if isinstance(input_size, tuple):
            input_size_ = max(input_size)
        else:
            input_size_ = input_size
        color_jitter = T.ColorJitter(cj_bright, cj_contrast, cj_sat, cj_hue)
        transform = [T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)), _random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees), T.RandomHorizontalFlip(p=hf_prob), T.RandomVerticalFlip(p=vf_prob), T.RandomApply([color_jitter], p=cj_prob), T.RandomGrayscale(p=random_gray_scale), GaussianBlur(kernel_size=kernel_size * input_size_, prob=gaussian_blur), T.ToTensor()]
        if normalize:
            transform += [T.Normalize(mean=normalize['mean'], std=normalize['std'])]
        transform = T.Compose(transform)
        super(ImageCollateFunction, self).__init__(transform)


class MultiViewCollateFunction(nn.Module):
    """Generates multiple views for each image in the batch.

    Attributes:
        transforms:
            List of transformation functions. Each function is used to generate
            one view of the back.
    
    """

    def __init__(self, transforms: List[torchvision.transforms.Compose]):
        super().__init__()
        self.transforms = transforms

    def forward(self, batch: List[tuple]):
        """Turns a batch of tuples into a tuple of batches.

        Args:
            batch:
                The input batch.
        
        Returns:
            A (views, labels, fnames) tuple where views is a list of tensors
            with each tensor containing one view of the batch.

        """
        views = []
        for transform in self.transforms:
            view = torch.stack([transform(img) for img, _, _ in batch])
            views.append(view)
        labels = torch.LongTensor([label for _, label, _ in batch])
        fnames = [fname for _, _, fname in batch]
        return views, labels, fnames


class SimCLRCollateFunction(ImageCollateFunction):
    """Implements the transformations for SimCLR.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Sigma of gaussian blur is kernel_size * input_size.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None, 
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple, 
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in 
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # SimCLR for ImageNet
        >>> collate_fn = SimCLRCollateFunction()
        >>> 
        >>> # SimCLR for CIFAR-10
        >>> collate_fn = SimCLRCollateFunction(
        >>>     input_size=32,
        >>>     gaussian_blur=0.,
        >>> )

    """

    def __init__(self, input_size: int=224, cj_prob: float=0.8, cj_strength: float=0.5, min_scale: float=0.08, random_gray_scale: float=0.2, gaussian_blur: float=0.5, kernel_size: float=0.1, vf_prob: float=0.0, hf_prob: float=0.5, rr_prob: float=0.0, rr_degrees: Union[None, float, Tuple[float, float]]=None, normalize: dict=imagenet_normalize):
        super(SimCLRCollateFunction, self).__init__(input_size=input_size, cj_prob=cj_prob, cj_bright=cj_strength * 0.8, cj_contrast=cj_strength * 0.8, cj_sat=cj_strength * 0.8, cj_hue=cj_strength * 0.2, min_scale=min_scale, random_gray_scale=random_gray_scale, gaussian_blur=gaussian_blur, kernel_size=kernel_size, vf_prob=vf_prob, hf_prob=hf_prob, rr_prob=rr_prob, rr_degrees=rr_degrees, normalize=normalize)


class MoCoCollateFunction(ImageCollateFunction):
    """Implements the transformations for MoCo v1.

    For MoCo v2, simply use the SimCLR settings.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Sigma of gaussian blur is kernel_size * input_size.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None, 
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple, 
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in 
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # MoCo v1 for ImageNet
        >>> collate_fn = MoCoCollateFunction()
        >>> 
        >>> # MoCo v1 for CIFAR-10
        >>> collate_fn = MoCoCollateFunction(
        >>>     input_size=32,
        >>> )

    """

    def __init__(self, input_size: int=224, cj_prob: float=0.8, cj_strength: float=0.4, min_scale: float=0.2, random_gray_scale: float=0.2, gaussian_blur: float=0.0, kernel_size: float=0.1, vf_prob: float=0.0, hf_prob: float=0.5, rr_prob: float=0.0, rr_degrees: Union[None, float, Tuple[float, float]]=None, normalize: dict=imagenet_normalize):
        super(MoCoCollateFunction, self).__init__(input_size=input_size, cj_prob=cj_prob, cj_bright=cj_strength, cj_contrast=cj_strength, cj_sat=cj_strength, cj_hue=cj_strength, min_scale=min_scale, random_gray_scale=random_gray_scale, gaussian_blur=gaussian_blur, kernel_size=kernel_size, vf_prob=vf_prob, hf_prob=hf_prob, rr_prob=rr_prob, rr_degrees=rr_degrees, normalize=normalize)


class MultiCropCollateFunction(MultiViewCollateFunction):
    """Implements the multi-crop transformations for SwaV.

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        transforms:
            Transforms which are applied to all crops.

    """

    def __init__(self, crop_sizes: List[int], crop_counts: List[int], crop_min_scales: List[float], crop_max_scales: List[float], transforms: T.Compose):
        if len(crop_sizes) != len(crop_counts):
            raise ValueError(f'Length of crop_sizes and crop_counts must be equal but are {len(crop_sizes)} and {len(crop_counts)}.')
        if len(crop_sizes) != len(crop_min_scales):
            raise ValueError(f'Length of crop_sizes and crop_min_scales must be equal but are {len(crop_sizes)} and {len(crop_min_scales)}.')
        if len(crop_sizes) != len(crop_min_scales):
            raise ValueError(f'Length of crop_sizes and crop_max_scales must be equal but are {len(crop_sizes)} and {len(crop_min_scales)}.')
        crop_transforms = []
        for i in range(len(crop_sizes)):
            random_resized_crop = T.RandomResizedCrop(crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i]))
            crop_transforms.extend([T.Compose([random_resized_crop, transforms])] * crop_counts[i])
        super().__init__(crop_transforms)


class SwaVCollateFunction(MultiCropCollateFunction):
    """Implements the multi-crop transformations for SwaV.

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None, 
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple, 
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in 
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Sigma of gaussian blur is kernel_size * input_size.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # SwaV for Imagenet
        >>> collate_fn = SwaVCollateFunction()
        >>> 
        >>> # SwaV w/ 2x160 and 4x96 crops 
        >>> collate_fn = SwaVCollateFunction(
        >>>     crop_sizes=[160, 96],
        >>>     crop_counts=[2, 4],
        >>> )
    
    """

    def __init__(self, crop_sizes: List[int]=[224, 96], crop_counts: List[int]=[2, 6], crop_min_scales: List[float]=[0.14, 0.05], crop_max_scales: List[float]=[1.0, 0.14], hf_prob: float=0.5, vf_prob: float=0.0, rr_prob: float=0.0, rr_degrees: Union[None, float, Tuple[float, float]]=None, cj_prob: float=0.8, cj_strength: float=0.8, random_gray_scale: float=0.2, gaussian_blur: float=0.0, kernel_size: float=1.0, normalize: dict=imagenet_normalize):
        color_jitter = T.ColorJitter(cj_strength, cj_strength, cj_strength, cj_strength / 4.0)
        transforms = T.Compose([T.RandomHorizontalFlip(p=hf_prob), T.RandomVerticalFlip(p=vf_prob), _random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees), T.ColorJitter(), T.RandomApply([color_jitter], p=cj_prob), T.RandomGrayscale(p=random_gray_scale), GaussianBlur(kernel_size, prob=gaussian_blur), T.ToTensor(), T.Normalize(mean=normalize['mean'], std=normalize['std'])])
        super(SwaVCollateFunction, self).__init__(crop_sizes=crop_sizes, crop_counts=crop_counts, crop_min_scales=crop_min_scales, crop_max_scales=crop_max_scales, transforms=transforms)


class RandomSolarization(object):
    """Implementation of random image Solarization.

    Utilizes the integrated image operation `solarize` from Pillow. Solarization
    inverts all pixel values above a threshold (default: 128).

    Attributes:
        probability:
            Probability to apply the transformation
        threshold:
            Threshold for solarization.
    """

    def __init__(self, prob: float=0.5, threshold: int=128):
        self.prob = prob
        self.threshold = threshold

    def __call__(self, sample):
        """Solarizes the given input image

        Args:
            sample:
                PIL image to which solarize will be applied.

        Returns:
            Solarized image or original image.

        """
        prob = np.random.random_sample()
        if prob < self.prob:
            return ImageOps.solarize(sample, threshold=self.threshold)
        return sample


class DINOCollateFunction(MultiViewCollateFunction):
    """Implements the global and local view augmentations for DINO [0].

    This class generates two global and a user defined number of local views
    for each image in a batch. The code is adapted from [1].
    
    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: https://github.com/facebookresearch/dino

    Attributes:
        global_crop_size:
            Crop size of the global views.
        global_crop_scale:
            Tuple of min and max scales relative to global_crop_size. 
        local_crop_size:
            Crop size of the local views.
        local_crop_scale:
            Tuple of min and max scales relative to local_crop_size. 
        n_local_views:
            Number of generated local views.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None, 
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple, 
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in 
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Tuple of probabilities to apply gaussian blur on the different
            views. The input is ordered as follows:
            (global_view_0, global_view_1, local_views)
        kernel_size:
            Sigma of gaussian blur is kernel_size * input_size.
        kernel_scale:
            Fraction of the kernel size which is used for upper and lower
            limits of the randomized kernel size.
        solarization:
            Probability to apply solarization on the second global view.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(self, global_crop_size=224, global_crop_scale=(0.4, 1.0), local_crop_size=96, local_crop_scale=(0.05, 0.4), n_local_views=6, hf_prob=0.5, vf_prob=0, rr_prob=0, rr_degrees: Union[None, float, Tuple[float, float]]=None, cj_prob=0.8, cj_bright=0.4, cj_contrast=0.4, cj_sat=0.2, cj_hue=0.1, random_gray_scale=0.2, gaussian_blur=(1.0, 0.1, 0.5), kernel_size=1.4, kernel_scale=0.6, solarization_prob=0.2, normalize=imagenet_normalize):
        flip_and_color_jitter = T.Compose([T.RandomHorizontalFlip(p=hf_prob), T.RandomVerticalFlip(p=vf_prob), _random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees), T.RandomApply([T.ColorJitter(brightness=cj_bright, contrast=cj_contrast, saturation=cj_sat, hue=cj_hue)], p=cj_prob), T.RandomGrayscale(p=random_gray_scale)])
        normalize = T.Compose([T.ToTensor(), T.Normalize(mean=normalize['mean'], std=normalize['std'])])
        global_crop = T.RandomResizedCrop(global_crop_size, scale=global_crop_scale, interpolation=Image.BICUBIC)
        global_transform_0 = T.Compose([global_crop, flip_and_color_jitter, GaussianBlur(kernel_size=kernel_size, prob=gaussian_blur[0], scale=kernel_scale), normalize])
        global_transform_1 = T.Compose([global_crop, flip_and_color_jitter, GaussianBlur(kernel_size=kernel_size, prob=gaussian_blur[1], scale=kernel_scale), RandomSolarization(prob=solarization_prob), normalize])
        local_transform = T.Compose([T.RandomResizedCrop(local_crop_size, scale=local_crop_scale, interpolation=Image.BICUBIC), flip_and_color_jitter, GaussianBlur(kernel_size=kernel_size, prob=gaussian_blur[2], scale=kernel_scale), normalize])
        local_transforms = [local_transform] * n_local_views
        transforms = [global_transform_0, global_transform_1]
        transforms.extend(local_transforms)
        super().__init__(transforms)


class MAECollateFunction(MultiViewCollateFunction):
    """Implements the view augmentation for MAE [0].

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377

    Attributes:
        input_size:
            Size of the input image in pixels.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

     """

    def __init__(self, input_size: Union[int, Tuple[int, int]]=224, min_scale: float=0.2, normalize: dict=imagenet_normalize):
        transforms = [T.RandomResizedCrop(input_size, scale=(min_scale, 1.0), interpolation=3), T.RandomHorizontalFlip(), T.ToTensor()]
        if normalize:
            transforms.append(T.Normalize(mean=normalize['mean'], std=normalize['std']))
        super().__init__([T.Compose(transforms)])

    def forward(self, batch: List[tuple]):
        views, labels, fnames = super().forward(batch)
        return views[0], labels, fnames


class Jigsaw(object):
    """Implementation of Jigsaw image augmentation, inspired from PyContrast library.
    
    Generates n_grid**2 random crops and returns a list.
    
    This augmentation is instrumental to PIRL.
    
    Attributes:
        n_grid:
            Side length of the meshgrid, sqrt of the number of crops.
        img_size:
            Size of image.
        crop_size:
            Size of crops.
        transform:
            Transformation to apply on each crop.
    
    Examples:
        >>> from lightly.transforms import Jigsaw
        >>>
        >>> jigsaw_crop = Jigsaw(n_grid=3, img_size=255, crop_size=64, transform=transforms.ToTensor())
        >>>
        >>> # img is a PIL image
        >>> crops = jigsaw_crops(img)
    """

    def __init__(self, n_grid=3, img_size=255, crop_size=64, transform=transforms.ToTensor()):
        self.n_grid = n_grid
        self.img_size = img_size
        self.crop_size = crop_size
        self.grid_size = int(img_size / self.n_grid)
        self.side = self.grid_size - self.crop_size
        self.transform = transform
        yy, xx = np.meshgrid(np.arange(n_grid), np.arange(n_grid))
        self.yy = np.reshape(yy * self.grid_size, (n_grid * n_grid,))
        self.xx = np.reshape(xx * self.grid_size, (n_grid * n_grid,))

    def __call__(self, img):
        """Performs the Jigsaw augmentation
        Args:
            img:
                PIL image to perform Jigsaw augmentation on.

        Returns:
            Torch tensor with stacked crops.
        """
        r_x = np.random.randint(0, self.side + 1, self.n_grid * self.n_grid)
        r_y = np.random.randint(0, self.side + 1, self.n_grid * self.n_grid)
        img = np.asarray(img, np.uint8)
        crops = []
        for i in range(self.n_grid * self.n_grid):
            crops.append(img[self.xx[i] + r_x[i]:self.xx[i] + r_x[i] + self.crop_size, self.yy[i] + r_y[i]:self.yy[i] + r_y[i] + self.crop_size, :])
        crops = [Image.fromarray(crop) for crop in crops]
        crops = torch.stack([self.transform(crop) for crop in crops])
        crops = crops[np.random.permutation(self.n_grid ** 2)]
        return crops


class PIRLCollateFunction(nn.Module):
    """Implements the transformations for PIRL [0]. The jigsaw augmentation
    is applied during the forward pass.

    - [0] PIRL, 2019: https://arxiv.org/abs/1912.01991

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        hf_prob:
            Probability that horizontal flip is applied.
        n_grid:
            Sqrt of the number of grids in the jigsaw image.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # PIRL for ImageNet
        >>> collate_fn = PIRLCollateFunction()
        >>> 
        >>> # PIRL for CIFAR-10
        >>> collate_fn = PIRLCollateFunction(
        >>>     input_size=32,
        >>> )

    """

    def __init__(self, input_size: int=64, cj_prob: float=0.8, cj_bright: float=0.4, cj_contrast: float=0.4, cj_sat: float=0.4, cj_hue: float=0.4, min_scale: float=0.08, random_gray_scale: float=0.2, hf_prob: float=0.5, n_grid: int=3, normalize: dict=imagenet_normalize):
        super(PIRLCollateFunction, self).__init__()
        if isinstance(input_size, tuple):
            input_size_ = max(input_size)
        else:
            input_size_ = input_size
        color_jitter = T.ColorJitter(cj_bright, cj_contrast, cj_sat, cj_hue)
        transform = [T.RandomHorizontalFlip(p=hf_prob), T.RandomApply([color_jitter], p=cj_prob), T.RandomGrayscale(p=random_gray_scale), T.ToTensor()]
        if normalize:
            transform += [T.Normalize(mean=normalize['mean'], std=normalize['std'])]
        self.no_augment = T.Compose([T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)), T.ToTensor(), T.Normalize(mean=normalize['mean'], std=normalize['std'])])
        self.jigsaw = Jigsaw(n_grid=n_grid, img_size=input_size_, crop_size=int(input_size_ // n_grid), transform=T.Compose(transform))

    def forward(self, batch: List[tuple]):
        """Overriding the BaseCollateFunction class's forward method because
        for PIRL we need only one augmented batch, as opposed to both, which the
        BaseCollateFunction creates."""
        batch_size = len(batch)
        img_transforms = [self.jigsaw(batch[i][0]).unsqueeze_(0) for i in range(batch_size)]
        img = [self.no_augment(batch[i][0]).unsqueeze_(0) for i in range(batch_size)]
        labels = torch.LongTensor([item[1] for item in batch])
        fnames = [item[2] for item in batch]
        transforms = torch.cat(img, 0), torch.cat(img_transforms, 0)
        return transforms, labels, fnames


class MSNCollateFunction(MultiViewCollateFunction):
    """Implements the transformations for MSN [0].

    Generates a set of random and focal views for each input image. The generated output
    is (views, target, filenames) where views is list with the following entries:
    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].

    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141

    Attributes:
        random_size:
            Size of the random image views in pixels.
        focal_size:
            Size of the focal image views in pixels.
        random_views:
            Number of random views to generate.
        focal_views:
            Number of focal views to generate.
        random_crop_scale:
            Minimum and maximum size of the randomized crops for the relative to random_size.
        focal_crop_scale:
            Minimum and maximum size of the randomized crops relative to focal_size.
        cj_prob:
            Probability that color jittering is applied.
        cj_strength:
            Strength of the color jitter.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Sigma of gaussian blur is kernel_size * input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.
    """

    def __init__(self, random_size: int=224, focal_size: int=96, random_views: int=2, focal_views: int=10, random_crop_scale: Tuple[float, float]=(0.3, 1.0), focal_crop_scale: Tuple[float, float]=(0.05, 0.3), cj_prob: float=0.8, cj_strength: float=1.0, gaussian_blur: float=0.5, kernel_size: float=0.1, random_gray_scale: float=0.2, hf_prob: float=0.5, vf_prob: float=0.0, normalize: dict=imagenet_normalize) ->None:
        color_jitter = T.ColorJitter(brightness=0.8 * cj_strength, contrast=0.8 * cj_strength, saturation=0.8 * cj_strength, hue=0.2 * cj_strength)
        transform = T.Compose([T.RandomResizedCrop(size=random_size, scale=random_crop_scale), T.RandomHorizontalFlip(p=hf_prob), T.RandomVerticalFlip(p=vf_prob), T.RandomApply([color_jitter], p=cj_prob), T.RandomGrayscale(p=random_gray_scale), GaussianBlur(kernel_size=kernel_size, prob=gaussian_blur), T.ToTensor(), T.Normalize(mean=normalize['mean'], std=normalize['std'])])
        focal_transform = T.Compose([T.RandomResizedCrop(size=focal_size, scale=focal_crop_scale), T.RandomHorizontalFlip(p=hf_prob), T.RandomVerticalFlip(p=vf_prob), T.RandomApply([color_jitter], p=cj_prob), T.RandomGrayscale(p=random_gray_scale), GaussianBlur(kernel_size=kernel_size, prob=gaussian_blur), T.ToTensor(), T.Normalize(mean=normalize['mean'], std=normalize['std'])])
        transforms = [transform] * random_views
        transforms += [focal_transform] * focal_views
        super().__init__(transforms=transforms)


class SMoGCollateFunction(MultiViewCollateFunction):
    """Implements the transformations for SMoG.

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        gaussian_blur_probs:
            Probability of Gaussian blur for each crop category.
        gaussian_blur_kernel_sizes:
            Kernel size of Gaussian blur for each crop category.
        solarize_probs:
            Probability of solarization for each crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        random_gray_scale:
            Probability of conversion to grayscale.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(self, crop_sizes: List[int]=[224, 96], crop_counts: List[int]=[4, 4], crop_min_scales: List[float]=[0.2, 0.05], crop_max_scales: List[float]=[1.0, 0.2], gaussian_blur_probs: List[float]=[0.5, 0.1], gaussian_blur_kernel_sizes: List[float]=[0.1, 0.1], solarize_probs: List[float]=[0.0, 0.2], hf_prob: float=0.5, cj_prob: float=1.0, cj_strength: float=0.5, random_gray_scale: float=0.2, normalize: dict=imagenet_normalize):
        transforms = []
        for i in range(len(crop_sizes)):
            random_resized_crop = T.RandomResizedCrop(crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i]))
            color_jitter = T.ColorJitter(0.8 * cj_strength, 0.8 * cj_strength, 0.4 * cj_strength, 0.2 * cj_strength)
            transforms.extend([T.Compose([random_resized_crop, T.RandomHorizontalFlip(p=hf_prob), T.RandomApply([color_jitter], p=cj_prob), T.RandomGrayscale(p=random_gray_scale), GaussianBlur(prob=gaussian_blur_probs[i], kernel_size=gaussian_blur_kernel_sizes[i]), RandomSolarization(prob=solarize_probs[i]), T.ToTensor(), T.Normalize(mean=normalize['mean'], std=normalize['std'])])] * crop_counts[i])
        super().__init__(transforms)


class BarlowTwinsLoss(torch.nn.Module):
    """Implementation of the Barlow Twins Loss from Barlow Twins[0] paper.
    This code specifically implements the Figure Algorithm 1 from [0].
    
    [0] Zbontar,J. et.al, 2021, Barlow Twins... https://arxiv.org/abs/2103.03230

        Examples:

        >>> # initialize loss function
        >>> loss_fn = BarlowTwinsLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimSiam model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)

    """

    def __init__(self, lambda_param: float=0.005, gather_distributed: bool=False):
        """Lambda param configuration with default value like in [0]

        Args:
            lambda_param: 
                Parameter for importance of redundancy reduction term. 
                Defaults to 5e-3 [0].
            gather_distributed:
                If True then the cross-correlation matrices from all gpus are 
                gathered and summed before the loss calculation.
        """
        super(BarlowTwinsLoss, self).__init__()
        self.lambda_param = lambda_param
        self.gather_distributed = gather_distributed

    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) ->torch.Tensor:
        device = z_a.device
        z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0)
        z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0)
        N = z_a.size(0)
        D = z_a.size(1)
        c = torch.mm(z_a_norm.T, z_b_norm) / N
        if self.gather_distributed and dist.is_initialized():
            world_size = dist.get_world_size()
            if world_size > 1:
                c = c / world_size
                dist.all_reduce(c)
        c_diff = (c - torch.eye(D, device=device)).pow(2)
        c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param
        loss = c_diff.sum()
        return loss


def negative_mises_fisher_weights(out0: Tensor, out1: Tensor, sigma: float=0.5) ->torch.Tensor:
    """Negative Mises-Fisher weighting function as presented in Decoupled
    Contrastive Learning [0].

    The implementation was inspired by [1].
    
    - [0] Chun-Hsiao Y. et. al., 2021, Decoupled Contrastive Learning https://arxiv.org/abs/2110.06848
    - [1] https://github.com/raminnakhli/Decoupled-Contrastive-Learning

    Args:
        out0:
            Output projections of the first set of transformed images.
            Shape: (batch_size, embedding_size)
        out1:
            Output projections of the second set of transformed images.
            Shape: (batch_size, embedding_size)
        sigma:
            Similarities are scaled by inverse sigma.
    Returns:
        A tensor with shape (batch_size,) where each entry is the weight for one
        of the input images.
    
    """
    similarity = torch.einsum('nm,nm->n', out0.detach(), out1.detach()) / sigma
    return 2 - out0.shape[0] * nn.functional.softmax(similarity, dim=0)


class DCLWLoss(DCLLoss):
    """Implementation of the Weighted Decoupled Contrastive Learning Loss from
    Decoupled Contrastive Learning [0].
    
    This code implements Equation 6 in [0] with a negative Mises-Fisher 
    weighting function. The loss returns the mean over all images `i` and 
    views `k` in the mini-batch. The implementation was inspired by [1].

    - [0] Chun-Hsiao Y. et. al., 2021, Decoupled Contrastive Learning https://arxiv.org/abs/2110.06848
    - [1] https://github.com/raminnakhli/Decoupled-Contrastive-Learning

    Attributes:
        temperature:
            Similarities are scaled by inverse temperature.
        sigma:
            Similar to temperature but applies the inverse scaling in the
            weighting function.
        gather_distributed:
            If True then negatives from all gpus are gathered before the 
            loss calculation.

    Examples:

        >>> loss_fn = DCLWLoss(temperature=0.07)
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # embed images using some model, for example SimCLR
        >>> out0 = model(t0)
        >>> out1 = model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    
    """

    def __init__(self, temperature: float=0.1, sigma: float=0.5, gather_distributed: bool=False):
        super().__init__(temperature=temperature, weight_fn=partial(negative_mises_fisher_weights, sigma=sigma), gather_distributed=gather_distributed)


class HypersphereLoss(torch.nn.Module):
    """
    Implementation of the loss described in 'Understanding Contrastive Representation Learning through
    Alignment and Uniformity on the Hypersphere.' [0]
    
    [0] Tongzhou Wang. et.al, 2020, ... https://arxiv.org/abs/2005.10242

    Note:
        In order for this loss to function as advertized, an l1-normalization to the hypersphere is required.
        This loss function applies this l1-normalization internally in the loss-layer.
        However, it is recommended that the same normalization is also applied in your architecture,
        considering that this l1-loss is also intended to be applied during inference.
        Perhaps there may be merit in leaving it out of the inferrence pathway, but this use has not been tested.

        Moreover it is recommended that the layers preceeding this loss function are either a linear layer without activation,
        a batch-normalization layer, or both. The directly upstream architecture can have a large influence
        on the ability of this loss to achieve its stated aim of promoting uniformity on the hypersphere;
        and if by contrast the last layer going into the embedding is a RELU or similar nonlinearity,
        we may see that we will never get very close to achieving the goal of uniformity on the hypersphere,
        but will confine ourselves to the subspace of positive activations.
        Similar architectural considerations are relevant to most contrastive loss functions,
        but we call it out here explicitly.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = HypersphereLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimSiam model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)

    """

    def __init__(self, t=1.0, lam=1.0, alpha=2.0):
        """Parameters as described in [0]

        Args:
            t : float
                Temperature parameter;
                proportional to the inverse variance of the Gaussians used to measure uniformity
            lam : float:
                Weight balancing the alignment and uniformity loss terms
            alpha : float
                Power applied to the alignment term of the loss. At its default value of 2,
                distances between positive samples are penalized in an l-2 sense.

        """
        super(HypersphereLoss, self).__init__()
        self.t = t
        self.lam = lam
        self.alpha = alpha

    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) ->torch.Tensor:
        """

        Args:
            x (torch.Tensor, [b, d], float)
            y (torch.Tensor, [b, d], float)

        Returns:
            Loss (torch.Tensor, [], float)

        """
        x = F.normalize(z_a)
        y = F.normalize(z_b)

        def lalign(x, y):
            return (x - y).norm(dim=1).pow(self.alpha).mean()

        def lunif(x):
            sq_pdist = torch.pdist(x, p=2).pow(2)
            return sq_pdist.mul(-self.t).exp().mean().log()
        return lalign(x, y) + self.lam * (lunif(x) + lunif(y)) / 2


class NegativeCosineSimilarity(torch.nn.Module):
    """Implementation of the Negative Cosine Simililarity used in the SimSiam[0] paper.

    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566

    Examples:

        >>> # initialize loss function
        >>> loss_fn = NegativeCosineSimilarity()
        >>>
        >>> # generate two representation tensors
        >>> # with batch size 10 and dimension 128
        >>> x0 = torch.randn(10, 128)
        >>> x1 = torch.randn(10, 128)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(x0, x1)
    """

    def __init__(self, dim: int=1, eps: float=1e-08) ->None:
        """Same parameters as in torch.nn.CosineSimilarity

        Args:
            dim (int, optional):
                Dimension where cosine similarity is computed. Default: 1
            eps (float, optional):
                Small value to avoid division by zero. Default: 1e-8
        """
        super().__init__()
        self.dim = dim
        self.eps = eps

    def forward(self, x0: torch.Tensor, x1: torch.Tensor) ->torch.Tensor:
        return -cosine_similarity(x0, x1, self.dim, self.eps).mean()


class CO2Regularizer(MemoryBankModule):
    """Implementation of the CO2 regularizer [0] for self-supervised learning.

    [0] CO2, 2021, https://arxiv.org/abs/2010.02217

    Attributes:
        alpha:
            Weight of the regularization term.
        t_consistency:
            Temperature used during softmax calculations.
        memory_bank_size:
            Number of negative samples to store in the memory bank.
            Use 0 to use the second batch for negative samples.

    Examples:
        >>> # initialize loss function for MoCo
        >>> loss_fn = NTXentLoss(memory_bank_size=4096)
        >>>
        >>> # initialize CO2 regularizer
        >>> co2 = CO2Regularizer(alpha=1.0, memory_bank_size=4096)
        >>>
        >>> # generate two random trasnforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through the MoCo model
        >>> out0, out1 = model(t0, t1)
        >>> 
        >>> # calculate loss and apply regularizer
        >>> loss = loss_fn(out0, out1) + co2(out0, out1)

    """

    def __init__(self, alpha: float=1, t_consistency: float=0.05, memory_bank_size: int=0):
        super(CO2Regularizer, self).__init__(size=memory_bank_size)
        self.log_target = True
        try:
            self.kl_div = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)
        except TypeError:
            self.log_target = False
            self.kl_div = torch.nn.KLDivLoss(reduction='batchmean')
        self.t_consistency = t_consistency
        self.alpha = alpha

    def _get_pseudo_labels(self, out0: torch.Tensor, out1: torch.Tensor, negatives: torch.Tensor=None):
        """Computes the soft pseudo labels across negative samples.

        Args:
            out0:
                Output projections of the first set of transformed images (query).
                Shape: bsz x n_ftrs
            out1:
                Output projections of the second set of transformed images (positive sample).
                Shape: bsz x n_ftrs
            negatives:
                Negative samples to compare against. If this is None, the second
                batch of images will be used as negative samples.
                Shape: memory_bank_size x n_ftrs

        Returns:
            Log probability that a positive samples will classify each negative
            sample as the positive sample.
            Shape: bsz x (bsz - 1) or bsz x memory_bank_size

        """
        batch_size, _ = out0.shape
        if negatives is None:
            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)
            l_neg = torch.einsum('nc,ck->nk', [out0, out1.t()])
            l_neg = l_neg.masked_select(~torch.eye(batch_size, dtype=bool, device=l_neg.device)).view(batch_size, batch_size - 1)
        else:
            negatives = negatives
            l_pos = torch.einsum('nc,nc->n', [out0, out1]).unsqueeze(-1)
            l_neg = torch.einsum('nc,ck->nk', [out0, negatives.clone().detach()])
        logits = torch.cat([l_pos, l_neg], dim=1)
        logits = logits / self.t_consistency
        return torch.nn.functional.log_softmax(logits, dim=-1)

    def forward(self, out0: torch.Tensor, out1: torch.Tensor):
        """Computes the CO2 regularization term for two model outputs.

        Args:
            out0:
                Output projections of the first set of transformed images.
            out1:
                Output projections of the second set of transformed images.

        Returns:
            The regularization term multiplied by the weight factor alpha.

        """
        out0 = torch.nn.functional.normalize(out0, dim=1)
        out1 = torch.nn.functional.normalize(out1, dim=1)
        out1, negatives = super(CO2Regularizer, self).forward(out1, update=True)
        p = self._get_pseudo_labels(out0, out1, negatives)
        q = self._get_pseudo_labels(out1, out0, negatives)
        if self.log_target:
            div = self.kl_div(p, q) + self.kl_div(q, p)
        else:
            div = self.kl_div(p, torch.exp(q)) + self.kl_div(q, torch.exp(p))
        return self.alpha * 0.5 * div


class SymNegCosineSimilarityLoss(torch.nn.Module):
    """Implementation of the Symmetrized Loss used in the SimSiam[0] paper.

    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566
    
    Examples:

        >>> # initialize loss function
        >>> loss_fn = SymNegCosineSimilarityLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimSiam model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)

    """

    def __init__(self) ->None:
        super().__init__()
        warnings.warn(Warning('SymNegCosineSimiliarityLoss will be deprecated in favor of ' + 'NegativeCosineSimilarity in the future.'), PendingDeprecationWarning)

    def _neg_cosine_simililarity(self, x, y):
        v = -torch.nn.functional.cosine_similarity(x, y.detach(), dim=-1).mean()
        return v

    def forward(self, out0: torch.Tensor, out1: torch.Tensor):
        """Forward pass through Symmetric Loss.

            Args:
                out0:
                    Output projections of the first set of transformed images.
                    Expects the tuple to be of the form (z0, p0), where z0 is
                    the output of the backbone and projection mlp, and p0 is the
                    output of the prediction head.
                out1:
                    Output projections of the second set of transformed images.
                    Expects the tuple to be of the form (z1, p1), where z1 is
                    the output of the backbone and projection mlp, and p1 is the
                    output of the prediction head.
 
            Returns:
                Contrastive Cross Entropy Loss value.

            Raises:
                ValueError if shape of output is not multiple of batch_size.
        """
        z0, p0 = out0
        z1, p1 = out1
        loss = self._neg_cosine_simililarity(p0, z1) / 2 + self._neg_cosine_simililarity(p1, z0) / 2
        return loss


class SplitBatchNorm(nn.BatchNorm2d):
    """Simulates multi-gpu behaviour of BatchNorm in one gpu by splitting.

    Implementation was adapted from:
    https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py

    Attributes:
        num_features:
            Number of input features.
        num_splits:
            Number of splits.

    """

    def __init__(self, num_features, num_splits, **kw):
        super().__init__(num_features, **kw)
        self.num_splits = num_splits
        self.register_buffer('running_mean', torch.zeros(num_features * self.num_splits))
        self.register_buffer('running_var', torch.ones(num_features * self.num_splits))

    def train(self, mode=True):
        if self.training is True and mode is False:
            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)
            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)
        return super().train(mode)

    def forward(self, input):
        """Computes the SplitBatchNorm on the input.

        """
        N, C, H, W = input.shape
        if self.training or not self.track_running_stats:
            result = nn.functional.batch_norm(input.view(-1, C * self.num_splits, H, W), self.running_mean, self.running_var, self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits), True, self.momentum, self.eps).view(N, C, H, W)
        else:
            result = nn.functional.batch_norm(input, self.running_mean[:self.num_features], self.running_var[:self.num_features], self.weight, self.bias, False, self.momentum, self.eps)
        return result


class BYOLPredictionHead(ProjectionHead):
    """Prediction head used for BYOL.

    "This MLP consists in a linear layer with output size 4096 followed by
    batch normalization, rectified linear units (ReLU), and a final
    linear layer with output dimension 256." [0]

    [0]: BYOL, 2020, https://arxiv.org/abs/2006.07733

    """

    def __init__(self, input_dim: int=256, hidden_dim: int=4096, output_dim: int=256):
        super(BYOLPredictionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


class SMoGPrototypes(nn.Module):
    """SMoG prototypes module for synchronous momentum grouping.
    
    """

    def __init__(self, group_features: torch.Tensor, beta: float):
        super(SMoGPrototypes, self).__init__()
        self.group_features = nn.Parameter(group_features, requires_grad=False)
        self.beta = beta

    def forward(self, x: torch.Tensor, group_features: torch.Tensor, temperature: float=0.1) ->torch.Tensor:
        """Computes the logits for given model outputs and group features.

        Args:
            x:
                Tensor of shape bsz x dim.
            group_features:
                Momentum updated group features of shape n_groups x dim.
            temperature:
                Temperature parameter for calculating the logits.

        Returns:
            The logits.

        """
        x = torch.nn.functional.normalize(x, dim=1)
        group_features = torch.nn.functional.normalize(group_features, dim=1)
        logits = torch.mm(x, group_features.t())
        return logits / temperature

    def get_updated_group_features(self, x: torch.Tensor) ->None:
        """Performs the synchronous momentum update of the group vectors.

        Args:
            x:
                Tensor of shape bsz x dim.

        Returns:
            The updated group features.

        """
        assignments = self.assign_groups(x)
        group_features = torch.clone(self.group_features.data)
        for assigned_class in torch.unique(assignments):
            mask = assignments == assigned_class
            group_features[assigned_class] = self.beta * self.group_features[assigned_class] + (1 - self.beta) * x[mask].mean(axis=0)
        return group_features

    def set_group_features(self, x: torch.Tensor) ->None:
        """Sets the group features and asserts they don't require gradient. """
        self.group_features.data = x

    @torch.no_grad()
    def assign_groups(self, x: torch.Tensor) ->torch.LongTensor:
        """Assigns each representation in x to a group based on cosine similarity.

        Args:
            Tensor of shape bsz x dim.

        Returns:
            LongTensor of shape bsz indicating group assignments.
        
        """
        return torch.argmax(self.forward(x, self.group_features), dim=-1)


class SMoGProjectionHead(ProjectionHead):
    """Projection head used for SMoG.

    "The two kinds of head are both a two-layer MLP and their hidden layer is
    followed by a BatchNorm [28] and an activation function. (...) The output
    layer of projection head also has BN" [0]

    [0]: SMoG, 2022, https://arxiv.org/pdf/2207.06167.pdf
    
    """

    def __init__(self, input_dim: int=2048, hidden_dim: int=2048, output_dim: int=128):
        super(SMoGProjectionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(2048), nn.ReLU()), (hidden_dim, output_dim, nn.BatchNorm1d(128, affine=False), None)])


class SMoGPredictionHead(ProjectionHead):
    """Prediction head used for SMoG.

    "The two kinds of head are both a two-layer MLP and their hidden layer is
    followed by a BatchNorm [28] and an activation function. (...) The output
    layer of projection head also has BN" [0]

    [0]: SMoG, 2022, https://arxiv.org/pdf/2207.06167.pdf
    
    """

    def __init__(self, input_dim: int=128, hidden_dim: int=2048, output_dim: int=128):
        super(SMoGPredictionHead, self).__init__([(input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()), (hidden_dim, output_dim, None, None)])


class NNMemoryBankModule(MemoryBankModule):
    """Nearest Neighbour Memory Bank implementation

    This class implements a nearest neighbour memory bank as described in the
    NNCLR paper[0]. During the forward pass we return the nearest neighbour
    from the memory bank.

    [0] NNCLR, 2021, https://arxiv.org/abs/2104.14548

    Attributes:
        size:
            Number of keys the memory bank can store. If set to 0,
            memory bank is not used.

    Examples:
        >>> model = NNCLR(backbone)
        >>> criterion = NTXentLoss(temperature=0.1)
        >>>
        >>> nn_replacer = NNmemoryBankModule(size=2 ** 16)
        >>>
        >>> # forward pass
        >>> (z0, p0), (z1, p1) = model(x0, x1)
        >>> z0 = nn_replacer(z0.detach(), update=False)
        >>> z1 = nn_replacer(z1.detach(), update=True)
        >>>
        >>> loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))

    """

    def __init__(self, size: int=2 ** 16):
        super(NNMemoryBankModule, self).__init__(size)

    def forward(self, output: torch.Tensor, update: bool=False):
        """Returns nearest neighbour of output tensor from memory bank

        Args:
            output: The torch tensor for which you want the nearest neighbour
            update: If `True` updated the memory bank by adding output to it

        """
        output, bank = super(NNMemoryBankModule, self).forward(output, update=update)
        bank = bank.t()
        output_normed = torch.nn.functional.normalize(output, dim=1)
        bank_normed = torch.nn.functional.normalize(bank, dim=1)
        similarity_matrix = torch.einsum('nd,md->nm', output_normed, bank_normed)
        index_nearest_neighbours = torch.argmax(similarity_matrix, dim=1)
        nearest_neighbours = torch.index_select(bank, dim=0, index=index_nearest_neighbours)
        return nearest_neighbours


def get_norm_layer(num_features: int, num_splits: int, **kw):
    """Utility to switch between BatchNorm2d and SplitBatchNorm.

    """
    if num_splits > 0:
        return SplitBatchNorm(num_features, num_splits)
    else:
        return nn.BatchNorm2d(num_features)


class BasicBlock(nn.Module):
    """ Implementation of the ResNet Basic Block.

     Attributes:
        in_planes:
            Number of input channels.
        planes:
            Number of channels.
        stride:
            Stride of the first convolutional.
    """
    expansion = 1

    def __init__(self, in_planes: int, planes: int, stride: int=1, num_splits: int=0):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = get_norm_layer(planes, num_splits)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = get_norm_layer(planes, num_splits)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False), get_norm_layer(self.expansion * planes, num_splits))

    def forward(self, x: torch.Tensor):
        """Forward pass through basic ResNet block.

        Args:
            x:
                Tensor of shape bsz x channels x W x H

        Returns:
            Tensor of shape bsz x channels x W x H
        """
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
    """ Implementation of the ResNet Bottleneck Block.

    Attributes:
        in_planes:
            Number of input channels.
        planes:
            Number of channels.
        stride:
            Stride of the first convolutional.

    """
    expansion = 4

    def __init__(self, in_planes: int, planes: int, stride: int=1, num_splits: int=0):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = get_norm_layer(planes, num_splits)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = get_norm_layer(planes, num_splits)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = get_norm_layer(self.expansion * planes, num_splits)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False), get_norm_layer(self.expansion * planes, num_splits))

    def forward(self, x):
        """Forward pass through bottleneck ResNet block.

        Args:
            x:
                Tensor of shape bsz x channels x W x H

        Returns:
            Tensor of shape bsz x channels x W x H
        """
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = F.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    """ResNet implementation.

    [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
    Deep Residual Learning for Image Recognition. arXiv:1512.03385

    Attributes:
        block:
            ResNet building block type.
        layers:
            List of blocks per layer.
        num_classes:
            Number of classes in final softmax layer.
        width:
            Multiplier for ResNet width.
    """

    def __init__(self, block: nn.Module=BasicBlock, layers: List[int]=[2, 2, 2, 2], num_classes: int=10, width: float=1.0, num_splits: int=0):
        super(ResNet, self).__init__()
        self.in_planes = int(64 * width)
        self.base = int(64 * width)
        self.conv1 = nn.Conv2d(3, self.base, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = get_norm_layer(self.base, num_splits)
        self.layer1 = self._make_layer(block, self.base, layers[0], stride=1, num_splits=num_splits)
        self.layer2 = self._make_layer(block, self.base * 2, layers[1], stride=2, num_splits=num_splits)
        self.layer3 = self._make_layer(block, self.base * 4, layers[2], stride=2, num_splits=num_splits)
        self.layer4 = self._make_layer(block, self.base * 8, layers[3], stride=2, num_splits=num_splits)
        self.linear = nn.Linear(self.base * 8 * block.expansion, num_classes)

    def _make_layer(self, block, planes, layers, stride, num_splits):
        strides = [stride] + [1] * (layers - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride, num_splits))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x: torch.Tensor):
        """Forward pass through ResNet.

        Args:
            x:
                Tensor of shape bsz x channels x W x H
        
        Returns:
            Output tensor of shape bsz x num_classes

        """
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class OriginalDINOLoss(nn.Module):
    """Copy paste from the original DINO paper. We use this to verify our
    implementation.

    The only change from the original code is that distributed training is no
    longer assumed.

    Source: https://github.com/facebookresearch/dino/blob/cb711401860da580817918b9167ed73e3eef3dcf/main_dino.py#L363
    
    """

    def __init__(self, out_dim, ncrops, warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs, nepochs, student_temp=0.1, center_momentum=0.9):
        super().__init__()
        self.student_temp = student_temp
        self.center_momentum = center_momentum
        self.ncrops = ncrops
        self.register_buffer('center', torch.zeros(1, out_dim))
        self.teacher_temp_schedule = np.concatenate((np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs), np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp))

    def forward(self, student_output, teacher_output, epoch):
        """
        Cross-entropy between softmax outputs of the teacher and student networks.
        """
        student_out = student_output / self.student_temp
        student_out = student_out.chunk(self.ncrops)
        temp = self.teacher_temp_schedule[epoch]
        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)
        teacher_out = teacher_out.detach().chunk(2)
        total_loss = 0
        n_loss_terms = 0
        for iq, q in enumerate(teacher_out):
            for v in range(len(student_out)):
                if v == iq:
                    continue
                s_out = F.log_softmax(student_out[v], dim=-1)
                loss = torch.sum(-q * s_out, dim=-1)
                total_loss += loss.mean()
                n_loss_terms += 1
        total_loss /= n_loss_terms
        self.update_center(teacher_output)
        return total_loss

    @torch.no_grad()
    def update_center(self, teacher_output):
        """
        Update center used for teacher output.
        """
        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)
        batch_center = batch_center / len(teacher_output)
        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (BarlowTwinsLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (BasicBlock,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Bottleneck,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (CO2Regularizer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (DCLLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (DCLWLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (HypersphereLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (MemoryBankModule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (NNMemoryBankModule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (NegativeCosineSimilarity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (SplitBatchNorm,
     lambda: ([], {'num_features': 4, 'num_splits': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SwaVLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (SymNegCosineSimilarityLoss,
     lambda: ([], {}),
     lambda: ([(torch.rand([4, 4]), torch.rand([4, 4, 4, 4])), (torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]))], {}),
     False),
    (VICRegLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
]

class Test_lightly_ai_lightly(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])



from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from typing import TYPE_CHECKING


from collections import OrderedDict


from typing import Any


from typing import Callable


from typing import Dict


from typing import List


from typing import Union


from functools import partial


from collections import namedtuple


from typing import Mapping


from typing import Sequence


from typing import Iterable


from typing import Optional


from typing import Tuple


from abc import ABC


from abc import abstractmethod


from torch.utils.data import DataLoader


import numpy as np


from torch import int as tint


from torch import long


from torch import short


from torch import Tensor


import collections.abc


from torch.utils.data.dataloader import default_collate


from torch.utils.data import Dataset


from typing import Iterator


from collections import Counter


import logging


import random


from torch.utils.data.sampler import BatchSampler


from torch.utils.data.sampler import Sampler


from itertools import combinations


from itertools import product


from random import sample


from torch.nn import functional as F


import torch.utils.data as data


from torch.utils.model_zoo import tqdm


import itertools


import pandas as pd


import scipy.sparse as sp


from torch.nn.modules import *


import torch.nn as nn


import torch.nn.functional as F


import math


from torch import nn


from torch.nn.modules.loss import *


from torch.nn.modules.loss import _Loss


from itertools import filterfalse as ifilterfalse


from math import e


from torch.autograd import Function


from torch.autograd import Variable


from torch.nn import TripletMarginLoss


import torchvision


from torchvision.models import ResNet


from torch.optim import *


from torch.optim.optimizer import Optimizer


import collections


from collections import defaultdict


from torch.optim import Optimizer


from torch.optim.optimizer import required


from torch.optim.lr_scheduler import *


from torch.optim.lr_scheduler import _LRScheduler


import re


import torch.backends


from functools import lru_cache


import warnings


from torch.utils.data import BatchSampler


from torch.utils.data import DistributedSampler


from torch.utils.data import Sampler


from itertools import tee


import queue


import torch.distributed as dist


import torch.multiprocessing as mp


import functools


from typing import Generator


from numbers import Number


from torch import optim


from torch.optim import lr_scheduler


from torch.optim.lr_scheduler import ReduceLROnPlateau


from torch.utils import data


from torch.utils.data import sampler


from typing import TypeVar


import copy


from torch.nn import Module


from torch.nn.utils import prune


from torch import quantization


from torch.backends import cudnn


from torch import jit


from torch.utils.data.dataset import Dataset


import time


from torchvision.models import mobilenet


from torchvision.models import resnet


from torch.utils.data.distributed import DistributedSampler


from torch.optim.lr_scheduler import StepLR


from collections import deque


from torch.utils.data.dataset import IterableDataset


from sklearn.linear_model import LogisticRegression


from torch.optim import Adam


import enum


from torch.utils.data.dataloader import DataLoader


from torch.utils.data import TensorDataset


from random import randint


from random import shuffle


from itertools import chain


from torch import tensor


class AMSoftmax(nn.Module):
    """Implementation of
    `AMSoftmax: Additive Margin Softmax for Face Verification`_.

    .. _AMSoftmax\\: Additive Margin Softmax for Face Verification:
        https://arxiv.org/pdf/1801.05599.pdf

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.
        s: norm of input feature.
            Default: ``64.0``.
        m: margin.
            Default: ``0.5``.
        eps: operation accuracy.
            Default: ``1e-6``.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = AMSoftmax(5, 10, s=1.31, m=0.5)
        >>> loss_fn = nn.CrossEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int', s: 'float'=64.0, m: 'float'=0.5, eps: 'float'=1e-06):
        super(AMSoftmax, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.eps = eps
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'ArcFace(in_features={self.in_features},out_features={self.out_features},s={self.s},m={self.m},eps={self.eps})'
        return rep

    def forward(self, input: 'torch.Tensor', target: 'torch.LongTensor'=None) ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))
        if target is None:
            return cos_theta
        cos_theta = torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps)
        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)
        logits = torch.where(one_hot.bool(), cos_theta - self.m, cos_theta)
        logits *= self.s
        return logits


class ArcFace(nn.Module):
    """Implementation of
    `ArcFace: Additive Angular Margin Loss for Deep Face Recognition`_.

    .. _ArcFace\\: Additive Angular Margin Loss for Deep Face Recognition:
        https://arxiv.org/abs/1801.07698v1

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.
        s: norm of input feature.
            Default: ``64.0``.
        m: margin.
            Default: ``0.5``.
        eps: operation accuracy.
            Default: ``1e-6``.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = ArcFace(5, 10, s=1.31, m=0.5)
        >>> loss_fn = nn.CrossEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int', s: 'float'=64.0, m: 'float'=0.5, eps: 'float'=1e-06):
        super(ArcFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.threshold = math.pi - m
        self.eps = eps
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'ArcFace(in_features={self.in_features},out_features={self.out_features},s={self.s},m={self.m},eps={self.eps})'
        return rep

    def forward(self, input: 'torch.Tensor', target: 'torch.LongTensor'=None) ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))
        if target is None:
            return cos_theta
        theta = torch.acos(torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps))
        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)
        mask = torch.where(theta > self.threshold, torch.zeros_like(one_hot), one_hot)
        logits = torch.cos(torch.where(mask.bool(), theta + self.m, theta))
        logits *= self.s
        return logits


class SubCenterArcFace(nn.Module):
    """Implementation of
    `Sub-center ArcFace: Boosting Face Recognition
    by Large-scale Noisy Web Faces`_.

    .. _Sub-center ArcFace\\: Boosting Face Recognition         by Large-scale Noisy Web Faces:
        https://ibug.doc.ic.ac.uk/media/uploads/documents/eccv_1445.pdf

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.
        s: norm of input feature,
            Default: ``64.0``.
        m: margin.
            Default: ``0.5``.
        k: number of possible class centroids.
            Default: ``3``.
        eps (float, optional): operation accuracy.
            Default: ``1e-6``.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = SubCenterArcFace(5, 10, s=1.31, m=0.35, k=2)
        >>> loss_fn = nn.CrosEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int', s: 'float'=64.0, m: 'float'=0.5, k: 'int'=3, eps: 'float'=1e-06):
        super(SubCenterArcFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.k = k
        self.eps = eps
        self.weight = nn.Parameter(torch.FloatTensor(k, in_features, out_features))
        nn.init.xavier_uniform_(self.weight)
        self.threshold = math.pi - self.m

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'SubCenterArcFace(in_features={self.in_features},out_features={self.out_features},s={self.s},m={self.m},k={self.k},eps={self.eps})'
        return rep

    def forward(self, input: 'torch.Tensor', target: 'torch.LongTensor'=None) ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes.
        """
        feats = F.normalize(input).unsqueeze(0).expand(self.k, *input.shape)
        wght = F.normalize(self.weight, dim=1)
        cos_theta = torch.bmm(feats, wght)
        cos_theta = torch.max(cos_theta, dim=0)[0]
        theta = torch.acos(torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps))
        if target is None:
            return cos_theta
        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)
        selected = torch.where(theta > self.threshold, torch.zeros_like(one_hot), one_hot)
        logits = torch.cos(torch.where(selected.bool(), theta + self.m, theta))
        logits *= self.s
        return logits


class ArcMarginProduct(nn.Module):
    """Implementation of Arc Margin Product.

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = ArcMarginProduct(5, 10)
        >>> loss_fn = nn.CrosEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int'):
        super(ArcMarginProduct, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'ArcMarginProduct(in_features={self.in_features},out_features={self.out_features})'
        return rep

    def forward(self, input: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cosine = F.linear(F.normalize(input), F.normalize(self.weight))
        return cosine


class Flatten(nn.Module):
    """Flattens the input. Does not affect the batch size.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()

    def forward(self, x):
        """Forward call."""
        return x.view(x.shape[0], -1)


class Lambda(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, lambda_fn):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.lambda_fn = lambda_fn

    def forward(self, x):
        """Forward call."""
        return self.lambda_fn(x)


class Normalize(nn.Module):
    """Performs :math:`L_p` normalization of inputs over specified dimension.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self, **normalize_kwargs):
        """
        Args:
            **normalize_kwargs: see ``torch.nn.functional.normalize`` params
        """
        super().__init__()
        self.normalize_kwargs = normalize_kwargs

    def forward(self, x):
        """Forward call."""
        return F.normalize(x, **self.normalize_kwargs)


class GaussianNoise(nn.Module):
    """
    A gaussian noise module.

    Shape:

    - Input: (batch, \\*)
    - Output: (batch, \\*) (same shape as input)
    """

    def __init__(self, stddev: 'float'=0.1):
        """
        Args:
            stddev: The standard deviation of the normal distribution.
                Default: 0.1.
        """
        super().__init__()
        self.stddev = stddev

    def forward(self, x: 'torch.Tensor'):
        """Forward call."""
        noise = torch.empty_like(x)
        noise.normal_(0, self.stddev)


class ResidualBlock(nn.Module):
    """ResidualBlock"""

    def __init__(self, block):
        """ResidualBlock"""
        super().__init__()
        self.block = block

    def forward(self, x: 'torch.Tensor'):
        """Forward call."""
        return self.block(x) + x


class CosFace(nn.Module):
    """Implementation of
    `CosFace\\: Large Margin Cosine Loss for Deep Face Recognition`_.

    .. _CosFace\\: Large Margin Cosine Loss for Deep Face Recognition:
        https://arxiv.org/abs/1801.09414

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.
        s: norm of input feature.
            Default: ``64.0``.
        m: margin.
            Default: ``0.35``.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = CosFaceLoss(5, 10, s=1.31, m=0.1)
        >>> loss_fn = nn.CrosEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int', s: 'float'=64.0, m: 'float'=0.35):
        super(CosFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = s
        self.m = m
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'CosFace(in_features={self.in_features},out_features={self.out_features},s={self.s},m={self.m})'
        return rep

    def forward(self, input: 'torch.Tensor', target: 'torch.LongTensor'=None) ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cosine = F.linear(F.normalize(input), F.normalize(self.weight))
        phi = cosine - self.m
        if target is None:
            return cosine
        one_hot = torch.zeros_like(cosine)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)
        logits = one_hot * phi + (1.0 - one_hot) * cosine
        logits *= self.s
        return logits


class AdaCos(nn.Module):
    """Implementation of
    `AdaCos\\: Adaptively Scaling Cosine Logits for Effectively Learning Deep Face Representations`_.

    .. _AdaCos\\: Adaptively Scaling Cosine Logits for        Effectively Learning Deep Face Representations:
        https://arxiv.org/abs/1905.00292

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.
        dynamical_s: option to use dynamical scale parameter.
            If ``False`` then will be used initial scale.
            Default: ``True``.
        eps: operation accuracy.
            Default: ``1e-6``.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = AdaCos(5, 10)
        >>> loss_fn = nn.CrosEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int', dynamical_s: 'bool'=True, eps: 'float'=1e-06):
        super(AdaCos, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.s = math.sqrt(2) * math.log(out_features - 1)
        self.eps = eps
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.xavier_uniform_(self.weight)

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'AdaCos(in_features={self.in_features},out_features={self.out_features},s={self.s},eps={self.eps})'
        return rep

    def forward(self, input: 'torch.Tensor', target: 'torch.LongTensor'=None) ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            target: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        cos_theta = F.linear(F.normalize(input), F.normalize(self.weight))
        theta = torch.acos(torch.clamp(cos_theta, -1.0 + self.eps, 1.0 - self.eps))
        if target is None:
            return cos_theta
        one_hot = torch.zeros_like(cos_theta)
        one_hot.scatter_(1, target.view(-1, 1).long(), 1)
        if self.train:
            with torch.no_grad():
                b_avg = torch.where(one_hot < 1, torch.exp(self.s * cos_theta), torch.zeros_like(cos_theta)).sum(1).mean()
                theta_median = theta[one_hot > 0].median()
                theta_median = torch.min(torch.full_like(theta_median, math.pi / 4), theta_median)
                self.s = (torch.log(b_avg) / torch.cos(theta_median)).item()
        logits = self.s * cos_theta
        return logits


class CurricularFace(nn.Module):
    """Implementation of
    `CurricularFace: Adaptive Curriculum Learning        Loss for Deep Face Recognition`_.

    .. _CurricularFace\\: Adaptive Curriculum Learning        Loss for Deep Face Recognition:
        https://arxiv.org/abs/2004.00288

    Official `pytorch implementation`_.

    .. _pytorch implementation:
        https://github.com/HuangYG123/CurricularFace

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.
        s: norm of input feature.
            Default: ``64.0``.
        m: margin.
            Default: ``0.5``.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = CurricularFace(5, 10, s=1.31, m=0.5)
        >>> loss_fn = nn.CrosEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', out_features: 'int', s: 'float'=64.0, m: 'float'=0.5):
        super(CurricularFace, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.m = m
        self.s = s
        self.cos_m = math.cos(m)
        self.sin_m = math.sin(m)
        self.threshold = math.cos(math.pi - m)
        self.mm = math.sin(math.pi - m) * m
        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))
        self.register_buffer('t', torch.zeros(1))
        nn.init.normal_(self.weight, std=0.01)

    def __repr__(self) ->str:
        rep = f'CurricularFace(in_features={self.in_features},out_features={self.out_features},m={self.m},s={self.s})'
        return rep

    def forward(self, input: 'torch.Tensor', label: 'torch.LongTensor'=None) ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.
            label: target classes,
                expected shapes ``B`` where
                ``B`` is batch dimension.
                If `None` then will be returned
                projection on centroids.
                Default is `None`.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes.
        """
        cos_theta = torch.mm(F.normalize(input), F.normalize(self.weight, dim=0))
        cos_theta = cos_theta.clamp(-1, 1)
        if label is None:
            return cos_theta
        target_logit = cos_theta[torch.arange(0, input.size(0)), label].view(-1, 1)
        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))
        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m
        mask = cos_theta > cos_theta_m
        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)
        hard_example = cos_theta[mask]
        with torch.no_grad():
            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t
        cos_theta[mask] = hard_example * (self.t + hard_example)
        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)
        output = cos_theta * self.s
        return output


class FactorizedLinear(nn.Module):
    """Factorized wrapper for ``nn.Linear``

    Args:
        nn_linear: torch ``nn.Linear`` module
        dim_ratio: dimension ration to use after weights SVD
    """

    def __init__(self, nn_linear: 'nn.Linear', dim_ratio: 'Union[int, float]'=1.0):
        super().__init__()
        self.bias = nn.parameter.Parameter(nn_linear.bias.data, requires_grad=True)
        u, vh = self._spectral_init(nn_linear.weight.data, dim_ratio=dim_ratio)
        self.u = nn.parameter.Parameter(u, requires_grad=True)
        self.vh = nn.parameter.Parameter(vh, requires_grad=True)
        self.dim_ratio = dim_ratio
        self.in_features = u.size(0)
        self.out_features = vh.size(1)

    @staticmethod
    def _spectral_init(m, dim_ratio: 'Union[int, float]'=1):
        u, s, vh = torch.linalg.svd(m, full_matrices=False)
        u = u @ torch.diag(torch.sqrt(s))
        vh = torch.diag(torch.sqrt(s)) @ vh
        if dim_ratio < 1:
            dims = int(u.size(1) * dim_ratio)
            u = u[:, :dims]
            vh = vh[:dims, :]
        return u, vh

    def extra_repr(self) ->str:
        """Extra representation log."""
        return f'in_features={self.in_features}, out_features={self.out_features}, bias=True, dim_ratio={self.dim_ratio}'

    def forward(self, x: 'torch.Tensor'):
        """Forward call."""
        return x @ (self.u @ self.vh).transpose(0, 1) + self.bias


class TemporalLastPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None) ->torch.Tensor:
        """Forward call."""
        x_out = x[:, -1:, :]
        return x_out


class TemporalAvgPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None) ->torch.Tensor:
        """Forward call."""
        if mask is None:
            x_out = x.mean(1, keepdim=True)
        else:
            x_masked = torch.sum(x * mask.float(), dim=1, keepdim=True)
            mask_sum = torch.sum(mask.float(), dim=1, keepdim=True)
            x_out = x_masked / mask_sum
        return x_out


class TemporalMaxPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None) ->torch.Tensor:
        """Forward call."""
        if mask is not None:
            x_mask = (~mask.bool()).float() * (-x.max()).float()
            x = torch.sum(x + x_mask, dim=1, keepdim=True)
        x_out = x.max(1, keepdim=True)[0]
        return x_out


def outer_init(layer: 'nn.Module') ->None:
    """
    Initialization for output layers of policy and value networks typically
    used in deep reinforcement learning literature.

    Args:
        layer: torch nn.Module instance
    """
    if isinstance(layer, (nn.Linear, nn.Conv1d, nn.Conv2d)):
        v = 0.003
        nn.init.uniform_(layer.weight.data, -v, v)
        if layer.bias is not None:
            nn.init.uniform_(layer.bias.data, -v, v)


class TemporalAttentionPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""
    name2activation = {'softmax': nn.Softmax(dim=1), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid()}

    def __init__(self, in_features, activation=None, kernel_size=1, **params):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_features = in_features
        activation = activation or 'softmax'
        self.attention_pooling = nn.Sequential(nn.Conv1d(in_channels=in_features, out_channels=1, kernel_size=kernel_size, **params), TemporalAttentionPooling.name2activation[activation])
        self.attention_pooling.apply(outer_init)

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None) ->torch.Tensor:
        """
        Forward call.

        Args:
            x: tensor of size
                (batch_size, history_len, feature_size)
            mask: mask to use

        Returns:
            pooling result
        """
        batch_size, history_len, feature_size = x.shape
        x = x.view(batch_size, history_len, -1)
        x_a = x.transpose(1, 2)
        x_attn = (self.attention_pooling(x_a) * x_a).transpose(1, 2)
        x_attn = x_attn.sum(1, keepdim=True)
        return x_attn


class TemporalConcatPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_features, history_len=1):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_features = in_features
        self.out_features = in_features * history_len

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None) ->torch.Tensor:
        """
        Concat pooling forward.

        Args:
            x: tensor of size
                (batch_size, history_len, feature_size)
            mask: mask to use

        Returns:
            concated result
        """
        x = x.view(x.shape[0], -1)
        return x


class TemporalDropLastWrapper(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, net):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.net = net

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None):
        """@TODO: Docs. Contribution is welcome."""
        x = x[:, :-1, :]
        x_out = self.net(x)
        return x_out


def _get_pooling(key, in_features, **params):
    """@TODO: Docs. Contribution is welcome."""
    key_prefix = key.split('_', 1)[0]
    if key_prefix == 'last':
        return TemporalLastPooling()
    elif key_prefix == 'avg':
        layer = TemporalAvgPooling()
    elif key_prefix == 'max':
        layer = TemporalMaxPooling()
    elif key_prefix in ['softmax', 'tanh', 'sigmoid']:
        layer = TemporalAttentionPooling(in_features=in_features, activation=key_prefix, **params)
    else:
        raise NotImplementedError()
    if 'droplast' in key:
        layer = TemporalDropLastWrapper(layer)
    return layer


class LamaPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""
    available_groups = ['last', 'avg', 'avg_droplast', 'max', 'max_droplast', 'sigmoid', 'sigmoid_droplast', 'softmax', 'softmax_droplast', 'tanh', 'tanh_droplast']

    def __init__(self, in_features, groups=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_features = in_features
        self.groups = groups or ['last', 'avg_droplast', 'max_droplast', 'softmax_droplast']
        self.out_features = in_features * len(self.groups)
        groups = {}
        for key in self.groups:
            if isinstance(key, str):
                groups[key] = _get_pooling(key, self.in_features)
            elif isinstance(key, dict):
                key_key = key.pop('key')
                groups[key_key] = _get_pooling(key_key, in_features, **key)
            else:
                raise NotImplementedError()
        self.groups = nn.ModuleDict(groups)

    def forward(self, x: 'torch.Tensor', mask: 'torch.Tensor'=None) ->torch.Tensor:
        """
        Forward method of the LAMA.

        Args:
            x: tensor of size
                (batch_size, history_len, feature_size)
            mask: mask to use for attention compute

        Returns:
            torch.Tensor: LAMA pooling result
        """
        batch_size, history_len, feature_size = x.shape
        features_list = []
        for pooling_fn in self.groups.values():
            features = pooling_fn(x, mask)
            features_list.append(features)
        x = torch.cat(features_list, dim=1)
        x = x.view(batch_size, -1)
        return x


class GlobalAvgPool2d(nn.Module):
    """Applies a 2D global average pooling operation over an input signal
    composed of several input planes.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self):
        """Constructor method for the ``GlobalAvgPool2d`` class."""
        super().__init__()

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        h, w = x.shape[2:]
        return F.avg_pool2d(input=x, kernel_size=(h, w))

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features


class GlobalMaxPool2d(nn.Module):
    """Applies a 2D global max pooling operation over an input signal
    composed of several input planes.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self):
        """Constructor method for the ``GlobalMaxPool2d`` class."""
        super().__init__()

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        h, w = x.shape[2:]
        return F.max_pool2d(input=x, kernel_size=(h, w))

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features


class GlobalConcatPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self):
        """Constructor method for the ``GlobalConcatPool2d`` class."""
        super().__init__()
        self.avg = GlobalAvgPool2d()
        self.max = GlobalMaxPool2d()

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.avg(x), self.max(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features * 2


class GlobalAttnPool2d(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        activation_fn = REGISTRY.get_if_str(activation_fn)
        self.attn = nn.Sequential(nn.Conv2d(in_features, 1, kernel_size=1, stride=1, padding=0, bias=False), activation_fn())

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        x_a = self.attn(x)
        x = x * x_a
        x = torch.sum(x, dim=[-2, -1], keepdim=True)
        return x

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features


class GlobalAvgAttnPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.avg = GlobalAvgPool2d()
        self.attn = GlobalAttnPool2d(in_features, activation_fn)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.avg(x), self.attn(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features * 2


class GlobalMaxAttnPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.max = GlobalMaxPool2d()
        self.attn = GlobalAttnPool2d(in_features, activation_fn)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.max(x), self.attn(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features * 2


class GlobalConcatAttnPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.avg = GlobalAvgPool2d()
        self.max = GlobalMaxPool2d()
        self.attn = GlobalAttnPool2d(in_features, activation_fn)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.avg(x), self.max(x), self.attn(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample

        Returns:
            number of output features
        """
        return in_features * 3


class GeM2d(nn.Module):
    """Implementation of GeM: Generalized Mean Pooling.
    Example:
        >>> x = torch.randn(2,1280,8,8) #output of last convolutional layer of the network
        >>> gem_pool = GeM2d(p = 2.2 , p_trainable = False)
        >>> op = gem_pool(x)
        >>> op.shape
        torch.Size([1, 1280, 1, 1])
        >>> op
        tensor([[[[1.0660]],
             [[1.1599]],
             [[0.5934]],
             ...,
             [[0.6889]],
             [[1.0361]],
             [[0.9717]]]], grad_fn=<PowBackward0>)
    """

    def __init__(self, p: 'float'=3.0, p_trainable: 'bool'=False, eps: 'float'=1e-07):
        """
        Args:
            p: The pooling parameter.
                Default: 3.0
            p_trainable: Whether the pooling parameter(p) should be trainable.
                    Default: False
            eps: epsilon for numerical stability.
        """
        super().__init__()
        if p_trainable:
            if p not in [math.inf, float('inf')]:
                self.p = nn.Parameter(torch.ones(1) * p)
            else:
                self.p = math.inf
        else:
            self.p = p
        self.eps = eps

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Forward call."""
        h, w = x.shape[2:]
        if self.p in [math.inf, float('inf')]:
            return F.max_pool2d(x, kernel_size=(h, w))
        else:
            x = x.clamp(min=self.eps).pow(self.p)
            return F.avg_pool2d(x, kernel_size=(h, w)).pow(1.0 / self.p)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample.

        Returns:
            number of output features
        """
        return in_features


class RMSNorm(nn.Module):
    """An implementation of RMS Normalization.

    @TODO: Docs (link to paper). Contribution is welcome.
    """

    def __init__(self, dimension: 'int', epsilon: 'float'=1e-08, is_bias: 'bool'=False):
        """
        Args:
            dimension: the dimension of the layer output to normalize
            epsilon: an epsilon to prevent dividing by zero
                in case the layer has zero variance. (default = 1e-8)
            is_bias: a boolean value whether to include bias term
                while normalization
        """
        super().__init__()
        self.dimension = dimension
        self.epsilon = epsilon
        self.is_bias = is_bias
        self.scale = nn.Parameter(torch.ones(self.dimension))
        if self.is_bias:
            self.bias = nn.Parameter(torch.zeros(self.dimension))

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """@TODO: Docs. Contribution is welcome."""
        x_std = torch.sqrt(torch.mean(x ** 2, -1, keepdim=True))
        x_norm = x / (x_std + self.epsilon)
        if self.is_bias:
            return self.scale * x_norm + self.bias
        return self.scale * x_norm


class cSE(nn.Module):
    """
    The channel-wise SE (Squeeze and Excitation) block from the
    `Squeeze-and-Excitation Networks`__ paper.

    Adapted from
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939
    and
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178

    Shape:

    - Input: (batch, channels, height, width)
    - Output: (batch, channels, height, width) (same shape as input)

    __ https://arxiv.org/abs/1709.01507
    """

    def __init__(self, in_channels: 'int', r: 'int'=16):
        """
        Args:
            in_channels: The number of channels
                in the feature map of the input.
            r: The reduction ratio of the intermediate channels.
                Default: 16.
        """
        super().__init__()
        self.linear1 = nn.Linear(in_channels, in_channels // r)
        self.linear2 = nn.Linear(in_channels // r, in_channels)

    def forward(self, x: 'torch.Tensor'):
        """Forward call."""
        input_x = x
        x = x.view(*x.shape[:-2], -1).mean(-1)
        x = F.relu(self.linear1(x), inplace=True)
        x = self.linear2(x)
        x = x.unsqueeze(-1).unsqueeze(-1)
        x = torch.sigmoid(x)
        x = torch.mul(input_x, x)
        return x


class sSE(nn.Module):
    """
    The sSE (Channel Squeeze and Spatial Excitation) block from the
    `Concurrent Spatial and Channel ‘Squeeze & Excitation’
    in Fully Convolutional Networks`__ paper.

    Adapted from
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178

    Shape:

    - Input: (batch, channels, height, width)
    - Output: (batch, channels, height, width) (same shape as input)

    __ https://arxiv.org/abs/1803.02579
    """

    def __init__(self, in_channels: 'int'):
        """
        Args:
            in_channels: The number of channels
                in the feature map of the input.
        """
        super().__init__()
        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1, stride=1)

    def forward(self, x: 'torch.Tensor'):
        """Forward call."""
        input_x = x
        x = self.conv(x)
        x = torch.sigmoid(x)
        x = torch.mul(input_x, x)
        return x


class scSE(nn.Module):
    """
    The scSE (Concurrent Spatial and Channel Squeeze and Channel Excitation)
    block from the `Concurrent Spatial and Channel ‘Squeeze & Excitation’
    in Fully Convolutional Networks`__ paper.

    Adapted from
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178

    Shape:

    - Input: (batch, channels, height, width)
    - Output: (batch, channels, height, width) (same shape as input)

    __ https://arxiv.org/abs/1803.02579
    """

    def __init__(self, in_channels: 'int', r: 'int'=16):
        """
        Args:
            in_channels: The number of channels
                in the feature map of the input.
            r: The reduction ratio of the intermediate channels.
                Default: 16.
        """
        super().__init__()
        self.cse_block = cSE(in_channels, r)
        self.sse_block = sSE(in_channels)

    def forward(self, x: 'torch.Tensor'):
        """Forward call."""
        cse = self.cse_block(x)
        sse = self.sse_block(x)
        x = torch.add(cse, sse)
        return x


class SoftMax(nn.Module):
    """Implementation of
    `Significance of Softmax-based Features in Comparison to
    Distance Metric Learning-based Features`_.

    .. _Significance of Softmax-based Features in Comparison to         Distance Metric Learning-based Features:
        https://arxiv.org/abs/1712.10151

    Args:
        in_features: size of each input sample.
        out_features: size of each output sample.

    Shape:
        - Input: :math:`(batch, H_{in})` where
          :math:`H_{in} = in\\_features`.
        - Output: :math:`(batch, H_{out})` where
          :math:`H_{out} = out\\_features`.

    Example:
        >>> layer = SoftMax(5, 10)
        >>> loss_fn = nn.CrosEntropyLoss()
        >>> embedding = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(10)
        >>> output = layer(embedding, target)
        >>> loss = loss_fn(output, target)
        >>> self.engine.backward(loss)

    """

    def __init__(self, in_features: 'int', num_classes: 'int'):
        super(SoftMax, self).__init__()
        self.in_features = in_features
        self.out_features = num_classes
        self.weight = nn.Parameter(torch.FloatTensor(num_classes, in_features))
        self.bias = nn.Parameter(torch.FloatTensor(num_classes))
        nn.init.xavier_uniform_(self.weight)
        nn.init.zeros_(self.bias)

    def __repr__(self) ->str:
        """Object representation."""
        rep = f'SoftMax(in_features={self.in_features},out_features={self.out_features})'
        return rep

    def forward(self, input: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            input: input features,
                expected shapes ``BxF`` where ``B``
                is batch dimension and ``F`` is an
                input feature dimension.

        Returns:
            tensor (logits) with shapes ``BxC``
            where ``C`` is a number of classes
            (out_features).
        """
        return F.linear(input, self.weight, self.bias)


class NaiveCrossEntropyLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, size_average=True):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.size_average = size_average

    def forward(self, input_: 'torch.Tensor', target: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``input_`` and ``target`` tensors.

        Args:
            input_: input tensor of shape ...
            target: target tensor of shape ...

        @TODO: Docs (add shapes). Contribution is welcome.
        """
        assert input_.size() == target.size()
        input_ = F.log_softmax(input_)
        loss = -torch.sum(input_ * target)
        loss = loss / input_.size()[0] if self.size_average else loss
        return loss


class SymmetricCrossEntropyLoss(nn.Module):
    """The Symmetric Cross Entropy loss.

    It has been proposed in `Symmetric Cross Entropy for Robust Learning
    with Noisy Labels`_.

    .. _Symmetric Cross Entropy for Robust Learning with Noisy Labels:
        https://arxiv.org/abs/1908.06112
    """

    def __init__(self, alpha: 'float'=1.0, beta: 'float'=1.0):
        """
        Args:
            alpha(float):
                corresponds to overfitting issue of CE
            beta(float):
                corresponds to flexible exploration on the robustness of RCE
        """
        super(SymmetricCrossEntropyLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta

    def forward(self, input_: 'torch.Tensor', target: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``input_`` and ``target`` tensors.

        Args:
            input_: input tensor of size
                (batch_size, num_classes)
            target: target tensor of size (batch_size), where
                values of a vector correspond to class index

        Returns:
            torch.Tensor: computed loss
        """
        num_classes = input_.shape[1]
        target_one_hot = F.one_hot(target, num_classes).float()
        assert target_one_hot.shape == input_.shape
        input_ = torch.clamp(input_, min=1e-07, max=1.0)
        target_one_hot = torch.clamp(target_one_hot, min=0.0001, max=1.0)
        cross_entropy = (-torch.sum(target_one_hot * torch.log(input_), dim=1)).mean()
        reverse_cross_entropy = (-torch.sum(input_ * torch.log(target_one_hot), dim=1)).mean()
        loss = self.alpha * cross_entropy + self.beta * reverse_cross_entropy
        return loss


class MaskCrossEntropyLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, *args, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ce_loss = nn.CrossEntropyLoss(*args, **kwargs, reduction='none')

    def forward(self, logits: 'torch.Tensor', target: 'torch.Tensor', mask: 'torch.Tensor') ->torch.Tensor:
        """
        Calculates loss between ``logits`` and ``target`` tensors.

        Args:
            logits: model logits
            target: true targets
            mask: targets mask

        Returns:
            torch.Tensor: computed loss
        """
        loss = self.ce_loss.forward(logits, target)
        loss = torch.mean(loss[mask == 1])
        return loss


def _convert_label_to_similarity(normed_features: 'Tensor', labels: 'Tensor') ->Tuple[Tensor, Tensor]:
    similarity_matrix = normed_features @ normed_features.transpose(1, 0)
    label_matrix = labels.unsqueeze(1) == labels.unsqueeze(0)
    positive_matrix = label_matrix.triu(diagonal=1)
    negative_matrix = label_matrix.logical_not().triu(diagonal=1)
    similarity_matrix = similarity_matrix.view(-1)
    positive_matrix = positive_matrix.view(-1)
    negative_matrix = negative_matrix.view(-1)
    sp, sn = similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]
    return sp, sn


class CircleLoss(nn.Module):
    """
    CircleLoss from
    `Circle Loss: A Unified Perspective of Pair Similarity Optimization`_ paper.

    Adapter from:
    https://github.com/TinyZeaMays/CircleLoss

    Example:
        >>> import torch
        >>> from torch.nn import functional as F
        >>> from catalyst.contrib.losses import CircleLoss
        >>>
        >>> features = F.normalize(torch.rand(256, 64, requires_grad=True))
        >>> labels = torch.randint(high=10, size=(256))
        >>> criterion = CircleLoss(margin=0.25, gamma=256)
        >>> criterion(features, labels)

    .. _`Circle Loss: A Unified Perspective of Pair Similarity Optimization`:
        https://arxiv.org/abs/2002.10857
    """

    def __init__(self, margin: 'float', gamma: 'float') ->None:
        """

        Args:
            margin: margin to use
            gamma: gamma to use
        """
        super().__init__()
        self.margin = margin
        self.gamma = gamma
        self.soft_plus = nn.Softplus()

    def forward(self, normed_features: 'Tensor', labels: 'Tensor') ->Tensor:
        """

        Args:
            normed_features: batch with samples features of shape
                [bs; feature_len]
            labels: batch with samples correct labels of shape [bs; ]

        Returns:
            torch.Tensor: circle loss
        """
        sp, sn = _convert_label_to_similarity(normed_features, labels)
        ap = torch.clamp_min(-sp.detach() + 1 + self.margin, min=0.0)
        an = torch.clamp_min(sn.detach() + self.margin, min=0.0)
        delta_p = 1 - self.margin
        delta_n = self.margin
        logit_p = -ap * (sp - delta_p) * self.gamma
        logit_n = an * (sn - delta_n) * self.gamma
        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))
        return loss


class ContrastiveEmbeddingLoss(nn.Module):
    """The Contrastive embedding loss.

    It has been proposed in `Dimensionality Reduction
    by Learning an Invariant Mapping`_.

    .. _Dimensionality Reduction by Learning an Invariant Mapping:
        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    """

    def __init__(self, margin=1.0, reduction='mean'):
        """
        Args:
            margin: margin parameter
            reduction: criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, embeddings_left: 'torch.Tensor', embeddings_right: 'torch.Tensor', distance_true) ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Args:
            embeddings_left: left objects embeddings
            embeddings_right: right objects embeddings
            distance_true: true distances

        Returns:
            torch.Tensor: loss
        """
        diff = embeddings_left - embeddings_right
        distance_pred = torch.sqrt(torch.sum(torch.pow(diff, 2), 1))
        bs = len(distance_true)
        margin_distance = self.margin - distance_pred
        margin_distance = torch.clamp(margin_distance, min=0.0)
        loss = (1 - distance_true) * torch.pow(distance_pred, 2) + distance_true * torch.pow(margin_distance, 2)
        if self.reduction == 'mean':
            loss = torch.sum(loss) / 2.0 / bs
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


class ContrastiveDistanceLoss(nn.Module):
    """The Contrastive distance loss.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, margin=1.0, reduction='mean'):
        """
        Args:
            margin: margin parameter
            reduction: criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, distance_pred, distance_true) ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Args:
            distance_pred: predicted distances
            distance_true: true distances

        Returns:
            torch.Tensor: loss
        """
        bs = len(distance_true)
        margin_distance = self.margin - distance_pred
        margin_distance = torch.clamp(margin_distance, min=0.0)
        loss = (1 - distance_true) * torch.pow(distance_pred, 2) + distance_true * torch.pow(margin_distance, 2)
        if self.reduction == 'mean':
            loss = torch.sum(loss) / 2.0 / bs
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


class ContrastivePairwiseEmbeddingLoss(nn.Module):
    """ContrastivePairwiseEmbeddingLoss – proof of concept criterion.

    Still work in progress.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, margin=1.0, reduction='mean'):
        """
        Args:
            margin: margin parameter
            reduction: criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, embeddings_pred, embeddings_true) ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Work in progress.

        Args:
            embeddings_pred: predicted embeddings
            embeddings_true: true embeddings

        Returns:
            torch.Tensor: loss
        """
        device = embeddings_pred.device
        pairwise_similarity = torch.einsum('se,ae->sa', embeddings_pred, embeddings_true)
        bs = embeddings_pred.shape[0]
        batch_idx = torch.arange(bs, device=device)
        loss = F.cross_entropy(pairwise_similarity, batch_idx, reduction=self.reduction)
        return loss


class BarlowTwinsLoss(nn.Module):
    """The Contrastive embedding loss.

    It has been proposed in `Barlow Twins:
    Self-Supervised Learning via Redundancy Reduction`_.

    Example:

    .. code-block:: python

        import torch
        from torch.nn import functional as F
        from catalyst.contrib import BarlowTwinsLoss

        embeddings_left = F.normalize(torch.rand(256, 64, requires_grad=True))
        embeddings_right = F.normalize(torch.rand(256, 64, requires_grad=True))
        criterion = BarlowTwinsLoss(offdiag_lambda = 1)
        criterion(embeddings_left, embeddings_right)

    .. _`Barlow Twins: Self-Supervised Learning via Redundancy Reduction`:
        https://arxiv.org/abs/2103.03230
    """

    def __init__(self, offdiag_lambda=1.0, eps=1e-12):
        """
        Args:
            offdiag_lambda: trade-off parameter
            eps: shift for the varience (var + eps)
        """
        super().__init__()
        self.offdiag_lambda = offdiag_lambda
        self.eps = eps

    def forward(self, embeddings_left: 'torch.Tensor', embeddings_right: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Args:
            embeddings_left: left objects embeddings [batch_size, features_dim]
            embeddings_right: right objects embeddings [batch_size, features_dim]

        Raises:
            ValueError: if the batch size is 1
            ValueError: if embeddings_left and embeddings_right shapes are different
            ValueError: if embeddings shapes are not in a form (batch_size, features_dim)

        Returns:
            torch.Tensor: loss
        """
        shape_left, shape_right = embeddings_left.shape, embeddings_right.shape
        if len(shape_left) != 2:
            raise ValueError(f'Left shape should be (batch_size, feature_dim),but got - {shape_left}!')
        elif len(shape_right) != 2:
            raise ValueError(f'Right shape should be (batch_size, feature_dim),but got - {shape_right}!')
        if shape_left[0] == 1:
            raise ValueError(f'Batch size should be >= 2, but got - {shape_left[0]}!')
        if shape_left != shape_right:
            raise ValueError(f'Shapes should be equall, but got - {shape_left} and {shape_right}!')
        z_left = (embeddings_left - embeddings_left.mean(dim=0)) / (embeddings_left.var(dim=0) + self.eps).pow(1 / 2)
        z_right = (embeddings_right - embeddings_right.mean(dim=0)) / (embeddings_right.var(dim=0) + self.eps).pow(1 / 2)
        batch_size = z_left.shape[0]
        cross_correlation = torch.matmul(z_left.T, z_right) / batch_size
        on_diag = torch.diagonal(cross_correlation)
        off_diag = cross_correlation.clone().fill_diagonal_(0)
        on_diag_loss = on_diag.add_(-1).pow_(2).sum()
        off_diag_loss = off_diag.pow_(2).sum()
        loss = on_diag_loss + self.offdiag_lambda * off_diag_loss
        return loss


def _dice(tp: 'torch.Tensor', fp: 'torch.Tensor', fn: 'torch.Tensor', eps: 'float'=1e-07) ->torch.Tensor:
    union = tp + fp + fn
    score = (2 * tp + eps * (union == 0).float()) / (2 * tp + fp + fn + eps)
    return score


def get_segmentation_statistics(outputs: 'torch.Tensor', targets: 'torch.Tensor', class_dim: 'int'=1, threshold: 'float'=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Computes true positive, false positive, false negative
    for a multilabel segmentation problem.

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1)
        threshold: threshold for outputs binarization

    Returns:
        Segmentation stats

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.get_segmentation_statistics(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
        )
        # (
        #     tensor([ 0.,  0.,  0., 16.,  8.,  4.]),  # per class TP
        #     tensor([0., 8., 0., 0., 0., 0.]),        # per class FP
        #     tensor([16.,  8.,  0.,  0.,  0.,  4.]),  # per class TN
        # )
    """
    assert outputs.shape == targets.shape, f'targets(shape {targets.shape}) and outputs(shape {outputs.shape}) must have the same shape'
    if threshold is not None:
        outputs = (outputs > threshold).float()
    n_dims = len(outputs.shape)
    dims = list(range(n_dims))
    if class_dim < 0:
        class_dim = n_dims + class_dim
    dims.pop(class_dim)
    sum_per_class = partial(torch.sum, dim=dims)
    tp = sum_per_class(outputs * targets)
    class_union = sum_per_class(outputs) + sum_per_class(targets)
    class_union -= tp
    fp = sum_per_class(outputs * (1 - targets))
    fn = sum_per_class(targets * (1 - outputs))
    return tp, fp, fn


def _get_region_based_metrics(outputs: 'torch.Tensor', targets: 'torch.Tensor', metric_fn: 'Callable', class_dim=None, threshold: 'float'=None, mode: 'str'='per-class', weights: 'Optional[List[float]]'=None) ->torch.Tensor:
    """
    Get aggregated metric

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        metric_fn: metric function, that get statistics and return score
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1), if
            mode = "micro" means nothing
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
             and metric are calculated generally. If mode='macro', metric are
             calculated per-class and than are averaged over all classes. If
             mode='weighted', metric are calculated per-class and than summed
             over all classes with weights. If mode='per-class', metric are
             calculated separately for all classes
        weights: class weights(for mode="weighted")

    Returns:
        computed metric
    """
    assert mode in ['per-class', 'micro', 'macro', 'weighted']
    segmentation_stats = get_segmentation_statistics(outputs=outputs, targets=targets, class_dim=class_dim, threshold=threshold)
    if mode == 'micro':
        segmentation_stats = [torch.sum(stats) for stats in segmentation_stats]
        metric = metric_fn(*segmentation_stats)
    metrics_per_class = metric_fn(*segmentation_stats)
    if mode == 'macro':
        metric = torch.mean(metrics_per_class)
    elif mode == 'weighted':
        assert len(weights) == len(segmentation_stats[0])
        device = metrics_per_class.device
        metrics = torch.tensor(weights) * metrics_per_class
        metric = torch.sum(metrics)
    elif mode == 'per-class':
        metric = metrics_per_class
    return metric


def dice(outputs: 'torch.Tensor', targets: 'torch.Tensor', class_dim: 'int'=1, threshold: 'float'=None, mode: 'str'='per-class', weights: 'Optional[List[float]]'=None, eps: 'float'=1e-07) ->torch.Tensor:
    """
    Computes the dice score,
    dice score = 2 * intersection / (intersection + union)) =     = 2 * tp / (2 * tp + fp + fn)

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1), if
            mode = "micro" means nothing
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
            and metric are calculated generally. If mode='macro', metric are
            calculated per-class and than are averaged over all classes. If
            mode='weighted', metric are calculated per-class and than summed
            over all classes with weights. If mode='per-class', metric are
            calculated separately for all classes
        weights: class weights(for mode="weighted")
        eps: epsilon to avoid zero division

    Returns:
        Dice score for each class(if mode='weighted') or aggregated Dice

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.dice(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="per-class"
        )
        # tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.6667])

        metrics.dice(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="macro"
        )
        # tensor(0.6111)

        metrics.dice(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="micro"
        )
        # tensor(0.6087)
    """
    metric_fn = partial(_dice, eps=eps)
    score = _get_region_based_metrics(outputs=outputs, targets=targets, metric_fn=metric_fn, class_dim=class_dim, threshold=threshold, mode=mode, weights=weights)
    return score


class DiceLoss(nn.Module):
    """The Dice loss.
    DiceLoss = 1 - dice score
    dice score = 2 * intersection / (intersection + union)) =     = 2 * tp / (2 * tp + fp + fn)
    """

    def __init__(self, class_dim: 'int'=1, mode: 'str'='macro', weights: 'List[float]'=None, eps: 'float'=1e-07):
        """
        Args:
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated per-class and than are averaged over all classes.
                If mode='weighted', metric are calculated per-class and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        assert mode in ['micro', 'macro', 'weighted']
        self.loss_fn = partial(dice, eps=eps, class_dim=class_dim, threshold=None, mode=mode, weights=weights)

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``logits`` and ``target`` tensors."""
        dice_score = self.loss_fn(outputs, targets)
        return 1 - dice_score


class FocalLossBinary(_Loss):
    """Compute focal loss for binary classification problem.

    It has been proposed in `Focal Loss for Dense Object Detection`_ paper.

    .. _Focal Loss for Dense Object Detection: https://arxiv.org/abs/1708.02002
    """

    def __init__(self, ignore: 'int'=None, reduced: 'bool'=False, gamma: 'float'=2.0, alpha: 'float'=0.25, threshold: 'float'=0.5, reduction: 'str'='mean'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        if reduced:
            self.loss_fn = partial(metrics.reduced_focal_loss, gamma=gamma, threshold=threshold, reduction=reduction)
        else:
            self.loss_fn = partial(metrics.sigmoid_focal_loss, gamma=gamma, alpha=alpha, reduction=reduction)

    def forward(self, logits, targets):
        """
        Args:
            logits: [bs; ...]
            targets: [bs; ...]

        Returns:
            computed loss
        """
        targets = targets.view(-1)
        logits = logits.view(-1)
        if self.ignore is not None:
            not_ignored = targets != self.ignore
            logits = logits[not_ignored]
            targets = targets[not_ignored]
        loss = self.loss_fn(logits, targets)
        return loss


class FocalLossMultiClass(FocalLossBinary):
    """Compute focal loss for multiclass problem. Ignores targets having -1 label.

    It has been proposed in `Focal Loss for Dense Object Detection`_ paper.

    .. _Focal Loss for Dense Object Detection: https://arxiv.org/abs/1708.02002
    """

    def forward(self, logits, targets):
        """
        Args:
            logits: [bs; num_classes; ...]
            targets: [bs; ...]

        Returns:
            computed loss
        """
        num_classes = logits.size(1)
        loss = 0
        targets = targets.view(-1)
        logits = logits.view(-1, num_classes)
        if self.ignore is not None:
            not_ignored = targets != self.ignore
        for class_id in range(num_classes):
            cls_label_target = (targets == class_id + 0).long()
            cls_label_input = logits[..., class_id]
            if self.ignore is not None:
                cls_label_target = cls_label_target[not_ignored]
                cls_label_input = cls_label_input[not_ignored]
            loss += self.loss_fn(cls_label_input, cls_label_target)
        return loss


class MeanOutputLoss(nn.Module):
    """
    Criterion to compute simple mean of the output, completely ignoring target
    (maybe useful e.g. for WGAN real/fake validity averaging.
    """

    def forward(self, output, target):
        """Compute criterion.
        @TODO: Docs (add typing). Contribution is welcome.
        """
        return output.mean()


class GradientPenaltyLoss(nn.Module):
    """Criterion to compute gradient penalty.

    WARN: SHOULD NOT BE RUN WITH CriterionCallback,
        use special GradientPenaltyCallback instead
    """

    def forward(self, fake_data, real_data, critic, critic_condition_args):
        """Compute gradient penalty.
        @TODO: Docs. Contribution is welcome.
        """
        device = real_data.device
        alpha = torch.rand((real_data.size(0), 1, 1, 1), device=device)
        interpolates = (alpha * real_data + (1 - alpha) * fake_data).detach()
        interpolates.requires_grad_(True)
        with torch.set_grad_enabled(True):
            d_interpolates = critic(interpolates, *critic_condition_args)
        fake = torch.ones((real_data.size(0), 1), device=device, requires_grad=False)
        gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True)[0]
        gradients = gradients.view(gradients.size(0), -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        return gradient_penalty


def _iou(tp: 'torch.Tensor', fp: 'torch.Tensor', fn: 'torch.Tensor', eps: 'float'=1e-07) ->torch.Tensor:
    union = tp + fp + fn
    score = (tp + eps * (union == 0).float()) / (tp + fp + fn + eps)
    return score


def iou(outputs: 'torch.Tensor', targets: 'torch.Tensor', class_dim: 'int'=1, threshold: 'float'=None, mode: 'str'='per-class', weights: 'Optional[List[float]]'=None, eps: 'float'=1e-07) ->torch.Tensor:
    """
    Computes the iou/jaccard score,
    iou score = intersection / union = tp / (tp + fp + fn)

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1), if
            mode = "micro" means nothing
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
            and metric are calculated generally. If mode='macro', metric are
            calculated per-class and than are averaged over all classes. If
            mode='weighted', metric are calculated per-class and than summed
            over all classes with weights. If mode='per-class', metric are
            calculated separately for all classes
        weights: class weights(for mode="weighted")
        eps: epsilon to avoid zero division

    Returns:
        IoU (Jaccard) score for each class(if mode='weighted') or aggregated IOU

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.iou(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="per-class"
        )
        # tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.5])

        metrics.iou(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="macro"
        )
        # tensor(0.5833)

        metrics.iou(
            outputs=pred,
            targets=targets,
            class_dim=1,
            threshold=0.5,
            mode="micro"
        )
        # tensor(0.4375)
    """
    metric_fn = partial(_iou, eps=eps)
    score = _get_region_based_metrics(outputs=outputs, targets=targets, metric_fn=metric_fn, class_dim=class_dim, threshold=threshold, mode=mode, weights=weights)
    return score


class IoULoss(nn.Module):
    """The intersection over union (Jaccard) loss.
    IOULoss = 1 - iou score
    iou score = intersection / union = tp / (tp + fp + fn)
    """

    def __init__(self, class_dim: 'int'=1, mode: 'str'='macro', weights: 'List[float]'=None, eps: 'float'=1e-07):
        """
        Args:
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated per-class and than are averaged over all classes.
                If mode='weighted', metric are calculated per-class and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        assert mode in ['micro', 'macro', 'weighted']
        self.loss_fn = partial(iou, eps=eps, class_dim=class_dim, threshold=None, mode=mode, weights=weights)

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``logits`` and ``target`` tensors."""
        iou_score = self.loss_fn(outputs, targets)
        return 1 - iou_score


def _flatten_binary_scores(logits, targets, ignore=None):
    """
    Flattens predictions in the batch (binary case).
    Remove targets equal to "ignore"
    """
    logits = logits.reshape(-1)
    targets = targets.reshape(-1)
    if ignore is None:
        return logits, targets
    valid = targets != ignore
    logits_ = logits[valid]
    targets_ = targets[valid]
    return logits_, targets_


def _lovasz_grad(gt_sorted):
    """
    Compute gradient of the Lovasz extension w.r.t sorted errors,
    see Alg. 1 in paper
    """
    p = len(gt_sorted)
    gts = gt_sorted.sum()
    intersection = gts - gt_sorted.float().cumsum(0)
    union = gts + (1 - gt_sorted).float().cumsum(0)
    jaccard = 1.0 - intersection / union
    if p > 1:
        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]
    return jaccard


def _lovasz_hinge_flat(logits, targets):
    """The binary Lovasz hinge loss.

    Args:
        logits: [P] Variable, logits at each prediction
            (between -iinfinity and +iinfinity)
        targets: [P] Tensor, binary ground truth targets (0 or 1)
    """
    if len(targets) == 0:
        return logits.sum() * 0.0
    signs = 2.0 * targets.float() - 1.0
    errors = 1.0 - logits * signs
    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)
    perm = perm.data
    gt_sorted = targets[perm]
    grad = _lovasz_grad(gt_sorted)
    loss = torch.dot(F.relu(errors_sorted), grad)
    return loss


def isnan(x):
    return x != x


def mean(values, ignore_nan=False, empty=0):
    """
    Nanmean compatible with generators.
    """
    values = iter(values)
    if ignore_nan:
        values = ifilterfalse(isnan, values)
    try:
        n = 1
        acc = next(values)
    except StopIteration:
        if empty == 'raise':
            raise ValueError('Empty mean')
        return empty
    for n, v in enumerate(values, 2):
        acc += v
    if n == 1:
        return acc
    return acc / n


def _lovasz_hinge(logits, targets, per_image=True, ignore=None):
    """The binary Lovasz hinge loss.

    Args:
        logits: [B, H, W] Variable, logits at each pixel
            (between -infinity and +infinity)
        targets: [B, H, W] Tensor, binary ground truth masks (0 or 1)
        per_image: compute the loss per image instead of per batch
        ignore: void class id
    """
    if per_image:
        loss = mean(_lovasz_hinge_flat(*_flatten_binary_scores(logit.unsqueeze(0), target.unsqueeze(0), ignore)) for logit, target in zip(logits, targets))
    else:
        loss = _lovasz_hinge_flat(*_flatten_binary_scores(logits, targets, ignore))
    return loss


class LovaszLossBinary(_Loss):
    """Creates a criterion that optimizes a binary Lovasz loss.

    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate
    for the optimization of the intersection-over-union measure
    in neural networks`_.

    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization
        of the intersection-over-union measure in neural networks:
        https://arxiv.org/abs/1705.08790
    """

    def __init__(self, per_image=False, ignore=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        self.per_image = per_image

    def forward(self, logits, targets):
        """Forward propagation method for the Lovasz loss.

        Args:
            logits: [bs; ...]
            targets: [bs; ...]

        @TODO: Docs. Contribution is welcome.
        """
        loss = _lovasz_hinge(logits, targets, per_image=self.per_image, ignore=self.ignore)
        return loss


def _flatten_probabilities(probabilities, targets, ignore=None):
    """
    Flattens predictions in the batch
    """
    if probabilities.dim() == 3:
        B, H, W = probabilities.size()
        probabilities = probabilities.view(B, 1, H, W)
    B, C, H, W = probabilities.size()
    probabilities = probabilities.permute(0, 2, 3, 1).contiguous().view(-1, C)
    targets = targets.view(-1)
    if ignore is None:
        return probabilities, targets
    valid = targets != ignore
    probabilities_ = probabilities[valid.nonzero().squeeze()]
    targets_ = targets[valid]
    return probabilities_, targets_


def _lovasz_softmax_flat(probabilities, targets, classes='present'):
    """The multiclass Lovasz-Softmax loss.

    Args:
        probabilities: [P, C]
            class probabilities at each prediction (between 0 and 1)
        targets: [P] ground truth targets (between 0 and C - 1)
        classes: "all" for all,
            "present" for classes present in targets,
             or a list of classes to average.
    """
    if probabilities.numel() == 0:
        return probabilities * 0.0
    C = probabilities.size(1)
    losses = []
    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes
    for c in class_to_sum:
        fg = (targets == c).float()
        if classes == 'present' and fg.sum() == 0:
            continue
        if C == 1:
            if len(class_to_sum) > 1:
                raise ValueError('Sigmoid output possible only with 1 class')
            class_pred = probabilities[:, 0]
        else:
            class_pred = probabilities[:, c]
        errors = (fg - class_pred).abs()
        errors_sorted, perm = torch.sort(errors, 0, descending=True)
        perm = perm.data
        fg_sorted = fg[perm]
        losses.append(torch.dot(errors_sorted, _lovasz_grad(fg_sorted)))
    return mean(losses)


def _lovasz_softmax(probabilities, targets, classes='present', per_image=False, ignore=None):
    """The multiclass Lovasz-Softmax loss.

    Args:
        probabilities: [B, C, H, W]
            class probabilities at each prediction (between 0 and 1).
            Interpreted as binary (sigmoid) output
            with outputs of size [B, H, W].
        targets: [B, H, W] ground truth targets (between 0 and C - 1)
        classes: "all" for all,
            "present" for classes present in targets,
            or a list of classes to average.
        per_image: compute the loss per image instead of per batch
        ignore: void class targets
    """
    if per_image:
        loss = mean(_lovasz_softmax_flat(*_flatten_probabilities(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes) for prob, lab in zip(probabilities, targets))
    else:
        loss = _lovasz_softmax_flat(*_flatten_probabilities(probabilities, targets, ignore), classes=classes)
    return loss


class LovaszLossMultiClass(_Loss):
    """Creates a criterion that optimizes a multiclass Lovasz loss.

    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate
    for the optimization of the intersection-over-union measure
    in neural networks`_.

    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization
        of the intersection-over-union measure in neural networks:
        https://arxiv.org/abs/1705.08790
    """

    def __init__(self, per_image=False, ignore=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        self.per_image = per_image

    def forward(self, logits, targets):
        """Forward propagation method for the Lovasz loss.

        Args:
            logits: [bs; num_classes; ...]
            targets: [bs; ...]

        @TODO: Docs. Contribution is welcome.
        """
        loss = _lovasz_softmax(logits, targets, per_image=self.per_image, ignore=self.ignore)
        return loss


class LovaszLossMultiLabel(_Loss):
    """Creates a criterion that optimizes a multilabel Lovasz loss.

    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate
    for the optimization of the intersection-over-union measure
    in neural networks`_.

    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization
        of the intersection-over-union measure in neural networks:
        https://arxiv.org/abs/1705.08790
    """

    def __init__(self, per_image=False, ignore=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        self.per_image = per_image

    def forward(self, logits, targets):
        """Forward propagation method for the Lovasz loss.

        Args:
            logits: [bs; num_classes; ...]
            targets: [bs; num_classes; ...]

        @TODO: Docs. Contribution is welcome.
        """
        losses = [_lovasz_hinge(logits[:, i, ...], targets[:, i, ...], per_image=self.per_image, ignore=self.ignore) for i in range(logits.shape[1])]
        loss = torch.mean(torch.stack(losses))
        return loss


_EPS = 1e-08


def _create_margin_mask(labels: 'torch.Tensor') ->torch.Tensor:
    equal_labels_mask = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))
    marign_mask = 2 * equal_labels_mask.float() - 1
    return marign_mask


def _skip_labels_mask(labels: 'torch.Tensor', skip_labels: 'Union[int, List[int]]') ->torch.Tensor:
    skip_labels = torch.tensor(skip_labels, dtype=labels.dtype, device=labels.device).reshape(-1)
    skip_condition = (labels.unsqueeze(-1) == skip_labels).any(-1)
    skip_mask = ~(skip_condition.unsqueeze(-1) & skip_condition.unsqueeze(0))
    return skip_mask


def euclidean_distance(x: 'torch.Tensor', y: 'torch.Tensor'=None) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    x_norm = (x ** 2).sum(1).unsqueeze(1)
    if y is not None:
        y_norm = (y ** 2).sum(1).unsqueeze(0)
    else:
        y = x
        y_norm = x_norm.t()
    dist = x_norm + y_norm - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))
    dist.clamp_min_(0.0)
    return dist


def margin_loss(embeddings: 'torch.Tensor', labels: 'torch.Tensor', alpha: 'float'=0.2, beta: 'float'=1.0, skip_labels: 'Union[int, List[int]]'=-1) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    embeddings = F.normalize(embeddings, p=2.0, dim=1)
    distances = euclidean_distance(embeddings, embeddings)
    margin_mask = _create_margin_mask(labels)
    skip_mask = _skip_labels_mask(labels, skip_labels).float()
    loss = torch.mul(skip_mask, F.relu(alpha + torch.mul(margin_mask, torch.sub(distances, beta))))
    return loss.sum() / (skip_mask.sum() + _EPS)


class MarginLoss(nn.Module):
    """Margin loss criterion"""

    def __init__(self, alpha: 'float'=0.2, beta: 'float'=1.0, skip_labels: 'Union[int, List[int]]'=-1):
        """
        Margin loss constructor.

        Args:
            alpha: alpha
            beta: beta
            skip_labels (int or List[int]): labels to skip
        """
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.skip_labels = skip_labels

    def forward(self, embeddings: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """
        Forward method for the margin loss.

        Args:
            embeddings: tensor with embeddings
            targets: tensor with target labels

        Returns:
            computed loss
        """
        return margin_loss(embeddings, targets, alpha=self.alpha, beta=self.beta, skip_labels=self.skip_labels)


class NTXentLoss(nn.Module):
    """A Contrastive embedding loss.

    It has been proposed in `A Simple Framework
    for Contrastive Learning of Visual Representations`_.

    Example:

    .. code-block:: python

        import torch
        from torch.nn import functional as F
        from catalyst.contrib import NTXentLoss

        embeddings_left = F.normalize(torch.rand(256, 64, requires_grad=True))
        embeddings_right = F.normalize(torch.rand(256, 64, requires_grad=True))
        criterion = NTXentLoss(tau = 0.1)
        criterion(embeddings_left, embeddings_right)

    .. _`A Simple Framework for Contrastive Learning of Visual Representations`:
        https://arxiv.org/abs/2002.05709
    """

    def __init__(self, tau: 'float', reduction: 'str'='mean') ->None:
        """

        Args:
            tau: temperature
            reduction (string, optional): specifies the reduction to apply to the output:
                ``"none"`` | ``"mean"`` | ``"sum"``.
                ``"none"``: no reduction will be applied,
                ``"mean"``: the sum of the output will be divided by the number of
                positive pairs in the output,
                ``"sum"``: the output will be summed.

        Raises:
            ValueError: if reduction is not mean, sum or none
        """
        super().__init__()
        self.tau = tau
        self.cosine_sim = nn.CosineSimilarity()
        self.reduction = reduction
        if self.reduction not in ['none', 'mean', 'sum']:
            raise ValueError(f'Reduction should be: mean, sum, none. But got - {self.reduction}!')

    def forward(self, features1: 'torch.Tensor', features2: 'torch.Tensor') ->torch.Tensor:
        """

        Args:
            features1: batch with samples features of shape
                [bs; feature_len]
            features2: batch with samples features of shape
                [bs; feature_len]

        Returns:
            torch.Tensor: NTXent loss
        """
        assert features1.shape == features2.shape, f'Invalid shape of input features: {features1.shape} and {features2.shape}'
        feature_matrix = torch.cat([features1, features2])
        feature_matrix = torch.nn.functional.normalize(feature_matrix)
        cosine_matrix = (2 - torch.cdist(feature_matrix, feature_matrix) ** 2) / 2
        exp_cosine_matrix = torch.exp(cosine_matrix / self.tau)
        exp_sim_sum = exp_cosine_matrix.sum(dim=1) - e ** (1 / self.tau)
        neg_loss = torch.log(exp_sim_sum)
        pos_loss = self.cosine_sim(features1, features2) / self.tau
        pos_loss = torch.cat([pos_loss, pos_loss])
        loss = -pos_loss + neg_loss
        if self.reduction == 'mean':
            loss = loss.mean()
        elif self.reduction == 'sum':
            loss = loss.sum()
        return loss


class Pointwise(nn.Module):
    """Base class for pointwise loss functions.

    Pointwise approaches look at a single document at a time in the loss function.
    For a single documents predict it relevance to the query in time.
    The score is independent for the order of the documents that are in the query's results.

    Input space: single document d1
    Output space: scores or relevant classes
    """

    def forward(self, score: 'torch.Tensor'):
        raise NotImplementedError()


class PairwiseLoss(nn.Module):
    """Base class for pairwise loss functions.

    Pairwise approached looks at a pair of documents at a time in the loss function.
    Given a pair of documents the algorithm try to come up with the optimal ordering
    For that pair and compare it with the ground truth. The goal for the ranker is to
    minimize the number of inversions in ranker.

    Input space: pairs of documents (d1, d2)
    Output space: preferences (yes/no) for a given doc. pair
    """

    @staticmethod
    def _assert_equal_size(positive_score: 'torch.Tensor', negative_score: 'torch.Tensor') ->None:
        if positive_score.size() != negative_score.size():
            raise ValueError(f'Shape mismatch: {positive_score.size()}, {negative_score.size()}')

    def forward(self, positive_score: 'torch.Tensor', negative_score: 'torch.Tensor'):
        raise NotImplementedError()


class ListwiseLoss(nn.Module):
    """Base class for listwise loss functions.

    Listwise approach directly looks at the entire list of documents and comes up with
    an optimal ordering for it.

    Input space: document set
    Output space: permutations - ranking of documents
    """

    @staticmethod
    def _assert_equal_size(outputs: 'torch.Tensor', targets: 'torch.Tensor') ->None:
        if outputs.size() != targets.size():
            raise ValueError(f'Shape mismatch: {outputs.size()}, {targets.size()}')

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor'):
        raise NotImplementedError()


class BPRLoss(PairwiseLoss):
    """Bayesian Personalised Ranking loss function.

    It has been proposed in `BPRLoss\\: Bayesian Personalized Ranking from Implicit Feedback`_.

    .. _BPRLoss\\: Bayesian Personalized Ranking from Implicit Feedback:
        https://arxiv.org/pdf/1205.2618.pdf

    Args:
        gamma (float): Small value to avoid division by zero. Default: ``1e-10``.

    Example:

    .. code-block:: python

        import torch
        from catalyst.contrib.losses import recsys

        pos_score = torch.randn(3, requires_grad=True)
        neg_score = torch.randn(3, requires_grad=True)

        output = recsys.BPRLoss()(pos_score, neg_score)
        output.backward()
    """

    def __init__(self, gamma=1e-10) ->None:
        super().__init__()
        self.gamma = gamma

    def forward(self, positive_score: 'torch.Tensor', negative_score: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the BPR loss.

        Args:
            positive_score: Tensor containing predictions for known positive items.
            negative_score: Tensor containing predictions for sampled negative items.

        Returns:
            computed loss
        """
        self._assert_equal_size(positive_score, negative_score)
        loss = -torch.log(self.gamma + torch.sigmoid(positive_score - negative_score))
        return loss.mean()


class LogisticLoss(PairwiseLoss):
    """Logistic loss function.

    Example:

    .. code-block:: python

        import torch
        from catalyst.contrib.losses import recsys

        pos_score = torch.randn(3, requires_grad=True)
        neg_score = torch.randn(3, requires_grad=True)

        output = recsys.LogisticLoss()(pos_score, neg_score)
        output.backward()
    """

    def __init__(self) ->None:
        super().__init__()

    def forward(self, positive_score: 'torch.Tensor', negative_score: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the logistic loss.

        Args:
            positive_score: Tensor containing predictions for known positive items.
            negative_score: Tensor containing predictions for sampled negative items.

        Returns:
            computed loss
        """
        self._assert_equal_size(positive_score, negative_score)
        positives_loss = 1.0 - torch.sigmoid(positive_score)
        negatives_loss = torch.sigmoid(negative_score)
        loss = positives_loss + negatives_loss
        return loss.mean()


class HingeLoss(PairwiseLoss):
    """Hinge loss function.

    Example:

    .. code-block:: python

        import torch
        from catalyst.contrib.losses import recsys

        pos_score = torch.randn(3, requires_grad=True)
        neg_score = torch.randn(3, requires_grad=True)

        output = recsys.HingeLoss()(pos_score, neg_score)
        output.backward()
    """

    def __init__(self) ->None:
        super().__init__()

    def forward(self, positive_score: 'torch.Tensor', negative_score: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the hinge loss.

        Args:
            positive_score: Tensor containing predictions for known positive items.
            negative_score: Tensor containing predictions for sampled negative items.

        Returns:
            computed loss
        """
        self._assert_equal_size(positive_score, negative_score)
        loss = torch.clamp(1.0 - (positive_score - negative_score), min=0.0)
        return loss.mean()


class AdaptiveHingeLoss(PairwiseLoss):
    """Adaptive hinge loss function.

    Takes a set of predictions for implicitly negative items, and selects those
    that are highest, thus sampling those negatives that are closes to violating
    the ranking implicit in the pattern of user interactions.

    Example:

    .. code-block:: python

        import torch
        from catalyst.contrib.losses import recsys

        pos_score = torch.randn(3, requires_grad=True)
        neg_scores = torch.randn(5, 3, requires_grad=True)

        output = recsys.AdaptiveHingeLoss()(pos_score, neg_scores)
        output.backward()
    """

    def __init__(self) ->None:
        super().__init__()
        self._hingeloss = HingeLoss()

    def forward(self, positive_score: 'torch.Tensor', negative_scores: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the adaptive hinge loss.

        Args:
            positive_score: Tensor containing predictions for known positive items.
            negative_scores: Iterable of tensors containing predictions for sampled negative items.
                More tensors increase the likelihood of finding ranking-violating pairs,
                but risk overfitting.

        Returns:
            computed loss
        """
        self._assert_equal_size(positive_score, negative_scores[0])
        highest_negative_score, _ = torch.max(negative_scores, 0)
        return self._hingeloss.forward(positive_score, highest_negative_score.squeeze())


class WARP(Function):
    """Autograd function of WARP loss."""

    @staticmethod
    def forward(ctx: 'nn.Module', outputs: 'torch.Tensor', targets: 'torch.Tensor', max_num_trials: 'Optional[int]'=None):
        batch_size = targets.size()[0]
        if max_num_trials is None:
            max_num_trials = targets.size()[1] - 1
        positive_indices = torch.zeros(outputs.size())
        negative_indices = torch.zeros(outputs.size())
        L = torch.zeros(outputs.size()[0])
        all_labels_idx = torch.arange(targets.size()[1])
        Y = float(targets.size()[1])
        J = torch.nonzero(targets)
        for i in range(batch_size):
            msk = torch.ones(targets.size()[1], dtype=bool)
            j = J[i, 1]
            positive_indices[i, j] = 1
            msk[j] = False
            sample_score_margin = -1
            num_trials = 0
            neg_labels_idx = all_labels_idx[msk]
            while sample_score_margin < 0 and num_trials < max_num_trials:
                neg_idx = neg_labels_idx[torch.randint(0, neg_labels_idx.size(0), (1,))]
                msk[neg_idx] = False
                neg_labels_idx = all_labels_idx[msk]
                num_trials += 1
                sample_score_margin = 1 + outputs[i, neg_idx] - outputs[i, j]
            if sample_score_margin < 0:
                continue
            else:
                loss_weight = np.log(np.floor((Y - 1) / num_trials))
                L[i] = loss_weight
                negative_indices[i, neg_idx] = 1
        loss = L * (1 - torch.sum(positive_indices * outputs, dim=1) + torch.sum(negative_indices * outputs, dim=1))
        ctx.save_for_backward(outputs, targets)
        ctx.L = L
        ctx.positive_indices = positive_indices
        ctx.negative_indices = negative_indices
        return torch.sum(loss, dim=0, keepdim=True)

    @staticmethod
    def backward(ctx, grad_output):
        outputs, targets = ctx.saved_variables
        L = Variable(torch.unsqueeze(ctx.L, 1), requires_grad=False)
        positive_indices = Variable(ctx.positive_indices, requires_grad=False)
        negative_indices = Variable(ctx.negative_indices, requires_grad=False)
        grad_input = grad_output * L * (negative_indices - positive_indices)
        return grad_input, None, None


class WARPLoss(ListwiseLoss):
    """Weighted Approximate-Rank Pairwise (WARP) loss function.

    It has been proposed in `WSABIE\\: Scaling Up To Large Vocabulary Image Annotation`_ paper.

    .. _WSABIE\\: Scaling Up To Large Vocabulary Image Annotation:
        https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37180.pdf

    WARP loss randomly sample output labels of a model, until it finds a pair
    which it knows are wrongly labelled and will then only apply an update to
    these two incorrectly labelled examples.

    Adapted from:
    https://github.com/gabrieltseng/datascience-projects/blob/master/misc/warp.py

    Args:
        max_num_trials: Number of attempts allowed to find a violating negative example.
            In practice it means that we optimize for ranks 1 to max_num_trials-1.

    Example:

    .. code-block:: python

        import torch
        from catalyst.contrib.losses import recsys

        outputs = torch.randn(5, 3, requires_grad=True)
        targets = torch.randn(5, 3, requires_grad=True)

        output = recsys.WARPLoss()(outputs, targets)
        output.backward()
    """

    def __init__(self, max_num_trials: 'Optional[int]'=None):
        super().__init__()
        self.max_num_trials = max_num_trials

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the WARP loss.

        Args:
            outputs: Iterable of tensors containing predictions for all items.
            targets: Iterable of tensors containing true labels for all items.

        Returns:
            computed loss
        """
        self._assert_equal_size(outputs, targets)
        return WARP.apply(outputs, targets, self.max_num_trials)


class RocStarLoss(PairwiseLoss):
    """Roc-star loss function.

    Smooth approximation for ROC-AUC. It has been proposed in
    `Roc-star\\: An objective function for ROC-AUC that actually works`_.

    .. _Roc-star\\: An objective function for ROC-AUC that actually works:
        https://github.com/iridiumblue/roc-star

    Adapted from:
    https://github.com/iridiumblue/roc-star/issues/2

    Args:
        delta: Param from the article. Default: ``1.0``.
        sample_size: Number of examples to take for ROC AUC approximation. Default: ``100``.
        sample_size_gamma: Number of examples to take for Gamma parameter approximation.
            Default: ``1000``.
        update_gamma_each: Number of steps after which to recompute gamma value.
            Default: ``50``.

    Example:

        .. code-block:: python

            import torch
            from catalyst.contrib.losses import recsys

            outputs = torch.randn(5, 1, requires_grad=True)
            targets = torch.randn(5, 1, requires_grad=True)

            output = recsys.RocStarLoss()(outputs, targets)
            output.backward()
    """

    def __init__(self, delta: 'float'=1.0, sample_size: 'int'=100, sample_size_gamma: 'int'=1000, update_gamma_each: 'int'=50):
        super().__init__()
        self.delta = delta
        self.sample_size = sample_size
        self.sample_size_gamma = sample_size_gamma
        self.update_gamma_each = update_gamma_each
        self.steps = 0
        self.gamma = None
        size = max(sample_size, sample_size_gamma)
        self.outputs_history = torch.rand((size + 2, 1))
        self.targets_history = torch.cat((torch.randint(2, (size, 1)), torch.LongTensor([[0], [1]])))

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Forward propagation method for the roc-star loss.

        Args:
            outputs: Tensor of model predictions in [0, 1] range. Shape ``(B x 1)``.
            targets: Tensor of true labels in {0, 1}. Shape ``(B x 1)``.

        Returns:
            computed loss
        """
        self._assert_equal_size(outputs, targets)
        if torch.sum(targets) == 0 or torch.sum(targets) == targets.shape[0]:
            return torch.sum(outputs) * 1e-08
        if self.steps % self.update_gamma_each == 0:
            self._update_gamma()
        self.steps += 1
        positive = outputs[targets > 0]
        negative = outputs[targets < 1]
        outputs_history = self.outputs_history[-self.sample_size:]
        targets_history = self.targets_history[-self.sample_size:]
        positive_history = outputs_history[targets_history > 0]
        negative_history = outputs_history[targets_history < 1]
        if positive.size(0) > 0:
            diff = negative_history.view(1, -1) + self.gamma - positive.view(-1, 1)
            loss_positive = nn.functional.relu(diff ** 2).mean()
        else:
            loss_positive = 0
        if negative.size(0) > 0:
            diff = negative.view(1, -1) + self.gamma - positive_history.view(-1, 1)
            loss_negative = nn.functional.relu(diff ** 2).mean()
        else:
            loss_negative = 0
        loss = loss_negative + loss_positive
        batch_size = outputs.size(0)
        self.outputs_history = torch.cat((self.outputs_history[batch_size:], outputs.clone().detach()))
        self.targets_history = torch.cat((self.targets_history[batch_size:], targets.clone().detach()))
        return loss

    def _update_gamma(self):
        outputs = self.outputs_history[-self.sample_size_gamma:]
        targets = self.targets_history[-self.sample_size_gamma:]
        positive = outputs[targets > 0]
        negative = outputs[targets < 1]
        diff = positive.view(-1, 1) - negative.view(1, -1)
        AUC = (diff > 0).type(torch.float).mean()
        num_wrong_ordered = (1 - AUC) * diff.flatten().size(0)
        correct_ordered = diff[diff > 0].flatten().sort().values
        idx = min(int(num_wrong_ordered * self.delta), len(correct_ordered) - 1)
        if idx >= 0:
            self.gamma = correct_ordered[idx]


class HuberLossV0(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, clip_delta=1.0, reduction='mean'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.clip_delta = clip_delta
        self.reduction = reduction or 'none'

    def forward(self, output: 'torch.Tensor', target: 'torch.Tensor', weights=None) ->torch.Tensor:
        """@TODO: Docs. Contribution is welcome."""
        diff = target - output
        diff_abs = torch.abs(diff)
        quadratic_part = torch.clamp(diff_abs, max=self.clip_delta)
        linear_part = diff_abs - quadratic_part
        loss = 0.5 * quadratic_part ** 2 + self.clip_delta * linear_part
        if weights is not None:
            loss = torch.mean(loss * weights, dim=1)
        else:
            loss = torch.mean(loss, dim=1)
        if self.reduction == 'mean':
            loss = torch.mean(loss)
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


def _ce_with_logits(logits, target):
    """Returns cross entropy for giving logits"""
    return torch.sum(-target * torch.log_softmax(logits, -1), -1)


class CategoricalRegressionLoss(nn.Module):
    """CategoricalRegressionLoss"""

    def __init__(self, num_atoms: 'int', v_min: 'int', v_max: 'int'):
        super().__init__()
        self.num_atoms = num_atoms
        self.v_min = v_min
        self.v_max = v_max
        self.delta_z = (self.v_max - self.v_min) / (self.num_atoms - 1)
        self.z = torch.linspace(start=self.v_min, end=self.v_max, steps=self.num_atoms)

    def forward(self, logits_t: 'torch.Tensor', logits_tp1: 'torch.Tensor', atoms_target_t: 'torch.Tensor') ->torch.Tensor:
        """Compute the loss

        Args:
            logits_t (torch.Tensor): predicted atoms at step T.
                shape: [bs; num_atoms]
            logits_tp1 (torch.Tensor): predicted atoms at step T+1.
                shape: [bs; num_atoms]
            atoms_target_t (torch.Tensor): target atoms at step T.
                shape: [bs; num_atoms]

        Returns:
            torch.Tensor: computed loss
        """
        probs_tp1 = torch.softmax(logits_tp1, dim=-1)
        tz = torch.clamp(atoms_target_t, self.v_min, self.v_max)
        tz_z = torch.abs(tz[:, None, :] - self.z[None, :, None])
        tz_z = torch.clamp(1.0 - tz_z / self.delta_z, 0.0, 1.0)
        probs_target_t = torch.einsum('bij,bj->bi', (tz_z, probs_tp1)).detach()
        loss = _ce_with_logits(logits_t, probs_target_t).mean()
        return loss


class QuantileRegressionLoss(nn.Module):
    """QuantileRegressionLoss"""

    def __init__(self, num_atoms: 'int'=51, clip_delta: 'float'=1.0):
        """Init."""
        super().__init__()
        self.num_atoms = num_atoms
        tau_min = 1 / (2 * self.num_atoms)
        tau_max = 1 - tau_min
        self.tau = torch.linspace(start=tau_min, end=tau_max, steps=self.num_atoms)
        self.criterion = HuberLossV0(clip_delta=clip_delta)

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Compute the loss.

        Args:
            outputs (torch.Tensor): predicted atoms, shape: [bs; num_atoms]
            targets (torch.Tensor): target atoms, shape: [bs; num_atoms]

        Returns:
            torch.Tensor: computed loss
        """
        atoms_diff = targets[:, None, :] - outputs[:, :, None]
        delta_atoms_diff = atoms_diff.lt(0).detach()
        huber_weights = torch.abs(self.tau[None, :, None] - delta_atoms_diff) / self.num_atoms
        loss = self.criterion(outputs[:, :, None], targets[:, None, :], huber_weights).mean()
        return loss


class RSquareLoss(nn.Module):
    """RSquareLoss"""

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Compute the loss.

        Args:
            outputs (torch.Tensor): model outputs
            targets (torch.Tensor): targets

        Returns:
            torch.Tensor: computed loss
        """
        var_y = torch.var(targets, unbiased=False)
        return 1.0 - F.mse_loss(outputs, targets, reduction='mean') / var_y


class SmoothingDiceLoss(nn.Module):
    """
    The Smoothing Dice loss.
    ``SmoothingDiceloss = 1 - smoothing dice score``
    ``smoothing dice score = 2 * intersection / (|outputs|^2 + |targets|^2)``
    Criterion was inspired by https://arxiv.org/abs/1606.04797

    Examples:
        >>> import torch
        >>> from catalyst.contrib.losses import SmoothingDiceLoss
        >>> targets = torch.abs(torch.randn((1, 2, 3, 3), ))
        >>> prediction = torch.abs(torch.randn((1, 2, 3, 3)))
        >>> criterion = SmoothingDiceLoss()
        >>> loss = criterion(prediction, targets)
        >>> print(loss)
    """

    def __init__(self, class_dim: 'int'=1, mode: 'str'='macro', weights: 'List[float]'=None, eps: 'float'=1e-07):
        """
        Args:
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated per-class and than are averaged over all classes.
                If mode='weighted', metric are calculated per-class and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        self.class_dim = class_dim
        self.mode = mode
        if self.mode == 'weighted':
            assert weights is not None
            self.weights = torch.Tensor(weights)
        self.eps = eps

    def _get_sum_per_class(self, outputs_shape: 'List[int]') ->Callable[[torch.Tensor], torch.Tensor]:
        """
        Creates a channel summing function

        Args:
            outputs_shape: shape of output tensor

        Returns:
             function that sums tensors over all channels except the
             classification

        """
        n_dims = len(outputs_shape)
        dims = list(range(n_dims))
        if self.class_dim < 0:
            self.class_dim = n_dims + self.class_dim
        dims.pop(self.class_dim)
        sum_per_class = partial(torch.sum, dim=dims)
        return sum_per_class

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``logits`` and ``target`` tensors."""
        sum_per_class = self._get_sum_per_class(outputs.shape)
        outputs_2 = outputs ** 2
        targets_2 = targets ** 2
        tp = sum_per_class(outputs * targets)
        outputs_per_class = sum_per_class(outputs_2)
        targets_per_class = sum_per_class(targets_2)
        if self.mode == 'micro':
            tp = tp.sum()
            outputs_per_class = outputs_per_class.sum()
            targets_per_class = targets_per_class.sum()
        smoothing_dice = 2 * tp / (outputs_per_class + targets_per_class + self.eps)
        if self.mode == 'macro':
            smoothing_dice = smoothing_dice.mean()
        if self.mode == 'weighted':
            device = smoothing_dice.device
            smoothing_dice = (smoothing_dice * self.weights).sum()
        return 1 - smoothing_dice


class SupervisedContrastiveLoss(nn.Module):
    """A Contrastive embedding loss that uses targets.

    It has been proposed in `Supervised Contrastive Learning`_.

    .. _`Supervised Contrastive Learning`:
        https://arxiv.org/pdf/2004.11362.pdf
    """

    def __init__(self, tau: 'float', reduction: 'str'='mean', pos_aggregation='in') ->None:
        """
        Args:
            tau: temperature
            reduction: specifies the reduction to apply to the output:
                ``"none"`` | ``"mean"`` | ``"sum"``.
                ``"none"``: no reduction will be applied,
                ``"mean"``: the sum of the output will be divided by the number of
                positive pairs in the output,
                ``"sum"``: the output will be summed.
            pos_aggregation: specifies the place of positive pairs aggregation:
                ``"in"`` | ``"out"``.
                ``"in"``: maximization of log(average positive exponentiate similarity)
                ``"out"``: maximization of average positive similarity

        Raises:
            ValueError: if reduction is not mean, sum or none
            ValueError: if positive aggregation is not in or out
        """
        super().__init__()
        self.tau = tau
        self.self_similarity = 1 / self.tau
        self.exp_self_similarity = e ** (1 / self.tau)
        self.reduction = reduction
        self.pos_aggregation = pos_aggregation
        if self.reduction not in ['none', 'mean', 'sum']:
            raise ValueError(f'Reduction should be: mean, sum, none. But got - {self.reduction}!')
        if self.pos_aggregation not in ['in', 'out']:
            raise ValueError(f'Positive aggregation should be: in or out.But got - {self.pos_aggregation}!')

    def forward(self, features: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            features: [bs; feature_len]
            targets: [bs]

        Returns:
            computed loss
        """
        features = torch.nn.functional.normalize(features)
        cosine_matrix = (2 - torch.cdist(features, features) ** 2) / 2
        exp_cosine_matrix = torch.exp(cosine_matrix / self.tau)
        pos_place = targets.repeat(targets.shape[0], 1) == targets.reshape(targets.shape[0], 1)
        number_of_positives = pos_place.sum(dim=1) - 1
        assert (number_of_positives == 0).sum().item() == 0, 'There must be at least one positive example for each sample!'
        if self.pos_aggregation == 'in':
            pos_loss = (exp_cosine_matrix * pos_place).sum(dim=1) - self.exp_self_similarity
            pos_loss = torch.log(pos_loss) - torch.log(number_of_positives.float())
        elif self.pos_aggregation == 'out':
            pos_loss = ((torch.log(exp_cosine_matrix) * pos_place).sum(dim=1) - self.self_similarity) / number_of_positives
        exp_sim_sum = exp_cosine_matrix.sum(dim=1) - self.exp_self_similarity
        neg_loss = torch.log(exp_sim_sum)
        loss = -pos_loss + neg_loss
        if self.reduction == 'mean':
            loss = loss.mean()
        elif self.reduction == 'sum':
            loss = loss.sum()
        return loss


def _trevsky(tp: 'torch.Tensor', fp: 'torch.Tensor', fn: 'torch.Tensor', alpha: 'float', beta: 'float', eps: 'float'=1e-07) ->torch.Tensor:
    union = tp + fp + fn
    score = (tp + eps * (union == 0).float()) / (tp + fp * beta + fn * alpha + eps)
    return score


def trevsky(outputs: 'torch.Tensor', targets: 'torch.Tensor', alpha: 'float', beta: 'Optional[float]'=None, class_dim: 'int'=1, threshold: 'float'=None, mode: 'str'='per-class', weights: 'Optional[List[float]]'=None, eps: 'float'=1e-07) ->torch.Tensor:
    """
    Computes the trevsky score,
    trevsky score = tp / (tp + fp * beta + fn * alpha)

    Args:
        outputs: [N; K; ...] tensor that for each of the N examples
            indicates the probability of the example belonging to each of
            the K classes, according to the model.
        targets:  binary [N; K; ...] tensor that encodes which of the K
            classes are associated with the N-th input
        alpha: false negative coefficient, bigger alpha bigger penalty for
            false negative. Must be in (0, 1)
        beta: false positive coefficient, bigger alpha bigger penalty for false
            positive. Must be in (0, 1), if None beta = (1 - alpha)
        class_dim: indicates class dimention (K) for
            ``outputs`` and ``targets`` tensors (default = 1)
        threshold: threshold for outputs binarization
        mode: class summation strategy. Must be one of ['micro', 'macro',
            'weighted', 'per-class']. If mode='micro', classes are ignored,
            and metric are calculated generally. If mode='macro', metric are
            calculated per-class and than are averaged over all classes. If
            mode='weighted', metric are calculated per-class and than summed
            over all classes with weights. If mode='per-class', metric are
            calculated separately for all classes
        weights: class weights(for mode="weighted")
        eps: epsilon to avoid zero division

    Returns:
        Trevsky score for each class(if mode='weighted') or aggregated score

    Example:

    .. code-block:: python

        import torch
        from catalyst import metrics

        size = 4
        half_size = size // 2
        shape = (1, 1, size, size)
        empty = torch.zeros(shape)
        full = torch.ones(shape)
        left = torch.ones(shape)
        left[:, :, :, half_size:] = 0
        right = torch.ones(shape)
        right[:, :, :, :half_size] = 0
        top_left = torch.zeros(shape)
        top_left[:, :, :half_size, :half_size] = 1
        pred = torch.cat([empty, left, empty, full, left, top_left], dim=1)
        targets = torch.cat([full, right, empty, full, left, left], dim=1)

        metrics.trevsky(
            outputs=pred,
            targets=targets,
            alpha=0.2,
            class_dim=1,
            threshold=0.5,
            mode="per-class"
        )
        # tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.8333])

        metrics.trevsky(
            outputs=pred,
            targets=targets,
            alpha=0.2,
            class_dim=1,
            threshold=0.5,
            mode="macro"
        )
        # tensor(0.6389)

        metrics.trevsky(
            outputs=pred,
            targets=targets,
            alpha=0.2,
            class_dim=1,
            threshold=0.5,
            mode="micro"
        )
        # tensor(0.7000)
    """
    if beta is None:
        assert 0 < alpha < 1, 'if beta=None, alpha must be in (0, 1)'
        beta = 1 - alpha
    metric_fn = partial(_trevsky, alpha=alpha, beta=beta, eps=eps)
    score = _get_region_based_metrics(outputs=outputs, targets=targets, metric_fn=metric_fn, class_dim=class_dim, threshold=threshold, mode=mode, weights=weights)
    return score


class TrevskyLoss(nn.Module):
    """The trevsky loss.
    TrevskyIndex = TP / (TP + alpha * FN + betta * FP)
    TrevskyLoss = 1 - TrevskyIndex
    """

    def __init__(self, alpha: 'float', beta: 'Optional[float]'=None, class_dim: 'int'=1, mode: 'str'='macro', weights: 'List[float]'=None, eps: 'float'=1e-07):
        """
        Args:
            alpha: false negative coefficient, bigger alpha bigger penalty for
                false negative. Must be in (0, 1)
            beta: false positive coefficient, bigger alpha bigger penalty for
                false positive. Must be in (0, 1), if None beta = (1 - alpha)
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated separately and than are averaged over all classes.
                If mode='weighted', metric are calculated separately and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        assert mode in ['micro', 'macro', 'weighted']
        self.loss_fn = partial(trevsky, eps=eps, alpha=alpha, beta=beta, class_dim=class_dim, threshold=None, mode=mode, weights=weights)

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``logits`` and ``target`` tensors."""
        trevsky_score = self.loss_fn(outputs, targets)
        return 1 - trevsky_score


class FocalTrevskyLoss(nn.Module):
    """The focal trevsky loss.
    TrevskyIndex = TP / (TP + alpha * FN + betta * FP)
    FocalTrevskyLoss = (1 - TrevskyIndex)^gamma
    Node: focal will use per image, so loss will pay more attention on complicated images
    """

    def __init__(self, alpha: 'float', beta: 'Optional[float]'=None, gamma: 'float'=4 / 3, class_dim: 'int'=1, mode: 'str'='macro', weights: 'List[float]'=None, eps: 'float'=1e-07):
        """
        Args:
            alpha: false negative coefficient, bigger alpha bigger penalty for
                false negative. Must be in (0, 1)
            beta: false positive coefficient, bigger alpha bigger penalty for
                false positive. Must be in (0, 1), if None beta = (1 - alpha)
            gamma: focal coefficient. It determines how much the weight of
            simple examples is reduced.
            class_dim: indicates class dimention (K) for
                ``outputs`` and ``targets`` tensors (default = 1)
            mode: class summation strategy. Must be one of ['micro', 'macro',
                'weighted']. If mode='micro', classes are ignored, and metric
                are calculated generally. If mode='macro', metric are
                calculated separately and than are averaged over all classes.
                If mode='weighted', metric are calculated separately and than
                summed over all classes with weights.
            weights: class weights(for mode="weighted")
            eps: epsilon to avoid zero division
        """
        super().__init__()
        self.gamma = gamma
        self.trevsky_loss = TrevskyLoss(alpha=alpha, beta=beta, class_dim=class_dim, mode=mode, weights=weights, eps=eps)

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """Calculates loss between ``logits`` and ``target`` tensors."""
        loss = 0
        batch_size = len(outputs)
        for output_sample, target_sample in zip(outputs, targets):
            output_sample = torch.unsqueeze(output_sample, dim=0)
            target_sample = torch.unsqueeze(target_sample, dim=0)
            sample_loss = self.trevsky_loss(output_sample, target_sample)
            loss += sample_loss ** self.gamma
        loss = loss / batch_size
        return loss


TORCH_BOOL = torch.bool if torch.__version__ > '1.1.0' else torch.ByteTensor


class TripletLoss(nn.Module):
    """Triplet loss with hard positive/negative mining.

    Adapted from: https://github.com/NegatioN/OnlineMiningTripletLoss
    """

    def __init__(self, margin: 'float'=0.3):
        """
        Args:
            margin: margin for triplet
        """
        super().__init__()
        self.margin = margin
        self.ranking_loss = nn.MarginRankingLoss(margin=margin)

    def _pairwise_distances(self, embeddings, squared=False):
        """Compute the 2D matrix of distances between all the embeddings.

        Args:
            embeddings: tensor of shape (batch_size, embed_dim)
            squared: if true, output is the pairwise squared euclidean
                distance matrix. If false, output is the pairwise euclidean
                distance matrix

        Returns:
            torch.Tensor: pairwise matrix of size (batch_size, batch_size)
        """
        square = torch.mm(embeddings, embeddings.t())
        diag = torch.diag(square)
        distances = diag.view(-1, 1) - 2.0 * square + diag.view(1, -1)
        distances[distances < 0] = 0
        if not squared:
            mask = distances.eq(0).float()
            distances = distances + mask * 1e-16
            distances = (1.0 - mask) * torch.sqrt(distances)
        return distances

    def _get_anchor_positive_triplet_mask(self, labels):
        """
        Return a 2D mask where mask[a, p] is True
        if a and p are distinct and have same label.

        Args:
            labels: tf.int32 `Tensor` with shape [batch_size]

        Returns:
            torch.Tensor: mask with shape [batch_size, batch_size]
        """
        indices_equal = torch.eye(labels.size(0)).type(torch.bool)
        indices_equal = indices_equal
        indices_equal = indices_equal.type(TORCH_BOOL)
        indices_not_equal = ~indices_equal
        labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
        return labels_equal & indices_not_equal

    def _get_anchor_negative_triplet_mask(self, labels):
        """Return 2D mask where mask[a, n] is True if a and n have same label.

        Args:
            labels: tf.int32 `Tensor` with shape [batch_size]

        Returns:
            torch.Tensor: mask with shape [batch_size, batch_size]
        """
        return ~(labels.unsqueeze(0) == labels.unsqueeze(1))

    def _batch_hard_triplet_loss(self, embeddings, labels, margin, squared=True):
        """
        Build the triplet loss over a batch of embeddings.
        For each anchor, we get the hardest positive and
        hardest negative to form a triplet.

        Args:
            labels: labels of the batch, of size (batch_size)
            embeddings: tensor of shape (batch_size, embed_dim)
            margin: margin for triplet loss
            squared: Boolean. If true, output is the pairwise squared
                     euclidean distance matrix. If false, output is the
                     pairwise euclidean distance matrix.

        Returns:
            torch.Tensor: scalar tensor containing the triplet loss
        """
        pairwise_dist = self._pairwise_distances(embeddings, squared=squared)
        mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels).float()
        anchor_positive_dist = mask_anchor_positive * pairwise_dist
        hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)
        mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels).float()
        max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)
        anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)
        hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)
        tl = hardest_positive_dist - hardest_negative_dist + margin
        tl[tl < 0] = 0
        loss = tl.mean()
        return loss

    def forward(self, embeddings, targets):
        """Forward propagation method for the triplet loss.

        Args:
            embeddings: tensor of shape (batch_size, embed_dim)
            targets: labels of the batch, of size (batch_size)

        Returns:
            torch.Tensor: scalar tensor containing the triplet loss
        """
        return self._batch_hard_triplet_loss(embeddings, targets, self.margin)


def create_negative_mask(labels: 'torch.Tensor', neg_label: 'int'=-1) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    neg_labels = torch.ge(labels, neg_label)
    pos_labels = ~neg_labels
    i_less_neg = pos_labels.unsqueeze(1).unsqueeze(2)
    j_less_neg = pos_labels.unsqueeze(1).unsqueeze(0)
    k_less_neg = pos_labels.unsqueeze(0).unsqueeze(0)
    anchors = labels.unsqueeze(1).unsqueeze(2)
    negatives = labels.unsqueeze(0).unsqueeze(0)
    k_equal = torch.eq(anchors + neg_label, negatives)
    k_less_or_equal = k_equal | k_less_neg
    mask = i_less_neg & j_less_neg & k_less_or_equal
    return mask


def batch_all(labels: 'torch.Tensor', exclude_negatives: 'bool'=True) ->torch.Tensor:
    """Create a 3D mask of all possible triplets.
    @TODO: Docs. Contribution is welcome.
    """
    batch_size = labels.size(0)
    indices_equal = torch.eye(batch_size, device=labels.device).type(torch.bool)
    indices_not_equal = ~indices_equal
    i_not_equal_j = indices_not_equal.unsqueeze(2)
    i_not_equal_k = indices_not_equal.unsqueeze(1)
    j_not_equal_k = indices_not_equal.unsqueeze(0)
    distinct_indices = i_not_equal_j & i_not_equal_k & j_not_equal_k
    label_equal = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))
    yi_equal_yj = label_equal.unsqueeze(2)
    yi_equal_yk = label_equal.unsqueeze(1)
    yi_not_equal_yk = ~yi_equal_yk
    valid_labels = yi_equal_yj & yi_not_equal_yk
    mask = distinct_indices & valid_labels
    if exclude_negatives:
        mask = mask & create_negative_mask(labels)
    return mask.float()


def cosine_distance(x: 'torch.Tensor', z: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
    """Calculate cosine distance between x and z.
    @TODO: Docs. Contribution is welcome.
    """
    x = F.normalize(x)
    if z is not None:
        z = F.normalize(z)
    else:
        z = x.clone()
    return torch.sub(1, torch.mm(x, z.transpose(0, 1)))


def triplet_loss(embeddings: 'torch.Tensor', labels: 'torch.Tensor', margin: 'float'=0.3) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    cosine_dists = cosine_distance(embeddings)
    mask = batch_all(labels)
    anchor_positive_dist = cosine_dists.unsqueeze(2)
    anchor_negative_dist = cosine_dists.unsqueeze(1)
    triplet_loss_value = F.relu(anchor_positive_dist - anchor_negative_dist + margin)
    triplet_loss_value = torch.mul(triplet_loss_value, mask)
    num_positive_triplets = torch.gt(triplet_loss_value, _EPS).sum().float()
    triplet_loss_value = triplet_loss_value.sum() / (num_positive_triplets + _EPS)
    return triplet_loss_value


class TripletLossV2(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, margin=0.3):
        """
        Args:
            margin: margin for triplet.
        """
        super().__init__()
        self.margin = margin

    def forward(self, embeddings, targets):
        """@TODO: Docs. Contribution is welcome."""
        return triplet_loss(embeddings, targets, margin=self.margin)


class TripletPairwiseEmbeddingLoss(nn.Module):
    """TripletPairwiseEmbeddingLoss – proof of concept criterion.

    Still work in progress.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, margin: 'float'=0.3, reduction: 'str'='mean'):
        """
        Args:
            margin: margin parameter
            reduction: criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, embeddings_pred, embeddings_true):
        """
        Work in progress.

        Args:
            embeddings_pred: predicted embeddings
                with shape [batch_size, embedding_size]
            embeddings_true: true embeddings
                with shape [batch_size, embedding_size]

        Returns:
            torch.Tensor: loss
        """
        device = embeddings_pred.device
        pairwise_similarity = torch.einsum('se,ae->sa', embeddings_pred, embeddings_true)
        bs = embeddings_pred.shape[0]
        batch_idx = torch.arange(bs, device=device)
        negative_similarity = pairwise_similarity + torch.diag(torch.full([bs], -10 ** 9, device=device))
        hard_negative_ids = negative_similarity.argmax(dim=-1)
        negative_similarities = pairwise_similarity[batch_idx, hard_negative_ids]
        positive_similarities = pairwise_similarity[batch_idx, batch_idx]
        loss = torch.relu(self.margin - positive_similarities + negative_similarities)
        if self.reduction == 'mean':
            loss = torch.sum(loss) / bs
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


def convert_labels2list(labels: 'Union[Tensor, List[int]]') ->List[int]:
    """
    This function allows to work with 2 types of indexing:
    using a integer tensor and a list of indices.

    Args:
        labels: labels of batch samples

    Returns:
        labels of batch samples in the aligned format

    Raises:
        TypeError: if type of input labels is not tensor and list
    """
    if isinstance(labels, Tensor):
        labels = labels.squeeze()
        assert len(labels.shape) == 1 and labels.dtype in [short, tint, long], 'Labels cannot be interpreted as indices.'
        labels_list = labels.tolist()
    elif isinstance(labels, list):
        labels_list = labels.copy()
    else:
        raise TypeError(f'Unexpected type of labels: {type(labels)}).')
    return labels_list


class TripletMarginLossWithSampler(nn.Module):
    """
    This class combines in-batch sampling of triplets and
    default TripletMargingLoss from PyTorch.
    """

    def __init__(self, margin: 'float', sampler_inbatch: "'IInbatchTripletSampler'"):
        """
        Args:
            margin: margin value
            sampler_inbatch: sampler for forming triplets inside the batch
        """
        super().__init__()
        self._sampler_inbatch = sampler_inbatch
        self._triplet_margin_loss = TripletMarginLoss(margin=margin)

    def forward(self, features: 'Tensor', labels: 'Union[Tensor, List[int]]') ->Tensor:
        """
        Args:
            features: features with shape [batch_size, features_dim]
            labels: labels of samples having batch_size elements

        Returns: loss value

        """
        labels_list = convert_labels2list(labels)
        features_anchor, features_positive, features_negative = self._sampler_inbatch.sample(features=features, labels=labels_list)
        loss = self._triplet_margin_loss(anchor=features_anchor, positive=features_positive, negative=features_negative)
        return loss


def wing_loss(outputs: 'torch.Tensor', targets: 'torch.Tensor', width: 'int'=5, curvature: 'float'=0.5, reduction: 'str'='mean') ->torch.Tensor:
    """The Wing loss.

    It has been proposed in `Wing Loss for Robust Facial Landmark Localisation
    with Convolutional Neural Networks`_.

    Args:
        @TODO: Docs. Contribution is welcome.

    Adapted from:
    https://github.com/BloodAxe/pytorch-toolbelt (MIT License)

    .. _Wing Loss for Robust Facial Landmark Localisation with Convolutional
        Neural Networks: https://arxiv.org/abs/1711.06753
    """
    diff_abs = (targets - outputs).abs()
    loss = diff_abs.clone()
    idx_smaller = diff_abs < width
    idx_bigger = diff_abs >= width
    loss[idx_smaller] = width * torch.log(1 + diff_abs[idx_smaller] / curvature)
    c = width - width * math.log(1 + width / curvature)
    loss[idx_bigger] = loss[idx_bigger] - c
    if reduction == 'sum':
        loss = loss.sum()
    if reduction == 'mean':
        loss = loss.mean()
    return loss


class WingLoss(nn.Module):
    """Creates a criterion that optimizes a Wing loss.

    It has been proposed in `Wing Loss for Robust Facial Landmark Localisation
    with Convolutional Neural Networks`_.

    Adapted from:
    https://github.com/BloodAxe/pytorch-toolbelt

    .. _Wing Loss for Robust Facial Landmark Localisation with Convolutional
        Neural Networks: https://arxiv.org/abs/1711.06753
    """

    def __init__(self, width: 'int'=5, curvature: 'float'=0.5, reduction: 'str'='mean'):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        super().__init__()
        self.loss_fn = partial(wing_loss, width=width, curvature=curvature, reduction=reduction)

    def forward(self, outputs: 'torch.Tensor', targets: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        loss = self.loss_fn(outputs, targets)
        return loss


class MnistSimpleNet(nn.Module):
    """Simple MNIST convolutional network for test purposes."""

    def __init__(self, out_features: 'int', normalize: 'bool'=True):
        """
        Args:
            out_features: size of the output tensor
            normalize: boolean flag to add normalize layer
        """
        super().__init__()
        layers = [nn.Conv2d(1, 32, 3, 1), nn.ReLU(), nn.Conv2d(32, 64, 3, 1), nn.ReLU(), nn.MaxPool2d(2), Flatten(), nn.Linear(9216, 128), nn.ReLU(), nn.Linear(128, out_features)]
        if normalize:
            layers.append(Normalize())
        self._net = nn.Sequential(*layers)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            x: input 1d image tensor with the size of [28 x 28]

        Returns:
            extracted features
        """
        return self._net(x)


class MnistBatchNormNet(nn.Module):
    """Simple MNIST convolutional network with batch norm layers for test purposes."""

    def __init__(self, out_features: 'int'):
        """
        Args:
            out_features: size of the output tensor
        """
        super().__init__()
        layers = [nn.Conv2d(1, 32, 3, 1), nn.LeakyReLU(), nn.BatchNorm2d(32), nn.Conv2d(32, 64, 3, 1), nn.LeakyReLU(), nn.MaxPool2d(2), Flatten(), nn.BatchNorm1d(9216), nn.Linear(9216, 128), nn.LeakyReLU(), nn.Linear(128, out_features), nn.BatchNorm1d(out_features)]
        self._net = nn.Sequential(*layers)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        Args:
            x: input 1d image tensor with the size of [28 x 28]

        Returns:
            extracted features
        """
        return self._net(x)


class ResnetEncoder(nn.Module):
    """Specifies ResNet encoders for classification network.

    Examples:
        >>> encoders = ResnetEncoder(
        >>>    arch="resnet18",
        >>>    pretrained=False,
        >>>    state_dict="/model/path/resnet18-5c106cde.pth"
        >>> )
    """

    def __init__(self, arch: 'str'='resnet18', pretrained: 'bool'=True, frozen: 'bool'=True, pooling: 'str'=None, pooling_kwargs: 'dict'=None, cut_layers: 'int'=2, state_dict: 'Union[dict, str, Path]'=None):
        """
        Args:
            arch: Name for resnet. Have to be one of
                resnet18, resnet34, resnet50, resnet101, resnet152
            pretrained: If True, returns a model pre-trained on ImageNet
            frozen: If frozen, sets requires_grad to False
            pooling: pooling
            pooling_kwargs: params for pooling
            state_dict (Union[dict, str, Path]): Path to ``torch.Model``
                or a dict containing parameters and persistent buffers.
        """
        super().__init__()
        resnet = torchvision.models.__dict__[arch](pretrained=pretrained)
        if state_dict is not None:
            if isinstance(state_dict, (Path, str)):
                state_dict = torch.load(str(state_dict))
            resnet.load_state_dict(state_dict)
        modules = list(resnet.children())[:-cut_layers]
        if frozen:
            for module in modules:
                utils.set_requires_grad(module, requires_grad=False)
        if pooling is not None:
            pooling_kwargs = pooling_kwargs or {}
            pooling_layer_fn = REGISTRY.get(pooling)
            pooling_layer = pooling_layer_fn(in_features=resnet.fc.in_features, **pooling_kwargs) if 'attn' in pooling.lower() else pooling_layer_fn(**pooling_kwargs)
            modules += [pooling_layer]
            if hasattr(pooling_layer, 'out_features'):
                out_features = pooling_layer.out_features(in_features=resnet.fc.in_features)
            else:
                out_features = None
        else:
            out_features = resnet.fc.in_features
        modules += [Flatten()]
        self.out_features = out_features
        self.encoder = nn.Sequential(*modules)

    def forward(self, image):
        """Extract the image feature vectors."""
        features = self.encoder(image)
        return features


class ModelForwardWrapper(nn.Module):
    """Model that calls specified method instead of forward.

    Args:
        model: @TODO: docs
        method_name: @TODO: docs

    (Workaround, single method tracing is not supported)
    """

    def __init__(self, model, method_name):
        """Init"""
        super().__init__()
        self.model = model
        self.method_name = method_name

    def forward(self, *args, **kwargs):
        """Forward pass.

        Args:
            *args: some args
            **kwargs: some kwargs

        Returns:
            output: specified method output
        """
        return getattr(self.model, self.method_name)(*args, **kwargs)


class SSDCriterion(nn.Module):

    def __init__(self, num_classes, ignore_class=0):
        super().__init__()
        self.num_classes = num_classes
        self.ignore_class = ignore_class

    def _hard_negative_mining(self, cls_loss, pos):
        """Return negative indices that is 3x the number as positive indices.

        Args:
            cls_loss: (torch.Tensor) cross entropy loss between `cls_preds` and `cls_targets`.
                Expected shape [B, M] where B - batch, M - anchors.
            pos: (torch.Tensor) positive class mask.
                Expected shape [B, M] where B - batch, M - anchors.

        Return:
            (torch.Tensor) negative indices, sized [N,#anchors].
        """
        cls_loss = cls_loss * (pos.float() - 1)
        _, idx = cls_loss.sort(1)
        _, rank = idx.sort(1)
        num_neg = 3 * pos.sum(1)
        neg = rank < num_neg[:, None]
        return neg

    def forward(self, loc_preds, loc_targets, cls_preds, cls_targets):
        """Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).
        Loss:
            loss = SmoothL1Loss(loc_preds, loc_targets) + CrossEntropyLoss(cls_preds, cls_targets).

        Args:
            loc_preds: (torch.Tensor) predicted locations.
                Expected shapes - [B, M, 4] where B - batch, M - anchors.
            loc_targets: (torch.Tensor) encoded target locations.
                Expected shapes - [B, M, 4] where B - batch, M - anchors.
            cls_preds: (torch.Tensor) predicted class confidences.
                Expected shapes - [B, M, CLASS]
                where B - batch, M - anchors, CLASS - number of classes.
            cls_targets: (torch.LongTensor) encoded target labels.
                Expected shapes - [B, M] where B - batch, M - anchors.

        Returns:
            regression loss and classification loss
        """
        pos = cls_targets != self.ignore_class
        batch_size = pos.size(0)
        num_pos = pos.sum().item()
        mask = pos.unsqueeze(2).expand_as(loc_preds)
        loc_loss = F.smooth_l1_loss(loc_preds[mask], loc_targets[mask], reduction='sum')
        cls_loss = F.cross_entropy(cls_preds.view(-1, self.num_classes), cls_targets.view(-1), reduction='none')
        cls_loss = cls_loss.view(batch_size, -1)
        cls_loss[cls_targets == self.ignore_class] = 0
        neg = self._hard_negative_mining(cls_loss, pos)
        cls_loss = cls_loss[pos | neg].sum()
        loc_loss = loc_loss / num_pos
        cls_loss = cls_loss / num_pos
        return loc_loss, cls_loss


def neg_loss(pred, gt):
    """Modified focal loss. Exactly the same as CornerNet.
    Runs faster and costs a little bit more memory

    Args:
        pred (torch.Tensor): predicted center heatmaps,
            should have shapes [batch, c, h, w]
        gt (torch.Tensor): ground truth center heatmaps,
            should have shapes [batch, c, h, w]

    Returns:
        torch.Tensor with focal loss value.
    """
    pred = pred.unsqueeze(1).float()
    gt = gt.unsqueeze(1).float()
    positive_inds = gt.eq(1).float()
    negative_inds = gt.lt(1).float()
    negative_weights = torch.pow(1 - gt, 4)
    loss = 0
    positive_loss = torch.log(pred + 1e-12) * torch.pow(1 - pred, 3) * positive_inds
    negative_loss = torch.log(1 - pred + 1e-12) * torch.pow(pred, 3) * negative_weights * negative_inds
    num_pos = positive_inds.float().sum()
    positive_loss = positive_loss.sum()
    negative_loss = negative_loss.sum()
    if num_pos == 0:
        loss = loss - negative_loss
    else:
        loss = loss - (positive_loss + negative_loss) / num_pos
    return loss


class CenterNetCriterion(nn.Module):

    def __init__(self, num_classes=1, mask_loss_weight=1.0, regr_loss_weight=1.0, size_average=True):
        """
        Args:
            num_classes (int): Number of classes in model.
                Default is ``1``.
            mask_loss_weight (float): heatmap loss weight coefficient.
                Default is ``1.0``.
            regr_loss_weight (float): HW regression loss weight coefficient.
                Default is ``1.0``.
            size_average (bool): loss batch scaling.
                Default is ``True``.
        """
        super().__init__()
        self.num_classes = num_classes
        self.mask_loss_weight = mask_loss_weight
        self.regr_loss_weight = regr_loss_weight
        self.size_average = size_average

    def forward(self, predicted_heatmap, predicted_regr, target_heatmap, target_regr):
        """Compute loss for CenterNet.

        Args:
            predicted_heatmap (torch.Tensor): center heatmap prediction logits,
                expected shapes [batch, height, width, num classes].
            predicted_regr (torch.Tensor): predicted HW regression,
                expected shapes [batch, height, width, 2].
            target_heatmap ([type]): ground truth center heatmap,
                expected shapes [batch, height, width, num classes],
                each value should be in range [0,1].
            target_regr (torch.Tensor): ground truth HW regression,
                expected shapes [batch, height, width, 2].

        Returns:
            torch.Tensor with loss value.
        """
        pred_mask = torch.sigmoid(predicted_heatmap)
        mask_loss = neg_loss(pred_mask, target_heatmap)
        mask_loss *= self.mask_loss_weight
        regr_loss = (torch.abs(predicted_regr - target_regr).sum(1)[:, None, :, :] * target_heatmap).sum()
        regr_loss = regr_loss / target_heatmap.sum()
        regr_loss *= self.regr_loss_weight
        loss = mask_loss + regr_loss
        if not self.size_average:
            loss *= predicted_heatmap.shape[0]
        return loss, mask_loss, regr_loss


class DoubleConv(nn.Module):
    """(conv => BN => ReLU) * 2"""

    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True), nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))

    def forward(self, x):
        x = self.conv(x)
        return x


class Interpolate(nn.Module):

    def __init__(self, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):
        super().__init__()
        self.size = size
        self.scale_factor = scale_factor
        self.mode = mode
        self.align_corners = align_corners
        self.recompute_scale_factor = recompute_scale_factor

    def forward(self, inputs):
        return F.interpolate(inputs, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners, recompute_scale_factor=self.recompute_scale_factor)


class UpDoubleConv(nn.Module):

    def __init__(self, in_channels, out_channels, mode=None):
        super().__init__()
        self.in_ch = in_channels
        self.out_ch = out_channels
        self.mode = mode
        if mode is None:
            self.up = nn.ConvTranspose2d(in_channels, in_channels, 2, stride=2)
        else:
            align_corners = None if mode == 'nearest' else True
            self.up = Interpolate(scale_factor=2, mode=mode, align_corners=align_corners)
        self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1, x2=None):
        x1 = self.up(x1)
        if x2 is not None:
            x = torch.cat([x2, x1], dim=1)
            diffY = x2.size()[2] - x1.size()[2]
            diffX = x2.size()[3] - x1.size()[3]
            x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))
        else:
            x = x1
        x = self.conv(x)
        return x


_backbones = {'resnet18': (resnet.resnet18, 512), 'resnet34': (resnet.resnet34, 512), 'resnet50': (resnet.resnet50, 2048), 'resnet101': (resnet.resnet101, 2048), 'resnet152': (resnet.resnet152, 2048), 'mobilenet_v2': (mobilenet.mobilenet_v2, 1280)}


class CenterNet(nn.Module):

    def __init__(self, num_classes=1, backbone='resnet18', upsample_mode='nearest'):
        super().__init__()
        basemodel = _backbones[backbone][0](pretrained=True)
        if backbone == 'mobilenet_v2':
            layers = list(basemodel.children())[:-1]
        else:
            layers = list(basemodel.children())[:-2]
        basemodel = nn.Sequential(*layers)
        self.base_model = basemodel
        self.upsample_mode = upsample_mode
        num_ch = _backbones[backbone][1]
        self.up1 = UpDoubleConv(num_ch, 512, upsample_mode)
        self.up2 = UpDoubleConv(512, 256, upsample_mode)
        self.up3 = UpDoubleConv(256, 256, upsample_mode)
        self.out_classification = nn.Conv2d(256, num_classes, 1)
        self.out_residue = nn.Conv2d(256, 2, 1)

    def forward(self, x):
        x = self.base_model(x)
        x = self.up1(x)
        x = self.up2(x)
        x = self.up3(x)
        c = self.out_classification(x)
        r = self.out_residue(x)
        return c, r


_channels_map = {'resnet18': [256, 512, 512, 256, 256, 128], 'resnet34': [256, 512, 512, 256, 256, 256], 'resnet50': [1024, 512, 512, 256, 256, 256], 'resnet101': [1024, 512, 512, 256, 256, 256], 'resnet152': [1024, 512, 512, 256, 256, 256]}


class ResnetBackbone(nn.Module):

    def __init__(self, backbone='resnet50', backbone_path=None):
        """
        Args:
            backbone (str): resnet backbone to use.
                Expected one of ["resnet18", "resnet34", "resnet50", "resnet101", "resnet152"]
                Default is "resnet50".
            backbone_path (str): path to pretrained backbone model.
                If ``None`` then will be used torchvision pretrained model.
                Default is None.
        """
        super().__init__()
        self.out_channels = _channels_map[backbone]
        if backbone == 'resnet18':
            backbone = resnet.resnet18(pretrained=not backbone_path)
        elif backbone == 'resnet34':
            backbone = resnet.resnet34(pretrained=not backbone_path)
        elif backbone == 'resnet50':
            backbone = resnet.resnet50(pretrained=not backbone_path)
        elif backbone == 'resnet101':
            backbone = resnet.resnet101(pretrained=not backbone_path)
        elif backbone == 'resnet152':
            backbone = resnet.resnet152(pretrained=not backbone_path)
        else:
            raise ValueError(f"Unknown ResNet backbone - '{backbone}'!")
        if backbone_path:
            backbone.load_state_dict(torch.load(backbone_path))
        self.feature_extractor = nn.Sequential(*list(backbone.children())[:7])
        conv4_block1 = self.feature_extractor[-1][0]
        conv4_block1.conv1.stride = 1, 1
        conv4_block1.conv2.stride = 1, 1
        conv4_block1.downsample[0].stride = 1, 1

    def forward(self, x):
        x = self.feature_extractor(x)
        return x


class SingleShotDetector(nn.Module):

    def __init__(self, backbone='resnet18', num_classes=80):
        """
        Source:
            https://github.com/NVIDIA/DeepLearningExamples/blob/70fcb70ff4bc49cc723195b35cfa8d4ce94a7f76/PyTorch/Detection/SSD/src/model.py

        Args:
            backbone (str): model backbone to use
            n_classes (int): number of classes to predict
        """
        super().__init__()
        self.feature_extractor = ResnetBackbone(backbone)
        self.label_num = num_classes + 1
        self._build_additional_features(self.feature_extractor.out_channels)
        self.num_defaults = [4, 6, 6, 6, 4, 4]
        self.loc = []
        self.conf = []
        for nd, oc in zip(self.num_defaults, self.feature_extractor.out_channels):
            self.loc.append(nn.Conv2d(oc, nd * 4, kernel_size=3, padding=1))
            self.conf.append(nn.Conv2d(oc, nd * self.label_num, kernel_size=3, padding=1))
        self.loc = nn.ModuleList(self.loc)
        self.conf = nn.ModuleList(self.conf)
        self._init_weights()

    def _build_additional_features(self, input_size):
        self.additional_blocks = []
        for i, (input_size, output_size, channels) in enumerate(zip(input_size[:-1], input_size[1:], [256, 256, 128, 128, 128])):
            if i < 3:
                layer = nn.Sequential(nn.Conv2d(input_size, channels, kernel_size=1, bias=False), nn.BatchNorm2d(channels), nn.ReLU(inplace=True), nn.Conv2d(channels, output_size, kernel_size=3, padding=1, stride=2, bias=False), nn.BatchNorm2d(output_size), nn.ReLU(inplace=True))
            else:
                layer = nn.Sequential(nn.Conv2d(input_size, channels, kernel_size=1, bias=False), nn.BatchNorm2d(channels), nn.ReLU(inplace=True), nn.Conv2d(channels, output_size, kernel_size=3, bias=False), nn.BatchNorm2d(output_size), nn.ReLU(inplace=True))
            self.additional_blocks.append(layer)
        self.additional_blocks = nn.ModuleList(self.additional_blocks)

    def _init_weights(self):
        layers = [*self.additional_blocks, *self.loc, *self.conf]
        for layer in layers:
            for param in layer.parameters():
                if param.dim() > 1:
                    nn.init.xavier_uniform_(param)

    def bbox_view(self, src, loc, conf):
        ret = []
        for s, l, c in zip(src, loc, conf):
            ret.append((l(s).view(s.size(0), -1, 4), c(s).view(s.size(0), -1, self.label_num)))
        locs, confs = list(zip(*ret))
        locs, confs = torch.cat(locs, 1).contiguous(), torch.cat(confs, 1).contiguous()
        return locs, confs

    def forward(self, x):
        """
        Args:
            x (torch.Tensor): batch of data, expected shapes [B, 3, H, W]

        Returns:
            bbox locations (torch.Tensor) with shapes [B, A, 4],
                where B - batch size, A - num anchors
            class confidence logits (torch.Tensor) with shapes [B, A, N_CLASSES],
                where B - batch size, A - num anchors
        """
        x = self.feature_extractor(x)
        detection_feed = [x]
        for layer in self.additional_blocks:
            x = layer(x)
            detection_feed.append(x)
        locs, confs = self.bbox_view(detection_feed, self.loc, self.conf)
        return locs, confs


def get_activation(name='silu', inplace=True):
    if name == 'silu':
        module = nn.SiLU(inplace=inplace)
    elif name == 'relu':
        module = nn.ReLU(inplace=inplace)
    elif name == 'lrelu':
        module = nn.LeakyReLU(0.1, inplace=inplace)
    else:
        raise AttributeError('Unsupported act type: {}'.format(name))
    return module


class BaseConv(nn.Module):
    """A Conv2d -> Batchnorm -> silu/leaky relu block"""

    def __init__(self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act='silu'):
        super().__init__()
        pad = (ksize - 1) // 2
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=ksize, stride=stride, padding=pad, groups=groups, bias=bias)
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = get_activation(act, inplace=True)

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def fuseforward(self, x):
        return self.act(self.conv(x))


class DWConv(nn.Module):
    """Depthwise Conv + Conv"""

    def __init__(self, in_channels, out_channels, ksize, stride=1, act='silu'):
        super().__init__()
        self.dconv = BaseConv(in_channels, in_channels, ksize=ksize, stride=stride, groups=in_channels, act=act)
        self.pconv = BaseConv(in_channels, out_channels, ksize=1, stride=1, groups=1, act=act)

    def forward(self, x):
        x = self.dconv(x)
        return self.pconv(x)


class Bottleneck(nn.Module):

    def __init__(self, in_channels, out_channels, shortcut=True, expansion=0.5, depthwise=False, act='silu'):
        super().__init__()
        hidden_channels = int(out_channels * expansion)
        Conv = DWConv if depthwise else BaseConv
        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)
        self.conv2 = Conv(hidden_channels, out_channels, 3, stride=1, act=act)
        self.use_add = shortcut and in_channels == out_channels

    def forward(self, x):
        y = self.conv2(self.conv1(x))
        if self.use_add:
            y = y + x
        return y


class ResLayer(nn.Module):
    """Residual layer with `in_channels` inputs."""

    def __init__(self, in_channels: 'int'):
        super().__init__()
        mid_channels = in_channels // 2
        self.layer1 = BaseConv(in_channels, mid_channels, ksize=1, stride=1, act='lrelu')
        self.layer2 = BaseConv(mid_channels, in_channels, ksize=3, stride=1, act='lrelu')

    def forward(self, x):
        out = self.layer2(self.layer1(x))
        return x + out


class SPPBottleneck(nn.Module):
    """Spatial pyramid pooling layer used in YOLOv3-SPP"""

    def __init__(self, in_channels, out_channels, kernel_sizes=(5, 9, 13), activation='silu'):
        super().__init__()
        hidden_channels = in_channels // 2
        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=activation)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=ks, stride=1, padding=ks // 2) for ks in kernel_sizes])
        conv2_channels = hidden_channels * (len(kernel_sizes) + 1)
        self.conv2 = BaseConv(conv2_channels, out_channels, 1, stride=1, act=activation)

    def forward(self, x):
        x = self.conv1(x)
        x = torch.cat([x] + [m(x) for m in self.m], dim=1)
        x = self.conv2(x)
        return x


class CSPLayer(nn.Module):
    """C3 in yolov5, CSP Bottleneck with 3 convolutions"""

    def __init__(self, in_channels, out_channels, n=1, shortcut=True, expansion=0.5, depthwise=False, act='silu'):
        """
        Args:
            in_channels (int): input channels.
            out_channels (int): output channels.
            n (int): number of Bottlenecks. Default value: 1.
        """
        super().__init__()
        hidden_channels = int(out_channels * expansion)
        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)
        self.conv2 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)
        self.conv3 = BaseConv(2 * hidden_channels, out_channels, 1, stride=1, act=act)
        module_list = [Bottleneck(hidden_channels, hidden_channels, shortcut, 1.0, depthwise, act=act) for _ in range(n)]
        self.m = nn.Sequential(*module_list)

    def forward(self, x):
        x_1 = self.conv1(x)
        x_2 = self.conv2(x)
        x_1 = self.m(x_1)
        x = torch.cat((x_1, x_2), dim=1)
        return self.conv3(x)


class Focus(nn.Module):
    """Focus width and height information into channel space."""

    def __init__(self, in_channels, out_channels, ksize=1, stride=1, act='silu'):
        super().__init__()
        self.conv = BaseConv(in_channels * 4, out_channels, ksize, stride, act=act)

    def forward(self, x):
        patch_top_left = x[..., ::2, ::2]
        patch_top_right = x[..., ::2, 1::2]
        patch_bot_left = x[..., 1::2, ::2]
        patch_bot_right = x[..., 1::2, 1::2]
        x = torch.cat((patch_top_left, patch_bot_left, patch_top_right, patch_bot_right), dim=1)
        return self.conv(x)


class Darknet(nn.Module):
    depth2blocks = {(21): [1, 2, 2, 1], (53): [2, 8, 8, 4]}

    def __init__(self, depth, in_channels=3, stem_out_channels=32, out_features=('dark3', 'dark4', 'dark5')):
        """
        Args:
            depth (int): depth of darknet used in model, usually use [21, 53] for this param.
            in_channels (int): number of input channels, for example, use 3 for RGB image.
            stem_out_channels (int): number of output chanels of darknet stem.
                It decides channels of darknet layer2 to layer5.
            out_features (Tuple[str]): desired output layer name.
        """
        super().__init__()
        assert out_features, 'please provide output features of Darknet'
        self.out_features = out_features
        self.stem = nn.Sequential(BaseConv(in_channels, stem_out_channels, ksize=3, stride=1, act='lrelu'), *self.make_group_layer(stem_out_channels, num_blocks=1, stride=2))
        in_channels = stem_out_channels * 2
        num_blocks = Darknet.depth2blocks[depth]
        self.dark2 = nn.Sequential(*self.make_group_layer(in_channels, num_blocks[0], stride=2))
        in_channels *= 2
        self.dark3 = nn.Sequential(*self.make_group_layer(in_channels, num_blocks[1], stride=2))
        in_channels *= 2
        self.dark4 = nn.Sequential(*self.make_group_layer(in_channels, num_blocks[2], stride=2))
        in_channels *= 2
        self.dark5 = nn.Sequential(*self.make_group_layer(in_channels, num_blocks[3], stride=2), *self.make_spp_block([in_channels, in_channels * 2], in_channels * 2))

    def make_group_layer(self, in_channels: 'int', num_blocks: 'int', stride: 'int'=1):
        """starts with conv layer then has `num_blocks` `ResLayer`"""
        return [BaseConv(in_channels, in_channels * 2, ksize=3, stride=stride, act='lrelu'), *[ResLayer(in_channels * 2) for _ in range(num_blocks)]]

    def make_spp_block(self, filters_list, in_filters):
        m = nn.Sequential(*[BaseConv(in_filters, filters_list[0], 1, stride=1, act='lrelu'), BaseConv(filters_list[0], filters_list[1], 3, stride=1, act='lrelu'), SPPBottleneck(in_channels=filters_list[1], out_channels=filters_list[0], activation='lrelu'), BaseConv(filters_list[0], filters_list[1], 3, stride=1, act='lrelu'), BaseConv(filters_list[1], filters_list[0], 1, stride=1, act='lrelu')])
        return m

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        outputs['stem'] = x
        x = self.dark2(x)
        outputs['dark2'] = x
        x = self.dark3(x)
        outputs['dark3'] = x
        x = self.dark4(x)
        outputs['dark4'] = x
        x = self.dark5(x)
        outputs['dark5'] = x
        return {k: v for k, v in outputs.items() if k in self.out_features}


class CSPDarknet(nn.Module):

    def __init__(self, dep_mul, wid_mul, out_features=('dark3', 'dark4', 'dark5'), depthwise=False, act='silu'):
        super().__init__()
        assert out_features, 'please provide output features of Darknet'
        self.out_features = out_features
        Conv = DWConv if depthwise else BaseConv
        base_channels = int(wid_mul * 64)
        base_depth = max(round(dep_mul * 3), 1)
        self.stem = Focus(3, base_channels, ksize=3, act=act)
        self.dark2 = nn.Sequential(Conv(base_channels, base_channels * 2, 3, 2, act=act), CSPLayer(base_channels * 2, base_channels * 2, n=base_depth, depthwise=depthwise, act=act))
        self.dark3 = nn.Sequential(Conv(base_channels * 2, base_channels * 4, 3, 2, act=act), CSPLayer(base_channels * 4, base_channels * 4, n=base_depth * 3, depthwise=depthwise, act=act))
        self.dark4 = nn.Sequential(Conv(base_channels * 4, base_channels * 8, 3, 2, act=act), CSPLayer(base_channels * 8, base_channels * 8, n=base_depth * 3, depthwise=depthwise, act=act))
        self.dark5 = nn.Sequential(Conv(base_channels * 8, base_channels * 16, 3, 2, act=act), SPPBottleneck(base_channels * 16, base_channels * 16, activation=act), CSPLayer(base_channels * 16, base_channels * 16, n=base_depth, shortcut=False, depthwise=depthwise, act=act))

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        outputs['stem'] = x
        x = self.dark2(x)
        outputs['dark2'] = x
        x = self.dark3(x)
        outputs['dark3'] = x
        x = self.dark4(x)
        outputs['dark4'] = x
        x = self.dark5(x)
        outputs['dark5'] = x
        return {k: v for k, v in outputs.items() if k in self.out_features}


class YOLOPAFPN(nn.Module):
    """
    YOLOv3 model. Darknet 53 is the default backbone of this model.
    """

    def __init__(self, depth=1.0, width=1.0, in_features=('dark3', 'dark4', 'dark5'), in_channels=[256, 512, 1024], depthwise=False, act='silu'):
        super().__init__()
        self.backbone = CSPDarknet(depth, width, depthwise=depthwise, act=act)
        self.in_features = in_features
        self.in_channels = in_channels
        Conv = DWConv if depthwise else BaseConv
        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')
        self.lateral_conv0 = BaseConv(int(in_channels[2] * width), int(in_channels[1] * width), 1, 1, act=act)
        self.C3_p4 = CSPLayer(int(2 * in_channels[1] * width), int(in_channels[1] * width), round(3 * depth), False, depthwise=depthwise, act=act)
        self.reduce_conv1 = BaseConv(int(in_channels[1] * width), int(in_channels[0] * width), 1, 1, act=act)
        self.C3_p3 = CSPLayer(int(2 * in_channels[0] * width), int(in_channels[0] * width), round(3 * depth), False, depthwise=depthwise, act=act)
        self.bu_conv2 = Conv(int(in_channels[0] * width), int(in_channels[0] * width), 3, 2, act=act)
        self.C3_n3 = CSPLayer(int(2 * in_channels[0] * width), int(in_channels[1] * width), round(3 * depth), False, depthwise=depthwise, act=act)
        self.bu_conv1 = Conv(int(in_channels[1] * width), int(in_channels[1] * width), 3, 2, act=act)
        self.C3_n4 = CSPLayer(int(2 * in_channels[1] * width), int(in_channels[2] * width), round(3 * depth), False, depthwise=depthwise, act=act)

    def forward(self, input):
        """
        Args:
            inputs: input images.

        Returns:
            Tuple[Tensor]: FPN feature.
        """
        out_features = self.backbone(input)
        features = [out_features[f] for f in self.in_features]
        [x2, x1, x0] = features
        fpn_out0 = self.lateral_conv0(x0)
        f_out0 = self.upsample(fpn_out0)
        f_out0 = torch.cat([f_out0, x1], 1)
        f_out0 = self.C3_p4(f_out0)
        fpn_out1 = self.reduce_conv1(f_out0)
        f_out1 = self.upsample(fpn_out1)
        f_out1 = torch.cat([f_out1, x2], 1)
        pan_out2 = self.C3_p3(f_out1)
        p_out1 = self.bu_conv2(pan_out2)
        p_out1 = torch.cat([p_out1, fpn_out1], 1)
        pan_out1 = self.C3_n3(p_out1)
        p_out0 = self.bu_conv1(pan_out1)
        p_out0 = torch.cat([p_out0, fpn_out0], 1)
        pan_out0 = self.C3_n4(p_out0)
        outputs = pan_out2, pan_out1, pan_out0
        return outputs


class IOUloss(nn.Module):

    def __init__(self, reduction='none', loss_type='iou'):
        super(IOUloss, self).__init__()
        self.reduction = reduction
        self.loss_type = loss_type

    def forward(self, pred, target):
        assert pred.shape[0] == target.shape[0]
        pred = pred.view(-1, 4)
        target = target.view(-1, 4)
        tl = torch.max(pred[:, :2] - pred[:, 2:] / 2, target[:, :2] - target[:, 2:] / 2)
        br = torch.min(pred[:, :2] + pred[:, 2:] / 2, target[:, :2] + target[:, 2:] / 2)
        area_p = torch.prod(pred[:, 2:], 1)
        area_g = torch.prod(target[:, 2:], 1)
        en = (tl < br).type(tl.type()).prod(dim=1)
        area_i = torch.prod(br - tl, 1) * en
        iou = area_i / (area_p + area_g - area_i + 1e-16)
        if self.loss_type == 'iou':
            loss = 1 - iou ** 2
        elif self.loss_type == 'giou':
            c_tl = torch.min(pred[:, :2] - pred[:, 2:] / 2, target[:, :2] - target[:, 2:] / 2)
            c_br = torch.max(pred[:, :2] + pred[:, 2:] / 2, target[:, :2] + target[:, 2:] / 2)
            area_c = torch.prod(c_br - c_tl, 1)
            giou = iou - (area_c - area_i) / area_c.clamp(1e-16)
            loss = 1 - giou.clamp(min=-1.0, max=1.0)
        if self.reduction == 'mean':
            loss = loss.mean()
        elif self.reduction == 'sum':
            loss = loss.sum()
        return loss


def bboxes_iou(bboxes_a, bboxes_b, xyxy=True):
    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:
        raise IndexError()
    if xyxy:
        tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])
        br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])
        area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)
        area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)
    else:
        tl = torch.max(bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2, bboxes_b[:, :2] - bboxes_b[:, 2:] / 2)
        br = torch.min(bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2, bboxes_b[:, :2] + bboxes_b[:, 2:] / 2)
        area_a = torch.prod(bboxes_a[:, 2:], 1)
        area_b = torch.prod(bboxes_b[:, 2:], 1)
    en = (tl < br).type(tl.type()).prod(dim=2)
    area_i = torch.prod(br - tl, 2) * en
    return area_i / (area_a[:, None] + area_b - area_i)


logger = logging.getLogger('yolo-x')


class YOLOXHead(nn.Module):

    def __init__(self, num_classes, width=1.0, strides=[8, 16, 32], in_channels=[256, 512, 1024], act='silu', depthwise=False):
        """
        Args:
            act (str): activation type of conv.
                Default is `"silu"`.
            depthwise (bool): wheather apply depthwise conv in conv branch.
                Default is `False`.
        """
        super().__init__()
        self.n_anchors = 1
        self.num_classes = num_classes
        self.decode_in_inference = True
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()
        self.cls_preds = nn.ModuleList()
        self.reg_preds = nn.ModuleList()
        self.obj_preds = nn.ModuleList()
        self.stems = nn.ModuleList()
        _conv_class = DWConv if depthwise else BaseConv
        for i in range(len(in_channels)):
            self.stems.append(BaseConv(in_channels=int(in_channels[i] * width), out_channels=int(256 * width), ksize=1, stride=1, act=act))
            self.cls_convs.append(nn.Sequential(*[_conv_class(in_channels=int(256 * width), out_channels=int(256 * width), ksize=3, stride=1, act=act), _conv_class(in_channels=int(256 * width), out_channels=int(256 * width), ksize=3, stride=1, act=act)]))
            self.reg_convs.append(nn.Sequential(*[_conv_class(in_channels=int(256 * width), out_channels=int(256 * width), ksize=3, stride=1, act=act), _conv_class(in_channels=int(256 * width), out_channels=int(256 * width), ksize=3, stride=1, act=act)]))
            self.cls_preds.append(nn.Conv2d(in_channels=int(256 * width), out_channels=self.n_anchors * self.num_classes, kernel_size=1, stride=1, padding=0))
            self.reg_preds.append(nn.Conv2d(in_channels=int(256 * width), out_channels=4, kernel_size=1, stride=1, padding=0))
            self.obj_preds.append(nn.Conv2d(in_channels=int(256 * width), out_channels=self.n_anchors * 1, kernel_size=1, stride=1, padding=0))
        self.use_l1 = False
        self.l1_loss = nn.L1Loss(reduction='none')
        self.bcewithlog_loss = nn.BCEWithLogitsLoss(reduction='none')
        self.iou_loss = IOUloss(reduction='none')
        self.strides = strides
        self.grids = [torch.zeros(1)] * len(in_channels)

    def initialize_biases(self, prior_prob):
        for conv in self.cls_preds:
            b = conv.bias.view(self.n_anchors, -1)
            b.data.fill_(-math.log((1 - prior_prob) / prior_prob))
            conv.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)
        for conv in self.obj_preds:
            b = conv.bias.view(self.n_anchors, -1)
            b.data.fill_(-math.log((1 - prior_prob) / prior_prob))
            conv.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)

    def forward(self, xin, labels=None, imgs=None):
        outputs = []
        origin_preds = []
        x_shifts = []
        y_shifts = []
        expanded_strides = []
        for k, (cls_conv, reg_conv, stride_this_level, x) in enumerate(zip(self.cls_convs, self.reg_convs, self.strides, xin)):
            x = self.stems[k](x)
            cls_x = x
            reg_x = x
            cls_feat = cls_conv(cls_x)
            cls_output = self.cls_preds[k](cls_feat)
            reg_feat = reg_conv(reg_x)
            reg_output = self.reg_preds[k](reg_feat)
            obj_output = self.obj_preds[k](reg_feat)
            if self.training:
                output = torch.cat([reg_output, obj_output, cls_output], 1)
                output, grid = self.get_output_and_grid(output, k, stride_this_level, xin[0].type())
                x_shifts.append(grid[:, :, 0])
                y_shifts.append(grid[:, :, 1])
                expanded_strides.append(torch.zeros(1, grid.shape[1]).fill_(stride_this_level).type_as(xin[0]))
                if self.use_l1:
                    batch_size = reg_output.shape[0]
                    hsize, wsize = reg_output.shape[-2:]
                    reg_output = reg_output.view(batch_size, self.n_anchors, 4, hsize, wsize)
                    reg_output = reg_output.permute(0, 1, 3, 4, 2).reshape(batch_size, -1, 4)
                    origin_preds.append(reg_output.clone())
            else:
                output = torch.cat([reg_output, obj_output.sigmoid(), cls_output.sigmoid()], 1)
            outputs.append(output)
        if self.training:
            return self.get_losses(imgs, x_shifts, y_shifts, expanded_strides, labels, torch.cat(outputs, 1), origin_preds, dtype=xin[0].dtype)
        else:
            self.hw = [x.shape[-2:] for x in outputs]
            outputs = torch.cat([x.flatten(start_dim=2) for x in outputs], dim=2).permute(0, 2, 1)
            if self.decode_in_inference:
                return self.decode_outputs(outputs, dtype=xin[0].type())
            else:
                return outputs

    def get_output_and_grid(self, output, k, stride, dtype):
        grid = self.grids[k]
        batch_size = output.shape[0]
        n_ch = 5 + self.num_classes
        hsize, wsize = output.shape[-2:]
        if grid.shape[2:4] != output.shape[2:4]:
            yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])
            grid = torch.stack((xv, yv), 2).view(1, 1, hsize, wsize, 2).type(dtype)
            self.grids[k] = grid
        output = output.view(batch_size, self.n_anchors, n_ch, hsize, wsize)
        output = output.permute(0, 1, 3, 4, 2).reshape(batch_size, self.n_anchors * hsize * wsize, -1)
        grid = grid.view(1, -1, 2)
        output[..., :2] = (output[..., :2] + grid) * stride
        output[..., 2:4] = torch.exp(output[..., 2:4]) * stride
        return output, grid

    def decode_outputs(self, outputs, dtype):
        grids = []
        strides = []
        for (hsize, wsize), stride in zip(self.hw, self.strides):
            yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])
            grid = torch.stack((xv, yv), 2).view(1, -1, 2)
            grids.append(grid)
            shape = grid.shape[:2]
            strides.append(torch.full((*shape, 1), stride))
        grids = torch.cat(grids, dim=1).type(dtype)
        strides = torch.cat(strides, dim=1).type(dtype)
        outputs[..., :2] = (outputs[..., :2] + grids) * strides
        outputs[..., 2:4] = torch.exp(outputs[..., 2:4]) * strides
        return outputs

    def get_losses(self, imgs, x_shifts, y_shifts, expanded_strides, labels, outputs, origin_preds, dtype):
        bbox_preds = outputs[:, :, :4]
        obj_preds = outputs[:, :, 4].unsqueeze(-1)
        cls_preds = outputs[:, :, 5:]
        mixup = labels.shape[2] > 5
        if mixup:
            label_cut = labels[..., :5]
        else:
            label_cut = labels
        nlabel = (label_cut.sum(dim=2) > 0).sum(dim=1)
        total_num_anchors = outputs.shape[1]
        x_shifts = torch.cat(x_shifts, 1)
        y_shifts = torch.cat(y_shifts, 1)
        expanded_strides = torch.cat(expanded_strides, 1)
        if self.use_l1:
            origin_preds = torch.cat(origin_preds, 1)
        cls_targets = []
        reg_targets = []
        l1_targets = []
        obj_targets = []
        fg_masks = []
        num_fg = 0.0
        num_gts = 0.0
        for batch_idx in range(outputs.shape[0]):
            num_gt = int(nlabel[batch_idx])
            num_gts += num_gt
            if num_gt == 0:
                cls_target = outputs.new_zeros((0, self.num_classes))
                reg_target = outputs.new_zeros((0, 4))
                l1_target = outputs.new_zeros((0, 4))
                obj_target = outputs.new_zeros((total_num_anchors, 1))
                fg_mask = outputs.new_zeros(total_num_anchors).bool()
            else:
                gt_bboxes_per_image = labels[batch_idx, :num_gt, 1:5]
                gt_classes = labels[batch_idx, :num_gt, 0]
                bboxes_preds_per_image = bbox_preds[batch_idx]
                try:
                    gt_matched_classes, fg_mask, pred_ious_this_matching, matched_gt_inds, num_fg_img = self.get_assignments(batch_idx, num_gt, total_num_anchors, gt_bboxes_per_image, gt_classes, bboxes_preds_per_image, expanded_strides, x_shifts, y_shifts, cls_preds, bbox_preds, obj_preds, labels, imgs)
                except RuntimeError:
                    logger.error('OOM RuntimeError is raised due to the huge memory cost during label assignment. CPU mode is applied in this batch. If you want to avoid this issue, try to reduce the batch size or image size.')
                    torch.cuda.empty_cache()
                    gt_matched_classes, fg_mask, pred_ious_this_matching, matched_gt_inds, num_fg_img = self.get_assignments(batch_idx, num_gt, total_num_anchors, gt_bboxes_per_image, gt_classes, bboxes_preds_per_image, expanded_strides, x_shifts, y_shifts, cls_preds, bbox_preds, obj_preds, labels, imgs, 'cpu')
                torch.cuda.empty_cache()
                num_fg += num_fg_img
                cls_target = F.one_hot(gt_matched_classes, self.num_classes) * pred_ious_this_matching.unsqueeze(-1)
                obj_target = fg_mask.unsqueeze(-1)
                reg_target = gt_bboxes_per_image[matched_gt_inds]
                if self.use_l1:
                    l1_target = self.get_l1_target(outputs.new_zeros((num_fg_img, 4)), gt_bboxes_per_image[matched_gt_inds], expanded_strides[0][fg_mask], x_shifts=x_shifts[0][fg_mask], y_shifts=y_shifts[0][fg_mask])
            cls_targets.append(cls_target)
            reg_targets.append(reg_target)
            obj_targets.append(obj_target)
            fg_masks.append(fg_mask)
            if self.use_l1:
                l1_targets.append(l1_target)
        cls_targets = torch.cat(cls_targets, 0)
        reg_targets = torch.cat(reg_targets, 0)
        obj_targets = torch.cat(obj_targets, 0)
        fg_masks = torch.cat(fg_masks, 0)
        if self.use_l1:
            l1_targets = torch.cat(l1_targets, 0)
        num_fg = max(num_fg, 1)
        loss_iou = self.iou_loss(bbox_preds.view(-1, 4)[fg_masks], reg_targets).sum() / num_fg
        loss_obj = self.bcewithlog_loss(obj_preds.view(-1, 1), obj_targets).sum() / num_fg
        loss_cls = self.bcewithlog_loss(cls_preds.view(-1, self.num_classes)[fg_masks], cls_targets).sum() / num_fg
        if self.use_l1:
            loss_l1 = self.l1_loss(origin_preds.view(-1, 4)[fg_masks], l1_targets).sum() / num_fg
        else:
            loss_l1 = 0.0
        reg_weight = 5.0
        loss = reg_weight * loss_iou + loss_obj + loss_cls + loss_l1
        return loss, reg_weight * loss_iou, loss_obj, loss_cls, loss_l1, num_fg / max(num_gts, 1)

    def get_l1_target(self, l1_target, gt, stride, x_shifts, y_shifts, eps=1e-08):
        l1_target[:, 0] = gt[:, 0] / stride - x_shifts
        l1_target[:, 1] = gt[:, 1] / stride - y_shifts
        l1_target[:, 2] = torch.log(gt[:, 2] / stride + eps)
        l1_target[:, 3] = torch.log(gt[:, 3] / stride + eps)
        return l1_target

    @torch.no_grad()
    def get_assignments(self, batch_idx, num_gt, total_num_anchors, gt_bboxes_per_image, gt_classes, bboxes_preds_per_image, expanded_strides, x_shifts, y_shifts, cls_preds, bbox_preds, obj_preds, labels, imgs, mode='gpu'):
        if mode == 'cpu':
            None
            gt_bboxes_per_image = gt_bboxes_per_image.cpu().float()
            bboxes_preds_per_image = bboxes_preds_per_image.cpu().float()
            gt_classes = gt_classes.cpu().float()
            expanded_strides = expanded_strides.cpu().float()
            x_shifts = x_shifts.cpu()
            y_shifts = y_shifts.cpu()
        fg_mask, is_in_boxes_and_center = self.get_in_boxes_info(gt_bboxes_per_image, expanded_strides, x_shifts, y_shifts, total_num_anchors, num_gt)
        bboxes_preds_per_image = bboxes_preds_per_image[fg_mask]
        cls_preds_ = cls_preds[batch_idx][fg_mask]
        obj_preds_ = obj_preds[batch_idx][fg_mask]
        num_in_boxes_anchor = bboxes_preds_per_image.shape[0]
        if mode == 'cpu':
            gt_bboxes_per_image = gt_bboxes_per_image.cpu()
            bboxes_preds_per_image = bboxes_preds_per_image.cpu()
        pair_wise_ious = bboxes_iou(gt_bboxes_per_image, bboxes_preds_per_image, False)
        gt_cls_per_image = F.one_hot(gt_classes, self.num_classes).float().unsqueeze(1).repeat(1, num_in_boxes_anchor, 1)
        pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-08)
        if mode == 'cpu':
            cls_preds_, obj_preds_ = cls_preds_.cpu(), obj_preds_.cpu()
        cls_preds_ = cls_preds_.float().unsqueeze(0).repeat(num_gt, 1, 1).sigmoid_() * obj_preds_.unsqueeze(0).repeat(num_gt, 1, 1).sigmoid_()
        pair_wise_cls_loss = F.binary_cross_entropy(cls_preds_.sqrt_(), gt_cls_per_image, reduction='none').sum(-1)
        del cls_preds_
        cost = pair_wise_cls_loss + 3.0 * pair_wise_ious_loss + 100000.0 * ~is_in_boxes_and_center
        num_fg, gt_matched_classes, pred_ious_this_matching, matched_gt_inds = self.dynamic_k_matching(cost, pair_wise_ious, gt_classes, num_gt, fg_mask)
        del pair_wise_cls_loss, cost, pair_wise_ious, pair_wise_ious_loss
        if mode == 'cpu':
            gt_matched_classes = gt_matched_classes
            fg_mask = fg_mask
            pred_ious_this_matching = pred_ious_this_matching
            matched_gt_inds = matched_gt_inds
        return gt_matched_classes, fg_mask, pred_ious_this_matching, matched_gt_inds, num_fg

    def get_in_boxes_info(self, gt_bboxes_per_image, expanded_strides, x_shifts, y_shifts, total_num_anchors, num_gt):
        expanded_strides_per_image = expanded_strides[0]
        x_shifts_per_image = x_shifts[0] * expanded_strides_per_image
        y_shifts_per_image = y_shifts[0] * expanded_strides_per_image
        x_centers_per_image = (x_shifts_per_image + 0.5 * expanded_strides_per_image).unsqueeze(0).repeat(num_gt, 1)
        y_centers_per_image = (y_shifts_per_image + 0.5 * expanded_strides_per_image).unsqueeze(0).repeat(num_gt, 1)
        gt_bboxes_per_image_l = (gt_bboxes_per_image[:, 0] - 0.5 * gt_bboxes_per_image[:, 2]).unsqueeze(1).repeat(1, total_num_anchors)
        gt_bboxes_per_image_r = (gt_bboxes_per_image[:, 0] + 0.5 * gt_bboxes_per_image[:, 2]).unsqueeze(1).repeat(1, total_num_anchors)
        gt_bboxes_per_image_t = (gt_bboxes_per_image[:, 1] - 0.5 * gt_bboxes_per_image[:, 3]).unsqueeze(1).repeat(1, total_num_anchors)
        gt_bboxes_per_image_b = (gt_bboxes_per_image[:, 1] + 0.5 * gt_bboxes_per_image[:, 3]).unsqueeze(1).repeat(1, total_num_anchors)
        b_l = x_centers_per_image - gt_bboxes_per_image_l
        b_r = gt_bboxes_per_image_r - x_centers_per_image
        b_t = y_centers_per_image - gt_bboxes_per_image_t
        b_b = gt_bboxes_per_image_b - y_centers_per_image
        bbox_deltas = torch.stack([b_l, b_t, b_r, b_b], 2)
        is_in_boxes = bbox_deltas.min(dim=-1).values > 0.0
        is_in_boxes_all = is_in_boxes.sum(dim=0) > 0
        center_radius = 2.5
        gt_bboxes_per_image_l = gt_bboxes_per_image[:, 0].unsqueeze(1).repeat(1, total_num_anchors) - center_radius * expanded_strides_per_image.unsqueeze(0)
        gt_bboxes_per_image_r = gt_bboxes_per_image[:, 0].unsqueeze(1).repeat(1, total_num_anchors) + center_radius * expanded_strides_per_image.unsqueeze(0)
        gt_bboxes_per_image_t = gt_bboxes_per_image[:, 1].unsqueeze(1).repeat(1, total_num_anchors) - center_radius * expanded_strides_per_image.unsqueeze(0)
        gt_bboxes_per_image_b = gt_bboxes_per_image[:, 1].unsqueeze(1).repeat(1, total_num_anchors) + center_radius * expanded_strides_per_image.unsqueeze(0)
        c_l = x_centers_per_image - gt_bboxes_per_image_l
        c_r = gt_bboxes_per_image_r - x_centers_per_image
        c_t = y_centers_per_image - gt_bboxes_per_image_t
        c_b = gt_bboxes_per_image_b - y_centers_per_image
        center_deltas = torch.stack([c_l, c_t, c_r, c_b], 2)
        is_in_centers = center_deltas.min(dim=-1).values > 0.0
        is_in_centers_all = is_in_centers.sum(dim=0) > 0
        is_in_boxes_anchor = is_in_boxes_all | is_in_centers_all
        is_in_boxes_and_center = is_in_boxes[:, is_in_boxes_anchor] & is_in_centers[:, is_in_boxes_anchor]
        return is_in_boxes_anchor, is_in_boxes_and_center

    def dynamic_k_matching(self, cost, pair_wise_ious, gt_classes, num_gt, fg_mask):
        matching_matrix = torch.zeros_like(cost)
        ious_in_boxes_matrix = pair_wise_ious
        n_candidate_k = 10
        topk_ious, _ = torch.topk(ious_in_boxes_matrix, n_candidate_k, dim=1)
        dynamic_ks = torch.clamp(topk_ious.sum(1).int(), min=1)
        for gt_idx in range(num_gt):
            _, pos_idx = torch.topk(cost[gt_idx], k=dynamic_ks[gt_idx].item(), largest=False)
            matching_matrix[gt_idx][pos_idx] = 1.0
        del topk_ious, dynamic_ks, pos_idx
        anchor_matching_gt = matching_matrix.sum(0)
        if (anchor_matching_gt > 1).sum() > 0:
            cost_min, cost_argmin = torch.min(cost[:, anchor_matching_gt > 1], dim=0)
            matching_matrix[:, anchor_matching_gt > 1] *= 0.0
            matching_matrix[cost_argmin, anchor_matching_gt > 1] = 1.0
        fg_mask_inboxes = matching_matrix.sum(0) > 0.0
        num_fg = fg_mask_inboxes.sum().item()
        fg_mask[fg_mask.clone()] = fg_mask_inboxes
        matched_gt_inds = matching_matrix[:, fg_mask_inboxes].argmax(0)
        gt_matched_classes = gt_classes[matched_gt_inds]
        pred_ious_this_matching = (matching_matrix * pair_wise_ious).sum(0)[fg_mask_inboxes]
        return num_fg, gt_matched_classes, pred_ious_this_matching, matched_gt_inds


class YOLOX(nn.Module):
    """
    YOLOX model module.
    The network returns loss values from three YOLO layers during training and detection results during test.
    NOTE:
        - model predicts bounding boxes in image size ranges
        - bounding boxes format is [x_center, y_center, width, height]
        - output format is [x_center, y_center, width, height, confidence, class_0_prob, ..., class_N_prob]
    """

    def __init__(self, backbone=None, head=None):
        super().__init__()
        if backbone is None:
            backbone = YOLOPAFPN()
        if head is None:
            head = YOLOXHead(80)
        self.backbone = backbone
        self.head = head

    def forward(self, x, targets=None):
        fpn_outs = self.backbone(x)
        if self.training:
            assert targets is not None
            loss, iou_loss, conf_loss, cls_loss, l1_loss, num_fg = self.head(fpn_outs, targets, x)
            return loss
        else:
            outputs = self.head(fpn_outs)
            return outputs


class MacridVAE(nn.Module):

    def __init__(self, q_dims, kfac=7, tau=0.1, nogb=False, dropout=0.5):
        super().__init__()
        self.q_dims = q_dims
        self.kfac = kfac
        self.tau = tau
        self.nogb = nogb
        self.item_embedding = nn.Embedding(self.q_dims[0], self.q_dims[-1])
        self.k_embedding = nn.Embedding(self.kfac, self.q_dims[-1])
        self.encoder = nn.Sequential()
        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-2], self.q_dims[1:-1])):
            self.encoder.add_module(f'encoder_fc_{i + 1}', nn.Linear(d_in, d_out))
            self.encoder.add_module(f'encoder_tanh_{i + 1}', nn.Tanh())
        self.encoder.add_module(f'encoder_fc_{len(self.q_dims) - 1}', nn.Linear(self.q_dims[-2], self.q_dims[-1] * 2))
        self.drop = nn.Dropout(dropout)
        self.encoder.apply(self.init_weights)

    def forward(self, x):
        x = F.normalize(x)
        x = self.drop(x)
        cores = F.normalize(self.k_embedding.weight)
        items = F.normalize(self.item_embedding.weight)
        cates_logits = torch.matmul(items, cores.transpose(0, 1)) / self.tau
        if self.nogb:
            cates = torch.softmax(cates_logits, dim=-1)
        else:
            cates_sample = F.gumbel_softmax(cates_logits, tau=1, hard=False, dim=-1)
            cates_mode = torch.softmax(cates_logits, dim=-1)
            cates = self.training * cates_sample + (1 - self.training) * cates_mode
        probs = None
        mulist = []
        logvarlist = []
        for k in range(self.kfac):
            cates_k = cates[:, k].reshape(1, -1)
            x_k = x * cates_k
            h = self.encoder(x_k)
            mu, logvar = h[:, :self.q_dims[-1]], h[:, self.q_dims[-1]:]
            mulist.append(mu)
            logvarlist.append(logvar)
            z = self.reparameterize(mu, logvar)
            z_k = F.normalize(z)
            logits_k = torch.matmul(z_k, items.transpose(0, 1)) / self.tau
            probs_k = torch.exp(logits_k)
            probs_k = probs_k * cates_k
            probs = probs_k if probs is None else probs + probs_k
        logits = torch.log(probs)
        return logits, mulist, logvarlist

    def reparameterize(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu

    def init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            nn.init.constant_(m.bias.data, 0)
        elif isinstance(m, nn.Embedding):
            nn.init.xavier_normal_(m.weight.data)


class MultiDAE(nn.Module):

    def __init__(self, p_dims, q_dims=None, dropout=0.5):
        super().__init__()
        self.p_dims = p_dims
        if q_dims:
            assert q_dims[0] == p_dims[-1], 'In and Out dimensions must equal to each other'
            assert q_dims[-1] == p_dims[0], 'Latent dimension for p- and q- network mismatches.'
            self.q_dims = q_dims
        else:
            self.q_dims = p_dims[::-1]
        self.encoder = nn.Sequential()
        self.encoder.add_module('normalize', Normalize())
        self.encoder.add_module('dropout', nn.Dropout(dropout))
        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-1], self.q_dims[1:])):
            self.encoder.add_module(f'encoder_fc_{i + 1}', nn.Linear(d_in, d_out))
            self.encoder.add_module(f'encoder_tanh_{i + 1}', nn.Tanh())
        self.decoder = nn.Sequential()
        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-2], self.p_dims[1:-1])):
            self.decoder.add_module(f'decoder_fc_{i + 1}', nn.Linear(d_in, d_out))
            self.decoder.add_module(f'decoder_tanh_{i + 1}', nn.Tanh())
        self.decoder.add_module(f'decoder_fc_{len(self.p_dims) - 1}', nn.Linear(self.p_dims[-2], self.p_dims[-1]))
        self.encoder.apply(self.init_weights)
        self.decoder.apply(self.init_weights)

    def forward(self, x):
        net = nn.Sequential(self.encoder, self.decoder)
        return net(x)

    def init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            nn.init.constant_(m.bias.data, 0)


class MultiVAE(nn.Module):

    def __init__(self, p_dims, q_dims=None, dropout=0.5):
        super().__init__()
        self.p_dims = p_dims
        if q_dims:
            assert q_dims[0] == p_dims[-1], 'In and Out dimensions must equal to each other'
            assert q_dims[-1] == p_dims[0], 'Latent dimension for p- and q- network mismatches.'
            self.q_dims = q_dims
        else:
            self.q_dims = p_dims[::-1]
        self.encoder = nn.Sequential()
        self.encoder.add_module('normalize', Normalize())
        self.encoder.add_module('dropout', nn.Dropout(dropout))
        for i, (d_in, d_out) in enumerate(zip(self.q_dims[:-2], self.q_dims[1:-1])):
            self.encoder.add_module(f'encoder_fc_{i + 1}', nn.Linear(d_in, d_out))
            self.encoder.add_module(f'encoder_tanh_{i + 1}', nn.Tanh())
        self.encoder.add_module(f'encoder_fc_{len(self.q_dims) - 1}', nn.Linear(self.q_dims[-2], self.q_dims[-1] * 2))
        self.decoder = nn.Sequential()
        for i, (d_in, d_out) in enumerate(zip(self.p_dims[:-2], self.p_dims[1:-1])):
            self.decoder.add_module(f'decoder_fc_{i + 1}', nn.Linear(d_in, d_out))
            self.decoder.add_module(f'decoder_tanh_{i + 1}', nn.Tanh())
        self.decoder.add_module(f'decoder_fc_{len(self.p_dims) - 1}', nn.Linear(self.p_dims[-2], self.p_dims[-1]))
        self.encoder.apply(self.init_weights)
        self.decoder.apply(self.init_weights)

    def forward(self, x):
        z = self.encoder(x)
        mu, logvar = z[:, :self.q_dims[-1]], z[:, self.q_dims[-1]:]
        z = self.reparameterize(mu, logvar)
        z = self.decoder(z)
        return z, mu, logvar

    def reparameterize(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu

    def init_weights(self, m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            nn.init.constant_(m.bias.data, 0)


class ContrastiveModel(torch.nn.Module):
    """Contrastive model with projective head.

    Args:
        model: projective head for the train time
        encoder: model for the future uses
    """

    def __init__(self, model, encoder):
        super(ContrastiveModel, self).__init__()
        self.model = model
        self.encoder = encoder

    def forward(self, x):
        """Forward method.

        Args:
            x: input for the encoder

        Returns:
            (embeddings, projections)
        """
        emb = self.encoder(x)
        projection = self.model(emb)
        return emb, projection


class DummyModel(nn.Module):
    """Dummy model"""

    def __init__(self, num_features: 'int', num_classes: 'int') ->None:
        super().__init__()
        self.model = nn.Sequential(nn.Flatten(), nn.Linear(in_features=num_features, out_features=num_classes))

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        Forward

        Args:
            x: inputs

        Returns:
            model's output
        """
        return self.model(x)


class CustomModule(nn.Module):

    def __init__(self, in_features: 'int', out_features1: 'int', out_features2: 'int'):
        super().__init__()
        self.shared = nn.Linear(in_features, 128)
        self.head1 = nn.Linear(128, out_features1)
        self.head2 = nn.Linear(128, out_features2)

    def forward(self, x):
        x = self.shared(x)
        y1 = self.head1(x)
        y2 = self.head2(x)
        return y1, y2


class Net(nn.Module):
    """Dummy network class."""

    def __init__(self, init_weight=4):
        """Initialization of network and filling it with given numbers."""
        super(Net, self).__init__()
        self.fc = nn.Linear(2, 1)
        self.fc.weight.data.fill_(init_weight)
        self.fc.bias.data.fill_(init_weight)


LOG_SCALE_MAX = 2


LOG_SCALE_MIN = -10


def normal_sample(loc, log_scale):
    scale = torch.exp(0.5 * log_scale)
    return loc + scale * torch.randn_like(scale)


class VAE(nn.Module):

    def __init__(self, in_features, hid_features):
        super().__init__()
        self.hid_features = hid_features
        self.encoder = nn.Linear(in_features, hid_features * 2)
        self.decoder = nn.Sequential(nn.Linear(hid_features, in_features), nn.Sigmoid())

    def forward(self, x, deterministic=False):
        z = self.encoder(x)
        bs, z_dim = z.shape
        loc, log_scale = z[:, :z_dim // 2], z[:, z_dim // 2:]
        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)
        z_ = loc if deterministic else normal_sample(loc, log_scale)
        z_ = z_.view(bs, -1)
        x_ = self.decoder(z_)
        return x_, loc, log_scale


import torch
from torch.nn import MSELoss, ReLU
from types import SimpleNamespace


TESTCASES = [
    # (nn.Module, init_args, forward_args)
    (AMSoftmax,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (AdaCos,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ArcFace,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ArcMarginProduct,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (BPRLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (BarlowTwinsLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (BaseConv,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'ksize': 4, 'stride': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Bottleneck,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (CSPLayer,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (CategoricalRegressionLoss,
     lambda: ([], {'num_atoms': 4, 'v_min': 4, 'v_max': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (CenterNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {})),
    (CenterNetCriterion,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (ContrastiveDistanceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (ContrastiveEmbeddingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (ContrastiveModel,
     lambda: ([], {'model': torch.nn.ReLU(), 'encoder': torch.nn.ReLU()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ContrastivePairwiseEmbeddingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (CosFace,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (CurricularFace,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (CustomModule,
     lambda: ([], {'in_features': 4, 'out_features1': 4, 'out_features2': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (DWConv,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'ksize': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (DiceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (DoubleConv,
     lambda: ([], {'in_ch': 4, 'out_ch': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (DummyModel,
     lambda: ([], {'num_features': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Focus,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GaussianNoise,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GeM2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GlobalAvgPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GlobalConcatPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GlobalMaxPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (HingeLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (HuberLossV0,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (IOUloss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (IoULoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (LamaPooling,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (Lambda,
     lambda: ([], {'lambda_fn': torch.nn.ReLU()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (LogisticLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (LovaszLossBinary,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (LovaszLossMultiClass,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (LovaszLossMultiLabel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (MacridVAE,
     lambda: ([], {'q_dims': [4, 4]}),
     lambda: ([torch.rand([4, 4])], {})),
    (MarginLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (MeanOutputLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (MultiDAE,
     lambda: ([], {'p_dims': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (MultiVAE,
     lambda: ([], {'p_dims': [4, 4]}),
     lambda: ([torch.rand([4, 4])], {})),
    (NTXentLoss,
     lambda: ([], {'tau': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (NaiveCrossEntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (Normalize,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (RMSNorm,
     lambda: ([], {'dimension': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (RSquareLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (ResLayer,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ResidualBlock,
     lambda: ([], {'block': torch.nn.ReLU()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (ResnetBackbone,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {})),
    (SPPBottleneck,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (SingleShotDetector,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 512, 512])], {})),
    (SmoothingDiceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (SoftMax,
     lambda: ([], {'in_features': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (SubCenterArcFace,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (TemporalAttentionPooling,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {})),
    (TemporalAvgPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TemporalConcatPooling,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TemporalDropLastWrapper,
     lambda: ([], {'net': torch.nn.ReLU()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TemporalLastPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TemporalMaxPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TripletLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (TripletLossV2,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (TripletPairwiseEmbeddingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (UpDoubleConv,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (VAE,
     lambda: ([], {'in_features': 4, 'hid_features': 4}),
     lambda: ([torch.rand([4, 4])], {})),
    (WARPLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {})),
    (WingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (cSE,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (sSE,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (scSE,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
]


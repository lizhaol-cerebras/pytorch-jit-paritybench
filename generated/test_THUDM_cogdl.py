import sys
_module = sys.modules[__name__]
del sys
cogdl = _module
attack = _module
base = _module
injection = _module
fgsm = _module
pgd = _module
rand = _module
speit = _module
tdgia = _module
modification = _module
dice = _module
fga = _module
flip = _module
nea = _module
pgd = _module
prbcd = _module
stack = _module
configs = _module
data = _module
batch = _module
data = _module
dataloader = _module
dataset = _module
sampler = _module
datasets = _module
customized_data = _module
gatne = _module
gcc_data = _module
geom_data = _module
grb_data = _module
gtn_data = _module
han_data = _module
kg_data = _module
matlab_matrix = _module
oagbert_data = _module
ogb = _module
planetoid_data = _module
rd2cd_data = _module
rec_data = _module
saint_data = _module
tu_data = _module
experiments = _module
layers = _module
actgcn_layer = _module
actgcnii_layer = _module
actlinear_layer = _module
actmlp_layer = _module
actsage_layer = _module
base_layer = _module
deepergcn_layer = _module
disengcn_layer = _module
gat_layer = _module
gcn_layer = _module
gcnii_layer = _module
gin_layer = _module
gine_layer = _module
han_layer = _module
jittor = _module
mixhop_layer = _module
mlp_layer = _module
pprgo_layer = _module
reversible_layer = _module
rgcn_layer = _module
sage_layer = _module
saint_layer = _module
se_layer = _module
set2set = _module
sgc_layer = _module
loggers = _module
base_logger = _module
tensorboard_logger = _module
wandb_logger = _module
models = _module
base_model = _module
defense = _module
gcnsvd = _module
gnnguard = _module
robustgcn = _module
emb = _module
complex = _module
deepwalk = _module
dgk = _module
distmult = _module
dngr = _module
gatne = _module
graph2vec = _module
grarep = _module
hin2vec = _module
hope = _module
knowledge_base = _module
line = _module
metapath2vec = _module
netmf = _module
netsmf = _module
node2vec = _module
prone = _module
pronepp = _module
pte = _module
rotate = _module
sdne = _module
spectral = _module
transe = _module
nn = _module
actgcn = _module
agc = _module
autognn = _module
compgcn = _module
correct_smooth = _module
daegc = _module
deepergcn = _module
dgi = _module
diffpool = _module
disengcn = _module
drgat = _module
drgcn = _module
dropedge_gcn = _module
gae = _module
gat = _module
gcc_model = _module
gcn = _module
gcnii = _module
gcnmix = _module
gdc_gcn = _module
gin = _module
grace = _module
grand = _module
graph_unet = _module
graphsage = _module
graphsaint = _module
gtn = _module
han = _module
infograph = _module
m3s = _module
mixhop = _module
mlp = _module
moe_gcn = _module
mvgrl = _module
patchy_san = _module
ppnp = _module
pprgo = _module
revgcn = _module
rgcn = _module
sagn = _module
sgc = _module
sign = _module
sortpool = _module
srgcn = _module
oag = _module
bert_model = _module
dual_position_bert_model = _module
oagbert = _module
oagbert_metainfo = _module
utils = _module
operators = _module
edge_softmax = _module
fused_gat = _module
jt_spmm = _module
linear = _module
mhspmm = _module
ops = _module
sample = _module
scatter_max = _module
spmm = _module
options = _module
pipelines = _module
trainer = _module
controller = _module
data_controller = _module
training_controller = _module
embed_trainer = _module
trainer = _module
trainer_utils = _module
evaluator = _module
graph_utils = _module
grb_utils = _module
index = _module
link_prediction_utils = _module
optimizer = _module
ppr_utils = _module
prone_utils = _module
rwalk = _module
sampling = _module
spmm_utils = _module
srgcn_utils = _module
transform = _module
utils = _module
wrappers = _module
data_wrapper = _module
base_data_wrapper = _module
graph_classification = _module
graph_classification_dw = _module
graph_embedding_dw = _module
infograph_dw = _module
patchy_san_dw = _module
heterogeneous = _module
heterogeneous_embedding_dw = _module
heterogeneous_gnn_dw = _module
multiplex_embedding_dw = _module
link_prediction = _module
embedding_link_prediction_dw = _module
gnn_kg_link_prediction_dw = _module
gnn_link_prediction_dw = _module
triple_link_prediction_dw = _module
node_classification = _module
cluster_dw = _module
graphsage_dw = _module
m3s_dw = _module
network_embedding_dw = _module
node_classification_dw = _module
pprgo_dw = _module
sagn_dw = _module
unsup_graphsage_dw = _module
pretraining = _module
gcc_dw = _module
default_match = _module
model_wrapper = _module
base_model_wrapper = _module
clustering = _module
agc_mw = _module
daegc_mw = _module
gae_mw = _module
graph_classification_mw = _module
graph_embedding_mw = _module
infograph_mw = _module
heterogeneous_embedding_mw = _module
heterogeneous_gnn_mw = _module
multiplex_embedding_mw = _module
embedding_link_prediction_mw = _module
gnn_kg_link_prediction_mw = _module
gnn_link_prediction_mw = _module
triple_link_prediction_mw = _module
correct_smooth_mw = _module
dgi_mw = _module
gcnmix_mw = _module
grace_mw = _module
grand_mw = _module
graphsage_mw = _module
m3s_mw = _module
mvgrl_mw = _module
network_embedding_mw = _module
node_classification_mw = _module
pprgo_mw = _module
sagn_mw = _module
self_auxiliary_mw = _module
unsup_graphsage_mw = _module
gcc_mw = _module
tools = _module
memory_moco = _module
wrapper_utils = _module
conf = _module
test_adv = _module
test_denfense = _module
test_injection = _module
test_modification = _module
VRGCN = _module
dataloder = _module
main = _module
custom_dataset = _module
custom_gcn = _module
custom_triple_dataset = _module
cv_search = _module
gnn = _module
logger = _module
dgi = _module
gat = _module
gcn = _module
gin = _module
grand = _module
graphsage = _module
mixhop = _module
mlp = _module
sgc = _module
sign = _module
dgraphfin = _module
evaluator = _module
utils = _module
run_gcc = _module
generate_emb = _module
calculate_paper_similarity = _module
generate_title = _module
oagbert = _module
oagbert_encode_paper = _module
oagbert_metainfo_zh = _module
oagbert_metainfo_zh_similarity = _module
gnn = _module
gnn = _module
pipeline = _module
chebnet = _module
dgcnn = _module
gat = _module
gcn = _module
unet = _module
quick_start = _module
recommendation = _module
conv = _module
run = _module
display_data = _module
download = _module
train = _module
setup = _module
test_customized_data = _module
test_data = _module
test_gcc_data = _module
test_geom_data = _module
test_kg_data = _module
test_matlab_data = _module
test_oagbert_data = _module
test_ogb = _module
test_planetoid = _module
test_rd2cd_data = _module
test_rec_data = _module
test_saint_data = _module
test_deepwalk = _module
test_contrastive_models = _module
test_generative_models = _module
test_attack_defense = _module
test_attributed_graph_clustering = _module
test_encode_paper = _module
test_graph_classification = _module
test_heterogeneous_node_classification = _module
test_link_prediction = _module
test_multiplex_link_prediction = _module
test_node_classification = _module
test_triple_link_prediction = _module
test_unsupervised_graph_classification = _module
test_unsupervised_node_classification = _module
test_args = _module
test_experiments = _module
test_layers = _module
test_oag = _module
test_ops = _module
test_options = _module
test_pipelines = _module
test_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from abc import ABCMeta


from abc import abstractmethod


import random


import time


import numpy as np


import scipy.sparse as sp


import torch


import torch.nn.functional as F


import torch.nn as nn


import math


from typing import Tuple


import re


import copy


import torch.utils.data


from torch.utils.data.dataloader import default_collate


import collections


from itertools import repeat


from typing import List


import inspect


from sklearn.preprocessing import StandardScaler


from collections import defaultdict


import scipy.io as sio


from itertools import product


import scipy.io


from torch import Tensor


import itertools


from collections import namedtuple


import warnings


import torch.multiprocessing as mp


from typing import Optional


from torch.utils.checkpoint import checkpoint


from torch.utils.checkpoint import get_device_states


from torch.utils.checkpoint import set_device_states


from torch import nn


from typing import Type


from typing import Any


import types


from scipy.sparse import lil_matrix


from sklearn.metrics.pairwise import cosine_similarity


from sklearn.preprocessing import normalize


from sklearn import preprocessing


from torch.nn.parameter import Parameter


from sklearn.cluster import SpectralClustering


from functools import partial


from scipy.linalg import block_diag


from torch.nn.modules.module import Module


from scipy.linalg import expm


import functools


import logging


from torch.utils import checkpoint


from torch.nn import Module


import torch.nn.init as init


from torch.nn import CrossEntropyLoss


from torch.utils.cpp_extension import load


import matplotlib.cm as cm


import matplotlib.pyplot as plt


import torch.distributed as dist


from torch.nn.parallel import DistributedDataParallel


from torch.cuda.amp import GradScaler


from torch.cuda.amp import autocast


from typing import Dict


from typing import Union


from typing import Callable


from sklearn.metrics import f1_score


import scipy


from functools import reduce


from scipy.special import iv


from torch.utils.data import DataLoader


import scipy.sparse.linalg as slinalg


from scipy.sparse import linalg


from sklearn.model_selection import StratifiedKFold


import scipy.sparse as sparse


import sklearn.preprocessing as preprocessing


from torch.utils.data import Sampler


from torch.utils.data import BatchSampler


from sklearn.cluster import KMeans


from sklearn.linear_model import LogisticRegression


from sklearn.metrics import auc


from sklearn.metrics import precision_recall_curve


from sklearn.metrics import roc_auc_score


from sklearn.multiclass import OneVsRestClassifier


from sklearn.metrics import accuracy_score


from sklearn.utils import shuffle as skshuffle


from sklearn.model_selection import GridSearchCV


from sklearn.model_selection import KFold


from sklearn.svm import SVC


from sklearn.metrics.cluster import normalized_mutual_info_score


from scipy.optimize import linear_sum_assignment


import pandas as pd


class linear(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input, weight, bias=None, scheme=None, rp_ratio=2):
        if rp_ratio > 1:
            D = input.shape[1]
            rmat = (torch.bernoulli(torch.ones((D, D // rp_ratio)) * 0.5) * 2.0 - 1) * math.sqrt(1.0 / (D // rp_ratio))
            input_rp = torch.mm(input, rmat)
            quantized = quantize_activation(input_rp, scheme)
        else:
            quantized = quantize_activation(input, scheme)
        empty_cache(config.empty_cache_threshold)
        ctx.scheme = scheme
        if rp_ratio > 1:
            ctx.saved = quantized, weight, bias, rmat
            ctx.other_args = input_rp.shape
        else:
            ctx.saved = quantized, weight, bias
            ctx.other_args = input.shape
        return F.linear(input, weight, bias)

    @staticmethod
    def backward(ctx, grad_output):
        if ctx.scheme:
            ctx.scheme.set_scale(grad_output)
        q_input_shape = ctx.other_args
        if len(ctx.saved) == 4:
            quantized, weight, bias, rmat = ctx.saved
            input_rp = dequantize_activation(quantized, q_input_shape)
            input = torch.mm(input_rp, rmat.t())
            del quantized, ctx.saved, input_rp
        else:
            quantized, weight, bias = ctx.saved
            input = dequantize_activation(quantized, q_input_shape)
            del quantized, ctx.saved
        empty_cache(config.empty_cache_threshold)
        C_in = input.shape[-1]
        C_out = grad_output.shape[-1]
        grad_output_flatten = grad_output.view(-1, C_out)
        input_flatten = input.view(-1, C_in)
        grad_input = grad_output_flatten.mm(weight)
        grad_weight = grad_output_flatten.t().mm(input_flatten)
        if bias is not None:
            grad_bias = grad_output_flatten.sum(0)
        else:
            grad_bias = None
        if ctx.scheme:
            ctx.scheme.if_allocate_perlayer()
        return grad_input, grad_weight, grad_bias, None, None


class QLinear(nn.Linear):
    num_layers = 0

    def __init__(self, input_features, output_features, bias=True, group=0, rp_ratio=2):
        super(QLinear, self).__init__(input_features, output_features, bias)
        if config.adaptive_conv_scheme:
            self.scheme = QScheme(self, group=group)
        else:
            self.scheme = None
        self.rp_ratio = rp_ratio

    def forward(self, input):
        if config.training:
            return linear.apply(input, self.weight, self.bias, self.scheme, self.rp_ratio)
        else:
            return super(QLinear, self).forward(input)


CONFIGS = {'fast_spmm': None, 'csrmhspmm': None, 'csr_edge_softmax': None, 'fused_gat_func': None, 'fast_spmm_cpu': None, 'spmm_flag': False, 'mh_spmm_flag': False, 'fused_gat_flag': False, 'spmm_cpu_flag': False}


def initialize_spmm():
    if CONFIGS['spmm_flag']:
        return
    CONFIGS['spmm_flag'] = True
    if torch.cuda.is_available():
        CONFIGS['fast_spmm'] = csrspmm


def initialize_spmm_cpu():
    if CONFIGS['spmm_cpu_flag']:
        return
    CONFIGS['spmm_cpu_flag'] = True
    CONFIGS['fast_spmm_cpu'] = spmm_cpu


def spmm_scatter(row, col, values, b):
    """
    Args:
        (row, col): Tensor, shape=(2, E)
        values : Tensor, shape=(E,)
        b : Tensor, shape=(N, d)
    """
    output = b.index_select(0, col) * values.unsqueeze(-1)
    output = torch.zeros_like(b).scatter_add_(0, row.unsqueeze(-1).expand_as(output), output)
    return output


def spmm(graph, x, actnn=False, fast_spmm=None, fast_spmm_cpu=None):
    if hasattr(graph, 'grb_adj') and graph.grb_adj is not None:
        if graph.grb_adj.is_sparse:
            x = torch.sparse.mm(graph.grb_adj, x)
        else:
            x = torch.mm(graph.grb_adj, x)
        return x
    if fast_spmm is None:
        initialize_spmm()
        fast_spmm = CONFIGS['fast_spmm']
    if fast_spmm_cpu is None:
        initialize_spmm_cpu()
        fast_spmm_cpu = CONFIGS['fast_spmm_cpu']
    if fast_spmm is not None and str(x.device) != 'cpu':
        if graph.out_norm is not None:
            x = graph.out_norm * x
        row_ptr, col_indices = graph.row_indptr, graph.col_indices
        csr_data = graph.raw_edge_weight
        if x.dtype == torch.half:
            csr_data = csr_data.half()
        x = fast_spmm(row_ptr.int(), col_indices.int(), x, csr_data, graph.is_symmetric(), actnn=actnn)
        if graph.in_norm is not None:
            x = graph.in_norm * x
    elif fast_spmm_cpu is not None and str(x.device) == 'cpu' and x.requires_grad is False:
        if graph.out_norm is not None:
            x = graph.out_norm * x
        row_ptr, col_indices = graph.row_indptr, graph.col_indices
        csr_data = graph.raw_edge_weight
        x = fast_spmm_cpu(row_ptr.int(), col_indices.int(), csr_data, x)
        if graph.in_norm is not None:
            x = graph.in_norm * x
    else:
        row, col = graph.edge_index
        x = spmm_scatter(row, col, graph.edge_weight, x)
    return x


class ActGCNLayer(nn.Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, dropout=0.0, activation=None, residual=False, norm=None, bias=True, rp_ratio=1):
        super(ActGCNLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.linear = QLinear(in_features, out_features, bias=bias, rp_ratio=rp_ratio)
        if dropout > 0:
            self.dropout = QDropout(dropout)
        else:
            self.dropout = None
        if residual:
            self.residual = QLinear(in_features, out_features, rp_ratio=rp_ratio)
        else:
            self.residual = None
        if activation is not None:
            self.act = QReLU()
        else:
            self.act = None
        if norm is not None:
            if norm == 'batchnorm':
                self.norm = QBatchNorm1d(out_features)
            else:
                raise NotImplementedError
        else:
            self.norm = None
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.out_features)
        torch.nn.init.uniform_(self.linear.weight, -stdv, stdv)

    def forward(self, graph, x):
        support = self.linear(x)
        out = spmm(graph, support, actnn=True)
        if self.norm is not None:
            out = self.norm(out)
        if self.act is not None:
            out = self.act(out)
        if self.residual is not None:
            out = out + self.residual(x)
        if self.dropout is not None:
            out = self.dropout(out)
        return out


class ActGCNIILayer(nn.Module):

    def __init__(self, n_channels, alpha=0.1, beta=1, residual=False):
        super(ActGCNIILayer, self).__init__()
        self.n_channels = n_channels
        self.alpha = alpha
        self.beta = beta
        self.residual = residual
        self.linear = QLinear(n_channels, n_channels)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.n_channels)
        torch.nn.init.uniform_(self.linear.weight, -stdv, stdv)

    def forward(self, graph, x, init_x):
        """Symmetric normalization"""
        hidden = spmm(graph, x, actnn=True)
        hidden = (1 - self.alpha) * hidden + self.alpha * init_x
        h = self.beta * self.linear(hidden) + (1 - self.beta) * hidden
        if self.residual:
            h = h + x
        return h


class ActMLP(nn.Module):
    """Multilayer perception with normalization

    .. math::
        x^{(i+1)} = \\sigma(W^{i}x^{(i)})

    Parameters
    ----------
    in_feats : int
        Size of each input sample.
    out_feats : int
        Size of each output sample.
    hidden_dim : int
        Size of hidden layer dimension.
    use_bn : bool, optional
        Apply batch normalization if True, default: `True).
    """

    def __init__(self, in_feats, out_feats, hidden_size, num_layers, dropout=0.0, activation='relu', norm=None, act_first=False, bias=True):
        super(ActMLP, self).__init__()
        self.norm = norm
        if activation == 'relu':
            self.activation = QReLU()
        else:
            self.activation = lambda x: x
        self.act_first = act_first
        if dropout > 0.0:
            self.dropout = QDropout(dropout)
        else:
            self.dropout = None
        shapes = [in_feats] + [hidden_size] * (num_layers - 1) + [out_feats]
        self.mlp = nn.ModuleList([QLinear(shapes[layer], shapes[layer + 1], bias=bias) for layer in range(num_layers)])
        if norm is not None and num_layers > 1:
            if norm == 'batchnorm':
                self.norm_list = nn.ModuleList(QBatchNorm1d(x) for x in shapes[1:-1])
            else:
                raise NotImplementedError(f'{norm} is not implemented in CogDL.')
        self.reset_parameters()

    def reset_parameters(self):
        for layer in self.mlp:
            layer.reset_parameters()
        if hasattr(self, 'norm_list'):
            for n in self.norm_list:
                n.reset_parameters()

    def forward(self, x):
        for i, fc in enumerate(self.mlp[:-1]):
            x = fc(x)
            if self.act_first:
                x = self.activation(x)
            if self.norm:
                x = self.norm_list[i](x)
            if not self.act_first:
                x = self.activation(x)
            if self.dropout is not None:
                x = self.dropout(x)
        x = self.mlp[-1](x)
        return x


class MeanAggregator(object):

    def __call__(self, graph, x):
        graph.row_norm()
        x = spmm(graph, x)
        return x


class SumAggregator(object):

    def __call__(self, graph, x):
        x = spmm(graph, x)
        return x


class ActSAGELayer(nn.Module):

    def __init__(self, in_feats, out_feats, normalize=False, aggr='mean', dropout=0.0, norm=None, activation=None):
        super(ActSAGELayer, self).__init__()
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.fc = QLinear(2 * in_feats, out_feats)
        self.normalize = normalize
        self.dropout = dropout
        if aggr == 'mean':
            self.aggr = MeanAggregator()
        elif aggr == 'sum':
            self.aggr = SumAggregator()
        else:
            raise NotImplementedError
        if dropout > 0:
            self.dropout = QDropout(dropout)
        else:
            self.dropout = None
        if activation is not None:
            self.act = QReLU()
        else:
            self.act = None
        if norm is not None:
            if norm == 'batchnorm':
                self.norm = QBatchNorm1d(out_feats)
            else:
                raise NotImplementedError
        else:
            self.norm = None

    def forward(self, graph, x):
        out = self.aggr(graph, x)
        out = torch.cat([x, out], dim=-1)
        out = self.fc(out)
        if self.normalize:
            out = F.normalize(out, p=2.0, dim=-1)
        if self.norm is not None:
            out = self.norm(out)
        if self.act is not None:
            out = self.act(out)
        if self.dropout is not None:
            out = self.dropout(out)
        return out


class BaseLayer(nn.Module):

    def __init__(self, **kwargs) ->None:
        super().__init__(**kwargs)

    def forward(self, graph, x):
        m = self.message(x[graph.edge_index[0]])
        return self.aggregate(graph, m)

    def message(self, x):
        return x

    def aggregate(self, graph, x):
        result = torch.zeros(graph.num_nodes, x.shape[1], dtype=x.dtype)
        result.scatter_add_(0, graph.edge_index[1].unsqueeze(1).expand(-1, x.shape[1]), x)
        return result


class BondEncoder(nn.Module):

    def __init__(self, bond_dim_list, emb_size):
        super(BondEncoder, self).__init__()
        self.bond_emb_list = nn.ModuleList()
        for i, size in enumerate(bond_dim_list):
            x = nn.Embedding(size, emb_size)
            self.bond_emb_list.append(x)

    def forward(self, edge_attr):
        out = 0
        for i in range(edge_attr.shape[1]):
            out += self.bond_emb_list[i](edge_attr[:, i])
        return out


class EdgeEncoder(nn.Module):

    def __init__(self, in_feats, out_feats, bias=False):
        super(EdgeEncoder, self).__init__()
        self.nn = nn.Linear(in_feats, out_feats, bias=bias)

    def forward(self, edge_attr):
        return self.nn(edge_attr)


class BaseModel(nn.Module):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        pass

    @classmethod
    def build_model_from_args(cls, args):
        """Build a new model instance."""
        raise NotImplementedError('Models must implement the build_model_from_args method')

    def __init__(self):
        super(BaseModel, self).__init__()
        self.model_name = self.__class__.__name__
        self.loss_fn = None
        self.evaluator = None

    def _forward_unimplemented(self, *input: Any) ->None:
        pass

    def forward(self, *args):
        raise NotImplementedError

    def predict(self, data):
        return self.forward(data)

    @property
    def device(self):
        return next(self.parameters()).device

    def set_loss_fn(self, loss_fn):
        self.loss_fn = loss_fn


class BaseGraph(object):

    def __init__(self):
        pass

    def eval(self):
        pass

    def train(self):
        pass

    def __getitem__(self, key):
        """Gets the data of the attribute :obj:`key`."""
        return getattr(self, key)

    def __setitem__(self, key, value):
        """Sets the attribute :obj:`key` to :obj:`value`."""
        setattr(self, key, value)

    @property
    def keys(self):
        """Returns all names of graph attributes."""
        keys = [key for key in self.__dict__.keys() if self[key] is not None]
        keys = [key for key in keys if key[:2] != '__' and key[-2:] != '__']
        return keys

    def __len__(self):
        """Returns the number of all present attributes."""
        return 1

    def __contains__(self, key):
        """Returns :obj:`True`, if the attribute :obj:`key` is present in the
        data."""
        return key in self.keys

    def __iter__(self):
        """Iterates over all present attributes in the data, yielding their
        attribute names and content."""
        for key in sorted(self.keys):
            yield key, self[key]

    def __call__(self, *keys):
        """Iterates over all attributes :obj:`*keys` in the data, yielding
        their attribute names and content.
        If :obj:`*keys` is not given this method will iterative over all
        present attributes."""
        for key in (sorted(self.keys) if not keys else keys):
            if self[key] is not None:
                yield key, self[key]

    def cat_dim(self, key, value):
        """Returns the dimension in which the attribute :obj:`key` with
        content :obj:`value` gets concatenated when creating batches.

        .. note::

            This method is for internal use only, and should only be overridden
            if the batch concatenation process is corrupted for a specific data
            attribute.
        """
        return -1 if bool(re.search('(index|face)', key)) else 0

    def __inc__(self, key, value):
        """ "Returns the incremental count to cumulatively increase the value
        of the next attribute of :obj:`key` when creating batches.

        .. note::

            This method is for internal use only, and should only be overridden
            if the batch concatenation process is corrupted for a specific data
            attribute.
        """
        return self.__num_nodes__ if bool(re.search('(index|face)', key)) else 0

    def __cat_dim__(self, key, value=None):
        return self.cat_dim(key, value)

    def apply(self, func, *keys):
        """Applies the function :obj:`func` to all attributes :obj:`*keys`.
        If :obj:`*keys` is not given, :obj:`func` is applied to all present
        attributes.
        """
        for key, item in self(*keys):
            if isinstance(item, Adjacency):
                self[key] = func(item)
            if not isinstance(item, torch.Tensor):
                continue
            self[key] = func(item)
        return self

    def contiguous(self, *keys):
        """Ensures a contiguous memory layout for all attributes :obj:`*keys`.
        If :obj:`*keys` is not given, all present attributes are ensured to
        have a contiguous memory layout."""
        return self.apply(lambda x: x.contiguous(), *keys)

    def to(self, device, *keys):
        """Performs tensor dtype and/or device conversion to all attributes
        :obj:`*keys`.
        If :obj:`*keys` is not given, the conversion is applied to all present
        attributes."""
        return self.apply(lambda x: x, *keys)

    def cuda(self, *keys):
        return self.apply(lambda x: x, *keys)


class RandomWalker(object):

    def __init__(self, adj=None, num_nodes=None):
        if adj is None:
            self.indptr = None
            self.indices = None
        else:
            if isinstance(adj, torch.Tensor):
                if num_nodes is None:
                    num_nodes = int(torch.max(adj)) + 1
                row, col = adj.cpu().numpy()
                data = np.ones(row.shape[0])
                adj = sp.csr_matrix((data, (row, col)), shape=(num_nodes, num_nodes))
            adj = adj.tocsr()
            self.indptr = adj.indptr
            self.indices = adj.indices

    def build_up(self, adj, num_nodes):
        if self.indptr is not None:
            return
        if isinstance(adj, torch.Tensor) or isinstance(adj, tuple):
            row, col = adj
            if num_nodes is None:
                num_nodes = int(max(row.max(), col.max())) + 1
            row, col = row.cpu().numpy(), col.cpu().numpy()
            data = np.ones(row.shape[0])
            adj = sp.csr_matrix((data, (row, col)), shape=(num_nodes, num_nodes))
        adj = adj.tocsr()
        self.indptr = adj.indptr
        self.indices = adj.indices

    def walk(self, start, walk_length, restart_p=0.0, parallel=True):
        assert self.indptr is not None, 'Please build the adj_list first'
        if isinstance(start, torch.Tensor):
            start = start.cpu().numpy()
        if isinstance(start, list):
            start = np.asarray(start, dtype=np.int32)
        if parallel:
            result = random_walk_parallel(start, walk_length, self.indptr, self.indices, restart_p)
        else:
            result = random_walk_single(start, walk_length, self.indptr, self.indices, restart_p)
        result = np.array(result, dtype=np.int64)
        return result

    def walk_one(self, start, length, p):
        walk_res = [np.zeros(length, dtype=np.int32)] * len(start)
        p = 0.0
        for i in range(len(start)):
            node = start[i]
            result = [np.int32(0)] * length
            index = np.int32(0)
            _node = node
            while index < length:
                start1 = self.indptr[node]
                end1 = self.indptr[node + 1]
                sample1 = random.randint(start1, end1 - 1)
                node = self.indices[sample1]
                if np.random.uniform(0, 1) > p:
                    result[index] = node
                else:
                    result[index] = _node
                index += 1
            k = int(np.floor(np.random.rand() * len(result)))
            walk_res[i] = result[k]
        return walk_res


def sorted_coo2csr(row, col, data, num_nodes=None, return_index=False):
    indptr = torch.bincount(row)
    indptr = indptr.cumsum(dim=0)
    zero = torch.zeros(1, device=indptr.device)
    indptr = torch.cat([zero, indptr])
    if return_index:
        return indptr, torch.arange(0, row.shape[0])
    return indptr, col, data


def _coo2csr(edge_index, data, num_nodes=None, ordered=False, return_index=False):
    if ordered:
        return sorted_coo2csr(edge_index[0], edge_index[1], data, return_index=return_index)
    if num_nodes is None:
        num_nodes = torch.max(edge_index) + 1
    device = edge_index[0].device
    sorted_index = torch.argsort(edge_index[0])
    sorted_index = sorted_index.long()
    edge_index = edge_index[:, sorted_index]
    indices = edge_index[1]
    row = edge_index[0]
    indptr = torch.zeros(num_nodes + 1, dtype=torch.int32, device=device)
    elements, counts = torch.unique(row, return_counts=True)
    elements = elements.long() + 1
    indptr[elements] = counts
    indptr = indptr.cumsum(dim=0)
    if return_index:
        return indptr, sorted_index
    if data is not None:
        data = data[sorted_index]
    return indptr, indices, data


def coo2csr_index(row, col, num_nodes=None):
    if num_nodes is None:
        num_nodes = torch.max(torch.stack([row, col])).item() + 1
    if coo2csr_cpu_index is None:
        return _coo2csr(torch.stack([row, col]), None, num_nodes=num_nodes, return_index=True)
    device = row.device
    row = row.long().cpu()
    col = col.long().cpu()
    indptr, reindex = coo2csr_cpu_index(row, col, num_nodes)
    return indptr, reindex


def csr2coo(indptr, indices, data):
    num_nodes = indptr.size(0) - 1
    row = torch.arange(num_nodes, device=indptr.device)
    row_count = indptr[1:] - indptr[:-1]
    row = row.repeat_interleave(row_count)
    return row, indices, data


def get_degrees(row, col, num_nodes=None):
    device = row.device
    if num_nodes is None:
        num_nodes = max(row.max().item(), col.max().item()) + 1
    b = torch.ones(col.shape[0], device=device)
    out = torch.zeros(num_nodes, device=device)
    degrees = out.scatter_add_(dim=0, index=row, src=b)
    return degrees.float()


def row_normalization(num_nodes, row, col, val=None):
    device = row.device
    if val is None:
        val = torch.ones(row.shape[0], device=device)
    row_sum = get_degrees(row, col, num_nodes)
    row_sum_inv = row_sum.pow(-1).view(-1)
    row_sum_inv[torch.isinf(row_sum_inv)] = 0
    return val * row_sum_inv[row]


def symmetric_normalization(num_nodes, row, col, val=None):
    device = row.device
    if val is None:
        val = torch.ones(row.shape[0])
    row_sum = get_degrees(row, col, num_nodes)
    row_sum_inv_sqrt = row_sum.pow(-0.5)
    row_sum_inv_sqrt[row_sum_inv_sqrt == float('inf')] = 0
    return row_sum_inv_sqrt[col] * val * row_sum_inv_sqrt[row]


class Adjacency(BaseGraph):

    def __init__(self, row=None, col=None, row_ptr=None, weight=None, attr=None, num_nodes=None, types=None, **kwargs):
        super(Adjacency, self).__init__()
        self.row = row
        self.col = col
        self.row_ptr = row_ptr
        self.weight = weight
        self.attr = attr
        self.types = types
        self.__num_nodes__ = num_nodes
        self.__normed__ = None
        self.__in_norm__ = self.__out_norm__ = None
        self.__symmetric__ = True
        for key, item in kwargs.items():
            self[key] = item

    def set_weight(self, weight):
        self.weight = weight
        self.__normed__ = None
        self.__in_norm__ = self.__out_norm__ = None
        self.__symmetric__ = False

    def get_weight(self, indicator=None):
        """If `indicator` is not None, the normalization will not be implemented"""
        if self.weight is None or self.weight.shape[0] != self.col.shape[0]:
            self.weight = torch.ones(self.num_edges, device=self.device)
        weight = self.weight
        if indicator is not None:
            return weight
        if self.__in_norm__ is not None:
            if self.row is None:
                num_nodes = self.row_ptr.size(0) - 1
                row = torch.arange(num_nodes, device=self.device)
                row_count = self.row_ptr[1:] - self.row_ptr[:-1]
                self.row = row.repeat_interleave(row_count)
            weight = self.__in_norm__[self.row].view(-1)
        if self.__out_norm__ is not None:
            weight = self.__out_norm__[self.col].view(-1)
        return weight

    def add_remaining_self_loops(self):
        if self.attr is not None and len(self.attr.shape) == 1:
            edge_index, weight_attr = add_remaining_self_loops((self.row, self.col), edge_weight=self.attr, fill_value=0, num_nodes=self.num_nodes)
            self.row, self.col = edge_index
            self.attr = weight_attr
            self.weight = torch.ones_like(self.row).float()
        else:
            edge_index, self.weight = add_remaining_self_loops((self.row, self.col), fill_value=1, num_nodes=self.num_nodes)
            self.row, self.col = edge_index
            self.attr = None
        self.row_ptr, reindex = coo2csr_index(self.row, self.col, num_nodes=self.num_nodes)
        self.row = self.row[reindex]
        self.col = self.col[reindex]

    def padding_self_loops(self):
        device = self.row.device
        row, col = torch.arange(self.num_nodes, device=device), torch.arange(self.num_nodes, device=device)
        self.row = torch.cat((self.row, row))
        self.col = torch.cat((self.col, col))
        if self.weight is not None:
            values = torch.zeros(self.num_nodes, device=device) + 0.01
            self.weight = torch.cat((self.weight, values))
        if self.attr is not None:
            attr = torch.zeros(self.num_nodes, device=device)
            self.attr = torch.cat((self.attr, attr))
        self.row_ptr, reindex = coo2csr_index(self.row, self.col, num_nodes=self.num_nodes)
        self.row = self.row[reindex]
        self.col = self.col[reindex]

    def remove_self_loops(self):
        mask = self.row == self.col
        inv_mask = ~mask
        self.row = self.row[inv_mask]
        self.col = self.col[inv_mask]
        for item in self.__attr_keys__():
            if self[item] is not None:
                self[item] = self[item][inv_mask]
        self.convert_csr()

    def sym_norm(self):
        if self.row is None:
            self.generate_normalization('sym')
        else:
            self.normalize_adj('sym')

    def row_norm(self):
        if self.row is None:
            self.generate_normalization('row')
        else:
            self.normalize_adj('row')
            self.__symmetric__ = False

    def col_norm(self):
        if self.row is None:
            self.generate_normalization('col')
        else:
            self.normalize_adj('col')
            self.__symmetric__ = False

    def generate_normalization(self, norm='sym'):
        if self.__normed__:
            return
        degrees = (self.row_ptr[1:] - self.row_ptr[:-1]).float()
        if norm == 'sym':
            edge_norm = torch.pow(degrees, -0.5)
            edge_norm[torch.isinf(edge_norm)] = 0
            self.__out_norm__ = self.__in_norm__ = edge_norm.view(-1, 1)
        elif norm == 'row':
            edge_norm = torch.pow(degrees, -1)
            edge_norm[torch.isinf(edge_norm)] = 0
            self.__out_norm__ = None
            self.__in_norm__ = edge_norm.view(-1, 1)
        elif norm == 'col':
            self.row, _, _ = csr2coo(self.row_ptr, self.col, self.weight)
            self.weight = row_normalization(self.num_nodes, self.col, self.row, self.weight)
        else:
            raise NotImplementedError
        self.__normed__ = norm

    def normalize_adj(self, norm='sym'):
        if self.__normed__:
            return
        if self.weight is None or self.weight.shape[0] != self.col.shape[0]:
            self.weight = torch.ones(self.num_edges, device=self.device)
        if norm == 'sym':
            self.weight = symmetric_normalization(self.num_nodes, self.row, self.col, self.weight)
        elif norm == 'row':
            self.weight = row_normalization(self.num_nodes, self.row, self.col, self.weight)
        elif norm == 'col':
            self.weight = row_normalization(self.num_nodes, self.col, self.row, self.weight)
        else:
            raise NotImplementedError
        self.__normed__ = norm

    def convert_csr(self):
        self._to_csr()

    def _to_csr(self):
        self.row_ptr, reindex = coo2csr_index(self.row, self.col, num_nodes=self.num_nodes)
        self.col = self.col[reindex]
        self.row = self.row[reindex]
        for key in self.__attr_keys__():
            if key == 'weight' and self[key] is None:
                self.weight = torch.ones(self.row.shape[0])
            if self[key] is not None:
                self[key] = self[key][reindex]

    def is_symmetric(self):
        return self.__symmetric__

    def set_symmetric(self, val):
        assert val in [True, False]
        self.__symmetric__ = val

    def degrees(self, node_idx=None):
        if self.row_ptr is not None:
            degs = (self.row_ptr[1:] - self.row_ptr[:-1]).float()
            if node_idx is not None:
                return degs[node_idx]
            return degs
        else:
            return get_degrees(self.row, self.col, num_nodes=self.num_nodes)

    @property
    def edge_index(self):
        if self.row is None:
            self.row, _, _ = csr2coo(self.row_ptr, self.col, self.weight)
        return self.row, self.col

    @edge_index.setter
    def edge_index(self, edge_index):
        row, col = edge_index
        self.row, self.col = row, col
        self.row_ptr = None

    @property
    def row_indptr(self):
        if self.row_ptr is None:
            self._to_csr()
        return self.row_ptr

    @property
    def num_edges(self):
        if self.row is not None:
            return self.row.shape[0]
        elif self.row_ptr is not None:
            return self.row_ptr[-1]
        else:
            return None

    @property
    def num_nodes(self):
        if self.__num_nodes__ is not None:
            return self.__num_nodes__
        if self.row_ptr is not None:
            return self.row_ptr.shape[0] - 1
        else:
            self.__num_nodes__ = max(self.row.max().item(), self.col.max().item()) + 1
            return self.__num_nodes__

    @property
    def row_ptr_v(self):
        return self.row_ptr

    @property
    def device(self):
        return self.row.device if self.row is not None else self.row_ptr.device

    @property
    def keys(self):
        """Returns all names of graph attributes."""
        keys = [key for key in self.__dict__.keys() if self[key] is not None]
        keys = [key for key in keys if key[:2] != '__' and key[-2:] != '__']
        return keys

    def __out_repr__(self):
        if self.row is not None:
            info = ['{}={}'.format('edge_index', [2] + list(self.row.size()))]
        else:
            info = ['{}={}'.format(key, list(self[key].size())) for key in ['row', 'col'] if self[key] is not None]
        attr_key = self.__attr_keys__()
        info += ['edge_{}={}'.format(key, list(self[key].size())) for key in attr_key if self[key] is not None]
        return info

    def __getitem__(self, item):
        assert type(item) == str, f'{item} must be str'
        if item[0] == '_' and item[1] != '_':
            item = item[1:]
        if item.startswith('edge_') and item != 'edge_index':
            item = item[5:]
        if item in self.__dict__:
            return self.__dict__[item]
        else:
            raise KeyError(f'{item} not in Adjacency')

    def __copy__(self):
        result = self.__class__()
        for key in self.keys:
            setattr(result, key, copy.copy(self[key]))
        result.__num_nodes__ = self.__num_nodes__
        return result

    def __deepcopy__(self, memodict={}):
        result = self.__class__()
        memodict[id(self)] = result
        for k in self.keys:
            v = self[k]
            setattr(result, k, copy.deepcopy(v, memodict))
        result.__num_nodes__ = self.__num_nodes__
        return result

    def __repr__(self):
        info = ['{}={}'.format(key, list(self[key].size())) for key in self.keys if not key.startswith('__') and self[key] is not None]
        return '{}({})'.format(self.__class__.__name__, ', '.join(info))

    def __attr_keys__(self):
        return [x for x in self.keys if 'row' not in x and 'col' not in x]

    def clone(self):
        return Adjacency.from_dict({k: v.clone() for k, v in self})

    def to_scipy_csr(self):
        data = self.get_weight().cpu().numpy()
        num_nodes = int(self.num_nodes)
        if self.row_ptr is None:
            row = self.row.cpu().numpy()
            col = self.col.cpu().numpy()
            mx = sp.csr_matrix((data, (row, col)), shape=(num_nodes, num_nodes))
        else:
            row_ptr = self.row_ptr.cpu().numpy()
            col_ind = self.col.cpu().numpy()
            mx = sp.csr_matrix((data, col_ind, row_ptr), shape=(num_nodes, num_nodes))
        return mx

    def to_networkx(self, weighted=True):
        gnx = nx.Graph()
        gnx.add_nodes_from(np.arange(self.num_nodes))
        row, col = self.edge_index
        row = row.tolist()
        col = col.tolist()
        if weighted:
            weight = self.get_weight().tolist()
            gnx.add_weighted_edges_from([(row[i], col[i], weight[i]) for i in range(len(row))])
        else:
            edges = torch.stack((row, col)).cpu().numpy().transpose()
            gnx.add_edges_from(edges)
        return gnx

    def random_walk(self, seeds, length=1, restart_p=0.0, parallel=True):
        if not hasattr(self, '__walker__'):
            scipy_adj = self.to_scipy_csr()
            self.__walker__ = RandomWalker(scipy_adj)
        return self.__walker__.walk(seeds, length, restart_p=restart_p, parallel=parallel)

    @staticmethod
    def from_dict(dictionary):
        """Creates a data object from a python dictionary."""
        data = Adjacency()
        for key, item in dictionary.items():
            data[key] = item
        return data


EDGE_ATTR = 'edge_attr'


EDGE_INDEX = 'edge_index'


def is_adj_key(key):
    return key in ['row', 'col', 'row_ptr', 'attr', 'weight', 'types'] or key.startswith('edge_')


EDGE_WEIGHT = 'edge_weight'


def is_read_adj_key(key):
    return sum([(x in key) for x in [EDGE_INDEX, EDGE_WEIGHT, EDGE_ATTR]]) > 0 or is_adj_key(key)


def is_adj_key_train(key):
    return key.endswith('_train') and is_read_adj_key(key)


subgraph_c = None


class MLP(BaseModel):

    def __init__(self, in_feats, out_feats, hidden_size, num_layers, dropout=0.0, activation='relu', norm=None, act_first=False, bias=True):
        super(MLP, self).__init__()
        self.nn = MLPLayer(in_feats, out_feats, hidden_size, num_layers, dropout, activation, norm, act_first, bias)

    def reset_parameters(self):
        self.nn.reset_parameters()

    def forward(self, x):
        if isinstance(x, Graph):
            x = x.x
        return F.log_softmax(self.nn(x), dim=-1)

    def predict(self, data):
        return self.forward(data.x)


def batch_max_pooling(x, batch):
    if torch.cuda.is_available() and str(x.device) != 'cpu':
        try:
            col = torch.arange(0, len(batch))
            rowptr, colind = coo2csr_index(batch, col, num_nodes=batch.max().item() + 1)
            x = scatter_max(rowptr.int(), colind.int(), x)
            return x
        except Exception:
            pass
    x, _ = scatter_max(x, batch, dim=0)
    return x


def edge_softmax_val(graph, edge_val):
    """
    Args:
        graph: cogdl.Graph
        edge_val: torch.Tensor, shape=(E, 1)
    Returns:
        Softmax values of edge values for nodes
    """
    edge_val_max = edge_val.max().item()
    while edge_val_max > 10:
        edge_val -= edge_val / 2
        edge_val_max = edge_val.max().item()
    with graph.local_graph():
        edge_val = torch.exp(edge_val)
        graph.edge_weight = edge_val
        x = torch.ones(graph.num_nodes, 1)
        node_sum = spmm(graph, x).squeeze()
        row = graph.edge_index[0]
        softmax_values = edge_val / node_sum[row]
        return softmax_values


def initialize_edge_softmax():
    if CONFIGS['mh_spmm_flag']:
        return
    CONFIGS['mh_spmm_flag'] = True
    if torch.cuda.is_available():
        CONFIGS['csrmhspmm'] = csrmhspmm
        CONFIGS['csr_edge_softmax'] = csr_edge_softmax


def edge_softmax(graph, edge_val, csr_edge_softmax=None):
    if csr_edge_softmax is None:
        initialize_edge_softmax()
        csr_edge_softmax = CONFIGS['csr_edge_softmax']
    if csr_edge_softmax is not None and edge_val.device.type != 'cpu':
        if len(edge_val.shape) == 1:
            edge_val = edge_val.view(-1, 1)
            val = csr_edge_softmax(graph.row_indptr.int(), edge_val)
            val = val.view(-1)
        else:
            val = csr_edge_softmax(graph.row_indptr.int(), edge_val)
        return val
    else:
        val = []
        for i in range(edge_val.shape[1]):
            val.append(edge_softmax_val(graph, edge_val[:, i]))
        return torch.stack(val).t()


class GENConv(nn.Module):

    def __init__(self, in_feats: int, out_feats: int, aggr: str='softmax_sg', beta: float=1.0, p: float=1.0, learn_beta: bool=False, learn_p: bool=False, use_msg_norm: bool=False, learn_msg_scale: bool=True, norm: Optional[str]=None, residual: bool=False, activation: Optional[str]=None, num_mlp_layers: int=2, edge_attr_size: Optional[list]=None):
        super(GENConv, self).__init__()
        self.use_msg_norm = use_msg_norm
        self.mlp = MLP(in_feats, out_feats, in_feats * 2, num_layers=num_mlp_layers, activation=activation, norm=norm)
        self.message_encoder = torch.nn.ReLU()
        self.aggr = aggr
        if aggr == 'softmax_sg':
            self.beta = torch.nn.Parameter(torch.Tensor([beta]), requires_grad=learn_beta)
        else:
            self.register_buffer('beta', None)
        if aggr == 'powermean':
            self.p = torch.nn.Parameter(torch.Tensor([p]), requires_grad=learn_p)
        else:
            self.register_buffer('p', None)
        self.eps = 1e-07
        self.s = torch.nn.Parameter(torch.Tensor([1.0]), requires_grad=learn_msg_scale and use_msg_norm)
        self.residual = residual
        if edge_attr_size is not None and edge_attr_size[0] > 0:
            if len(edge_attr_size) > 1:
                self.edge_encoder = BondEncoder(edge_attr_size, in_feats)
            else:
                self.edge_encoder = EdgeEncoder(edge_attr_size[0], in_feats)
        else:
            self.edge_encoder = None

    def message_norm(self, x, msg):
        x_norm = torch.norm(x, dim=1, p=2)
        msg_norm = F.normalize(msg, p=2, dim=1)
        msg_norm = msg_norm * x_norm.unsqueeze(-1)
        return x + self.s * msg_norm

    def forward(self, graph, x):
        edge_index = graph.edge_index
        dim = x.shape[1]
        edge_msg = x[edge_index[1]]
        if self.edge_encoder is not None and graph.edge_attr is not None:
            edge_msg += self.edge_encoder(graph.edge_attr)
        edge_msg = self.message_encoder(edge_msg) + self.eps
        if self.aggr == 'softmax_sg':
            h = edge_softmax(graph, self.beta * edge_msg.contiguous())
            h = edge_msg * h
        elif self.aggr == 'softmax':
            h = edge_softmax(graph, edge_msg)
            h = edge_msg * h
        elif self.aggr == 'powermean':
            deg = graph.degrees()
            torch.clamp_(edge_msg, 1e-07, 1.0)
            h = edge_msg.pow(self.p) / deg[edge_index[0]].unsqueeze(-1)
        elif self.aggr == 'mean':
            deg = graph.degrees()
            deg_rev = deg.pow(-1)
            deg_rev[torch.isinf(deg_rev)] = 0
            h = edge_msg * deg_rev[edge_index[0]].unsqueeze(-1)
        else:
            h = edge_msg
        if self.aggr == 'max':
            h = batch_max_pooling(h, edge_index[0])
        else:
            h = torch.zeros_like(x).scatter_add_(dim=0, index=edge_index[0].unsqueeze(-1).repeat(1, dim), src=h)
        if self.aggr == 'powermean':
            h = h.pow(1.0 / self.p)
        if self.use_msg_norm:
            h = self.message_norm(x, h)
        if self.residual:
            h = h + x
        h = self.mlp(h)
        return h


def identity_act(input):
    return input


def get_activation(act: str, inplace=False):
    if act == 'relu':
        return nn.ReLU(inplace=inplace)
    elif act == 'sigmoid':
        return nn.Sigmoid()
    elif act == 'tanh':
        return nn.Tanh()
    elif act == 'gelu':
        return nn.GELU()
    elif act == 'prelu':
        return nn.PReLU()
    elif act == 'identity':
        return identity_act
    else:
        return identity_act


def get_norm_layer(norm: str, channels: int):
    """
    Args:
        norm: str
            type of normalization: `layernorm`, `batchnorm`, `instancenorm`
        channels: int
            size of features for normalization
    """
    if norm == 'layernorm':
        return torch.nn.LayerNorm(channels)
    elif norm == 'batchnorm':
        return torch.nn.BatchNorm1d(channels)
    elif norm == 'instancenorm':
        return torch.nn.InstanceNorm1d(channels)
    else:
        return torch.nn.Identity()


class ResGNNLayer(nn.Module):
    """
    Implementation of DeeperGCN in paper `"DeeperGCN: All You Need to Train Deeper GCNs" <https://arxiv.org/abs/2006.07739>`_

    Parameters
    -----------
    conv : nn.Module
        An instance of GNN Layer, recieving (graph, x) as inputs
    n_channels : int
        size of input features
    activation : str
    norm: str
        type of normalization, ``batchnorm`` as default
    dropout : float
    checkpoint_grad : bool
    """

    def __init__(self, conv, in_channels, activation='relu', norm='batchnorm', dropout=0.0, out_norm=None, out_channels=-1, residual=True, checkpoint_grad=False):
        super(ResGNNLayer, self).__init__()
        self.conv = conv
        self.activation = get_activation(activation)
        self.dropout = dropout
        self.norm = get_norm_layer(norm, in_channels)
        self.residual = residual
        if out_norm:
            self.out_norm = get_norm_layer(norm, out_channels)
        else:
            self.out_norm = None
        self.checkpoint_grad = checkpoint_grad

    def forward(self, graph, x, dropout=None, *args, **kwargs):
        h = self.norm(x)
        h = self.activation(h)
        if isinstance(dropout, float) or dropout is None:
            h = F.dropout(h, p=self.dropout, training=self.training)
        elif self.training:
            h = h * dropout
        if self.checkpoint_grad:
            h = checkpoint(self.conv, graph, h, *args, **kwargs)
        else:
            h = self.conv(graph, h, *args, **kwargs)
        if self.residual:
            h = h + x
        if self.out_norm:
            return self.out_norm(h)
        else:
            return h


class DisenGCNLayer(nn.Module):
    """
    Implementation of `"Disentangled Graph Convolutional Networks" <http://proceedings.mlr.press/v97/ma19a.html>`_.
    """

    def __init__(self, in_feats, out_feats, K, iterations, tau=1.0, activation='leaky_relu'):
        super(DisenGCNLayer, self).__init__()
        self.K = K
        self.tau = tau
        self.iterations = iterations
        self.factor_dim = int(out_feats / K)
        self.weight = nn.Parameter(torch.Tensor(in_feats, out_feats))
        self.bias = nn.Parameter(torch.Tensor(out_feats))
        self.reset_parameters()
        if activation == 'leaky_relu':
            self.activation = nn.LeakyReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        elif activation == 'prelu':
            self.activation = nn.PReLU()
        elif activation == 'relu':
            self.activation = nn.ReLU()
        else:
            raise NotImplementedError

    def reset_parameters(self):
        nn.init.xavier_normal_(self.weight.data, gain=1.414)
        nn.init.zeros_(self.bias.data)

    def forward(self, graph, x):
        num_nodes = x.shape[0]
        device = x.device
        h = self.activation(torch.matmul(x, self.weight) + self.bias)
        h = h.split(self.factor_dim, dim=-1)
        h = torch.cat([dt.unsqueeze(0) for dt in h], dim=0)
        norm = h.pow(2).sum(dim=-1).sqrt().unsqueeze(-1)
        h_normed = h / norm
        h_src = h_dst = h_normed.permute(1, 0, 2)
        add_shape = h.shape
        edge_index = graph.edge_index
        for _ in range(self.iterations):
            src_edge_attr = h_dst[edge_index[0]] * h_src[edge_index[1]]
            src_edge_attr = src_edge_attr.sum(dim=-1) / self.tau
            edge_attr_softmax = edge_softmax(graph, src_edge_attr).T
            edge_attr_softmax = edge_attr_softmax.unsqueeze(-1)
            dst_edge_attr = h_src.index_select(0, edge_index[1]).permute(1, 0, 2)
            dst_edge_attr = dst_edge_attr * edge_attr_softmax
            edge_index_ = edge_index[0].unsqueeze(-1).unsqueeze(0).repeat(self.K, 1, h.shape[-1])
            node_attr = torch.zeros(add_shape).scatter_add_(1, edge_index_, dst_edge_attr)
            node_attr = node_attr + h_normed
            node_attr_norm = node_attr.pow(2).sum(-1).sqrt().unsqueeze(-1)
            node_attr = (node_attr / node_attr_norm).permute(1, 0, 2)
            h_dst = node_attr
        h_dst = h_dst.reshape(num_nodes, -1)
        return h_dst


class EdgeSoftmax(torch.nn.Module):

    def __init__(self):
        super().__init__()
        initialize_edge_softmax()
        self.csr_edge_softmax = CONFIGS['csr_edge_softmax']

    def forward(self, graph, edge_val):
        return edge_softmax(graph, edge_val, self.csr_edge_softmax)


def mh_spmm(graph, attention, h, csrmhspmm=None, fast_spmm=None):
    if csrmhspmm is None:
        initialize_edge_softmax()
        csrmhspmm = CONFIGS['csrmhspmm']
    nhead = h.shape[1]
    if csrmhspmm is not None and h.device.type != 'cpu':
        if nhead > 1:
            h_prime = csrmhspmm(graph.row_indptr.int(), graph.col_indices.int(), h, attention)
            out = h_prime.view(h_prime.shape[0], -1)
        else:
            edge_weight = attention.view(-1)
            with graph.local_graph():
                graph.edge_weight = edge_weight
                out = spmm(graph, h.squeeze(1), fast_spmm=fast_spmm)
    else:
        with graph.local_graph():
            h_prime = []
            h = h.permute(1, 0, 2).contiguous()
            for i in range(nhead):
                edge_weight = attention[:, i]
                graph.edge_weight = edge_weight.contiguous()
                hidden = h[i]
                assert not torch.isnan(hidden).any()
                h_prime.append(spmm(graph, hidden, fast_spmm=fast_spmm))
        out = torch.cat(h_prime, dim=1)
    return out


class MultiHeadSpMM(torch.nn.Module):

    def __init__(self):
        super().__init__()
        initialize_spmm()
        initialize_edge_softmax()
        self.spmm = CONFIGS['fast_spmm']
        self.csrmhspmm = CONFIGS['csrmhspmm']

    def forward(self, graph, attention, h):
        return mh_spmm(graph, attention, h, csrmhspmm=self.csrmhspmm, fast_spmm=self.spmm)


def check_fused_gat():
    return CONFIGS['fused_gat_func'] is not None


def initialize_fused_gat():
    if CONFIGS['fused_gat_flag']:
        return
    CONFIGS['fused_gat_flag'] = True
    if torch.cuda.is_available():
        CONFIGS['fused_gat_func'] = fused_gat_func


def fused_gat_op(attn_row, attn_col, graph, negative_slope, in_feat, fused_gat_func=None):
    if fused_gat_func is None:
        initialize_fused_gat()
        fused_gat_func = CONFIGS['fused_gat_func']
    return fused_gat_func(attn_row, attn_col, graph.row_indptr.int(), graph.col_indices.int(), graph.row_indptr.int(), graph.col_indices.int(), negative_slope, in_feat)


class GATLayer(nn.Module):
    """
    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903
    """

    def __init__(self, in_feats, out_feats, nhead=1, alpha=0.2, attn_drop=0.5, activation=None, residual=False, norm=None):
        super(GATLayer, self).__init__()
        self.in_features = in_feats
        self.out_features = out_feats
        self.alpha = alpha
        self.nhead = nhead
        self.W = nn.Parameter(torch.FloatTensor(in_feats, out_feats * nhead))
        self.a_l = nn.Parameter(torch.zeros(size=(1, nhead, out_feats)))
        self.a_r = nn.Parameter(torch.zeros(size=(1, nhead, out_feats)))
        self.edge_softmax = EdgeSoftmax()
        self.mhspmm = MultiHeadSpMM()
        self.dropout = nn.Dropout(attn_drop)
        self.leakyrelu = nn.LeakyReLU(self.alpha)
        self.act = None if activation is None else get_activation(activation)
        self.norm = None if norm is None else get_norm_layer(norm, out_feats * nhead)
        if residual:
            self.residual = nn.Linear(in_feats, out_feats * nhead)
        else:
            self.register_buffer('residual', None)
        self.reset_parameters()

    def reset_parameters(self):

        def reset(tensor):
            stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))
            tensor.data.uniform_(-stdv, stdv)
        reset(self.a_l)
        reset(self.a_r)
        reset(self.W)

    def forward(self, graph, x):
        h = torch.matmul(x, self.W).view(-1, self.nhead, self.out_features)
        h[torch.isnan(h)] = 0.0
        row, col = graph.edge_index
        h_l = (self.a_l * h).sum(dim=-1)
        h_r = (self.a_r * h).sum(dim=-1)
        if self.dropout.p == 0.0 and graph.is_symmetric() and check_fused_gat():
            out = fused_gat_op(h_l, h_r, graph, self.alpha, h)
            out = out.view(out.shape[0], -1)
        else:
            edge_attention = self.leakyrelu(h_l[row] + h_r[col])
            edge_attention = self.edge_softmax(graph, edge_attention)
            edge_attention = self.dropout(edge_attention)
            out = self.mhspmm(graph, edge_attention, h)
        if self.residual:
            res = self.residual(x)
            out += res
        if self.norm is not None:
            out = self.norm(out)
        if self.act is not None:
            out = self.act(out)
        return out

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'


class GCNLayer(torch.nn.Module):
    """
    Args:
        in_feats: int
            Input feature size
        out_feats: int
            Output feature size
    """

    def __init__(self, in_feats, out_feats):
        super(GCNLayer, self).__init__()
        self.fc = torch.nn.Linear(in_feats, out_feats)

    def forward(self, graph, x):
        h = self.fc(x)
        h = spmm(graph, h)
        return h


class GCNIILayer(nn.Module):

    def __init__(self, n_channels, alpha=0.1, beta=1, residual=False):
        super(GCNIILayer, self).__init__()
        self.n_channels = n_channels
        self.alpha = alpha
        self.beta = beta
        self.residual = residual
        self.linear = nn.Linear(n_channels, n_channels)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.n_channels)
        torch.nn.init.uniform_(self.linear.weight, -stdv, stdv)

    def forward(self, graph, x, init_x):
        """Symmetric normalization"""
        hidden = spmm(graph, x)
        hidden = (1 - self.alpha) * hidden + self.alpha * init_x
        h = self.beta * self.linear(hidden) + (1 - self.beta) * hidden
        if self.residual:
            h = h + x
        return h


class GINLayer(nn.Module):
    """Graph Isomorphism Network layer from paper `"How Powerful are Graph
    Neural Networks?" <https://arxiv.org/pdf/1810.00826.pdf>`__.

    .. math::
        h_i^{(l+1)} = f_\\Theta \\left((1 + \\epsilon) h_i^{l} +
        \\mathrm{sum}\\left(\\left\\{h_j^{l}, j\\in\\mathcal{N}(i)
        \\right\\}\\right)\\right)

    Parameters
    ----------
    apply_func : callable layer function)
        layer or function applied to update node feature
    eps : float32, optional
        Initial `\\epsilon` value.
    train_eps : bool, optional
        If True, `\\epsilon` will be a learnable parameter.
    """

    def __init__(self, apply_func=None, eps=0, train_eps=True):
        super(GINLayer, self).__init__()
        if train_eps:
            self.eps = torch.nn.Parameter(torch.FloatTensor([eps]))
        else:
            self.register_buffer('eps', torch.FloatTensor([eps]))
        self.apply_func = apply_func

    def forward(self, graph, x):
        out = (1 + self.eps) * x + spmm(graph, x)
        if self.apply_func is not None:
            out = self.apply_func(out)
        return out


class GINELayer(BaseLayer):
    """The modified GINConv operator from the `"Graph convolutions that can finally model local structure" paper
     <https://arxiv.org/pdf/2011.15069.pdf>`__.

    Parameters
    ----------
    apply_func : callable layer function)
        layer or function applied to update node feature
    eps : float32, optional
        Initial `\\epsilon` value.
    train_eps : bool, optional
        If True, `\\epsilon` will be a learnable parameter.
    """

    def __init__(self, apply_func=None, eps=0, train_eps=True):
        super(GINELayer, self).__init__()
        if train_eps:
            self.eps = torch.nn.Parameter(torch.FloatTensor([eps]))
        else:
            self.register_buffer('eps', torch.FloatTensor([eps]))
        self.apply_func = apply_func

    def forward(self, graph, x):
        out = spmm(graph, x)
        out += (1 + self.eps) * x
        if self.apply_func is not None:
            out = self.apply_func(out)
        return out

    def message(self, x, attr):
        return F.relu(x + attr)


class AttentionLayer(nn.Module):

    def __init__(self, num_features):
        super(AttentionLayer, self).__init__()
        self.linear = nn.Linear(num_features, 1)

    def forward(self, x):
        att = self.linear(x).view(-1, 1, x.shape[1])
        return torch.matmul(att, x).squeeze(1)


class HANLayer(nn.Module):

    def __init__(self, num_edge, w_in, w_out):
        super(HANLayer, self).__init__()
        self.gat_layer = nn.ModuleList()
        for _ in range(num_edge):
            self.gat_layer.append(GATLayer(w_in, w_out // 8, 8))
        self.att_layer = AttentionLayer(w_out)

    def forward(self, graph, x):
        adj = graph.adj
        output = []
        with graph.local_graph():
            for i, edge in enumerate(adj):
                graph.edge_index = edge[0]
                output.append(self.gat_layer[i](graph, x))
        output = torch.stack(output, dim=1)
        return self.att_layer(output)


class MixHopLayer(nn.Module):

    def __init__(self, num_features, adj_pows, dim_per_pow):
        super(MixHopLayer, self).__init__()
        self.num_features = num_features
        self.adj_pows = adj_pows
        self.dim_per_pow = dim_per_pow
        self.total_dim = 0
        self.linears = torch.nn.ModuleList()
        for dim in dim_per_pow:
            self.linears.append(nn.Linear(num_features, dim))
            self.total_dim += dim

    def reset_parameters(self):
        for linear in self.linears:
            linear.reset_parameters()

    def adj_pow_x(self, graph, x, p):
        for _ in range(p):
            x = spmm(graph, x)
        return x

    def forward(self, graph, x):
        graph.sym_norm()
        output_list = []
        for p, linear in zip(self.adj_pows, self.linears):
            output = linear(self.adj_pow_x(graph, x, p))
            output_list.append(output)
        return torch.cat(output_list, dim=1)


class LinearLayer(nn.Module):

    def __init__(self, in_features, out_features, bias=True):
        super(LinearLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, mode='fan_out', a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / nn.math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, input):
        return torch.nn.functional.linear(input, self.weight, self.bias)


class PPRGoLayer(nn.Module):

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout, activation='relu'):
        super(PPRGoLayer, self).__init__()
        self.dropout = dropout
        self.nlayers = num_layers
        shapes = [hidden_size] * (num_layers - 1) + [out_feats]
        self.layers = nn.ModuleList()
        self.layers.append(LinearLayer(in_feats, hidden_size, bias=False))
        for i in range(num_layers - 1):
            self.layers.append(nn.Linear(shapes[i], shapes[i + 1], bias=False))
        self.activation = get_activation(activation)

    def forward(self, x):
        h = x
        for i, layer in enumerate(self.layers):
            h = nn.functional.dropout(h, p=self.dropout, training=self.training)
            h = layer(h)
            if i != self.nlayers - 1:
                h = self.activation(h)
        return h


class InvertibleCheckpointFunction(torch.autograd.Function):

    @staticmethod
    def forward(ctx, fn, fn_inverse, keep_input, num_bwd_passes, preserve_rng_state, num_inputs, *inputs_and_weights):
        ctx.fn = fn
        ctx.fn_inverse = fn_inverse
        ctx.keep_input = keep_input
        ctx.weights = inputs_and_weights[num_inputs:]
        ctx.num_bwd_passes = num_bwd_passes
        ctx.preserve_rng_state = preserve_rng_state
        ctx.num_inputs = num_inputs
        inputs = inputs_and_weights[:num_inputs]
        if preserve_rng_state:
            ctx.fwd_cpu_state = torch.get_rng_state()
            ctx.had_cuda_in_fwd = False
            if torch.cuda._initialized:
                ctx.had_cuda_in_fwd = True
                ctx.fwd_gpu_devices, ctx.fwd_gpu_states = get_device_states(*inputs)
        ctx.input_requires_grad = [element.requires_grad for element in inputs]
        with torch.no_grad():
            x = []
            for element in inputs:
                if isinstance(element, torch.Tensor):
                    x.append(element.detach())
                else:
                    x.append(element)
            outputs = ctx.fn(*x)
        if not isinstance(outputs, tuple):
            outputs = outputs,
        detached_outputs = tuple([element.detach_() for element in outputs])
        if not ctx.keep_input:
            inputs[0].storage().resize_(0)
        ctx.inputs = [inputs] * num_bwd_passes
        ctx.outputs = [detached_outputs] * num_bwd_passes
        return detached_outputs

    @staticmethod
    def backward(ctx, *grad_outputs):
        if not torch.autograd._is_checkpoint_valid():
            raise RuntimeError('InvertibleCheckpointFunction is not compatible with .grad(), please use .backward() if possible')
        if len(ctx.outputs) == 0:
            raise RuntimeError('Trying to perform backward on the InvertibleCheckpointFunction for more than {} times! Try raising `num_bwd_passes` by one.'.format(ctx.num_bwd_passes))
        inputs = ctx.inputs.pop()
        outputs = ctx.outputs.pop()
        if not ctx.keep_input:
            rng_devices = []
            if ctx.preserve_rng_state and ctx.had_cuda_in_fwd:
                rng_devices = ctx.fwd_gpu_devices
            with torch.random.fork_rng(devices=rng_devices, enabled=ctx.preserve_rng_state):
                if ctx.preserve_rng_state:
                    torch.set_rng_state(ctx.fwd_cpu_state)
                    if ctx.had_cuda_in_fwd:
                        set_device_states(ctx.fwd_gpu_devices, ctx.fwd_gpu_states)
                with torch.no_grad():
                    inputs_inverted = ctx.fn_inverse(*(outputs + inputs[1:]))
                    for element in outputs:
                        element.storage().resize_(0)
                    if not isinstance(inputs_inverted, tuple):
                        inputs_inverted = inputs_inverted,
                    for element_original, element_inverted in zip(inputs, inputs_inverted):
                        element_original.storage().resize_(int(np.prod(element_original.size())))
                        element_original.set_(element_inverted)
        with torch.set_grad_enabled(True):
            detached_inputs = []
            for element in inputs:
                if isinstance(element, torch.Tensor):
                    detached_inputs.append(element.detach())
                else:
                    detached_inputs.append(element)
            detached_inputs = tuple(detached_inputs)
            for det_input, requires_grad in zip(detached_inputs, ctx.input_requires_grad):
                det_input.requires_grad = requires_grad
            temp_output = ctx.fn(*detached_inputs)
        if not isinstance(temp_output, tuple):
            temp_output = temp_output,
        filtered_detached_inputs = tuple(filter(lambda x: x.requires_grad, detached_inputs))
        gradients = torch.autograd.grad(outputs=temp_output, inputs=filtered_detached_inputs + ctx.weights, grad_outputs=grad_outputs)
        input_gradients = []
        i = 0
        for rg in ctx.input_requires_grad:
            if rg:
                input_gradients.append(gradients[i])
                i += 1
            else:
                input_gradients.append(None)
        gradients = tuple(input_gradients) + gradients[-len(ctx.weights):]
        return (None, None, None, None, None, None) + gradients


class InvertibleModuleWrapper(nn.Module):

    def __init__(self, fn, keep_input=False, keep_input_inverse=False, num_bwd_passes=1, disable=False, preserve_rng_state=False):
        """
        The InvertibleModuleWrapper which enables memory savings during training by exploiting
        the invertible properties of the wrapped module.
        Parameters
        ----------
            fn : :obj:`torch.nn.Module`
                A torch.nn.Module which has a forward and an inverse function implemented with
                :math:`x == m.inverse(m.forward(x))`
            keep_input : :obj:`bool`, optional
                Set to retain the input information on forward, by default it can be discarded since it will be
                reconstructed upon the backward pass.
            keep_input_inverse : :obj:`bool`, optional
                Set to retain the input information on inverse, by default it can be discarded since it will be
                reconstructed upon the backward pass.
            num_bwd_passes :obj:`int`, optional
                Number of backward passes to retain a link with the output. After the last backward pass the output
                is discarded and memory is freed.
                Warning: if this value is raised higher than the number of required passes memory will not be freed
                correctly anymore and the training process can quickly run out of memory.
                Hence, The typical use case is to keep this at 1, until it raises an error for raising this value.
            disable : :obj:`bool`, optional
                This will disable using the InvertibleCheckpointFunction altogether.
                Essentially this renders the function as `y = fn(x)` without any of the memory savings.
                Setting this to true will also ignore the keep_input and keep_input_inverse properties.
            preserve_rng_state : :obj:`bool`, optional
                Setting this will ensure that the same RNG state is used during reconstruction of the inputs.
                I.e. if keep_input = False on forward or keep_input_inverse = False on inverse. By default
                this is False since most invertible modules should have a valid inverse and hence are
                deterministic.
        Attributes
        ----------
            keep_input : :obj:`bool`, optional
                Set to retain the input information on forward, by default it can be discarded since it will be
                reconstructed upon the backward pass.
            keep_input_inverse : :obj:`bool`, optional
                Set to retain the input information on inverse, by default it can be discarded since it will be
                reconstructed upon the backward pass.
        """
        super(InvertibleModuleWrapper, self).__init__()
        self.disable = disable
        self.keep_input = keep_input
        self.keep_input_inverse = keep_input_inverse
        self.num_bwd_passes = num_bwd_passes
        self.preserve_rng_state = preserve_rng_state
        self._fn = fn

    def forward(self, *xin):
        """Forward operation :math:`R(x) = y`
        Parameters
        ----------
            *xin : :obj:`torch.Tensor` tuple
                Input torch tensor(s).
        Returns
        -------
            :obj:`torch.Tensor` tuple
                Output torch tensor(s) *y.
        """
        if not self.disable:
            y = InvertibleCheckpointFunction.apply(self._fn.forward, self._fn.inverse, self.keep_input, self.num_bwd_passes, self.preserve_rng_state, len(xin), *(xin + tuple([p for p in self._fn.parameters() if p.requires_grad])))
        else:
            y = self._fn(*xin)
        if isinstance(y, tuple) and len(y) == 1:
            return y[0]
        return y

    def inverse(self, *yin):
        """Inverse operation :math:`R^{-1}(y) = x`
        Parameters
        ----------
            *yin : :obj:`torch.Tensor` tuple
                Input torch tensor(s).
        Returns
        -------
            :obj:`torch.Tensor` tuple
                Output torch tensor(s) *x.
        """
        if not self.disable:
            x = InvertibleCheckpointFunction.apply(self._fn.inverse, self._fn.forward, self.keep_input_inverse, self.num_bwd_passes, self.preserve_rng_state, len(yin), *(yin + tuple([p for p in self._fn.parameters() if p.requires_grad])))
        else:
            x = self._fn.inverse(*yin)
        if isinstance(x, tuple) and len(x) == 1:
            return x[0]
        return x


class AdditiveCoupling(nn.Module):

    def __init__(self, fm, gm=None, split_dim=-1):
        """
        This computes the output :math:`y` on forward given input :math:`x` and arbitrary modules :math:`Fm` and :math:`Gm` according to:
        :math:`(x1, x2) = x`
        :math:`y1 = x1 + Fm(x2)`
        :math:`y2 = x2 + Gm(y1)`
        :math:`y = (y1, y2)`
        Parameters
        ----------
            Fm : :obj:`torch.nn.Module`
                A torch.nn.Module encapsulating an arbitrary function
            Gm : :obj:`torch.nn.Module`
                A torch.nn.Module encapsulating an arbitrary function
                (If not specified a deepcopy of Fm is used as a Module)
            implementation_fwd : :obj:`int`
                Switch between different Additive Operation implementations for forward pass. Default = -1
            implementation_bwd : :obj:`int`
                Switch between different Additive Operation implementations for inverse pass. Default = -1
            split_dim : :obj:`int`
                Dimension to split the input tensors on. Default = 1, generally corresponding to channels.
        """
        super(AdditiveCoupling, self).__init__()
        if fm is None:
            gm = copy.deepcopy(fm)
        self.gm = gm
        self.fm = fm
        self.split_dim = split_dim

    def forward(self, x, graph):
        x1, x2 = torch.chunk(x, 2, dim=self.split_dim)
        x1, x2 = x1.contiguous(), x2.contiguous()
        fmd = self.fm.forward(graph, x2)
        y1 = x1 + fmd
        gmd = self.gm.forward(graph, y1)
        y2 = x2 + gmd
        out = torch.cat([y1, y2], dim=self.split_dim)
        return out

    def inverse(self, y, graph):
        y1, y2 = torch.chunk(y, 2, dim=self.split_dim)
        y1, y2 = y1.contiguous(), y2.contiguous()
        gmd = self.gm.forward(graph, y1)
        x2 = y2 - gmd
        fmd = self.fm.forward(graph, x2)
        x1 = y1 - fmd
        x = torch.cat([x1, x2], dim=self.split_dim)
        return x


class GroupAdditiveCoupling(torch.nn.Module):

    def __init__(self, func_modules, split_dim=-1, group=2):
        super(GroupAdditiveCoupling, self).__init__()
        self.func_modules = func_modules
        self.split_dim = split_dim
        self.group = group

    def forward(self, x, graph, *args):
        xs = torch.chunk(x, self.group, dim=self.split_dim)
        if len(args) > 0:
            chunked_args = list(map(lambda arg: torch.chunk(arg, self.group, dim=self.split_dim), args))
            args_chunks = list(zip(*chunked_args))
        y_in = sum(xs[1:])
        ys = []
        for i in range(self.group):
            if len(args) > 0:
                fmd = self.func_modules[i].forward(graph, y_in, *args_chunks[i])
            else:
                fmd = self.func_modules[i].forward(graph, y_in)
            y = xs[i] + fmd
            y_in = y
            ys.append(y)
        out = torch.cat(ys, dim=self.split_dim)
        return out

    def inverse(self, y, graph, *args):
        ys = torch.chunk(y, self.group, dim=self.split_dim)
        if len(args) > 0:
            chunked_args = list(map(lambda arg: torch.chunk(arg, self.group, dim=self.split_dim), args))
            args_chunks = list(zip(*chunked_args))
        xs = []
        for i in range(self.group - 1, -1, -1):
            if i != 0:
                y_in = ys[i - 1]
            else:
                y_in = sum(xs)
            if len(args) > 0:
                fmd = self.func_modules[i].forward(graph, y_in, *args_chunks[i])
            else:
                fmd = self.func_modules[i].forward(graph, y_in)
            x = ys[i] - fmd
            xs.append(x)
        x = torch.cat(xs[::-1], dim=self.split_dim)
        return x


class RevGNNLayer(nn.Module):

    def __init__(self, conv, group):
        super(RevGNNLayer, self).__init__()
        self.groups = nn.ModuleList()
        for i in range(group):
            if i == 0:
                self.groups.append(conv)
            else:
                self.groups.append(copy.deepcopy(conv))
        inv_module = GroupAdditiveCoupling(self.groups, group=group)
        self.nn = InvertibleModuleWrapper(fn=inv_module, keep_input=False)

    def forward(self, *args, **kwargs):
        items = list(args)
        items[1], items[0] = items[0], items[1]
        return self.nn(*items, **kwargs)


class RGCNLayer(nn.Module):
    """
    Implementation of Relational-GCN in paper `"Modeling Relational Data with Graph Convolutional Networks"
    <https://arxiv.org/abs/1703.06103>`_

    Parameters
    ----------
    in_feats : int
        Size of each input embedding.
    out_feats : int
        Size of each output embedding.
    num_edge_type : int
        The number of edge type in knowledge graph.
    regularizer : str, optional
        Regularizer used to avoid overfitting, ``basis`` or ``bdd``, default : ``basis``.
    num_bases : int, optional
        The number of basis, only used when `regularizer` is `basis`, default : ``None``.
    self_loop : bool, optional
        Add self loop embedding if True, default : ``True``.
    dropout : float
    self_dropout : float, optional
        Dropout rate of self loop embedding, default : ``0.0``
    layer_norm : bool, optional
        Use layer normalization if True, default : ``True``
    bias : bool
    """

    def __init__(self, in_feats, out_feats, num_edge_types, regularizer='basis', num_bases=None, self_loop=True, dropout=0.0, self_dropout=0.0, layer_norm=True, bias=True):
        super(RGCNLayer, self).__init__()
        self.num_bases = num_bases
        self.regularizer = regularizer
        self.num_edge_types = num_edge_types
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.self_loop = self_loop
        self.dropout = dropout
        self.self_dropout = self_dropout
        if self.num_bases is None or self.num_bases > num_edge_types or self.num_bases < 0:
            self.num_bases = num_edge_types
        if regularizer == 'basis':
            self.weight = nn.Parameter(torch.Tensor(self.num_bases, in_feats, out_feats))
            if self.num_bases < num_edge_types:
                self.alpha = nn.Parameter(torch.Tensor(num_edge_types, self.num_bases))
            else:
                self.register_buffer('alpha', None)
        elif regularizer == 'bdd':
            assert in_feats % num_bases == 0 and out_feats % num_bases == 0
            self.block_in_feats = in_feats // num_bases
            self.block_out_feats = out_feats // num_bases
            self.weight = nn.Parameter(torch.Tensor(num_edge_types, self.num_bases, self.block_in_feats * self.block_out_feats))
        else:
            raise NotImplementedError
        if bias is True:
            self.bias = nn.Parameter(torch.Tensor(out_feats))
        else:
            self.register_buffer('bias', None)
        if self_loop:
            self.weight_self_loop = nn.Parameter(torch.Tensor(in_feats, out_feats))
        else:
            self.register_buffer('weight_self_loop', None)
        if layer_norm:
            self.layer_norm = nn.LayerNorm(out_feats, elementwise_affine=True)
        else:
            self.register_buffer('layer_norm', None)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))
        if self.alpha is not None:
            nn.init.xavier_uniform_(self.alpha, gain=nn.init.calculate_gain('relu'))
        if self.bias is not None:
            nn.init.zeros_(self.bias)
        if self.self_loop is not None:
            nn.init.xavier_uniform_(self.weight_self_loop, gain=nn.init.calculate_gain('relu'))

    def forward(self, graph, x):
        if self.regularizer == 'basis':
            h_list = self.basis_forward(graph, x)
        else:
            h_list = self.bdd_forward(graph, x)
        h_result = sum(h_list)
        h_result = F.dropout(h_result, p=self.dropout, training=self.training)
        if self.layer_norm is not None:
            h_result = self.layer_norm(h_result)
        if self.bias is not None:
            h_result = h_result + self.bias
        if self.self_loop is not None:
            h_result += F.dropout(torch.matmul(x, self.weight_self_loop), p=self.self_dropout, training=self.training)
        return h_result

    def basis_forward(self, graph, x):
        edge_type = graph.edge_attr
        if self.num_bases < self.num_edge_types:
            weight = torch.matmul(self.alpha, self.weight.view(self.num_bases, -1))
            weight = weight.view(self.num_edge_types, self.in_feats, self.out_feats)
        else:
            weight = self.weight
        edge_index = torch.stack(graph.edge_index)
        edge_weight = graph.edge_weight
        graph.row_norm()
        h = torch.matmul(x, weight)
        h_list = []
        for edge_t in range(self.num_edge_types):
            g = graph.__class__()
            edge_mask = edge_type == edge_t
            if edge_mask.sum() == 0:
                h_list.append(0)
                continue
            g.edge_index = edge_index[:, edge_mask]
            g.edge_weight = edge_weight[edge_mask]
            g.padding_self_loops()
            temp = spmm(graph, h[edge_t])
            h_list.append(temp)
            return h_list

    def bdd_forward(self, graph, x):
        edge_type = graph.edge_attr
        edge_index = torch.stack(graph.edge_index)
        _x = x.view(-1, self.num_bases, self.block_in_feats)
        edge_weight = torch.ones(edge_type.shape)
        edge_weight = row_normalization(x.shape[0], edge_index, edge_weight)
        h_list = []
        for edge_t in range(self.num_edge_types):
            _weight = self.weight[edge_t].view(self.num_bases, self.block_in_feats, self.block_out_feats)
            edge_mask = edge_type == edge_t
            _edge_index_t = edge_index.t()[edge_mask].t()
            h_t = torch.einsum('abc,bcd->abd', _x, _weight).reshape(-1, self.out_feats)
            h_t = spmm(_edge_index_t, edge_weight[edge_mask], h_t)
            h_list.append(h_t)
        return h_list


class MaxAggregator(object):

    def __init__(self):
        self.scatter_max = scatter_max

    def __call__(self, graph, x):
        x = self.scatter_max(graph.row_indptr.int(), graph.col_indices.int(), x)
        return x


class SAGELayer(nn.Module):

    def __init__(self, in_feats, out_feats, normalize=False, aggr='mean', dropout=0.0, norm=None, activation=None, residual=False):
        super(SAGELayer, self).__init__()
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.fc = nn.Linear(2 * in_feats, out_feats)
        self.normalize = normalize
        if dropout > 0:
            self.dropout = nn.Dropout(dropout)
        else:
            self.dropout = None
        if aggr == 'mean':
            self.aggr = MeanAggregator()
        elif aggr == 'sum':
            self.aggr = SumAggregator()
        elif aggr == 'max':
            self.aggr = MaxAggregator()
        else:
            raise NotImplementedError
        if activation is not None:
            self.act = get_activation(activation, inplace=True)
        else:
            self.act = None
        if norm is not None:
            self.norm = get_norm_layer(norm, out_feats)
        else:
            self.norm = None
        if residual:
            self.residual = nn.Linear(in_features=in_feats, out_features=out_feats)
        else:
            self.residual = None

    def forward(self, graph, x):
        out = self.aggr(graph, x)
        out = torch.cat([x, out], dim=-1)
        out = self.fc(out)
        if self.normalize:
            out = F.normalize(out, p=2.0, dim=-1)
        if self.norm is not None:
            out = self.norm(out)
        if self.act is not None:
            out = self.act(out)
        if self.residual:
            out = out + self.residual(x)
        if self.dropout is not None:
            out = self.dropout(out)
        return out


F_ACT = {'relu': nn.ReLU(), 'I': lambda x: x}


class SAINTLayer(nn.Module):

    def __init__(self, dim_in, dim_out, dropout=0.0, act='relu', order=1, aggr='mean', bias='norm-nn', **kwargs):
        """
        Layer implemented here combines the GraphSAGE-mean [1] layer with MixHop [2] layer.
        We define the concept of `order`: an order-k layer aggregates neighbor information
        from 0-hop all the way to k-hop. The operation is approximately:
            X W_0 [+] A X W_1 [+] ... [+] A^k X W_k
        where [+] is some aggregation operation such as addition or concatenation.

        Special cases:
            Order = 0  -->  standard MLP layer
            Order = 1  -->  standard GraphSAGE layer

        [1]: https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf
        [2]: https://arxiv.org/abs/1905.00067

        Inputs:
            dim_in      int, feature dimension for input nodes
            dim_out     int, feature dimension for output nodes
            dropout     float, dropout on weight matrices W_0 to W_k
            act         str, activation function. See F_ACT at the top of this file
            order       int, see definition above
            aggr        str, if 'mean' then [+] operation adds features of various hops
                            if 'concat' then [+] concatenates features of various hops
            bias        str, if 'bias' then apply a bias vector to features of each hop
                            if 'norm' then perform batch-normalization on output features

        Outputs:
            None
        """
        super(SAINTLayer, self).__init__()
        assert bias in ['bias', 'norm', 'norm-nn']
        self.order, self.aggr = order, aggr
        self.act, self.bias = F_ACT[act], bias
        self.dropout = dropout
        self.f_lin, self.f_bias = [], []
        self.offset, self.scale = [], []
        self.num_param = 0
        for o in range(self.order + 1):
            self.f_lin.append(nn.Linear(dim_in, dim_out, bias=False))
            nn.init.xavier_uniform_(self.f_lin[-1].weight)
            self.f_bias.append(nn.Parameter(torch.zeros(dim_out)))
            self.num_param += dim_in * dim_out
            self.num_param += dim_out
            self.offset.append(nn.Parameter(torch.zeros(dim_out)))
            self.scale.append(nn.Parameter(torch.ones(dim_out)))
            if self.bias == 'norm' or self.bias == 'norm-nn':
                self.num_param += 2 * dim_out
        self.f_lin = nn.ModuleList(self.f_lin)
        self.f_dropout = nn.Dropout(p=self.dropout)
        self.params = nn.ParameterList(self.f_bias + self.offset + self.scale)
        self.f_bias = self.params[:self.order + 1]
        if self.bias == 'norm':
            self.offset = self.params[self.order + 1:2 * self.order + 2]
            self.scale = self.params[2 * self.order + 2:]
        elif self.bias == 'norm-nn':
            final_dim_out = dim_out * ((aggr == 'concat') * (order + 1) + (aggr == 'mean'))
            self.f_norm = nn.BatchNorm1d(final_dim_out, eps=1e-09, track_running_stats=True)
        self.num_param = int(self.num_param)

    def _f_feat_trans(self, _feat, _id):
        feat = self.act(self.f_lin[_id](_feat) + self.f_bias[_id])
        if self.bias == 'norm':
            mean = feat.mean(dim=1).view(feat.shape[0], 1)
            var = feat.var(dim=1, unbiased=False).view(feat.shape[0], 1) + 1e-09
            feat_out = (feat - mean) * self.scale[_id] * torch.rsqrt(var) + self.offset[_id]
        else:
            feat_out = feat
        return feat_out

    def forward(self, graph, x):
        """
        Inputs:
            graph           normalized adj matrix of the subgraph
            x               2D matrix of input node features

        Outputs:
            feat_out        2D matrix of output node features
        """
        feat_in = self.f_dropout(x)
        feat_hop = [feat_in]
        for o in range(self.order):
            feat_hop.append(spmm(graph, x))
        feat_partial = [self._f_feat_trans(ft, idf) for idf, ft in enumerate(feat_hop)]
        if self.aggr == 'mean':
            feat_out = feat_partial[0]
            for o in range(len(feat_partial) - 1):
                feat_out += feat_partial[o + 1]
        elif self.aggr == 'concat':
            feat_out = torch.cat(feat_partial, 1)
        else:
            raise NotImplementedError
        if self.bias == 'norm-nn':
            feat_out = self.f_norm(feat_out)
        return feat_out


class SELayer(nn.Module):
    """Squeeze-and-excitation networks"""

    def __init__(self, in_channels, se_channels):
        super(SELayer, self).__init__()
        self.in_channels = in_channels
        self.se_channels = se_channels
        self.encoder_decoder = nn.Sequential(nn.Linear(in_channels, se_channels), nn.ELU(), nn.Linear(se_channels, in_channels), nn.Sigmoid())

    def forward(self, x):
        """"""
        x_global = torch.mean(x, dim=0)
        s = self.encoder_decoder(x_global)
        return x * s


def batch_sum_pooling(x, batch):
    batch_size = int(torch.max(batch.cpu())) + 1
    res = torch.zeros(batch_size, x.size(1))
    out = res.scatter_add_(dim=0, index=batch.unsqueeze(-1).expand_as(x), src=x)
    return out
    return out


class Set2Set(torch.nn.Module):
    """The global pooling operator based on iterative content-based attention
    from the `"Order Matters: Sequence to sequence for sets"
    <https://arxiv.org/abs/1511.06391>`_ paper

    .. math::
        \\mathbf{q}_t &= \\mathrm{LSTM}(\\mathbf{q}^{*}_{t-1})

        \\alpha_{i,t} &= \\mathrm{softmax}(\\mathbf{x}_i \\cdot \\mathbf{q}_t)

        \\mathbf{r}_t &= \\sum_{i=1}^N \\alpha_{i,t} \\mathbf{x}_i

        \\mathbf{q}^{*}_t &= \\mathbf{q}_t \\, \\Vert \\, \\mathbf{r}_t,

    where :math:`\\mathbf{q}^{*}_T` defines the output of the layer with twice
    the dimensionality as the input.

    Args:
        in_channels (int): Size of each input sample.
        processing_steps (int): Number of iterations :math:`T`.
        num_layers (int, optional): Number of recurrent layers, *.e.g*, setting
            :obj:`num_layers=2` would mean stacking two LSTMs together to form
            a stacked LSTM, with the second LSTM taking in outputs of the first
            LSTM and computing the final results. (default: :obj:`1`)
    """

    def __init__(self, in_feats, processing_steps, num_layers=1):
        super(Set2Set, self).__init__()
        self.in_channels = in_feats
        self.out_channels = 2 * in_feats
        self.processing_steps = processing_steps
        self.num_layers = num_layers
        self.lstm = torch.nn.LSTM(self.out_channels, self.in_channels, num_layers)
        self.reset_parameters()

    def reset_parameters(self):
        self.lstm.reset_parameters()

    def forward(self, x, batch):
        batch_size = batch.max().item() + 1
        h = x.new_zeros((self.num_layers, batch_size, self.in_channels)), x.new_zeros((self.num_layers, batch_size, self.in_channels))
        q_star = x.new_zeros(batch_size, self.out_channels)
        for i in range(self.processing_steps):
            q, h = self.lstm(q_star.unsqueeze(0), h)
            q = q.view(batch_size, self.in_channels)
            e = (x * q[batch]).sum(dim=-1, keepdim=True)
            a = edge_softmax(e, batch)
            r = batch_sum_pooling(a * x, batch)
            q_star = torch.cat([q, r], dim=-1)
        return q_star


class SGCLayer(nn.Module):

    def __init__(self, in_features, out_features, order=3):
        super(SGCLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.order = order
        self.W = nn.Linear(in_features, out_features)

    def forward(self, graph, x):
        output = self.W(x)
        for _ in range(self.order):
            output = spmm(graph, output)
        return output


class GCNSVD(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout=0.0, activation=F.relu, residual=False, norm=None, feat_norm=None, adj_norm_func=None, k=50):
        super(GCNSVD, self).__init__()
        self.in_features = in_feats
        self.out_features = out_feats
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_size) is int:
            hidden_size = [hidden_size] * (num_layers - 1)
        elif type(hidden_size) is list or type(hidden_size) is tuple:
            assert len(hidden_size) == num_layers - 1, 'Incompatible sizes between hidden_size and n_layers.'
        n_features = [in_feats] + hidden_size + [out_feats]
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            if norm == 'layernorm' and i == 0:
                self.layers.append(nn.LayerNorm(n_features[i]))
            self.layers.append(GCNLayer(in_features=n_features[i], out_features=n_features[i + 1], activation=activation if i != num_layers - 1 else None, residual=residual if i != num_layers - 1 else False, dropout=dropout if i != num_layers - 1 else 0.0, norm=norm if i != num_layers - 1 else None))
        self.k = k

    def forward(self, graph):
        adj, h = utils.getGRBGraph(graph)
        adj = self.truncatedSVD(graph, self.k)
        adj = utils.adj_preprocess(adj=adj, adj_norm_func=self.adj_norm_func, device=h.device)
        for layer in self.layers:
            if isinstance(layer, nn.LayerNorm):
                h = layer(h)
            else:
                h = layer(graph, h)
        return h

    def truncatedSVD(self, graph, k=50):
        edge_index = graph.edge_index
        row, col = edge_index[0].cpu().data.numpy()[:], edge_index[1].cpu().data.numpy()[:]
        adj = sp.csr_matrix((np.ones(len(row)), (row, col)))
        if sp.issparse(adj):
            adj = adj.asfptype()
            U, S, V = sp.linalg.svds(adj, k=k)
            diag_S = np.diag(S)
        else:
            U, S, V = np.linalg.svd(adj)
            U = U[:, :k]
            S = S[:k]
            V = V[:k, :]
            diag_S = np.diag(S)
        new_adj = U @ diag_S @ V
        new_adj = sp.csr_matrix(new_adj)
        return new_adj


def GCNAdjNorm(adj, order=-0.5):
    """

    Description
    -----------
    Normalization of adjacency matrix proposed in `GCN <https://arxiv.org/abs/1609.02907>`__.

    Parameters
    ----------
    adj : scipy.sparse.csr.csr_matrix or torch.FloatTensor
        Adjacency matrix in form of ``N * N`` sparse matrix (or in form of ``N * N`` dense tensor).
    order : float, optional
        Order of degree matrix. Default: ``-0.5``.


    Returns
    -------
    adj : scipy.sparse.csr.csr_matrix
        Normalized adjacency matrix in form of ``N * N`` sparse matrix.

    """
    if sp.issparse(adj):
        adj = sp.eye(adj.shape[0]) + adj
        adj.data[np.where((adj.data > 0) * (adj.data == 1))[0]] = 1
        adj = sp.coo_matrix(adj)
        rowsum = np.array(adj.sum(1))
        d_inv = np.power(rowsum, order).flatten()
        d_inv[np.isinf(d_inv)] = 0.0
        d_mat_inv = sp.diags(d_inv)
        adj = d_mat_inv @ adj @ d_mat_inv
    else:
        rowsum = torch.sparse.mm(adj, torch.ones((adj.shape[0], 1), device=adj.device)) + 1
        d_inv = torch.pow(rowsum, order).flatten()
        d_inv[torch.isinf(d_inv)] = 0.0
        self_loop_idx = torch.stack((torch.arange(adj.shape[0], device=adj.device), torch.arange(adj.shape[0], device=adj.device)))
        self_loop_val = torch.ones_like(self_loop_idx[0], dtype=adj.dtype)
        indices = torch.cat((self_loop_idx, adj.indices()), dim=1)
        values = torch.cat((self_loop_val, adj.values()))
        values = d_inv[indices[0]] * values * d_inv[indices[1]]
        adj = torch.sparse.FloatTensor(indices, values, adj.shape).coalesce()
    return adj


def RobustGCNAdjNorm(adj):
    """

    Description
    -----------
    Normalization of adjacency matrix proposed in `RobustGCN <http://pengcui.thumedialab.com/papers/RGCN.pdf>`__.

    Parameters
    ----------
    adj : tuple of scipy.sparse.csr.csr_matrix
        Tuple of adjacency matrix in form of ``N * N`` sparse matrix.

    Returns
    -------
    adj0 : scipy.sparse.csr.csr_matrix
        Adjacency matrix in form of ``N * N`` sparse matrix.
    adj1 : scipy.sparse.csr.csr_matrix
        Adjacency matrix in form of ``N * N`` sparse matrix.

    """
    adj0 = GCNAdjNorm(adj, order=-0.5)
    adj1 = GCNAdjNorm(adj, order=-1)
    return adj0, adj1


class RobustGCNConv(nn.Module):
    """

    Description
    -----------
    RobustGCN convolutional layer.

    Parameters
    ----------
    in_features : int
        Dimension of input features.
    out_features : int
        Dimension of output features.
    act0 : func of torch.nn.functional, optional
        Activation function. Default: ``F.elu``.
    act1 : func of torch.nn.functional, optional
        Activation function. Default: ``F.relu``.
    initial : bool, optional
        Whether to initialize variance.
    dropout : float, optional
            Rate of dropout. Default: ``0.0``.

    """

    def __init__(self, in_features, out_features, act0=F.elu, act1=F.relu, initial=False, dropout=0.0):
        super(RobustGCNConv, self).__init__()
        self.mean_conv = nn.Linear(in_features, out_features)
        self.var_conv = nn.Linear(in_features, out_features)
        self.act0 = act0
        self.act1 = act1
        self.initial = initial
        if dropout > 0.0:
            self.dropout = nn.Dropout(dropout)
        else:
            self.dropout = None

    def forward(self, mean, var=None, adj0=None, adj1=None):
        """

        Parameters
        ----------
        mean : torch.Tensor
            Tensor of mean of input features.
        var : torch.Tensor, optional
            Tensor of variance of input features. Default: ``None``.
        adj0 : torch.SparseTensor, optional
            Sparse tensor of adjacency matrix 0. Default: ``None``.
        adj1 : torch.SparseTensor, optional
            Sparse tensor of adjacency matrix 1. Default: ``None``.

        Returns
        -------

        """
        mean = self.mean_conv(mean)
        if self.initial:
            var = mean * 1
        else:
            var = self.var_conv(var)
        mean = self.act0(mean)
        var = self.act1(var)
        attention = torch.exp(-var)
        mean = mean * attention
        var = var * attention * attention
        mean = torch.spmm(adj0, mean)
        var = torch.spmm(adj1, var)
        if self.dropout:
            mean = self.act0(mean)
            var = self.act1(var)
            if self.dropout is not None:
                mean = self.dropout(mean)
                var = self.dropout(var)
        return mean, var


class RobustGCN(BaseModel):
    """

    Description
    -----------
    Robust Graph Convolutional Networks (`RobustGCN <http://pengcui.thumedialab.com/papers/RGCN.pdf>`__)

    Parameters
    ----------
    in_features : int
        Dimension of input features.
    out_features : int
        Dimension of output features.
    hidden_features : int or list of int
        Dimension of hidden features. List if multi-layer.
    feat_norm : str, optional
        Type of features normalization, choose from ["arctan", "tanh", None]. Default: ``None``.
    adj_norm_func : func of utils.normalize, optional
        Function that normalizes adjacency matrix. Default: ``RobustAdjNorm``.
    dropout : float, optional
            Rate of dropout. Default: ``0.0``.

    """

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout=0.0, feat_norm=None, adj_norm_func=RobustGCNAdjNorm):
        super(RobustGCN, self).__init__()
        self.in_features = in_feats
        self.out_features = out_feats
        self.feat_norm = feat_norm
        self.adj_norm_func = adj_norm_func
        if type(hidden_size) is int:
            hidden_size = [hidden_size] * (num_layers - 1)
        elif type(hidden_size) is list or type(hidden_size) is tuple:
            assert len(hidden_size) == num_layers - 1, 'Incompatible sizes between hidden_size and n_layers.'
        n_features = [in_feats] + hidden_size + [out_feats]
        self.act0 = F.elu
        self.act1 = F.relu
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            self.layers.append(RobustGCNConv(n_features[i], n_features[i + 1], act0=self.act0, act1=self.act1, initial=True if i == 0 else False, dropout=dropout if i != num_layers - 1 else 0.0))

    def forward(self, graph):
        """

        Parameters
        ----------
        x : torch.Tensor
            Tensor of input features.
        adj : list of torch.SparseTensor
            List of sparse tensor of adjacency matrix.

        Returns
        -------
        x : torch.Tensor
            Output of model (logits without activation).

        """
        adj, x = getGRBGraph(graph)
        adj0, adj1 = copy.deepcopy(adj), copy.deepcopy(adj)
        mean = x
        var = x
        for layer in self.layers:
            mean, var = layer(mean, var=var, adj0=adj0, adj1=adj1)
        sample = torch.randn(var.shape)
        output = mean + sample * torch.pow(var, 0.5)
        return output


class DeepGraphKernel(BaseModel):
    """The Hin2vec model from the `"Deep Graph Kernels"
    <https://dl.acm.org/citation.cfm?id=2783417&CFID=763322570&CFTOKEN=93890155>`_ paper.

    Args:
        hidden_size (int) : The dimension of node representation.
        min_count (int) : Parameter in word2vec.
        window (int) : The actual context size which is considered in language model.
        sampling_rate (float) : Parameter in word2vec.
        iteration (int) : The number of iteration in WL method.
        epochs (int) : The number of training iteration.
        alpha (float) : The learning rate of word2vec.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=128)
        parser.add_argument('--window_size', type=int, default=5)
        parser.add_argument('--min-count', type=int, default=1)
        parser.add_argument('--sampling', type=float, default=0.0001)
        parser.add_argument('--iteration', type=int, default=2)
        parser.add_argument('--epochs', type=int, default=20)
        parser.add_argument('--alpha', type=float, default=0.01)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.hidden_size, args.min_count, args.window_size, args.sampling, args.iteration, args.epochs, args.alpha)

    @staticmethod
    def feature_extractor(data, rounds, name):
        edge_index = torch.stack(data.edge_index)
        graph = nx.from_edgelist(np.array(edge_index.T.cpu(), dtype=int))
        if data.x is not None:
            feature = {int(key): str(val.argmax(axis=0)) for key, val in enumerate(np.array(data.x.cpu()))}
        else:
            feature = dict(nx.degree(graph))
        graph_wl_features = DeepGraphKernel.wl_iterations(graph, feature, rounds)
        return graph_wl_features

    @staticmethod
    def wl_iterations(graph, features, rounds):
        nodes = graph.nodes
        wl_features = [str(val) for _, val in features.items()]
        for i in range(rounds):
            new_feats = {}
            for node in nodes:
                neighbors = graph.neighbors(node)
                neigh_feats = [features[x] for x in neighbors]
                neigh_feats = [features[node]] + sorted(neigh_feats)
                hash_feat = hashlib.md5('_'.join([str(x) for x in neigh_feats]).encode())
                hash_feat = hash_feat.hexdigest()
                new_feats[node] = hash_feat
            wl_features = wl_features + list(new_feats.values())
            features = new_feats
        return wl_features

    def __init__(self, hidden_dim, min_count, window_size, sampling_rate, rounds, epochs, alpha, n_workers=4):
        super(DeepGraphKernel, self).__init__()
        self.hidden_dim = hidden_dim
        self.min_count = min_count
        self.window = window_size
        self.sampling_rate = sampling_rate
        self.n_workers = n_workers
        self.rounds = rounds
        self.model = None
        self.gl_collections = None
        self.epochs = epochs
        self.alpha = alpha

    def forward(self, graphs, **kwargs):
        if self.gl_collections is None:
            self.gl_collections = Parallel(n_jobs=self.n_workers)(delayed(DeepGraphKernel.feature_extractor)(graph, self.rounds, str(i)) for i, graph in enumerate(graphs))
        model = Word2Vec(self.gl_collections, vector_size=self.hidden_dim, window=self.window, min_count=self.min_count, sample=self.sampling_rate, workers=self.n_workers, epochs=self.epochs, alpha=self.alpha)
        vectors = np.asarray([model.wv[str(node)] for node in model.wv.index_to_key])
        S = vectors.dot(vectors.T)
        node2id = dict(zip(model.wv.index_to_key, range(len(model.wv.index_to_key))))
        num_graph, size_vocab = len(graphs), len(node2id)
        norm_prob = np.zeros((num_graph, size_vocab))
        for i, gls in enumerate(self.gl_collections):
            for gl in gls:
                if gl in node2id:
                    norm_prob[i, node2id[gl]] += 1
        embedding = norm_prob.dot(S)
        return embedding

    def save_embedding(self, output_path):
        self.model.wv.save('model.wv')
        self.model.wv.save_word2vec_format('model.emb')


class DNGR_layer(nn.Module):

    def __init__(self, num_node, hidden_size1, hidden_size2):
        super(DNGR_layer, self).__init__()
        self.num_node = num_node
        self.hidden_size1 = hidden_size1
        self.hidden_size2 = hidden_size2
        self.encoder = nn.Sequential(nn.Linear(self.num_node, self.hidden_size1), nn.Tanh(), nn.Linear(self.hidden_size1, self.hidden_size2), nn.Tanh())
        self.decoder = nn.Sequential(nn.Linear(self.hidden_size2, self.hidden_size1), nn.Tanh(), nn.Linear(self.hidden_size1, self.num_node), nn.Tanh())

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded


class DNGR(BaseModel):
    """The DNGR model from the `"Deep Neural Networks for Learning Graph Representations"
    <https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12423/11715>`_ paper

    Args:
        hidden_size1 (int) : The size of the first hidden layer.
        hidden_size2 (int) : The size of the second hidden layer.
        noise (float) : Denoise rate of DAE.
        alpha (float) : Parameter in DNGR.
        step (int) : The max step in random surfing.
        epochs (int) : The max epoches in training step.
        lr (float) : Learning rate in DNGR.
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--hidden-size1', type=int, default=1000, help='Hidden size in first layer of Auto-Encoder')
        parser.add_argument('--hidden-size2', type=int, default=128, help='Hidden size in second layer of Auto-Encoder')
        parser.add_argument('--noise', type=float, default=0.2, help='denoise rate of DAE')
        parser.add_argument('--alpha', type=float, default=0.98, help='alhpa is a hyperparameter in DNGR')
        parser.add_argument('--step', type=int, default=10, help='step is a hyperparameter in DNGR')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.hidden_size1, args.hidden_size2, args.noise, args.alpha, args.step, args.epochs, args.lr, args.cpu)

    def __init__(self, hidden_size1, hidden_size2, noise, alpha, step, epochs, lr, cpu):
        super(DNGR, self).__init__()
        self.hidden_size1 = hidden_size1
        self.hidden_size2 = hidden_size2
        self.noise = noise
        self.alpha = alpha
        self.step = step
        self.epochs = epochs
        self.lr = lr
        self.cpu = cpu

    def scale_matrix(self, mat):
        mat = mat - np.diag(np.diag(mat))
        D_inv = np.diagflat(np.reciprocal(np.sum(mat, axis=0)))
        mat = np.dot(D_inv, mat)
        return mat

    def random_surfing(self, adj_matrix):
        adj_matrix = self.scale_matrix(adj_matrix)
        P0 = np.eye(self.num_node, dtype='float32')
        M = np.zeros((self.num_node, self.num_node), dtype='float32')
        P = np.eye(self.num_node, dtype='float32')
        for i in range(0, self.step):
            P = self.alpha * np.dot(P, adj_matrix) + (1 - self.alpha) * P0
            M = M + P
        return M

    def get_ppmi_matrix(self, mat):
        mat = self.random_surfing(mat)
        M = self.scale_matrix(mat)
        col_s = np.sum(M, axis=0).reshape(1, self.num_node)
        row_s = np.sum(M, axis=1).reshape(self.num_node, 1)
        D = np.sum(col_s)
        rowcol_s = np.dot(row_s, col_s)
        PPMI = np.log(np.divide(D * M, rowcol_s))
        PPMI[np.isnan(PPMI)] = 0.0
        PPMI[np.isinf(PPMI)] = 0.0
        PPMI[np.isneginf(PPMI)] = 0.0
        PPMI[PPMI < 0] = 0.0
        return PPMI

    def get_denoised_matrix(self, mat):
        return mat * (np.random.random(mat.shape) > self.noise)

    def get_emb(self, matrix):
        ut, s, _ = sp.linalg.svds(matrix, self.hidden_size2)
        emb_matrix = ut * np.sqrt(s)
        emb_matrix = preprocessing.normalize(emb_matrix, 'l2')
        return emb_matrix

    def forward(self, graph, return_dict=False):
        device = 'cuda' if torch.cuda.is_available() and not self.cpu else 'cpu'
        G = graph.to_networkx()
        self.num_node = G.number_of_nodes()
        A = nx.adjacency_matrix(G).todense()
        PPMI = self.get_ppmi_matrix(A)
        None
        input_mat = torch.from_numpy(self.get_denoised_matrix(PPMI).astype(np.float32))
        model = DNGR_layer(self.num_node, self.hidden_size1, self.hidden_size2)
        input_mat = input_mat
        model = model
        opt = torch.optim.Adam(model.parameters(), lr=self.lr)
        loss_func = nn.MSELoss()
        epoch_iter = tqdm(range(self.epochs))
        for epoch in epoch_iter:
            opt.zero_grad()
            encoded, decoded = model.forward(input_mat)
            Loss = loss_func(decoded, input_mat)
            Loss.backward()
            epoch_iter.set_description(f'Epoch: {epoch:03d},  Loss: {Loss:.8f}')
            opt.step()
        embeddings, _ = model.forward(input_mat)
        embeddings = embeddings.detach().cpu().numpy()
        if return_dict:
            features_matrix = dict()
            for vid, node in enumerate(G.nodes()):
                features_matrix[node] = embeddings[vid]
        else:
            features_matrix = np.zeros((graph.num_nodes, embeddings.shape[1]))
            nx_nodes = G.nodes()
            features_matrix[nx_nodes] = embeddings[np.arange(graph.num_nodes)]
        return features_matrix


class GATNEModel(nn.Module):

    def __init__(self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a):
        super(GATNEModel, self).__init__()
        self.num_nodes = num_nodes
        self.embedding_size = embedding_size
        self.embedding_u_size = embedding_u_size
        self.edge_type_count = edge_type_count
        self.dim_a = dim_a
        self.node_embeddings = Parameter(torch.FloatTensor(num_nodes, embedding_size))
        self.node_type_embeddings = Parameter(torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size))
        self.trans_weights = Parameter(torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size))
        self.trans_weights_s1 = Parameter(torch.FloatTensor(edge_type_count, embedding_u_size, dim_a))
        self.trans_weights_s2 = Parameter(torch.FloatTensor(edge_type_count, dim_a, 1))
        self.reset_parameters()

    def reset_parameters(self):
        self.node_embeddings.data.uniform_(-1.0, 1.0)
        self.node_type_embeddings.data.uniform_(-1.0, 1.0)
        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))
        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))
        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))

    def forward(self, train_inputs, train_types, node_neigh):
        node_embed = self.node_embeddings[train_inputs]
        node_embed_neighbors = self.node_type_embeddings[node_neigh]
        node_embed_tmp = torch.cat([node_embed_neighbors[:, i, :, i, :].unsqueeze(1) for i in range(self.edge_type_count)], dim=1)
        node_type_embed = torch.sum(node_embed_tmp, dim=2)
        trans_w = self.trans_weights[train_types]
        trans_w_s1 = self.trans_weights_s1[train_types]
        trans_w_s2 = self.trans_weights_s2[train_types]
        attention = F.softmax(torch.matmul(F.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2).squeeze()).unsqueeze(1)
        node_type_embed = torch.matmul(attention, node_type_embed)
        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze()
        last_node_embed = F.normalize(node_embed, dim=1)
        return last_node_embed


class NSLoss(nn.Module):

    def __init__(self, num_nodes, num_sampled, embedding_size):
        super(NSLoss, self).__init__()
        self.num_nodes = num_nodes
        self.num_sampled = num_sampled
        self.embedding_size = embedding_size
        self.weights = Parameter(torch.FloatTensor(num_nodes, embedding_size))
        self.sample_weights = F.normalize(torch.Tensor([((math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)) for k in range(num_nodes)]), dim=0)
        self.reset_parameters()

    def reset_parameters(self):
        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))

    def forward(self, input, embs, label):
        n = input.shape[0]
        log_target = torch.log(torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1)))
        negs = torch.multinomial(self.sample_weights, self.num_sampled * n, replacement=True).view(n, self.num_sampled)
        noise = torch.neg(self.weights[negs])
        sum_log_sampled = torch.sum(torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1).squeeze()
        loss = log_target + sum_log_sampled
        return -loss.sum() / n


def generate_pairs(all_walks, vocab, window_size=5):
    pairs = []
    skip_window = window_size // 2
    for layer_id, walks in enumerate(all_walks):
        for walk in walks:
            for i in range(len(walk)):
                for j in range(1, skip_window + 1):
                    if i - j >= 0:
                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))
                    if i + j < len(walk):
                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))
    return pairs


def generate_vocab(all_walks):
    index_to_key = []
    raw_vocab = defaultdict(int)
    for walks in all_walks:
        for walk in walks:
            for word in walk:
                raw_vocab[word] += 1
    vocab = {}
    for word, v in raw_vocab.items():
        vocab[word] = Vocab(count=v, index=len(index_to_key))
        index_to_key.append(word)
    index_to_key.sort(key=lambda word: vocab[word].count, reverse=True)
    for i, word in enumerate(index_to_key):
        vocab[word].index = i
    return vocab, index_to_key


class RWGraph:

    def __init__(self, nx_G, node_type=None):
        self.G = nx_G
        self.node_type = node_type

    def walk(self, walk_length, start, schema=None):
        G = self.G
        rand = random.Random()
        if schema:
            schema_items = schema.split('-')
            assert schema_items[0] == schema_items[-1]
        walk = [start]
        while len(walk) < walk_length:
            cur = walk[-1]
            candidates = []
            for node in G[cur].keys():
                if schema is None or self.node_type[node] == schema_items[len(walk) % (len(schema_items) - 1)]:
                    candidates.append(node)
            if candidates:
                walk.append(rand.choice(candidates))
            else:
                break
        return walk

    def simulate_walks(self, num_walks, walk_length, schema=None):
        G = self.G
        walks = []
        nodes = list(G.nodes())
        if schema is not None:
            schema_list = schema.split(',')
        for walk_iter in range(num_walks):
            random.shuffle(nodes)
            for node in nodes:
                if schema is None:
                    walks.append(self.walk(walk_length=walk_length, start=node))
                else:
                    for schema_iter in schema_list:
                        if schema_iter.split('-')[0] == self.node_type[node]:
                            walks.append(self.walk(walk_length=walk_length, start=node, schema=schema_iter))
        return walks


def get_G_from_edges(edges):
    edge_dict = dict()
    for edge in edges:
        edge_key = str(edge[0]) + '_' + str(edge[1])
        if edge_key not in edge_dict:
            edge_dict[edge_key] = 1
        else:
            edge_dict[edge_key] += 1
    tmp_G = nx.Graph()
    for edge_key in edge_dict:
        weight = edge_dict[edge_key]
        x = int(edge_key.split('_')[0])
        y = int(edge_key.split('_')[1])
        tmp_G.add_edge(x, y)
        tmp_G[x][y]['weight'] = weight
    return tmp_G


def generate_walks(network_data, num_walks, walk_length, schema=None):
    all_walks = []
    for layer_id in network_data:
        tmp_data = network_data[layer_id]
        layer_walker = RWGraph(get_G_from_edges(tmp_data))
        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, schema=schema)
        all_walks.append(layer_walks)
    return all_walks


def get_batches(pairs, neighbors, batch_size):
    n_batches = (len(pairs) + (batch_size - 1)) // batch_size
    for idx in range(n_batches):
        x, y, t, neigh = [], [], [], []
        for i in range(batch_size):
            index = idx * batch_size + i
            if index >= len(pairs):
                break
            x.append(pairs[index][0])
            y.append(pairs[index][1])
            t.append(pairs[index][2])
            neigh.append(neighbors[pairs[index][0]])
        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)


class GATNE(BaseModel):
    """The GATNE model from the `"Representation Learning for Attributed Multiplex Heterogeneous Network"
    <https://dl.acm.org/doi/10.1145/3292500.3330964>`_ paper

    Args:
        walk_length (int) : The walk length.
        walk_num (int) : The number of walks to sample for each node.
        window_size (int) : The actual context size which is considered in language model.
        worker (int) : The number of workers for word2vec.
        epochs (int) : The number of training epochs.
        batch_size (int) : The size of each training batch.
        edge_dim (int) : Number of edge embedding dimensions.
        att_dim (int) : Number of attention dimensions.
        negative_samples (int) : Negative samples for optimization.
        neighbor_samples (int) : Neighbor samples for aggregation
        schema (str) : The metapath schema used in model. Metapaths are splited with ",",
        while each node type are connected with "-" in each metapath. For example:"0-1-0,0-1-2-1-0"
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--walk-length', type=int, default=10, help='Length of walk per source. Default is 10.')
        parser.add_argument('--walk-num', type=int, default=10, help='Number of walks per source. Default is 10.')
        parser.add_argument('--window-size', type=int, default=5, help='Window size of skip-gram model. Default is 5.')
        parser.add_argument('--worker', type=int, default=10, help='Number of parallel workers. Default is 10.')
        parser.add_argument('--epochs', type=int, default=20, help='Number of epochs. Default is 20.')
        parser.add_argument('--batch-size', type=int, default=256, help='Number of batch_size. Default is 256.')
        parser.add_argument('--edge-dim', type=int, default=10, help='Number of edge embedding dimensions. Default is 10.')
        parser.add_argument('--att-dim', type=int, default=20, help='Number of attention dimensions. Default is 20.')
        parser.add_argument('--negative-samples', type=int, default=5, help='Negative samples for optimization. Default is 5.')
        parser.add_argument('--neighbor-samples', type=int, default=10, help='Neighbor samples for aggregation. Default is 10.')
        parser.add_argument('--schema', type=str, default=None, help='Input schema for metapath random walk.')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.hidden_size, args.walk_length, args.walk_num, args.window_size, args.worker, args.epochs, args.batch_size, args.edge_dim, args.att_dim, args.negative_samples, args.neighbor_samples, args.schema)

    def __init__(self, dimension, walk_length, walk_num, window_size, worker, epochs, batch_size, edge_dim, att_dim, negative_samples, neighbor_samples, schema):
        super(GATNE, self).__init__()
        self.embedding_size = dimension
        self.walk_length = walk_length
        self.walk_num = walk_num
        self.window_size = window_size
        self.worker = worker
        self.epochs = epochs
        self.batch_size = batch_size
        self.embedding_u_size = edge_dim
        self.dim_att = att_dim
        self.num_sampled = negative_samples
        self.neighbor_samples = neighbor_samples
        self.schema = schema
        self.multiplicity = True

    def forward(self, network_data):
        device = 'cpu' if not torch.cuda.is_available() else 'cuda'
        all_walks = generate_walks(network_data, self.walk_num, self.walk_length, schema=self.schema)
        vocab, index_to_key = generate_vocab(all_walks)
        train_pairs = generate_pairs(all_walks, vocab)
        edge_types = list(network_data.keys())
        num_nodes = len(index_to_key)
        edge_type_count = len(edge_types)
        epochs = self.epochs
        batch_size = self.batch_size
        embedding_size = self.embedding_size
        embedding_u_size = self.embedding_u_size
        num_sampled = self.num_sampled
        dim_att = self.dim_att
        neighbor_samples = self.neighbor_samples
        neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]
        for r in range(edge_type_count):
            g = network_data[edge_types[r]]
            for x, y in g:
                ix = vocab[x].index
                iy = vocab[y].index
                neighbors[ix][r].append(iy)
                neighbors[iy][r].append(ix)
            for i in range(num_nodes):
                if len(neighbors[i][r]) == 0:
                    neighbors[i][r] = [i] * neighbor_samples
                elif len(neighbors[i][r]) < neighbor_samples:
                    neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples - len(neighbors[i][r]))))
                elif len(neighbors[i][r]) > neighbor_samples:
                    neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))
        model = GATNEModel(num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_att)
        nsloss = NSLoss(num_nodes, num_sampled, embedding_size)
        model
        nsloss
        optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params': nsloss.parameters()}], lr=0.0001)
        for epoch in range(epochs):
            random.shuffle(train_pairs)
            batches = get_batches(train_pairs, neighbors, batch_size)
            data_iter = tqdm.tqdm(batches, desc='epoch %d' % epoch, total=(len(train_pairs) + (batch_size - 1)) // batch_size, bar_format='{l_bar}{r_bar}')
            avg_loss = 0.0
            for i, data in enumerate(data_iter):
                optimizer.zero_grad()
                embs = model(data[0], data[2], data[3])
                loss = nsloss(data[0], embs, data[1])
                loss.backward()
                optimizer.step()
                avg_loss += loss.item()
                if i % 5000 == 0:
                    post_fix = {'epoch': epoch, 'iter': i, 'avg_loss': avg_loss / (i + 1), 'loss': loss.item()}
                    data_iter.write(str(post_fix))
        final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))
        for i in range(num_nodes):
            train_inputs = torch.tensor([i for _ in range(edge_type_count)])
            train_types = torch.tensor(list(range(edge_type_count)))
            node_neigh = torch.tensor([neighbors[i] for _ in range(edge_type_count)])
            node_emb = model(train_inputs, train_types, node_neigh)
            for j in range(edge_type_count):
                final_model[edge_types[j]][index_to_key[i]] = node_emb[j].cpu().detach().numpy()
        return final_model


class Graph2Vec(BaseModel):
    """The Graph2Vec model from the `"graph2vec: Learning Distributed Representations of Graphs"
    <https://arxiv.org/abs/1707.05005>`_ paper

    Args:
        hidden_size (int) : The dimension of node representation.
        min_count (int) : Parameter in doc2vec.
        window_size (int) : The actual context size which is considered in language model.
        sampling_rate (float) : Parameter in doc2vec.
        dm (int) :  Parameter in doc2vec.
        iteration (int) : The number of iteration in WL method.
        lr (float) : Learning rate in doc2vec.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=128)
        parser.add_argument('--window-size', type=int, default=0)
        parser.add_argument('--min-count', type=int, default=5)
        parser.add_argument('--dm', type=int, default=0)
        parser.add_argument('--sampling', type=float, default=0.0001)
        parser.add_argument('--iteration', type=int, default=2)
        parser.add_argument('--lr', type=float, default=0.025)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.hidden_size, args.min_count, args.window_size, args.sampling, args.dm, args.iteration, args.epochs, args.lr)

    @staticmethod
    def feature_extractor(data, rounds, name):
        edge_index = torch.stack(data.edge_index)
        graph = nx.from_edgelist(np.array(edge_index.T.cpu(), dtype=int))
        if data.x is not None:
            feature = {int(key): str(val) for key, val in enumerate(np.array(data.x.cpu()))}
        else:
            feature = dict(nx.degree(graph))
        graph_wl_features = Graph2Vec.wl_iterations(graph, feature, rounds)
        doc = TaggedDocument(words=graph_wl_features, tags=['g_' + name])
        return doc

    @staticmethod
    def wl_iterations(graph, features, rounds):
        nodes = graph.nodes
        wl_features = [str(val) for _, val in features.items()]
        for i in range(rounds):
            new_feats = {}
            for node in nodes:
                neighbors = graph.neighbors(node)
                neigh_feats = [features[x] for x in neighbors]
                neigh_feats = [features[node]] + sorted(neigh_feats)
                hash_feat = hashlib.md5('_'.join([str(x) for x in neigh_feats]).encode())
                hash_feat = hash_feat.hexdigest()
                new_feats[node] = hash_feat
            wl_features = wl_features + list(new_feats.values())
            features = new_feats
        return wl_features

    def __init__(self, dimension, min_count, window_size, dm, sampling_rate, rounds, epochs, lr, worker=4):
        super(Graph2Vec, self).__init__()
        self.dimension = dimension
        self.min_count = min_count
        self.window_size = window_size
        self.sampling_rate = sampling_rate
        self.dm = dm
        self.worker = worker
        self.rounds = rounds
        self.model = None
        self.doc_collections = None
        self.epochs = epochs
        self.lr = lr

    def forward(self, graphs, **kwargs):
        if self.doc_collections is None:
            self.doc_collections = Parallel(n_jobs=self.worker)(delayed(Graph2Vec.feature_extractor)(graph, self.rounds, str(i)) for i, graph in enumerate(graphs))
        self.model = Doc2Vec(self.doc_collections, vector_size=self.dimension, window=self.window_size, min_count=self.min_count, dm=self.dm, sample=self.sampling_rate, workers=self.worker, epochs=self.epochs, alpha=self.lr)
        vectors = np.array([self.model['g_' + str(i)] for i in range(len(graphs))])
        return vectors

    def save_embedding(self, output_path):
        self.model.wv.save(os.path.join(output_path, 'model.wv'))
        self.model.wv.save_word2vec_format(os.path.join('model.emb'))


class Hin2vec_layer(nn.Module):

    def __init__(self, num_node, num_relation, hidden_size, device):
        super(Hin2vec_layer, self).__init__()
        self.num_node = num_node
        self.Wx = Parameter(torch.randn(num_node, hidden_size))
        self.Wr = Parameter(torch.randn(num_relation, hidden_size))
        self.device = device
        self.X = F.one_hot(torch.arange(num_node), num_node).float()
        self.R = F.one_hot(torch.arange(num_relation), num_relation).float()
        self.criterion = nn.CrossEntropyLoss()

    def regulartion(self, embr):
        clamp_embr = torch.clamp(embr, -6.0, 6.0)
        sigmod1 = torch.sigmoid(clamp_embr)
        re_embr = torch.mul(sigmod1, 1 - sigmod1)
        return re_embr

    def forward(self, x, y, r, l):
        x_one, y_one, r_one = torch.index_select(self.X, 0, x), torch.index_select(self.X, 0, y), torch.index_select(self.R, 0, r)
        self.embx, self.emby, self.embr = torch.mm(x_one, self.Wx), torch.mm(y_one, self.Wx), torch.mm(r_one, self.Wr)
        self.re_embr = self.regulartion(self.embr)
        self.preds = torch.unsqueeze(torch.sigmoid(torch.sum(torch.mul(torch.mul(self.embx, self.emby), self.re_embr), 1)), 1)
        self.logits = torch.cat((self.preds, 1 - self.preds), 1)
        return self.logits, self.criterion(self.logits, l)

    def get_emb(self):
        x = F.one_hot(torch.arange(0, self.num_node), num_classes=self.num_node).float()
        return torch.mm(x, self.Wx)


class RWgraph:

    def __init__(self, nx_G, node_type=None):
        self.G = nx_G
        self.node_type = node_type

    def _walk(self, start_node, walk_length):
        walk = [start_node]
        while len(walk) < walk_length:
            cur = walk[-1]
            cur_nbrs = list(self.G.neighbors(cur))
            if len(cur_nbrs) == 0:
                break
            k = int(np.floor(np.random.rand() * len(cur_nbrs)))
            walk.append(cur_nbrs[k])
        return walk

    def _simulate_walks(self, walk_length, num_walks):
        walks = []
        nodes = list(self.G.nodes())
        None
        for walk_iter in range(num_walks):
            None
            random.shuffle(nodes)
            for node in nodes:
                walks.append(self._walk(node, walk_length))
        return walks

    def data_preparation(self, walks, hop, negative):
        node_type = self.node_type
        num_node_type = len(set(node_type))
        type2list = [[] for _ in range(num_node_type)]
        for node, nt in enumerate(node_type):
            type2list[nt].append(node)
        None
        relation = dict()
        pairs = []
        for walk in walks:
            for i in range(len(walk) - hop):
                for j in range(1, hop + 1):
                    x, y = walk[i], walk[i + j]
                    if x == y:
                        continue
                    meta_str = '-'.join([str(node_type[a]) for a in walk[i:i + j + 1]])
                    if meta_str not in relation:
                        relation[meta_str] = len(meta_str)
                    pairs.append([x, y, relation[meta_str], 1])
                    for k in range(negative):
                        if random.random() > 0.5:
                            fx = random.choice(type2list[node_type[x]])
                            while fx == x:
                                fx = random.choice(type2list[node_type[x]])
                            pairs.append([fx, y, relation[meta_str], 0])
                        else:
                            fy = random.choice(type2list[node_type[y]])
                            while fy == y:
                                fy = random.choice(type2list[node_type[y]])
                            pairs.append([x, fy, relation[meta_str], 0])
        None
        return np.asarray(pairs), relation


class Hin2vec(BaseModel):
    """The Hin2vec model from the `"HIN2Vec: Explore Meta-paths in Heterogeneous Information Networks for Representation Learning"
    <https://dl.acm.org/doi/10.1145/3132847.3132953>`_ paper.

    Args:
        hidden_size (int) : The dimension of node representation.
        walk_length (int) : The walk length.
        walk_num (int) : The number of walks to sample for each node.
        batch_size (int) : The batch size of training in Hin2vec.
        hop (int) : The number of hop to construct training samples in Hin2vec.
        negative (int) : The number of nagative samples for each meta2path pair.
        epochs (int) : The number of training iteration.
        lr (float) : The initial learning rate of SGD.
        cpu (bool) : Use CPU or GPU to train hin2vec.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=128)
        parser.add_argument('--walk-length', type=int, default=80, help='Length of walk per source. Default is 80.')
        parser.add_argument('--walk-num', type=int, default=40, help='Number of walks per source. Default is 40.')
        parser.add_argument('--batch-size', type=int, default=1000, help='Batch size in SGD training process. Default is 1000.')
        parser.add_argument('--hop', type=int, default=2)
        parser.add_argument('--negative', type=int, default=5)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.hidden_size, args.walk_length, args.walk_num, args.batch_size, args.hop, args.negative, args.epochs, args.lr, args.cpu)

    def __init__(self, hidden_dim, walk_length, walk_num, batch_size, hop, negative, epochs, lr, cpu=True):
        super(Hin2vec, self).__init__()
        self.hidden_dim = hidden_dim
        self.walk_length = walk_length
        self.walk_num = walk_num
        self.batch_size = batch_size
        self.hop = hop
        self.negative = negative
        self.epochs = epochs
        self.lr = lr
        self.cpu = cpu

    def forward(self, data):
        device = 'cpu' if not torch.cuda.is_available() or self.cpu else 'cuda'
        G = nx.DiGraph()
        row, col = data.edge_index
        G.add_edges_from(list(zip(row.numpy(), col.numpy())))
        self.num_node = G.number_of_nodes()
        rw = RWgraph(G, data.pos.tolist())
        walks = rw._simulate_walks(self.walk_length, self.walk_num)
        pairs, relation = rw.data_preparation(walks, self.hop, self.negative)
        self.num_relation = len(relation)
        model = Hin2vec_layer(self.num_node, self.num_relation, self.hidden_dim, device)
        self.model = model
        num_batch = int(len(pairs) / self.batch_size)
        print_num_batch = 100
        None
        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)
        epoch_iter = tqdm(range(self.epochs))
        for epoch in epoch_iter:
            loss_n, pred, label = [], [], []
            for i in range(num_batch):
                batch_pairs = torch.from_numpy(pairs[i * self.batch_size:(i + 1) * self.batch_size])
                batch_pairs = batch_pairs
                batch_pairs = batch_pairs.T
                x, y, r, l = batch_pairs[0], batch_pairs[1], batch_pairs[2], batch_pairs[3]
                opt.zero_grad()
                logits, loss = self.model.forward(x, y, r, l)
                loss_n.append(loss.item())
                label.append(l)
                pred.extend(logits)
                if i % print_num_batch == 0 and i != 0:
                    label = torch.cat(label)
                    pred = torch.stack(pred, dim=0)
                    pred = pred.max(1)[1]
                    acc = pred.eq(label).sum().item() / len(label)
                    epoch_iter.set_description(f'Epoch: {i:03d}, Loss: {sum(loss_n) / print_num_batch:.5f}, Acc: {acc:.5f}')
                    loss_n, pred, label = [], [], []
                loss.backward()
                opt.step()
        embedding = self.model.get_emb()
        return embedding.cpu().detach().numpy()


class KGEModel(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--embedding_size', type=int, default=500, help='Dimensionality of embedded vectors')
        parser.add_argument('--gamma', type=float, default=12.0, help='Hyperparameter for embedding')
        parser.add_argument('--double_entity_embedding', action='store_true')
        parser.add_argument('--double_relation_embedding', action='store_true')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_entities, args.num_rels, args.embedding_size, args.gamma, args.double_entity_embedding, args.double_relation_embedding)

    def __init__(self, nentity, nrelation, hidden_dim, gamma, double_entity_embedding, double_relation_embedding):
        super(KGEModel, self).__init__()
        self.nentity = nentity
        self.nrelation = nrelation
        self.hidden_dim = hidden_dim
        self.epsilon = 2.0
        self.gamma = nn.Parameter(torch.Tensor([gamma]), requires_grad=False)
        self.embedding_range = nn.Parameter(torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]), requires_grad=False)
        self.entity_dim = hidden_dim * 2 if double_entity_embedding else hidden_dim
        self.relation_dim = hidden_dim * 2 if double_relation_embedding else hidden_dim
        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))
        nn.init.uniform_(tensor=self.entity_embedding, a=-self.embedding_range.item(), b=self.embedding_range.item())
        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))
        nn.init.uniform_(tensor=self.relation_embedding, a=-self.embedding_range.item(), b=self.embedding_range.item())

    def forward(self, sample, mode='single'):
        """
        Forward function that calculate the score of a batch of triples.
        In the 'single' mode, sample is a batch of triple.
        In the 'head-batch' or 'tail-batch' mode, sample consists two part.
        The first part is usually the positive sample.
        And the second part is the entities in the negative samples.
        Because negative samples and positive samples usually share two elements
        in their triple ((head, relation) or (relation, tail)).
        """
        if mode == 'single':
            batch_size, negative_sample_size = sample.size(0), 1
            head = torch.index_select(self.entity_embedding, dim=0, index=sample[:, 0]).unsqueeze(1)
            relation = torch.index_select(self.relation_embedding, dim=0, index=sample[:, 1]).unsqueeze(1)
            tail = torch.index_select(self.entity_embedding, dim=0, index=sample[:, 2]).unsqueeze(1)
        elif mode == 'head-batch':
            tail_part, head_part = sample
            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)
            head = torch.index_select(self.entity_embedding, dim=0, index=head_part.view(-1)).view(batch_size, negative_sample_size, -1)
            relation = torch.index_select(self.relation_embedding, dim=0, index=tail_part[:, 1]).unsqueeze(1)
            tail = torch.index_select(self.entity_embedding, dim=0, index=tail_part[:, 2]).unsqueeze(1)
        elif mode == 'tail-batch':
            head_part, tail_part = sample
            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)
            head = torch.index_select(self.entity_embedding, dim=0, index=head_part[:, 0]).unsqueeze(1)
            relation = torch.index_select(self.relation_embedding, dim=0, index=head_part[:, 1]).unsqueeze(1)
            tail = torch.index_select(self.entity_embedding, dim=0, index=tail_part.view(-1)).view(batch_size, negative_sample_size, -1)
        else:
            raise ValueError('mode %s not supported' % mode)
        score = self.score(head, relation, tail, mode)
        return score

    def score(self, head, relation, tail, mode):
        raise NotImplementedError


class RotatE(KGEModel):
    """
    Implementation of RotatE model from the paper `"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space"
    <https://openreview.net/forum?id=HkgEQnRqYQ>`.
    borrowed from `KnowledgeGraphEmbedding<https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding>`
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--embedding_size', type=int, default=500, help='Dimensionality of embedded vectors')
        parser.add_argument('--gamma', type=float, default=12.0, help='Hyperparameter for embedding')
        parser.add_argument('--double_entity_embedding', default=True)
        parser.add_argument('--double_relation_embedding', action='store_true')

    def __init__(self, nentity, nrelation, hidden_dim, gamma, double_entity_embedding, double_relation_embedding):
        super(RotatE, self).__init__(nentity, nrelation, hidden_dim, gamma, double_entity_embedding, double_relation_embedding)

    def score(self, head, relation, tail, mode):
        pi = 3.141592653589793
        re_head, im_head = torch.chunk(head, 2, dim=2)
        re_tail, im_tail = torch.chunk(tail, 2, dim=2)
        phase_relation = relation / (self.embedding_range.item() / pi)
        re_relation = torch.cos(phase_relation)
        im_relation = torch.sin(phase_relation)
        if mode == 'head-batch':
            re_score = re_relation * re_tail + im_relation * im_tail
            im_score = re_relation * im_tail - im_relation * re_tail
            re_score = re_score - re_head
            im_score = im_score - im_head
        else:
            re_score = re_head * re_relation - im_head * im_relation
            im_score = re_head * im_relation + im_head * re_relation
            re_score = re_score - re_tail
            im_score = im_score - im_tail
        score = torch.stack([re_score, im_score], dim=0)
        score = score.norm(dim=0)
        score = self.gamma.item() - score.sum(dim=2)
        return score


class SDNE_layer(nn.Module):

    def __init__(self, num_node, hidden_size1, hidden_size2, droput, alpha, beta, nu1, nu2):
        super(SDNE_layer, self).__init__()
        self.num_node = num_node
        self.hidden_size1 = hidden_size1
        self.hidden_size2 = hidden_size2
        self.droput = droput
        self.alpha = alpha
        self.beta = beta
        self.nu1 = nu1
        self.nu2 = nu2
        self.encode0 = nn.Linear(self.num_node, self.hidden_size1)
        self.encode1 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.decode0 = nn.Linear(self.hidden_size2, self.hidden_size1)
        self.decode1 = nn.Linear(self.hidden_size1, self.num_node)

    def forward(self, adj_mat, l_mat):
        t0 = F.leaky_relu(self.encode0(adj_mat))
        t0 = F.leaky_relu(self.encode1(t0))
        self.embedding = t0
        t0 = F.leaky_relu(self.decode0(t0))
        t0 = F.leaky_relu(self.decode1(t0))
        L_1st = 2 * torch.trace(torch.mm(torch.mm(torch.t(self.embedding), l_mat), self.embedding))
        L_2nd = torch.sum((adj_mat - t0) * adj_mat * self.beta * ((adj_mat - t0) * adj_mat * self.beta))
        L_reg = 0
        for param in self.parameters():
            L_reg += self.nu1 * torch.sum(torch.abs(param)) + self.nu2 * torch.sum(param * param)
        return self.alpha * L_1st, L_2nd, self.alpha * L_1st + L_2nd, L_reg

    def get_emb(self, adj):
        t0 = self.encode0(adj)
        t0 = self.encode1(t0)
        return t0


class SDNE(BaseModel):
    """The SDNE model from the `"Structural Deep Network Embedding"
    <https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf>`_ paper

    Args:
        hidden_size1 (int) : The size of the first hidden layer.
        hidden_size2 (int) : The size of the second hidden layer.
        droput (float) : Droput rate.
        alpha (float) : Trade-off parameter between 1-st and 2-nd order objective function in SDNE.
        beta (float) : Parameter of 2-nd order objective function in SDNE.
        nu1 (float) : Parameter of l1 normlization in SDNE.
        nu2 (float) : Parameter of l2 normlization in SDNE.
        epochs (int) : The max epoches in training step.
        lr (float) : Learning rate in SDNE.
        cpu (bool) : Use CPU or GPU to train hin2vec.
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--hidden-size1', type=int, default=1000, help='Hidden size in first layer of Auto-Encoder')
        parser.add_argument('--hidden-size2', type=int, default=128, help='Hidden size in second layer of Auto-Encoder')
        parser.add_argument('--droput', type=float, default=0.5, help='Dropout rate')
        parser.add_argument('--alpha', type=float, default=0.1, help='alhpa is a hyperparameter in SDNE')
        parser.add_argument('--beta', type=float, default=5, help='beta is a hyperparameter in SDNE')
        parser.add_argument('--nu1', type=float, default=0.0001, help='nu1 is a hyperparameter in SDNE')
        parser.add_argument('--nu2', type=float, default=0.001, help='nu2 is a hyperparameter in SDNE')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.hidden_size1, args.hidden_size2, args.droput, args.alpha, args.beta, args.nu1, args.nu2, args.epochs, args.lr, args.cpu)

    def __init__(self, hidden_size1, hidden_size2, droput, alpha, beta, nu1, nu2, epochs, lr, cpu):
        super(SDNE, self).__init__()
        self.hidden_size1 = hidden_size1
        self.hidden_size2 = hidden_size2
        self.droput = droput
        self.alpha = alpha
        self.beta = beta
        self.nu1 = nu1
        self.nu2 = nu2
        self.epochs = epochs
        self.lr = lr
        self.cpu = cpu

    def forward(self, graph, return_dict=False):
        device = 'cuda' if torch.cuda.is_available() and not self.cpu else 'cpu'
        G = graph.to_networkx()
        num_node = G.number_of_nodes()
        model = SDNE_layer(num_node, self.hidden_size1, self.hidden_size2, self.droput, self.alpha, self.beta, self.nu1, self.nu2)
        A = torch.from_numpy(nx.adjacency_matrix(G).todense().astype(np.float32))
        L = torch.from_numpy(nx.laplacian_matrix(G).todense().astype(np.float32))
        A, L = A, L
        model = model
        opt = torch.optim.Adam(model.parameters(), lr=self.lr)
        epoch_iter = tqdm(range(self.epochs))
        for epoch in epoch_iter:
            opt.zero_grad()
            L_1st, L_2nd, L_all, L_reg = model.forward(A, L)
            Loss = L_all + L_reg
            Loss.backward()
            epoch_iter.set_description(f'Epoch: {epoch:03d}, L_1st: {L_1st:.4f}, L_2nd: {L_2nd:.4f}, L_reg: {L_reg:.4f}')
            opt.step()
        embeddings = model.get_emb(A)
        embeddings = embeddings.detach().cpu().numpy()
        if return_dict:
            features_matrix = dict()
            for vid, node in enumerate(G.nodes()):
                features_matrix[node] = embeddings[vid]
        else:
            features_matrix = np.zeros((graph.num_nodes, embeddings.shape[1]))
            nx_nodes = G.nodes()
            features_matrix[nx_nodes] = embeddings[np.arange(graph.num_nodes)]
        return features_matrix


class TransE(KGEModel):
    """The TransE model from paper `"Translating Embeddings for Modeling Multi-relational Data"
    <http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf>`
    borrowed from `KnowledgeGraphEmbedding<https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding>`
    """

    def __init__(self, nentity, nrelation, hidden_dim, gamma, double_entity_embedding, double_relation_embedding):
        super(TransE, self).__init__(nentity, nrelation, hidden_dim, gamma, double_entity_embedding, double_relation_embedding)

    def score(self, head, relation, tail, mode):
        if mode == 'head-batch':
            score = head + (relation - tail)
        else:
            score = head + relation - tail
        score = self.gamma.item() - torch.norm(score, p=1, dim=2)
        return score


class ActGCN(BaseModel):
    """The GCN model from the `"Semi-Supervised Classification with Graph Convolutional Networks"
    <https://arxiv.org/abs/1609.02907>`_ paper

    Args:
        in_features (int) : Number of input features.
        out_features (int) : Number of classes.
        hidden_size (int) : The dimension of node representation.
        dropout (float) : Dropout rate for model training.
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--residual', action='store_true')
        parser.add_argument('--norm', type=str, default=None)
        parser.add_argument('--activation', type=str, default='relu')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.dropout, args.activation, args.residual, args.norm, args.rp_ratio)

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout, activation='relu', residual=False, norm=None, rp_ratio=1):
        super(ActGCN, self).__init__()
        shapes = [in_feats] + [hidden_size] * (num_layers - 1) + [out_feats]
        self.layers = nn.ModuleList([ActGCNLayer(shapes[i], shapes[i + 1], dropout=dropout if i != num_layers - 1 else 0, residual=residual if i != num_layers - 1 else None, norm=norm if i != num_layers - 1 else None, activation=activation if i != num_layers - 1 else None, rp_ratio=rp_ratio) for i in range(num_layers)])
        self.num_layers = num_layers

    def embed(self, graph):
        graph.sym_norm()
        h = graph.x
        for i in range(self.num_layers - 1):
            h = self.layers[i](graph, h)
        return h

    def forward(self, graph):
        graph.sym_norm()
        h = graph.x
        for i in range(self.num_layers):
            h = self.layers[i](graph, h)
        return h


class AGC(BaseModel):
    """The AGC model from the `"Attributed Graph Clustering via Adaptive Graph Convolution"
    <https://arxiv.org/abs/1906.01210>`_ paper

    Args:
        num_clusters (int) : Number of clusters.
        max_iter     (int) : Max iteration to increase k
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-clusters', type=int, default=7)
        parser.add_argument('--max-iter', type=int, default=10)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_clusters, args.max_iter, args.cpu)

    def __init__(self, num_clusters, max_iter, cpu):
        super(AGC, self).__init__()
        self.num_clusters = num_clusters
        self.max_iter = max_iter
        self.cpu = cpu

    def forward(self, data):
        device = 'cuda' if torch.cuda.is_available() and not self.cpu else 'cpu'
        data = data
        self.num_nodes = data.x.shape[0]
        graph = data
        graph.add_remaining_self_loops()
        graph.sym_norm()
        graph.edge_weight = data.edge_weight * 0.5
        pre_intra = 1e+27
        pre_feat = None
        for t in range(1, self.max_iter + 1):
            x = data.x
            for i in range(t):
                x = spmm(graph, x)
            k = torch.mm(x, x.t())
            w = (torch.abs(k) + torch.abs(k.t())) / 2
            clustering = SpectralClustering(n_clusters=self.num_clusters, assign_labels='discretize', random_state=0).fit(w.detach().cpu())
            clusters = clustering.labels_
            intra = self.compute_intra(x.cpu().numpy(), clusters)
            None
            if intra > pre_intra:
                features_matrix = pre_feat
                return features_matrix
            pre_intra = intra
            pre_feat = w
        features_matrix = w
        return features_matrix.cpu()

    def compute_intra(self, x, clusters):
        num_nodes = x.shape[0]
        intra = np.zeros(self.num_clusters)
        num_per_cluster = np.zeros(self.num_clusters)
        for i in range(num_nodes):
            for j in range(i + 1, num_nodes):
                if clusters[i] == clusters[j]:
                    intra[clusters[i]] += np.sum((x[i] - x[j]) ** 2) ** 0.5
                    num_per_cluster[clusters[i]] += 1
        intra = np.array(list(filter(lambda x: x > 0, intra)))
        num_per_cluster = np.array(list(filter(lambda x: x > 0, num_per_cluster)))
        return np.mean(intra / num_per_cluster)


def drgat_model(num_features, hidden_size, num_classes, dropout, num_heads):
    layers = nn.ModuleList()
    layers.append(nn.Dropout(p=dropout))
    layers.append(SELayer(num_features, se_channels=int(np.sqrt(num_features))))
    layers.append(GATLayer(num_features, hidden_size, nhead=num_heads, attn_drop=dropout))
    layers.append(nn.ELU())
    layers.append(nn.Dropout(p=dropout))
    layers.append(SELayer(hidden_size * num_heads, se_channels=int(np.sqrt(hidden_size * num_heads))))
    layers.append(GATLayer(hidden_size * num_heads, num_classes, nhead=1, attn_drop=dropout))
    layers.append(nn.ELU())
    return layers


def gat_model(in_feats, hidden_size, out_feats, nhead, attn_drop, alpha, residual, norm, num_layers, dropout, last_nhead):
    layers = nn.ModuleList()
    layers.append(GATLayer(in_feats, hidden_size, nhead=nhead, attn_drop=attn_drop, alpha=alpha, residual=residual, norm=norm))
    if num_layers != 1:
        layers.append(nn.ELU())
    for i in range(num_layers - 2):
        if dropout > 0.0:
            layers.append(nn.Dropout(dropout))
        layers.append(GATLayer(hidden_size * nhead, hidden_size, nhead=nhead, attn_drop=attn_drop, alpha=alpha, residual=residual, norm=norm))
        layers.append(nn.ELU())
    if dropout > 0.0:
        layers.append(nn.Dropout(p=dropout))
    layers.append(GATLayer(hidden_size * nhead, out_feats, attn_drop=attn_drop, alpha=alpha, nhead=last_nhead, residual=False))
    return layers


def gcn_model(in_feats, hidden_size, num_layers, out_feats, dropout, residual, norm, activation):
    shapes = [in_feats] + [hidden_size] * (num_layers - 1) + [out_feats]
    return nn.ModuleList([GCNLayer(shapes[i], shapes[i + 1], dropout=dropout if i != num_layers - 1 else 0, residual=residual if i != num_layers - 1 else None, norm=norm if i != num_layers - 1 else None, activation=activation if i != num_layers - 1 else None) for i in range(num_layers)])


def gcnii_model(in_feats, hidden_size, out_feats, dropout, num_layers, alpha, lmbda, residual):
    layers = nn.ModuleList()
    layers.append(nn.Dropout(p=dropout))
    layers.append(nn.Linear(in_feats, hidden_size))
    layers.append(nn.ReLU())
    for i in range(num_layers):
        layers.append(nn.Dropout(p=dropout))
        layers.append(GCNIILayer(hidden_size, alpha, math.log(lmbda / (i + 1) + 1), residual))
        layers.append(nn.ReLU())
    layers.append(nn.Dropout(p=dropout))
    layers.append(nn.Linear(hidden_size, out_feats))
    return layers


def grand_model(in_feats, hidden_size, out_feats, dropout, dropout2, norm):
    layers = nn.ModuleList()
    if norm == 'batchnorm':
        layers.append(nn.BatchNorm1d(in_feats))
    layers.append(nn.Dropout(p=dropout))
    layers.append(nn.Linear(in_feats, hidden_size))
    layers.append(nn.ReLU())
    if norm == 'batchnorm':
        layers.append(nn.BatchNorm1d(hidden_size))
    layers.append(nn.Dropout(p=dropout2))
    layers.append(nn.Linear(hidden_size, out_feats))
    return layers


class AutoGNN(BaseModel):
    """
    Args
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=8)
        parser.add_argument('--layers-type', type=str, default='gcn')
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--dropout', type=float, default=0.6)
        parser.add_argument('--norm', type=str, default=None)
        parser.add_argument('--residual', action='store_true')
        parser.add_argument('--activation', type=str, default='relu')
        parser.add_argument('--attn-drop', type=float, default=0.5)
        parser.add_argument('--alpha', type=float, default=0.2)
        parser.add_argument('--nhead', type=int, default=8)
        parser.add_argument('--last-nhead', type=int, default=1)
        parser.add_argument('--weight-decay', type=float, default=0.0)
        parser.add_argument('--dropoutn', type=float, default=0.5)

    @classmethod
    def build_model_from_args(cls, args):
        if not hasattr(args, 'attn_drop'):
            args.attn_drop = 0.5
        if not hasattr(args, 'alpha'):
            args.alpha = 0.2
        if not hasattr(args, 'nhead'):
            args.nhead = 8
        if not hasattr(args, 'last_nhead'):
            args.last_nhead = 1
        if not hasattr(args, 'dropoutn'):
            args.dropoutn = 0.5
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.layers_type, args.dropout, args.activation, args.norm, args.residual, args.attn_drop, args.alpha, args.nhead, args.last_nhead, args.dropoutn)

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, layers_type, dropout, activation=None, norm=None, residual=False, attn_drop=0.5, alpha=0.2, nhead=8, last_nhead=1, dropoutn=0.5):
        super(AutoGNN, self).__init__()
        self.dropout = dropout
        self.layers_type = layers_type
        if self.layers_type == 'gcn':
            self.layers = gcn_model(in_feats, hidden_size, num_layers, out_feats, dropout, residual, norm, activation)
            self.num_layers = num_layers
        elif self.layers_type == 'gat':
            self.layers = gat_model(in_feats, hidden_size, out_feats, nhead, attn_drop, alpha, residual, norm, num_layers, dropout, last_nhead)
            self.num_layers = num_layers
            self.last_nhead = last_nhead
        elif self.layers_type == 'grand':
            self.layers = grand_model(in_feats, hidden_size, out_feats, dropout, dropoutn, norm)
            self.dropnode_rate = attn_drop
            self.order = nhead
        elif self.layers_type == 'gcnii':
            self.layers = gcnii_model(in_feats, hidden_size, out_feats, dropout, num_layers, alpha, dropoutn, residual)
        elif self.layers_type == 'drgat':
            self.layers = drgat_model(in_feats, hidden_size, out_feats, dropout, nhead)
        self.autognn_parameters = list(self.layers.parameters())

    def drop_node(self, x):
        n = x.shape[0]
        drop_rates = torch.ones(n) * self.dropnode_rate
        if self.training:
            masks = torch.bernoulli(1.0 - drop_rates).unsqueeze(1)
            x = masks * x
        else:
            x = x * (1.0 - self.dropnode_rate)
        return x

    def rand_prop(self, graph, x):
        x = self.drop_node(x)
        y = x
        for i in range(self.order):
            x = spmm(graph, x).detach_()
            y.add_(x)
        return y.div_(self.order + 1.0).detach_()

    def normalize_x(self, x):
        row_sum = x.sum(1)
        row_inv = row_sum.pow_(-1)
        row_inv.masked_fill_(row_inv == float('inf'), 0)
        x = x * row_inv[:, None]
        return x

    def forward(self, graph):
        if self.layers_type == 'gcn':
            graph.sym_norm()
            h = graph.x
        elif self.layers_type == 'gat':
            h = graph.x
        elif self.layers_type == 'grand':
            graph.sym_norm()
            x = graph.x
            x = self.normalize_x(x)
            h = self.rand_prop(graph, x)
        elif self.layers_type == 'gcnii':
            graph.sym_norm()
            h = graph.x
        elif self.layers_type == 'drgat':
            h = graph.x
        init_h = None
        for i, layer in enumerate(self.layers):
            if type(layer).__name__ == 'GATLayer' or type(layer).__name__ == 'GCNLayer':
                h = layer(graph, h)
            elif type(layer).__name__ == 'GCNIILayer':
                h = layer(graph, h, init_h)
            else:
                h = layer(h)
            if i == 2:
                init_h = h
        return h


class BasesRelEmbLayer(nn.Module):

    def __init__(self, num_bases, num_rels, in_feats):
        super(BasesRelEmbLayer, self).__init__()
        self.num_bases = num_bases
        self.num_resl = num_rels
        self.in_feats = in_feats
        self.weight = nn.Parameter(torch.Tensor(num_bases, in_feats))
        self.alpha = nn.Parameter(torch.Tensor(2 * num_rels, num_bases))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        nn.init.xavier_uniform_(self.alpha)

    def forward(self):
        weight = torch.matmul(self.alpha, self.weight)
        return weight


class CompGCNLayer(nn.Module):
    """
    Implementation of CompGCN in paper `"Composition-based Multi-Relational Graph Convolutional Networks"`
    <https://arxiv.org/abs/1911.03082>

    Parameters
    ----------
    in_feats : int
        Size of each input embedding
    out_feats : int
        Size of each output embedding
    num_rel : int
        The number of relations
    opn : str
        Operation to mix two head and tail embeddings
    num_bases : Optional[Int]
        The number of basis, default : ``None``.
    activation : Function
    dropout : float
    bias : bool
    """

    def __init__(self, in_feats, out_feats, num_rels, opn='mult', num_bases=None, activation=lambda x: x, dropout=0.0, bias=True):
        super(CompGCNLayer, self).__init__()
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.num_rels = num_rels
        self.opn = opn
        self.use_bases = num_bases is not None and num_bases > 0
        self.weight_in = self.get_param(in_feats, out_feats)
        self.weight_out = self.get_param(in_feats, out_feats)
        self.weight_rel = self.get_param(in_feats, out_feats)
        self.weight_loop = self.get_param(in_feats, out_feats)
        self.loop_rel = self.get_param(1, in_feats)
        if self.use_bases:
            self.basis_weight = BasesRelEmbLayer(num_bases, num_rels, in_feats)
        else:
            self.register_buffer('basis_weight', None)
        self.dropout = dropout
        self.activation = activation
        if bias:
            self.bias = nn.Parameter(torch.zeros(out_feats))
        else:
            self.register_buffer('bias', None)
        self.bn = nn.BatchNorm1d(out_feats)

    def get_param(self, num_in, num_out):
        weight = nn.Parameter(torch.Tensor(num_in, num_out))
        nn.init.xavier_normal_(weight.data)
        return weight

    def forward(self, graph, x, rel_embed):
        device = x.device
        if self.use_bases:
            rel_embed = self.basis_weight()
        edge_index = graph.edge_index
        edge_type = graph.edge_attr
        rel_embed = torch.cat((rel_embed, self.loop_rel), dim=0)
        num_edges = edge_index[0].shape[0] // 2
        num_entities = x.shape[0]
        row, col = edge_index
        i_row, i_col = row[:num_edges], col[:num_edges]
        rev_row, rev_col = row[num_edges:], col[num_edges:]
        loop_index = torch.stack((torch.arange(num_entities), torch.arange(num_entities)))
        types, rev_types = edge_type[:num_edges], edge_type[num_edges:]
        loop_types = torch.full((num_entities,), rel_embed.shape[0] - 1, dtype=torch.long)
        in_norm = row_normalization(num_entities, i_row, i_col)
        rev_norm = row_normalization(num_entities, rev_row, rev_col)
        emb = self.message_passing(x, rel_embed, (i_row, i_col), types, 'in', in_norm)
        rev_emb = self.message_passing(x, rel_embed, (rev_row, rev_col), rev_types, 'out', rev_norm)
        loop_emb = self.message_passing(x, rel_embed, loop_index, loop_types, 'loop')
        out = 1 / 3 * (emb + rev_emb + loop_emb)
        if self.bias is not None:
            out += self.bias
        out = self.bn(out)
        return self.activation(out), torch.matmul(rel_embed, self.weight_rel)[:-1]

    def message_passing(self, x, rel_embed, edge_index, edge_types, mode, edge_weight=None):
        device = x.device
        tail_emb = x[edge_index[1]]
        rel_emb = rel_embed[edge_types]
        weight = getattr(self, f'weight_{mode}')
        trans_embed = self.rel_transform(tail_emb, rel_emb)
        trans_embed = torch.matmul(trans_embed, weight)
        dim = trans_embed.shape[1]
        if edge_weight is not None:
            trans_embed = trans_embed * edge_weight.unsqueeze(-1)
        embed = torch.zeros(x.shape[0], dim).scatter_add_(0, edge_index[0].unsqueeze(-1).repeat(1, dim), trans_embed)
        return F.dropout(embed, p=self.dropout, training=self.training)

    def rel_transform(self, ent_embed, rel_embed):
        if self.opn == 'sub':
            trans_embed = ent_embed - rel_embed
        elif self.opn == 'mult':
            trans_embed = ent_embed * rel_embed
        else:
            raise NotImplementedError(f'{self.opn}')
        return trans_embed


class CompGCN(nn.Module):

    def __init__(self, num_entities, num_rels, num_bases, in_feats, hidden_size, out_feats, layers, dropout, activation, opn):
        super(CompGCN, self).__init__()
        self.opn = opn
        self.num_rels = num_rels
        self.num_entities = num_entities
        if num_bases is not None and num_bases > 0:
            self.init_rel = nn.Embedding(num_bases, in_feats)
        else:
            self.init_rel = nn.Embedding(2 * num_rels, in_feats)
        self.convs = nn.ModuleList()
        if num_bases > 0:
            self.convs.append(CompGCNLayer(in_feats=in_feats, out_feats=hidden_size, num_rels=num_rels, opn=self.opn, num_bases=num_bases, activation=activation, dropout=dropout))
        else:
            self.convs.append(CompGCNLayer(in_feats=in_feats, out_feats=hidden_size, num_rels=num_rels, opn=self.opn, activation=activation, dropout=dropout))
        if layers == 2:
            self.convs.append(CompGCNLayer(in_feats=hidden_size, out_feats=out_feats, num_rels=num_rels, opn=self.opn, activation=activation, dropout=dropout))

    def forward(self, graph, x):
        rel_embed = self.init_rel.weight
        node_embed = x
        for layer in self.convs:
            node_embed, rel_embed = layer(graph, node_embed, rel_embed)
        return node_embed, rel_embed


class GNNLinkPredict(nn.Module):

    def __init__(self):
        super(GNNLinkPredict, self).__init__()
        self.edge_set = None

    def forward(self, graph):
        raise NotImplementedError

    def get_edge_set(self, edge_index, edge_types):
        if self.edge_set is None:
            edge_list = torch.stack((edge_index[0], edge_index[1], edge_types))
            edge_list = edge_list.cpu().T.numpy().tolist()
            torch.cuda.empty_cache()
            self.edge_set = set([tuple(x) for x in edge_list])

    def _loss(self, head_embed, tail_embed, rel_embed, labels, scoring):
        score = scoring(head_embed, tail_embed, rel_embed)
        prediction_loss = F.binary_cross_entropy_with_logits(score, labels.float())
        return prediction_loss

    def _regularization(self, embs):
        loss = 0
        for emb in embs:
            loss += torch.mean(emb.pow(2))
        return loss


def sampling_edge_uniform(edge_index, edge_types, edge_set, sampling_rate, num_rels, label_smoothing=0.0, num_entities=1):
    """
    Args:
        edge_index: edge index of graph
        edge_types:
        edge_set: set of all edges of the graph, (h, t, r)
        sampling_rate:
        num_rels:
        label_smoothing(Optional):
        num_entities (Optional):

    Returns:
        sampled_edges: sampled existing edges
        rels: types of smapled existing edges
        sampled_edges_all: existing edges with corrupted edges
        sampled_types_all: types of existing and corrupted edges
        labels: 0/1
    """
    num_edges = edge_index[0].shape[0]
    row, col = edge_index
    num_sampled_edges = int(num_edges * sampling_rate)
    selected_edges = np.random.choice(range(num_edges), num_sampled_edges, replace=False)
    row, col = row[selected_edges], col[selected_edges]
    sampled_edges = torch.stack([row, col])
    sampled_nodes = torch.unique(sampled_edges).cpu().numpy()
    heads = sampled_edges[0].cpu().numpy()
    tails = sampled_edges[1].cpu().numpy()
    rels = edge_types[selected_edges].cpu().numpy()
    torch.cuda.empty_cache()

    def to_set(head, tail, rel):
        triplets = np.stack((head, tail, rel), axis=0).T
        triplets_set = set([tuple(x) for x in triplets])
        return triplets_set
    corrupt_heads = np.random.choice(sampled_nodes, num_sampled_edges, replace=True)
    corrupt_heads = to_set(corrupt_heads, tails, rels)
    corrupt_tails = np.random.choice(sampled_nodes, num_sampled_edges, replace=True)
    corrupt_tails = to_set(heads, corrupt_tails, rels)
    corrupt_rels = np.random.choice(num_rels, num_sampled_edges, replace=True)
    corrupt_rels = to_set(heads, tails, corrupt_rels)
    corrupt_triplets = corrupt_heads.union(corrupt_tails).union(corrupt_rels)
    corrupt_triplets = corrupt_triplets.difference(edge_set)
    corrupt_triplets = torch.tensor(list(corrupt_triplets)).T
    _edge_index = corrupt_triplets[0:2]
    _edge_types = corrupt_triplets[2]
    sampled_edges_all = torch.cat((sampled_edges, _edge_index), dim=-1)
    edge_types_all = torch.cat((edge_types[selected_edges], _edge_types), dim=-1)
    labels = torch.tensor([1] * num_sampled_edges + [0] * _edge_index.shape[1])
    if label_smoothing > 0:
        labels = (1.0 - label_smoothing) * labels + 1.0 / num_entities
    return sampled_edges, torch.from_numpy(rels), sampled_edges_all, edge_types_all, labels


def autoscale_post(x, lower, upper):
    return torch.clamp(x, lower, upper)


def correlation_autoscale(preds, y, resid, residual_nid, scale=1.0):
    orig_diff = y[residual_nid].abs().sum() / residual_nid.shape[0]
    resid_scale = orig_diff / resid.abs().sum(dim=1, keepdim=True)
    resid_scale[resid_scale.isinf()] = 1.0
    cur_idxs = resid_scale > 1000
    resid_scale[cur_idxs] = 1.0
    res_result = preds + resid_scale * resid
    res_result[res_result.isnan()] = preds[res_result.isnan()]
    return res_result


def correlation_fixed(preds, y, resid, residual_nid, scale=1.0):
    return preds + scale * resid


def fixed_post(x, y, nid):
    x[nid] = y[nid]
    return x


def outcome_correlation(g, labels, alpha, nprop, post_step, alpha_term=True):
    result = labels.clone()
    for _ in range(nprop):
        result = alpha * spmm(g, result)
        if alpha_term:
            result += (1 - alpha) * labels
        else:
            result += labels
        result = post_step(result)
    return result


def pre_outcome_correlation(preds, labels, label_nid):
    """Generates the initial labels used for outcome correlation"""
    c = labels.max() + 1
    y = preds.clone()
    if len(label_nid) > 0:
        y[label_nid] = F.one_hot(labels[label_nid], c).float().squeeze(1)
    return y


def pre_residual_correlation(preds, labels, split_idx):
    labels[labels.isnan()] = 0
    labels = labels.long()
    nclass = labels.max().item() + 1
    nnode = preds.shape[0]
    err = torch.zeros((nnode, nclass), device=preds.device)
    err[split_idx] = F.one_hot(labels[split_idx], nclass).float().squeeze(1) - preds[split_idx]
    return err


class CorrectSmooth(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--correct-alpha', type=float, default=1.0)
        parser.add_argument('--smooth-alpha', type=float, default=0.8)
        parser.add_argument('--num-correct-prop', type=int, default=50)
        parser.add_argument('--num-smooth-prop', type=int, default=50)
        parser.add_argument('--autoscale', action='store_true')
        parser.add_argument('--correct-norm', type=str, default='sym')
        parser.add_argument('--smooth-norm', type=str, default='row')
        parser.add_argument('--scale', type=float, default=1.0)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.correct_alpha, args.smooth_alpha, args.num_correct_prop, args.num_smooth_prop, args.autoscale, args.correct_norm, args.smooth_norm, args.scale)

    def __init__(self, correct_alpha, smooth_alpha, num_correct_prop, num_smooth_prop, autoscale=False, correct_norm='row', smooth_norm='col', scale=1.0):
        super(CorrectSmooth, self).__init__()
        self.op_dict = {'correct_g': correct_norm, 'smooth_g': smooth_norm, 'num_correct_prop': num_correct_prop, 'num_smooth_prop': num_smooth_prop, 'correct_alpha': correct_alpha, 'smooth_alpha': smooth_alpha, 'autoscale': autoscale, 'scale': scale}

    def __call__(self, graph, x, train_only=True):
        g1 = graph
        g2 = Graph(edge_index=g1.edge_index)
        g1.normalize(self.op_dict['correct_g'])
        g2.normalize(self.op_dict['smooth_g'])
        train_nid, valid_nid, _ = g1.train_nid, g1.val_nid, g1.test_nid
        y = g1.y
        if train_only:
            label_nid = train_nid
            residual_nid = train_nid
        else:
            label_nid = torch.cat((train_nid, valid_nid))
            residual_nid = train_nid
        y = pre_residual_correlation(x, y, residual_nid)
        if self.op_dict['autoscale']:
            post_func = partial(autoscale_post, lower=-1.0, upper=1.0)
            scale_func = correlation_autoscale
        else:
            post_func = partial(fixed_post, y=y, nid=residual_nid)
            scale_func = correlation_fixed
        resid = outcome_correlation(g1, y, self.op_dict['correct_alpha'], nprop=self.op_dict['num_correct_prop'], post_step=post_func)
        res_result = scale_func(x, y, resid, residual_nid, self.op_dict['scale'])
        y = pre_outcome_correlation(res_result, g1.y, label_nid)
        result = outcome_correlation(g2, y, self.op_dict['smooth_alpha'], nprop=self.op_dict['num_smooth_prop'], post_step=partial(autoscale_post, lower=0, upper=1))
        return result


def diffusion(g, x, nhtop, p=1, alpha=0.5):
    x = x ** p
    for _ in range(nhtop):
        x = (1 - alpha) * x + alpha * spmm(g, x)
        x = x ** p
    return x


class CorrectSmoothMLP(BaseModel):

    @staticmethod
    def add_args(parser):
        CorrectSmooth.add_args(parser)
        MLP.add_args(parser)
        parser.add_argument('--use-embeddings', action='store_true')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args)

    def __init__(self, args):
        super(CorrectSmoothMLP, self).__init__()
        if args.use_embeddings:
            args.num_features = args.num_features * 2
        args.act_first = True if args.dataset == 'ogbn-products' else False
        self.use_embeddings = args.use_embeddings
        self.mlp = MLP.build_model_from_args(args)
        self.c_s = CorrectSmooth.build_model_from_args(args)
        self.rescale_feats = args.rescale_feats if hasattr(args, 'rescale_feats') else args.dataset == 'ogbn-arxiv'
        self.cache_x = None

    def forward(self, graph):
        if self.cache_x is not None:
            x = self.cache_x
        elif self.use_embeddings:
            _x = graph.x.contiguous()
            _x = diffusion(graph, _x, nhtop=10)
            x = torch.cat([graph.x, _x], dim=1)
            if self.rescale_feats:
                x = (x - x.mean(0)) / x.std(0)
            self.cache_x = x
        else:
            x = graph.x
        out = self.mlp(x)
        return out

    def predict(self, data):
        out = self.forward(data)
        return out

    def postprocess(self, data, out):
        if len(data.y.shape) == 1:
            out = F.softmax(out, dim=-1)
        out = self.c_s(data, out)
        return out


class DAEGC(BaseModel):
    """The DAEGC model from the `"Attributed Graph Clustering: A Deep Attentional Embedding Approach"
    <https://arxiv.org/abs/1906.06532>`_ paper

    Args:
        num_clusters (int) : Number of clusters.
        T (int) : Number of iterations to recalculate P and Q
        gamma (float) : Hyperparameter that controls two parts of the loss.
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--hidden-size', type=int, default=256)
        parser.add_argument('--embedding-size', type=int, default=16)
        parser.add_argument('--num-heads', type=int, default=1)
        parser.add_argument('--dropout', type=float, default=0)
        parser.add_argument('--epochs', type=int, default=100)
        parser.add_argument('--lr', type=float, default=0.001)
        parser.add_argument('--gamma', type=float, default=10)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.embedding_size, args.num_heads, args.dropout, args.num_clusters)

    def __init__(self, num_features, hidden_size, embedding_size, num_heads, dropout, num_clusters):
        super(DAEGC, self).__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.embedding_size = embedding_size
        self.dropout = dropout
        self.num_clusters = num_clusters
        self.att1 = GATLayer(num_features, hidden_size, attn_drop=dropout, alpha=0.2, nhead=num_heads)
        self.att2 = GATLayer(hidden_size * num_heads, embedding_size, attn_drop=dropout, alpha=0.2, nhead=1)
        self.cluster_center = torch.nn.Parameter(torch.FloatTensor(self.num_clusters))

    def set_cluster_center(self, center):
        self.cluster_center.data = center

    def get_cluster_center(self):
        return self.cluster_center.data.detach()

    def forward(self, graph):
        x = graph.x
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.elu(self.att1(graph, x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.elu(self.att2(graph, x))
        return F.normalize(x, p=2, dim=1)

    def get_2hop(self, edge_index):
        """add 2-hop neighbors as new edges"""
        G = nx.Graph()
        edge_index = torch.stack(edge_index)
        G.add_edges_from(edge_index.t().tolist())
        H = nx.Graph()
        for i in range(G.number_of_nodes()):
            layers = dict(nx.bfs_successors(G, source=i, depth_limit=2))
            for succ in layers:
                for idx in layers[succ]:
                    H.add_edge(i, idx)
        return torch.tensor(list(H.edges())).t()

    def get_features(self, data):
        return self.forward(data).detach()

    def recon_loss(self, z, adj):
        return F.binary_cross_entropy(F.sigmoid(torch.mm(z, z.t())), adj, reduction='sum')


class DeeperGCN(BaseModel):
    """Implementation of DeeperGCN in paper `"DeeperGCN: All You Need to Train Deeper GCNs" <https://arxiv.org/abs/2006.07739>`_

    Args:
        in_feat (int): the dimension of input features
        hidden_size (int): the dimension of hidden representation
        out_feat (int): the dimension of output features
        num_layers (int): the number of layers
        activation (str, optional): activation function. Defaults to "relu".
        dropout (float, optional): dropout rate. Defaults to 0.0.
        aggr (str, optional): aggregation function. Defaults to "max".
        beta (float, optional): a coefficient for aggregation function. Defaults to 1.0.
        p (float, optional): a coefficient for aggregation function. Defaults to 1.0.
        learn_beta (bool, optional): whether beta is learnable. Defaults to False.
        learn_p (bool, optional): whether p is learnable. Defaults to False.
        learn_msg_scale (bool, optional): whether message scale is learnable. Defaults to True.
        use_msg_norm (bool, optional): use message norm or not. Defaults to False.
        edge_attr_size (int, optional): the dimension of edge features. Defaults to None.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--num-layers', type=int, default=14)
        parser.add_argument('--hidden-size', type=int, default=128)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--activation', type=str, default='relu')
        parser.add_argument('--aggr', type=str, default='softmax_sg')
        parser.add_argument('--beta', type=float, default=1.0)
        parser.add_argument('--p', type=float, default=1.0)
        parser.add_argument('--learn-beta', action='store_true')
        parser.add_argument('--learn-p', action='store_true')
        parser.add_argument('--learn-msg-scale', action='store_true')
        parser.add_argument('--use-msg-norm', action='store_true')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(in_feat=args.num_features, hidden_size=args.hidden_size, out_feat=args.num_classes, num_layers=args.num_layers, activation=args.activation, dropout=args.dropout, aggr=args.aggr, beta=args.beta, p=args.p, learn_beta=args.learn_beta, learn_p=args.learn_p, learn_msg_scale=args.learn_msg_scale, use_msg_norm=args.use_msg_norm, edge_attr_size=args.edge_attr_size)

    def __init__(self, in_feat, hidden_size, out_feat, num_layers, activation='relu', dropout=0.0, aggr='max', beta=1.0, p=1.0, learn_beta=False, learn_p=False, learn_msg_scale=True, use_msg_norm=False, edge_attr_size=None):
        super(DeeperGCN, self).__init__()
        self.dropout = dropout
        self.feat_encoder = nn.Linear(in_feat, hidden_size)
        self.layers = nn.ModuleList()
        for i in range(num_layers - 1):
            self.layers.append(ResGNNLayer(conv=GENConv(in_feats=hidden_size, out_feats=hidden_size, aggr=aggr, beta=beta, p=p, learn_beta=learn_beta, learn_p=learn_p, use_msg_norm=use_msg_norm, learn_msg_scale=learn_msg_scale, edge_attr_size=edge_attr_size), in_channels=hidden_size, activation=activation, dropout=dropout, checkpoint_grad=False))
        self.norm = nn.BatchNorm1d(hidden_size, affine=True)
        self.activation = get_activation(activation)
        self.fc = nn.Linear(hidden_size, out_feat)

    def forward(self, graph):
        x = graph.x
        h = self.feat_encoder(x)
        for layer in self.layers:
            h = layer(graph, h)
        h = self.activation(self.norm(h))
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.fc(h)
        return h

    def predict(self, graph):
        return self.forward(graph)


class GCNConv(nn.Module):

    def __init__(self, in_feats, out_feats):
        super(GCNConv, self).__init__()
        self.weight = nn.Linear(in_features=in_feats, out_features=out_feats)
        self.edge_index = None
        self.edge_attr = None

    def forward(self, graph, x):
        h = self.weight(x)
        h = spmm(graph, h)
        return h

    def forward_aux(self, x):
        return self.weight(x)


class GCN(BaseModel):

    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout):
        super(GCN, self).__init__()
        self.num_features = num_features
        self.num_classes = num_classes
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]
        self.convs = nn.ModuleList([GCNConv(shapes[layer], shapes[layer + 1], cached=False) for layer in range(num_layers)])

    def forward(self, graph):
        x = graph.x
        edge_index, edge_weight = torch.stack(graph.edge_index), graph.edge_weight
        for conv in self.convs[:-1]:
            x = F.relu(conv(x, edge_index, edge_weight))
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index, edge_weight)
        return F.log_softmax(x, dim=1)


class DGIModel(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--hidden-size', type=int, default=512)
        parser.add_argument('--activation', type=str, default='prelu')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.activation)

    def __init__(self, in_feats, hidden_size, activation):
        super(DGIModel, self).__init__()
        self.gcn = GCN(in_feats, hidden_size, activation)
        self.sparse = True

    def forward(self, graph):
        graph.sym_norm()
        x = graph.x
        logits = self.gcn(graph, x, self.sparse)
        return logits

    def embed(self, data):
        h_1 = self.gcn(data, data.x, self.sparse)
        return h_1.detach()


class EntropyLoss(nn.Module):

    def forward(self, adj, anext, s_l):
        entropy = torch.distributions.Categorical(probs=s_l).entropy().mean()
        assert not torch.isnan(entropy)
        return entropy


class LinkPredLoss(nn.Module):

    def forward(self, adj, anext, s_l):
        link_pred_loss = (adj - s_l.matmul(s_l.transpose(-1, -2))).norm(dim=(1, 2))
        link_pred_loss = link_pred_loss / (adj.size(1) * adj.size(2))
        return link_pred_loss.mean()


class GraphSAGE(nn.Module):
    """GraphSAGE from `"Inductive Representation Learning on Large Graphs" <https://arxiv.org/pdf/1706.02216.pdf>`__.

    ..math::
        h^{i+1}_{\\mathcal{N}(v)}=AGGREGATE_{k}(h_{u}^{k})
        h^{k+1}_{v} = \\sigma(\\mathbf{W}^{k}·CONCAT(h_{v}^{k}, h_{\\mathcal{N}(v)}))

    Args:
        in_feats (int) : Size of each input sample.
        hidden_dim (int) : Size of hidden layer dimension.
        out_feats (int) : Size of each output sample.
        num_layers (int) : Number of GraphSAGE Layers.
        dropout (float, optional) : Size of dropout, default: ``0.5``.
        normalize (bool, optional) : Normalze features after each layer if True, default: ``True``.
    """

    def __init__(self, in_feats, hidden_dim, out_feats, num_layers, dropout=0.5, normalize=False, concat=False, use_bn=False):
        super(GraphSAGE, self).__init__()
        self.convlist = nn.ModuleList()
        self.bn_list = nn.ModuleList()
        self.num_layers = num_layers
        self.dropout = dropout
        self.use_bn = use_bn
        aggr = 'concat' if concat else 'mean'
        if num_layers == 1:
            self.convlist.append(SAGELayer(in_feats, out_feats, normalize, aggr))
        else:
            self.convlist.append(SAGELayer(in_feats, hidden_dim, normalize, aggr))
            if use_bn:
                self.bn_list.append(nn.BatchNorm1d(hidden_dim))
            for _ in range(num_layers - 2):
                self.convlist.append(SAGELayer(hidden_dim, hidden_dim, normalize, aggr))
                if use_bn:
                    self.bn_list.append(nn.BatchNorm1d(hidden_dim))
            self.convlist.append(SAGELayer(hidden_dim, out_feats, normalize, aggr))

    def forward(self, graph, x):
        h = x
        for i in range(self.num_layers - 1):
            h = F.dropout(h, p=self.dropout, training=self.training)
            h = self.convlist[i](graph, h)
            if self.use_bn:
                h = self.bn_list[i](h)
        return self.convlist[self.num_layers - 1](graph, h)


class BatchedGraphSAGE(nn.Module):
    """GraphSAGE with mini-batch

    Args:
        in_feats (int) : Size of each input sample.
        out_feats (int) : Size of each output sample.
        use_bn (bool) : Apply batch normalization if True, default: ``True``.
        self_loop (bool) : Add self loop if True, default: ``True``.
    """

    def __init__(self, in_feats, out_feats, use_bn=True, self_loop=True):
        super(BatchedGraphSAGE, self).__init__()
        self.self_loop = self_loop
        self.use_bn = use_bn
        self.weight = nn.Linear(in_feats, out_feats, bias=True)
        nn.init.xavier_uniform_(self.weight.weight.data, gain=nn.init.calculate_gain('relu'))

    def forward(self, x, adj):
        device = x.device
        if self.self_loop:
            adj = adj + torch.eye(x.shape[1])
        adj = adj / adj.sum(dim=1, keepdim=True)
        h = torch.matmul(adj, x)
        h = self.weight(h)
        h = F.normalize(h, dim=2, p=2)
        h = F.relu(h)
        return h


class BatchedDiffPoolLayer(nn.Module):
    """DIFFPOOL from paper `"Hierarchical Graph Representation Learning
    with Differentiable Pooling" <https://arxiv.org/pdf/1806.08804.pdf>`__.

    .. math::
        X^{(l+1)} = S^{l)}^T Z^{(l)}
        A^{(l+1)} = S^{(l)}^T A^{(l)} S^{(l)}
        Z^{(l)} = GNN_{l, embed}(A^{(l)}, X^{(l)})
        S^{(l)} = softmax(GNN_{l,pool}(A^{(l)}, X^{(l)}))

    Parameters
    ----------
    in_feats : int
        Size of each input sample.
    out_feats : int
        Size of each output sample.
    assign_dim : int
        Size of next adjacency matrix.
    batch_size : int
        Size of each mini-batch.
    dropout : float, optional
        Size of dropout, default: ``0.5``.
    link_pred_loss : bool, optional
        Use link prediction loss if True, default: ``True``.
    """

    def __init__(self, in_feats, out_feats, assign_dim, batch_size, dropout=0.5, link_pred_loss=True, entropy_loss=True):
        super(BatchedDiffPoolLayer, self).__init__()
        self.assign_dim = assign_dim
        self.dropout = dropout
        self.use_link_pred = link_pred_loss
        self.batch_size = batch_size
        self.embd_gnn = SAGELayer(in_feats, out_feats, normalize=False)
        self.pool_gnn = SAGELayer(in_feats, assign_dim, normalize=False)
        self.loss_dict = dict()

    def forward(self, graph, x, batch):
        embed = self.embd_gnn(graph, x)
        pooled = F.softmax(self.pool_gnn(graph, x), dim=-1)
        device = x.device
        masked_tensor = []
        value_set, value_counts = torch.unique(batch, return_counts=True)
        batch_size = len(value_set)
        for i in value_counts:
            masked = torch.ones((i, int(pooled.size()[1] / batch_size)))
            masked_tensor.append(masked)
        masked = torch.FloatTensor(block_diag(*masked_tensor))
        result = torch.nn.functional.softmax(masked * pooled, dim=-1)
        result = result * masked
        result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)
        h = torch.matmul(result.t(), embed)
        adj = torch.sparse_coo_tensor(torch.stack(graph.edge_index), graph.edge_weight)
        adj_new = torch.sparse.mm(adj, result)
        adj_new = torch.mm(result.t(), adj_new)
        if self.use_link_pred:
            adj_loss = torch.norm(adj.to_dense() - torch.mm(result, result.t())) / np.power(len(batch), 2)
            self.loss_dict['adj_loss'] = adj_loss
        entropy_loss = torch.distributions.Categorical(probs=pooled).entropy().mean()
        assert not torch.isnan(entropy_loss)
        self.loss_dict['entropy_loss'] = entropy_loss
        return adj_new, h

    def get_loss(self):
        loss_n = 0
        for _, value in self.loss_dict.items():
            loss_n += value
        return loss_n


class BatchedDiffPool(nn.Module):
    """DIFFPOOL layer with batch forward

    Parameters
    ----------
    in_feats : int
        Size of each input sample.
    next_size : int
        Size of next adjacency matrix.
    emb_size : int
        Dimension of next node feature matrix.
    use_bn : bool, optional
        Apply batch normalization if True, default: ``True``.
    self_loop : bool, optional
        Add self loop if True, default: ``True``.
    use_link_loss : bool, optional
        Use link prediction loss if True, default: ``True``.
    use_entropy : bool, optioinal
        Use entropy prediction loss if True, default: ``True``.
    """

    def __init__(self, in_feats, next_size, emb_size, use_bn=True, self_loop=True, use_link_loss=False, use_entropy=True):
        super(BatchedDiffPool, self).__init__()
        self.use_link_loss = use_link_loss
        self.use_bn = use_bn
        self.feat_trans = BatchedGraphSAGE(in_feats, emb_size)
        self.assign_trans = BatchedGraphSAGE(in_feats, next_size)
        self.link_loss = LinkPredLoss()
        self.entropy = EntropyLoss()
        self.loss_module = nn.ModuleList()
        if use_link_loss:
            self.loss_module.append(LinkPredLoss())
        if use_entropy:
            self.loss_module.append(EntropyLoss())
        self.loss = {}

    def forward(self, x, adj):
        h = self.feat_trans(x, adj)
        next_l = F.softmax(self.assign_trans(x, adj), dim=-1)
        h = torch.matmul(next_l.transpose(-1, -2), h)
        next = torch.matmul(next_l.transpose(-1, -2), torch.matmul(adj, next_l))
        for layer in self.loss_module:
            self.loss[str(type(layer).__name__)] = layer(adj, next, next_l)
        return h, next

    def get_loss(self):
        value = 0
        for _, v in self.loss.items():
            value += v
        return value


def split_dataset_general(dataset, args):
    droplast = args.model == 'diffpool'
    train_size = int(len(dataset) * args.train_ratio)
    test_size = int(len(dataset) * args.test_ratio)
    index = list(range(len(dataset)))
    random.shuffle(index)
    train_index = index[:train_size]
    test_index = index[-test_size:]
    bs = args.batch_size
    train_dataset = dict(dataset=[dataset[i] for i in train_index], batch_size=bs, drop_last=droplast)
    test_dataset = dict(dataset=[dataset[i] for i in test_index], batch_size=bs, drop_last=droplast)
    if args.train_ratio + args.test_ratio < 1:
        val_index = index[train_size:-test_size]
        valid_dataset = dict(dataset=[dataset[i] for i in val_index], batch_size=bs, drop_last=droplast)
    else:
        valid_dataset = test_dataset
    return train_dataset, valid_dataset, test_dataset


def toBatchedGraph(batch_adj, batch_feat, node_per_pool_graph):
    adj_list = [batch_adj[i:i + node_per_pool_graph, i:i + node_per_pool_graph] for i in range(0, batch_adj.size()[0], node_per_pool_graph)]
    feat_list = [batch_feat[i:i + node_per_pool_graph, :] for i in range(0, batch_adj.size()[0], node_per_pool_graph)]
    adj_list = list(map(lambda x: torch.unsqueeze(x, 0), adj_list))
    feat_list = list(map(lambda x: torch.unsqueeze(x, 0), feat_list))
    adj = torch.cat(adj_list, dim=0)
    feat = torch.cat(feat_list, dim=0)
    return adj, feat


class DiffPool(BaseModel):
    """DIFFPOOL from paper `Hierarchical Graph Representation Learning
    with Differentiable Pooling <https://arxiv.org/pdf/1806.08804.pdf>`__.

    Parameters
    ----------
    in_feats : int
        Size of each input sample.
    hidden_dim : int
        Size of hidden layer dimension of GNN.
    embed_dim : int
        Size of embeded node feature, output size of GNN.
    num_classes : int
        Number of target classes.
    num_layers : int
        Number of GNN layers.
    num_pool_layers : int
        Number of pooling.
    assign_dim : int
        Embedding size after the first pooling.
    pooling_ratio : float
        Size of each poolling ratio.
    batch_size : int
        Size of each mini-batch.
    dropout : float, optional
        Size of dropout, default: `0.5`.
    no_link_pred : bool, optional
        If True, use link prediction loss, default: `True`.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--num-pooling-layers', type=int, default=1)
        parser.add_argument('--no-link-pred', dest='no_link_pred', action='store_true')
        parser.add_argument('--pooling-ratio', type=float, default=0.15)
        parser.add_argument('--embedding-dim', type=int, default=64)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.1)
        parser.add_argument('--batch-size', type=int, default=20)
        parser.add_argument('--train-ratio', type=float, default=0.7)
        parser.add_argument('--test-ratio', type=float, default=0.1)
        parser.add_argument('--lr', type=float, default=0.001)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.embedding_dim, args.num_classes, args.num_layers, args.num_pooling_layers, int(args.max_graph_size * args.pooling_ratio) * args.batch_size, args.pooling_ratio, args.batch_size, args.dropout, args.no_link_pred)

    @classmethod
    def split_dataset(cls, dataset, args):
        return split_dataset_general(dataset, args)

    def __init__(self, in_feats, hidden_dim, embed_dim, num_classes, num_layers, num_pool_layers, assign_dim, pooling_ratio, batch_size, dropout=0.5, no_link_pred=True, concat=False, use_bn=False):
        super(DiffPool, self).__init__()
        self.assign_dim = assign_dim
        self.assign_dim_list = [assign_dim]
        self.use_bn = use_bn
        self.dropout = dropout
        self.use_link_loss = not no_link_pred
        self.diffpool_layers = nn.ModuleList()
        self.before_pooling = GraphSAGE(in_feats, hidden_dim, embed_dim, num_layers=num_layers, dropout=dropout, use_bn=self.use_bn)
        self.init_diffpool = BatchedDiffPoolLayer(embed_dim, hidden_dim, assign_dim, batch_size, dropout, self.use_link_loss)
        pooled_emb_dim = embed_dim
        self.after_pool = nn.ModuleList()
        after_per_pool = nn.ModuleList()
        for _ in range(num_layers - 1):
            after_per_pool.append(BatchedGraphSAGE(hidden_dim, hidden_dim))
        after_per_pool.append(BatchedGraphSAGE(hidden_dim, pooled_emb_dim))
        self.after_pool.append(after_per_pool)
        for _ in range(num_pool_layers - 1):
            self.assign_dim = int(self.assign_dim // batch_size * pooling_ratio) * batch_size
            self.diffpool_layers.append(BatchedDiffPool(pooled_emb_dim, self.assign_dim, hidden_dim, use_bn=self.use_bn, use_link_loss=self.use_link_loss))
            for _ in range(num_layers - 1):
                after_per_pool.append(BatchedGraphSAGE(hidden_dim, hidden_dim))
            after_per_pool.append(BatchedGraphSAGE(hidden_dim, pooled_emb_dim))
            self.after_pool.append(after_per_pool)
            self.assign_dim_list.append(self.assign_dim)
        if concat:
            out_dim = pooled_emb_dim * (num_pool_layers + 1)
        else:
            out_dim = pooled_emb_dim
        self.fc = nn.Linear(out_dim, num_classes)

    def reset_parameters(self):
        for i in self.modules():
            if isinstance(i, nn.Linear):
                nn.init.xavier_uniform_(i.weight.data, gain=nn.init.calculate_gain('relu'))
                if i.bias is not None:
                    nn.init.constant_(i.bias.data, 0.0)

    def after_pooling_forward(self, gnn_layers, adj, x, concat=False):
        readouts = []
        h = x
        for layer in gnn_layers:
            h = layer(h, adj)
            readouts.append(h)
        return h

    def forward(self, batch):
        readouts_all = []
        init_emb = self.before_pooling(batch, batch.x)
        adj, h = self.init_diffpool(batch, init_emb, batch.batch)
        value_set, value_counts = torch.unique(batch.batch, return_counts=True)
        batch_size = len(value_set)
        adj, h = toBatchedGraph(adj, h, adj.size(0) // batch_size)
        h = self.after_pooling_forward(self.after_pool[0], adj, h)
        readout = torch.sum(h, dim=1)
        readouts_all.append(readout)
        for i, diff_layer in enumerate(self.diffpool_layers):
            h, adj = diff_layer(h, adj)
            h = self.after_pooling_forward(self.after_pool[i + 1], adj, h)
            readout = torch.sum(h, dim=1)
            readouts_all.append(readout)
        pred = self.fc(readout)
        return pred

    def graph_classificatoin_loss(self, batch):
        pred = self.forward(batch)
        pred = F.log_softmax(pred, dim=-1)
        loss_n = F.nll_loss(pred, batch.y)
        loss_n += self.init_diffpool.get_loss()
        for layer in self.diffpool_layers:
            loss_n += layer.get_loss()
        return loss_n


class DisenGCN(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--K', type=int, nargs='+', default=[16, 8])
        parser.add_argument('--iterations', type=int, default=7)
        parser.add_argument('--tau', type=float, default=1)
        parser.add_argument('--activation', type=str, default='leaky_relu')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(in_feats=args.num_features, hidden_size=args.hidden_size, num_classes=args.num_classes, K=args.K, iterations=args.iterations, tau=args.tau, dropout=args.dropout, activation=args.activation)

    def __init__(self, in_feats, hidden_size, num_classes, K, iterations, tau, dropout, activation):
        super(DisenGCN, self).__init__()
        self.K = K
        self.iterations = iterations
        self.dropout = dropout
        self.activation = activation
        self.num_layers = len(K)
        self.weight = nn.Parameter(torch.Tensor(hidden_size, num_classes))
        self.bias = nn.Parameter(torch.Tensor(num_classes))
        self.reset_parameters()
        shapes = [in_feats] + [hidden_size] * self.num_layers
        self.layers = nn.ModuleList(DisenGCNLayer(shapes[i], shapes[i + 1], K[i], iterations, tau, activation) for i in range(self.num_layers))

    def reset_parameters(self):
        nn.init.xavier_normal_(self.weight.data, gain=1.414)
        nn.init.zeros_(self.bias.data)

    def forward(self, graph):
        h = graph.x
        graph.remove_self_loops()
        for layer in self.layers:
            h = layer(graph, h)
            h = F.dropout(h, p=self.dropout, training=self.training)
        out = torch.matmul(h, self.weight) + self.bias
        return out

    def predict(self, data):
        return self.forward(data)


class DrGAT(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=8)
        parser.add_argument('--nhead', type=int, default=8)
        parser.add_argument('--dropout', type=float, default=0.6)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.num_classes, args.hidden_size, args.nhead, args.dropout)

    def __init__(self, num_features, num_classes, hidden_size, num_heads, dropout):
        super(DrGAT, self).__init__()
        self.num_features = num_features
        self.num_classes = num_classes
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.dropout = dropout
        self.conv1 = GATLayer(num_features, hidden_size, nhead=num_heads, attn_drop=dropout)
        self.conv2 = GATLayer(hidden_size * num_heads, num_classes, nhead=1, attn_drop=dropout)
        self.se1 = SELayer(num_features, se_channels=int(np.sqrt(num_features)))
        self.se2 = SELayer(hidden_size * num_heads, se_channels=int(np.sqrt(hidden_size * num_heads)))

    def forward(self, graph):
        x = graph.x
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.se1(x)
        x = F.elu(self.conv1(graph, x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.se2(x)
        x = F.elu(self.conv2(graph, x))
        return x


class DrGCN(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=16)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--norm', type=str, default=None)
        parser.add_argument('--activation', type=str, default='relu')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.num_classes, args.hidden_size, args.num_layers, args.dropout, args.norm, args.activation)

    def __init__(self, num_features, num_classes, hidden_size, num_layers, dropout, norm=None, activation='relu'):
        super(DrGCN, self).__init__()
        self.num_features = num_features
        self.num_classes = num_classes
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        shapes = [num_features] + [hidden_size] * (num_layers - 1) + [num_classes]
        self.convs = nn.ModuleList([GCNLayer(shapes[layer], shapes[layer + 1], activation=activation, norm=norm) for layer in range(num_layers - 1)])
        self.convs.append(GCNLayer(shapes[-2], shapes[-1]))
        self.ses = nn.ModuleList([SELayer(shapes[layer], se_channels=int(np.sqrt(shapes[layer]))) for layer in range(num_layers)])

    def forward(self, graph):
        graph.sym_norm()
        x = graph.x
        x = self.ses[0](x)
        for se, conv in zip(self.ses[1:], self.convs[:-1]):
            x = conv(graph, x)
            x = se(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](graph, x)
        return x

    def predict(self, graph):
        return self.forward(graph)


class GraphConvolutionBS(Module):
    """
    GCN Layer with BN, Self-loop and Res connection.
    """

    def __init__(self, in_features, out_features, activation=lambda x: x, withbn=True, withloop=True, bias=True, res=False):
        """
        Initial function.
        :param in_features: the input feature dimension.
        :param out_features: the output feature dimension.
        :param activation: the activation function.
        :param withbn: using batch normalization.
        :param withloop: using self feature modeling.
        :param bias: enable bias.
        :param res: enable res connections.
        """
        super(GraphConvolutionBS, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.sigma = activation
        self.res = res
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        self.self_weight = Parameter(torch.FloatTensor(in_features, out_features)) if withloop else None
        self.bn = torch.nn.BatchNorm1d(out_features) if withbn else None
        self.bias = Parameter(torch.FloatTensor(out_features)) if bias else None
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.self_weight is not None:
            stdv = 1.0 / math.sqrt(self.self_weight.size(1))
            self.self_weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, graph, x):
        support = torch.mm(x, self.weight)
        output = spmm(graph, support)
        output = output + torch.mm(x, self.self_weight) if self.self_weight is not None else output
        output = output + self.bias if self.bias is not None else output
        output = self.bn(output) if self.bn is not None else output
        return self.sigma(output) + input if self.res else self.sigma(output)

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'


class GraphBaseBlock(Module):
    """
    The base block for Multi-layer GCN / ResGCN / Dense GCN
    """

    def __init__(self, in_features, out_features, nbaselayer, withbn=True, withloop=True, activation=F.relu, dropout=True, aggrmethod='concat', dense=False):
        """
        The base block for constructing DeepGCN model.
        :param in_features: the input feature dimension.
        :param out_features: the hidden feature dimension.
        :param nbaselayer: the number of layers in the base block.
        :param withbn: using batch normalization in graph convolution.
        :param withloop: using self feature modeling in graph convolution.
        :param activation: the activation function, default is ReLu.
        :param dropout: the dropout ratio.
        :param aggrmethod: the aggregation function for baseblock, can be "concat" and "add". For "resgcn", the default
                           is "add", for others the default is "concat".
        :param dense: enable dense connection
        """
        super(GraphBaseBlock, self).__init__()
        self.in_features = in_features
        self.hiddendim = out_features
        self.nhiddenlayer = nbaselayer
        self.activation = activation
        self.aggrmethod = aggrmethod
        self.dense = dense
        self.dropout = dropout
        self.withbn = withbn
        self.withloop = withloop
        self.hiddenlayers = nn.ModuleList()
        self.__makehidden()
        if self.aggrmethod == 'concat' and dense is False:
            self.out_features = in_features + out_features
        elif self.aggrmethod == 'concat' and dense is True:
            self.out_features = in_features + out_features * nbaselayer
        elif self.aggrmethod == 'add':
            if in_features != self.hiddendim:
                raise RuntimeError('The dimension of in_features and hiddendim should be matched in add model.')
            self.out_features = out_features
        elif self.aggrmethod == 'nores':
            self.out_features = out_features
        else:
            raise NotImplementedError("The aggregation method only support 'concat','add' and 'nores'.")

    def __makehidden(self):
        for i in range(self.nhiddenlayer):
            if i == 0:
                layer = GraphConvolutionBS(self.in_features, self.hiddendim, self.activation, self.withbn, self.withloop)
            else:
                layer = GraphConvolutionBS(self.hiddendim, self.hiddendim, self.activation, self.withbn, self.withloop)
            self.hiddenlayers.append(layer)

    def _doconcat(self, x, subx):
        if x is None:
            return subx
        if self.aggrmethod == 'concat':
            return torch.cat((x, subx), 1)
        elif self.aggrmethod == 'add':
            return x + subx
        elif self.aggrmethod == 'nores':
            return x

    def forward(self, graph, x):
        h = x
        denseout = None
        for gc in self.hiddenlayers:
            denseout = self._doconcat(denseout, h)
            h = gc(graph, h)
            h = F.dropout(h, self.dropout, training=self.training)
        if not self.dense:
            return self._doconcat(h, x)
        return self._doconcat(h, denseout) if denseout is not None else h

    def get_outdim(self):
        return self.out_features

    def __repr__(self):
        return '%s %s (%d - [%d:%d] > %d)' % (self.__class__.__name__, self.aggrmethod, self.in_features, self.hiddendim, self.nhiddenlayer, self.out_features)


class MultiLayerGCNBlock(Module):
    """
    Muti-Layer GCN with same hidden dimension.
    """

    def __init__(self, in_features, out_features, nbaselayer, withbn=True, withloop=True, activation=F.relu, dropout=True, aggrmethod=None, dense=None):
        """
        The multiple layer GCN block.
        :param in_features: the input feature dimension.
        :param out_features: the hidden feature dimension.
        :param nbaselayer: the number of layers in the base block.
        :param withbn: using batch normalization in graph convolution.
        :param withloop: using self feature modeling in graph convolution.
        :param activation: the activation function, default is ReLu.
        :param dropout: the dropout ratio.
        :param aggrmethod: not applied.
        :param dense: not applied.
        """
        super(MultiLayerGCNBlock, self).__init__()
        self.model = GraphBaseBlock(in_features=in_features, out_features=out_features, nbaselayer=nbaselayer, withbn=withbn, withloop=withloop, activation=activation, dropout=dropout, dense=False, aggrmethod='nores')

    def forward(self, graph, x):
        return self.model.forward(graph, x)

    def get_outdim(self):
        return self.model.get_outdim()

    def __repr__(self):
        return '%s %s (%d - [%d:%d] > %d)' % (self.__class__.__name__, self.aggrmethod, self.model.in_features, self.model.hiddendim, self.model.nhiddenlayer, self.model.out_features)


class ResGCNBlock(Module):
    """
    The multiple layer GCN with residual connection block.
    """

    def __init__(self, in_features, out_features, nbaselayer, withbn=True, withloop=True, activation=F.relu, dropout=True, aggrmethod=None, dense=None):
        """
        The multiple layer GCN with residual connection block.
        :param in_features: the input feature dimension.
        :param out_features: the hidden feature dimension.
        :param nbaselayer: the number of layers in the base block.
        :param withbn: using batch normalization in graph convolution.
        :param withloop: using self feature modeling in graph convolution.
        :param activation: the activation function, default is ReLu.
        :param dropout: the dropout ratio.
        :param aggrmethod: not applied.
        :param dense: not applied.
        """
        super(ResGCNBlock, self).__init__()
        self.model = GraphBaseBlock(in_features=in_features, out_features=out_features, nbaselayer=nbaselayer, withbn=withbn, withloop=withloop, activation=activation, dropout=dropout, dense=False, aggrmethod='add')

    def forward(self, graph, x):
        return self.model.forward(graph, x)

    def get_outdim(self):
        return self.model.get_outdim()

    def __repr__(self):
        return '%s %s (%d - [%d:%d] > %d)' % (self.__class__.__name__, self.aggrmethod, self.model.in_features, self.model.hiddendim, self.model.nhiddenlayer, self.model.out_features)


class DenseGCNBlock(Module):
    """
    The multiple layer GCN with dense connection block.
    """

    def __init__(self, in_features, out_features, nbaselayer, withbn=True, withloop=True, activation=F.relu, dropout=True, aggrmethod='concat', dense=True):
        """
        The multiple layer GCN with dense connection block.
        :param in_features: the input feature dimension.
        :param out_features: the hidden feature dimension.
        :param nbaselayer: the number of layers in the base block.
        :param withbn: using batch normalization in graph convolution.
        :param withloop: using self feature modeling in graph convolution.
        :param activation: the activation function, default is ReLu.
        :param dropout: the dropout ratio.
        :param aggrmethod: the aggregation function for the output. For denseblock, default is "concat".
        :param dense: default is True, cannot be changed.
        """
        super(DenseGCNBlock, self).__init__()
        self.model = GraphBaseBlock(in_features=in_features, out_features=out_features, nbaselayer=nbaselayer, withbn=withbn, withloop=withloop, activation=activation, dropout=dropout, dense=True, aggrmethod=aggrmethod)

    def forward(self, graph, x):
        return self.model.forward(graph, x)

    def get_outdim(self):
        return self.model.get_outdim()

    def __repr__(self):
        return '%s %s (%d - [%d:%d] > %d)' % (self.__class__.__name__, self.aggrmethod, self.model.in_features, self.model.hiddendim, self.model.nhiddenlayer, self.model.out_features)


class InceptionGCNBlock(Module):
    """
    The multiple layer GCN with inception connection block.
    """

    def __init__(self, in_features, out_features, nbaselayer, withbn=True, withloop=True, activation=F.relu, dropout=True, aggrmethod='concat', dense=False):
        """
        The multiple layer GCN with inception connection block.
        :param in_features: the input feature dimension.
        :param out_features: the hidden feature dimension.
        :param nbaselayer: the number of layers in the base block.
        :param withbn: using batch normalization in graph convolution.
        :param withloop: using self feature modeling in graph convolution.
        :param activation: the activation function, default is ReLu.
        :param dropout: the dropout ratio.
        :param aggrmethod: the aggregation function for baseblock, can be "concat" and "add". For "resgcn", the default
                           is "add", for others the default is "concat".
        :param dense: not applied. The default is False, cannot be changed.
        """
        super(InceptionGCNBlock, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.hiddendim = out_features
        self.nbaselayer = nbaselayer
        self.activation = activation
        self.aggrmethod = aggrmethod
        self.dropout = dropout
        self.withbn = withbn
        self.withloop = withloop
        self.midlayers = nn.ModuleList()
        self.__makehidden()
        if self.aggrmethod == 'concat':
            self.out_features = in_features + out_features * nbaselayer
        elif self.aggrmethod == 'add':
            if in_features != self.hiddendim:
                raise RuntimeError("The dimension of in_features and hiddendim should be matched in 'add' model.")
            self.out_features = out_features
        else:
            raise NotImplementedError("The aggregation method only support 'concat', 'add'.")

    def __makehidden(self):
        for j in range(self.nbaselayer):
            reslayer = nn.ModuleList()
            for i in range(j + 1):
                if i == 0:
                    layer = GraphConvolutionBS(self.in_features, self.hiddendim, self.activation, self.withbn, self.withloop)
                else:
                    layer = GraphConvolutionBS(self.hiddendim, self.hiddendim, self.activation, self.withbn, self.withloop)
                reslayer.append(layer)
            self.midlayers.append(reslayer)

    def forward(self, graph, x):
        for reslayer in self.midlayers:
            subx = x
            for gc in reslayer:
                subx = gc(graph, x)
                subx = F.dropout(subx, self.dropout, training=self.training)
            x = self._doconcat(x, subx)
        return x

    def get_outdim(self):
        return self.out_features

    def _doconcat(self, x, subx):
        if self.aggrmethod == 'concat':
            return torch.cat((x, subx), 1)
        elif self.aggrmethod == 'add':
            return x + subx

    def __repr__(self):
        return '%s %s (%d - [%d:%d] > %d)' % (self.__class__.__name__, self.aggrmethod, self.in_features, self.hiddendim, self.nbaselayer, self.out_features)


class Dense(Module):
    """
    Simple Dense layer, Do not consider adj.
    """

    def __init__(self, in_features, out_features, activation=lambda x: x, bias=True, res=False):
        super(Dense, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.sigma = activation
        self.weight = Parameter(torch.FloatTensor(in_features, out_features))
        self.res = res
        self.bn = nn.BatchNorm1d(out_features)
        self.bias = Parameter(torch.FloatTensor(out_features)) if bias else None
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, graph, x):
        output = torch.mm(x, self.weight)
        output = output + self.bias if self.bias is not None else output
        output = self.bn(output)
        return self.sigma(output)

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'


class DropEdge_GCN(BaseModel):
    """
     DropEdge: Towards Deep Graph Convolutional Networks on Node Classification
     Applying DropEdge to GCN @ https://arxiv.org/pdf/1907.10903.pdf

    The model for the single kind of deepgcn blocks.
    The model architecture likes:
    inputlayer(nfeat)--block(nbaselayer, nhid)--...--outputlayer(nclass)--softmax(nclass)
                        |------  nhidlayer  ----|
    The total layer is nhidlayer*nbaselayer + 2.
    All options are configurable.

     Args:
         Initial function.
         :param nfeat: the input feature dimension.
         :param nhid:  the hidden feature dimension.
         :param nclass: the output feature dimension.
         :param nhidlayer: the number of hidden blocks.
         :param dropout:  the dropout ratio.
         :param baseblock: the baseblock type, can be "mutigcn", "resgcn", "densegcn" and "inceptiongcn".
         :param inputlayer: the input layer type, can be "gcn", "dense", "none".
         :param outputlayer: the input layer type, can be "gcn", "dense".
         :param nbaselayer: the number of layers in one hidden block.
         :param activation: the activation function, default is ReLu.
         :param withbn: using batch normalization in graph convolution.
         :param withloop: using self feature modeling in graph convolution.
         :param aggrmethod: the aggregation function for baseblock, can be "concat" and "add". For "resgcn", the default
                            is "add", for others the default is "concat".
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num_features', type=int)
        parser.add_argument('--num_classes', type=int)
        parser.add_argument('--baseblock', default='mutigcn', help='Choose the model to be trained.( mutigcn, resgcn, densegcn, inceptiongcn)')
        parser.add_argument('--inputlayer', default='gcn', help='The input layer of the model.')
        parser.add_argument('--outputlayer', default='gcn', help='The output layer of the model.')
        parser.add_argument('--hidden-size', type=int, default=64, help='Number of hidden units.')
        parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability).')
        parser.add_argument('--withbn', action='store_true', default=False, help='Enable Bath Norm GCN')
        parser.add_argument('--withloop', action='store_true', default=False, help='Enable loop layer GCN')
        parser.add_argument('--nhiddenlayer', type=int, default=1, help='The number of hidden layers.')
        parser.add_argument('--nbaseblocklayer', type=int, default=0, help='The number of layers in each baseblock')
        parser.add_argument('--aggrmethod', default='default', help='The aggrmethod for the layer aggreation. The options includes add and concat. Only valid in resgcn, densegcn and inceptiongcn')
        parser.add_argument('--task_type', default='full', help='The node classification task type (full and semi). Only valid for cora, citeseer and pubmed dataset.')
        parser.add_argument('--activation', default=F.relu, help='activiation function')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.nhiddenlayer, args.dropout, args.baseblock, args.inputlayer, args.outputlayer, args.nbaseblocklayer, args.activation, args.withbn, args.withloop, args.aggrmethod)

    def __init__(self, nfeat, nhid, nclass, nhidlayer, dropout, baseblock, inputlayer, outputlayer, nbaselayer, activation, withbn, withloop, aggrmethod):
        super(DropEdge_GCN, self).__init__()
        self.dropout = dropout
        if baseblock == 'resgcn':
            self.BASEBLOCK = ResGCNBlock
        elif baseblock == 'densegcn':
            self.BASEBLOCK = DenseGCNBlock
        elif baseblock == 'mutigcn':
            self.BASEBLOCK = MultiLayerGCNBlock
        elif baseblock == 'inceptiongcn':
            self.BASEBLOCK = InceptionGCNBlock
        else:
            raise NotImplementedError('Current baseblock %s is not supported.' % baseblock)
        if inputlayer == 'gcn':
            self.ingc = GraphConvolutionBS(nfeat, nhid, activation, withbn, withloop)
            baseblockinput = nhid
        elif inputlayer == 'none':
            self.ingc = lambda x: x
            baseblockinput = nfeat
        else:
            self.ingc = Dense(nfeat, nhid, activation)
            baseblockinput = nhid
        outactivation = lambda x: x
        if outputlayer == 'gcn':
            self.outgc = GraphConvolutionBS(baseblockinput, nclass, outactivation, withbn, withloop)
        else:
            self.outgc = Dense(nhid, nclass, activation)
        self.midlayer = nn.ModuleList()
        for i in range(nhidlayer):
            gcb = self.BASEBLOCK(in_features=baseblockinput, out_features=nhid, nbaselayer=nbaselayer, withbn=withbn, withloop=withloop, activation=activation, dropout=dropout, dense=False, aggrmethod=aggrmethod)
            self.midlayer.append(gcb)
            baseblockinput = gcb.get_outdim()
        outactivation = lambda x: x
        self.outgc = GraphConvolutionBS(baseblockinput, nclass, outactivation, withbn, withloop)
        self.reset_parameters()

    def reset_parameters(self):
        pass

    def forward(self, graph):
        x = graph.x
        x = self.ingc(graph, x)
        x = F.dropout(x, self.dropout, training=self.training)
        for i in range(len(self.midlayer)):
            midgc = self.midlayer[i]
            x = midgc(graph, x)
        x = self.outgc(graph, x)
        x = F.log_softmax(x, dim=1)
        return x

    def predict(self, data):
        return self.forward(data)


class GAE(GCN):

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_layers, args.dropout)

    def __init__(self, in_feats, hidden_size, num_layers, dropout):
        super(GAE, self).__init__(in_feats, hidden_size, 1, num_layers, dropout)

    def make_loss(self, data, adj):
        embeddings = self.embed(data)
        return F.binary_cross_entropy(F.softmax(torch.mm(embeddings, embeddings.t())), adj, reduction='sum') / data.x.shape[0]

    def get_features(self, data):
        return self.embed(data).detach()


class VGAE(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size)

    def __init__(self, num_features, hidden_size):
        super(VGAE, self).__init__()
        self.num_features = num_features
        self.hidden_size = hidden_size
        self.conv1 = GCNLayer(self.num_features, self.hidden_size)
        self.conv2_mean = GCNLayer(self.hidden_size, self.hidden_size)
        self.conv2_var = GCNLayer(self.hidden_size, self.hidden_size)

    def reparameterize(self, mean, log_var):
        log_var = log_var.clamp(max=10)
        sigma = torch.exp(log_var)
        z = mean + torch.randn_like(log_var) * sigma
        return z

    def encode(self, graph):
        graph.add_remaining_self_loops()
        graph.sym_norm()
        h = graph.x
        h = self.conv1(graph, h)
        h = F.relu(h)
        mean = self.conv2_mean(graph, h)
        log_var = self.conv2_var(graph, h)
        return mean, log_var

    def decode(self, x):
        return torch.sigmoid(torch.matmul(x, x.t()))

    def forward(self, graph):
        mean, log_var = self.encode(graph)
        return self.reparameterize(mean, log_var)

    def get_features(self, graph):
        return self.forward(graph).detach()

    def make_loss(self, data, adj):
        mean, log_var = self.encode(data)
        z = self.reparameterize(mean, log_var)
        mat = self.decode(z)
        recon_loss = F.binary_cross_entropy(mat, adj, reduction='sum')
        var = torch.exp(log_var)
        kl_loss = 0.5 * torch.mean(torch.sum(mean * mean + var - log_var - 1, dim=1))
        None
        return recon_loss + kl_loss


class GAT(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats, num_heads, dropout):
        super(GAT, self).__init__()
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.dropout = dropout
        self.conv1 = GATConv(in_feats, hidden_size, heads=num_heads, dropout=dropout)
        self.conv2 = GATConv(hidden_size * num_heads, out_feats, dropout=dropout)

    def forward(self, graph):
        x = graph.x
        edge_index = torch.stack(graph.edge_index)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.elu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.elu(self.conv2(x, edge_index))
        return x


class ApplyNodeFunc(nn.Module):
    """Update the node feature hv with MLP, BN and ReLU."""

    def __init__(self, mlp, use_selayer):
        super(ApplyNodeFunc, self).__init__()
        self.mlp = mlp
        self.bn = SELayer(self.mlp.output_dim, int(np.sqrt(self.mlp.output_dim))) if use_selayer else nn.BatchNorm1d(self.mlp.output_dim)

    def forward(self, h):
        h = self.mlp(h)
        h = self.bn(h)
        h = F.relu(h)
        return h


class GATModel(nn.Module):

    def __init__(self, in_feats, hidden_size, num_layers, nhead, dropout=0.0, attn_drop=0.0, alpha=0.2, residual=False):
        super(GATModel, self).__init__()
        assert hidden_size % nhead == 0
        self.layers = nn.ModuleList([GATLayer(in_feats=in_feats if i > 0 else hidden_size // nhead, out_feats=hidden_size // nhead, nhead=nhead, attn_drop=0.0, alpha=0.2, residual=False, activation=F.leaky_relu if i + 1 < num_layers else None) for i in range(num_layers)])

    def forward(self, graph, x):
        for i, layer in enumerate(self.layers):
            x = layer(graph, x)
        return x


def batch_mean_pooling(x, batch):
    values, counts = torch.unique(batch, return_counts=True)
    res = torch.zeros(len(values), x.size(1))
    res = res.scatter_add_(dim=0, index=batch.unsqueeze(-1).expand_as(x), src=x)
    return res / counts.unsqueeze(-1)


class GINModel(nn.Module):

    def __init__(self, num_layers, in_feats, hidden_dim, out_feats, num_mlp_layers, eps=0, pooling='sum', train_eps=False, dropout=0.5, final_dropout=0.2, use_selayer=False):
        super(GINModel, self).__init__()
        self.gin_layers = nn.ModuleList()
        self.batch_norm = nn.ModuleList()
        self.num_layers = num_layers
        for i in range(num_layers - 1):
            if i == 0:
                mlp = MLP(in_feats, hidden_dim, hidden_dim, num_mlp_layers, norm='batchnorm')
            else:
                mlp = MLP(hidden_dim, hidden_dim, hidden_dim, num_mlp_layers, norm='batchnorm')
            self.gin_layers.append(GINLayer(ApplyNodeFunc(mlp, use_selayer), eps, train_eps))
            self.batch_norm.append(nn.BatchNorm1d(hidden_dim))
        self.linear_prediction = nn.ModuleList()
        for i in range(self.num_layers):
            if i == 0:
                self.linear_prediction.append(nn.Linear(in_feats, out_feats))
            else:
                self.linear_prediction.append(nn.Linear(hidden_dim, out_feats))
        self.dropout = nn.Dropout(dropout)
        if pooling == 'sum':
            self.pool = batch_sum_pooling
        elif pooling == 'mean':
            self.pool = batch_mean_pooling
        elif pooling == 'max':
            self.pool = batch_max_pooling
        else:
            raise NotImplementedError
        self.final_drop = nn.Dropout(final_dropout)

    def forward(self, batch, n_feat):
        h = n_feat
        layer_rep = [h]
        for i in range(self.num_layers - 1):
            h = self.gin_layers[i](batch, h)
            h = self.batch_norm[i](h)
            h = F.relu(h)
            layer_rep.append(h)
        score_over_layer = 0
        all_outputs = []
        for i, h in enumerate(layer_rep):
            pooled_h = self.pool(h, batch.batch)
            all_outputs.append(pooled_h)
            score_over_layer += self.final_drop(self.linear_prediction[i](pooled_h))
        return score_over_layer, all_outputs[1:]


class GCCModel(BaseModel):
    """
    MPNN from
    `Neural Message Passing for Quantum Chemistry <https://arxiv.org/abs/1704.01212>`__
    Parameters
    ----------
    node_input_dim : int
        Dimension of input node feature, default to be 15.
    edge_input_dim : int
        Dimension of input edge feature, default to be 15.
    output_dim : int
        Dimension of prediction, default to be 12.
    node_hidden_dim : int
        Dimension of node feature in hidden layers, default to be 64.
    edge_hidden_dim : int
        Dimension of edge feature in hidden layers, default to be 128.
    num_step_message_passing : int
        Number of message passing steps, default to be 6.
    num_step_set2set : int
        Number of set2set steps
    num_layer_set2set : int
        Number of set2set layers
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--positional-embedding-size', type=int, default=32)
        parser.add_argument('--degree-embedding-size', type=int, default=16)
        parser.add_argument('--max-node-freq', type=int, default=16)
        parser.add_argument('--max-edge-freq', type=int, default=16)
        parser.add_argument('--max-degree', type=int, default=512)
        parser.add_argument('--freq-embedding-size', type=int, default=16)
        parser.add_argument('--num-layers', type=int, default=5)
        parser.add_argument('--num-heads', type=int, default=2)
        parser.add_argument('--output-size', type=int, default=64)
        parser.add_argument('--norm', type=bool, default=True)
        parser.add_argument('--gnn-model', type=str, default='gin')
        parser.add_argument('--degree-input', type=bool, default=True)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(positional_embedding_size=args.positional_embedding_size, max_node_freq=args.max_node_freq, max_edge_freq=args.max_edge_freq, max_degree=args.max_degree, num_layers=args.num_layers, num_heads=args.num_heads, degree_embedding_size=args.degree_embedding_size, node_hidden_dim=args.hidden_size, norm=args.norm, gnn_model=args.gnn_model, output_dim=args.output_size, degree_input=args.degree_input)

    def __init__(self, positional_embedding_size=32, max_node_freq=8, max_edge_freq=8, max_degree=128, freq_embedding_size=32, degree_embedding_size=32, output_dim=32, node_hidden_dim=32, edge_hidden_dim=32, num_layers=6, num_heads=4, num_step_set2set=6, num_layer_set2set=3, norm=False, gnn_model='gin', degree_input=True):
        super(GCCModel, self).__init__()
        if degree_input:
            node_input_dim = positional_embedding_size + degree_embedding_size + 1
        else:
            node_input_dim = positional_embedding_size + 1
        if gnn_model == 'gat':
            self.gnn = GATModel(in_feats=node_input_dim, hidden_size=node_hidden_dim, num_layers=num_layers, nhead=num_heads, dropout=0.0)
        elif gnn_model == 'gin':
            self.gnn = GINModel(num_layers=num_layers, num_mlp_layers=2, in_feats=node_input_dim, hidden_dim=node_hidden_dim, out_feats=output_dim, final_dropout=0.5, train_eps=False, pooling='sum')
        self.gnn_model = gnn_model
        self.max_node_freq = max_node_freq
        self.max_edge_freq = max_edge_freq
        self.max_degree = max_degree
        self.degree_input = degree_input
        self.output_dim = output_dim
        self.hidden_size = node_hidden_dim
        if degree_input:
            self.degree_embedding = nn.Embedding(num_embeddings=max_degree + 1, embedding_dim=degree_embedding_size)
        self.set2set = Set2Set(node_hidden_dim, num_step_set2set, num_layer_set2set)
        if gnn_model != 'gin':
            self.lin_readout = nn.Sequential(nn.Linear(2 * node_hidden_dim, node_hidden_dim), nn.ReLU(), nn.Linear(node_hidden_dim, output_dim))
        else:
            self.lin_readout = None
        self.norm = norm

    def forward(self, g, return_all_outputs=False):
        """Predict molecule labels
        Parameters
        ----------
        g : Graph
        n_feat : tensor of dtype float32 and shape (B1, D1)
            Node features. B1 for number of nodes and D1 for
            the node feature size.
        e_feat : tensor of dtype float32 and shape (B2, D2)
            Edge features. B2 for number of edges and D2 for
            the edge feature size.
        Returns
        -------
        res : Predicted labels
        """
        device = self.device
        pos_undirected = g.pos_undirected
        seed_emb = g.seed.unsqueeze(1).float()
        if not torch.is_tensor(seed_emb):
            seed_emb = torch.Tensor(seed_emb)
        if self.degree_input:
            degrees = g.degrees()
            if device != torch.device('cpu'):
                degrees = degrees
            degrees = degrees.long()
            deg_emb = self.degree_embedding(degrees.clamp(0, self.max_degree))
            n_feat = torch.cat((pos_undirected, deg_emb, seed_emb), dim=-1)
        else:
            n_feat = torch.cat((pos_undirected, seed_emb), dim=-1)
        if self.gnn_model == 'gin':
            x, all_outputs = self.gnn(g, n_feat)
        else:
            x, all_outputs = self.gnn(g, n_feat), None
            x = self.set2set(g, x)
            x = self.lin_readout(x)
        if self.norm:
            x = F.normalize(x, p=2, dim=-1, eps=1e-05)
        if return_all_outputs:
            return x, all_outputs
        else:
            return x


def mix_hidden_state(feat, target, train_index, alpha):
    if alpha > 0:
        lamb = np.random.beta(alpha, alpha)
    else:
        lamb = 1
    permuted_index = train_index[torch.randperm(train_index.size(0))]
    feat[train_index] = lamb * feat[train_index] + (1 - lamb) * feat[permuted_index]
    return feat, target[train_index], target[permuted_index], lamb


class GCNMix(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--alpha', type=float, default=1.0)
        parser.add_argument('--k', type=int, default=10)
        parser.add_argument('--temperature', type=float, default=0.1)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(in_feat=args.num_features, hidden_size=args.hidden_size, num_classes=args.num_classes, k=args.k, temperature=args.temperature, alpha=args.alpha, dropout=args.dropout)

    def __init__(self, in_feat, hidden_size, num_classes, k, temperature, alpha, dropout):
        super(GCNMix, self).__init__()
        self.dropout = dropout
        self.alpha = alpha
        self.k = k
        self.temperature = temperature
        self.input_gnn = GCNConv(in_feat, hidden_size)
        self.hidden_gnn = GCNConv(hidden_size, num_classes)
        self.loss_f = nn.BCELoss()

    def forward(self, graph):
        graph.sym_norm()
        x = graph.x
        h = F.dropout(x, p=self.dropout, training=self.training)
        h = self.input_gnn(graph, h)
        h = F.relu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.hidden_gnn(graph, h)
        return h

    def forward_aux(self, x, label, train_index, mix_hidden=True, layer_mix=1):
        h = F.dropout(x, p=self.dropout, training=self.training)
        assert layer_mix in (0, 1)
        if layer_mix == 0:
            h, target, target_mix, lamb = mix_hidden_state(h, label, train_index, self.alpha)
        h = self.input_gnn.forward_aux(h)
        h = F.relu(h)
        if layer_mix == 1:
            h, target, target_mix, lamb = mix_hidden_state(h, label, train_index, self.alpha)
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.hidden_gnn.forward_aux(h)
        target_label = lamb * target + (1 - lamb) * target_mix
        return h, target_label

    def predict_noise(self, data, tau=1):
        out = self.forward(data) / tau
        return out


class GDC_GCN(BaseModel):
    """The GDC model from the `"Diffusion Improves Graph Learning"
    <https://arxiv.org/abs/1911.05485>`_ paper, with the PPR and heat matrix variants
    combined with GCN

    Args:
        num_features (int)  : Number of input features in ppr-preprocessed dataset.
        num_classes (int)   : Number of classes.
        hidden_size (int)   : The dimension of node representation.
        dropout (float)     : Dropout rate for model training.
        alpha (float)       : PPR polynomial filter param, 0 to 1.
        t (float)           : Heat polynomial filter param
        k (int)             : Top k nodes retained during sparsification.
        eps (float)         : Threshold for clipping.
        gdc_type (str)      : "none", "ppr", "heat"
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--alpha', type=float, default=0.05)
        parser.add_argument('--t', type=float, default=5.0)
        parser.add_argument('--k', type=int, default=128)
        parser.add_argument('--eps', type=float, default=0.01)
        parser.add_argument('--gdc-type', default='ppr')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.dropout, args.alpha, args.t, args.k, args.eps, args.gdc_type)

    def __init__(self, nfeat, nhid, nclass, dropout, alpha, t, k, eps, gdctype):
        super(GDC_GCN, self).__init__()
        self.alpha = alpha
        self.t = t
        self.k = k
        self.eps = eps
        self.gdc_type = gdctype
        self.data = None
        self.nfeat = nfeat
        self.gc1 = GCNLayer(nfeat, nhid)
        self.gc2 = GCNLayer(nhid, nclass)
        self.dropout = dropout

    def forward(self, graph):
        if self.data is None:
            self.reset_data(graph)
        graph = self.data
        x = graph.x
        if self.gdc_type == 'none':
            graph.sym_norm()
        x = F.dropout(x, self.dropout, training=self.training)
        x = F.relu(self.gc1(graph, x))
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(graph, x)
        return x

    def predict(self, data=None):
        self.data.apply(lambda x: x)
        return self.forward(self.data)

    def reset_data(self, data):
        if self.data is None:
            data
            self.data = self.preprocessing(data, gdc_type=self.gdc_type)
            data

    def preprocessing(self, data, gdc_type='ppr'):

        def get_diffusion(x, edges):
            adj_matrix = self._get_adj_matrix(x, edges)
            if gdc_type == 'none':
                None
                processed_matrix = adj_matrix
            elif gdc_type == 'ppr':
                None
                processed_matrix = self._get_ppr_matrix(adj_matrix, alpha=self.alpha)
            elif gdc_type == 'heat':
                None
                processed_matrix = self._get_heat_matrix(adj_matrix, t=self.t)
            else:
                raise ValueError
            if gdc_type == 'ppr' or gdc_type == 'heat':
                if self.k:
                    None
                    processed_matrix = self._get_top_k_matrix(processed_matrix, k=self.k)
                elif self.eps:
                    None
                    processed_matrix = self._get_clipped_matrix(processed_matrix, eps=self.eps)
                else:
                    raise ValueError
            edges_i = []
            edges_j = []
            edge_attr = []
            for i, row in enumerate(processed_matrix):
                for j in np.where(row > 0)[0]:
                    edges_i.append(i)
                    edges_j.append(j)
                    edge_attr.append(processed_matrix[i, j])
            edge_index = [edges_i, edges_j]
            return torch.as_tensor(edge_index, dtype=torch.long), torch.as_tensor(edge_attr, dtype=torch.float)
        edge_index, edge_weight = get_diffusion(data.x, data.edge_index)
        data.edge_index = edge_index
        data.edge_weight = edge_weight
        if self.training and data._adj_train is not None:
            data.eval()
            edge_index, edge_weight = get_diffusion(data.x, data.edge_index)
            data.edge_index = edge_index
            data.edge_weight = edge_weight
            data.train()
        return data

    def _get_adj_matrix(self, x, edge_index):
        num_nodes = x.shape[0]
        adj_matrix = np.zeros(shape=(num_nodes, num_nodes))
        adj_matrix[edge_index[0], edge_index[1]] = 1.0
        return adj_matrix

    def _get_ppr_matrix(self, adj_matrix, alpha=0.1):
        num_nodes = adj_matrix.shape[0]
        A_tilde = adj_matrix + np.eye(num_nodes)
        D_tilde = np.diag(1 / np.sqrt(A_tilde.sum(axis=1)))
        H = D_tilde @ A_tilde @ D_tilde
        return alpha * np.linalg.inv(np.eye(num_nodes) - (1 - alpha) * H)

    def _get_heat_matrix(self, adj_matrix, t=5.0):
        num_nodes = adj_matrix.shape[0]
        A_tilde = adj_matrix + np.eye(num_nodes)
        D_tilde = np.diag(1 / np.sqrt(A_tilde.sum(axis=1)))
        H = D_tilde @ A_tilde @ D_tilde
        return expm(-t * (np.eye(num_nodes) - H))

    def _get_top_k_matrix(self, A, k=128):
        num_nodes = A.shape[0]
        row_idx = np.arange(num_nodes)
        A[A.argsort(axis=0)[:num_nodes - k], row_idx] = 0.0
        norm = A.sum(axis=0)
        norm[norm <= 0] = 1
        return A / norm

    def _get_clipped_matrix(self, A, eps=0.01):
        A[A < eps] = 0.0
        norm = A.sum(axis=0)
        norm[norm <= 0] = 1
        return A / norm


class GIN(BaseModel):
    """Graph Isomorphism Network from paper `"How Powerful are Graph
    Neural Networks?" <https://arxiv.org/pdf/1810.00826.pdf>`__.

    Args:
        num_layers : int
            Number of GIN layers
        in_feats : int
            Size of each input sample
        out_feats : int
            Size of each output sample
        hidden_dim : int
            Size of each hidden layer dimension
        num_mlp_layers : int
            Number of MLP layers
        eps : float32, optional
            Initial `\\epsilon` value, default: ``0``
        pooling : str, optional
            Aggregator type to use, default:　``sum``
        train_eps : bool, optional
            If True, `\\epsilon` will be a learnable parameter, default: ``True``
    """

    def split_dataset(cls, dataset, args):
        return split_dataset_general(dataset, args)

    def __init__(self, num_layers, in_feats, out_feats, hidden_dim, num_mlp_layers=1, eps=0, pooling='sum', train_eps=False, dropout=0.5):
        super(GIN, self).__init__()
        self.gin_layers = nn.ModuleList()
        self.batch_norm = nn.ModuleList()
        self.num_layers = num_layers
        self.layer2 = nn.Linear(hidden_dim, out_feats)
        for i in range(num_layers - 1):
            if i == 0:
                self.mlp = MLP(in_feats, hidden_dim, hidden_dim, num_mlp_layers, norm='batchnorm')
            else:
                self.mlp = MLP(hidden_dim, hidden_dim, hidden_dim, num_mlp_layers, norm='batchnorm')
            self.gin_layers.append(GINLayer(self.mlp, eps, train_eps))
            self.batch_norm.append(nn.BatchNorm1d(hidden_dim))

    def reset_parameters(self):
        self.mlp.reset_parameters()
        self.layer2.reset_parameters()

    def forward(self, graph):
        h = graph.x
        for i in range(self.num_layers - 1):
            h = self.gin_layers[i](graph, h)
            h = self.batch_norm[i](h)
            h = F.relu(h)
            h = self.layer2(h)
        return F.log_softmax(h, dim=-1)


class Grand(BaseModel):
    """
    Implementation of GRAND in paper `"Graph Random Neural Networks for Semi-Supervised Learning on Graphs"`
    <https://arxiv.org/abs/2005.11079>

    Parameters
    ----------
    nfeat : int
        Size of each input features.
    nhid : int
        Size of hidden features.
    nclass : int
        Number of output classes.
    input_droprate : float
        Dropout rate of input features.
    hidden_droprate : float
        Dropout rate of hidden features.
    use_bn : bool
        Using batch normalization.
    dropnode_rate : float
        Rate of dropping elements of input features
    tem : float
        Temperature to sharpen predictions.
    lam : float
         Proportion of consistency loss of unlabelled data
    order : int
        Order of adjacency matrix
    sample : int
        Number of augmentations for consistency loss
    alpha : float
    """

    def __init__(self, nfeat, nhid, nclass, input_droprate=0.5, hidden_droprate=0.5, use_bn=False, dropnode_rate=0.1, order=2, alpha=0.5):
        super(Grand, self).__init__()
        self.layer1 = nn.Linear(nfeat, nhid)
        self.layer2 = nn.Linear(nhid, nclass)
        self.input_droprate = input_droprate
        self.hidden_droprate = hidden_droprate
        self.bn1 = nn.BatchNorm1d(nfeat)
        self.bn2 = nn.BatchNorm1d(nhid)
        self.use_bn = use_bn
        self.order = order
        self.dropnode_rate = dropnode_rate
        self.alpha = alpha

    def drop_node(self, x):
        n = x.shape[0]
        drop_rates = torch.ones(n) * self.dropnode_rate
        if self.training:
            masks = torch.bernoulli(1.0 - drop_rates).unsqueeze(1)
            x = masks * x
        else:
            x = x * (1.0 - self.dropnode_rate)
        return x

    def reset_parameters(self):
        self.layer1.reset_parameters()
        self.layer2.reset_parameters()

    def rand_prop(self, graph, x):
        x = self.drop_node(x)
        y = x
        for i in range(self.order):
            x = spmm(graph, x).detach_()
            y.add_(x)
        return y.div_(self.order + 1.0).detach_()

    def normalize_x(self, x):
        row_sum = x.sum(1)
        row_inv = row_sum.pow_(-1)
        row_inv.masked_fill_(row_inv == float('inf'), 0)
        x = x * row_inv[:, None]
        return x

    def forward(self, graph):
        graph.sym_norm()
        x = graph.x
        x = self.normalize_x(x)
        x = self.rand_prop(graph, x)
        if self.use_bn:
            x = self.bn1(x)
        x = F.dropout(x, self.input_droprate, training=self.training)
        x = F.relu(self.layer1(x))
        if self.use_bn:
            x = self.bn2(x)
        x = F.dropout(x, self.hidden_droprate, training=self.training)
        x = self.layer2(x)
        return F.log_softmax(x, dim=-1)

    def predict(self, data):
        return self.forward(data)


class UnPool(nn.Module):

    def __init__(self):
        super(UnPool, self).__init__()

    def forward(self, num_nodes: int, h: torch.Tensor, indices: torch.Tensor) ->torch.Tensor:
        new_h = torch.zeros(num_nodes, h.shape[1])
        new_h[indices] = h
        return new_h


def filter_adj(row, col, edge_attr, mask):
    return (row[mask], col[mask]), None if edge_attr is None else edge_attr[mask]


def dropout_adj(edge_index: Tuple, edge_weight: Optional[torch.Tensor]=None, drop_rate: float=0.5, renorm: Optional[str]='sym', training: bool=False):
    if not training or drop_rate == 0:
        if edge_weight is None:
            edge_weight = torch.ones(edge_index[0].shape[0], device=edge_index[0].device)
        return edge_index, edge_weight
    if drop_rate < 0.0 or drop_rate > 1.0:
        raise ValueError('Dropout probability has to be between 0 and 1, but got {}'.format(drop_rate))
    row, col = edge_index
    num_nodes = int(max(row.max(), col.max())) + 1
    self_loop = row == col
    mask = torch.full((row.shape[0],), 1 - drop_rate, dtype=torch.float, device=row.device)
    mask = torch.bernoulli(mask)
    mask = self_loop | mask
    edge_index, edge_weight = filter_adj(row, col, edge_weight, mask)
    if renorm == 'sym':
        edge_weight = symmetric_normalization(num_nodes, edge_index[0], edge_index[1])
    elif renorm == 'row':
        edge_weight = row_normalization(num_nodes, edge_index[0], edge_index[1])
    return edge_index, edge_weight


def sage_sampler(adjlist, edge_index, num_sample):
    if adjlist == {}:
        row, col = edge_index
        row = row.cpu().numpy()
        col = col.cpu().numpy()
        for i in zip(row, col):
            if not i[0] in adjlist:
                adjlist[i[0]] = [i[1]]
            else:
                adjlist[i[0]].append(i[1])
    sample_list = []
    for i in adjlist:
        list = [[i, j] for j in adjlist[i]]
        if len(list) > num_sample:
            list = random.sample(list, num_sample)
        sample_list.extend(list)
    edge_idx = torch.as_tensor(sample_list, dtype=torch.long).t()
    return edge_idx


class Graphsage(BaseModel):

    def sampling(self, edge_index, num_sample):
        return sage_sampler(self.adjlist, edge_index, num_sample)

    def __init__(self, num_features, num_classes, hidden_size, num_layers, sample_size, dropout, aggr):
        super(Graphsage, self).__init__()
        assert num_layers == len(sample_size)
        self.adjlist = {}
        self.num_features = num_features
        self.num_classes = num_classes
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.sample_size = sample_size
        self.dropout = dropout
        shapes = [num_features] + hidden_size + [num_classes]
        self.convs = nn.ModuleList([SAGELayer(shapes[layer], shapes[layer + 1], aggr=aggr) for layer in range(num_layers)])

    def mini_forward(self, graph):
        x = graph.x
        for i in range(self.num_layers):
            edge_index_sp = self.sampling(graph.edge_index, self.sample_size[i])
            with graph.local_graph():
                graph.edge_index = edge_index_sp
                x = self.convs[i](graph, x)
            if i != self.num_layers - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        return x

    def forward(self, *args):
        if isinstance(args[0], Graph):
            return self.mini_forward(*args)
        else:
            device = next(self.parameters()).device
            x, adjs = args
            for i, (src_id, graph, size) in enumerate(adjs):
                graph = graph
                output = self.convs[i](graph, x)
                x = output[:size[1]]
                if i != self.num_layers - 1:
                    x = F.relu(x)
                    x = F.dropout(x, p=self.dropout, training=self.training)
            return x

    def inference(self, x_all, data_loader):
        device = next(self.parameters()).device
        for i in range(len(self.convs)):
            output = []
            for src_id, graph, size in data_loader:
                x = x_all[src_id]
                graph = graph
                x = self.convs[i](graph, x)
                x = x[:size[1]]
                if i != self.num_layers - 1:
                    x = F.relu(x)
                output.append(x.cpu())
            x_all = torch.cat(output, dim=0)
        return x_all


class SAGE(BaseModel):

    def __init__(self, in_feats, out_feats, hidden_size, num_layers, aggr='mean', dropout=0.5, norm='batchnorm', activation='relu', normalize=False):
        super(SAGE, self).__init__()
        shapes = [in_feats] + [hidden_size] * (num_layers - 1) + [out_feats]
        self.num_layers = num_layers
        self.layers = nn.ModuleList([SAGELayer(shapes[i], shapes[i + 1], aggr=aggr, normalize=normalize if i != num_layers - 1 else False, dropout=dropout if i != num_layers - 1 else False, norm=norm if i != num_layers - 1 else None, activation=activation if i != num_layers - 1 else None) for i in range(num_layers)])

    def forward(self, graph):
        x = graph.x
        for layer in self.layers:
            x = layer(graph, x)
        return x


def parse_arch(architecture, aggr, act, bias, hidden_size, num_features):
    num_layers = len(architecture.split('-'))
    bias_layer = [bias] * num_layers
    act_layer = [act] * num_layers
    aggr_layer = [aggr] * num_layers
    dims_layer = [hidden_size] * num_layers
    order_layer = [int(order) for order in architecture.split('-')]
    return [num_features] + dims_layer, order_layer, act_layer, bias_layer, aggr_layer


class GraphSAINT(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--hidden-size', type=int, default=128)
        parser.add_argument('--architecture', type=str, default='1-1-0')
        parser.add_argument('--aggr', type=str, default='concat')
        parser.add_argument('--act', type=str, default='relu')
        parser.add_argument('--bias', type=str, default='norm')
        parser.add_argument('--dropout', type=float, default=0.1)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.num_classes, args.architecture, args.aggr, args.act, args.bias, args.dropout, args.hidden_size)

    def __init__(self, num_features, num_classes, architecture, aggr, act, bias, dropout, hidden_size):
        """
        Build the multi-layer GNN architecture.

        Inputs:
            num_classes         int, number of classes a node can belong to
            arch_gcn            dict, config for each GNN layer
            train_params        dict, training hyperparameters (e.g., learning rate)
            feat_full           np array of shape N x f, where N is the total num of
                                nodes and f is the dimension for input node feature
            label_full          np array, for single-class classification, the shape
                                is N x 1 and for multi-class classification, the
                                shape is N x c (where c = num_classes)
            cpu_eval            bool, if True, will put the model on CPU.

        Outputs:
            None
        """
        super(GraphSAINT, self).__init__()
        self.aggregator_cls = SAINTLayer
        self.mulhead = 1
        self.dropout = dropout
        self.sigmoid_loss = True
        self.num_classes = num_classes
        self.num_layers = len(architecture.split('-'))
        _dims, self.order_layer, self.act_layer, self.bias_layer, self.aggr_layer = parse_arch(architecture, aggr, act, bias, hidden_size, num_features)
        self.set_idx_conv()
        self.set_dims(_dims)
        self.loss = 0
        self.opt_op = None
        self.num_params = 0
        self.aggregators, num_param = self.get_aggregators()
        self.num_params += num_param
        self.conv_layers = nn.ModuleList(self.aggregators)
        self.classifier = SAINTLayer(self.dims_feat[-1], self.num_classes, act='I', order=0, dropout=self.dropout, bias='bias')
        self.num_params += self.classifier.num_param

    def set_dims(self, dims):
        """
        Set the feature dimension / weight dimension for each GNN or MLP layer.
        We will use the dimensions set here to initialize PyTorch layers.

        Inputs:
            dims        list, length of node feature for each hidden layer

        Outputs:
            None
        """
        self.dims_feat = [dims[0]] + [(((self.aggr_layer[layer] == 'concat') * self.order_layer[layer] + 1) * dims[layer + 1]) for layer in range(len(dims) - 1)]
        self.dims_weight = [(self.dims_feat[layer], dims[layer + 1]) for layer in range(len(dims) - 1)]

    def set_idx_conv(self):
        """
        Set the index of GNN layers for the full neural net. For example, if
        the full NN is having 1-0-1-0 arch (1-hop graph conv, followed by 0-hop
        MLP, ...). Then the layer indices will be 0, 2.
        """
        idx_conv = np.where(np.array(self.order_layer) >= 1)[0]
        idx_conv = list(idx_conv[1:] - 1)
        idx_conv.append(len(self.order_layer) - 1)
        _o_arr = np.array(self.order_layer)[idx_conv]
        if np.prod(np.ediff1d(_o_arr)) == 0:
            self.idx_conv = idx_conv
        else:
            self.idx_conv = list(np.where(np.array(self.order_layer) == 1)[0])

    def forward(self, graph):
        x = graph.x
        for layer in self.conv_layers:
            x = layer(graph, x)
        emb_subg_norm = F.normalize(x, p=2, dim=1)
        pred_subg = self.classifier(None, emb_subg_norm)
        return pred_subg

    def _loss(self, preds, labels, norm_loss):
        """
        The predictor performs sigmoid (for multi-class) or softmax (for single-class)
        """
        if self.sigmoid_loss:
            norm_loss = norm_loss.unsqueeze(1)
            return torch.nn.BCEWithLogitsLoss(weight=norm_loss, reduction='sum')(preds, labels)
        else:
            _ls = torch.nn.CrossEntropyLoss(reduction='none')(preds, labels)
            return (norm_loss * _ls).sum()

    def get_aggregators(self):
        """
        Return a list of aggregator instances. to be used in self.build()
        """
        num_param = 0
        aggregators = []
        for layer in range(self.num_layers):
            aggr = self.aggregator_cls(*self.dims_weight[layer], dropout=self.dropout, act=self.act_layer[layer], order=self.order_layer[layer], aggr=self.aggr_layer[layer], bias=self.bias_layer[layer], mulhead=self.mulhead)
            num_param += aggr.num_param
            aggregators.append(aggr)
        return aggregators, num_param

    def predict(self, data):
        return self.forward(data)


def coalesce(row, col, value=None):
    device = row.device
    if torch.is_tensor(row):
        row = row.cpu().numpy()
    if torch.is_tensor(col):
        col = col.cpu().numpy()
    indices = np.lexsort((col, row))
    row = torch.from_numpy(row[indices]).long()
    col = torch.from_numpy(col[indices]).long()
    num = col.shape[0] + 1
    idx = torch.full((num,), -1, dtype=torch.long)
    max_num = max(row.max(), col.max()) + 100
    idx[1:] = (row + 1) * max_num + col
    mask = idx[1:] > idx[:-1]
    if mask.all():
        return row, col, value
    row = row[mask]
    if value is not None:
        _value = torch.zeros(row.shape[0], dtype=torch.float)
        value = _value.scatter_add_(dim=0, src=value, index=col)
    col = col[mask]
    return row, col, value


class GTConv(nn.Module):

    def __init__(self, in_channels, out_channels, num_nodes):
        super(GTConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels))
        self.bias = None
        self.scale = nn.Parameter(torch.Tensor([0.1]), requires_grad=False)
        self.num_nodes = num_nodes
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.constant_(self.weight, 1)
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, A):
        filter = F.softmax(self.weight, dim=1)
        num_channels = filter.shape[0]
        results = []
        for i in range(num_channels):
            for j, (edge_index, edge_value) in enumerate(A):
                if j == 0:
                    total_edge_index = edge_index
                    total_edge_value = edge_value * filter[i][j]
                else:
                    total_edge_index = torch.cat((total_edge_index, edge_index), dim=1)
                    total_edge_value = torch.cat((total_edge_value, edge_value * filter[i][j]))
            row, col = total_edge_index.detach()
            row, col, value = coalesce(row, col, total_edge_value)
            index = torch.stack([row, col])
            results.append((index, value))
        return results


class GTLayer(nn.Module):

    def __init__(self, in_channels, out_channels, num_nodes, first=True):
        super(GTLayer, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.first = first
        self.num_nodes = num_nodes
        if self.first:
            self.conv1 = GTConv(in_channels, out_channels, num_nodes)
            self.conv2 = GTConv(in_channels, out_channels, num_nodes)
        else:
            self.conv1 = GTConv(in_channels, out_channels, num_nodes)

    def forward(self, A, H_=None):
        if self.first:
            result_A = self.conv1(A)
            result_B = self.conv2(A)
            W = [F.softmax(self.conv1.weight, dim=1).detach(), F.softmax(self.conv2.weight, dim=1).detach()]
        else:
            result_A = H_
            result_B = self.conv1(A)
            W = [F.softmax(self.conv1.weight, dim=1).detach()]
        H = []
        device = result_A[0][0].device
        for i in range(len(result_A)):
            a_edge, a_value = result_A[i][0], result_A[i][1]
            b_edge, b_value = result_B[i][0], result_B[i][1]
            edges, values = spspmm(a_edge, a_value, b_edge, b_value, self.num_nodes, self.num_nodes, self.num_nodes)
            H.append((edges, values))
        return H, W


def remove_self_loops(indices, values=None):
    row, col = indices
    mask = indices[0] != indices[1]
    row = row[mask]
    col = col[mask]
    if values is not None:
        values = values[mask]
    return (row, col), values


class GTN(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--num-nodes', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--num-edge', type=int, default=2)
        parser.add_argument('--num-channels', type=int, default=2)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_edge, args.num_channels, args.num_features, args.hidden_size, args.num_classes, args.num_nodes, args.num_layers)

    def __init__(self, num_edge, num_channels, w_in, w_out, num_class, num_nodes, num_layers):
        super(GTN, self).__init__()
        self.num_edge = num_edge
        self.num_channels = num_channels
        self.num_nodes = num_nodes
        self.w_in = w_in
        self.w_out = w_out
        self.num_class = num_class
        self.num_layers = num_layers
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(GTLayer(num_edge, num_channels, num_nodes, first=True))
            else:
                layers.append(GTLayer(num_edge, num_channels, num_nodes, first=False))
        self.layers = nn.ModuleList(layers)
        self.cross_entropy_loss = nn.CrossEntropyLoss()
        self.gcn = GCNLayer(in_features=self.w_in, out_features=w_out)
        self.linear1 = nn.Linear(self.w_out * self.num_channels, self.w_out)
        self.linear2 = nn.Linear(self.w_out, self.num_class)

    def normalization(self, H):
        norm_H = []
        for i in range(self.num_channels):
            edge, value = H[i]
            edge, value = remove_self_loops(edge, value)
            edge = torch.stack(edge)
            deg_row, deg_col = self.norm(edge.detach(), self.num_nodes, value.detach())
            value = deg_col * value
            norm_H.append((edge, value))
        return norm_H

    def norm(self, edge_index, num_nodes, edge_weight, improved=False, dtype=None):
        with torch.no_grad():
            if edge_weight is None:
                edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)
            edge_weight = edge_weight.view(-1)
            assert edge_weight.size(0) == edge_index.size(1)
            row, col = edge_index
            deg = torch.zeros((num_nodes,))
            deg = deg.scatter_add_(dim=0, src=edge_weight, index=row).squeeze()
            deg_inv_sqrt = deg.pow(-1)
            deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        return deg_inv_sqrt[row], deg_inv_sqrt[col]

    def forward(self, graph):
        A = graph.adj
        X = graph.x
        Ws = []
        for i in range(self.num_layers):
            if i == 0:
                H, W = self.layers[i](A)
            else:
                H = self.normalization(H)
                H, W = self.layers[i](A, H)
            Ws.append(W)
        with graph.local_graph():
            for i in range(self.num_channels):
                if i == 0:
                    edge_index, edge_weight = H[i][0], H[i][1]
                    graph.edge_index = edge_index.detach()
                    graph.edge_weight = edge_weight
                    X_ = self.gcn(graph, X)
                    X_ = F.relu(X_)
                else:
                    edge_index, edge_weight = H[i][0], H[i][1]
                    graph.edge_index = edge_index.detach()
                    graph.edge_weight = edge_weight
                    X_ = torch.cat((X_, F.relu(self.gcn(graph, X))), dim=1)
        X_ = self.linear1(X_)
        X_ = F.relu(X_)
        out = self.linear2(X_)
        return out


class HAN(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--num-nodes', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--num-edge', type=int, default=2)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_edge, args.num_features, args.hidden_size, args.num_classes, args.num_nodes, args.num_layers)

    def __init__(self, num_edge, w_in, w_out, num_class, num_nodes, num_layers):
        super(HAN, self).__init__()
        self.num_edge = num_edge
        self.num_nodes = num_nodes
        self.w_in = w_in
        self.w_out = w_out
        self.num_class = num_class
        self.num_layers = num_layers
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(HANLayer(num_edge, w_in, w_out))
            else:
                layers.append(HANLayer(num_edge, w_out, w_out))
        self.layers = nn.ModuleList(layers)
        self.cross_entropy_loss = nn.CrossEntropyLoss()
        self.linear = nn.Linear(self.w_out, self.num_class)

    def forward(self, graph):
        X = graph.x
        for i in range(self.num_layers):
            X = self.layers[i](graph, X)
        out = self.linear(X)
        return out


class Encoder(nn.Module):
    """Encoder stacked with GIN layers

    Parameters
    ----------
    in_feats : int
        Size of each input sample.
    hidden_feats : int
        Size of output embedding.
    num_layers : int, optional
        Number of GIN layers, default: ``3``.
    num_mlp_layers : int, optional
        Number of MLP layers for each GIN layer, default: ``2``.
    pooling : str, optional
        Aggragation type, default : ``sum``.

    """

    def __init__(self, in_feats, hidden_dim, num_layers=3, num_mlp_layers=2, pooling='sum'):
        super(Encoder, self).__init__()
        self.num_layers = num_layers
        self.gnn_layers = nn.ModuleList()
        self.bn_layers = nn.ModuleList()
        for i in range(num_layers):
            if i == 0:
                mlp = MLP(in_feats, hidden_dim, hidden_dim, num_mlp_layers, norm='batchnorm')
            else:
                mlp = MLP(hidden_dim, hidden_dim, hidden_dim, num_mlp_layers, norm='batchnorm')
            self.gnn_layers.append(GINLayer(mlp, eps=0, train_eps=True))
            self.bn_layers.append(nn.BatchNorm1d(hidden_dim))
        if pooling == 'sum':
            self.pooling = batch_sum_pooling
        elif pooling == 'mean':
            self.pooling = batch_mean_pooling
        else:
            raise NotImplementedError

    def forward(self, graph, x=None, *args):
        batch = graph.batch
        if x is None:
            x = torch.ones((batch.shape[0], 1))
        layer_rep = []
        for i in range(self.num_layers):
            x = F.relu(self.bn_layers[i](self.gnn_layers[i](graph, x)))
            layer_rep.append(x)
        pooled_rep = [self.pooling(h, batch) for h in layer_rep]
        node_rep = torch.cat(layer_rep, dim=1)
        graph_rep = torch.cat(pooled_rep, dim=1)
        return graph_rep, node_rep


class FF(nn.Module):
    """Residual MLP layers.

    ..math::
        out = \\mathbf{MLP}(x) + \\mathbf{Linear}(x)

    Paramaters
    ----------
    in_feats : int
        Size of each input sample
    out_feats : int
        Size of each output sample
    """

    def __init__(self, in_feats, out_feats):
        super(FF, self).__init__()
        self.block = MLP(in_feats, out_feats, out_feats, num_layers=3)
        self.shortcut = nn.Linear(in_feats, out_feats)

    def forward(self, x):
        return F.relu(self.block(x)) + self.shortcut(x)


class InfoGraph(BaseModel):
    """Implementation of Infograph in paper `"InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation
     Learning via Mutual Information Maximization" <https://openreview.net/forum?id=r1lfF2NYvH>__. `

     Parameters
     ----------
     in_feats : int
        Size of each input sample.
    out_feats : int
        Size of each output sample.
    num_layers : int, optional
        Number of MLP layers in encoder, default: ``3``.
    unsup : bool, optional
        Use unsupervised model if True, default: ``True``.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=512)
        parser.add_argument('--batch-size', type=int, default=20)
        parser.add_argument('--target', dest='target', type=int, default=0, help='')
        parser.add_argument('--train-num', dest='train_num', type=int, default=5000)
        parser.add_argument('--num-layers', type=int, default=1)
        parser.add_argument('--sup', dest='sup', action='store_true')
        parser.add_argument('--epochs', type=int, default=15)
        parser.add_argument('--lr', type=float, default=0.0001)
        parser.add_argument('--train-ratio', type=float, default=0.7)
        parser.add_argument('--test-ratio', type=float, default=0.1)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.sup)

    @classmethod
    def split_dataset(cls, dataset, args):
        return split_dataset_general(dataset, args)

    def __init__(self, in_feats, hidden_dim, out_feats, num_layers=3, sup=False):
        super(InfoGraph, self).__init__()
        self.sup = sup
        self.emb_dim = hidden_dim
        self.out_feats = out_feats
        self.num_layers = num_layers
        self.sem_fc1 = nn.Linear(num_layers * hidden_dim, hidden_dim)
        self.sem_fc2 = nn.Linear(hidden_dim, out_feats)
        if not sup:
            self.unsup_encoder = Encoder(in_feats, hidden_dim, num_layers)
            self.register_parameter('sem_encoder', None)
        else:
            self.unsup_encoder = Encoder(in_feats, hidden_dim, num_layers)
            self.sem_encoder = Encoder(in_feats, hidden_dim, num_layers)
        self._fc1 = FF(num_layers * hidden_dim, hidden_dim)
        self._fc2 = FF(num_layers * hidden_dim, hidden_dim)
        self.local_dis = FF(num_layers * hidden_dim, hidden_dim)
        self.global_dis = FF(num_layers * hidden_dim, hidden_dim)
        self.criterion = nn.MSELoss()

    def reset_parameters(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight.data)

    def forward(self, batch):
        if self.sup:
            return self.sup_forward(batch, batch.x)
        else:
            return self.unsup_forward(batch, batch.x)

    def sup_forward(self, batch, x):
        node_feat, graph_feat = self.sem_encoder(batch, x)
        node_feat = F.relu(self.sem_fc1(node_feat))
        node_feat = self.sem_fc2(node_feat)
        return node_feat

    def unsup_forward(self, batch, x):
        graph_feat, node_feat = self.unsup_encoder(batch, x)
        if self.training:
            return graph_feat, node_feat
        else:
            return graph_feat


class M3S(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0)
        parser.add_argument('--num-clusters', type=int, default=50)
        parser.add_argument('--num-stages', type=int, default=10)
        parser.add_argument('--epochs-per-stage', type=int, default=50)
        parser.add_argument('--label-rate', type=float, default=1)
        parser.add_argument('--num-new-labels', type=int, default=2)
        parser.add_argument('--alpha', type=float, default=1)
        parser.add_argument('--approximate', action='store_true')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.dropout)

    def __init__(self, num_features, hidden_size, num_classes, dropout):
        super(M3S, self).__init__()
        self.dropout = dropout
        self.gcn1 = GCNLayer(num_features, hidden_size)
        self.gcn2 = GCNLayer(hidden_size, num_classes)

    def embed(self, graph):
        graph.sym_norm()
        h = graph.x
        h = self.gcn1(graph, h)
        h = F.relu(F.dropout(h, self.dropout, training=self.training))
        return h.detach().cpu().numpy()

    def forward(self, graph):
        graph.sym_norm()
        h = graph.x
        h = self.gcn1(graph, h)
        h = F.dropout(F.relu(h), self.dropout, training=self.training)
        h = self.gcn2(graph, h)
        return h

    def predict(self, data):
        return self.forward(data)


class MixHop(BaseModel):

    def __init__(self, num_features, num_classes, dropout, layer1_pows, layer2_pows):
        super(MixHop, self).__init__()
        self.dropout = dropout
        self.num_features = num_features
        self.num_classes = num_classes
        self.dropout = dropout
        layer_pows = [layer1_pows, layer2_pows]
        shapes = [num_features] + [sum(layer1_pows), sum(layer2_pows)]
        self.mixhops = nn.ModuleList([MixHopLayer(shapes[layer], [0, 1, 2], layer_pows[layer]) for layer in range(len(layer_pows))])
        self.fc = nn.Linear(shapes[-1], num_classes)

    def forward(self, graph):
        x = graph.x
        for mixhop in self.mixhops:
            x = F.relu(mixhop(graph, x))
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc(x)
        return x

    def predict(self, data):
        return self.forward(data)


class GraphConvBlock(nn.Module):

    def __init__(self, conv_func, conv_params, in_feats, out_feats, dropout=0.0, residual=False):
        super(GraphConvBlock, self).__init__()
        self.graph_conv = conv_func(**conv_params, in_features=in_feats, out_features=out_feats)
        self.pos_ff = CustomizedMoEPositionwiseFF(out_feats, out_feats * 2, dropout, moe_num_expert=64, moe_top_k=2)
        self.dropout = dropout
        if residual is True:
            assert in_feats is not None
            self.res_connection = nn.Linear(in_feats, out_feats)
        else:
            self.res_connection = None

    def reset_parameters(self):
        """Reinitialize model parameters."""
        if self.res_connection is not None:
            self.res_connection.reset_parameters()

    def forward(self, graph, feats):
        new_feats = self.graph_conv(graph, feats)
        if self.res_connection is not None:
            res = self.res_connection
            new_feats = new_feats + res
            new_feats = F.dropout(new_feats, p=self.dropout, training=self.training)
        new_feats = self.pos_ff(new_feats)
        return new_feats


class MoEGCN(BaseModel):
    """The GCN model from the `"Semi-Supervised Classification with Graph Convolutional Networks"
    <https://arxiv.org/abs/1609.02907>`_ paper

    Args:
        in_features (int) : Number of input features.
        out_features (int) : Number of classes.
        hidden_size (int) : The dimension of node representation.
        dropout (float) : Dropout rate for model training.
    """

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--no-residual', action='store_true')
        parser.add_argument('--norm', type=str, default='batchnorm')
        parser.add_argument('--activation', type=str, default='relu')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.dropout, args.activation, not args.no_residual, args.norm)

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout, activation='relu', residual=True, norm=None):
        super(MoEGCN, self).__init__()
        shapes = [in_feats] + [hidden_size] * num_layers
        conv_func = GCNLayer
        conv_params = {'dropout': dropout, 'norm': norm, 'residual': residual, 'activation': activation}
        self.layers = nn.ModuleList([GraphConvBlock(conv_func, conv_params, shapes[i], shapes[i + 1], dropout=dropout) for i in range(num_layers)])
        self.num_layers = num_layers
        self.dropout = dropout
        self.act = get_activation(activation)
        self.final_cls = nn.Linear(hidden_size, out_feats)

    def embed(self, graph):
        graph.sym_norm()
        h = graph.x
        for i in range(self.num_layers - 1):
            h = self.layers[i](graph, h)
        return h

    def forward(self, graph):
        graph.sym_norm()
        h = graph.x
        for i in range(self.num_layers):
            h = self.layers[i](graph, h)
        h = self.final_cls(h)
        return h

    def predict(self, data):
        return self.forward(data)


class AvgReadout(nn.Module):

    def __init__(self):
        super(AvgReadout, self).__init__()

    def forward(self, seq, msk=None):
        dim = len(seq.shape) - 2
        if msk is None:
            return torch.mean(seq, dim)
        else:
            return torch.sum(seq * msk, dim) / torch.sum(msk)


class Discriminator(nn.Module):

    def __init__(self, n_h):
        super(Discriminator, self).__init__()
        self.f_k = nn.Bilinear(n_h, n_h, 1)
        for m in self.modules():
            self.weights_init(m)

    def weights_init(self, m):
        if isinstance(m, nn.Bilinear):
            torch.nn.init.xavier_uniform_(m.weight.data)
            if m.bias is not None:
                m.bias.data.fill_(0.0)

    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):
        c_x = torch.unsqueeze(c, 0)
        c_x = c_x.expand_as(h_pl)
        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)
        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)
        if s_bias1 is not None:
            sc_1 += s_bias1
        if s_bias2 is not None:
            sc_2 += s_bias2
        logits = torch.cat((sc_1, sc_2))
        return logits


def construct_sparse(neighbors, weights, shape):
    i = np.repeat(np.arange(len(neighbors)), np.fromiter(map(len, neighbors), dtype=np.int))
    j = np.concatenate(neighbors)
    return sp.coo_matrix((np.concatenate(weights), (i, j)), shape)


def ppr_topk(adj_matrix, alpha, epsilon, nodes, topk):
    """Calculate the PPR matrix approximately using Anderson."""
    out_degree = np.sum(adj_matrix > 0, axis=1).A1
    nnodes = adj_matrix.shape[0]
    neighbors, weights = calc_ppr_topk_parallel(adj_matrix.indptr, adj_matrix.indices, out_degree, numba.float32(alpha), numba.float32(epsilon), nodes, topk)
    return construct_sparse(neighbors, weights, (len(nodes), nnodes))


def topk_ppr_matrix(adj_matrix, alpha, eps, idx, topk, normalization='row'):
    """Create a sparse matrix where each node has up to the topk PPR neighbors and their weights."""
    topk_matrix = ppr_topk(adj_matrix, alpha, eps, idx, topk).tocsr()
    if normalization == 'sym':
        deg = adj_matrix.sum(1).A1
        deg_sqrt = np.sqrt(np.maximum(deg, 1e-12))
        deg_inv_sqrt = 1.0 / deg_sqrt
        row, col = topk_matrix.nonzero()
        topk_matrix.data = deg_sqrt[idx[row]] * topk_matrix.data * deg_inv_sqrt[col]
    elif normalization == 'col':
        deg = adj_matrix.sum(1).A1
        deg_inv = 1.0 / np.maximum(deg, 1e-12)
        row, col = topk_matrix.nonzero()
        topk_matrix.data = deg[idx[row]] * topk_matrix.data * deg_inv[col]
    elif normalization == 'row':
        pass
    else:
        raise ValueError(f'Unknown PPR normalization: {normalization}')
    return topk_matrix


def build_topk_ppr_matrix_from_data(edge_index, *args, **kwargs):
    if isinstance(edge_index, torch.Tensor) or isinstance(edge_index, tuple):
        row, col = edge_index
        row, col = row.numpy(), col.numpy()
        num_node = int(max(row.max(), col.max())) + 1
        val = np.ones(row.shape[0])
        adj_matrix = sp.csr_matrix((val, (row, col)), shape=(num_node, num_node))
    else:
        adj_matrix = edge_index
    return topk_ppr_matrix(adj_matrix, *args, **kwargs)


def compute_ppr(adj, index, alpha=0.4, epsilon=0.0001, k=8, norm='row'):
    return build_topk_ppr_matrix_from_data(adj, alpha, epsilon, index, k, norm).tocsr()


class MVGRL(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--hidden-size', type=int, default=512)
        parser.add_argument('--sample-size', type=int, default=2000)
        parser.add_argument('--batch-size', type=int, default=4)
        parser.add_argument('--alpha', type=float, default=0.2)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.sample_size, args.batch_size, args.alpha, args.dataset)

    def __init__(self, in_feats, hidden_size, sample_size=2000, batch_size=4, alpha=0.2, dataset='cora'):
        super(MVGRL, self).__init__()
        self.sample_size = sample_size
        self.batch_size = batch_size
        self.hidden_size = hidden_size
        self.alpha = alpha
        self.sparse = True
        self.dataset_name = dataset
        self.gcn1 = GCN(in_feats, hidden_size, 'prelu')
        self.gcn2 = GCN(in_feats, hidden_size, 'prelu')
        self.read = AvgReadout()
        self.sigm = nn.Sigmoid()
        self.disc = Discriminator(hidden_size)
        self.loss_f = nn.BCEWithLogitsLoss()
        self.cache = None

    def _forward(self, adj, diff, seq1, seq2, msk):
        out_shape = list(seq1.shape[:-1]) + [self.hidden_size]
        seq1 = seq1.view(-1, seq1.shape[-1])
        seq2 = seq2.view(-1, seq2.shape[-1])
        h_1 = self.gcn1(adj, seq1, True)
        h_1 = h_1.view(out_shape)
        c_1 = self.read(h_1, msk)
        c_1 = self.sigm(c_1)
        h_2 = self.gcn2(diff, seq1, True)
        h_2 = h_2.view(out_shape)
        c_2 = self.read(h_2, msk)
        c_2 = self.sigm(c_2)
        h_3 = self.gcn1(adj, seq2, True)
        h_4 = self.gcn2(diff, seq2, True)
        h_3 = h_3.view(out_shape)
        h_4 = h_4.view(out_shape)
        ret = self.disc(c_1, c_2, h_1, h_2, h_3, h_4)
        return ret, h_1, h_2

    def augment(self, graph):
        num_nodes = graph.num_nodes
        adj = sp.coo_matrix((graph.edge_weight.cpu().numpy(), (graph.edge_index[0].cpu().numpy(), graph.edge_index[1].cpu().numpy())), shape=(graph.num_nodes, graph.num_nodes))
        diff = compute_ppr(adj.tocsr(), np.arange(num_nodes), self.alpha).tocoo()
        return adj, diff

    def preprocess(self, graph):
        None
        graph.add_remaining_self_loops()
        graph.sym_norm()
        adj, diff = self.augment(graph)
        if self.cache is None:
            self.cache = dict()
        graphs = []
        for g in [adj, diff]:
            row = torch.from_numpy(g.row).long()
            col = torch.from_numpy(g.col).long()
            val = torch.from_numpy(g.data).float()
            edge_index = torch.stack([row, col])
            graphs.append(Graph(edge_index=edge_index, edge_weight=val))
        self.cache['diff'] = graphs[1]
        self.cache['adj'] = graphs[0]
        None

    def forward(self, graph):
        if not self.training:
            return self.embed(graph)
        x = graph.x
        if self.cache is None or 'diff' not in self.cache:
            self.preprocess(graph)
        diff, adj = self.cache['diff'], self.cache['adj']
        self.sample_size = min(self.sample_size, graph.num_nodes - self.batch_size)
        idx = np.random.randint(0, graph.num_nodes - self.sample_size + 1, self.batch_size)
        logits = []
        for i in idx:
            ba = adj.subgraph(list(range(i, i + self.sample_size)))
            bd = diff.subgraph(list(range(i, i + self.sample_size)))
            bf = x[i:i + self.sample_size]
            idx = np.random.permutation(self.sample_size)
            shuf_fts = bf[idx, :]
            logit, _, _ = self._forward(ba, bd, bf, shuf_fts, None)
            logits.append(logit)
        return torch.stack(logits)

    def loss(self, data):
        if self.sample_size > data.num_nodes:
            self.sample_size = data.num_nodes
        if self.cache is None:
            self.device = next(self.gcn1.parameters()).device
            lbl_1 = torch.ones(self.batch_size, self.sample_size * 2)
            lbl_2 = torch.zeros(self.batch_size, self.sample_size * 2)
            lbl = torch.cat((lbl_1, lbl_2), 1)
            lbl = lbl
            self.cache = {'labels': lbl}
        lbl = self.cache['labels']
        logits = self.forward(data)
        loss = self.loss_f(logits, lbl)
        return loss

    def embed(self, data, msk=None):
        adj = self.cache['adj']
        diff = self.cache['diff']
        h_1 = self.gcn1(adj, data.x, True)
        h_2 = self.gcn2(diff, data.x, True)
        return (h_1 + h_2).detach()


def assemble_neighbor(G, node, num_neighbor, sorted_nodes):
    """assemble neighbors for node with BFS strategy"""
    neighbors_dict = dict()
    new_neighbors_dict = dict()
    neighbors_dict[node] = 0
    new_neighbors_dict[node] = 0
    while len(neighbors_dict) < num_neighbor and len(new_neighbors_dict) > 0:
        temp_neighbor_dict = dict()
        for v, d in new_neighbors_dict.items():
            for new_v in G.neighbors(v):
                if new_v not in temp_neighbor_dict:
                    temp_neighbor_dict[new_v] = d + 1
        n = len(neighbors_dict)
        for v, d in temp_neighbor_dict.items():
            if v not in neighbors_dict:
                neighbors_dict[v] = d
        new_neighbors_dict = temp_neighbor_dict
        if n == len(neighbors_dict):
            break
    while len(neighbors_dict) < num_neighbor:
        rand_node = sorted_nodes[random.randint(0, len(sorted_nodes) - 1)][0]
        if rand_node not in neighbors_dict:
            neighbors_dict[rand_node] = 10
    return neighbors_dict


def cmp(s1, s2):
    list1 = [int(l) for l in s1.strip().split(' ')]
    list2 = [int(l) for l in s2.strip().split(' ')]
    i = 0
    while i < len(list1) and i < len(list2):
        if list1[i] < list2[i]:
            return -1
        if list1[i] > list2[i]:
            return 1
        i += 1
    if i < len(list1):
        return 1
    elif i < len(list2):
        return -1
    else:
        return 0


def one_dim_wl(graph_list, init_labels, iteration=5):
    """1-dimension Wl method used for node normalization for all the subgraphs"""
    sorted_labels = sorted(list(set(init_labels.values())))
    label_dict = dict([(label, index) for index, label in enumerate(sorted_labels)])
    graph_label_list = []
    for t in range(iteration):
        new_label_dict = dict()
        if t == 0:
            for i in range(len(graph_list)):
                labels = dict()
                for id, v in enumerate(graph_list[i].nodes()):
                    labels[v] = str(init_labels[v])
                    new_label_dict[labels[v]] = 1
                graph_label_list.append(labels)
        else:
            for i in range(len(graph_list)):
                labels = dict()
                for id, v in enumerate(graph_list[i].nodes()):
                    neighbor_labels = [graph_label_list[i][v2] for v2 in graph_list[i].neighbors(v)]
                    sorted_labels = [str(l) for l in sorted(neighbor_labels)]
                    new_label = str(graph_label_list[i][v]) + ' ' + ' '.join(sorted_labels)
                    new_label_dict[new_label] = 1
                    labels[v] = new_label
                graph_label_list[i] = labels.copy()
        sorted_list = sorted(new_label_dict.keys(), key=functools.cmp_to_key(cmp), reverse=False)
        for new_label in sorted_list:
            if new_label not in label_dict:
                label_dict[new_label] = len(label_dict)
        for i in range(len(graph_list)):
            for id, v in enumerate(graph_list[i].nodes()):
                graph_label_list[i][v] = label_dict[graph_label_list[i][v]]
    return graph_label_list


def node_selection_with_1d_wl(G, features, num_channel, num_sample, num_neighbor, stride):
    """construct features for cnn"""
    X = np.zeros((num_channel, num_sample, num_neighbor), dtype=np.float32)
    node2id = dict([(node, vid) for vid, node in enumerate(G.nodes())])
    betweenness = nx.betweenness_centrality(G)
    sorted_nodes = sorted(betweenness.items(), key=lambda d: d[1], reverse=False)
    i = 0
    j = 0
    root_list = []
    distance_list = []
    graph_list = []
    while j < num_sample:
        if i < len(sorted_nodes):
            neighbors_dict = assemble_neighbor(G, sorted_nodes[i][0], num_neighbor, sorted_nodes)
            sub_g = G.subgraph(neighbors_dict.keys())
            root_list.append(sorted_nodes[i][0])
            distance_list.append(neighbors_dict)
            graph_list.append(sub_g)
        else:
            X[:, j, :] = np.zeros((num_channel, num_neighbor), dtype=np.float32)
        i += stride
        j += 1
    init_labels = dict([(v, features[id].argmax(axis=0)) for v, id in node2id.items()])
    graph_labels_list = one_dim_wl(graph_list, init_labels)
    for i in range(len(root_list)):
        graph_labels_list[i][root_list[i]] = 0
        sorted_measurement = dict([(v, [measure, distance_list[i][v]]) for v, measure in graph_labels_list[i].items()])
        sorted_neighbor = sorted(sorted_measurement.items(), key=lambda d: d[1], reverse=False)[:num_neighbor]
        reorder_dict = dict(zip(sorted(sorted_measurement.keys()), range(len(sorted_measurement))))
        X[:, i, :] = features[[reorder_dict[v] for v, measure in sorted_neighbor]].T
    return X.reshape(num_channel, num_sample * num_neighbor)


def get_single_feature(data, num_features, num_classes, num_sample, num_neighbor, stride=1):
    """construct features"""
    data_list = [data]
    X = np.zeros((len(data_list), num_features, num_sample * num_neighbor), dtype=np.float32)
    for i in range(len(data_list)):
        edge_index, features = data_list[i].edge_index, data_list[i].x
        G = nx.Graph()
        row, col = edge_index[0].numpy(), edge_index[1].numpy()
        G.add_edges_from(list(zip(row, col)))
        if G.number_of_nodes() > num_neighbor:
            X[i] = node_selection_with_1d_wl(G, features.cpu().numpy(), num_features, num_sample, num_neighbor, stride)
    X = X.astype(np.float32)
    return X


class PatchySAN(BaseModel):
    """The Patchy-SAN model from the `"Learning Convolutional Neural Networks for Graphs"
    <https://arxiv.org/abs/1605.05273>`_ paper.

    Args:
        batch_size (int) : The batch size of training.
        sample (int) : Number of chosen vertexes.
        stride (int) : Node selection stride.
        neighbor (int) : The number of neighbor for each node.
        iteration (int) : The number of training iteration.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-sample', default=30, type=int, help='Number of chosen vertexes')
        parser.add_argument('--num-neighbor', default=10, type=int, help='Number of neighbor in constructing features')
        parser.add_argument('--iteration', default=5, type=int, help='Number of iteration')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.num_classes, args.num_sample, args.num_neighbor, args.iteration)

    @classmethod
    def split_dataset(cls, dataset, args):
        for i, data in enumerate(dataset):
            new_feature = get_single_feature(dataset[i], args.num_features, args.num_classes, args.num_sample, args.num_neighbor, args.stride)
            dataset[i].x = torch.from_numpy(new_feature)
        return split_dataset_general(dataset, args)

    def __init__(self, num_features, num_classes, num_sample, num_neighbor, iteration):
        super(PatchySAN, self).__init__()
        self.num_features = num_features
        self.num_classes = num_classes
        self.num_sample = num_sample
        self.num_neighbor = num_neighbor
        self.iteration = iteration
        self.build_model(self.num_features, self.num_sample, self.num_neighbor, self.num_classes)

    def build_model(self, num_channel, num_sample, num_neighbor, num_class):
        rep1, stride1 = 4, 4
        num_filter1, num_filter2 = 16, 8
        self.conv1 = nn.Conv1d(num_channel, num_filter1, rep1, stride=stride1, groups=1)
        self.conv2 = nn.Conv1d(num_filter1, num_filter2, num_neighbor, stride=1, groups=1)
        num_lin = (int(num_sample * num_neighbor / stride1) - num_neighbor + 1) * num_filter2
        self.lin1 = torch.nn.Linear(num_lin, 128)
        self.lin2 = torch.nn.Linear(128, num_class)
        self.nn = nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU(), nn.Flatten(), self.lin1, nn.ReLU(), nn.Dropout(0.2), self.lin2, nn.Softmax())

    def forward(self, batch):
        logits = self.nn(batch.x)
        return logits


class PPNP(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        parser.add_argument('--num-features', type=int)
        parser.add_argument('--num-classes', type=int)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--propagation-type', type=str, default='appnp')
        parser.add_argument('--alpha', type=float, default=0.1)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--num-iterations', type=int, default=10)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.dropout, args.propagation_type, args.alpha, args.num_iterations)

    def __init__(self, nfeat, nhid, nclass, num_layers, dropout, propagation, alpha, niter, cache=True):
        super(PPNP, self).__init__()
        self.nn = MLP(nfeat, nclass, nhid, num_layers, dropout)
        if propagation not in ('appnp', 'ppnp'):
            None
            propagation = 'appnp'
        self.propagation = propagation
        self.alpha = alpha
        self.niter = niter
        self.dropout = dropout
        self.vals = None
        self.use_cache = cache
        self.cache = dict()

    def forward(self, graph):

        def get_ready_format(input, edge_index, edge_attr=None):
            if isinstance(edge_index, tuple):
                edge_index = torch.stack(edge_index)
            if edge_attr is None:
                edge_attr = torch.ones(edge_index.shape[1]).float()
            adj = torch.sparse_coo_tensor(edge_index, edge_attr, (input.shape[0], input.shape[0]))
            return adj
        x = graph.x
        graph.sym_norm()
        x = F.dropout(x, p=self.dropout, training=self.training)
        local_preds = self.nn.forward(x)
        if self.propagation == 'ppnp':
            if self.vals is None:
                self.vals = self.alpha * torch.inverse(torch.eye(x.shape[0]) - (1 - self.alpha) * get_ready_format(x, graph.edge_index, graph.edge_attr))
            final_preds = F.dropout(self.vals) @ local_preds
        else:
            preds = local_preds
            with graph.local_graph():
                graph.edge_weight = F.dropout(graph.edge_weight, p=self.dropout, training=self.training)
                graph.set_symmetric()
                for _ in range(self.niter):
                    new_features = spmm(graph, preds)
                    preds = (1 - self.alpha) * new_features + self.alpha * local_preds
                final_preds = preds
        return final_preds

    def predict(self, graph):
        return self.forward(graph)


class PPRGo(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=32)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--dropout', type=float, default=0.1)
        parser.add_argument('--activation', type=str, default='relu')
        parser.add_argument('--nprop-inference', type=int, default=2)
        parser.add_argument('--alpha', type=float, default=0.5)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(in_feats=args.num_features, hidden_size=args.hidden_size, out_feats=args.num_classes, num_layers=args.num_layers, alpha=args.alpha, dropout=args.dropout, activation=args.activation, nprop=args.nprop_inference, norm=args.norm if hasattr(args, 'norm') else 'sym')

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, alpha, dropout, activation='relu', nprop=2, norm='sym'):
        super(PPRGo, self).__init__()
        self.alpha = alpha
        self.norm = norm
        self.nprop = nprop
        self.fc = PPRGoLayer(in_feats, hidden_size, out_feats, num_layers, dropout, activation)

    def forward(self, x, targets, ppr_scores):
        h = self.fc(x)
        h = ppr_scores.unsqueeze(1) * h
        batch_size = targets[-1] + 1
        out = torch.zeros(batch_size, h.shape[1]).to(x.device)
        out = out.scatter_add_(dim=0, index=targets[:, None].repeat(1, h.shape[1]), src=h)
        return out

    def predict(self, graph, batch_size=10000):
        device = next(self.parameters()).device
        x = graph.x
        num_nodes = x.shape[0]
        pred_logits = []
        with torch.no_grad():
            for i in range(0, num_nodes, batch_size):
                batch_x = x[i:i + batch_size]
                batch_logits = self.fc(batch_x)
                pred_logits.append(batch_logits.cpu())
        pred_logits = torch.cat(pred_logits, dim=0)
        pred_logits = pred_logits
        with graph.local_graph():
            if self.norm == 'sym':
                graph.sym_norm()
            elif self.norm == 'row':
                graph.row_norm()
            else:
                raise NotImplementedError
            edge_weight = graph.edge_weight * (1 - self.alpha)
            graph.edge_weight = edge_weight
            predictions = pred_logits
            for _ in range(self.nprop):
                predictions = spmm(graph, predictions) + self.alpha * pred_logits
        return predictions


def shared_dropout(x, dropout):
    m = torch.zeros_like(x).bernoulli_(1 - dropout)
    mask = m.requires_grad_(False) / (1 - dropout)
    return mask


class RevGCN(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--group', type=int, default=2)
        parser.add_argument('--drop-edge-rate', type=float, default=0.0)
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--norm', type=str, default='batchnorm')
        parser.add_argument('--activation', type=str, default='relu')
        parser.add_argument('--num-layers', type=int, default=2)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.num_classes, args.hidden_size, args.num_layers, args.dropout, args.drop_edge_rate, args.activation, args.norm, args.group)

    def __init__(self, in_feats, out_feats, hidden_size, num_layers, dropout=0.5, drop_edge_rate=0.1, activation='relu', norm='batchnorm', group=2):
        super(RevGCN, self).__init__()
        self.dropout = dropout
        self.drop_edge_rate = drop_edge_rate
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.norm = get_norm_layer(norm, hidden_size)
        self.act = get_activation(activation)
        for i in range(num_layers):
            if i == 0:
                self.layers.append(GCNLayer(in_feats, hidden_size, residual=True))
            elif i == num_layers - 1:
                self.layers.append(GCNLayer(hidden_size, out_feats, residual=True))
            else:
                conv = GCNLayer(hidden_size // group, hidden_size // group)
                res_conv = ResGNNLayer(conv, hidden_size // group, activation=activation, norm=norm, out_norm=norm, out_channels=hidden_size // group)
                self.layers.append(RevGNNLayer(res_conv, group))

    def forward(self, graph):
        graph.requires_grad = False
        edge_index, edge_weight = dropout_adj(graph.edge_index, drop_rate=self.drop_edge_rate, renorm=None, training=self.training)
        h = graph.x
        h = F.dropout(h, self.dropout, training=self.training)
        with graph.local_graph():
            graph.edge_index = edge_index
            graph.sym_norm()
            assert (graph.degrees() > 0).all()
            h = self.layers[0](graph, h)
            mask = shared_dropout(h, self.dropout)
            for i in range(1, len(self.layers) - 1):
                h = self.layers[i](graph, h, mask)
            h = self.norm(h)
            h = self.act(h)
            h = F.dropout(h, p=self.dropout, training=self.training)
            h = self.layers[-1](graph, h)
        return h


class RevGEN(BaseModel):

    @staticmethod
    def add_args(parser):
        DeeperGCN.add_args(parser)
        parser.add_argument('--group', type=int, default=2)
        parser.add_argument('--norm', type=str, default='batchnorm')
        parser.add_argument('--last-norm', type=str, default='batchnorm')
        parser.add_argument('--use-one-hot-emb', action='store_true')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.group, args.activation, args.norm, args.last_norm, args.dropout, args.aggr, args.beta, args.p, args.learn_beta, args.learn_p, args.learn_msg_scale, args.use_msg_norm, edge_attr_size=args.edge_attr_size, one_hot_emb=args.use_one_hot_emb if hasattr(args, 'use_one_hot_emb') else False)

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, group=2, activation='relu', norm='batchnorm', last_norm='batchnorm', dropout=0.0, aggr='softmax_sg', beta=1.0, p=1.0, learn_beta=False, learn_p=False, learn_msg_scale=True, use_msg_norm=False, edge_attr_size: Optional[list]=None, one_hot_emb: bool=False):
        super(RevGEN, self).__init__()
        self.input_fc = nn.Linear(in_feats, hidden_size)
        self.output_fc = nn.Linear(hidden_size, out_feats)
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            conv = GENConv(hidden_size // group, hidden_size // group, aggr, beta, p, learn_beta, learn_p, use_msg_norm, learn_msg_scale, residual=True, edge_attr_size=edge_attr_size)
            res_conv = ResGNNLayer(conv, hidden_size // group, norm=norm, activation=activation, residual=False)
            self.layers.append(RevGNNLayer(res_conv, group))
        self.activation = get_activation(activation)
        self.norm = get_norm_layer(last_norm, hidden_size)
        self.dropout = dropout
        if one_hot_emb:
            self.one_hot_encoder = nn.Linear(in_feats // 2, in_feats // 2)
        self.use_one_hot_emb = one_hot_emb

    def forward(self, graph):
        graph.requires_grad = False
        x = graph.x
        if self.use_one_hot_emb:
            x = x.split(2, dim=-1)
            x[1] = self.one_hot_encoder(x[1])
            x = torch.cat((x[0], x[1]), dim=1)
        h = self.input_fc(x)
        mask = shared_dropout(h, self.dropout)
        for layer in self.layers:
            h = layer(graph, h, mask)
        h = self.activation(self.norm(h))
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.output_fc(h)
        return h


class RevGAT(BaseModel):

    @staticmethod
    def add_args(parser):
        GAT.add_args(parser)
        parser.add_argument('--norm', type=str, default='batchnorm')
        parser.add_argument('--activation', type=str, default='relu')
        parser.add_argument('--group', type=int, default=2)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.group, args.alpha, args.nhead, args.dropout, args.attn_drop, args.activation, args.norm)

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, group=2, alpha=0.2, nhead=1, dropout=0.5, attn_drop=0.5, activation='relu', norm='batchnorm'):
        super(RevGAT, self).__init__()
        self.dropout = dropout
        self.num_layers = num_layers
        self.layers = nn.ModuleList()
        self.norm = get_norm_layer(norm, hidden_size * nhead)
        self.act = get_activation(activation)
        for i in range(num_layers):
            if i == 0:
                self.layers.append(GATLayer(in_feats, hidden_size, nhead, alpha, attn_drop, residual=True))
            elif i == num_layers - 1:
                self.layers.append(GATLayer(hidden_size * nhead, out_feats, 1, alpha, attn_drop, residual=True))
            else:
                conv = GATLayer(hidden_size * nhead // group, hidden_size // group, nhead=nhead, alpha=alpha, attn_drop=attn_drop)
                res_conv = ResGNNLayer(conv, hidden_size * nhead // group, activation=activation, norm=norm, out_norm=norm, out_channels=hidden_size * nhead // group)
                self.layers.append(RevGNNLayer(res_conv, group))

    def forward(self, graph):
        graph.requires_grad = False
        h = graph.x
        h = F.dropout(h, self.dropout, training=self.training)
        h = self.layers[0](graph, h)
        mask = shared_dropout(h, self.dropout)
        for i in range(1, len(self.layers) - 1):
            h = self.layers[i](graph, h, mask)
            if torch.isnan(h).any():
                None
                input()
        h = self.norm(h)
        h = self.act(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.layers[-1](graph, h)
        return h


class RGCN(nn.Module):

    def __init__(self, in_feats, out_feats, num_layers, num_rels, regularizer='basis', num_bases=None, self_loop=True, dropout=0.0, self_dropout=0.0):
        super(RGCN, self).__init__()
        shapes = [in_feats] + [out_feats] * num_layers
        self.num_layers = num_layers
        self.layers = nn.ModuleList(RGCNLayer(shapes[i], shapes[i + 1], num_rels, regularizer, num_bases, self_loop, dropout, self_dropout) for i in range(num_layers))

    def forward(self, graph, x):
        h = x
        for i in range(len(self.layers)):
            h = self.layers[i](graph, h)
            if i < self.num_layers - 1:
                h = F.relu(h)
        return h


class LinkPredictRGCN(GNNLinkPredict, BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=200)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--regularizer', type=str, default='basis')
        parser.add_argument('--self-loop', action='store_false')
        parser.add_argument('--penalty', type=float, default=0.001)
        parser.add_argument('--dropout', type=float, default=0.2)
        parser.add_argument('--self-dropout', type=float, default=0.4)
        parser.add_argument('--num-bases', type=int, default=5)
        parser.add_argument('--sampling-rate', type=float, default=0.01)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(num_entities=args.num_entities, num_rels=args.num_rels, hidden_size=args.hidden_size, num_layers=args.num_layers, regularizer=args.regularizer, num_bases=args.num_bases, self_loop=args.self_loop, sampling_rate=args.sampling_rate, penalty=args.penalty, dropout=args.dropout, self_dropout=args.self_dropout)

    def __init__(self, num_entities, num_rels, hidden_size, num_layers, regularizer='basis', num_bases=None, self_loop=True, sampling_rate=0.01, penalty=0, dropout=0.0, self_dropout=0.0):
        BaseModel.__init__(self)
        GNNLinkPredict.__init__(self)
        self.penalty = penalty
        self.num_nodes = num_entities
        self.num_rels = num_rels
        self.sampling_rate = sampling_rate
        self.edge_set = None
        self.model = RGCN(in_feats=hidden_size, out_feats=hidden_size, num_layers=num_layers, num_rels=num_rels, regularizer=regularizer, num_bases=num_bases, self_loop=self_loop, dropout=dropout, self_dropout=self_dropout)
        self.rel_weight = nn.Embedding(num_rels, hidden_size)
        self.emb = nn.Embedding(num_entities, hidden_size)

    def forward(self, graph):
        reindexed_nodes, reindexed_edges = torch.unique(torch.stack(graph.edge_index), sorted=True, return_inverse=True)
        x = self.emb(reindexed_nodes)
        self.cahce_index = reindexed_nodes
        graph.edge_index = reindexed_edges
        output = self.model(graph, x)
        return output

    def loss(self, graph, scoring):
        edge_index = graph.edge_index
        edge_types = graph.edge_attr
        self.get_edge_set(edge_index, edge_types)
        batch_edges, batch_attr, samples, rels, labels = sampling_edge_uniform(edge_index, edge_types, self.edge_set, self.sampling_rate, self.num_rels)
        graph = graph.__class__(edge_index=batch_edges, edge_attr=batch_attr)
        output = self.forward(graph)
        edge_weight = self.rel_weight(rels)
        sampled_nodes, reindexed_edges = torch.unique(samples, sorted=True, return_inverse=True)
        assert (sampled_nodes == self.cahce_index).any()
        sampled_types = torch.unique(rels)
        loss_n = self._loss(output[reindexed_edges[0]], output[reindexed_edges[1]], edge_weight, labels, scoring) + self.penalty * self._regularization([self.emb(sampled_nodes), self.rel_weight(sampled_types)])
        return loss_n

    def predict(self, graph):
        device = next(self.parameters()).device
        indices = torch.arange(0, self.num_nodes)
        x = self.emb(indices)
        output = self.model(graph, x)
        return output, self.rel_weight.weight


class SAGN(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=512)
        parser.add_argument('--negative-slope', type=float, default=0.2)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--input-drop', type=float, default=0.0)
        parser.add_argument('--attn-drop', type=float, default=0.4)
        parser.add_argument('--nhead', type=int, default=2)
        parser.add_argument('--mlp-layer', type=int, default=4)
        parser.add_argument('--use-labels', action='store_true')
        parser.add_argument('--nhop', type=int, default=4)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.num_classes, args.hidden_size, args.nhop, args.mlp_layer, args.nhead, args.dropout, args.input_drop, args.attn_drop, args.negative_slope, args.use_labels)

    def __init__(self, in_feats, out_feats, hidden_size, nhop, mlp_layer, nhead, dropout=0.5, input_drop=0.0, attn_drop=0.0, negative_slope=0.2, use_labels=False):
        super(SAGN, self).__init__()
        self.dropout = dropout
        self.nhead = nhead
        self.hidden_size = hidden_size
        self.attn_dropout = attn_drop
        self.input_dropout = input_drop
        self.use_labels = use_labels
        self.negative_slope = negative_slope
        self.norm = nn.BatchNorm1d(hidden_size)
        self.layers = nn.ModuleList([MLP(in_feats, hidden_size * nhead, hidden_size, mlp_layer, norm='batchnorm', dropout=dropout) for _ in range(nhop + 1)])
        self.mlp = MLP(hidden_size, out_feats, hidden_size, mlp_layer, norm='batchnorm', dropout=dropout)
        self.res_conn = nn.Linear(in_feats, hidden_size * nhead, bias=False)
        if use_labels:
            self.label_mlp = MLP(out_feats, out_feats, hidden_size, 2 * mlp_layer, norm='batchnorm', dropout=dropout)
        self.attn_l = nn.Parameter(torch.FloatTensor(size=(1, nhead, hidden_size)))
        self.attn_r = nn.Parameter(torch.FloatTensor(size=(1, nhead, hidden_size)))

    def reset_parameters(self):
        gain = nn.init.calculate_gain('relu')
        for layer in self.layers:
            layer.reset_parameters()
        nn.init.xavier_normal_(self.attn_r, gain=gain)
        nn.init.xavier_normal_(self.attn_l, gain=gain)
        if self.use_labels:
            self.label_mlp.reset_parameters()
        self.norm.reset_parameters()
        self.mlp.reset_parameters()

    def forward(self, features, y_emb=None):
        out = 0
        features = [F.dropout(x, p=self.input_dropout, training=self.training) for x in features]
        hidden = [self.layers[i](features[i]).view(-1, self.nhead, self.hidden_size) for i in range(len(features))]
        a_r = (hidden[0] * self.attn_r).sum(dim=-1).unsqueeze(-1)
        a_ls = [(h * self.attn_l).sum(dim=-1).unsqueeze(-1) for h in hidden]
        a = torch.cat([(a_l + a_r).unsqueeze(-1) for a_l in a_ls], dim=-1)
        a = F.leaky_relu(a, negative_slope=self.negative_slope)
        a = F.softmax(a, dim=-1)
        a = F.dropout(a, p=self.attn_dropout, training=self.training)
        for i in range(a.shape[-1]):
            out += hidden[i] * a[:, :, :, i]
        out += self.res_conn(features[0]).view(-1, self.nhead, self.hidden_size)
        out = out.mean(1)
        out = F.relu(self.norm(out))
        out = F.dropout(out, p=self.dropout, training=self.training)
        out = self.mlp(out)
        if self.use_labels and y_emb is not None:
            out += self.label_mlp(y_emb)
        return out


def get_adj(graph, remove_diag=False):
    if remove_diag:
        graph.remove_self_loops()
    else:
        graph.add_remaining_self_loops()
    return graph


def multi_hop_ppr_diffusion(graph, x, nhop, alpha=0.5):
    results = []
    for _ in range(nhop):
        x = (1 - alpha) * x + spmm(graph, x)
        results.append(x)
    return results


def multi_hop_sgc(graph, x, nhop):
    results = []
    for _ in range(nhop):
        x = spmm(graph, x)
        results.append(x)
    return results


def to_undirected(edge_index, num_nodes=None):
    """Converts the graph given by :attr:`edge_index` to an undirected graph,
    so that :math:`(j,i) \\in \\mathcal{E}` for every edge :math:`(i,j) \\in
    \\mathcal{E}`.

    Args:
        edge_index (LongTensor): The edge indices.
        num_nodes (int, optional): The number of nodes, *i.e.*
            :obj:`max_val + 1` of :attr:`edge_index`. (default: :obj:`None`)

    :rtype: :class:`LongTensor`
    """
    row, col = edge_index
    row, col = torch.cat([row, col], dim=0), torch.cat([col, row], dim=0)
    row, col, _ = coalesce(row, col, None)
    edge_index = torch.stack([row, col])
    return edge_index


class SIGN(BaseModel):

    @staticmethod
    def add_args(parser):
        """Add model-specific arguments to the parser."""
        MLP.add_args(parser)
        parser.add_argument('--dropedge-rate', type=float, default=0.2)
        parser.add_argument('--directed', action='store_true')
        parser.add_argument('--nhop', type=int, default=3)
        parser.add_argument('--adj-norm', type=str, default=['sym'], nargs='+')
        parser.add_argument('--remove-diag', action='store_true')
        parser.add_argument('--diffusion', type=str, default='ppr')

    def __init__(self, num_features, hidden_size, num_classes, num_layers, dropout=0.0, dropedge_rate=0.2, nhop=3, adj_norm=['sym'], diffusion='ppr', remove_diag=False, undirected=True, norm='batchnorm', activation='relu'):
        super(SIGN, self).__init__()
        self.dropedge_rate = dropedge_rate
        self.undirected = undirected
        self.num_propagations = nhop
        self.adj_norm = adj_norm
        self.remove_diag = remove_diag
        self.diffusion = diffusion
        num_features = num_features * (1 + nhop * len(adj_norm))
        self.mlp = MLP(in_feats=num_features, out_feats=num_classes, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, activation=activation, norm=norm)
        self.cache_x = None

    def _preprocessing(self, graph, x, drop_edge=False):
        device = x.device
        graph
        x = x
        graph.eval()
        op_embedding = [x]
        edge_index = graph.edge_index
        if self.undirected:
            edge_index = to_undirected(edge_index)
        if drop_edge:
            edge_index, _ = dropout_adj(edge_index, drop_rate=self.dropedge_rate)
        graph = get_adj(graph, remove_diag=self.remove_diag)
        for norm in self.adj_norm:
            with graph.local_graph():
                graph.edge_index = edge_index
                graph.normalize(norm)
                if self.diffusion == 'ppr':
                    results = multi_hop_ppr_diffusion(graph, graph.x, self.num_propagations)
                else:
                    results = multi_hop_sgc(graph, graph.x, self.num_propagations)
                op_embedding.extend(results)
        graph
        return torch.cat(op_embedding, dim=1)

    def preprocessing(self, graph, x):
        None
        dataset_name = None
        if dataset_name is not None:
            adj_norm = ','.join(self.adj_norm)
            dataset_name = f'{self.dataset_name}_{self.num_propagations}_{self.diffusion}_{adj_norm}.pt'
            if os.path.exists(dataset_name):
                return torch.load(dataset_name)
        if graph.is_inductive():
            graph.train()
            x_train = self._preprocessing(graph, x, drop_edge=True)
            graph.eval()
            x_all = self._preprocessing(graph, x, drop_edge=False)
            train_nid = graph.train_nid
            x_all[train_nid] = x_train[train_nid]
        else:
            x_all = self._preprocessing(graph, x, drop_edge=False)
        if dataset_name is not None:
            torch.save(x_all.cpu(), dataset_name)
        None
        return x_all

    def reset_parameters(self):
        self.mlp.nn.reset_parameters()

    def forward(self, graph):
        if self.cache_x is None:
            x = graph.x.contiguous()
            self.cache_x = self.preprocessing(graph, x)
        x = self.cache_x
        x = self.mlp(x)
        return F.log_softmax(x, dim=-1)


def scatter_sum(src, index, dim, dim_size):
    size = list(src.size())
    if dim_size is not None:
        size[dim] = dim_size
    else:
        size[dim] = int(index.max()) + 1
    out = torch.zeros(size, dtype=src.dtype, device=src.device)
    return out.scatter_add_(dim, index, src)


def spare2dense_batch(x, batch=None, fill_value=0):
    batch_size = batch[-1] + 1
    batch_num_nodes = scatter_sum(batch.new_ones(x.size(0)), batch, dim=0, dim_size=batch_size)
    max_num_nodes = batch_num_nodes.max().item()
    batch_cum_nodes = torch.cat([batch.new_zeros(1), batch_num_nodes.cumsum(dim=0)])
    idx = torch.arange(x.size(0), dtype=torch.long, device=x.device)
    idx = idx - batch_cum_nodes[batch] + batch * max_num_nodes
    new_size = [batch_size * max_num_nodes, x.size(1)]
    out = x.new_full(new_size, fill_value)
    out[idx] = x
    out = out.view([batch_size, max_num_nodes, x.size(1)])
    return out


class SortPool(BaseModel):
    """Implimentation of sortpooling in paper `"An End-to-End Deep Learning
    Architecture for Graph Classification" <https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf>__.`

    Parameters
    ----------
    in_feats : int
        Size of each input sample.
    out_feats : int
        Size of each output sample.
    hidden_dim : int
        Dimension of hidden layer embedding.
    num_classes : int
        Number of target classes.
    num_layers : int
        Number of graph neural network layers before pooling.
    k : int, optional
        Number of selected features to sort, default: ``30``.
    out_channel : int
        Number of the first convolution's output channels.
    kernel_size : int
        Size of the first convolution's kernel.
    dropout : float, optional
        Size of dropout, default: ``0.5``.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.1)
        parser.add_argument('--batch-size', type=int, default=20)
        parser.add_argument('--train-ratio', type=float, default=0.7)
        parser.add_argument('--test-ratio', type=float, default=0.1)
        parser.add_argument('--num-layers', type=int, default=2)
        parser.add_argument('--out-channels', type=int, default=32)
        parser.add_argument('--k', type=int, default=30)
        parser.add_argument('--kernel-size', type=int, default=5)

    @classmethod
    def build_model_from_args(cls, args):
        return cls(args.num_features, args.hidden_size, args.num_classes, args.num_layers, args.out_channels, args.kernel_size, args.k, args.dropout)

    @classmethod
    def split_dataset(cls, dataset, args):
        return split_dataset_general(dataset, args)

    def __init__(self, in_feats, hidden_dim, num_classes, num_layers, out_channel, kernel_size, k=30, dropout=0.5):
        super(SortPool, self).__init__()
        self.k = k
        self.dropout = dropout
        self.num_layers = num_layers
        self.gnn_convs = nn.ModuleList()
        self.gnn_convs.append(SAGELayer(in_feats, hidden_dim))
        for _ in range(self.num_layers - 1):
            self.gnn_convs.append(SAGELayer(hidden_dim, hidden_dim))
        self.conv1d = nn.Conv1d(hidden_dim, out_channel, kernel_size)
        self.fc1 = nn.Linear(out_channel * (self.k - kernel_size + 1), hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_classes)

    def forward(self, batch):
        h = batch.x
        for i in range(self.num_layers):
            h = self.gnn_convs[i](batch, h)
            h = F.relu(h)
        h, _ = h.sort(dim=-1)
        fill_value = h.min().item() - 1
        batch_h = spare2dense_batch(h, batch.batch, fill_value)
        batch_size, num_nodes, xdim = batch_h.size()
        _, order = batch_h[:, :, -1].sort(dim=-1, descending=True)
        order = order + torch.arange(batch_size, dtype=torch.long, device=order.device).view(-1, 1) * num_nodes
        batch_h = batch_h.view(batch_size * num_nodes, xdim)
        batch_h = batch_h[order].view(batch_size, num_nodes, xdim)
        if num_nodes >= self.k:
            batch_h = batch_h[:, :self.k].contiguous()
        else:
            fill_batch = batch_h.new_full((batch_size, self.k - num_nodes, xdim), fill_value)
            batch_h = torch.cat([batch_h, fill_batch], dim=1)
        batch_h[batch_h == fill_value] = 0
        h = batch_h
        h = h.permute(0, 2, 1)
        h = F.relu(self.conv1d(h)).view(batch_size, -1)
        h = F.relu(self.fc1(h))
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = self.fc2(h)
        return h


class NodeAdaptiveEncoder(nn.Module):

    def __init__(self, num_features, dropout=0.5):
        super(NodeAdaptiveEncoder, self).__init__()
        self.fc = nn.Parameter(torch.zeros(size=(num_features, 1)))
        nn.init.xavier_normal_(self.fc.data, gain=1.414)
        self.bf = nn.Parameter(torch.zeros(size=(1,)))
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x):
        h = torch.mm(x, self.fc) + self.bf
        h = torch.sigmoid(h)
        h = self.dropout(h)
        return torch.where(x < 0, torch.zeros_like(x), x) + h * torch.where(x > 0, torch.zeros_like(x), x)


def add_remaining_self_loops(edge_index, edge_weight=None, fill_value=1, num_nodes=None):
    device = edge_index[0].device
    row, col = edge_index[0], edge_index[1]
    if edge_weight is None:
        edge_weight = torch.ones(row.shape[0], device=device)
    if num_nodes is None:
        num_nodes = max(row.max().item(), col.max().item()) + 1
    if fill_value is None:
        fill_value = 1
    N = num_nodes
    mask = row != col
    loop_index = torch.arange(0, N, dtype=row.dtype, device=row.device)
    loop_index = loop_index.unsqueeze(0).repeat(2, 1)
    _row = torch.cat([row[mask], loop_index[0]])
    _col = torch.cat([col[mask], loop_index[1]])
    inv_mask = ~mask
    loop_weight = torch.full((N,), fill_value, dtype=edge_weight.dtype, device=edge_weight.device)
    remaining_edge_weight = edge_weight[inv_mask]
    if remaining_edge_weight.numel() > 0:
        loop_weight[row[inv_mask]] = remaining_edge_weight
    edge_weight = torch.cat([edge_weight[mask], loop_weight], dim=0)
    return (_row, _col), edge_weight


class SrgcnHead(nn.Module):

    def __init__(self, num_features, out_feats, attention, activation, normalization, nhop, subheads=2, dropout=0.5, node_dropout=0.5, alpha=0.2, concat=True):
        super(SrgcnHead, self).__init__()
        self.subheads = subheads
        self.concat = concat
        self.alpha = alpha
        self.nhop = nhop
        self.attention = attention(out_feats)
        self.activation = activation
        self.normalization = normalization()
        self.adaptive_enc = nn.ModuleList()
        self.weight = nn.ParameterList()
        self.bias = nn.ParameterList()
        for r in range(self.subheads):
            W = nn.Parameter(torch.zeros(size=(num_features, out_feats)))
            nn.init.xavier_normal_(W.data, gain=1.414)
            self.weight.append(W)
            self.bias.append(nn.Parameter(torch.zeros(size=(out_feats,))))
            self.adaptive_enc.append(NodeAdaptiveEncoder(out_feats, dropout))
        self.dropout = dropout
        self.node_dropout = node_dropout

    def forward(self, graph, x):
        edge_index = graph.edge_index
        N, dim = x.shape
        nl_adj_mat_ind = add_remaining_self_loops(edge_index, num_nodes=N)[0]
        nl_adj_mat_ind = torch.stack(nl_adj_mat_ind)
        nl_adj_mat_val = torch.ones(nl_adj_mat_ind.shape[1])
        for _ in range(self.nhop - 1):
            nl_adj_mat_ind, nl_adj_mat_val = spspmm(nl_adj_mat_ind, nl_adj_mat_val, nl_adj_mat_ind, nl_adj_mat_val, N, N, N, True)
        result = []
        for i in range(self.subheads):
            h = torch.mm(x, self.weight[i])
            adj_mat_ind, adj_mat_val = nl_adj_mat_ind, nl_adj_mat_val
            h = F.dropout(h, p=self.dropout, training=self.training)
            adj_mat_ind, adj_mat_val = self.attention(h, adj_mat_ind, adj_mat_val)
            adj_mat_val = self.normalization(adj_mat_ind, adj_mat_val, N)
            val_h = h
            with graph.local_graph():
                graph.edge_index = adj_mat_ind
                graph.edge_weight = adj_mat_val
                for _ in range(i + 1):
                    val_h = spmm(graph, val_h)
                val_h[val_h != val_h] = 0
                val_h = val_h + self.bias[i]
                val_h = self.adaptive_enc[i](val_h)
                val_h = self.activation(val_h)
                val_h = F.dropout(val_h, p=self.dropout, training=self.training)
                result.append(val_h)
        h_res = torch.cat(result, dim=1)
        return h_res


class SrgcnSoftmaxHead(nn.Module):

    def __init__(self, num_features, out_feats, attention, activation, nhop, normalization, dropout=0.5, node_dropout=0.5, alpha=0.2):
        super(SrgcnSoftmaxHead, self).__init__()
        self.alpha = alpha
        self.activation = activation
        self.nhop = nhop
        self.normalization = normalization()
        self.attention = attention(out_feats)
        self.weight = nn.Parameter(torch.zeros(size=(num_features, out_feats)))
        nn.init.xavier_normal_(self.weight.data, gain=1.414)
        self.bias = nn.Parameter(torch.zeros(size=(out_feats,)))
        self.adaptive_enc = NodeAdaptiveEncoder(out_feats, dropout)
        self.dropout = dropout
        self.node_dropout = node_dropout

    def forward(self, graph, x):
        N, dim = x.shape
        edge_index = graph.edge_index
        adj_mat_ind = add_remaining_self_loops(edge_index, num_nodes=N)[0]
        adj_mat_ind = torch.stack(adj_mat_ind)
        adj_mat_val = torch.ones(adj_mat_ind.shape[1])
        h = torch.mm(x, self.weight)
        h = F.dropout(h, p=self.dropout, training=self.training)
        for _ in range(self.nhop - 1):
            adj_mat_ind, adj_mat_val = spspmm(adj_mat_ind, adj_mat_val, adj_mat_ind, adj_mat_val, N, N, N, True)
        adj_mat_ind, adj_mat_val = self.attention(h, adj_mat_ind, adj_mat_val)
        adj_mat_val = self.normalization(adj_mat_ind, adj_mat_val, N)
        val_h = h
        with graph.local_graph():
            graph.edge_index = adj_mat_ind
            graph.edge_weight = adj_mat_val
            val_h = spmm(graph, val_h)
        val_h[val_h != val_h] = 0
        val_h = val_h + self.bias
        val_h = self.adaptive_enc(val_h)
        val_h = F.dropout(val_h, p=self.dropout, training=self.training)
        return val_h


class EdgeAttention(nn.Module):

    def __init__(self, in_feat):
        super(EdgeAttention, self).__init__()
        self.p = nn.Linear(in_feat, 1)
        self.q = nn.Linear(in_feat, 1)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x, edge_index, edge_attr):
        N, dim = x.shape
        row, col = edge_index
        deg = get_degrees(row, col, N)
        deg_inv_sqrt = deg.pow(-0.5)
        edge_attr_t = deg_inv_sqrt[row] * edge_attr * deg_inv_sqrt[col]
        p_val = F.relu(self.p(x))
        q_val = F.relu(self.q(x))
        p_val = self.dropout(p_val)
        q_val = self.dropout(q_val)
        p_adj_mat_val = edge_attr_t * p_val.view(-1)[edge_index[1]]
        q_adj_mat_val = edge_attr_t * q_val.view(-1)[edge_index[0]]
        return edge_index, p_adj_mat_val + q_adj_mat_val


class Gaussian(nn.Module):

    def __init__(self, in_feat):
        super(Gaussian, self).__init__()
        self.mu = 0.2
        self.theta = 1.0
        self.steps = 4

    def forward(self, x, edge_index, edge_attr):
        N = x.shape[0]
        row, col = edge_index
        deg = get_degrees(row, col, N)
        deg_inv = deg.pow(-1)
        adj = torch.sparse_coo_tensor(edge_index, deg_inv[row] * edge_attr, size=(N, N))
        identity = torch.sparse_coo_tensor([range(N)] * 2, torch.ones(N), size=(N, N))
        laplacian = identity - adj
        t0 = identity
        t1 = laplacian - self.mu * identity
        t1 = t1.mm(t1.to_dense()).to_sparse()
        l_x = -0.5 * (t1 - identity)
        ivs = [iv(i, self.theta) for i in range(self.steps)]
        ivs[1:] = [((-1) ** i * 2 * x) for i, x in enumerate(ivs[1:])]
        ivs = torch.tensor(ivs)
        result = [t0, l_x]
        for i in range(2, self.steps):
            result.append(2 * l_x.mm(result[i - 1].to_dense()).to_sparse().sub(result[i - 2]))
        result = [(result[i] * ivs[i]) for i in range(self.steps)]

        def fn(x, y):
            return x.add(y)
        res = reduce(fn, result)
        return res._indices(), res._values()


class HeatKernel(nn.Module):

    def __init__(self, in_feat):
        super(HeatKernel, self).__init__()
        self.t = nn.Parameter(torch.zeros(1))

    def forward(self, x, edge_index, edge_attr):
        row, col = edge_index
        deg = get_degrees(row, col, x.shape[0])
        deg_inv = deg.pow(-1)
        edge_attr_t = self.t * edge_attr * deg_inv[col] - self.t
        return edge_index, edge_attr_t.exp()


class Identity(nn.Module):

    def __init__(self, in_feat):
        super(Identity, self).__init__()

    def forward(self, x, edge_index, edge_attr):
        return edge_index, edge_attr


class NodeAttention(nn.Module):

    def __init__(self, in_feat):
        super(NodeAttention, self).__init__()
        self.p = nn.Linear(in_feat, 1)
        self.dropout = nn.Dropout(0.7)

    def forward(self, x, edge_index, edge_attr):
        device = x.device
        N, dim = x.shape
        diag_val = self.p(x)
        diag_val = torch.sigmoid(diag_val)
        self.dropout(diag_val)
        row, col = edge_index
        deg = get_degrees(row, col, N)
        deg_inv = deg.pow(-1)
        edge_attr_t = deg_inv[row] * edge_attr
        diag_ind = torch.LongTensor([range(N)] * 2)
        _, adj_mat_val = spspmm(edge_index, edge_attr_t, diag_ind, diag_val.view(-1), N, N, N, True)
        return edge_index, adj_mat_val


class PPR(nn.Module):

    def __init__(self, in_feat):
        super(PPR, self).__init__()
        self.alpha = 0.4
        self.steps = 4

    def forward(self, x, edge_index, edge_attr):
        row, col = edge_index
        deg = get_degrees(row, col, x.shape[0])
        deg_inv_sqrt = deg.pow(-0.5)
        edge_attr_t = deg_inv_sqrt[row] * edge_attr * deg_inv_sqrt[col]
        N = x.size(0)
        adj = torch.sparse_coo_tensor(edge_index, edge_attr_t, size=(N, N))
        theta = self.alpha * (1 - self.alpha)
        result = [theta * adj]
        for i in range(1, self.steps - 1):
            theta = theta * (1 - self.alpha)
            adj_ind, adj_val = spspmm(edge_index, edge_attr_t, result[i - 1]._indices(), result[i - 1]._values(), N, N, N, True)
            result.append(torch.sparse_coo_tensor(adj_ind, adj_val, size=(N, N)))
        identity = torch.sparse_coo_tensor([range(N)] * 2, torch.ones(N), size=(N, N))
        result.append(self.alpha * identity)

        def fn(x, y):
            return x.add(y)
        res = reduce(fn, result)
        return res._indices(), res._values()


def act_attention(attn_type):
    if attn_type == 'identity':
        return Identity
    elif attn_type == 'node':
        return NodeAttention
    elif attn_type == 'edge':
        return EdgeAttention
    elif attn_type == 'ppr':
        return PPR
    elif attn_type == 'heat':
        return HeatKernel
    elif attn_type == 'gaussian':
        return Gaussian
    else:
        raise ValueError('no such attention type')


def act_map(act):
    if act == 'linear':
        return lambda x: x
    elif act == 'elu':
        return torch.nn.functional.elu
    elif act == 'sigmoid':
        return torch.sigmoid
    elif act == 'tanh':
        return torch.tanh
    elif act == 'relu':
        return torch.nn.functional.relu
    elif act == 'relu6':
        return torch.nn.functional.relu6
    elif act == 'softplus':
        return torch.nn.functional.softplus
    elif act == 'leaky_relu':
        return torch.nn.functional.leaky_relu
    else:
        raise Exception('wrong activate function')


class ColumnUniform(nn.Module):

    def __init__(self):
        super(ColumnUniform, self).__init__()

    def forward(self, edge_index, edge_attr, N):
        device = edge_attr.device
        ones = torch.ones(N, 1, device=device)
        rownorm = 1.0 / spmm(edge_index, edge_attr, N, N, ones).view(-1)
        col = rownorm[edge_index[1]]
        edge_attr_t = col * edge_attr
        return edge_attr_t


class NormIdentity(nn.Module):

    def __init__(self):
        super(NormIdentity, self).__init__()

    def forward(self, edge_index, edge_attr, N):
        return edge_attr


class RowSoftmax(nn.Module):

    def __init__(self):
        super(RowSoftmax, self).__init__()

    def forward(self, edge_index, edge_attr, N):
        device = edge_attr.device
        edge_attr_t = torch.exp(edge_attr)
        ones = torch.ones(N, 1, device=device)
        rownorm = 1.0 / spmm(edge_index, edge_attr_t, N, N, ones).view(-1)
        row = rownorm[edge_index[0]]
        edge_attr_t = row * edge_attr_t
        return edge_attr_t


class RowUniform(nn.Module):

    def __init__(self):
        super(RowUniform, self).__init__()

    def forward(self, edge_index, edge_attr, N):
        device = edge_attr.device
        ones = torch.ones(N, 1, device=device)
        rownorm = 1.0 / spmm(edge_index, edge_attr, N, N, ones).view(-1)
        row = rownorm[edge_index[0]]
        edge_attr_t = row * edge_attr
        return edge_attr_t


class SymmetryNorm(nn.Module):

    def __init__(self):
        super(SymmetryNorm, self).__init__()

    def forward(self, edge_index, edge_attr, N):
        device = edge_attr.device
        ones = torch.ones(N, 1, device=device)
        rownorm = spmm(edge_index, edge_attr, N, N, ones).view(-1).pow(-0.5)
        row = rownorm[edge_index[0]]
        col = rownorm[edge_index[1]]
        edge_attr_t = row * edge_attr * col
        return edge_attr_t


def act_normalization(norm_type):
    if norm_type == 'identity':
        return NormIdentity
    elif norm_type == 'row_uniform':
        return RowUniform
    elif norm_type == 'row_softmax':
        return RowSoftmax
    elif norm_type == 'col_uniform':
        return ColumnUniform
    elif norm_type == 'symmetry':
        return SymmetryNorm
    else:
        raise ValueError('no such normalization type')


class SRGCN(BaseModel):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=8)
        parser.add_argument('--num-heads', type=int, default=8)
        parser.add_argument('--dropout', type=float, default=0.5)
        parser.add_argument('--node-dropout', type=float, default=0.5)
        parser.add_argument('--alpha', type=float, default=0.2)
        parser.add_argument('--lr', type=float, default=0.005)
        parser.add_argument('--subheads', type=int, default=1)
        parser.add_argument('--attention-type', type=str, default='node')
        parser.add_argument('--activation', type=str, default='leaky_relu')
        parser.add_argument('--nhop', type=int, default=1)
        parser.add_argument('--normalization', type=str, default='row_uniform')

    @classmethod
    def build_model_from_args(cls, args):
        return cls(in_feats=args.num_features, hidden_size=args.hidden_size, out_feats=args.num_classes, dropout=args.dropout, node_dropout=args.node_dropout, nhead=args.num_heads, subheads=args.subheads, alpha=args.alpha, attention=args.attention_type, activation=args.activation, nhop=args.nhop, normalization=args.normalization)

    def __init__(self, in_feats, hidden_size, out_feats, attention, activation, nhop, normalization, dropout, node_dropout, alpha, nhead, subheads):
        super(SRGCN, self).__init__()
        attn_f = act_attention(attention)
        activate_f = act_map(activation)
        norm_f = act_normalization(normalization)
        self.attentions = [SrgcnHead(num_features=in_feats, out_feats=hidden_size, attention=attn_f, activation=activate_f, nhop=nhop, normalization=norm_f, subheads=subheads, dropout=dropout, node_dropout=node_dropout, alpha=alpha, concat=True) for _ in range(nhead)]
        for i, attention in enumerate(self.attentions):
            self.add_module('attention_{}'.format(i), attention)
        self.out_att = SrgcnSoftmaxHead(num_features=hidden_size * nhead * subheads, out_feats=out_feats, attention=attn_f, activation=activate_f, normalization=act_normalization('row_softmax'), nhop=nhop, dropout=dropout, node_dropout=node_dropout)

    def forward(self, graph):
        x = torch.cat([att(graph, graph.x) for att in self.attentions], dim=1)
        x = F.elu(x)
        x = self.out_att(graph, x)
        return x

    def predict(self, data):
        return self.forward(data)


@torch.jit.script
def f_gelu(x):
    pdtype = x.dtype
    x = x.float()
    y = x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
    return y


def gelu(x):
    """Implementation of the gelu activation function.
    For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):
    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
    Also see https://arxiv.org/abs/1606.08415
    """
    return f_gelu(x)


def swish(x):
    return x * torch.sigmoid(x)


ACT2FN = {'gelu': gelu, 'relu': torch.nn.functional.relu, 'swish': swish}


@torch.jit.script
def bias_gelu(bias, y):
    x = bias + y
    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))


@torch.jit.script
def bias_tanh(bias, y):
    x = bias + y
    return torch.tanh(x)


class LinearActivation(Module):
    """Fused Linear and activation Module."""
    __constants__ = ['bias']

    def __init__(self, in_features, out_features, act='gelu', bias=True):
        super(LinearActivation, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.fused_gelu = False
        self.fused_tanh = False
        if isinstance(act, str):
            if bias and act == 'gelu':
                self.fused_gelu = True
            elif bias and act == 'tanh':
                self.fused_tanh = True
            else:
                self.act_fn = ACT2FN[act]
        else:
            self.act_fn = act
        self.weight = Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            init.uniform_(self.bias, -bound, bound)

    def forward(self, input):
        if self.fused_gelu:
            return bias_gelu(self.bias, F.linear(input, self.weight, None))
        elif self.fused_tanh:
            return bias_tanh(self.bias, F.linear(input, self.weight, None))
        else:
            return self.act_fn(F.linear(input, self.weight, self.bias))

    def extra_repr(self):
        return 'in_features={}, out_features={}, bias={}'.format(self.in_features, self.out_features, self.bias is not None)


class BertLayerNorm(nn.Module):

    def __init__(self, hidden_size, eps=1e-12):
        """Construct a layernorm module in the TF style (epsilon inside the square root)."""
        super(BertLayerNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.bias = nn.Parameter(torch.zeros(hidden_size))
        self.variance_epsilon = eps

    def forward(self, x):
        pdtype = x.dtype
        x = x.float()
        u = x.mean(-1, keepdim=True)
        s = (x - u).pow(2).mean(-1, keepdim=True)
        x = (x - u) / torch.sqrt(s + self.variance_epsilon)
        return self.weight * x + self.bias


class BertEmbeddings(nn.Module):
    """Construct the embeddings from word, position and token_type embeddings."""

    def __init__(self, config):
        super(BertEmbeddings, self).__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, token_type_ids=None):
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        embeddings = words_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class BertSelfAttention(nn.Module):

    def __init__(self, config):
        super(BertSelfAttention, self).__init__()
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.hidden_size, config.num_attention_heads))
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.softmax = nn.Softmax(dim=-1)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def transpose_key_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 3, 1)

    def forward(self, hidden_states, attention_mask):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_key_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        attention_scores = torch.matmul(query_layer, key_layer)
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        attention_scores = attention_scores + attention_mask
        attention_probs = self.softmax(attention_scores)
        attention_probs = self.dropout(attention_probs)
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)
        return context_layer


class BertSelfOutput(nn.Module):

    def __init__(self, config):
        super(BertSelfOutput, self).__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dense.bert_output_layer = True
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states


class BertAttention(nn.Module):

    def __init__(self, config):
        super(BertAttention, self).__init__()
        self.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)

    def forward(self, input_tensor, attention_mask):
        self_output = self.self(input_tensor, attention_mask)
        attention_output = self.output(self_output, input_tensor)
        return attention_output


class BertIntermediate(nn.Module):

    def __init__(self, config):
        super(BertIntermediate, self).__init__()
        self.dense_act = LinearActivation(config.hidden_size, config.intermediate_size, act=config.hidden_act)

    def forward(self, hidden_states):
        hidden_states = self.dense_act(hidden_states)
        return hidden_states


class BertOutput(nn.Module):

    def __init__(self, config):
        super(BertOutput, self).__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.dense.bert_output_layer = True
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states


class BertLayer(nn.Module):

    def __init__(self, config):
        super(BertLayer, self).__init__()
        self.attention = BertAttention(config)
        self.PreAttentionLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        self.PostAttentionLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

    def forward(self, hidden_states, attention_mask):
        input_layer_norm = self.PreAttentionLayerNorm(hidden_states)
        attention_output = self.attention(input_layer_norm, attention_mask)
        intermediate_input = hidden_states + attention_output
        intermediate_layer_norm = self.PostAttentionLayerNorm(intermediate_input)
        intermediate_output = self.intermediate(intermediate_layer_norm)
        layer_output = self.output(intermediate_output)
        return layer_output + intermediate_input


class BertEncoder(nn.Module):

    def __init__(self, config):
        super(BertEncoder, self).__init__()
        self.FinalLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        layer = BertLayer(config)
        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])

    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, checkpoint_activations=False):
        all_encoder_layers = []

        def custom(start, end):

            def custom_forward(*inputs):
                layers = self.layer[start:end]
                x_ = inputs[0]
                for layer in layers:
                    x_ = layer(x_, inputs[1])
                return x_
            return custom_forward
        if checkpoint_activations:
            l = 0
            num_layers = len(self.layer)
            chunk_length = math.ceil(math.sqrt(num_layers))
            while l < num_layers:
                hidden_states = checkpoint.checkpoint(custom(l, l + chunk_length), hidden_states, attention_mask * 1)
                l += chunk_length
        else:
            for i, layer_module in enumerate(self.layer):
                hidden_states = layer_module(hidden_states, attention_mask)
                if output_all_encoded_layers:
                    all_encoder_layers.append(hidden_states)
        if not output_all_encoded_layers or checkpoint_activations:
            hidden_states = self.FinalLayerNorm(hidden_states)
            all_encoder_layers.append(hidden_states)
        return all_encoder_layers


class BertPooler(nn.Module):

    def __init__(self, config):
        super(BertPooler, self).__init__()
        self.dense_act = LinearActivation(config.hidden_size, config.hidden_size, act='tanh')

    def forward(self, hidden_states):
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense_act(first_token_tensor)
        return pooled_output


class BertPredictionHeadTransform(nn.Module):

    def __init__(self, config):
        super(BertPredictionHeadTransform, self).__init__()
        self.dense_act = LinearActivation(config.hidden_size, config.hidden_size, act=config.hidden_act)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)

    def forward(self, hidden_states):
        hidden_states = self.dense_act(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        return hidden_states


class BertLMPredictionHead(nn.Module):

    def __init__(self, config, bert_model_embedding_weights):
        super(BertLMPredictionHead, self).__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.decoder = nn.Linear(bert_model_embedding_weights.size(1), bert_model_embedding_weights.size(0), bias=False)
        self.decoder.weight = bert_model_embedding_weights
        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))

    def forward(self, hidden_states, masked_token_indexes):
        hidden_states = self.transform(hidden_states)
        if masked_token_indexes is not None:
            hidden_states = torch.index_select(hidden_states.view(-1, hidden_states.shape[-1]), 0, masked_token_indexes)
        hidden_states = self.decoder(hidden_states) + self.bias
        return hidden_states


class BertPreTrainingHeads(nn.Module):

    def __init__(self, config, bert_model_embedding_weights):
        super(BertPreTrainingHeads, self).__init__()
        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, sequence_output, pooled_output, masked_token_indexes=None):
        prediction_scores = self.predictions(sequence_output, masked_token_indexes)
        seq_relationship_score = self.seq_relationship(pooled_output)
        return prediction_scores, seq_relationship_score


class BertConfig(object):
    """Configuration class to store the configuration of a `BertModel`."""

    def __init__(self):
        pass

    @classmethod
    def from_dict(cls, json_object):
        """Constructs a `BertConfig` from a Python dictionary of parameters."""
        config = cls()
        for key, value in json_object.items():
            config.__dict__[key] = value
        return config

    def __repr__(self):
        return str(self.to_json_string())

    def to_dict(self):
        """Serializes this instance to a Python dictionary."""
        output = copy.deepcopy(self.__dict__)
        return output

    def to_json_string(self):
        """Serializes this instance to a JSON string."""
        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + '\n'


class BertPreTrainedModel(nn.Module):
    """An abstract class to handle weights initialization and
    a simple interface for dowloading and loading pretrained models.
    """

    def __init__(self, config, *inputs, **kwargs):
        super(BertPreTrainedModel, self).__init__()
        if not isinstance(config, BertConfig):
            raise ValueError('Parameter config in `{}(config)` should be an instance of class `BertConfig`. To create a model from a Google pretrained model use `model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`'.format(self.__class__.__name__, self.__class__.__name__))
        self.config = config

    def init_bert_weights(self, module):
        """Initialize the weights."""
        if isinstance(module, (nn.Linear, nn.Embedding)):
            num_layers = self.config.num_hidden_layers
            std = self.config.initializer_range
            if hasattr(module, 'bert_output_layer'):
                std = self.config.initializer_range / math.sqrt(2.0 * num_layers)
            module.weight.data.normal_(mean=0.0, std=std)
        elif isinstance(module, BertLayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()


logger = logging.getLogger(__name__)


class BertModel(BertPreTrainedModel):
    """BERT model ("Bidirectional Embedding Representations from a Transformer").

    Params:
        config: a BertConfig class instance with the configuration to build a new model

    Inputs:
        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]
            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts
            `extract_features.py`, `run_classifier.py` and `run_squad.py`)
        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token
            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to
            a `sentence B` token (see BERT paper for more details).
        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices
            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max
            input sequence length in the current batch. It's the mask that we typically use for attention when
            a batch has varying length sentences.
        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.

    Outputs: Tuple of (encoded_layers, pooled_output)
        `encoded_layers`: controled by `output_all_encoded_layers` argument:
            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end
                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each
                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],
            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding
                to the last attention block of shape [batch_size, sequence_length, hidden_size],
        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a
            classifier pretrained on top of the hidden state associated to the first character of the
            input (`CLS`) to train on the Next-Sentence task (see BERT's paper).

    Example usage:
    ```python
    # Already been converted into WordPiece token ids
    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])
    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])
    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])

    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)

    model = modeling.BertModel(config=config)
    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)
    ```
    """

    def __init__(self, config):
        super(BertModel, self).__init__(config)
        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)
        self.apply(self.init_bert_weights)
        logger.info('Init BERT pretrain model')

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, checkpoint_activations=False):
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        extended_attention_mask = extended_attention_mask
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        embedding_output = self.embeddings(input_ids, token_type_ids)
        encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=output_all_encoded_layers, checkpoint_activations=checkpoint_activations)
        sequence_output = encoded_layers[-1]
        pooled_output = self.pooler(sequence_output)
        if not output_all_encoded_layers:
            encoded_layers = encoded_layers[-1]
        return encoded_layers, pooled_output


class BertForPreTrainingPreLN(BertPreTrainedModel):
    """BERT model with pre-training heads.
    Params:
        config: a BertConfig class instance with the configuration to build a new model.

    """

    def __init__(self, config):
        super(BertForPreTrainingPreLN, self).__init__(config)
        self.bert = BertModel(config)
        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)
        self.apply(self.init_bert_weights)


class DualPositionBertEmbeddings(nn.Module):
    """Construct the embeddings from word, position and token_type embeddings."""

    def __init__(self, config):
        super(DualPositionBertEmbeddings, self).__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.position_embeddings_second = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, input_ids, token_type_ids, position_ids, position_ids_second):
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        words_embeddings = self.word_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        position_embeddings_second = self.position_embeddings(position_ids_second)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        embeddings = words_embeddings + position_embeddings + position_embeddings_second + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class DualPositionBertModel(BertModel):

    def __init__(self, config):
        super(DualPositionBertModel, self).__init__(config)
        self.embeddings = DualPositionBertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)
        self.apply(self.init_bert_weights)
        logger.info('Init BERT pretrain model')

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, checkpoint_activations=False, position_ids=None, position_ids_second=None):
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        if len(attention_mask.shape) == 2:
            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
        elif len(attention_mask.shape) == 3:
            extended_attention_mask = attention_mask.unsqueeze(1)
        else:
            raise Exception('invalid attention mask shape! shape: %s' % attention_mask.shape)
        extended_attention_mask = extended_attention_mask
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0
        embedding_output = self.embeddings(input_ids, token_type_ids, position_ids, position_ids_second)
        encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=output_all_encoded_layers, checkpoint_activations=checkpoint_activations)
        sequence_output = encoded_layers[-1]
        pooled_output = self.pooler(sequence_output)
        if not output_all_encoded_layers:
            encoded_layers = encoded_layers[-1]
        return encoded_layers, pooled_output


class DualPositionBertForPreTrainingPreLN(BertPreTrainedModel):
    """BERT model with pre-training heads and dual position
    Params:
        config: a BertConfig class instance with the configuration to build a new model.

    """

    def __init__(self, config):
        super(DualPositionBertForPreTrainingPreLN, self).__init__(config)
        self.bert = DualPositionBertModel(config)
        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)
        self.apply(self.init_bert_weights)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, position_ids=None, position_ids_second=None, log=True):
        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_all_encoded_layers=False, checkpoint_activations=False, position_ids=position_ids, position_ids_second=position_ids_second)
        if masked_lm_labels is not None:
            masked_token_indexes = torch.nonzero((masked_lm_labels + 1).view(-1)).view(-1)
            prediction_scores, _ = self.cls(sequence_output, pooled_output, masked_token_indexes)
            target = torch.index_select(masked_lm_labels.view(-1), 0, masked_token_indexes)
            loss_fct = CrossEntropyLoss(ignore_index=-1)
            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), target)
            return masked_lm_loss
        else:
            prediction_scores, _ = self.cls(sequence_output, pooled_output)
            return prediction_scores


OAG_TOKEN_TYPE_NAMES = ['TEXT', 'AUTHOR', 'VENUE', 'AFF', 'FOS', 'FUND']


COLORCODES = {'black': '\x1b[30m', 'red': '\x1b[31m', 'green': '\x1b[32m', 'yellow': '\x1b[33m', 'blue': '\x1b[34m', 'magenta': '\x1b[35m', 'cyan': '\x1b[36m', 'white': '\x1b[37m', 'reset': '\x1b[0m'}


def colored(text, color):
    return COLORCODES.get(color, '') + text + COLORCODES.get('reset', '')


def stringLenCJK(string):
    return sum(1 + (unicodedata.east_asian_width(c) in 'WF') for c in string)


def stringRjustCJK(string, length):
    return ' ' * (length - stringLenCJK(string)) + string


class OAGMetaInfoBertModel(DualPositionBertForPreTrainingPreLN):

    def __init__(self, bert_config, tokenizer):
        super(OAGMetaInfoBertModel, self).__init__(bert_config)
        self.tokenizer = tokenizer
        self.spm = not isinstance(self.tokenizer, BertTokenizer)
        if self.spm:
            self.tokenizer.cls_token_id, self.tokenizer.mask_token_id, self.tokenizer.sep_token_id = self.tokenizer.PieceToId(['[CLS]', '[MASK]', '[SEP]'])

    def __recursively_build_spm_token_ids(self, text, splitters=[]):
        """
        SentencePiece tokenizer cannot directly decode control symbols such as [MASK] or [SEP]. This function will handle this problem.
        """
        if len(splitters) == 0:
            return self.tokenizer.encode(text)
        splitter = splitters[0]
        splitters = splitters[1:]
        start = 0
        parts = []
        while start >= 0 and start < len(text):
            end = text.find(splitter, start)
            if end >= 0:
                parts += self.__recursively_build_spm_token_ids(text[start:end].strip(), splitters)
                start = end + len(splitter)
                parts.append(self.tokenizer.PieceToId(splitter))
            else:
                end = len(text)
                parts += self.__recursively_build_spm_token_ids(text[start:end].strip(), splitters)
                break
        return parts

    def _convert_text_to_token_ids(self, text):
        if self.spm:
            return self.__recursively_build_spm_token_ids(text, splitters=['[PAD]', '[EOS]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '[BOS]'])
        else:
            return self.tokenizer(text, add_special_tokens=False)['input_ids'] if len(text) > 0 else []

    def _convert_ids_to_tokens(self, token_ids):
        if self.spm:
            _ids = []
            for _id in token_ids:
                if not isinstance(_id, int):
                    _id = _id.item()
                _ids.append(_id)
            return self.tokenizer.id_to_piece(_ids)
        else:
            return self.tokenizer.convert_ids_to_tokens(token_ids)

    def _convert_token_ids_to_text(self, token_ids):
        if self.spm:
            _ids = []
            for _id in token_ids:
                if not isinstance(_id, int):
                    _id = _id.item()
                _ids.append(_id)
            return self.tokenizer.decode(_ids)
        else:
            return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(token_ids))

    def print_oag_instance(self, input_ids, token_type_ids, input_masks, masked_lm_labels, position_ids, position_ids_second, predictions=None):
        COLORS = ['white', 'green', 'blue', 'red', 'yellow', 'magenta']
        try:
            termwidth, _ = os.get_terminal_size()
        except Exception:
            termwidth = 200
        K = predictions.shape[1] if predictions is not None else 0
        input_ids = [token_id for i, token_id in enumerate(input_ids) if input_masks[i].sum() > 0]
        position_ids = [position_id for i, position_id in enumerate(position_ids) if input_masks[i].sum() > 0]
        position_ids_second = [position_id for i, position_id in enumerate(position_ids_second) if input_masks[i].sum() > 0]
        token_type_ids = [token_type_id for i, token_type_id in enumerate(token_type_ids) if input_masks[i].sum() > 0]
        masks = [(0) for i in input_ids]
        prediction_topks = [[(0) for i in input_ids] for _ in range(K)]
        mask_indices = []
        for lm_pos, lm_id in enumerate(masked_lm_labels):
            if lm_id < 0:
                continue
            masks[lm_pos] = lm_id
            mask_indices.append(lm_pos)
        for k in range(K):
            for lm_pos, token_id in zip(mask_indices, predictions[:, k]):
                prediction_topks[k][lm_pos] = token_id
        input_tokens = self._convert_ids_to_tokens(input_ids)
        masks_tokens = self._convert_ids_to_tokens(masks)
        prediction_tokens = [self._convert_ids_to_tokens(prediction_topks[k]) for k in range(K)]
        input_tokens_str = ['']
        position_ids_str = ['']
        position_ids_second_str = ['']
        token_type_ids_str = ['']
        masks_str = ['']
        prediction_topk_strs = [[''] for _ in range(K)]
        current_length = 0
        for pos, (input_token, position_id, position_id_second, token_type_id, mask) in enumerate(zip(input_tokens, position_ids, position_ids_second, token_type_ids, masks_tokens)):
            token_type = OAG_TOKEN_TYPE_NAMES[token_type_id]
            length = max(stringLenCJK(input_token) + 1, 7, stringLenCJK(token_type) + 1, stringLenCJK(mask) + 1, *[(stringLenCJK(prediction_tokens[k][pos]) + 1) for k in range(K)])
            if current_length + length > termwidth:
                current_length = 0
                input_tokens_str.append('')
                position_ids_str.append('')
                position_ids_second_str.append('')
                token_type_ids_str.append('')
                masks_str.append('')
                for k in range(K):
                    prediction_topk_strs[k].append('')
            current_length += length
            input_tokens_str[-1] += colored(stringRjustCJK(input_token, length), COLORS[token_type_id])
            position_ids_str[-1] += stringRjustCJK(str(position_id), length)
            position_ids_second_str[-1] += stringRjustCJK(str(position_id_second), length)
            token_type_ids_str[-1] += stringRjustCJK(token_type, length)
            masks_str[-1] += colored(stringRjustCJK(mask, length) if mask != '[PAD]' else stringRjustCJK('', length), COLORS[token_type_id])
            for k in range(K):
                v = prediction_tokens[k][pos] if prediction_tokens[k][pos] != '[PAD]' else ''
                prediction_topk_strs[k][-1] += colored(stringRjustCJK(v, length), 'magenta' if v != mask and mask != '[CLS]' else 'cyan')
        ansi_escape = re.compile('\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')
        size = len(masks_str)
        for i in range(size):
            None
            None
            None
            None
            None
            if ansi_escape.sub('', masks_str[i]).strip() != '':
                None
            for k in range(K):
                if ansi_escape.sub('', prediction_topk_strs[k][i]).strip() != '':
                    None
            None

    def build_inputs(self, title='', abstract='', venue='', authors=[], concepts=[], affiliations=[], decode_span_type='FOS', decode_span_length=0, max_seq_length=512, mask_propmt_text=''):
        """build inputs from text information for model to use

        Args:
            title (str, optional): [paper title]. Defaults to ''.
            abstract (str, optional): [paper abstract]. Defaults to ''.
            venue (str, optional): [paper venue]. Defaults to ''.
            authors (list, optional): [paper author]. Defaults to [].
            concepts (list, optional): [paper concepts]. Defaults to [].
            affiliations (list, optional): [paper affiliations]. Defaults to [].
            decode_span_type (str, optional): [the span type to decode, choose from 'FOS','VENUE','AFF','AUTHOR']. Defaults to 'FOS'.
            decode_span_length (int, optional): [the length of span to decode]. Defaults to 0.
            max_seq_length (int, optional): [maximum sequence length for the input, the context information will be truncated if the total length exceeds this number]. Defaults to 512.
            mask_propmt_text (str, optional): [the prompt text to add after title and abstract]. Defaults to ''.

        Raises:
            Exception: [provided inputs are invalid]

        Returns:
            [tuple of list]: [input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans]
        """
        input_ids = []
        input_masks = []
        token_type_ids = []
        masked_lm_labels = []
        position_ids = []
        position_ids_second = []
        masked_positions = []
        num_spans = 0

        def add_span(token_type_id, token_ids, is_mask=False):
            nonlocal num_spans
            if len(token_ids) == 0:
                return
            length = len(token_ids)
            input_ids.extend(token_ids if not is_mask else [self.tokenizer.mask_token_id] * length)
            input_masks.extend([1] * length)
            token_type_ids.extend([token_type_id] * length)
            masked_lm_labels.extend([-1] * length if not is_mask else [self.tokenizer.cls_token_id] * length)
            position_ids.extend([num_spans] * length)
            position_ids_second.extend(list(range(length)))
            if is_mask:
                masked_positions.extend([(len(input_ids) - length + i) for i in range(decode_span_length)])
            num_spans += 1
        span_token_type_id = OAG_TOKEN_TYPE_NAMES.index(decode_span_type)
        if span_token_type_id < 0:
            raise Exception('Unexpected span type: %s' % decode_span_type)
        prompt_token_ids = self._convert_text_to_token_ids(mask_propmt_text)
        add_span(0, (self._convert_text_to_token_ids(title) + self._convert_text_to_token_ids(abstract) + prompt_token_ids)[:max_seq_length - decode_span_length])
        add_span(2, self._convert_text_to_token_ids(venue)[:max_seq_length - len(input_ids) - decode_span_length])
        for author in authors:
            add_span(1, self._convert_text_to_token_ids(author)[:max_seq_length - len(input_ids) - decode_span_length])
        for concept in concepts:
            add_span(4, self._convert_text_to_token_ids(concept)[:max_seq_length - len(input_ids) - decode_span_length])
        for affiliation in affiliations:
            add_span(3, self._convert_text_to_token_ids(affiliation)[:max_seq_length - len(input_ids) - decode_span_length])
        add_span(span_token_type_id, [(0) for _ in range(decode_span_length)], is_mask=True)
        return input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans

    def encode_paper(self, title='', abstract='', venue='', authors=[], concepts=[], affiliations=[], decode_span_type='FOS', decode_span_length=0, max_seq_length=512, mask_propmt_text='', reduction='first'):
        """encode paper from text information and run forward to get sequence and pool output for each entity

        Args:
            title (str, optional): [paper title]. Defaults to ''.
            abstract (str, optional): [paper abstract]. Defaults to ''.
            venue (str, optional): [paper venue]. Defaults to ''.
            authors (list, optional): [paper author]. Defaults to [].
            concepts (list, optional): [paper concepts]. Defaults to [].
            affiliations (list, optional): [paper affiliations]. Defaults to [].
            decode_span_type (str, optional): [the span type to decode, choose from 'FOS','VENUE','AFF','AUTHOR']. Defaults to 'FOS'.
            decode_span_length (int, optional): [the length of span to decode]. Defaults to 0.
            max_seq_length (int, optional): [maximum sequence length for the input, the context information will be truncated if the total length exceeds this number]. Defaults to 512.
            mask_propmt_text (str, optional): [the prompt text to add after title and abstract]. Defaults to ''.
            reduction (str, optional): [the way to get pooled_output, choose from 'cls','max','mean']. Defaults to 'cls'.

        Raises:
            Exception: [provided inputs are invalid]

        Returns:
            [dictionary of list of dictionary]: {
                'text': text_item,
                'venue': venue_item,
                'authors': [authors_item, authors_item, ...]
                'concepts': [concepts_item, concepts_item, ...]
                'affiliations': [affiliations_item, affiliations_item, ...]
                }
        """
        input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = self.build_inputs(title=title, abstract=abstract, venue=venue, authors=authors, concepts=concepts, affiliations=affiliations, decode_span_type=decode_span_type, decode_span_length=decode_span_length, max_seq_length=max_seq_length, mask_propmt_text=mask_propmt_text)
        search = {'text': [title + abstract], 'venue': [venue], 'authors': authors, 'concepts': concepts, 'affiliations': affiliations}
        item = {'originalText': '', 'inputText': '', 'type': '', 'tokens': [], 'token_ids': [], 'sequence_output': [], 'pooled_output': []}
        output = {'text': [], 'venue': [], 'authors': [], 'concepts': [], 'affiliations': []}
        split_index = {'text': [], 'venue': [], 'authors': [], 'concepts': [], 'affiliations': []}
        device = next(self.parameters()).device
        sequence_output, pooled_output = self.bert.forward(input_ids=torch.LongTensor(input_ids).unsqueeze(0), token_type_ids=torch.LongTensor(token_type_ids).unsqueeze(0), attention_mask=torch.LongTensor(input_masks).unsqueeze(0), output_all_encoded_layers=False, checkpoint_activations=False, position_ids=torch.LongTensor(position_ids).unsqueeze(0), position_ids_second=torch.LongTensor(position_ids_second).unsqueeze(0))
        entities = {(0): 'text', (2): 'venue', (1): 'authors', (4): 'concepts', (3): 'affiliations'}
        for num, name in entities.items():
            if num in token_type_ids:
                start_index = position_ids[token_type_ids.index(num)]
                split_index[name].append(position_ids.index(start_index) - 1)
                for i in range(0, len(search[name])):
                    split_index[name].append(len(position_ids) - 1 - list(reversed(position_ids)).index(start_index + i))
                    item = item.copy()
                    item['type'] = name.upper()
                    item['originalText'] = search[name][i]
                    item['token_ids'] = input_ids[split_index[name][i] + 1:split_index[name][i + 1] + 1]
                    item['tokens'] = self._convert_ids_to_tokens(item['token_ids'])
                    item['inputText'] = self._convert_token_ids_to_text(item['token_ids'])
                    item['sequence_output'] = sequence_output[:, split_index[name][i] + 1:split_index[name][i + 1] + 1, :].squeeze(0)
                    if reduction == 'mean':
                        item['pooled_output'] = item['sequence_output'].mean(dim=0, keepdim=False)
                    elif reduction == 'max':
                        item['pooled_output'], _ = item['sequence_output'].max(dim=0)
                    else:
                        item['pooled_output'] = pooled_output
                    output[name].append(item)
        return output

    def calculate_span_prob(self, title='', abstract='', venue='', authors=[], concepts=[], affiliations=[], decode_span_type='FOS', decode_span='', force_forward=False, max_seq_length=512, mask_propmt_text='', device=None, debug=False):
        """calculate span probability by greedy algorithm

        Args:
            title (str, optional): [paper title]. Defaults to ''.
            abstract (str, optional): [paper abstract]. Defaults to ''.
            venue (str, optional): [paper venue]. Defaults to ''.
            authors (list, optional): [paper author]. Defaults to [].
            concepts (list, optional): [paper concepts]. Defaults to [].
            affiliations (list, optional): [paper affiliations]. Defaults to [].
            decode_span_type (str, optional): [the span type to decode, choose from 'FOS','VENUE','AFF','AUTHOR']. Defaults to 'FOS'.
            decode_span_length (int, optional): [the length of span to decode]. Defaults to 0.
            force_forward (bool, optional): [if the decoding order is from left to right]. Defaults to False.
            max_seq_length (int, optional): [maximum sequence length for the input, the context information will be truncated if the total length exceeds this number]. Defaults to 512.
            mask_propmt_text (str, optional): [the prompt text to add after title and abstract]. Defaults to ''.
            device ([type], optional): [device for the inputs, default to cpu]. Defaults to None.
            debug (bool, optional): [if debug is true, instances will print to console]. Defaults to False.

        Returns:
            [tuple[float, list of floats]]: [the probability of the whole span, and the individual probability for each token in the decoded span]
        """
        decode_span_token_ids = self._convert_text_to_token_ids(decode_span)
        decode_span_length = len(decode_span_token_ids)
        input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = self.build_inputs(title=title, abstract=abstract, venue=venue, authors=authors, concepts=concepts, affiliations=affiliations, decode_span_type=decode_span_type, decode_span_length=decode_span_length, max_seq_length=max_seq_length, mask_propmt_text=mask_propmt_text)
        logprobs = 0.0
        logproblist = []

        def tensorize(x):
            return torch.LongTensor(x).unsqueeze(0)
        for i in range(decode_span_length):
            sequence_output, pooled_output = self.bert.forward(input_ids=tensorize(input_ids), token_type_ids=tensorize(token_type_ids), attention_mask=tensorize(input_masks), output_all_encoded_layers=False, checkpoint_activations=False, position_ids=tensorize(position_ids), position_ids_second=tensorize(position_ids_second))
            masked_token_indexes = torch.nonzero((tensorize(masked_lm_labels) + 1).view(-1), as_tuple=False).view(-1)
            prediction_scores, _ = self.cls(sequence_output, pooled_output, masked_token_indexes)
            prediction_scores = torch.nn.functional.log_softmax(prediction_scores, dim=1)
            token_log_probs = prediction_scores[torch.arange(len(decode_span_token_ids)), decode_span_token_ids]
            if force_forward:
                logprob, pos = token_log_probs[0], 0
            else:
                logprob, pos = token_log_probs.max(dim=0)
            logprobs += logprob.item()
            logproblist.append(logprob.item())
            real_pos = masked_positions[pos]
            target_token = decode_span_token_ids[pos]
            input_ids[real_pos] = decode_span_token_ids[pos]
            masked_lm_labels[real_pos] = -1
            masked_positions.pop(pos)
            decode_span_token_ids.pop(pos)
            if debug:
                self.print_oag_instance(input_ids=input_ids, token_type_ids=token_type_ids, input_masks=input_masks, masked_lm_labels=masked_lm_labels, position_ids=position_ids, position_ids_second=position_ids_second, predictions=torch.topk(prediction_scores, k=5, dim=1).indices.cpu().detach().numpy())
                input('logprobs: %.4f, logprob: %.4f, pos: %d, real_pos: %d, token: %s' % (logprobs, logprob, pos.item(), real_pos, self._convert_ids_to_tokens([target_token])[0]))
        return np.exp(logprobs), logproblist

    def decode_beamsearch(self, title='', abstract='', venue='', authors=[], concepts=[], affiliations=[], decode_span_type='', decode_span_length=0, beam_width=16, force_forward=False, max_seq_length=512, mask_propmt_text='', device=None, debug=False):
        """decode span by using beamsearch

        Args:
            title (str, optional): [paper title]. Defaults to ''.
            abstract (str, optional): [paper abstract]. Defaults to ''.
            venue (str, optional): [paper venue]. Defaults to ''.
            authors (list, optional): [paper author]. Defaults to [].
            concepts (list, optional): [paper concepts]. Defaults to [].
            affiliations (list, optional): [paper affiliations]. Defaults to [].
            decode_span_type (str, optional): [the span type to decode, choose from 'FOS','VENUE','AFF','AUTHOR']. Defaults to 'FOS'.
            decode_span_length (int, optional): [the length of span to decode]. Defaults to 0.
            beam_width (int, optional): [beam search width, notice that this function will run one step of beam search in a batch, which should ensure that your gpu (if using) should be able to hold this number of instances]. Defaults to 16.
            force_forward (bool, optional): [if the decoding order is from left to right]. Defaults to False.
            max_seq_length (int, optional): [maximum sequence length for the input, the context information will be truncated if the total length exceeds this number]. Defaults to 512.
            mask_propmt_text (str, optional): [the prompt text to add after title and abstract]. Defaults to ''.
            device ([type], optional): [device for the inputs, default to cpu]. Defaults to None.
            debug (bool, optional): [if debug is true, the beam search progress will be shown]. Defaults to False.

        Returns:
            [list of (string, float)]: [a list of generated spans with their probablities]
        """
        input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = self.build_inputs(title=title, abstract=abstract, venue=venue, authors=authors, concepts=concepts, affiliations=affiliations, decode_span_type=decode_span_type, decode_span_length=decode_span_length, max_seq_length=max_seq_length, mask_propmt_text=mask_propmt_text)
        q = [(input_ids, masked_lm_labels, masked_positions, 0)]

        def tensorize(x):
            return torch.LongTensor(x)
        for i in range(decode_span_length):
            sequence_output, pooled_output = self.bert.forward(input_ids=tensorize([_input_ids for _input_ids, _, _, _ in q]), token_type_ids=tensorize([token_type_ids for _ in q]), attention_mask=tensorize([input_masks for _ in q]), output_all_encoded_layers=False, checkpoint_activations=False, position_ids=tensorize([position_ids for _ in q]), position_ids_second=tensorize([position_ids_second for _ in q]))
            masked_token_indexes = torch.nonzero((tensorize([_masked_lm_labels for _, _masked_lm_labels, _, _ in q]) + 1).view(-1), as_tuple=False).view(-1)
            prediction_scores, _ = self.cls(sequence_output, pooled_output, masked_token_indexes)
            prediction_scores = torch.nn.functional.log_softmax(prediction_scores, dim=1)
            vocab_size = prediction_scores.shape[-1]
            _q = []
            mask_length = decode_span_length - i
            for idx, (_input_ids, _masked_lm_labels, _masked_positions, _last_logprob) in enumerate(q):
                if force_forward:
                    log_probs, indices = torch.topk(prediction_scores[idx * mask_length].view(-1), k=beam_width)
                else:
                    log_probs, indices = torch.topk(prediction_scores[idx * mask_length:(idx + 1) * mask_length].view(-1), k=beam_width)
                for log_prob, index in zip(log_probs.detach().numpy(), indices.detach().numpy()):
                    new_input_ids = _input_ids.copy()
                    new_masked_lm_labels = _masked_lm_labels.copy()
                    new_masked_positions = _masked_positions.copy()
                    fill_id = index % vocab_size
                    rel_fill_pos = index // vocab_size
                    fill_pos = _masked_positions[rel_fill_pos]
                    new_input_ids[fill_pos] = fill_id
                    new_masked_lm_labels[fill_pos] = -1
                    new_masked_positions.pop(rel_fill_pos)
                    _q.append((new_input_ids, new_masked_lm_labels, new_masked_positions, _last_logprob + log_prob))
            _q.sort(key=lambda tup: tup[-1], reverse=True)
            keys = set()
            q.clear()
            for tup in _q:
                key = tuple(tup[0][-decode_span_length:])
                if key not in keys:
                    keys.add(key)
                    q.append(tup)
                    if len(q) >= beam_width:
                        break
            if debug:
                None
                for _input_ids, _masked_lm_labels, _masked_positions, _last_logprob in q:
                    None
        results = []
        for _input_ids, _masked_lm_labels, _masked_positions, _logprob in q:
            generated_entity = self._convert_token_ids_to_text(_input_ids[-decode_span_length:])
            results.append((generated_entity, np.exp(_logprob)))
        return results

    def generate_title(self, abstract='', authors=[], venue='', affiliations=[], concepts=[], num_beams=1, no_repeat_ngram_size=3, num_return_sequences=1, min_length=10, max_length=30, device=None, early_stopping=False, debug=False):
        """generate paper titles given other information

        Args:
            abstract (str, optional): [paper abstract]. Defaults to ''.
            venue (str, optional): [paper venue]. Defaults to ''.
            authors (list, optional): [paper author]. Defaults to [].
            affiliations (list, optional): [paper affiliations]. Defaults to [].
            concepts (list, optional): [paper concepts]. Defaults to [].
            num_beams (int, optional): [beam search width, notice that this function will run one step of beam search in a batch, which should ensure that your gpu (if using) should be able to hold this number of instances]. Defaults to 1.
            no_repeat_ngram_size (int, optional): [n-grams phrases cannot repeat in title]. Defaults to 3.
            num_return_sequences (int, optional): [number of sequences to return]. Defaults to 1.
            min_length (int, optional): [the minimum length of generated title]. Defaults to 10.
            min_length (int, optional): [the maximum length of generated title]. Defaults to 30.
            early_stopping (bool, optional): [terminate generation while target number of generated sequences reach <EOS>]. Defaults to false.
            device ([type], optional): [device for the inputs, default to cpu]. Defaults to None.
            debug (bool, optional): [if debug is true, the beam search progress will be shown]. Defaults to False.

        Returns:
            [list of (string, float)]: [a list of generated titles with their probablities]
        """
        if num_return_sequences > num_beams:
            raise Exception('num_return_sequences(%d) cannot be larger than num_beams(%d)' % (num_return_sequences, num_beams))
        selected_ngrams = {}
        mask_token_id = self.tokenizer.mask_token_id
        eos_token_id = 1
        token_type_id = 0
        input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = self.build_inputs(title='[CLS] [SEP]', abstract=abstract, venue=venue, authors=authors, concepts=concepts, affiliations=affiliations, decode_span_type='TEXT', decode_span_length=0, max_seq_length=512, mask_propmt_text='')
        context_length = len(input_ids)
        num_spans = 0
        decode_pos = 1
        decode_postion_ids_second = 1
        for i in range(1, context_length):
            if token_type_ids[i] == 0:
                position_ids_second[i] = i + 1
        input_ids.insert(decode_pos, mask_token_id)
        token_type_ids.insert(decode_pos, token_type_id)
        position_ids.insert(decode_pos, num_spans)
        position_ids_second.insert(decode_pos, decode_postion_ids_second)
        masked_lm_labels.insert(decode_pos, self.tokenizer.cls_token_id)
        q = [(input_ids, 0)]
        selected_entities = []

        def tensorize(x):
            return torch.LongTensor(x)
        while True:
            batch_input_ids = tensorize([_input_ids for _input_ids, _ in q])
            batch_token_type_ids = tensorize([token_type_ids for _ in q])
            current_total_length = batch_input_ids.shape[1]
            current_entity_length = current_total_length - context_length
            batch_attention_mask = torch.ones((current_total_length, current_total_length))
            batch_attention_mask[decode_pos - current_entity_length + 1:decode_pos + 1, decode_pos - current_entity_length + 1:decode_pos + 1] = torch.tril(batch_attention_mask[decode_pos - current_entity_length + 1:decode_pos + 1, decode_pos - current_entity_length + 1:decode_pos + 1])
            batch_attention_mask = batch_attention_mask.unsqueeze(0).repeat(len(q), 1, 1)
            batch_position_ids = tensorize([position_ids for _ in q])
            batch_position_ids_second = tensorize([position_ids_second for _ in q])
            batch_masked_lm_labels = tensorize([masked_lm_labels for _ in q])
            sequence_output, pooled_output = self.bert.forward(input_ids=batch_input_ids, token_type_ids=batch_token_type_ids, attention_mask=batch_attention_mask, output_all_encoded_layers=False, checkpoint_activations=False, position_ids=batch_position_ids, position_ids_second=batch_position_ids_second)
            masked_token_indexes = torch.nonzero((batch_masked_lm_labels + 1).view(-1)).view(-1)
            prediction_scores, _ = self.cls(sequence_output, pooled_output, masked_token_indexes)
            prediction_scores = torch.nn.functional.log_softmax(prediction_scores, dim=1)
            for idx, (_input_ids, _) in enumerate(q):
                if current_entity_length >= no_repeat_ngram_size:
                    prefix_key = tuple(_input_ids[decode_pos - no_repeat_ngram_size + 1:decode_pos])
                    for token_id in selected_ngrams.get(prefix_key, set()):
                        prediction_scores[idx, token_id] = -10000
                prefix_key = tuple(_input_ids[decode_pos - current_entity_length:decode_pos])
                if prefix_key in selected_ngrams:
                    for token_id in selected_ngrams.get(prefix_key, set()):
                        prediction_scores[idx, token_id] = -10000
                if current_entity_length <= min_length:
                    prediction_scores[idx, eos_token_id] = -10000
                prediction_scores[idx, _input_ids[decode_pos]] = -10000
            decode_pos += 1
            _q = []
            log_probs, indices = torch.topk(prediction_scores, k=num_beams)
            for idx, (_input_ids, _last_logprob) in enumerate(q):
                for k in range(log_probs.shape[1]):
                    new_input_ids = _input_ids.copy()
                    new_input_ids.insert(decode_pos, indices[idx, k].item())
                    _q.append((new_input_ids, _last_logprob + log_probs[idx, k].item()))
            q = []
            for _input_ids, _last_logprob in _q:
                prefix_key = None
                if current_entity_length >= no_repeat_ngram_size:
                    prefix_key = tuple(_input_ids[decode_pos - no_repeat_ngram_size + 1:decode_pos])
                    if prefix_key not in selected_ngrams:
                        selected_ngrams[prefix_key] = set()
                    selected_ngrams[prefix_key].add(_input_ids[decode_pos])
                if _input_ids[decode_pos] == eos_token_id:
                    selected_entities.append((_input_ids, _last_logprob))
                else:
                    q.append((_input_ids, _last_logprob))
            q.sort(key=lambda tup: tup[-1], reverse=True)
            selected_entities.sort(key=lambda tup: tup[-1], reverse=True)
            q = q[:num_beams]
            if current_entity_length >= max_length + 2:
                break
            if len(selected_entities) >= num_return_sequences:
                if early_stopping or len(q) == 0 or q[0][-1] <= selected_entities[num_return_sequences - 1][-1]:
                    break
            token_type_ids.insert(decode_pos, token_type_id)
            position_ids.insert(decode_pos, num_spans)
            position_ids_second.insert(decode_pos, decode_postion_ids_second)
            masked_lm_labels[decode_pos - 1] = -1
            masked_lm_labels.insert(decode_pos, self.tokenizer.cls_token_id)
            if debug:
                self.print_oag_instance(input_ids=batch_input_ids[0].cpu().detach().numpy(), token_type_ids=batch_token_type_ids[0].cpu().detach().numpy(), input_masks=batch_attention_mask[0].cpu().detach().numpy(), masked_lm_labels=batch_masked_lm_labels[0].cpu().detach().numpy(), position_ids=batch_position_ids[0].cpu().detach().numpy(), position_ids_second=batch_position_ids_second[0].cpu().detach().numpy(), predictions=torch.topk(prediction_scores, k=5, dim=1).indices.cpu().detach().numpy())
                input('== Press Enter for next step ==')
        results = []
        for seq, logprob in selected_entities[:num_return_sequences]:
            token_ids = []
            for _id in seq[decode_pos - current_entity_length + 1:decode_pos]:
                if _id != eos_token_id:
                    token_ids.append(_id)
                else:
                    break
            results.append((self._convert_token_ids_to_text(token_ids), logprob))
        return results


PRETRAINED_MODEL_ARCHIVE_MAP = {'oagbert-v1': 'https://cloud.tsinghua.edu.cn/f/051c9f87d8544698826e/?dl=1', 'oagbert-test': 'https://cloud.tsinghua.edu.cn/f/9277b9229f6246479ec7/?dl=1', 'oagbert-v2-test': 'https://cloud.tsinghua.edu.cn/f/3b8c4525677f4816a138/?dl=1', 'oagbert-v2': 'https://cloud.tsinghua.edu.cn/f/89da6262f3424dd38b05/?dl=1', 'oagbert-v2-lm': 'https://cloud.tsinghua.edu.cn/f/e9e9f435633d4a4ba232/?dl=1', 'oagbert-v2-sim': 'https://cloud.tsinghua.edu.cn/f/e26cb053dbfb45c8af4c/?dl=1', 'oagbert-v2-zh': 'https://cloud.tsinghua.edu.cn/f/cf806c8008b542509201/?dl=1', 'oagbert-v2-zh-sim': 'https://cloud.tsinghua.edu.cn/f/bb6fbc9cda9342698c31/?dl=1'}


def makedirs(path):
    try:
        os.makedirs(osp.expanduser(osp.normpath(path)))
    except OSError as e:
        if e.errno != errno.EEXIST and osp.isdir(path):
            raise e


def download_url(url, folder, name=None, log=True):
    """Downloads the content of an URL to a specific folder.

    Args:
        url (string): The url.
        folder (string): The folder.
        name (string): saved filename.
        log (bool, optional): If :obj:`False`, will not print anything to the
            console. (default: :obj:`True`)
    """
    if log:
        None
    makedirs(folder)
    try:
        data = request.urlopen(url)
    except Exception as e:
        None
        None
        None
        exit(1)
    if name is None:
        filename = url.rpartition('/')[2]
    else:
        filename = name
    path = osp.join(folder, filename)
    with open(path, 'wb') as f:
        f.write(data.read())
    return path


def untar(path, fname, deleteTar=True):
    """
    Unpacks the given archive file to the same directory, then (by default)
    deletes the archive file.
    """
    None
    fullpath = os.path.join(path, fname)
    shutil.unpack_archive(fullpath, path)
    if deleteTar:
        os.remove(fullpath)


class OAGBertPretrainingModel(BertForPreTrainingPreLN):

    def __init__(self, bert_config):
        super(OAGBertPretrainingModel, self).__init__(bert_config)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, checkpoint_activations=False):
        return self.bert.forward(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_all_encoded_layers=output_all_encoded_layers, checkpoint_activations=checkpoint_activations)

    @staticmethod
    def _load(model_name_or_path: str, load_weights: bool=False):
        if not os.path.exists(model_name_or_path):
            if model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:
                if not os.path.exists(f'saved/{model_name_or_path}'):
                    archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[model_name_or_path]
                    download_url(archive_file, 'saved/', f'{model_name_or_path}.zip')
                    untar('saved/', f'{model_name_or_path}.zip')
                model_name_or_path = f'saved/{model_name_or_path}'
            else:
                raise KeyError('Cannot find the pretrained model {}'.format(model_name_or_path))
        try:
            version = open(os.path.join(model_name_or_path, 'version')).readline().strip()
        except Exception:
            version = None
        bert_config = BertConfig.from_dict(json.load(open(os.path.join(model_name_or_path, 'bert_config.json'))))
        if os.path.exists(os.path.join(model_name_or_path, 'vocab.txt')):
            tokenizer = BertTokenizer.from_pretrained(model_name_or_path)
        elif os.path.exists(os.path.join(model_name_or_path, 'vocab.model')):
            tokenizer = spm.SentencePieceProcessor(model_file=os.path.join(model_name_or_path, 'vocab.model'))
        else:
            raise FileNotFoundError('Cannot find vocabulary file')
        if version == '2':
            bert_model = OAGMetaInfoBertModel(bert_config, tokenizer)
        else:
            bert_model = OAGBertPretrainingModel(bert_config)
        model_weight_path = os.path.join(model_name_or_path, 'pytorch_model.bin')
        if load_weights and os.path.exists(model_weight_path):
            bert_model.load_state_dict(torch.load(model_weight_path))
        return bert_config, tokenizer, bert_model


class CrossEntropyLoss(nn.Module):

    def __call__(self, y_pred, y_true):
        y_true = y_true.long()
        y_pred = torch.nn.functional.log_softmax(y_pred, dim=-1)
        return torch.nn.functional.nll_loss(y_pred, y_true)


class BCEWithLogitsLoss(nn.Module):

    def __call__(self, y_pred, y_true, reduction='mean'):
        y_true = y_true.float()
        loss = torch.nn.BCEWithLogitsLoss(reduction=reduction)(y_pred, y_true)
        if reduction == 'none':
            loss = torch.sum(torch.mean(loss, dim=0))
        return loss


class DistMultLayer(nn.Module):

    def __init__(self):
        super(DistMultLayer, self).__init__()

    def forward(self, sub_emb, obj_emb, rel_emb):
        return torch.sum(sub_emb * obj_emb * rel_emb, dim=-1)

    def predict(self, sub_emb, obj_emb, rel_emb):
        return torch.matmul(sub_emb * rel_emb, obj_emb.t())


class ConvELayer(nn.Module):

    def __init__(self, dim, num_filter=20, kernel_size=7, k_w=10, dropout=0.3):
        super(ConvELayer, self).__init__()
        assert dim % k_w == 0
        self.k_w = k_w
        self.k_h = dim // k_w
        self.dim = dim
        self.bn0 = torch.nn.BatchNorm2d(1)
        self.bn1 = torch.nn.BatchNorm2d(num_filter)
        self.bn2 = torch.nn.BatchNorm1d(dim)
        self.hidden_drop = nn.Dropout(dropout)
        self.hidden_drop2 = nn.Dropout(dropout)
        self.feature_drop = nn.Dropout(dropout)
        self.conv = nn.Conv2d(1, out_channels=num_filter, kernel_size=(kernel_size, kernel_size), stride=1, padding=0, bias=True)
        flat_size_h = int(2 * self.k_w) - kernel_size + 1
        flat_size_w = self.k_h - kernel_size + 1
        self.flat_size = flat_size_h * flat_size_w * num_filter
        self.fc = nn.Linear(self.flat_size, dim)
        self.bias = nn.Parameter(torch.zeros(dim))

    def concat(self, ent, rel):
        ent = ent.view(-1, 1, self.dim)
        rel = rel.view(-1, 1, self.dim)
        ent_rel = torch.cat([ent, rel], dim=1)
        ent_rel = ent_rel.transpose(2, 1).reshape(-1, 1, 2 * self.k_w, self.k_h)
        return ent_rel

    def forward(self, sub_emb, obj_emb, rel_emb):
        h = self.concat(sub_emb, rel_emb)
        h = self.bn0(h)
        h = self.conv(h)
        h = F.relu(self.bn1(h))
        h = self.feature_drop(h)
        h = h.view(-1, self.flat_size)
        h = self.hidden_drop(self.fc(h))
        h = F.relu(self.bn2(self.hidden_drop2(h)))
        x = torch.sum(h * obj_emb + self.bias, dim=-1)
        return x

    def predict(self, sub_emb, obj_emb, rel_emb):
        h = self.concat(sub_emb, rel_emb)
        h = self.bn0(h)
        h = self.conv(h)
        h = F.relu(self.bn1(h))
        h = h.view(-1, self.flat_size)
        h = self.fc(h)
        h = F.relu(self.bn2(h))
        x = torch.matmul(h, obj_emb.t())
        return x


class NoamOptimizer(nn.Module):
    """A simple wrapper class for learning rate scheduling"""

    def __init__(self, optimizer, d_model, n_warmup_steps, init_lr=None):
        super(NoamOptimizer, self).__init__()
        self._optimizer = optimizer
        self.param_groups = optimizer.param_groups
        self.n_warmup_steps = n_warmup_steps
        self.n_current_steps = 0
        self.init_lr = np.power(d_model, -0.5) if init_lr is None else init_lr / np.power(self.n_warmup_steps, -0.5)

    def step(self):
        """Step with the inner optimizer"""
        self._update_learning_rate()
        self._optimizer.step()

    def zero_grad(self):
        """Zero out the gradients by the inner optimizer"""
        self._optimizer.zero_grad()

    def _get_lr_scale(self):
        return np.min([np.power(self.n_current_steps, -0.5), np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])

    def _update_learning_rate(self):
        """Learning rate scheduling per step"""
        self.n_current_steps += 1
        lr = self.init_lr * self._get_lr_scale()
        for param_group in self._optimizer.param_groups:
            param_group['lr'] = lr


class LinearOptimizer(nn.Module):
    """A simple wrapper class for learning rate scheduling"""

    def __init__(self, optimizer, n_warmup_steps, n_training_steps, init_lr=0.001):
        super(LinearOptimizer, self).__init__()
        self._optimizer = optimizer
        self.param_groups = optimizer.param_groups
        self.n_warmup_steps = n_warmup_steps
        self.n_current_steps = 0
        self.n_training_steps = n_training_steps
        self.init_lr = init_lr

    def step(self):
        """Step with the inner optimizer"""
        self._update_learning_rate()
        self._optimizer.step()

    def zero_grad(self):
        """Zero out the gradients by the inner optimizer"""
        self._optimizer.zero_grad()

    def _get_lr_scale(self):
        if self.n_current_steps < self.n_warmup_steps:
            return float(self.n_current_steps) / float(max(1, self.n_warmup_steps))
        return max(0.0, float(self.n_training_steps - self.n_current_steps) / float(max(1, self.n_training_steps - self.n_warmup_steps)))

    def _update_learning_rate(self):
        """Learning rate scheduling per step"""
        self.n_current_steps += 1
        lr = self.init_lr * self._get_lr_scale()
        for param_group in self._optimizer.param_groups:
            param_group['lr'] = lr


def spmm_cpu(graph, x, fast_spmm_cpu=None):
    if fast_spmm_cpu is None:
        initialize_spmm_cpu()
        fast_spmm_cpu = CONFIGS['fast_spmm_cpu']
    if fast_spmm_cpu is not None and str(x.device) == 'cpu':
        if graph.out_norm is not None:
            x = graph.out_norm * x
        row_ptr, col_indices = graph.row_indptr, graph.col_indices
        csr_data = graph.raw_edge_weight
        x = fast_spmm_cpu(row_ptr.int(), col_indices.int(), csr_data, x)
        if graph.in_norm is not None:
            x = graph.in_norm * x
    else:
        row, col = graph.edge_index
        x = spmm_scatter(row, col, graph.edge_weight, x)
    return x


class SpMM_CPU(torch.nn.Module):

    def __init__(self):
        super().__init__()
        initialize_spmm_cpu()
        self.fast_spmm_cpu = CONFIGS['fast_spmm_cpu']

    def forward(self, graph, x):
        return spmm_cpu(graph, x, self.fast_spmm_cpu)


class SpMM(torch.nn.Module):

    def __init__(self, actnn=False):
        super().__init__()
        initialize_spmm()
        self.actnn = actnn
        self.fast_spmm = CONFIGS['fast_spmm']

    def forward(self, graph, x):
        return spmm(graph, x, self.actnn, self.fast_spmm)


class FusedGATOp(torch.nn.Module):

    def __init__(self):
        super().__init__()
        initialize_fused_gat()
        self.fused_gat_func = CONFIGS['fused_gat_func']

    def forward(self, attn_row, attn_col, graph, negative_slope, in_feat):
        return fused_gat_op(attn_row, attn_col, graph, negative_slope, in_feat, fused_gat_op=self.fused_gat_func)


def dropout_features(x: torch.Tensor, droprate: float, training=True):
    n = x.shape[1]
    drop_rates = torch.ones(n, device=x.device) * droprate
    if training:
        masks = torch.bernoulli(1.0 - drop_rates).view(1, -1).expand_as(x)
        masks = masks
        masks = masks
        x = masks * x
    return x


class DropFeatures(torch.nn.Module):

    def __init__(self, drop_rate):
        super(DropFeatures, self).__init__()
        self.drop_rate = drop_rate

    def forward(self, x):
        return dropout_features(x, self.drop_rate, training=self.training)


class DropEdge(torch.nn.Module):

    def __init__(self, drop_rate: float=0.5, renorm: Optional[str]='sym'):
        super(DropEdge, self).__init__()
        self.drop_rate = drop_rate
        self.renorm = renorm

    def forward(self, edge_index, edge_weight=None):
        return dropout_adj(edge_index, edge_weight, self.drop_rate, self.renorm, self.training)


def drop_node(x, drop_rate=0.5, training=True):
    n = x.shape[0]
    drop_rates = torch.ones(n) * drop_rate
    if training:
        masks = torch.bernoulli(1.0 - drop_rates).unsqueeze(1)
        x = masks * x
        x = x / drop_rate
    return x


class DropNode(torch.nn.Module):

    def __init__(self, drop_rate=0.5):
        super(DropNode, self).__init__()
        self.drop_rate = drop_rate

    def forward(self, x):
        return drop_node(x, self.drop_rate, self.training)


class Accuracy(object):

    def __init__(self, mini_batch=False):
        super(Accuracy, self).__init__()
        self.mini_batch = mini_batch
        self.tp = list()
        self.total = list()

    def __call__(self, y_pred, y_true):
        pred = (y_pred.argmax(1) == y_true).int()
        tp = pred.sum().int()
        total = pred.shape[0]
        if torch.is_tensor(tp):
            tp = tp.item()
        self.tp.append(tp)
        self.total.append(total)
        return tp / total

    def evaluate(self):
        if len(self.tp) > 0:
            tp = np.sum(self.tp)
            total = np.sum(self.total)
            self.tp = list()
            self.total = list()
            return tp / total
        warnings.warn('pre-computing list is empty')
        return 0

    def clear(self):
        self.tp = list()
        self.total = list()


class MultiLabelMicroF1(Accuracy):

    def __init__(self, mini_batch=False):
        super(MultiLabelMicroF1, self).__init__(mini_batch)

    def __call__(self, y_pred, y_true, sigmoid=False):
        if sigmoid:
            border = 0.5
        else:
            border = 0
        y_pred[y_pred >= border] = 1
        y_pred[y_pred < border] = 0
        tp = (y_pred * y_true).sum().item()
        fp = ((1 - y_true) * y_pred).sum().item()
        fn = (y_true * (1 - y_pred)).sum().item()
        total = tp + fp + fn
        self.tp.append(int(tp))
        self.total.append(int(total))
        if total == 0:
            return 0
        return float(tp / total)


def merge_batch_indexes(values: list, method='mean'):
    if isinstance(values[0], dict) or isinstance(values[0], tuple):
        return values
    elif method == 'mean':
        return sum(values) / len(values)
    elif method == 'sum':
        return sum(values)
    else:
        return sum(values)


class BaseEvaluator(object):

    def __init__(self, eval_func):
        self.y_pred = list()
        self.y_true = list()
        self.eval_func = eval_func

    def __call__(self, y_pred, y_true):
        metric = self.eval_func(y_pred, y_true)
        self.y_pred.append(y_pred.cpu())
        self.y_true.append(y_true.cpu())
        return metric

    def clear(self):
        self.y_pred = list()
        self.y_true = list()

    def evaluate(self):
        if len(self.y_pred) > 0:
            y_pred = torch.cat(self.y_pred, dim=0)
            y_true = torch.cat(self.y_true, dim=0)
            self.clear()
            return self.eval_func(y_pred, y_true)
        return 0


class MultiClassMicroF1(Accuracy):

    def __init__(self, mini_batch=False):
        super(MultiClassMicroF1, self).__init__(mini_batch)


def setup_evaluator(metric: Union[str, Callable]):
    if isinstance(metric, str):
        metric = metric.lower()
        if metric == 'acc' or metric == 'accuracy':
            return Accuracy()
        elif metric == 'multilabel_microf1' or 'microf1' or 'micro_f1':
            return MultiLabelMicroF1()
        elif metric == 'multiclass_microf1':
            return MultiClassMicroF1()
        else:
            raise NotImplementedError
    else:
        return BaseEvaluator(metric)


class ModelWrapper(torch.nn.Module):

    @staticmethod
    def add_args(parser):
        pass

    def __init__(self):
        super(ModelWrapper, self).__init__()
        self.__model_keys__ = None
        self._loss_func = None
        self._evaluator = None
        self._evaluator_metric = None
        self.__record__ = dict()
        self.training_type = ''

    def forward(self):
        pass

    def pre_stage(self, stage, data_w):
        pass

    def post_stage(self, stage, data_w):
        pass

    def train_step(self, subgraph):
        pass

    def val_step(self, subgraph):
        pass

    def test_step(self, subgraph):
        pass

    def evaluate(self, pred: torch.Tensor, labels: torch.Tensor, metric: Union[str, Callable]='auto'):
        """
        method: str or callable function,
        """
        pred = pred.cpu()
        labels = labels.cpu()
        if self._evaluator is None:
            if metric == 'auto':
                if len(labels.shape) > 1:
                    metric = 'multilabel_microf1'
                    self._evaluator_metric = 'micro_f1'
                else:
                    metric = 'accuracy'
                    self._evaluator_metric = 'acc'
            self._evaluator = setup_evaluator(metric)
        return self._evaluator(pred, labels)

    @abstractmethod
    def setup_optimizer(self):
        raise NotImplementedError

    def set_early_stopping(self):
        """
        Return:
            1. `str`, the monitoring metric
            2. tuple(`str`, `str`), that is, (the monitoring metric, `small` or `big`). The second parameter means,
                `the smaller, the better` or `the bigger, the better`
        """
        return 'val_metric', '>'

    def on_train_step(self, *args, **kwargs):
        return self.train_step(*args, **kwargs)

    def on_val_step(self, *args, **kwargs):
        out = self.val_step(*args, **kwargs)
        self.set_notes(out, 'val')

    def on_test_step(self, *args, **kwargs):
        out = self.test_step(*args, **kwargs)
        self.set_notes(out, 'test')

    def set_notes(self, out, prefix='val'):
        if isinstance(out, dict):
            for key, val in out.items():
                self.note(key, val)
        elif isinstance(out, tuple) or isinstance(out, list):
            for i, val in enumerate(out):
                self.note(f'{prefix}_{i}', val)

    def note(self, name: str, data, merge='mean'):
        """
        name: str
        data: Any
        """
        if name not in self.__record__:
            name = name.lower()
            self.__record__[name] = [data]
        else:
            self.__record__[name].append(data)

    def collect_notes(self):
        if len(self.__record__) == 0:
            return None
        out = dict()
        for key, val in self.__record__.items():
            if key.endswith('_metric'):
                _val = self._evaluator.evaluate()
                if _val == 0:
                    _val = val[0]
            else:
                _val = merge_batch_indexes(val)
            out[key] = _val
        self.__record__ = dict()
        return out

    @property
    def default_loss_fn(self):
        if self._loss_func is None:
            raise RuntimeError('`loss_fn` must be set for your ModelWrapper using `mw.default_loss_fn = your_loss_fn`.', f'Now self.loss_fn is {self._loss_fn}')
        return self._loss_func

    @default_loss_fn.setter
    def default_loss_fn(self, loss_fn):
        self._loss_func = loss_fn

    @property
    def default_evaluator(self):
        return self._evaluator

    @default_evaluator.setter
    def default_evaluator(self, x):
        self._evaluator = x

    @property
    def device(self):
        return next(self.parameters()).device

    @property
    def evaluation_metric(self):
        return self._evaluator_metric

    def set_evaluation_metric(self):
        if isinstance(self._evaluator, MultiLabelMicroF1):
            self._evaluator_metric = 'micro_f1'
        elif isinstance(self._evaluator, Accuracy):
            self._evaluator_metric = 'acc'
        else:
            evaluation_metric = self.set_early_stopping()
            if not isinstance(evaluation_metric, str):
                evaluation_metric = evaluation_metric[0]
            if evaluation_metric.startswith('val'):
                evaluation_metric = evaluation_metric[3:]
            self._evaluator_metric = evaluation_metric

    def load_checkpoint(self, path):
        pass

    def save_checkpoint(self, path):
        pass

    def _find_model(self):
        models = []
        for k, v in self.__dict__.items():
            if isinstance(v, torch.nn.Module):
                models.append(k)
        self.__model_keys__ = models

    @property
    def wrapped_model(self):
        if hasattr(self, 'model'):
            return getattr(self, 'model')
        assert len(self._model_key_) == 1, f'{len(self._model_key_)} exists'
        return getattr(self, self._model_key_[0])

    @wrapped_model.setter
    def wrapped_model(self, model):
        if len(self._model_key_) == 0:
            self.__model_keys__ = [None]
        setattr(self, self._model_key_[0], model)

    @property
    def _model_key_(self):
        if self.__model_keys__ is None:
            self._find_model()
        return self.__model_keys__


class EmbeddingModelWrapper(ModelWrapper):

    def setup_optimizer(self):
        pass


class UnsupervisedModelWrapper(ModelWrapper):

    def __init__(self):
        super(UnsupervisedModelWrapper, self).__init__()
        self.training_type = 'unsupervised'


def evaluate_clustering(features_matrix, labels, cluster_method, num_clusters, num_nodes, full=True):
    None
    if cluster_method == 'kmeans':
        kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(features_matrix)
        clusters = kmeans.labels_
    else:
        clustering = SpectralClustering(n_clusters=num_clusters, assign_labels='discretize', random_state=0).fit(features_matrix)
        clusters = clustering.labels_
    None
    truth = labels.cpu().numpy()
    if full:
        mat = np.zeros([num_clusters, num_clusters])
        for i in range(num_nodes):
            mat[clusters[i]][truth[i]] -= 1
        _, row_idx = linear_sum_assignment(mat)
        acc = -mat[_, row_idx].sum() / num_nodes
        for i in range(num_nodes):
            clusters[i] = row_idx[clusters[i]]
        macro_f1 = f1_score(truth, clusters, average='macro')
        return dict(acc=acc, nmi=normalized_mutual_info_score(clusters, truth), macro_f1=macro_f1)
    else:
        return dict(nmi=normalized_mutual_info_score(clusters, truth))


class DAEGCModelWrapper(ModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-clusters', type=int, default=7)
        parser.add_argument('--cluster-method', type=str, default='kmeans', help='option: kmeans or spectral')
        parser.add_argument('--evaluation', type=str, default='full', help='option: full or NMI')
        parser.add_argument('--T', type=int, default=5)

    def __init__(self, model, optimizer_cfg, num_clusters, cluster_method='kmeans', evaluation='full', T=5):
        super(DAEGCModelWrapper, self).__init__()
        self.model = model
        self.num_clusters = num_clusters
        self.optimizer_cfg = optimizer_cfg
        self.cluster_method = cluster_method
        self.full = evaluation == 'full'
        self.t = T
        self.stage = 0
        self.count = 0

    def train_step(self, subgraph):
        graph = subgraph
        if self.stage == 0:
            z = self.model(graph)
            loss = self.recon_loss(z, graph.adj_mx)
        else:
            cluster_center = self.model.get_cluster_center()
            z = self.model(graph)
            Q = self.getQ(z, cluster_center)
            self.count += 1
            if self.count % self.t == 0:
                P = self.getP(Q).detach()
            loss = self.recon_loss(z, graph.adj_mx) + self.gamma * self.cluster_loss(P, Q)
        return loss

    def test_step(self, subgraph):
        graph = subgraph
        features_matrix = self.model(graph)
        features_matrix = features_matrix.detach().cpu().numpy()
        return evaluate_clustering(features_matrix, graph.y, self.cluster_method, self.num_clusters, graph.num_nodes, self.full)

    def recon_loss(self, z, adj):
        return F.binary_cross_entropy(F.softmax(torch.mm(z, z.t())), adj, reduction='sum')

    def cluster_loss(self, P, Q):
        return torch.nn.KLDivLoss(reduce=True, size_average=False)(P.log(), Q)

    def setup_optimizer(self):
        lr, wd = self.optimizer_cfg['lr'], self.optimizer_cfg['weight_decay']
        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=wd)

    def pre_stage(self, stage, data_w):
        self.stage = stage
        if stage == 0:
            data = data_w.get_dataset().data
            data.add_remaining_self_loops()
            data.store('edge_index')
            data.adj_mx = torch.sparse_coo_tensor(torch.stack(data.edge_index), torch.ones(data.edge_index[0].shape[0]), torch.Size([data.x.shape[0], data.x.shape[0]])).to_dense()
            edge_index_2hop = data.edge_index
            data.edge_index = edge_index_2hop

    def post_stage(self, stage, data_w):
        if stage == 0:
            data = data_w.get_dataset().data
            data.restore('edge_index')
            data
            kmeans = KMeans(n_clusters=self.num_clusters, random_state=0).fit(self.model(data).detach().cpu().numpy())
            self.model.set_cluster_center(torch.tensor(kmeans.cluster_centers_, device=self.device))

    def getQ(self, z, cluster_center):
        Q = None
        for i in range(z.shape[0]):
            dis = torch.sum((z[i].repeat(self.num_clusters, 1) - cluster_center) ** 2, dim=1)
            t = 1 / (1 + dis)
            t = t / torch.sum(t)
            if Q is None:
                Q = t.clone().unsqueeze(0)
            else:
                Q = torch.cat((Q, t.unsqueeze(0)), 0)
        return Q

    def getP(self, Q):
        P = torch.sum(Q, dim=0).repeat(Q.shape[0], 1)
        P = Q ** 2 / P
        P = P / (torch.ones(1, self.num_clusters, device=self.device) * torch.sum(P, dim=1).unsqueeze(-1))
        return P


class GAEModelWrapper(ModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--num-clusters', type=int, default=7)
        parser.add_argument('--cluster-method', type=str, default='kmeans', help='option: kmeans or spectral')
        parser.add_argument('--evaluation', type=str, default='full', help='option: full or NMI')

    def __init__(self, model, optimizer_cfg, num_clusters, cluster_method='kmeans', evaluation='full'):
        super(GAEModelWrapper, self).__init__()
        self.model = model
        self.num_clusters = num_clusters
        self.optimizer_cfg = optimizer_cfg
        self.cluster_method = cluster_method
        self.full = evaluation == 'full'

    def train_step(self, subgraph):
        graph = subgraph
        loss = self.model.make_loss(graph, graph.adj_mx)
        return loss

    def test_step(self, subgraph):
        graph = subgraph
        features_matrix = self.model(graph)
        features_matrix = features_matrix.detach().cpu().numpy()
        return evaluate_clustering(features_matrix, graph.y, self.cluster_method, self.num_clusters, graph.num_nodes, self.full)

    def pre_stage(self, stage, data_w):
        if stage == 0:
            data = data_w.get_dataset().data
            adj_mx = torch.sparse_coo_tensor(torch.stack(data.edge_index), torch.ones(data.edge_index[0].shape[0]), torch.Size([data.x.shape[0], data.x.shape[0]])).to_dense()
            data.adj_mx = adj_mx

    def setup_optimizer(self):
        lr, wd = self.optimizer_cfg['lr'], self.optimizer_cfg['weight_decay']
        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=wd)


class GraphClassificationModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(GraphClassificationModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg

    def train_step(self, batch):
        pred = self.model(batch)
        y = batch.y
        loss = self.default_loss_fn(pred, y)
        return loss

    def val_step(self, batch):
        pred = self.model(batch)
        y = batch.y
        val_loss = self.default_loss_fn(pred, y)
        metric = self.evaluate(pred, y, metric='auto')
        self.note('val_loss', val_loss)
        self.note('val_metric', metric)

    def test_step(self, batch):
        pred = self.model(batch)
        y = batch.y
        test_loss = self.default_loss_fn(pred, y)
        metric = self.evaluate(pred, y, metric='auto')
        self.note('test_loss', test_loss)
        self.note('test_metric', metric)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


def files_exist(files):
    return all([osp.exists(f) for f in files])


def to_list(x):
    if not isinstance(x, collections.Iterable) or isinstance(x, str):
        x = [x]
    return x


class Dataset(torch.utils.data.Dataset):
    """Dataset base class for creating graph datasets.

    Args:
        root (string): Root directory where the dataset should be saved.
        transform (callable, optional): A function/transform that takes in an
            :obj:`cogdl.data.Data` object and returns a transformed
            version. The data object will be transformed before every access.
            (default: :obj:`None`)
        pre_transform (callable, optional): A function/transform that takes in
            an :obj:`cogdl.data.Data` object and returns a
            transformed version. The data object will be transformed before
            being saved to disk. (default: :obj:`None`)
        pre_filter (callable, optional): A function that takes in an
            :obj:`cogdl.data.Data` object and returns a boolean
            value, indicating whether the data object should be included in the
            final dataset. (default: :obj:`None`)
    """

    @staticmethod
    def add_args(parser):
        """Add dataset-specific arguments to the parser."""
        pass

    @property
    def raw_file_names(self):
        """The name of the files to find in the :obj:`self.raw_dir` folder in
        order to skip the download."""
        raise NotImplementedError

    @property
    def processed_file_names(self):
        """The name of the files to find in the :obj:`self.processed_dir`
        folder in order to skip the processing."""
        raise NotImplementedError

    def download(self):
        """Downloads the dataset to the :obj:`self.raw_dir` folder."""
        raise NotImplementedError

    def process(self):
        """Processes the dataset to the :obj:`self.processed_dir` folder."""
        raise NotImplementedError

    def __len__(self):
        """The number of examples in the dataset."""
        return 1

    def get(self, idx):
        """Gets the data object at index :obj:`idx`."""
        raise NotImplementedError

    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):
        super(Dataset, self).__init__()
        self.root = osp.expanduser(osp.normpath(root))
        self.raw_dir = osp.join(self.root, 'raw')
        self.processed_dir = osp.join(self.root, 'processed')
        self.transform = transform
        self.pre_transform = pre_transform
        self.pre_filter = pre_filter
        self._download()
        self._process()

    @property
    def num_features(self):
        """Returns the number of features per node in the graph."""
        if hasattr(self, 'data') and isinstance(self.data, Graph):
            return self.data.num_features
        elif hasattr(self, 'data') and isinstance(self.data, list):
            return self.data[0].num_features
        else:
            return 0

    @property
    def raw_paths(self):
        """The filepaths to find in order to skip the download."""
        files = to_list(self.raw_file_names)
        return [osp.join(self.raw_dir, f) for f in files]

    @property
    def processed_paths(self):
        """The filepaths to find in the :obj:`self.processed_dir`
        folder in order to skip the processing."""
        files = to_list(self.processed_file_names)
        return [osp.join(self.processed_dir, f) for f in files]

    def _download(self):
        if files_exist(self.raw_paths):
            return
        makedirs(self.raw_dir)
        self.download()

    def _process(self):
        if files_exist(self.processed_paths):
            return
        None
        makedirs(self.processed_dir)
        self.process()
        None

    def get_evaluator(self):
        return Accuracy()

    def get_loss_fn(self):
        return CrossEntropyLoss()

    def __getitem__(self, idx):
        """Gets the data object at index :obj:`idx` and transforms it (in case
        a :obj:`self.transform` is given)."""
        assert idx == 0
        data = self.data
        data = data if self.transform is None else self.transform(data)
        return data

    @property
    def num_classes(self):
        """The number of classes in the dataset."""
        if hasattr(self, 'y') and self.y is not None:
            y = self.y
        elif hasattr(self, 'data') and hasattr(self.data, 'y') and self.data.y is not None:
            y = self.data.y
        else:
            return 0
        return y.max().item() + 1 if y.dim() == 1 else y.size(1)

    @property
    def edge_attr_size(self):
        return None

    @property
    def max_degree(self):
        return self.data.degrees().max().item() + 1

    @property
    def max_graph_size(self):
        return self.data.num_nodes

    @property
    def num_graphs(self):
        return 1

    def __repr__(self):
        return '{}'.format(self.__class__.__name__)


class MultiGraphDataset(Dataset):

    def __init__(self, root=None, transform=None, pre_transform=None, pre_filter=None):
        super(MultiGraphDataset, self).__init__(root, transform, pre_transform, pre_filter)
        self.data, self.slices = None, None

    @property
    def num_classes(self):
        if hasattr(self, 'y'):
            y = self.y
        elif hasattr(self, 'data') and hasattr(self.data[0], 'y'):
            y = torch.cat([x.y for x in self.data], dim=0)
        else:
            return 0
        return y.max().item() + 1 if y.dim() == 1 else y.size(1)

    @property
    def num_features(self):
        if isinstance(self[0], Graph):
            return self[0].num_features
        else:
            return 0

    @property
    def max_degree(self):
        max_degree = [x.degrees().max().item() for x in self.data]
        max_degree = np.max(max_degree) + 1
        return max_degree

    @property
    def num_graphs(self):
        return len(self.data)

    @property
    def max_graph_size(self):
        return np.max([g.num_nodes for g in self.data])

    def len(self):
        if isinstance(self.data, list):
            return len(self.data)
        else:
            for item in self.slices.values():
                return len(item) - 1
            return 0

    def _get(self, idx):
        data = self.data.__class__()
        if hasattr(self.data, '__num_nodes__'):
            data.num_nodes = self.data.__num_nodes__[idx]
        for key in self.data.__old_keys__():
            item, slices = self.data[key], self.slices[key]
            start, end = int(slices[idx]), int(slices[idx + 1])
            if key == 'edge_index':
                data[key] = item[0][start:end], item[1][start:end]
            else:
                if torch.is_tensor(item):
                    s = list(repeat(slice(None), item.dim()))
                    s[self.data.__cat_dim__(key, item)] = slice(start, end)
                elif start + 1 == end:
                    s = slices[start]
                else:
                    s = slice(start, end)
                data[key] = item[s]
        return data

    def get(self, idx):
        try:
            idx = int(idx)
        except Exception:
            idx = idx
        if torch.is_tensor(idx):
            idx = idx.numpy().tolist()
        if isinstance(idx, int):
            if self.slices is not None:
                return self._get(idx)
            return self.data[idx]
        if isinstance(idx, slice):
            start = idx.start
            end = idx.stop
            step = idx.step if idx.step else 1
            idx = list(range(start, end, step))
        if len(idx) > 1:
            if self.slices is not None:
                return [self._get(int(i)) for i in idx]
            return [self.data[i] for i in idx]

    def __getitem__(self, item):
        return self.get(item)

    def __len__(self):
        return len(self.data)

    def __repr__(self):
        return '{}({})'.format(self.__class__.__name__, len(self))


def evaluate_graph_embeddings_using_svm(embeddings, labels):
    result = []
    kf = KFold(n_splits=10)
    kf.get_n_splits(X=embeddings, y=labels)
    for train_index, test_index in kf.split(embeddings):
        x_train = embeddings[train_index]
        x_test = embeddings[test_index]
        y_train = labels[train_index]
        y_test = labels[test_index]
        params = {'C': [0.01, 0.1, 1]}
        svc = SVC()
        clf = GridSearchCV(svc, params)
        clf.fit(x_train, y_train)
        preds = clf.predict(x_test)
        f1 = f1_score(y_test, preds, average='micro')
        result.append(f1)
    test_f1 = np.mean(result)
    test_std = np.std(result)
    return dict(acc=test_f1, std=test_std)


class GraphEmbeddingModelWrapper(EmbeddingModelWrapper):

    def __init__(self, model):
        super(GraphEmbeddingModelWrapper, self).__init__()
        self.model = model

    def train_step(self, batch):
        if isinstance(batch, DataLoader) or isinstance(batch, MultiGraphDataset):
            graphs = [x for x in batch]
        else:
            graphs = batch
        emb = self.model(graphs)
        return emb

    def test_step(self, batch):
        x, y = batch
        return evaluate_graph_embeddings_using_svm(x, y)


class InfoGraphModelWrapper(ModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--sup', action='store_true')

    def __init__(self, model, optimizer_cfg, sup=False):
        super(InfoGraphModelWrapper, self).__init__()
        self.model = model
        hidden_size = optimizer_cfg['hidden_size']
        model_num_layers = model.num_layers
        self.local_dis = FF(model_num_layers * hidden_size, hidden_size)
        self.global_dis = FF(model_num_layers * hidden_size, hidden_size)
        self.optimizer_cfg = optimizer_cfg
        self.sup = sup
        self.criterion = torch.nn.MSELoss()

    def train_step(self, batch):
        if self.sup:
            pred = self.model.sup_forward(batch, batch.x)
            loss = self.sup_loss(pred, batch)
        else:
            graph_feat, node_feat = self.model.unsup_forward(batch, batch.x)
            loss = self.unsup_loss(graph_feat, node_feat, batch.batch)
        return loss

    def test_step(self, dataset):
        device = self.device
        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)
        preds = []
        with torch.no_grad():
            for batch in dataloader:
                preds.append(self.model(batch))
        preds = torch.cat(preds).cpu().numpy()
        labels = np.array([g.y.item() for g in dataset])
        result = evaluate_graph_embeddings_using_svm(preds, labels)
        self.note('test_metric', result['acc'])
        self.note('std', result['std'])

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam([{'params': self.model.parameters()}, {'params': self.global_dis.parameters()}, {'params': self.local_dis.parameters()}], lr=cfg['lr'], weight_decay=cfg['weight_decay'])

    def sup_loss(self, pred, batch):
        pred = F.softmax(pred, dim=1)
        loss = self.criterion(pred, batch)
        loss += self.unsup_loss(batch.x, batch.edge_index, batch.batch)[1]
        return loss

    def unsup_loss(self, graph_feat, node_feat, batch):
        local_encode = self.local_dis(node_feat)
        global_encode = self.global_dis(graph_feat)
        num_graphs = graph_feat.shape[0]
        num_nodes = node_feat.shape[0]
        pos_mask = torch.zeros((num_nodes, num_graphs))
        neg_mask = torch.ones((num_nodes, num_graphs))
        for nid, gid in enumerate(batch):
            pos_mask[nid][gid] = 1
            neg_mask[nid][gid] = 0
        glob_local_mi = torch.mm(local_encode, global_encode.t())
        loss = self.mi_loss(pos_mask, neg_mask, glob_local_mi, num_nodes, num_nodes * (num_graphs - 1))
        return loss

    @staticmethod
    def mi_loss(pos_mask, neg_mask, mi, pos_div, neg_div):
        pos_mi = pos_mask * mi
        neg_mi = neg_mask * mi
        pos_loss = (-math.log(2.0) + F.softplus(-pos_mi)).sum()
        neg_loss = (-math.log(2.0) + F.softplus(-neg_mi) + neg_mi).sum()
        return pos_loss / pos_div + neg_loss / neg_div


class HeterogeneousEmbeddingModelWrapper(EmbeddingModelWrapper):

    @staticmethod
    def add_args(parser: argparse.ArgumentParser):
        """Add task-specific arguments to the parser."""
        parser.add_argument('--hidden-size', type=int, default=128)

    def __init__(self, model, hidden_size=200):
        super(HeterogeneousEmbeddingModelWrapper, self).__init__()
        self.model = model
        self.hidden_size = hidden_size

    def train_step(self, batch):
        embeddings = self.model(batch)
        embeddings = np.hstack((embeddings, batch.x.numpy()))
        return embeddings

    def test_step(self, batch):
        embeddings, data = batch
        train_index = torch.cat((data.train_node, data.valid_node)).numpy()
        test_index = data.test_node.numpy()
        y = data.y.numpy()
        X_train, y_train = embeddings[train_index], y[train_index]
        X_test, y_test = embeddings[test_index], y[test_index]
        clf = LogisticRegression()
        clf.fit(X_train, y_train)
        preds = clf.predict(X_test)
        test_f1 = f1_score(y_test, preds, average='micro')
        return dict(f1=test_f1)


class HeterogeneousGNNModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(HeterogeneousGNNModelWrapper, self).__init__()
        self.optimizer_cfg = optimizer_cfg
        self.model = model

    def train_step(self, batch):
        graph = batch.data
        pred = self.model(graph)
        train_mask = graph.train_node
        loss = self.default_loss_fn(pred[train_mask], graph.y[train_mask])
        return loss

    def val_step(self, batch):
        graph = batch.data
        pred = self.model(graph)
        val_mask = graph.valid_node
        loss = self.default_loss_fn(pred[val_mask], graph.y[val_mask])
        metric = self.evaluate(pred[val_mask], graph.y[val_mask], metric='auto')
        self.note('val_loss', loss.item())
        self.note('val_metric', metric)

    def test_step(self, batch):
        graph = batch.data
        pred = self.model(graph)
        test_mask = graph.test_node
        loss = self.default_loss_fn(pred[test_mask], graph.y[test_mask])
        metric = self.evaluate(pred[test_mask], graph.y[test_mask], metric='auto')
        self.note('test_loss', loss.item())
        self.note('test_metric', metric)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        if hasattr(self.model, 'get_optimizer'):
            model_spec_optim = self.model.get_optimizer(cfg)
            if model_spec_optim is not None:
                return model_spec_optim
        return torch.optim.Adam(self.model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


def get_score(embs, node1, node2):
    vector1 = embs[int(node1)]
    vector2 = embs[int(node2)]
    return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))


def evaluate(embs, true_edges, false_edges):
    true_list = list()
    prediction_list = list()
    for edge in true_edges:
        true_list.append(1)
        prediction_list.append(get_score(embs, edge[0], edge[1]))
    for edge in false_edges:
        true_list.append(0)
        prediction_list.append(get_score(embs, edge[0], edge[1]))
    sorted_pred = prediction_list[:]
    sorted_pred.sort()
    threshold = sorted_pred[-len(true_edges)]
    y_pred = np.zeros(len(prediction_list), dtype=np.int32)
    for i in range(len(prediction_list)):
        if prediction_list[i] >= threshold:
            y_pred[i] = 1
    y_true = np.array(true_list)
    y_scores = np.array(prediction_list)
    ps, rs, _ = precision_recall_curve(y_true, y_scores)
    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)


def evaluate_multiplex(all_embs, test_data):
    total_roc_auc, total_f1_score, total_pr_auc = [], [], []
    for key in test_data.keys():
        embs = all_embs[key]
        roc_auc, f1_score, pr_auc = evaluate(embs, test_data[key][0], test_data[key][1])
        total_roc_auc.append(roc_auc)
        total_f1_score.append(f1_score)
        total_pr_auc.append(pr_auc)
    assert len(total_roc_auc) > 0
    roc_auc, f1_score, pr_auc = np.mean(total_roc_auc), np.mean(total_f1_score), np.mean(total_pr_auc)
    None
    return dict(ROC_AUC=roc_auc, PR_AUC=pr_auc, F1=f1_score)


class MultiplexEmbeddingModelWrapper(EmbeddingModelWrapper):

    @staticmethod
    def add_args(parser: argparse.ArgumentParser):
        """Add task-specific arguments to the parser."""
        parser.add_argument('--hidden-size', type=int, default=200)
        parser.add_argument('--eval-type', type=str, default='all', nargs='+')

    def __init__(self, model, hidden_size=200, eval_type='all'):
        super(MultiplexEmbeddingModelWrapper, self).__init__()
        self.model = model
        self.hidden_size = hidden_size
        self.eval_type = eval_type

    def train_step(self, batch):
        if hasattr(self.model, 'multiplicity'):
            all_embs = self.model(batch)
        else:
            all_embs = dict()
            for key in batch.keys():
                if self.eval_type == 'all' or key in self.eval_type:
                    G = Graph(edge_index=torch.LongTensor(batch[key]).transpose(0, 1))
                    embs = self.model(G, return_dict=True)
                all_embs[key] = embs
        return all_embs

    def test_step(self, batch):
        all_embs, test_data = batch
        return evaluate_multiplex(all_embs, test_data)


def get_rank(scores, target):
    _, indices = torch.sort(scores, dim=1, descending=True)
    rank = (indices == target.view(-1, 1)).nonzero()[:, 1]
    return rank.view(-1)


def get_raw_rank(heads, tails, rels, embedding, rel_embedding, batch_size, scoring):
    test_size = heads.shape[0]
    num_batch = (test_size + batch_size - 1) // batch_size
    ranks = []
    for i in range(num_batch):
        start = batch_size * i
        end = start + batch_size
        scores = torch.sigmoid(scoring.predict(embedding[heads[start:end]], embedding, rel_embedding[rels[start:end]]))
        target = tails[start:end]
        rank = get_rank(scores, target).cpu().numpy()
        torch.cuda.empty_cache()
        ranks.append(rank)
    return np.concatenate(ranks).astype(np.float)


def cal_mrr(embedding, rel_embedding, edge_index, edge_type, scoring, protocol='raw', batch_size=1000, hits=[]):
    with torch.no_grad():
        if protocol == 'raw':
            heads = edge_index[0]
            tails = edge_index[1]
            ranks_h = get_raw_rank(heads, tails, edge_type, embedding, rel_embedding, batch_size, scoring)
            ranks_t = get_raw_rank(tails, heads, edge_type, embedding, rel_embedding, batch_size, scoring)
            ranks = np.concatenate((ranks_h, ranks_t)) + 1
        elif protocol == 'filtered':
            raise NotImplementedError
        else:
            raise ValueError
        mrr = (1.0 / ranks).mean()
        hits_count = []
        for hit in hits:
            hits_count.append(np.mean((ranks <= hit).astype(np.float)))
    return mrr, hits_count


class GNNKGLinkPredictionModelWrapper(ModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--score-func', type=str, default='distmult')

    def __init__(self, model, optimizer_cfg, score_func):
        super(GNNKGLinkPredictionModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg
        hidden_size = optimizer_cfg['hidden_size']
        self.score_func = score_func
        if score_func == 'distmult':
            self.scoring = DistMultLayer()
        elif score_func == 'conve':
            self.scoring = ConvELayer(hidden_size)
        else:
            raise NotImplementedError

    def train_step(self, subgraph):
        graph = subgraph
        mask = graph.train_mask
        edge_index = torch.stack(graph.edge_index)
        edge_index, edge_types = edge_index[:, mask], graph.edge_attr[mask]
        with graph.local_graph():
            graph.edge_index = edge_index
            graph.edge_attr = edge_types
            loss = self.model.loss(graph, self.scoring)
        return loss

    def val_step(self, subgraph):
        train_mask = subgraph.train_mask
        eval_mask = subgraph.val_mask
        return self.eval_step(subgraph, train_mask, eval_mask)

    def test_step(self, subgraph):
        infer_mask = subgraph.train_mask | subgraph.val_mask
        eval_mask = subgraph.test_mask
        return self.eval_step(subgraph, infer_mask, eval_mask)

    def eval_step(self, graph, mask1, mask2):
        row, col = graph.edge_index
        edge_types = graph.edge_attr
        with graph.local_graph():
            graph.edge_index = row[mask1], col[mask1]
            graph.edge_attr = edge_types[mask1]
            output, rel_weight = self.model.predict(graph)
        mrr, hits = cal_mrr(output, rel_weight, (row[mask2], col[mask2]), edge_types[mask2], scoring=self.scoring, protocol='raw', batch_size=500, hits=[1, 3, 10])
        return dict(mrr=mrr, hits1=hits[0], hits3=hits[1], hits10=hits[2])

    def setup_optimizer(self):
        lr, weight_decay = self.optimizer_cfg['lr'], self.optimizer_cfg['weight_decay']
        return torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)

    def set_early_stopping(self):
        return 'mrr', '>'


def negative_edge_sampling(edge_index: Union[Tuple, torch.Tensor], num_nodes: Optional[int]=None, num_neg_samples: Optional[int]=None, undirected: bool=False):
    if num_nodes is None:
        num_nodes = len(torch.unique(edge_index))
    if num_neg_samples is None:
        num_neg_samples = edge_index[0].shape[0]
    size = num_nodes * num_nodes
    num_neg_samples = min(num_neg_samples, size - edge_index[1].shape[0])
    row, col = edge_index
    unique_pair = row * num_nodes + col
    num_samples = int(num_neg_samples * abs(1 / (1 - 1.1 * row.size(0) / size)))
    sample_result = torch.LongTensor(random.sample(range(size), min(num_samples, num_samples)))
    mask = torch.from_numpy(np.isin(sample_result, unique_pair.to('cpu')))
    selected = sample_result[~mask][:num_neg_samples]
    row = selected // num_nodes
    col = selected % num_nodes
    return torch.stack([row, col]).long()


class GNNLinkPredictionModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(GNNLinkPredictionModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg
        self.loss_fn = torch.nn.BCELoss()

    def train_step(self, subgraph):
        graph = subgraph
        train_neg_edges = negative_edge_sampling(graph.train_edges, graph.num_nodes)
        train_pos_edges = graph.train_edges
        edge_index = torch.cat([train_pos_edges, train_neg_edges], dim=1)
        labels = self.get_link_labels(train_pos_edges.shape[1], train_neg_edges.shape[1], self.device)
        with graph.local_graph():
            graph.edge_index = edge_index
            emb = self.model(graph)
        pred = (emb[edge_index[0]] * emb[edge_index[1]]).sum(1)
        pred = torch.sigmoid(pred)
        loss = self.loss_fn(pred, labels)
        return loss

    def val_step(self, subgraph):
        graph = subgraph
        pos_edges = graph.val_edges
        neg_edges = graph.val_neg_edges
        train_edges = graph.train_edges
        edges = torch.cat([pos_edges, neg_edges], dim=1)
        labels = self.get_link_labels(pos_edges.shape[1], neg_edges.shape[1], self.device).long()
        with graph.local_graph():
            graph.edge_index = train_edges
            with torch.no_grad():
                emb = self.model(graph)
                pred = (emb[edges[0]] * emb[edges[1]]).sum(-1)
        pred = torch.sigmoid(pred)
        auc_score = roc_auc_score(labels.cpu().numpy(), pred.cpu().numpy())
        self.note('auc', auc_score)

    def test_step(self, subgraph):
        graph = subgraph
        pos_edges = graph.test_edges
        neg_edges = graph.test_neg_edges
        train_edges = graph.train_edges
        edges = torch.cat([pos_edges, neg_edges], dim=1)
        labels = self.get_link_labels(pos_edges.shape[1], neg_edges.shape[1], self.device).long()
        with graph.local_graph():
            graph.edge_index = train_edges
            with torch.no_grad():
                emb = self.model(graph)
                pred = (emb[edges[0]] * emb[edges[1]]).sum(-1)
        pred = torch.sigmoid(pred)
        auc_score = roc_auc_score(labels.cpu().numpy(), pred.cpu().numpy())
        self.note('auc', auc_score)

    @staticmethod
    def get_link_labels(num_pos, num_neg, device=None):
        labels = torch.zeros(num_pos + num_neg)
        labels[:num_pos] = 1
        if device is not None:
            labels = labels
        return labels.float()

    def setup_optimizer(self):
        lr, wd = self.optimizer_cfg['lr'], self.optimizer_cfg['weight_decay']
        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=wd)

    def set_early_stopping(self):
        return 'auc', '>'


class TripleModelWrapper(ModelWrapper):

    @classmethod
    def add_args(self, parser):
        parser.add_argument('--negative_adversarial_sampling', default=False)
        parser.add_argument('--negative_sample_size', type=int, default=128)
        parser.add_argument('--uni_weight', action='store_true', help='Otherwise use subsampling weighting like in word2vec')
        parser.add_argument('--regularization', default=1e-09, type=float)
        parser.add_argument('--lr', default=0.001, type=float)
        parser.add_argument('--adversarial_temperature', default=1.0, type=float)
        parser.add_argument('--save-emb-path', default='./checkpoints')
        parser.add_argument('--eval-step', type=int, default=501)
        parser.add_argument('--do_test', default=True)
        parser.add_argument('--do_valid', default=True)
        self.parser = parser
        return self.parser

    def __init__(self, model, optimizer_cfg):
        super(TripleModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg
        self.args = self.parser.parse_args()

    def train_step(self, subgraph):
        """
        A single train step. Apply back-propation and return the loss
        """
        train_iterator = subgraph
        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)
        positive_sample = positive_sample
        negative_sample = negative_sample
        subsampling_weight = subsampling_weight
        negative_score = self.model((positive_sample, negative_sample), mode=mode)
        if self.args.negative_adversarial_sampling:
            negative_score = (F.softmax(negative_score * self.args.adversarial_temperature, dim=1).detach() * F.logsigmoid(-negative_score)).sum(dim=1)
        else:
            negative_score = F.logsigmoid(-negative_score).mean(dim=1)
        positive_score = self.model(positive_sample)
        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)
        if self.args.uni_weight:
            positive_sample_loss = -positive_score.mean()
            negative_sample_loss = -negative_score.mean()
        else:
            positive_sample_loss = -(subsampling_weight * positive_score).sum() / subsampling_weight.sum()
            negative_sample_loss = -(subsampling_weight * negative_score).sum() / subsampling_weight.sum()
        loss = (positive_sample_loss + negative_sample_loss) / 2
        if self.args.regularization != 0.0:
            regularization = self.args.regularization * (self.model.entity_embedding.norm(p=3) ** 3 + self.model.relation_embedding.norm(p=3).norm(p=3) ** 3)
            loss = loss + regularization
        return loss

    def test_step(self, subgraph):
        None
        metrics = self.eval_step(subgraph)
        return dict(mrr=metrics['MRR'], mr=metrics['MR'], hits1=metrics['HITS@1'], hits3=metrics['HITS@3'], hits10=metrics['HITS@10'])

    def val_step(self, subgraph):
        None
        metrics = self.eval_step(subgraph)
        return dict(mrr=metrics['MRR'], mr=metrics['MR'], hits1=metrics['HITS@1'], hits3=metrics['HITS@3'], hits10=metrics['HITS@10'])

    def eval_step(self, subgraph):
        test_dataloader_head, test_dataloader_tail = subgraph
        logs = []
        test_dataset_list = [test_dataloader_head, test_dataloader_tail]
        for test_dataset in test_dataset_list:
            pbar = tqdm(test_dataset)
            for positive_sample, negative_sample, filter_bias, mode in pbar:
                pbar.set_description('Evaluating the model: Use mode({})'.format(mode))
                positive_sample = positive_sample
                negative_sample = negative_sample
                filter_bias = filter_bias
                batch_size = positive_sample.size(0)
                score = self.model((positive_sample, negative_sample), mode)
                score += filter_bias
                argsort = torch.argsort(score, dim=1, descending=True)
                if mode == 'head-batch':
                    positive_arg = positive_sample[:, 0]
                elif mode == 'tail-batch':
                    positive_arg = positive_sample[:, 2]
                else:
                    raise ValueError('mode %s not supported' % mode)
                for i in range(batch_size):
                    ranking = (argsort[i, :] == positive_arg[i]).nonzero()
                    assert ranking.size(0) == 1
                    ranking = 1 + ranking.item()
                    logs.append({'MRR': 1.0 / ranking, 'MR': float(ranking), 'HITS@1': 1.0 if ranking <= 1 else 0.0, 'HITS@3': 1.0 if ranking <= 3 else 0.0, 'HITS@10': 1.0 if ranking <= 10 else 0.0})
        metrics = {}
        for metric in logs[0].keys():
            metrics[metric] = sum([log[metric] for log in logs]) / len(logs)
        None
        return metrics

    def setup_optimizer(self):
        lr, weight_decay = self.optimizer_cfg['lr'], self.optimizer_cfg['weight_decay']
        return torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)

    def set_early_stopping(self):
        return 'mrr', '>'


class LogReg(nn.Module):

    def __init__(self, ft_in, nb_classes):
        super(LogReg, self).__init__()
        self.fc = nn.Linear(ft_in, nb_classes)
        for m in self.modules():
            self.weights_init(m)

    def weights_init(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight.data)
            if m.bias is not None:
                m.bias.data.fill_(0.0)

    def forward(self, seq):
        ret = self.fc(seq)
        return ret


def accuracy(y_pred, y_true):
    y_true = y_true.squeeze().long()
    preds = y_pred.max(1)[1].type_as(y_true)
    correct = preds.eq(y_true).double()
    correct = correct.sum().item()
    return correct / len(y_true)


def multilabel_f1(y_pred, y_true, sigmoid=False):
    if sigmoid:
        y_pred[y_pred > 0.5] = 1
        y_pred[y_pred <= 0.5] = 0
    else:
        y_pred[y_pred > 0] = 1
        y_pred[y_pred <= 0] = 0
    tp = (y_true * y_pred).sum()
    fp = ((1 - y_true) * y_pred).sum()
    fn = (y_true * (1 - y_pred)).sum()
    epsilon = 1e-07
    precision = tp / (tp + fp + epsilon)
    recall = tp / (tp + fn + epsilon)
    f1 = 2 * precision * recall / (precision + recall + epsilon)
    return f1.item()


class LogRegTrainer(object):

    def train(self, data, labels, idx_train, idx_test, loss_fn=None, evaluator=None, run=20):
        device = data.device
        nhid = data.shape[-1]
        labels = labels
        train_embs = data[idx_train]
        test_embs = data[idx_test]
        train_lbls = labels[idx_train]
        test_lbls = labels[idx_test]
        tot = 0
        num_classes = int(labels.max()) + 1
        if loss_fn is None:
            loss_fn = nn.CrossEntropyLoss() if len(labels.shape) == 1 else nn.BCEWithLogitsLoss()
        if evaluator is None:
            evaluator = accuracy if len(labels.shape) == 1 else multilabel_f1
        for _ in range(run):
            log = LogReg(nhid, num_classes)
            optimizer = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)
            log
            for _ in range(100):
                log.train()
                optimizer.zero_grad()
                logits = log(train_embs)
                loss = loss_fn(logits, train_lbls)
                loss.backward()
                optimizer.step()
            log.eval()
            with torch.no_grad():
                logits = log(test_embs)
            metric = evaluator(logits, test_lbls)
            tot += metric
        return tot / run


def evaluate_node_embeddings_using_logreg(data, labels, train_idx, test_idx, run=20):
    result = LogRegTrainer().train(data, labels, train_idx, test_idx, run=run)
    return result


class DGIModelWrapper(UnsupervisedModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--hidden-size', type=int, default=512)

    def __init__(self, model, optimizer_cfg):
        super(DGIModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg
        self.read = AvgReadout()
        self.sigm = nn.Sigmoid()
        hidden_size = optimizer_cfg['hidden_size']
        assert hidden_size > 0
        self.disc = Discriminator(hidden_size)
        self.loss_fn = torch.nn.BCEWithLogitsLoss()
        self.act = nn.PReLU()

    def train_step(self, subgraph):
        graph = subgraph
        graph.sym_norm()
        x = graph.x
        shuffle_x = self.augment(graph)
        graph.x = x
        h_pos = self.act(self.model(graph))
        c = self.read(h_pos)
        c = self.sigm(c)
        graph.x = shuffle_x
        h_neg = self.act(self.model(graph))
        logits = self.disc(c, h_pos, h_neg)
        graph.x = x
        num_nodes = x.shape[0]
        labels = torch.zeros((num_nodes * 2,), device=x.device)
        labels[:num_nodes] = 1
        loss = self.loss_fn(logits, labels)
        return loss

    def test_step(self, graph):
        with torch.no_grad():
            pred = self.act(self.model(graph))
        y = graph.y
        result = evaluate_node_embeddings_using_logreg(pred, y, graph.train_mask, graph.test_mask)
        self.note('test_acc', result)

    @staticmethod
    def augment(graph):
        idx = np.random.permutation(graph.num_nodes)
        shuffle_x = graph.x[idx, :]
        return shuffle_x

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


def get_current_consistency_weight(final_consistency_weight, rampup_starts, rampup_ends, epoch):
    rampup_length = rampup_ends - rampup_starts
    rampup = 1.0
    epoch = epoch - rampup_starts
    if rampup_length != 0:
        current = np.clip(epoch, 0.0, rampup_length)
        phase = 1.0 - current / rampup_length
        rampup = float(np.exp(-5.0 * phase * phase))
    return final_consistency_weight * rampup


def get_one_hot_label(labels, index):
    num_classes = int(torch.max(labels) + 1)
    target = torch.zeros(labels.shape[0], num_classes)
    target[index, labels[index]] = 1
    return target


def sharpen(prob, temperature):
    prob = torch.pow(prob, 1.0 / temperature)
    row_sum = torch.sum(prob, dim=1).reshape(-1, 1)
    return prob / row_sum


class GCNMixModelWrapper(ModelWrapper):
    """
    GCNMixModelWrapper calls `forward_aux` in model
    `forward_aux` is similar to `forward` but ignores `spmm` operation.
    """

    @staticmethod
    def add_args(parser):
        parser.add_argument('--temperature', type=float, default=0.1)
        parser.add_argument('--rampup-starts', type=int, default=500)
        parser.add_argument('--rampup-ends', type=int, default=1000)
        parser.add_argument('--mixup-consistency', type=float, default=10.0)
        parser.add_argument('--ema-decay', type=float, default=0.999)
        parser.add_argument('--tau', type=float, default=1.0)
        parser.add_argument('--k', type=int, default=10)

    def __init__(self, model, optimizer_cfg, temperature, rampup_starts, rampup_ends, mixup_consistency, ema_decay, tau, k):
        super(GCNMixModelWrapper, self).__init__()
        self.optimizer_cfg = optimizer_cfg
        self.temperature = temperature
        self.ema_decay = ema_decay
        self.tau = tau
        self.k = k
        self.model = model
        self.model_ema = copy.deepcopy(self.model)
        for p in self.model_ema.parameters():
            p.detach_()
        self.epoch = 0
        self.opt = {'epoch': 0, 'final_consistency_weight': mixup_consistency, 'rampup_starts': rampup_starts, 'rampup_ends': rampup_ends}
        self.mix_loss = torch.nn.BCELoss()
        self.mix_transform = None

    def train_step(self, subgraph):
        if self.mix_transform is None:
            if len(subgraph.y.shape) > 1:
                self.mix_transform = torch.nn.Sigmoid()
            else:
                self.mix_transform = torch.nn.Softmax(-1)
        graph = subgraph
        device = graph.x.device
        train_mask = graph.train_mask
        self.opt['epoch'] += 1
        rand_n = random.randint(0, 1)
        if rand_n == 0:
            vector_labels = get_one_hot_label(graph.y, train_mask)
            loss = self.update_aux(graph, vector_labels, train_mask)
        else:
            loss = self.update_soft(graph)
        alpha = min(1 - 1 / (self.epoch + 1), self.ema_decay)
        for ema_param, param in zip(self.model_ema.parameters(), self.model.parameters()):
            ema_param.data.mul_(alpha).add_((1 - alpha) * param.data)
        return loss

    def val_step(self, subgraph):
        graph = subgraph
        val_mask = graph.val_mask
        pred = self.model_ema(graph)
        loss = self.default_loss_fn(pred[val_mask], graph.y[val_mask])
        metric = self.evaluate(pred[val_mask], graph.y[val_mask], metric='auto')
        self.note('val_loss', loss.item())
        self.note('val_metric', metric)

    def test_step(self, subgraph):
        test_mask = subgraph.test_mask
        pred = self.model_ema(subgraph)
        loss = self.default_loss_fn(pred[test_mask], subgraph.y[test_mask])
        metric = self.evaluate(pred[test_mask], subgraph.y[test_mask], metric='auto')
        self.note('test_loss', loss.item())
        self.note('test_metric', metric)

    def update_soft(self, graph):
        out = self.model(graph)
        train_mask = graph.train_mask
        loss_sum = self.default_loss_fn(out[train_mask], graph.y[train_mask])
        return loss_sum

    def update_aux(self, data, vector_labels, train_index):
        device = self.device
        train_unlabelled = torch.where(~data.train_mask)[0]
        temp_labels = torch.zeros(self.k, vector_labels.shape[0], vector_labels.shape[1])
        with torch.no_grad():
            for i in range(self.k):
                temp_labels[i, :, :] = self.model(data) / self.tau
        target_labels = temp_labels.mean(dim=0)
        target_labels = sharpen(target_labels, self.temperature)
        vector_labels[train_unlabelled] = target_labels[train_unlabelled]
        sampled_unlabelled = torch.randint(0, train_unlabelled.shape[0], size=(train_index.shape[0],))
        train_unlabelled = train_unlabelled[sampled_unlabelled]

        def get_loss(index):
            mix_logits, target = self.model.forward_aux(data.x, vector_labels, index, mix_hidden=True)
            temp_loss = self.mix_loss(self.mix_transform(mix_logits[index]), target)
            return temp_loss
        sup_loss = get_loss(train_index)
        unsup_loss = get_loss(train_unlabelled)
        mixup_weight = get_current_consistency_weight(self.opt['final_consistency_weight'], self.opt['rampup_starts'], self.opt['rampup_ends'], self.opt['epoch'])
        loss_sum = sup_loss + mixup_weight * unsup_loss
        return loss_sum

    def setup_optimizer(self):
        lr = self.optimizer_cfg['lr']
        wd = self.optimizer_cfg['weight_decay']
        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=wd)


class GraphSAGEModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(GraphSAGEModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg

    def train_step(self, batch):
        x_src, y, adjs = batch
        pred = self.model(x_src, adjs)
        loss = self.default_loss_fn(pred, y)
        return loss

    def val_step(self, batch):
        x_src, y, adjs = batch
        pred = self.model(x_src, adjs)
        loss = self.default_loss_fn(pred, y)
        metric = self.evaluate(pred, y, metric='auto')
        self.note('val_loss', loss.item())
        self.note('val_metric', metric)

    def test_step(self, batch):
        dataset, test_loader = batch
        graph = dataset.data
        if hasattr(self.model, 'inference'):
            pred = self.model.inference(graph.x, test_loader)
        else:
            pred = self.model(graph)
        pred = pred[graph.test_mask]
        y = graph.y[graph.test_mask]
        metric = self.evaluate(pred, y, metric='auto')
        self.note('test_loss', self.default_loss_fn(pred, y))
        self.note('test_metric', metric)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


class MVGRLModelWrapper(UnsupervisedModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(MVGRLModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg
        self.loss_f = nn.BCEWithLogitsLoss()

    def train_step(self, subgraph):
        graph = subgraph
        logits = self.model(graph)
        labels = torch.zeros_like(logits)
        num_outs = logits.shape[1]
        labels[:, :num_outs // 2] = 1
        loss = self.loss_f(logits, labels)
        return loss

    def test_step(self, graph):
        with torch.no_grad():
            pred = self.model(graph)
        y = graph.y
        result = evaluate_node_embeddings_using_logreg(pred, y, graph.train_mask, graph.test_mask)
        self.note('test_acc', result)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


class NodeClfModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(NodeClfModelWrapper, self).__init__()
        self.optimizer_cfg = optimizer_cfg
        self.model = model

    def train_step(self, subgraph):
        graph = subgraph
        pred = self.model(graph)
        train_mask = graph.train_mask
        loss = self.default_loss_fn(pred[train_mask], graph.y[train_mask])
        return loss

    def val_step(self, subgraph):
        graph = subgraph
        pred = self.model(graph)
        y = graph.y
        val_mask = graph.val_mask
        loss = self.default_loss_fn(pred[val_mask], y[val_mask])
        metric = self.evaluate(pred[val_mask], graph.y[val_mask], metric='auto')
        self.note('val_loss', loss.item())
        self.note('val_metric', metric)

    def test_step(self, batch):
        graph = batch
        pred = self.model(graph)
        test_mask = batch.test_mask
        loss = self.default_loss_fn(pred[test_mask], batch.y[test_mask])
        metric = self.evaluate(pred[test_mask], batch.y[test_mask], metric='auto')
        self.note('test_loss', loss.item())
        self.note('test_metric', metric)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        if hasattr(self.model, 'setup_optimizer'):
            model_spec_optim = self.model.setup_optimizer(cfg)
            if model_spec_optim is not None:
                return model_spec_optim
        return torch.optim.Adam(self.model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])

    def set_early_stopping(self):
        return 'val_metric', '>'


class PPRGoModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(PPRGoModelWrapper, self).__init__()
        self.optimizer_cfg = optimizer_cfg
        self.model = model

    def train_step(self, batch):
        x, targets, ppr_scores, y = batch
        pred = self.model(x, targets, ppr_scores)
        loss = self.default_loss_fn(pred, y)
        return loss

    def val_step(self, batch):
        graph = batch
        if isinstance(batch, list):
            x, targets, ppr_scores, y = batch
            pred = self.model(x, targets, ppr_scores)
        else:
            pred = self.model.predict(graph)
            y = graph.y[graph.val_mask]
            pred = pred[graph.val_mask]
        loss = self.default_loss_fn(pred, y)
        metric = self.evaluate(pred, y, metric='auto')
        self.note('val_loss', loss.item())
        self.note('val_metric', metric)

    def test_step(self, batch):
        graph = batch
        if isinstance(batch, list):
            x, targets, ppr_scores, y = batch
            pred = self.model(x, targets, ppr_scores)
        else:
            pred = self.model.predict(graph)
            test_mask = batch.test_mask
            pred = pred[test_mask]
            y = graph.y[test_mask]
        loss = self.default_loss_fn(pred, y)
        self.note('test_loss', loss.item())
        self.note('test_metric', self.evaluate(pred, y))

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


class SAGNModelWrapper(ModelWrapper):

    def __init__(self, model, optimizer_cfg):
        super(SAGNModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg

    def train_step(self, batch):
        batch_x, batch_y_emb, y = batch
        pred = self.model(batch_x, batch_y_emb)
        loss = self.default_loss_fn(pred, y)
        return loss

    def val_step(self, batch):
        batch_x, batch_y_emb, y = batch
        pred = self.model(batch_x, batch_y_emb)
        metric = self.evaluate(pred, y, metric='auto')
        self.note('val_loss', self.default_loss_fn(pred, y))
        self.note('val_metric', metric)

    def test_step(self, batch):
        batch_x, batch_y_emb, y = batch
        pred = self.model(batch_x, batch_y_emb)
        metric = self.evaluate(pred, y, metric='auto')
        self.note('test_loss', self.default_loss_fn(pred, y))
        self.note('test_metric', metric)

    def pre_stage(self, stage, data_w):
        device = next(self.model.parameters()).device
        if stage == 0:
            return None
        self.model.eval()
        preds = []
        eval_loader = data_w.post_stage_wrapper()
        with torch.no_grad():
            for batch in eval_loader:
                batch_x, batch_y_emb, _ = data_w.pre_stage_transform(batch)
                batch_x = batch_x
                batch_y_emb = batch_y_emb if batch_y_emb is not None else batch_y_emb
                pred = self.model(batch_x, batch_y_emb)
                preds.append(pred)
        probs = torch.cat(preds, dim=0)
        return probs

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


class SSLTask:

    def __init__(self, device):
        self.device = device
        self.cached_edges = None

    def transform_data(self, graph):
        raise NotImplementedError

    def make_loss(self, embeddings):
        raise NotImplementedError


class AttributeMask(SSLTask):

    def __init__(self, graph, hidden_size, train_mask, mask_ratio, device):
        super().__init__(device)
        self.linear = nn.Linear(hidden_size, graph.x.shape[1])
        self.cached_features = None
        self.mask_ratio = mask_ratio

    def transform_data(self, graph):
        device = graph.x.device
        x_feat = graph.x
        num_nodes = graph.num_nodes
        unlabelled = torch.where(~graph.train_mask)[0]
        perm = np.random.permutation(unlabelled.cpu().numpy())
        mask_nnz = int(num_nodes * self.mask_ratio)
        self.masked_nodes = perm[:mask_nnz]
        x_feat[self.masked_nodes] = 0
        self.pseudo_labels = x_feat[self.masked_nodes]
        graph.x = x_feat
        return graph

    def make_loss(self, embeddings):
        embeddings = self.linear(embeddings[self.masked_nodes])
        loss = F.mse_loss(embeddings, self.pseudo_labels, reduction='mean')
        return loss


class Distance2Clusters(SSLTask):

    def __init__(self, hidden_size, num_clusters, device):
        super().__init__(device)
        self.num_clusters = num_clusters
        self.linear = nn.Linear(hidden_size, num_clusters)
        self.gen_cluster_info_cache = False

    def transform_data(self, graph):
        if not self.gen_cluster_info_cache:
            self.gen_cluster_info(graph)
            self.gen_cluster_info_cache = True
        return graph

    def gen_cluster_info(self, graph, use_metis=False):
        edge_index = graph.edge_index
        num_nodes = graph.num_nodes
        x = graph.x
        G = nx.Graph()
        G.add_edges_from(torch.stack(edge_index).cpu().numpy().transpose())
        if use_metis:
            _, parts = metis.part_graph(G, self.num_clusters)
        else:
            from sklearn.cluster import KMeans
            clustering = KMeans(n_clusters=self.num_clusters, random_state=0).fit(x.cpu())
            parts = clustering.labels_
        node_clusters = [[] for i in range(self.num_clusters)]
        for i, p in enumerate(parts):
            node_clusters[p].append(i)
        self.central_nodes = np.array([])
        self.distance_vec = np.zeros((num_nodes, self.num_clusters))
        for i in range(self.num_clusters):
            subgraph = G.subgraph(node_clusters[i])
            center = None
            for node in subgraph.nodes:
                if center is None or subgraph.degree[node] > subgraph.degree[center]:
                    center = node
            np.append(self.central_nodes, center)
            distance = dict(nx.shortest_path_length(G, source=center))
            for node in distance:
                self.distance_vec[node][i] = distance[node]
        self.distance_vec = torch.tensor(self.distance_vec).float()

    def make_loss(self, embeddings):
        output = self.linear(embeddings)
        return F.mse_loss(output, self.distance_vec, reduction='mean')


class EdgeMask(SSLTask):

    def __init__(self, hidden_size, mask_ratio, device):
        super().__init__(device)
        self.linear = nn.Linear(hidden_size, 2)
        self.mask_ratio = mask_ratio

    def transform_data(self, graph):
        device = graph.x.device
        num_edges = graph.num_edges
        row, col = graph.edge_index
        edges = torch.stack([row, col])
        perm = np.random.permutation(num_edges)
        preserve_nnz = int(num_edges * (1 - self.mask_ratio))
        masked = perm[preserve_nnz:]
        preserved = perm[:preserve_nnz]
        self.masked_edges = edges[:, masked]
        self.cached_edges = edges[:, preserved]
        mask_num = len(masked)
        self.neg_edges = self.neg_sample(mask_num, graph)
        self.pseudo_labels = torch.cat([torch.ones(mask_num), torch.zeros(mask_num)]).long()
        self.node_pairs = torch.cat([self.masked_edges, self.neg_edges], 1)
        graph.edge_index = self.cached_edges
        return graph

    def make_loss(self, embeddings):
        embeddings = self.linear(torch.abs(embeddings[self.node_pairs[0]] - embeddings[self.node_pairs[1]]))
        output = F.log_softmax(embeddings, dim=1)
        return F.nll_loss(output, self.pseudo_labels)

    def neg_sample(self, edge_num, graph):
        edge_index = graph.edge_index
        num_nodes = graph.num_nodes
        edges = torch.stack(edge_index).t().cpu().numpy()
        exclude = set([(_[0], _[1]) for _ in list(edges)])
        itr = self.sample(exclude, num_nodes)
        sampled = [next(itr) for _ in range(edge_num)]
        return torch.tensor(sampled).t()

    def sample(self, exclude, num_nodes):
        while True:
            t = tuple(np.random.randint(0, num_nodes, 2))
            if t[0] != t[1] and t not in exclude:
                exclude.add(t)
                exclude.add((t[1], t[0]))
                yield t


class PairwiseAttrSim(SSLTask):

    def __init__(self, hidden_size, k, device):
        super().__init__(device)
        self.k = k
        self.linear = nn.Linear(hidden_size, 1)
        self.get_attr_sim_cache = False

    def get_avg_distance(self, graph, idx_sorted, k, sampled):
        edge_index = graph.edge_index
        num_nodes = graph.num_nodes
        self.G = nx.Graph()
        self.G.add_edges_from(torch.stack(edge_index).cpu().numpy().transpose())
        avg_min = 0
        avg_max = 0
        avg_sampled = 0
        for i in range(num_nodes):
            distance = dict(nx.shortest_path_length(self.G, source=i))
            sum = 0
            num = 0
            for node in idx_sorted[i, :k]:
                if node in distance:
                    sum += distance[node]
                    num += 1
            if num:
                avg_min += sum / num / num_nodes
            sum = 0
            num = 0
            for node in idx_sorted[i, -k - 1:]:
                if node in distance:
                    sum += distance[node]
                    num += 1
            if num:
                avg_max += sum / num / num_nodes
            sum = 0
            num = 0
            for node in idx_sorted[i, sampled]:
                if node in distance:
                    sum += distance[node]
                    num += 1
            if num:
                avg_sampled += sum / num / num_nodes
        return avg_min, avg_max, avg_sampled

    def get_attr_sim(self, graph):
        x = graph.x
        num_nodes = graph.num_nodes
        from sklearn.metrics.pairwise import cosine_similarity
        sims = cosine_similarity(x.cpu().numpy())
        idx_sorted = sims.argsort(1)
        self.node_pairs = None
        self.pseudo_labels = None
        sampled = self.sample(self.k, num_nodes)
        for i in range(num_nodes):
            for node in np.hstack((idx_sorted[i, :self.k], idx_sorted[i, -self.k - 1:], idx_sorted[i, sampled])):
                pair = torch.tensor([[i, node]])
                sim = torch.tensor([sims[i][node]])
                self.node_pairs = pair if self.node_pairs is None else torch.cat([self.node_pairs, pair], 0)
                self.pseudo_labels = sim if self.pseudo_labels is None else torch.cat([self.pseudo_labels, sim])
        None
        self.node_pairs = self.node_pairs.long()
        self.pseudo_labels = self.pseudo_labels.float()

    def sample(self, k, num_nodes):
        sampled = []
        for i in range(k):
            sampled.append(int(random.random() * (num_nodes - self.k * 2)) + self.k)
        return np.array(sampled)

    def transform_data(self, graph):
        if not self.get_attr_sim_cache:
            self.get_attr_sim(graph)
        return graph

    def make_loss(self, embeddings):
        node_pairs = self.node_pairs
        output = self.linear(torch.abs(embeddings[node_pairs[0]] - embeddings[node_pairs[1]]))
        return F.mse_loss(output, self.pseudo_labels, reduction='mean')


class PairwiseDistance(SSLTask):

    def __init__(self, hidden_size, class_split, sampling, dropedge_rate, num_centers, device):
        super().__init__(device)
        self.nclass = len(class_split) + 1
        self.class_split = class_split
        self.max_distance = self.class_split[self.nclass - 2][1]
        self.sampling = sampling
        self.dropedge_rate = dropedge_rate
        self.num_centers = num_centers
        self.linear = nn.Linear(hidden_size, self.nclass)
        self.get_distance_cache = False

    def get_distance(self, graph):
        num_nodes = graph.num_nodes
        num_edges = graph.num_edges
        edge_index = graph.edge_index
        if self.sampling:
            self.dis_node_pairs = [[] for i in range(self.nclass)]
            node_idx = random.sample(range(num_nodes), self.num_centers)
            adj = sp.coo_matrix((np.ones(num_edges), (edge_index[0].cpu().numpy(), edge_index[1].cpu().numpy())), shape=(num_nodes, num_nodes)).tocsr()
            num_samples = tqdm(range(self.num_centers))
            for i in num_samples:
                num_samples.set_description(f'Generating node pairs {i:03d}')
                idx = node_idx[i]
                queue = [idx]
                dis = -np.ones(num_nodes)
                dis[idx] = 0
                head = 0
                tail = 0
                cur_class = 0
                stack = []
                while head <= tail:
                    u = queue[head]
                    if cur_class != self.nclass - 1 and dis[u] >= self.class_split[cur_class][1]:
                        sampled = random.sample(stack, 1024) if len(stack) > 1024 else stack
                        if self.dis_node_pairs[cur_class] == []:
                            self.dis_node_pairs[cur_class] = np.array([[idx] * len(sampled), sampled]).transpose()
                        else:
                            self.dis_node_pairs[cur_class] = np.concatenate((self.dis_node_pairs[cur_class], np.array([[idx] * len(sampled), sampled]).transpose()), axis=0)
                        cur_class += 1
                        if cur_class == self.nclass - 1:
                            break
                        stack = []
                    if u != idx:
                        stack.append(u)
                    head += 1
                    i_s = adj.indptr[u]
                    i_e = adj.indptr[u + 1]
                    for i in range(i_s, i_e):
                        v = adj.indices[i]
                        if dis[v] == -1:
                            dis[v] = dis[u] + 1
                            tail += 1
                            queue.append(v)
                remain = list(np.where(dis == -1)[0])
                sampled = random.sample(remain, 1024) if len(remain) > 1024 else remain
                if self.dis_node_pairs[cur_class] == []:
                    self.dis_node_pairs[cur_class] = np.array([[idx] * len(sampled), sampled]).transpose()
                else:
                    self.dis_node_pairs[cur_class] = np.concatenate((self.dis_node_pairs[cur_class], np.array([[idx] * len(sampled), sampled]).transpose()), axis=0)
            if self.class_split[0][1] == 2:
                self.dis_node_pairs[0] = torch.stack(edge_index).cpu().numpy().transpose()
            num_per_class = np.min(np.array([len(dis) for dis in self.dis_node_pairs]))
            for i in range(self.nclass):
                sampled = np.random.choice(np.arange(len(self.dis_node_pairs[i])), num_per_class, replace=False)
                self.dis_node_pairs[i] = self.dis_node_pairs[i][sampled]
        else:
            G = nx.Graph()
            G.add_edges_from(torch.stack(edge_index).cpu().numpy().transpose())
            path_length = dict(nx.all_pairs_shortest_path_length(G, cutoff=self.max_distance))
            distance = -np.ones((num_nodes, num_nodes), dtype=np.int)
            for u, p in path_length.items():
                for v, d in p.items():
                    distance[u][v] = d - 1
            self.distance = distance
            self.dis_node_pairs = []
            for i in range(self.nclass - 1):
                tmp = np.array(np.where((distance >= self.class_split[i][0]) * (distance < self.class_split[i][1]))).transpose()
                np.random.shuffle(tmp)
                self.dis_node_pairs.append(tmp)
            tmp = np.array(np.where(distance == -1)).transpose()
            np.random.shuffle(tmp)
            self.dis_node_pairs.append(tmp)

    def transform_data(self, graph):
        if not self.get_distance_cache:
            self.get_distance(graph)
            self.get_distance_cache = True
        graph.edge_index, _ = dropout_adj(edge_index=graph.edge_index, drop_rate=self.dropedge_rate)
        return graph

    def make_loss(self, embeddings, sample=True, k=4000):
        node_pairs, pseudo_labels = self.sample(sample, k)
        embeddings = self.linear(torch.abs(embeddings[node_pairs[0]] - embeddings[node_pairs[1]]))
        output = F.log_softmax(embeddings, dim=1)
        return F.nll_loss(output, pseudo_labels)

    def sample(self, sample, k):
        sampled = torch.tensor([]).long()
        pseudo_labels = torch.tensor([]).long()
        for i in range(self.nclass):
            tmp = self.dis_node_pairs[i]
            if sample:
                x = int(random.random() * (len(tmp) - k))
                sampled = torch.cat([sampled, torch.tensor(tmp[x:x + k]).long().t()], 1)
                """
                indices = np.random.choice(np.arange(len(tmp)), k, replace=False)
                sampled = torch.cat([sampled, torch.tensor(tmp[indices]).long().t()], 1)
                """
                pseudo_labels = torch.cat([pseudo_labels, torch.ones(k).long() * i])
            else:
                sampled = torch.cat([sampled, torch.tensor(tmp).long().t()], 1)
                pseudo_labels = torch.cat([pseudo_labels, torch.ones(len(tmp)).long() * i])
        return sampled, pseudo_labels


class SelfAuxiliaryModelWrapper(UnsupervisedModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--auxiliary-task', type=str, default='edge_mask', help='Option: edge_mask, attribute_mask, distance2clusters, pairwise_distance, pairwise_attr_sim')
        parser.add_argument('--dropedge-rate', type=float, default=0.0)
        parser.add_argument('--mask-ratio', type=float, default=0.1)
        parser.add_argument('--sampling', action='store_true')

    def __init__(self, model, optimizer_cfg, auxiliary_task, dropedge_rate, mask_ratio, sampling):
        super(SelfAuxiliaryModelWrapper, self).__init__()
        self.auxiliary_task = auxiliary_task
        self.optimizer_cfg = optimizer_cfg
        self.hidden_size = optimizer_cfg['hidden_size']
        self.dropedge_rate = dropedge_rate
        self.mask_ratio = mask_ratio
        self.sampling = sampling
        self.model = model
        self.agent = None

    def train_step(self, subgraph):
        graph = subgraph
        with graph.local_graph():
            graph = self.agent.transform_data(graph)
            pred = self.model(graph)
        sup_loss = self.default_loss_fn(pred, graph.y)
        pred = self.model.embed(graph)
        ssl_loss = self.agent.make_loss(pred)
        return sup_loss + ssl_loss

    def test_step(self, graph):
        self.model.eval()
        with torch.no_grad():
            pred = self.model.embed(graph)
        y = graph.y
        result = evaluate_node_embeddings_using_logreg(pred, y, graph.train_mask, graph.test_mask)
        self.note('test_acc', result)

    def pre_stage(self, stage, data_w):
        if stage == 0:
            data = data_w.get_dataset().data
            self.generate_virtual_labels(data)

    def generate_virtual_labels(self, data):
        if self.auxiliary_task == 'edge_mask':
            self.agent = EdgeMask(self.hidden_size, self.mask_ratio, self.device)
        elif self.auxiliary_task == 'attribute_mask':
            self.agent = AttributeMask(data, self.hidden_size, data.train_mask, self.mask_ratio, self.device)
        elif self.auxiliary_task == 'pairwise_distance':
            self.agent = PairwiseDistance(self.hidden_size, [(1, 2), (2, 3), (3, 5)], self.sampling, self.dropedge_rate, 256, self.device)
        elif self.auxiliary_task == 'distance2clusters':
            self.agent = Distance2Clusters(self.hidden_size, 30, self.device)
        elif self.auxiliary_task == 'pairwise_attr_sim':
            self.agent = PairwiseAttrSim(self.hidden_size, 5, self.device)
        else:
            raise Exception('auxiliary task must be edge_mask, attribute_mask, pairwise_distance, distance2clusters,or pairwise_attr_sim')

    def setup_optimizer(self):
        lr, wd = self.optimizer_cfg['lr'], self.optimizer_cfg['weight_decay']
        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=wd)


class UnsupGraphSAGEModelWrapper(UnsupervisedModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--walk-length', type=int, default=10)
        parser.add_argument('--negative-samples', type=int, default=30)

    def __init__(self, model, optimizer_cfg, walk_length, negative_samples):
        super(UnsupGraphSAGEModelWrapper, self).__init__()
        self.model = model
        self.optimizer_cfg = optimizer_cfg
        self.walk_length = walk_length
        self.num_negative_samples = negative_samples

    def train_step(self, batch):
        x_src, adjs = batch
        out = self.model(x_src, adjs)
        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)
        pos_loss = torch.log(torch.sigmoid((out * pos_out).sum(-1)).mean())
        neg_loss = torch.log(torch.sigmoid(-(out * neg_out).sum(-1)).mean())
        loss = -pos_loss - neg_loss
        return loss

    def test_step(self, batch):
        dataset, test_loader = batch
        graph = dataset.data
        if hasattr(self.model, 'inference'):
            pred = self.model.inference(graph.x, test_loader)
        else:
            pred = self.model(graph)
        pred = pred.split(pred.size(0) // 3, dim=0)[0]
        pred = pred[graph.test_mask]
        y = graph.y[graph.test_mask]
        metric = self.evaluate(pred, y, metric='auto')
        self.note('test_loss', self.default_loss_fn(pred, y))
        self.note('test_metric', metric)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        return torch.optim.Adam(self.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])


class MemoryMoCo(nn.Module):
    """Fixed-size queue with momentum encoder"""

    def __init__(self, inputSize, outputSize, K, T=0.07, use_softmax=False):
        super(MemoryMoCo, self).__init__()
        self.outputSize = outputSize
        self.inputSize = inputSize
        self.queueSize = K
        self.T = T
        self.index = 0
        self.use_softmax = use_softmax
        self.register_buffer('params', torch.tensor([-1]))
        stdv = 1.0 / math.sqrt(inputSize / 3)
        self.register_buffer('memory', torch.rand(self.queueSize, inputSize).mul_(2 * stdv).add_(-stdv))
        None

    def forward(self, q, k):
        batchSize = q.shape[0]
        k = k.detach()
        Z = self.params[0].item()
        l_pos = torch.bmm(q.view(batchSize, 1, -1), k.view(batchSize, -1, 1))
        l_pos = l_pos.view(batchSize, 1)
        queue = self.memory.clone()
        l_neg = torch.mm(queue.detach(), q.transpose(1, 0))
        l_neg = l_neg.transpose(0, 1)
        out = torch.cat((l_pos, l_neg), dim=1)
        if self.use_softmax:
            out = torch.div(out, self.T)
            out = out.squeeze().contiguous()
        else:
            out = torch.exp(torch.div(out, self.T))
            if Z < 0:
                self.params[0] = out.mean() * self.outputSize
                Z = self.params[0].clone().detach().item()
                None
            out = torch.div(out, Z).squeeze().contiguous()
        with torch.no_grad():
            out_ids = torch.arange(batchSize, device=out.device)
            out_ids += self.index
            out_ids = torch.fmod(out_ids, self.queueSize)
            out_ids = out_ids.long()
            self.memory.index_copy_(0, out_ids, k)
            self.index = (self.index + batchSize) % self.queueSize
        return out


class NCESoftmaxLoss(nn.Module):
    """Softmax cross-entropy loss (a.k.a., info-NCE loss in CPC paper)"""

    def __init__(self):
        super(NCESoftmaxLoss, self).__init__()
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, x):
        bsz = x.shape[0]
        x = x.squeeze()
        label = torch.zeros([bsz], device=x.device).long()
        loss = self.criterion(x, label)
        return loss


def clear_bn(m):
    classname = m.__class__.__name__
    if classname.find('BatchNorm') != -1:
        m.reset_running_stats()


class TopKRanker(OneVsRestClassifier):

    def predict(self, X, top_k_list):
        assert X.shape[0] == len(top_k_list)
        probs = np.asarray(super(TopKRanker, self).predict_proba(X))
        all_labels = sp.lil_matrix(probs.shape)
        for i, k in enumerate(top_k_list):
            probs_ = probs[i, :]
            labels = self.classes_[probs_.argsort()[-k:]].tolist()
            for label in labels:
                all_labels[i, label] = 1
        return all_labels


def evaluate_nc(features_matrix, label_matrix, num_shuffle):
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)
    idx_list = []
    labels = label_matrix.argmax(axis=1).squeeze().tolist()
    for idx in skf.split(np.zeros(len(labels)), labels):
        idx_list.append(idx)
    all_results = defaultdict(list)
    for train_idx, test_idx in idx_list:
        X_train = features_matrix[train_idx]
        y_train = label_matrix[train_idx]
        X_test = features_matrix[test_idx]
        y_test = label_matrix[test_idx]
        clf = TopKRanker(LogisticRegression(solver='liblinear', C=1000))
        clf.fit(X_train, y_train)
        top_k_list = y_test.sum(axis=1).long().tolist()
        preds = clf.predict(X_test, top_k_list)
        result = f1_score(y_test, preds, average='micro')
        all_results[''].append(result)
    return dict(('Micro-F1_mean', sum(all_results[train_percent]) / len(all_results[train_percent])) for train_percent in sorted(all_results.keys()))


def moment_update(model, model_ema, m):
    """model_ema = m * model_ema + (1 - m) model"""
    for p1, p2 in zip(model.parameters(), model_ema.parameters()):
        p2.data.mul_(m).add_(p1.detach().data * (1 - m))


class GCCModelWrapper(ModelWrapper):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--nce-k', type=int, default=16384)
        parser.add_argument('--nce-t', type=float, default=0.07)
        parser.add_argument('--finetune', action='store_true')
        parser.add_argument('--pretrain', action='store_true')
        parser.add_argument('--freeze', action='store_true')
        parser.add_argument('--momentum', type=float, default=0.999)
        parser.add_argument('--save-model-path', type=str, default='saved', help='path to save model')
        parser.add_argument('--load-model-path', type=str, default='', help='path to load model')

    def __init__(self, model, optimizer_cfg, nce_k, nce_t, momentum, output_size, finetune=False, num_classes=1, num_shuffle=10, save_model_path='saved', load_model_path='', freeze=False, pretrain=False):
        super(GCCModelWrapper, self).__init__()
        self.model = model
        self.model_ema = copy.deepcopy(self.model)
        for p in self.model_ema.parameters():
            p.detach_()
        self.optimizer_cfg = optimizer_cfg
        self.output_size = output_size
        self.momentum = momentum
        self.contrast = MemoryMoCo(self.output_size, num_classes, nce_k, nce_t, use_softmax=True)
        self.criterion = nn.CrossEntropyLoss() if finetune else NCESoftmaxLoss()
        self.num_shuffle = num_shuffle
        self.finetune = finetune
        self.pretrain = pretrain
        self.freeze = freeze
        self.save_model_path = save_model_path
        self.load_model_path = load_model_path
        if finetune:
            self.linear = nn.Linear(self.output_size, num_classes)
        else:
            self.register_buffer('linear', None)

    def train_step(self, batch):
        if self.finetune:
            return self.train_step_finetune(batch)
        elif self.pretrain:
            self.model_ema.eval()

            def set_bn_train(m):
                classname = m.__class__.__name__
                if classname.find('BatchNorm') != -1:
                    m.train()
            self.model_ema.apply(set_bn_train)
            return self.train_step_pretraining(batch)
        elif self.freeze:
            pass

    def train_step_pretraining(self, batch):
        graph_q, graph_k = batch
        feat_q = self.model(graph_q)
        with torch.no_grad():
            feat_k = self.model_ema(graph_k)
        out = self.contrast(feat_q, feat_k)
        assert feat_q.shape == (graph_q.batch_size, self.output_size)
        moment_update(self.model, self.model_ema, self.momentum)
        loss = self.criterion(out)
        return loss

    def train_step_finetune(self, batch):
        graph, y = batch
        hidden = self.model(graph)
        pred = self.linear(hidden)
        loss = self.criterion(pred, y)
        return loss

    def ge_step(self, batch):
        graph_q, graph_k = batch
        with torch.no_grad():
            feat_q = self.model(graph_q)
            feat_k = self.model(graph_k)
        bsz = graph_q.batch_size
        assert feat_q.shape == (bsz, self.output_size)
        emb = ((feat_q + feat_k) / 2).detach().cpu()
        return emb

    def test_step(self, batch):
        if self.freeze:
            graph_q, graph_k, y = batch
            embeddings = self.ge_step((graph_q, graph_k))
            if len(y.shape) == 1:
                num_classes = y.max().cpu().item() + 1
                y = nn.functional.one_hot(y, num_classes)
            dic_results = evaluate_nc(embeddings, y.cpu(), self.num_shuffle)
            self.note('Micro-F1_mean', dic_results['Micro-F1_mean'])
        elif self.finetune:
            self.linear.eval()
            graph_q, y = batch
            bsz = graph_q.batch_size
            with torch.no_grad():
                feat_q = self.model(graph_q)
                assert feat_q.shape == (bsz, self.output_size)
                out = self.linear(feat_q)
            preds = out.argmax(dim=1)
            f1 = f1_score(y.cpu().numpy(), preds.cpu().numpy(), average='micro')
            self.note('Micro-F1', f1)

    def setup_optimizer(self):
        cfg = self.optimizer_cfg
        lr = cfg['lr']
        weight_decay = cfg['weight_decay']
        warm_steps = cfg['n_warmup_steps']
        epochs = cfg['epochs']
        batch_size = cfg['batch_size']
        if 'betas' in cfg:
            betas = cfg['betas']
        else:
            betas = None
        total = cfg['total']
        if warm_steps > 0 and warm_steps < 1:
            warm_steps = warm_steps * total
        if self.finetune:
            optimizer = torch.optim.Adam([{'params': self.model.parameters()}, {'params': self.linear.parameters()}], lr=lr, weight_decay=weight_decay)
        else:
            optimizer = torch.optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas if betas else (0.9, 0.999))
        optimizer = LinearOptimizer(optimizer, warm_steps, epochs * (total // batch_size), init_lr=lr)
        return optimizer

    def save_checkpoint(self, path):
        state = {'model': self.model.state_dict(), 'contrast': self.contrast.state_dict(), 'model_ema': self.model_ema.state_dict()}
        torch.save(state, path)

    def load_checkpoint(self, path):
        state = torch.load(path)
        self.model.load_state_dict(state['model'])
        self.model_ema.load_state_dict(state['model_ema'])
        self.contrast.load_state_dict(state['contrast'])

    def pre_stage(self, stage, data_w):
        if self.freeze or self.finetune:
            self.load_checkpoint(self.load_model_path)
            if self.finetune:
                self.model.apply(clear_bn)

    def post_stage(self, stage, data_w):
        if self.pretrain:
            filepath = os.path.join(self.save_model_path, 'gcc_pretrain.pt')
            self.save_checkpoint(filepath)
        else:
            pass


class Gnn(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats, dropout):
        super(Gnn, self).__init__()
        self.conv1 = GCNLayer(in_feats, hidden_size)
        self.conv2 = GCNLayer(hidden_size, out_feats)
        self.dropout = nn.Dropout(dropout)

    def forward(self, graph):
        graph.sym_norm()
        h = graph.x
        h = F.relu(self.conv1(graph, self.dropout(h)))
        h = self.conv2(graph, self.dropout(h))
        return F.log_softmax(h, dim=1)


class JKNet(BaseModel):

    def __init__(self, in_feats, out_feats, hidden_size, num_layers):
        super(JKNet, self).__init__()
        shapes = [in_feats] + [hidden_size] * num_layers
        self.layers = nn.ModuleList([GCNLayer(shapes[i], shapes[i + 1]) for i in range(num_layers)])
        self.fc = nn.Linear(hidden_size * num_layers, out_feats)

    def forward(self, graph):
        graph.sym_norm()
        h = graph.x
        out = []
        for layer in self.layers:
            h = layer(graph, h)
            out.append(h)
        out = torch.cat(out, dim=1)
        return self.fc(out)


class History(torch.nn.Module):
    """A historical embedding storage module."""

    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.emb = torch.empty(num_embeddings, embedding_dim, device='cpu', pin_memory=True)
        self._device = torch.device('cpu')
        self.reset_parameters()

    def reset_parameters(self):
        self.emb.fill_(0)

    def _apply(self, fn):
        self._device = fn(torch.zeros(1)).device
        return self

    @torch.no_grad()
    def pull(self, n_id=None):
        out = self.emb
        if n_id is not None:
            assert n_id.device == self.emb.device
            out = out.index_select(0, n_id)
        return out

    @torch.no_grad()
    def push(self, x, n_id=None):
        assert n_id.device == self.emb.device
        self.emb[n_id] = x

    def forward(self, *args, **kwargs):
        raise NotImplementedError


class VRGCN(torch.nn.Module):

    def __init__(self, num_nodes: int, in_channels, hidden_channels: int, out_channels: int, num_layers: int, dropout: float=0.0, residual: bool=False, device=None):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.dropout = dropout
        self.residual = residual
        self.num_layers = num_layers
        self.lins = nn.ModuleList()
        self.lins.append(nn.Linear(in_channels, hidden_channels))
        for i in range(num_layers - 2):
            self.lins.append(nn.Linear(hidden_channels, hidden_channels))
        self.lins.append(nn.Linear(hidden_channels, out_channels))
        self.norms = nn.ModuleList()
        for i in range(num_layers):
            norm = nn.LayerNorm(hidden_channels)
            self.norms.append(norm)
        self.histories = torch.nn.ModuleList([(History(num_nodes, hidden_channels) if i != 0 else History(num_nodes, in_channels)) for i in range(num_layers)])
        self._device = device

    def reset_parameters(self):
        for history in self.histories:
            history.reset_parameters()
        for lin in self.lins:
            lin.reset_parameters()
        for norm in self.norms:
            norm.reset_parameters()

    def forward(self, x, sample_ids_adjs, full_ids_adjs) ->Tensor:
        sample_ids, sample_adjs = sample_ids_adjs
        full_ids, full_adjs = full_ids_adjs
        """VR-GCN"""
        x = x[sample_ids[0]]
        x_list = []
        for i in range(self.num_layers):
            sample_adj, cur_id, target_id = sample_adjs[i], sample_ids[i], sample_ids[i + 1]
            full_id, full_adj = full_ids[i], full_adjs[i]
            full_adj = full_adj
            sample_adj = sample_adj
            x = x - self.histories[i].pull(cur_id).detach()
            h = self.histories[i].pull(full_id)
            x = spmm(sample_adj, x)[:target_id.shape[0]] + spmm(full_adj, h)[:target_id.shape[0]].detach()
            x = self.lins[i](x)
            if i != self.num_layers - 1:
                x = self.norms[i](x)
                x = x.relu_()
                x_list.append(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        """history embedding update"""
        for i in range(1, self.num_layers):
            self.histories[i].push(x_list[i - 1].detach(), sample_ids[i])
        return x.log_softmax(dim=-1)

    def initialize_history(self, x, test_loader):
        _, xs = self.inference_batch(x, test_loader)
        for i in range(self.num_layers):
            self.histories[i].push(xs[i].detach(), torch.arange(0, self.histories[i].num_embeddings))

    @torch.no_grad()
    def inference(self, x, adj):
        x = x
        origin_device = adj.device
        adj = adj
        xs = [x]
        for i in range(self.num_layers):
            x = spmm(adj, x)
            x = self.lins[i](x)
            if i != self.num_layers - 1:
                x = self.norms[i](x)
                x = x.relu_()
            xs.append(x)
        adj = adj
        return x, xs

    @torch.no_grad()
    def inference_batch(self, x, test_loader):
        device = self._device
        xs = [x]
        for i in range(self.num_layers):
            tmp_x = []
            for target_id, full_id, full_adj in test_loader:
                full_adj = full_adj
                agg_x = spmm(full_adj, x[full_id])[:target_id.shape[0]]
                agg_x = self.lins[i](agg_x)
                if i != self.num_layers - 1:
                    agg_x = self.norms[i](agg_x)
                    agg_x = agg_x.relu_()
                tmp_x.append(agg_x.cpu())
            x = torch.cat(tmp_x, dim=0)
            xs.append(x)
        return x, xs


class DGI(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats, activation='prelu'):
        super(DGI, self).__init__()
        self.gcn = GCN(in_feats, hidden_size, activation)
        self.sparse = True
        self.layer2 = nn.Linear(hidden_size, out_feats)

    def reset_parameters(self):
        self.gcn.fc.reset_parameters()
        self.layer2.reset_parameters()

    def forward(self, graph):
        graph.sym_norm()
        x = graph.x
        logits = self.gcn(graph, x, self.sparse)
        h = F.relu(logits)
        h = self.layer2(h)
        return F.log_softmax(h, dim=-1)


class sgc(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats):
        super(sgc, self).__init__()
        self.nn1 = SGCLayer(in_feats, hidden_size)
        self.nn2 = SGCLayer(hidden_size, out_feats)
        self.cache = dict()

    def reset_parameters(self):
        self.nn1.W.reset_parameters()
        self.nn2.W.reset_parameters()

    def forward(self, graph):
        graph.sym_norm()
        x = self.nn1(graph, graph.x)
        x = self.nn2(graph, x)
        return F.log_softmax(x, dim=-1)

    def predict(self, data):
        return self.forward(data)


class ChebyNet(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout, filter_size):
        super(ChebyNet, self).__init__()
        self.num_features = in_feats
        self.num_classes = out_feats
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.filter_size = filter_size
        shapes = [in_feats] + [hidden_size] * (num_layers - 1) + [out_feats]
        self.convs = nn.ModuleList([ChebConv(shapes[layer], shapes[layer + 1], filter_size) for layer in range(num_layers)])

    def forward(self, graph):
        x = graph.x
        edge_index = torch.stack(graph.edge_index)
        for conv in self.convs[:-1]:
            x = F.relu(conv(x, edge_index))
            x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.convs[-1](x, edge_index)
        return x


class DGCNN(BaseModel):
    """EdgeConv and DynamicGraph in paper `"Dynamic Graph CNN for Learning on
    Point Clouds" <https://arxiv.org/pdf/1801.07829.pdf>__ .`
    """

    @classmethod
    def split_dataset(cls, dataset, args):
        return split_dataset_general(dataset, args)

    def __init__(self, in_feats, hidden_size, out_feats, k=20, dropout=0.5):
        super(DGCNN, self).__init__()
        mlp1 = nn.Sequential(MLP(2 * in_feats, hidden_size, hidden_size, num_layers=3, norm='batchnorm'), nn.ReLU(), nn.BatchNorm1d(hidden_size))
        mlp2 = nn.Sequential(MLP(2 * hidden_size, 2 * hidden_size, 2 * hidden_size, num_layers=1, norm='batchnorm'), nn.ReLU(), nn.BatchNorm1d(2 * hidden_size))
        self.conv1 = DynamicEdgeConv(mlp1, k, 'max')
        self.conv2 = DynamicEdgeConv(mlp2, k, 'max')
        self.linear = nn.Linear(hidden_size + 2 * hidden_size, 1024)
        self.final_mlp = nn.Sequential(nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.Dropout(dropout), nn.Linear(512, 256), nn.BatchNorm1d(256), nn.Dropout(dropout), nn.Linear(256, out_feats))

    def forward(self, batch):
        h = batch.x
        h1 = self.conv1(h, batch.batch)
        h2 = self.conv2(h1, batch.batch)
        h = self.linear(torch.cat([h1, h2], dim=1))
        h = global_max_pool(h, batch.batch)
        out = self.final_mlp(h)
        return out


class UNet(BaseModel):

    def __init__(self, in_feats, hidden_size, out_feats, num_layers, dropout, num_nodes):
        super(UNet, self).__init__()
        self.in_feats = in_feats
        self.out_feats = out_feats
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.unet = GraphUNet(self.in_feats, self.hidden_size, self.out_feats, depth=3, pool_ratios=[2000 / num_nodes, 0.5], act=F.elu)

    def forward(self, graph):
        x = graph.x
        edge_index = torch.stack(graph.edge_index)
        edge_index, _ = dropout_adj(edge_index, p=0.2, force_undirected=True, num_nodes=x.shape[0], training=self.training)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.unet(x, edge_index)
        return x


class myGATConv(nn.Module):
    """
    cogdl implementation of Simple-HGN layer
    """

    def __init__(self, edge_feats, num_etypes, in_features, out_features, nhead, feat_drop=0.0, attn_drop=0.5, negative_slope=0.2, residual=False, activation=None, alpha=0.0):
        super(myGATConv, self).__init__()
        self.edge_feats = edge_feats
        self.in_features = in_features
        self.out_features = out_features
        self.nhead = nhead
        self.edge_emb = nn.Parameter(torch.zeros(size=(num_etypes, edge_feats)))
        self.W = nn.Parameter(torch.FloatTensor(in_features, out_features * nhead))
        self.W_e = nn.Parameter(torch.FloatTensor(edge_feats, edge_feats * nhead))
        self.a_l = nn.Parameter(torch.zeros(size=(1, nhead, out_features)))
        self.a_r = nn.Parameter(torch.zeros(size=(1, nhead, out_features)))
        self.a_e = nn.Parameter(torch.zeros(size=(1, nhead, edge_feats)))
        self.mhspmm = MultiHeadSpMM()
        self.feat_drop = nn.Dropout(feat_drop)
        self.dropout = nn.Dropout(attn_drop)
        self.leakyrelu = nn.LeakyReLU(negative_slope)
        self.act = None if activation is None else get_activation(activation)
        if residual:
            self.residual = nn.Linear(in_features, out_features * nhead)
        else:
            self.register_buffer('residual', None)
        self.reset_parameters()
        self.alpha = alpha

    def reset_parameters(self):

        def reset(tensor):
            stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))
            tensor.data.uniform_(-stdv, stdv)
        reset(self.a_l)
        reset(self.a_r)
        reset(self.a_e)
        reset(self.W)
        reset(self.W_e)
        reset(self.edge_emb)

    def forward(self, graph, x, res_attn=None):
        x = self.feat_drop(x)
        h = torch.matmul(x, self.W).view(-1, self.nhead, self.out_features)
        h[torch.isnan(h)] = 0.0
        e = torch.matmul(self.edge_emb, self.W_e).view(-1, self.nhead, self.edge_feats)
        row, col = graph.edge_index
        tp = graph.edge_type
        h_l = (self.a_l * h).sum(dim=-1)[row]
        h_r = (self.a_r * h).sum(dim=-1)[col]
        h_e = (self.a_e * e).sum(dim=-1)[tp]
        edge_attention = self.leakyrelu(h_l + h_r + h_e)
        edge_attention = edge_softmax(graph, edge_attention)
        edge_attention = self.dropout(edge_attention)
        if res_attn is not None:
            edge_attention = edge_attention * (1 - self.alpha) + res_attn * self.alpha
        out = self.mhspmm(graph, edge_attention, h)
        if self.residual:
            res = self.residual(x)
            out += res
        if self.act is not None:
            out = self.act(out)
        return out, edge_attention.detach()

    def __repr__(self):
        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'


class SimpleHGN(BaseModel):
    """The Simple-HGN model from the `"Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks"`_ paper"""

    def __init__(self, in_dims, num_classes, edge_dim=64, num_etypes=5, num_hidden=64, num_layers=2, heads=[8, 8, 1], feat_drop=0.5, attn_drop=0.5, negative_slope=0.05, residual=True, alpha=0.05):
        super(SimpleHGN, self).__init__()
        self.cross_entropy_loss = nn.CrossEntropyLoss()
        self.g = None
        self.num_layers = num_layers
        self.gat_layers = nn.ModuleList()
        self.activation = F.elu
        self.gat_layers.append(myGATConv(edge_dim, num_etypes, in_dims, num_hidden, heads[0], feat_drop, attn_drop, negative_slope, False, self.activation, alpha=alpha))
        for l in range(1, num_layers):
            self.gat_layers.append(myGATConv(edge_dim, num_etypes, num_hidden * heads[l - 1], num_hidden, heads[l], feat_drop, attn_drop, negative_slope, residual, self.activation, alpha=alpha))
        self.gat_layers.append(myGATConv(edge_dim, num_etypes, num_hidden * heads[-2], num_classes, heads[-1], feat_drop, attn_drop, negative_slope, residual, None, alpha=alpha))
        self.register_buffer('epsilon', torch.FloatTensor([1e-12]))

    def build_g_feat(self, A):
        edge2type = {}
        edges = []
        weights = []
        for k, mat in enumerate(A):
            edges.append(mat[0].cpu().numpy())
            weights.append(mat[1].cpu().numpy())
            for u, v in zip(*edges[-1]):
                edge2type[u, v] = k
        edges = np.concatenate(edges, axis=1)
        weights = np.concatenate(weights)
        edges = torch.tensor(edges)
        weights = torch.tensor(weights)
        g = Graph(edge_index=edges, edge_weight=weights)
        g = g
        e_feat = []
        for u, v in zip(*g.edge_index):
            u = u.cpu().item()
            v = v.cpu().item()
            e_feat.append(edge2type[u, v])
        e_feat = torch.tensor(e_feat, dtype=torch.long)
        g.edge_type = e_feat
        self.g = g

    def forward(self, data):
        A = data.adj
        X = data.x
        h = X
        if self.g is None:
            self.build_g_feat(A)
        res_attn = None
        for l in range(self.num_layers):
            h, res_attn = self.gat_layers[l](self.g, h, res_attn=res_attn)
            h = h.flatten(1)
        logits, _ = self.gat_layers[-1](self.g, h, res_attn=None)
        logits = logits / torch.max(torch.norm(logits, dim=1, keepdim=True), self.epsilon)
        return logits


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AttentionLayer,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (AvgReadout,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BasesRelEmbLayer,
     lambda: ([], {'num_bases': 4, 'num_rels': 4, 'in_feats': 4}),
     lambda: ([], {}),
     True),
    (BatchedDiffPool,
     lambda: ([], {'in_feats': 4, 'next_size': 4, 'emb_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (BatchedGraphSAGE,
     lambda: ([], {'in_feats': 4, 'out_feats': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (BertAttention,
     lambda: ([], {'config': _mock_config(hidden_size=4, num_attention_heads=4, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (BertLayerNorm,
     lambda: ([], {'hidden_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BertOutput,
     lambda: ([], {'config': _mock_config(intermediate_size=4, hidden_size=4, hidden_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BertPooler,
     lambda: ([], {'config': _mock_config(hidden_size=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BertSelfAttention,
     lambda: ([], {'config': _mock_config(hidden_size=4, num_attention_heads=4, attention_probs_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (BertSelfOutput,
     lambda: ([], {'config': _mock_config(hidden_size=4, hidden_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (DAEGCModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config(), 'num_clusters': 4}),
     lambda: ([], {}),
     False),
    (DGIModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config(hidden_size=4)}),
     lambda: ([], {}),
     False),
    (DNGR_layer,
     lambda: ([], {'num_node': 4, 'hidden_size1': 4, 'hidden_size2': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Dense,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (DistMultLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (DropEdge,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DropFeatures,
     lambda: ([], {'drop_rate': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DropNode,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (EdgeEncoder,
     lambda: ([], {'in_feats': 4, 'out_feats': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (EmbeddingModelWrapper,
     lambda: ([], {}),
     lambda: ([], {}),
     False),
    (EntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (GAEModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config(), 'num_clusters': 4}),
     lambda: ([], {}),
     False),
    (GATNEModel,
     lambda: ([], {'num_nodes': 4, 'embedding_size': 4, 'embedding_u_size': 4, 'edge_type_count': 4, 'dim_a': 4}),
     lambda: ([torch.ones([4, 4, 4], dtype=torch.int64), torch.ones([4], dtype=torch.int64), torch.ones([4, 4, 4], dtype=torch.int64)], {}),
     True),
    (GNNLinkPredictionModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (GraphClassificationModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (GraphEmbeddingModelWrapper,
     lambda: ([], {'model': _mock_layer()}),
     lambda: ([], {}),
     False),
    (GraphSAGEModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (HeterogeneousEmbeddingModelWrapper,
     lambda: ([], {'model': _mock_layer()}),
     lambda: ([], {}),
     False),
    (HeterogeneousGNNModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (Identity,
     lambda: ([], {'in_feat': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (LinearActivation,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LinkPredLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (LogReg,
     lambda: ([], {'ft_in': 4, 'nb_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MVGRLModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (MemoryMoCo,
     lambda: ([], {'inputSize': 4, 'outputSize': 4, 'K': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (ModelWrapper,
     lambda: ([], {}),
     lambda: ([], {}),
     False),
    (MultiplexEmbeddingModelWrapper,
     lambda: ([], {'model': _mock_layer()}),
     lambda: ([], {}),
     False),
    (NCESoftmaxLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (NodeAdaptiveEncoder,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (NodeClfModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (NormIdentity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (PPRGoLayer,
     lambda: ([], {'in_feats': 4, 'hidden_size': 4, 'out_feats': 4, 'num_layers': 1, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PPRGoModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (SAGNModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config()}),
     lambda: ([], {}),
     False),
    (SDNE_layer,
     lambda: ([], {'num_node': 4, 'hidden_size1': 4, 'hidden_size2': 4, 'droput': 4, 'alpha': 4, 'beta': 4, 'nu1': 4, 'nu2': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (SELayer,
     lambda: ([], {'in_channels': 4, 'se_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SelfAuxiliaryModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config(hidden_size=4), 'auxiliary_task': 4, 'dropedge_rate': 4, 'mask_ratio': 4, 'sampling': 4}),
     lambda: ([], {}),
     False),
    (UnsupGraphSAGEModelWrapper,
     lambda: ([], {'model': _mock_layer(), 'optimizer_cfg': _mock_config(), 'walk_length': 4, 'negative_samples': 4}),
     lambda: ([], {}),
     False),
    (UnsupervisedModelWrapper,
     lambda: ([], {}),
     lambda: ([], {}),
     False),
]

class Test_THUDM_cogdl(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

    def test_041(self):
        self._check(*TESTCASES[41])

    def test_042(self):
        self._check(*TESTCASES[42])

    def test_043(self):
        self._check(*TESTCASES[43])

    def test_044(self):
        self._check(*TESTCASES[44])

    def test_045(self):
        self._check(*TESTCASES[45])

    def test_046(self):
        self._check(*TESTCASES[46])

    def test_047(self):
        self._check(*TESTCASES[47])

    def test_048(self):
        self._check(*TESTCASES[48])

    def test_049(self):
        self._check(*TESTCASES[49])


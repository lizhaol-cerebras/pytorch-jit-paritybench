import sys
_module = sys.modules[__name__]
del sys
avalanche = _module
benchmarks = _module
classic = _module
ccifar10 = _module
ccifar100 = _module
ccub200 = _module
cfashion_mnist = _module
cimagenet = _module
cinaturalist = _module
classic_benchmarks_utils = _module
clear = _module
cmnist = _module
comniglot = _module
core50 = _module
ctiny_imagenet = _module
ctrl = _module
endless_cl_sim = _module
ex_model = _module
openloris = _module
stream51 = _module
datasets = _module
clear = _module
clear_data = _module
core50 = _module
core50_data = _module
cub200 = _module
dataset_utils = _module
downloadable_dataset = _module
endless_cl_sim = _module
endless_cl_sim_data = _module
external_datasets = _module
cifar = _module
fmnist = _module
mnist = _module
imagenet_data = _module
inaturalist = _module
inaturalist = _module
inaturalist_data = _module
lvis_dataset = _module
lvis_data = _module
lvis_dataset = _module
mini_imagenet = _module
mini_imagenet = _module
mini_imagenet_data = _module
omniglot = _module
openloris = _module
openloris_data = _module
penn_fudan = _module
penn_fudan_data = _module
penn_fudan_dataset = _module
stream51 = _module
stream51_data = _module
tiny_imagenet = _module
tiny_imagenet = _module
torchaudio_wrapper = _module
torchvision_wrapper = _module
generators = _module
benchmark_generators = _module
scenario_generators = _module
scenarios = _module
classification_scenario = _module
detection_scenario = _module
exmodel_scenario = _module
generic_benchmark_creation = _module
generic_definitions = _module
generic_scenario = _module
generic_scenario_creation = _module
lazy_dataset_sequence = _module
new_classes = _module
nc_scenario = _module
nc_utils = _module
new_instances = _module
ni_scenario = _module
ni_utils = _module
online_scenario = _module
rl_scenario = _module
scenario_utils = _module
utils = _module
classification_dataset = _module
collate_functions = _module
data = _module
data_attribute = _module
data_loader = _module
dataset_definitions = _module
dataset_utils = _module
datasets_from_filelists = _module
deprecated = _module
flat_data = _module
transform_groups = _module
transforms = _module
utils = _module
core = _module
evaluation = _module
metric_definitions = _module
metric_results = _module
metric_utils = _module
metrics = _module
accuracy = _module
amca = _module
checkpoint = _module
class_accuracy = _module
confusion_matrix = _module
cpu_usage = _module
detection = _module
detection_evaluators = _module
coco_evaluator = _module
lvis_evaluator = _module
disk_usage = _module
forgetting_bwt = _module
forward_transfer = _module
gpu_usage = _module
images_samples = _module
labels_repartition = _module
loss = _module
mac = _module
mean = _module
mean_scores = _module
ram_usage = _module
timing = _module
topk_acc = _module
plot_utils = _module
logging = _module
base_logger = _module
csv_logger = _module
interactive_logging = _module
tensorboard_logger = _module
text_logging = _module
wandb_logger = _module
models = _module
base_model = _module
batch_renorm = _module
bic_model = _module
dynamic_modules = _module
dynamic_optimizers = _module
generator = _module
helper_method = _module
icarl_resnet = _module
lenet5 = _module
mlp_tiny_imagenet = _module
mobilenetv1 = _module
ncm_classifier = _module
pnn = _module
pytorchcv_wrapper = _module
resnet32 = _module
simple_cnn = _module
simple_mlp = _module
simple_sequence_classifier = _module
slda_resnet = _module
slim_resnet18 = _module
utils = _module
training = _module
determinism = _module
cuda_rng = _module
rng_manager = _module
losses = _module
plugins = _module
agem = _module
bic = _module
checkpoint = _module
checkpoint_common_recipes = _module
clock = _module
cope = _module
cwr_star = _module
early_stopping = _module
ewc = _module
gdumb = _module
gem = _module
generative_replay = _module
gss_greedy = _module
lfl = _module
lr_scheduling = _module
lwf = _module
mas = _module
mir = _module
replay = _module
rwalk = _module
strategy_plugin = _module
synaptic_intelligence = _module
regularization = _module
storage_policy = _module
supervised = _module
ar1 = _module
cumulative = _module
deep_slda = _module
er_ace = _module
icarl = _module
joint_training = _module
lamaml = _module
mer = _module
naive_object_detection = _module
strategy_wrappers = _module
strategy_wrappers_online = _module
templates = _module
base = _module
base_online_sgd = _module
base_sgd = _module
common_templates = _module
observation_type = _module
batch_observation = _module
online_observation = _module
problem_type = _module
supervised_problem = _module
update_type = _module
meta_update = _module
sgd_update = _module
utils = _module
conf = _module
examples = _module
all_mnist = _module
all_mnist_early_stopping = _module
ar1 = _module
checkpointing = _module
clear = _module
clear_linear = _module
confusion_matrix = _module
continual_sequence_classification = _module
cope = _module
dataloader = _module
dataset_inspection = _module
deep_slda = _module
detection = _module
detection_examples_utils = _module
detection_lvis = _module
endless_cl_sim = _module
eval_plugin = _module
ewc_mnist = _module
ex_model_cl = _module
gem_agem_mnist = _module
generative_replay_MNIST_generator = _module
generative_replay_splitMNIST = _module
getting_started = _module
icarl = _module
joint_training = _module
lamaml_cifar100 = _module
lfl_mnist = _module
lwf_mnist = _module
mean_scores = _module
multihead = _module
naive = _module
nlp = _module
online_naive = _module
online_replay = _module
pytorchcv_models = _module
replay = _module
rwalk_mnist = _module
simple_ctrl = _module
standalone_metric = _module
synaptic_intelligence = _module
task_incremental = _module
task_incremental_with_checkpointing = _module
task_metrics = _module
tensorboard_logger = _module
test_install = _module
tvdetection = _module
coco_eval = _module
coco_utils = _module
engine = _module
group_by_aspect_ratio = _module
lvis_eval = _module
presets = _module
train = _module
transforms = _module
utils = _module
wandb_logger = _module
data_merging = _module
online_strategy = _module
replay_buffers = _module
setup = _module
tests = _module
test_classification_scenario = _module
test_generic_scenario = _module
test_online_scenario = _module
test_rl_scenario = _module
test_scenarios_typechecks = _module
test_avalanche_dataset = _module
test_data_attribute = _module
test_flat_data = _module
check_metrics_aligned = _module
task_incremental_with_checkpointing = _module
test_image_samples = _module
test_avalanche_classification_dataset = _module
test_cifar100_benchmarks = _module
test_cifar10_benchmarks = _module
test_core50 = _module
test_ctrl = _module
test_custom_streams = _module
test_dataloaders = _module
test_endless_cl_sim = _module
test_fmnist_benckmarks = _module
test_helper_method = _module
test_high_level_generators = _module
test_loggers = _module
test_metrics = _module
test_mnist_benckmarks = _module
test_models = _module
test_nc_mt_scenario = _module
test_nc_sit_scenario = _module
test_ni_sit_scenario = _module
test_tinyimagenet = _module
test_ar1 = _module
test_dictionary_mbatches = _module
test_losses = _module
test_online_strategies = _module
test_plugins = _module
test_regularization = _module
test_replay = _module
test_strategies = _module
test_strategies_accuracy = _module
test_stream_completeness = _module
test_supervised_regression = _module
test_training_utils = _module
unit_tests_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from torchvision.transforms import ToPILImage


from torchvision.transforms import ToTensor


from typing import Union


from typing import Any


from typing import Optional


from typing import Sequence


import torch


from torch import Tensor


from torchvision.transforms import Compose


from torchvision.transforms import Normalize


from torchvision.transforms import RandomRotation


import numpy as np


from typing import List


from torchvision.transforms.transforms import Compose


from torch import nn


from torch.utils.data import DataLoader


from torchvision.models import mobilenet_v2


from torchvision.transforms import RandomHorizontalFlip


from torchvision.transforms import RandomCrop


from torchvision.transforms import CenterCrop


from torchvision.transforms import Resize


import copy


from torchvision import transforms


import math


from torchvision.datasets.folder import default_loader


import logging


from warnings import warn


from abc import abstractmethod


from abc import ABC


from torch.utils.data.dataset import Dataset


from torch.utils.data.dataset import T_co


from torchvision.datasets.utils import download_and_extract_archive


from torchvision.datasets.utils import extract_archive


from torchvision.datasets.utils import download_url


from torchvision.datasets.utils import check_integrity


from torch.utils.data import Dataset


from typing import Dict


from typing import Tuple


import random


from torchaudio.datasets import SPEECHCOMMANDS


from functools import partial


from itertools import tee


from typing import Callable


from typing import Set


from typing import Iterable


from typing import Generator


import warnings


from typing import SupportsInt


import re


from typing import Generic


from typing import TypeVar


from typing import NamedTuple


from typing import Mapping


from torch.nn import Module


from typing import TYPE_CHECKING


from collections import OrderedDict


from copy import copy


from collections import defaultdict


from collections import deque


from torch.utils.data.dataset import Subset


from torch.utils.data.dataset import ConcatDataset


from torch.utils.data.dataset import TensorDataset


import itertools


from torch.utils.data.dataloader import default_collate


from torch.utils.data import Dataset as TorchDataset


from itertools import chain


from torch.utils.data import RandomSampler


from torch.utils.data import DistributedSampler


from torch.utils.data.dataloader import DataLoader


from torch.utils.data import Subset


from torch.utils.data import ConcatDataset


import torch.utils.data as data


from torchvision.transforms.functional import crop


from matplotlib.figure import Figure


from enum import Enum


import matplotlib.pyplot as plt


from matplotlib.axes import Axes


from numpy import ndarray


from numpy import arange


from torch.nn.functional import pad


from typing import Type


import torch.distributed as dist


from torchvision.utils import make_grid


from matplotlib.pyplot import subplots


from torch import arange


from torch.utils.tensorboard import SummaryWriter


from matplotlib.pyplot import Figure


from torchvision.transforms.functional import to_tensor


from typing import TextIO


from numpy import array


from matplotlib import transforms


import torch.nn as nn


from torch.nn import Sequential


from torch.nn import BatchNorm2d


from torch.nn import Conv2d


from torch.nn import ReLU


from torch.nn import ConstantPad3d


from torch.nn import Identity


from torch.nn import AdaptiveAvgPool2d


from torch.nn import Linear


from torch.nn.init import zeros_


from torch.nn.init import kaiming_normal_


from torch.nn.modules.flatten import Flatten


import torch.nn.functional as F


import torchvision.models as models


from torch.nn.functional import relu


from torch.nn.functional import avg_pool2d


from torch import default_generator


from torch.nn import BCELoss


from copy import deepcopy


from torch.optim.lr_scheduler import MultiStepLR


from typing import IO


from typing import BinaryIO


from torch.nn.functional import normalize


from torch.nn.modules import Module


from torch.nn.modules.batchnorm import _NormBase


from numpy import inf


from torch import cat


from torch.nn import CrossEntropyLoss


from torch.optim import SGD


from torch.optim import Optimizer


from math import ceil


import torchvision


from torchvision.datasets import MNIST


import torch.optim.lr_scheduler


from torch.optim import Adam


from torch.utils.data import random_split


from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor


from torchvision.models.detection.faster_rcnn import FastRCNNPredictor


import torch.nn


from torchvision.datasets import CIFAR10


import torch.utils.data


import time


import torchvision.models.detection.mask_rcnn


from itertools import repeat


from torch.utils.data.sampler import BatchSampler


from torch.utils.data.sampler import Sampler


from torch.utils.model_zoo import tqdm


import torchvision.models.detection


from torchvision.transforms import functional as F


from torchvision.transforms import transforms as T


from torch.utils.data import TensorDataset


from torchvision.transforms import Lambda


from sklearn.datasets import make_classification


from sklearn.model_selection import train_test_split


from numpy.testing import assert_almost_equal


from torch.optim.lr_scheduler import ReduceLROnPlateau


from torch import tensor


from torch import zeros


class BatchRenorm2D(Module):

    def __init__(self, num_features, gamma=None, beta=None, running_mean=None, running_var=None, eps=1e-05, momentum=0.01, r_d_max_inc_step=0.0001, r_max=1.0, d_max=0.0, max_r_max=3.0, max_d_max=5.0):
        super(BatchRenorm2D, self).__init__()
        self.eps = eps
        self.num_features = num_features
        self.momentum = torch.tensor(momentum, requires_grad=False)
        if gamma is None:
            self.gamma = torch.nn.Parameter(torch.ones((1, num_features, 1, 1)), requires_grad=True)
        else:
            self.gamma = torch.nn.Parameter(gamma.view(1, -1, 1, 1))
        if beta is None:
            self.beta = torch.nn.Parameter(torch.zeros((1, num_features, 1, 1)), requires_grad=True)
        else:
            self.beta = torch.nn.Parameter(beta.view(1, -1, 1, 1))
        if running_mean is None:
            self.running_avg_mean = torch.ones((1, num_features, 1, 1), requires_grad=False)
            self.running_avg_std = torch.zeros((1, num_features, 1, 1), requires_grad=False)
        else:
            self.running_avg_mean = running_mean.view(1, -1, 1, 1)
            self.running_avg_std = torch.sqrt(running_var.view(1, -1, 1, 1))
        self.max_r_max = max_r_max
        self.max_d_max = max_d_max
        self.r_max_inc_step = r_d_max_inc_step
        self.d_max_inc_step = r_d_max_inc_step
        self.r_max = r_max
        self.d_max = d_max

    def forward(self, x):
        device = self.gamma.device
        self.r_max = self.r_max if isinstance(self.r_max, float) else self.r_max
        self.d_max = self.d_max if isinstance(self.d_max, float) else self.d_max
        batch_ch_mean = torch.mean(x, dim=(0, 2, 3), keepdim=True)
        batch_ch_std = torch.sqrt(torch.var(x, dim=(0, 2, 3), keepdim=True, unbiased=False) + self.eps)
        batch_ch_std = batch_ch_std
        self.running_avg_std = self.running_avg_std
        self.running_avg_mean = self.running_avg_mean
        self.momentum = self.momentum
        if self.training:
            r = torch.clamp(batch_ch_std / self.running_avg_std, 1.0 / self.r_max, self.r_max).to(device).data
            d = torch.clamp((batch_ch_mean - self.running_avg_mean) / self.running_avg_std, -self.d_max, self.d_max).to(device).data
            x = (x - batch_ch_mean) * r / batch_ch_std + d
            x = self.gamma * x + self.beta
            if self.r_max < self.max_r_max:
                self.r_max += self.r_max_inc_step * x.shape[0]
            if self.d_max < self.max_d_max:
                self.d_max += self.d_max_inc_step * x.shape[0]
            self.running_avg_mean = self.running_avg_mean + self.momentum * (batch_ch_mean.data - self.running_avg_mean)
            self.running_avg_std = self.running_avg_std + self.momentum * (batch_ch_std.data - self.running_avg_std)
        else:
            x = (x - self.running_avg_mean) / self.running_avg_std
            x = self.gamma * x + self.beta
        return x


class BiasLayer(torch.nn.Module):
    """Bias layers with alpha and beta parameters
    
    Bias layers used in Bias Correction (BiC) plugin.
    "Wu, Yue, et al. "Large scale incremental learning." Proceedings 
    of the IEEE/CVF Conference on Computer Vision and Pattern 
    Recognition. 2019"
    """

    def __init__(self, device, clss):
        """
        :param device: device used by the main model. 'cpu' or 'cuda'
        :param clss: list of classes of the current layer. This are use 
            to identify the columns which are multiplied by the Bias 
            correction Layer.
        """
        super().__init__()
        self.alpha = torch.nn.Parameter(torch.ones(1, device=device))
        self.beta = torch.nn.Parameter(torch.zeros(1, device=device))
        self.clss = torch.Tensor(list(clss)).long()
        self.not_clss = None

    def forward(self, x):
        alpha = torch.ones_like(x)
        beta = torch.ones_like(x)
        alpha[:, self.clss] = self.alpha
        beta[:, self.clss] = self.beta
        return alpha * x + beta


class ExperienceMode(Enum):
    """ExperienceMode is an enum used to change visibility of experience's
    attributes.

    Example: task labels may be available during training but not evaluation.

    Current modes:
    - TRAIN: training time (e.g. train method in strategies).
    - INFERENCE: evaluation time (e.g. eval method in strategies).
    - LOGGING: maximum visibility. Useful when computing metrics.
    """
    TRAIN = 1
    EVAL = 2
    LOGGING = 3


class MaskedAttributeError(ValueError):
    """An error that is thrown when the user tries to access experience
    attributes which are private in the current experience's mode"""
    pass


class CLExperience(object):
    """Base Experience.

    Experiences have an index which track the experience's position
    inside the stream for evaluation purposes.
    """

    def __init__(self, current_experience: int=None, origin_stream=None):
        super().__init__()
        self.current_experience: TExperienceAttribute = ExperienceAttribute(current_experience)
        """Experience identifier (the position in the origin_stream)."""
        self.origin_stream: TExperienceAttribute = ExperienceAttribute(origin_stream)
        """Stream containing the experience."""
        self._exp_mode: ExperienceMode = ExperienceMode.TRAIN

    def __getattribute__(self, item):
        """Custom getattribute.

        Check that ExperienceAttribute are available in train/eval mode.
        """
        v = super().__getattribute__(item)
        if isinstance(v, ExperienceAttribute):
            if self._exp_mode == ExperienceMode.TRAIN and v.use_in_train:
                return v.value
            elif self._exp_mode == ExperienceMode.EVAL and v.use_in_eval:
                return v.value
            elif self._exp_mode == ExperienceMode.LOGGING:
                return v.value
            else:
                mode = 'train' if self._exp_mode == ExperienceMode.TRAIN else 'eval'
                se = f'Attribute {item} is not available for the experience in {mode} mode.'
                raise MaskedAttributeError(se)
        else:
            return v

    def train(self):
        """Return training experience.

        This is a copy of the experience itself where the private data (e.g.
        experience IDs) is removed to avoid its use during training.
        """
        exp = copy(self)
        exp._exp_mode = ExperienceMode.TRAIN
        return exp

    def eval(self):
        """Return inference experience.

        This is a copy of the experience itself where the inference data (e.g.
        experience IDs) is available.
        """
        exp = copy(self)
        exp._exp_mode = ExperienceMode.EVAL
        return exp

    def logging(self):
        """Return logging experience.

        This is a copy of the experience itself where all the attributes are
        available. Useful for logging and metric computations.
        """
        exp = copy(self)
        exp._exp_mode = ExperienceMode.LOGGING
        return exp


class DynamicModule(Module):
    """Dynamic Modules are Avalanche modules that can be incrementally
    expanded to allow architectural modifications (multi-head
    classifiers, progressive networks, ...).

    Compared to pytoch Modules, they provide an additional method,
    `model_adaptation`, which adapts the model given the current experience.
    """

    def adaptation(self, experience: CLExperience=None):
        """Adapt the module (freeze units, add units...) using the current
        data. Optimizers must be updated after the model adaptation.

        Avalanche strategies call this method to adapt the architecture
        *before* processing each experience. Strategies also update the
        optimizer automatically.

        .. warning::
            As a general rule, you should NOT use this method to train the
            model. The dataset should be used only to check conditions which
            require the model's adaptation, such as the discovery of new
            classes or tasks.

        :param experience: the current experience.
        :return:
        """
        if self.training:
            self.train_adaptation(experience)
        else:
            self.eval_adaptation(experience)

    def train_adaptation(self, experience: CLExperience):
        """Module's adaptation at training time.

        Avalanche strategies automatically call this method *before* training
        on each experience.
        """
        pass

    def eval_adaptation(self, experience: CLExperience):
        """Module's adaptation at evaluation time.

        Avalanche strategies automatically call this method *before* evaluating
        on each experience.

        .. warning::
            This method receives the experience's data at evaluation time
            because some dynamic models need it for adaptation. For example,
            an incremental classifier needs to be expanded even at evaluation
            time if new classes are available. However, you should **never**
            use this data to **train** the module's parameters.
        """
        pass


class MultiTaskModule(DynamicModule):
    """Base pytorch Module with support for task labels.

    Multi-task modules are ``torch.nn.Module`` for multi-task
    scenarios. The ``forward`` method accepts task labels, one for
    each sample in the mini-batch.

    By default the ``forward`` method splits the mini-batch by task
    and calls ``forward_single_task``. Subclasses must implement
    ``forward_single_task`` or override `forward. If ``task_labels == None``,
    the output is computed in parallel for each task.
    """

    def __init__(self):
        super().__init__()
        self.max_class_label = 0
        self.known_train_tasks_labels = set()
        """ Set of task labels encountered up to now. """

    def adaptation(self, experience: CLExperience=None):
        """Adapt the module (freeze units, add units...) using the current
        data. Optimizers must be updated after the model adaptation.

        Avalanche strategies call this method to adapt the architecture
        *before* processing each experience. Strategies also update the
        optimizer automatically.

        .. warning::
            As a general rule, you should NOT use this method to train the
            model. The dataset should be used only to check conditions which
            require the model's adaptation, such as the discovery of new
            classes or tasks.

        :param experience: the current experience.
        :return:
        """
        curr_classes = experience.classes_in_this_experience
        self.max_class_label = max(self.max_class_label, max(curr_classes) + 1)
        if self.training:
            self.train_adaptation(experience)
        else:
            self.eval_adaptation(experience)

    def eval_adaptation(self, experience: CLExperience):
        pass

    def train_adaptation(self, experience: CLExperience=None):
        """Update known task labels."""
        task_labels = experience.task_labels
        self.known_train_tasks_labels = self.known_train_tasks_labels.union(set(task_labels))

    def forward(self, x: torch.Tensor, task_labels: torch.Tensor) ->torch.Tensor:
        """compute the output given the input `x` and task labels.

        :param x:
        :param task_labels: task labels for each sample. if None, the
            computation will return all the possible outputs as a dictionary
            with task IDs as keys and the output of the corresponding task as
            output.
        :return:
        """
        if task_labels is None:
            return self.forward_all_tasks(x)
        if isinstance(task_labels, int):
            return self.forward_single_task(x, task_labels)
        else:
            unique_tasks = torch.unique(task_labels)
        out = torch.zeros(x.shape[0], self.max_class_label, device=x.device)
        for task in unique_tasks:
            task_mask = task_labels == task
            x_task = x[task_mask]
            out_task = self.forward_single_task(x_task, task.item())
            assert len(out_task.shape) == 2, 'multi-head assumes mini-batches of 2 dimensions <batch, classes>'
            n_labels_head = out_task.shape[1]
            out[task_mask, :n_labels_head] = out_task
        return out

    def forward_single_task(self, x: torch.Tensor, task_label: int) ->torch.Tensor:
        """compute the output given the input `x` and task label.

        :param x:
        :param task_label: a single task label.
        :return:
        """
        raise NotImplementedError()

    def forward_all_tasks(self, x: torch.Tensor):
        """compute the output given the input `x` and task label.
        By default, it considers only tasks seen at training time.

        :param x:
        :return: all the possible outputs are returned as a dictionary
            with task IDs as keys and the output of the corresponding
            task as output.
        """
        res = {}
        for task_id in self.known_train_tasks_labels:
            res[task_id] = self.forward_single_task(x, task_id)
        return res


class IncrementalClassifier(DynamicModule):
    """
    Output layer that incrementally adds units whenever new classes are
    encountered.

    Typically used in class-incremental benchmarks where the number of
    classes grows over time.
    """

    def __init__(self, in_features, initial_out_features=2, masking=True, mask_value=-1000):
        """
        :param in_features: number of input features.
        :param initial_out_features: initial number of classes (can be
            dynamically expanded).
        :param masking: whether unused units should be masked (default=True).
        :param mask_value: the value used for masked units (default=-1000).
        """
        super().__init__()
        self.masking = masking
        self.mask_value = mask_value
        self.classifier = torch.nn.Linear(in_features, initial_out_features)
        au_init = torch.zeros(initial_out_features, dtype=torch.bool)
        self.register_buffer('active_units', au_init)

    @torch.no_grad()
    def adaptation(self, experience: CLExperience):
        """If `dataset` contains unseen classes the classifier is expanded.

        :param experience: data from the current experience.
        :return:
        """
        in_features = self.classifier.in_features
        old_nclasses = self.classifier.out_features
        curr_classes = experience.classes_in_this_experience
        new_nclasses = max(self.classifier.out_features, max(curr_classes) + 1)
        if self.masking:
            if old_nclasses != new_nclasses:
                old_act_units = self.active_units
                self.active_units = torch.zeros(new_nclasses, dtype=torch.bool)
                self.active_units[:old_act_units.shape[0]] = old_act_units
            if self.training:
                self.active_units[curr_classes] = 1
        if old_nclasses == new_nclasses:
            return
        old_w, old_b = self.classifier.weight, self.classifier.bias
        self.classifier = torch.nn.Linear(in_features, new_nclasses)
        self.classifier.weight[:old_nclasses] = old_w
        self.classifier.bias[:old_nclasses] = old_b

    def forward(self, x, **kwargs):
        """compute the output given the input `x`. This module does not use
        the task label.

        :param x:
        :return:
        """
        out = self.classifier(x)
        if self.masking:
            out[..., torch.logical_not(self.active_units)] = self.mask_value
        return out


def _flatten_datasets_and_reindex(datasets, indices):
    """The same dataset may occurr multiple times in the list of datasets.

    Here, we flatten the list of datasets and fix the indices to account for
    the new dataset position in the list.
    """
    if not all(isinstance(d, Hashable) for d in datasets):
        return datasets, indices
    dset_uniques = set(datasets)
    if len(dset_uniques) == len(datasets):
        return datasets, indices
    cumulative_sizes = [0] + ConcatDataset.cumsum(datasets)
    data_sample_pairs = []
    if indices is None:
        for ii, dset in enumerate(datasets):
            data_sample_pairs.extend([(ii, jj) for jj in range(len(dset))])
    else:
        for idx in indices:
            d_idx = bisect.bisect_right(cumulative_sizes, idx) - 1
            s_idx = idx - cumulative_sizes[d_idx]
            data_sample_pairs.append((d_idx, s_idx))
    new_datasets = list(dset_uniques)
    new_dpos = {d: i for i, d in enumerate(new_datasets)}
    new_cumsizes = [0] + ConcatDataset.cumsum(new_datasets)
    new_indices = []
    for d_idx, s_idx in data_sample_pairs:
        new_d_idx = new_dpos[datasets[d_idx]]
        new_indices.append(new_cumsizes[new_d_idx] + s_idx)
    return new_datasets, new_indices


class ConstantSequence:
    """A memory-efficient constant sequence."""

    def __init__(self, constant_value: int, size: int):
        """Constructor

        :param constant_value: the fixed value
        :param size: length of the sequence
        """
        self._constant_value = constant_value
        self._size = size

    def __len__(self):
        return self._size

    def __getitem__(self, item_idx) ->int:
        if item_idx >= len(self):
            raise IndexError()
        return self._constant_value

    def subset(self, indices: List[int]) ->'ConstantSequence':
        """Subset

        :param indices: indices of the new data.
        :return:
        """
        return ConstantSequence(self._constant_value, len(indices))

    def concat(self, other: 'FlatData'):
        """Concatenation

        :param other: other dataset
        :return:
        """
        if isinstance(other, ConstantSequence) and self._constant_value == other._constant_value:
            return ConstantSequence(self._constant_value, len(self) + len(other))
        else:
            return FlatData([self, other])

    def __str__(self):
        return f'ConstantSequence(value={self._constant_value}, len={self._size}'

    def __hash__(self):
        return id(self)


class MultiHeadClassifier(MultiTaskModule):
    """Multi-head classifier with separate heads for each task.

    Typically used in task-incremental benchmarks where task labels are
    available and provided to the model.

    .. note::
        Each output head may have a different shape, and the number of
        classes can be determined automatically.

        However, since pytorch doest not support jagged tensors, when you
        compute a minibatch's output you must ensure that each sample
        has the same output size, otherwise the model will fail to
        concatenate the samples together.

        These can be easily ensured in two possible ways:

        - each minibatch contains a single task, which is the case in most
            common benchmarks in Avalanche. Some exceptions to this setting
            are multi-task replay or cumulative strategies.
        - each head has the same size, which can be enforced by setting a
            large enough `initial_out_features`.
    """

    def __init__(self, in_features, initial_out_features=2, masking=True, mask_value=-1000):
        """Init.

        :param in_features: number of input features.
        :param initial_out_features: initial number of classes (can be
            dynamically expanded).
        :param masking: whether unused units should be masked (default=True).
        :param mask_value: the value used for masked units (default=-1000).
        """
        super().__init__()
        self.masking = masking
        self.mask_value = mask_value
        self.in_features = in_features
        self.starting_out_features = initial_out_features
        self.classifiers = torch.nn.ModuleDict()
        first_head = IncrementalClassifier(self.in_features, self.starting_out_features, masking=False)
        self.classifiers['0'] = first_head
        self.max_class_label = max(self.max_class_label, initial_out_features)
        au_init = torch.zeros(initial_out_features, dtype=torch.bool)
        self.register_buffer('active_units_T0', au_init)

    @property
    def active_units(self):
        res = {}
        for tid in self.known_train_tasks_labels:
            mask = getattr(self, f'active_units_T{tid}')
            au = torch.arange(0, mask.shape[0])[mask].tolist()
            res[tid] = au
        return res

    @property
    def task_masks(self):
        res = {}
        for tid in self.known_train_tasks_labels:
            res[tid] = getattr(self, f'active_units_T{tid}')
        return res

    def adaptation(self, experience: CLExperience):
        """If `dataset` contains new tasks, a new head is initialized.

        :param experience: data from the current experience.
        :return:
        """
        super().adaptation(experience)
        curr_classes = experience.classes_in_this_experience
        task_labels = experience.task_labels
        if isinstance(task_labels, ConstantSequence):
            task_labels = [task_labels[0]]
        for tid in set(task_labels):
            tid = str(tid)
            if tid not in self.classifiers:
                new_head = IncrementalClassifier(self.in_features, self.starting_out_features)
                self.classifiers[tid] = new_head
                au_init = torch.zeros(self.starting_out_features, dtype=torch.bool)
                self.register_buffer(f'active_units_T{tid}', au_init)
            self.classifiers[tid].adaptation(experience)
            if self.masking:
                if len(task_labels) > 1:
                    raise NotImplementedError('Multi-Head unit masking is not supported when experiences have multiple task labels. Set masking=False in your MultiHeadClassifier to disable masking.')
                au_name = f'active_units_T{tid}'
                curr_head = self.classifiers[tid]
                old_nunits = self._buffers[au_name].shape[0]
                new_nclasses = max(curr_head.classifier.out_features, max(curr_classes) + 1)
                if old_nunits != new_nclasses:
                    old_act_units = self._buffers[au_name]
                    self._buffers[au_name] = torch.zeros(new_nclasses, dtype=torch.bool)
                    self._buffers[au_name][:old_act_units.shape[0]] = old_act_units
                if self.training:
                    self._buffers[au_name][curr_classes] = 1

    def forward_single_task(self, x, task_label):
        """compute the output given the input `x`. This module uses the task
        label to activate the correct head.

        :param x:
        :param task_label:
        :return:
        """
        task_label = str(task_label)
        out = self.classifiers[task_label](x)
        if self.masking:
            au_name = f'active_units_T{task_label}'
            curr_au = self._buffers[au_name]
            nunits, oldsize = out.shape[-1], curr_au.shape[0]
            if oldsize < nunits:
                old_mask = self._buffers[au_name]
                self._buffers[au_name] = torch.zeros(nunits, dtype=torch.bool)
                self._buffers[au_name][:oldsize] = old_mask
                curr_au = self._buffers[au_name]
            out[..., torch.logical_not(curr_au)] = self.mask_value
        return out


class TrainEvalModel(DynamicModule):
    """
    TrainEvalModel.
    This module allows to wrap together a common feature extractor and
    two classifiers: one used during training time and another
    used at test time. The classifier is switched when `self.adaptation()`
    is called.
    """

    def __init__(self, feature_extractor, train_classifier, eval_classifier):
        """
        :param feature_extractor: a differentiable feature extractor
        :param train_classifier: a differentiable classifier used
            during training
        :param eval_classifier: a classifier used during testing.
            Doesn't have to be differentiable.
        """
        super().__init__()
        self.feature_extractor = feature_extractor
        self.train_classifier = train_classifier
        self.eval_classifier = eval_classifier
        self.classifier = train_classifier

    def forward(self, x):
        x = self.feature_extractor(x)
        return self.classifier(x)

    def train_adaptation(self, experience: CLExperience=None):
        self.classifier = self.train_classifier

    def eval_adaptation(self, experience: CLExperience=None):
        self.classifier = self.eval_classifier


class MLP(nn.Module):
    """
    Simple nn.Module to create a multi-layer perceptron
    with BatchNorm and ReLU activations.

    :param hidden_size: An array indicating the number of neurons in each layer.
    :type hidden_size: int[]
    :param last_activation: Indicates whether to add BatchNorm and ReLU
                            after the last layer.
    :type last_activation: Boolean
    """

    def __init__(self, hidden_size, last_activation=True):
        super(MLP, self).__init__()
        q = []
        for i in range(len(hidden_size) - 1):
            in_dim = hidden_size[i]
            out_dim = hidden_size[i + 1]
            q.append(('Linear_%d' % i, nn.Linear(in_dim, out_dim)))
            if i < len(hidden_size) - 2 or i == len(hidden_size) - 2 and last_activation:
                q.append(('BatchNorm_%d' % i, nn.BatchNorm1d(out_dim)))
                q.append(('ReLU_%d' % i, nn.ReLU(inplace=True)))
        self.mlp = nn.Sequential(OrderedDict(q))

    def forward(self, x):
        return self.mlp(x)


class VAEMLPEncoder(nn.Module):
    """
    Encoder part of the VAE, computer the latent represenations of the input.

    :param shape: Shape of the input to the network: (channels, height, width)
    :param latent_dim: Dimension of last hidden layer
    """

    def __init__(self, shape, latent_dim=128):
        super(VAEMLPEncoder, self).__init__()
        flattened_size = torch.Size(shape).numel()
        self.encode = nn.Sequential(Flatten(), nn.Linear(in_features=flattened_size, out_features=400), nn.BatchNorm1d(400), nn.LeakyReLU(), MLP([400, latent_dim]))

    def forward(self, x, y=None):
        x = self.encode(x)
        return x


class VAEMLPDecoder(nn.Module):
    """
    Decoder part of the VAE. Reverses Encoder.

    :param shape: Shape of output: (channels, height, width).
    :param nhid: Dimension of input.
    """

    def __init__(self, shape, nhid=16):
        super(VAEMLPDecoder, self).__init__()
        flattened_size = torch.Size(shape).numel()
        self.shape = shape
        self.decode = nn.Sequential(MLP([nhid, 64, 128, 256, flattened_size], last_activation=False), nn.Sigmoid())
        self.invTrans = transforms.Compose([transforms.Normalize((0.1307,), (0.3081,))])

    def forward(self, z, y=None):
        if y is None:
            return self.invTrans(self.decode(z).view(-1, *self.shape))
        else:
            return self.invTrans(self.decode(torch.cat((z, y), dim=1)).view(-1, *self.shape))


class MlpVAE(Generator, nn.Module):
    """
    Variational autoencoder module:
    fully-connected and suited for any input shape and type.

    The encoder only computes the latent represenations
    and we have then two possible output heads:
    One for the usual output distribution and one for classification.
    The latter is an extension the conventional VAE and incorporates
    a classifier into the network.
    More details can be found in: https://arxiv.org/abs/1809.10635
    """

    def __init__(self, shape, nhid=16, n_classes=10, device='cpu'):
        """
        :param shape: Shape of each input sample
        :param nhid: Dimension of latent space of Encoder.
        :param n_classes: Number of classes -
                        defines classification head's dimension
        """
        super(MlpVAE, self).__init__()
        self.dim = nhid
        if device is None:
            device = 'cpu'
        self.device = torch.device(device)
        self.encoder = VAEMLPEncoder(shape, latent_dim=128)
        self.calc_mean = MLP([128, nhid], last_activation=False)
        self.calc_logvar = MLP([128, nhid], last_activation=False)
        self.classification = MLP([128, n_classes], last_activation=False)
        self.decoder = VAEMLPDecoder(shape, nhid)

    def get_features(self, x):
        """
        Get features for encoder part given input x
        """
        return self.encoder(x)

    def generate(self, batch_size=None):
        """
        Generate random samples.
        Output is either a single sample if batch_size=None,
        else it is a batch of samples of size "batch_size".
        """
        z = torch.randn((batch_size, self.dim)) if batch_size else torch.randn((1, self.dim))
        res = self.decoder(z)
        if not batch_size:
            res = res.squeeze(0)
        return res

    def sampling(self, mean, logvar):
        """
        VAE 'reparametrization trick'
        """
        eps = torch.randn(mean.shape)
        sigma = 0.5 * torch.exp(logvar)
        return mean + eps * sigma

    def forward(self, x):
        """
        Forward.
        """
        represntations = self.encoder(x)
        mean, logvar = self.calc_mean(represntations), self.calc_logvar(represntations)
        z = self.sampling(mean, logvar)
        return self.decoder(z), mean, logvar


class MultiTaskDecorator(MultiTaskModule):
    """
    Encapsulates an existing nn.Module to make it subclass MultiTaskModule,
    the user should still be able to interact with the encapsulated module
    as if it was the module itself.

    The only things that change are the following, the classifier from the
    given model will be replaced by a MultiHeadClassifier, and the forward()
    implementation will be overwritten by one that accepts task labels.
    The encapsulated module will then be automatically extended to
    fit new classes during calls to model.adaptation()
    """

    def __init__(self, model: nn.Module, classifier_name: str):
        """
        :param model: pytorch nn.Module that does not support multitask
        :param classifier_name: attribute name of the existing classification
                                layer inside the module
        """
        for m in model.modules():
            assert not isinstance(m, MultiTaskModule)
        self.__dict__['_initialized'] = False
        super().__init__()
        self.model = model
        self.classifier_name = classifier_name
        old_classifier = getattr(model, classifier_name)
        if isinstance(old_classifier, nn.Linear):
            in_size = old_classifier.in_features
            out_size = old_classifier.out_features
            old_params = [torch.clone(p.data) for p in old_classifier.parameters()]
            setattr(self.model, classifier_name, nn.Sequential())
        elif isinstance(old_classifier, nn.Sequential):
            in_size = old_classifier[-1].in_features
            out_size = old_classifier[-1].out_features
            old_params = [torch.clone(p.data) for p in old_classifier[-1].parameters()]
            del old_classifier[-1]
        else:
            raise NotImplementedError(f'Cannot handle the following type             of classification layer {type(old_classifier)}')
        setattr(self, classifier_name, MultiHeadClassifier(in_size, out_size))
        for param, param_old in zip(getattr(self, classifier_name).parameters(), old_params):
            param.data = param_old
        self.max_class_label = max(self.max_class_label, out_size)
        self._initialized = True

    def forward_single_task(self, x: torch.Tensor, task_label: int):
        out = self.model(x)
        return getattr(self, self.classifier_name)(out.view(out.size(0), -1), task_labels=task_label)

    def forward_all_tasks(self, x: torch.Tensor):
        """compute the output given the input `x` and task label.
        By default, it considers only tasks seen at training time.

        :param x:
        :return: all the possible outputs are returned as a dictionary
            with task IDs as keys and the output of the corresponding
            task as output.
        """
        out = self.model(x)
        return getattr(self, self.classifier_name)(out.view(out.size(0), -1), task_labels=None)

    def __getattr__(self, name):
        if name == 'model':
            return self.__dict__['_modules']['model']
        if name == self.classifier_name:
            return self.__dict__['_modules'][self.classifier_name]
        return getattr(self.model, name)

    def __setattr__(self, name, value):
        if not self.__dict__['_initialized'] or name in self.__dict__:
            super().__setattr__(name, value)
        else:
            return setattr(self.model, name, value)


class IdentityShortcut(Module):

    def __init__(self, transform_function: Callable[[Tensor], Tensor]):
        super(IdentityShortcut, self).__init__()
        self.transform_function = transform_function

    def forward(self, x: Tensor) ->Tensor:
        return self.transform_function(x)


def batch_norm(num_channels: int) ->BatchNorm2d:
    return BatchNorm2d(num_channels)


def conv3x3(in_planes, out_planes, stride=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


class ResidualBlock(Module):

    def __init__(self, input_num_filters: int, increase_dim: bool=False, projection: bool=False, last: bool=False):
        super().__init__()
        self.last: bool = last
        if increase_dim:
            first_stride = 2, 2
            out_num_filters = input_num_filters * 2
        else:
            first_stride = 1, 1
            out_num_filters = input_num_filters
        self.direct = Sequential(conv3x3(input_num_filters, out_num_filters, stride=first_stride), batch_norm(out_num_filters), ReLU(True), conv3x3(out_num_filters, out_num_filters, stride=(1, 1)), batch_norm(out_num_filters))
        self.shortcut: Module
        if increase_dim:
            if projection:
                self.shortcut = Sequential(Conv2d(input_num_filters, out_num_filters, kernel_size=(1, 1), stride=(2, 2), bias=False), batch_norm(out_num_filters))
            else:
                self.shortcut = Sequential(IdentityShortcut(lambda x: x[:, :, ::2, ::2]), ConstantPad3d((0, 0, 0, 0, out_num_filters // 4, out_num_filters // 4), 0.0))
        else:
            self.shortcut = Identity()

    def forward(self, x):
        if self.last:
            return self.direct(x) + self.shortcut(x)
        else:
            return torch.relu(self.direct(x) + self.shortcut(x))


class IcarlNet(Module):

    def __init__(self, num_classes: int, n=5, c=3):
        super().__init__()
        self.is_train = True
        input_dims = c
        output_dims = 16
        first_conv = Sequential(conv3x3(input_dims, output_dims, stride=(1, 1)), batch_norm(16), ReLU(True))
        input_dims = output_dims
        output_dims = 16
        layers_list = []
        for _ in range(n):
            layers_list.append(ResidualBlock(input_dims))
        first_block = Sequential(*layers_list)
        input_dims = output_dims
        output_dims = 32
        layers_list = [ResidualBlock(input_dims, increase_dim=True)]
        for _ in range(1, n):
            layers_list.append(ResidualBlock(output_dims))
        second_block = Sequential(*layers_list)
        input_dims = output_dims
        output_dims = 64
        layers_list = [ResidualBlock(input_dims, increase_dim=True)]
        for _ in range(1, n - 1):
            layers_list.append(ResidualBlock(output_dims))
        layers_list.append(ResidualBlock(output_dims, last=True))
        third_block = Sequential(*layers_list)
        final_pool = AdaptiveAvgPool2d(output_size=(1, 1))
        self.feature_extractor = Sequential(first_conv, first_block, second_block, third_block, final_pool, Flatten())
        input_dims = output_dims
        output_dims = num_classes
        self.classifier = Linear(input_dims, output_dims)

    def forward(self, x):
        x = self.feature_extractor(x)
        x = self.classifier(x)
        return x


class LeNet5(nn.Module):

    def __init__(self, n_classes, input_channels):
        """LeNet5 architecture.

        :param n_classes:
        :param input_channels:
        """
        super(LeNet5, self).__init__()
        self.feature_extractor = nn.Sequential(nn.Conv2d(in_channels=input_channels, out_channels=6, kernel_size=5, stride=1), nn.Tanh(), nn.AvgPool2d(kernel_size=2), nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1), nn.Tanh(), nn.AvgPool2d(kernel_size=2), nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1), nn.Tanh())
        self.ff = nn.Sequential(nn.Linear(in_features=120, out_features=84), nn.Tanh())
        self.classifier = nn.Sequential(nn.Linear(in_features=84, out_features=n_classes))

    def forward(self, x):
        x = self.feature_extractor(x)
        x = torch.flatten(x, 1)
        x = self.ff(x)
        logits = self.classifier(x)
        return logits


class SimpleMLP_TinyImageNet(nn.Module):
    """Multi-layer Perceptron for TinyImageNet benchmark."""

    def __init__(self, num_classes=200, num_channels=3):
        """
        :param num_classes: model output size
        :param num_channels: number of input channels
        """
        super(SimpleMLP_TinyImageNet, self).__init__()
        self.features = nn.Sequential(nn.Linear(num_channels * 64 * 64, 1024), nn.ReLU(inplace=True), nn.Dropout())
        self.classifier = nn.Linear(1024, num_classes)

    def forward(self, x):
        x = x.contiguous()
        x = x.view(x.size(0), -1)
        x = self.features(x)
        x = self.classifier(x)
        return x


def remove_DwsConvBlock(cur_layers):
    all_layers = []
    for layer in cur_layers:
        if isinstance(layer, DwsConvBlock):
            for ch in layer.children():
                all_layers.append(ch)
        else:
            all_layers.append(layer)
    return all_layers


def remove_sequential(network, all_layers):
    for layer in network.children():
        if isinstance(layer, nn.Sequential):
            remove_sequential(layer, all_layers)
        else:
            all_layers.append(layer)


class MobilenetV1(nn.Module):
    """MobileNet v1 implementation. This model
    can be instantiated from a pretrained network."""

    def __init__(self, pretrained=True, latent_layer_num=20):
        super().__init__()
        model = mobilenet_w1(pretrained=pretrained)
        model.features.final_pool = nn.AvgPool2d(4)
        all_layers = []
        remove_sequential(model, all_layers)
        all_layers = remove_DwsConvBlock(all_layers)
        lat_list = []
        end_list = []
        for i, layer in enumerate(all_layers[:-1]):
            if i <= latent_layer_num:
                lat_list.append(layer)
            else:
                end_list.append(layer)
        self.lat_features = nn.Sequential(*lat_list)
        self.end_features = nn.Sequential(*end_list)
        self.output = nn.Linear(1024, 50, bias=False)

    def forward(self, x, latent_input=None, return_lat_acts=False):
        if latent_input is not None:
            with torch.no_grad():
                orig_acts = self.lat_features(x)
            lat_acts = torch.cat((orig_acts, latent_input), 0)
        else:
            orig_acts = self.lat_features(x)
            lat_acts = orig_acts
        x = self.end_features(lat_acts)
        x = x.view(x.size(0), -1)
        logits = self.output(x)
        if return_lat_acts:
            return logits, orig_acts
        else:
            return logits


class NCMClassifier(nn.Module):
    """
    NCM Classifier.
    NCMClassifier performs nearest class mean classification
    measuring the distance between the input tensor and the
    ones stored in 'self.class_means'.
    """

    def __init__(self, class_mean=None):
        """
        :param class_mean: tensor of dimension (num_classes x feature_size)
            used to classify input patterns.
        """
        super().__init__()
        self.class_means = class_mean

    def forward(self, x):
        pred_inter = (x.T / torch.norm(x.T, dim=0)).T
        sqd = torch.cdist(self.class_means[:, :].T, pred_inter)
        return (-sqd).T


class LinearAdapter(nn.Module):
    """
    Linear adapter for Progressive Neural Networks.
    """

    def __init__(self, in_features, out_features_per_column, num_prev_modules):
        """
        :param in_features: size of each input sample
        :param out_features_per_column: size of each output sample
        :param num_prev_modules: number of previous modules
        """
        super().__init__()
        self.lat_layers = nn.ModuleList([])
        for _ in range(num_prev_modules):
            m = nn.Linear(in_features, out_features_per_column)
            self.lat_layers.append(m)

    def forward(self, x):
        assert len(x) == self.num_prev_modules
        hs = []
        for ii, lat in enumerate(self.lat_layers):
            hs.append(lat(x[ii]))
        return sum(hs)


class PNNColumn(nn.Module):
    """
    Progressive Neural Network column.
    """

    def __init__(self, in_features, out_features_per_column, num_prev_modules, adapter='mlp'):
        """
        :param in_features: size of each input sample
        :param out_features_per_column:
            size of each output sample (single column)
        :param num_prev_modules: number of previous columns
        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')
        """
        super().__init__()
        self.in_features = in_features
        self.out_features_per_column = out_features_per_column
        self.num_prev_modules = num_prev_modules
        self.itoh = nn.Linear(in_features, out_features_per_column)
        if adapter == 'linear':
            self.adapter = LinearAdapter(in_features, out_features_per_column, num_prev_modules)
        elif adapter == 'mlp':
            self.adapter = MLPAdapter(in_features, out_features_per_column, num_prev_modules)
        else:
            raise ValueError("`adapter` must be one of: {'mlp', `linear'}.")

    def freeze(self):
        for param in self.parameters():
            param.requires_grad = False

    def forward(self, x):
        prev_xs, last_x = x[:-1], x[-1]
        hs = self.adapter(prev_xs)
        hs += self.itoh(last_x)
        return hs


class PNNLayer(MultiTaskModule):
    """Progressive Neural Network layer.

    The adaptation phase assumes that each experience is a separate task.
    Multiple experiences with the same task label or multiple task labels
    within the same experience will result in a runtime error.
    """

    def __init__(self, in_features, out_features_per_column, adapter='mlp'):
        """
        :param in_features: size of each input sample
        :param out_features_per_column:
            size of each output sample (single column)
        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')
        """
        super().__init__()
        self.in_features = in_features
        self.out_features_per_column = out_features_per_column
        self.adapter = adapter
        self.task_to_module_idx = {}
        first_col = PNNColumn(in_features, out_features_per_column, 0, adapter=adapter)
        self.columns = nn.ModuleList([first_col])

    @property
    def num_columns(self):
        return len(self.columns)

    def adaptation(self, experience: CLExperience):
        """Training adaptation for PNN layer.

        Adds an additional column to the layer.

        :param dataset:
        :return:
        """
        dataset = experience.dataset
        super().train_adaptation(experience)
        task_labels = dataset.targets_task_labels
        if isinstance(task_labels, ConstantSequence):
            task_labels = [task_labels[0]]
        else:
            task_labels = set(task_labels)
        assert len(task_labels) == 1, 'PNN assumes a single task for each experience. Please use a compatible benchmark.'
        task_label = next(iter(task_labels))
        if task_label in self.task_to_module_idx:
            return
        if len(self.task_to_module_idx) == 0:
            self.task_to_module_idx[task_label] = 0
        else:
            self.task_to_module_idx[task_label] = self.num_columns
            self._add_column()

    def _add_column(self):
        """Add a new column."""
        for param in self.parameters():
            param.requires_grad = False
        self.columns.append(PNNColumn(self.in_features, self.out_features_per_column, self.num_columns, adapter=self.adapter))

    def forward_single_task(self, x, task_label):
        """Forward.

        :param x: list of inputs.
        :param task_label:
        :return:
        """
        col_idx = self.task_to_module_idx[task_label]
        hs = []
        for ii in range(col_idx + 1):
            hs.append(self.columns[ii](x[:ii + 1]))
        return hs


class PNN(MultiTaskModule):
    """
    Progressive Neural Network.

    The model assumes that each experience is a separate task.
    Multiple experiences with the same task label or multiple task labels
    within the same experience will result in a runtime error.
    """

    def __init__(self, num_layers=1, in_features=784, hidden_features_per_column=100, adapter='mlp'):
        """
        :param num_layers: number of layers (default=1)
        :param in_features: size of each input sample
        :param hidden_features_per_column:
            number of hidden units for each column
        :param adapter: adapter type. One of {'linear', 'mlp'} (default='mlp')
        """
        super().__init__()
        assert num_layers >= 1
        self.num_layers = num_layers
        self.in_features = in_features
        self.out_features_per_columns = hidden_features_per_column
        self.layers = nn.ModuleList()
        self.layers.append(PNNLayer(in_features, hidden_features_per_column))
        for _ in range(num_layers - 1):
            lay = PNNLayer(hidden_features_per_column, hidden_features_per_column, adapter=adapter)
            self.layers.append(lay)
        self.classifier = MultiHeadClassifier(hidden_features_per_column)

    def forward_single_task(self, x, task_label):
        """Forward.

        :param x:
        :param task_label:
        :return:
        """
        x = x.contiguous()
        x = x.view(x.size(0), self.in_features)
        num_columns = self.layers[0].num_columns
        col_idx = self.layers[-1].task_to_module_idx[task_label]
        x = [x for _ in range(num_columns)]
        for lay in self.layers:
            x = [F.relu(el) for el in lay(x, task_label)]
        return self.classifier(x[col_idx], task_label)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(in_planes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion * planes))

    def forward(self, x):
        out = relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        return self.relu(out)


class ResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes, nf):
        super(ResNet, self).__init__()
        self.in_planes = nf
        self.conv1 = conv3x3(3, nf * 1)
        self.bn1 = nn.BatchNorm2d(nf * 1)
        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)
        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        bsz = x.size(0)
        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class SimpleCNN(nn.Module):
    """
    Convolutional Neural Network

    **Example**::

        >>> from avalanche.models import SimpleCNN
        >>> n_classes = 10 # e.g. MNIST
        >>> model = SimpleCNN(num_classes=n_classes)
        >>> print(model) # View model details
    """

    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        self.features = nn.Sequential(nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(inplace=True), nn.Conv2d(32, 32, kernel_size=3, padding=0), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.25), nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(64, 64, kernel_size=3, padding=0), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.25), nn.Conv2d(64, 64, kernel_size=1, padding=0), nn.ReLU(inplace=True), nn.AdaptiveMaxPool2d(1), nn.Dropout(p=0.25))
        self.classifier = nn.Sequential(nn.Linear(64, num_classes))

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x


class MTSimpleCNN(SimpleCNN, MultiTaskModule):
    """
    Convolutional Neural Network
    with multi-head classifier
    """

    def __init__(self):
        super().__init__()
        self.classifier = MultiHeadClassifier(64)

    def forward(self, x, task_labels):
        x = self.features(x)
        x = x.squeeze()
        x = self.classifier(x, task_labels)
        return x


class BaseModel(ABC):
    """
    A base abstract class for models
    """

    @abstractmethod
    def get_features(self, x):
        """
        Get features from model given input
        """


class SimpleMLP(nn.Module, BaseModel):
    """
    Multi-Layer Perceptron with custom parameters.
    It can be configured to have multiple layers and dropout.

    **Example**::

        >>> from avalanche.models import SimpleMLP
        >>> n_classes = 10 # e.g. MNIST
        >>> model = SimpleMLP(num_classes=n_classes)
        >>> print(model) # View model details
    """

    def __init__(self, num_classes=10, input_size=28 * 28, hidden_size=512, hidden_layers=1, drop_rate=0.5):
        """
        :param num_classes: output size
        :param input_size: input size
        :param hidden_size: hidden layer size
        :param hidden_layers: number of hidden layers
        :param drop_rate: dropout rate. 0 to disable
        """
        super().__init__()
        layers = nn.Sequential(*(nn.Linear(input_size, hidden_size), nn.ReLU(inplace=True), nn.Dropout(p=drop_rate)))
        for layer_idx in range(hidden_layers - 1):
            layers.add_module(f'fc{layer_idx + 1}', nn.Sequential(*(nn.Linear(hidden_size, hidden_size), nn.ReLU(inplace=True), nn.Dropout(p=drop_rate))))
        self.features = nn.Sequential(*layers)
        self.classifier = nn.Linear(hidden_size, num_classes)
        self._input_size = input_size

    def forward(self, x):
        x = x.contiguous()
        x = x.view(x.size(0), self._input_size)
        x = self.features(x)
        x = self.classifier(x)
        return x

    def get_features(self, x):
        x = x.contiguous()
        x = x.view(x.size(0), self._input_size)
        x = self.features(x)
        return x


class MTSimpleMLP(MultiTaskModule):
    """Multi-layer perceptron with multi-head classifier"""

    def __init__(self, input_size=28 * 28, hidden_size=512):
        super().__init__()
        self.features = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU(inplace=True), nn.Dropout())
        self.classifier = MultiHeadClassifier(hidden_size)
        self._input_size = input_size

    def forward(self, x, task_labels):
        x = x.contiguous()
        x = x.view(x.size(0), self._input_size)
        x = self.features(x)
        x = self.classifier(x, task_labels)
        return x


class SimpleSequenceClassifier(torch.nn.Module):

    def __init__(self, input_size, hidden_size, n_classes, rnn_layers=1, batch_first=True):
        super().__init__()
        self.batch_first = batch_first
        self.rnn = torch.nn.LSTM(input_size, hidden_size, num_layers=rnn_layers, batch_first=batch_first)
        self.classifier = torch.nn.Linear(hidden_size, n_classes)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = out[:, -1] if self.batch_first else out[-1]
        out = self.classifier(out)
        return out


class MTSimpleSequenceClassifier(MultiTaskModule):

    def __init__(self, input_size, hidden_size, rnn_layers=1, batch_first=True):
        super().__init__()
        self.batch_first = batch_first
        self.rnn = torch.nn.LSTM(input_size, hidden_size, num_layers=rnn_layers, batch_first=batch_first)
        self.classifier = MultiHeadClassifier(hidden_size)

    def forward(self, x, task_labels):
        out, _ = self.rnn(x)
        out = out[:, -1] if self.batch_first else out[-1]
        out = self.classifier(out, task_labels)
        return out


class FeatureExtractorBackbone(nn.Module):
    """
    This PyTorch module allows us to extract features from a backbone network
    given a layer name.
    """

    def __init__(self, model, output_layer_name):
        super(FeatureExtractorBackbone, self).__init__()
        self.model = model
        self.output_layer_name = output_layer_name
        self.output = None
        self.add_hooks(self.model)

    def forward(self, x):
        self.model(x)
        return self.output

    def get_name_to_module(self, model):
        name_to_module = {}
        for m in model.named_modules():
            name_to_module[m[0]] = m[1]
        return name_to_module

    def get_activation(self):

        def hook(model, input, output):
            self.output = output.detach()
        return hook

    def add_hooks(self, model):
        """
        :param model:
        :param outputs: Outputs from layers specified in `output_layer_names`
        will be stored in `output` variable
        :param output_layer_names:
        :return:
        """
        name_to_module = self.get_name_to_module(model)
        name_to_module[self.output_layer_name].register_forward_hook(self.get_activation())


class SLDAResNetModel(nn.Module):
    """
    This is a model wrapper to reproduce experiments from the original
    paper of Deep Streaming Linear Discriminant Analysis by using
    a pretrained ResNet model.
    """

    def __init__(self, arch='resnet18', output_layer_name='layer4.1', imagenet_pretrained=True, device='cpu'):
        """Init.

        :param arch: backbone architecture. Default is resnet-18, but others
            can be used by modifying layer for
            feature extraction in ``self.feature_extraction_wrapper``.
        :param imagenet_pretrained: True if initializing backbone with imagenet
            pre-trained weights else False
        :param output_layer_name: name of the layer from feature extractor
        :param device: cpu, gpu or other device
        """
        super(SLDAResNetModel, self).__init__()
        feat_extractor = models.__dict__[arch](pretrained=imagenet_pretrained).eval()
        self.feature_extraction_wrapper = FeatureExtractorBackbone(feat_extractor, output_layer_name).eval()

    @staticmethod
    def pool_feat(features):
        feat_size = features.shape[-1]
        num_channels = features.shape[1]
        features2 = features.permute(0, 2, 3, 1)
        features3 = torch.reshape(features2, (features.shape[0], feat_size * feat_size, num_channels))
        feat = features3.mean(1)
        return feat

    def forward(self, x):
        """
        :param x: raw x data
        """
        feat = self.feature_extraction_wrapper(x)
        feat = SLDAResNetModel.pool_feat(feat)
        return feat


class MTSlimResNet18(MultiTaskModule, DynamicModule):
    """MultiTask Slimmed ResNet18."""

    def __init__(self, nclasses, nf=20):
        super().__init__()
        self.in_planes = nf
        block = BasicBlock
        num_blocks = [2, 2, 2, 2]
        self.conv1 = conv3x3(3, nf * 1)
        self.bn1 = nn.BatchNorm2d(nf * 1)
        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)
        self.linear = MultiHeadClassifier(nf * 8 * BasicBlock.expansion, nclasses)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x, task_labels):
        bsz = x.size(0)
        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out, task_labels)
        return out


class Flatten(nn.Module):
    """
    Simple nn.Module to flatten each tensor of a batch of tensors.
    """

    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        batch_size = x.shape[0]
        return x.view(batch_size, -1)


class L2Normalization(Module):
    """Module to L2-normalize the input. Typically used in last layer to
    normalize the embedding."""

    def __init__(self):
        super().__init__()

    def forward(self, x: Tensor) ->Tensor:
        return torch.nn.functional.normalize(x, p=2, dim=1)


class ToTensor(nn.Module):

    def forward(self, image: Tensor, target: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Dict[str, Tensor]]]:
        image = F.pil_to_tensor(image)
        image = F.convert_image_dtype(image)
        return image, target


class PILToTensor(nn.Module):

    def forward(self, image: Tensor, target: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Dict[str, Tensor]]]:
        image = F.pil_to_tensor(image)
        return image, target


class ConvertImageDtype(nn.Module):

    def __init__(self, dtype: torch.dtype) ->None:
        super().__init__()
        self.dtype = dtype

    def forward(self, image: Tensor, target: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Dict[str, Tensor]]]:
        image = F.convert_image_dtype(image, self.dtype)
        return image, target


class RandomIoUCrop(nn.Module):

    def __init__(self, min_scale: float=0.3, max_scale: float=1.0, min_aspect_ratio: float=0.5, max_aspect_ratio: float=2.0, sampler_options: Optional[List[float]]=None, trials: int=40):
        super().__init__()
        self.min_scale = min_scale
        self.max_scale = max_scale
        self.min_aspect_ratio = min_aspect_ratio
        self.max_aspect_ratio = max_aspect_ratio
        if sampler_options is None:
            sampler_options = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]
        self.options = sampler_options
        self.trials = trials

    def forward(self, image: Tensor, target: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Dict[str, Tensor]]]:
        if target is None:
            raise ValueError("The targets can't be None for this transform.")
        if isinstance(image, torch.Tensor):
            if image.ndimension() not in {2, 3}:
                raise ValueError(f'image should be 2/3 dimensional. Got {image.ndimension()} dimensions.')
            elif image.ndimension() == 2:
                image = image.unsqueeze(0)
        orig_w, orig_h = F.get_image_size(image)
        while True:
            idx = int(torch.randint(low=0, high=len(self.options), size=(1,)))
            min_jaccard_overlap = self.options[idx]
            if min_jaccard_overlap >= 1.0:
                return image, target
            for _ in range(self.trials):
                r = self.min_scale + (self.max_scale - self.min_scale) * torch.rand(2)
                new_w = int(orig_w * r[0])
                new_h = int(orig_h * r[1])
                aspect_ratio = new_w / new_h
                if not self.min_aspect_ratio <= aspect_ratio <= self.max_aspect_ratio:
                    continue
                r = torch.rand(2)
                left = int((orig_w - new_w) * r[0])
                top = int((orig_h - new_h) * r[1])
                right = left + new_w
                bottom = top + new_h
                if left == right or top == bottom:
                    continue
                cx = 0.5 * (target['boxes'][:, 0] + target['boxes'][:, 2])
                cy = 0.5 * (target['boxes'][:, 1] + target['boxes'][:, 3])
                is_within_crop_area = (left < cx) & (cx < right) & (top < cy) & (cy < bottom)
                if not is_within_crop_area.any():
                    continue
                boxes = target['boxes'][is_within_crop_area]
                ious = torchvision.ops.boxes.box_iou(boxes, torch.tensor([[left, top, right, bottom]], dtype=boxes.dtype, device=boxes.device))
                if ious.max() < min_jaccard_overlap:
                    continue
                target['boxes'] = boxes
                target['labels'] = target['labels'][is_within_crop_area]
                target['boxes'][:, 0::2] -= left
                target['boxes'][:, 1::2] -= top
                target['boxes'][:, 0::2].clamp_(min=0, max=new_w)
                target['boxes'][:, 1::2].clamp_(min=0, max=new_h)
                image = F.crop(image, top, left, new_h, new_w)
                return image, target


class RandomZoomOut(nn.Module):

    def __init__(self, fill: Optional[List[float]]=None, side_range: Tuple[float, float]=(1.0, 4.0), p: float=0.5):
        super().__init__()
        if fill is None:
            fill = [0.0, 0.0, 0.0]
        self.fill = fill
        self.side_range = side_range
        if side_range[0] < 1.0 or side_range[0] > side_range[1]:
            raise ValueError(f'Invalid canvas side range provided {side_range}.')
        self.p = p

    @torch.jit.unused
    def _get_fill_value(self, is_pil):
        return tuple(int(x) for x in self.fill) if is_pil else 0

    def forward(self, image: Tensor, target: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Dict[str, Tensor]]]:
        if isinstance(image, torch.Tensor):
            if image.ndimension() not in {2, 3}:
                raise ValueError(f'image should be 2/3 dimensional. Got {image.ndimension()} dimensions.')
            elif image.ndimension() == 2:
                image = image.unsqueeze(0)
        if torch.rand(1) >= self.p:
            return image, target
        orig_w, orig_h = F.get_image_size(image)
        r = self.side_range[0] + torch.rand(1) * (self.side_range[1] - self.side_range[0])
        canvas_width = int(orig_w * r)
        canvas_height = int(orig_h * r)
        r = torch.rand(2)
        left = int((canvas_width - orig_w) * r[0])
        top = int((canvas_height - orig_h) * r[1])
        right = canvas_width - (left + orig_w)
        bottom = canvas_height - (top + orig_h)
        if torch.jit.is_scripting():
            fill = 0
        else:
            fill = self._get_fill_value(F._is_pil_image(image))
        image = F.pad(image, [left, top, right, bottom], fill=fill)
        if isinstance(image, torch.Tensor):
            v = torch.tensor(self.fill, device=image.device, dtype=image.dtype).view(-1, 1, 1)
            image[..., :top, :] = image[..., :, :left] = image[..., top + orig_h:, :] = image[..., :, left + orig_w:] = v
        if target is not None:
            target['boxes'][:, 0::2] += left
            target['boxes'][:, 1::2] += top
        return image, target


class RandomPhotometricDistort(nn.Module):

    def __init__(self, contrast: Tuple[float]=(0.5, 1.5), saturation: Tuple[float]=(0.5, 1.5), hue: Tuple[float]=(-0.05, 0.05), brightness: Tuple[float]=(0.875, 1.125), p: float=0.5):
        super().__init__()
        self._brightness = T.ColorJitter(brightness=brightness)
        self._contrast = T.ColorJitter(contrast=contrast)
        self._hue = T.ColorJitter(hue=hue)
        self._saturation = T.ColorJitter(saturation=saturation)
        self.p = p

    def forward(self, image: Tensor, target: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Dict[str, Tensor]]]:
        if isinstance(image, torch.Tensor):
            if image.ndimension() not in {2, 3}:
                raise ValueError(f'image should be 2/3 dimensional. Got {image.ndimension()} dimensions.')
            elif image.ndimension() == 2:
                image = image.unsqueeze(0)
        r = torch.rand(7)
        if r[0] < self.p:
            image = self._brightness(image)
        contrast_before = r[1] < 0.5
        if contrast_before:
            if r[2] < self.p:
                image = self._contrast(image)
        if r[3] < self.p:
            image = self._saturation(image)
        if r[4] < self.p:
            image = self._hue(image)
        if not contrast_before:
            if r[5] < self.p:
                image = self._contrast(image)
        if r[6] < self.p:
            channels = F.get_image_num_channels(image)
            permutation = torch.randperm(channels)
            is_pil = F._is_pil_image(image)
            if is_pil:
                image = F.pil_to_tensor(image)
                image = F.convert_image_dtype(image)
            image = image[..., permutation, :, :]
            if is_pil:
                image = F.to_pil_image(image)
        return image, target


class _PlainMLP(nn.Module, BaseModel):
    """
    An internal MLP implementation without Dropout.

    Needed to reproduce tests for the ReduceLROnPlateau scheduler
    """

    def __init__(self, num_classes=10, input_size=28 * 28, hidden_size=512, hidden_layers=1):
        super().__init__()
        layers = nn.Sequential(*(nn.Linear(input_size, hidden_size), nn.ReLU(inplace=True)))
        for layer_idx in range(hidden_layers - 1):
            layers.add_module(f'fc{layer_idx + 1}', nn.Sequential(*(nn.Linear(hidden_size, hidden_size), nn.ReLU(inplace=True))))
        self.features = nn.Sequential(*layers)
        self.classifier = nn.Linear(hidden_size, num_classes)
        self._input_size = input_size

    def forward(self, x):
        x = x.contiguous()
        x = x.view(x.size(0), self._input_size)
        x = self.features(x)
        x = self.classifier(x)
        return x

    def get_features(self, x):
        x = x.contiguous()
        x = x.view(x.size(0), self._input_size)
        x = self.features(x)
        return x


class AbsLayer(Module):

    def forward(self, x: Tensor) ->Tensor:
        return torch.abs(x).reshape((-1, 1))


class AbsModel(Module):
    """Fake model, that simply compute the absolute value of the inputs"""

    def __init__(self):
        super().__init__()
        self.features = AbsLayer()
        self.classifier = Identity()

    def forward(self, x: Tensor) ->Tensor:
        x = self.features(x)
        x = self.classifier(x)
        return x


class TestMLP(nn.Module):

    def __init__(self, num_classes=10, input_size=6, hidden_size=50):
        super().__init__()
        self.layers = nn.Sequential(nn.Linear(input_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, hidden_size), nn.Tanh())
        self.classifier = nn.Linear(hidden_size, num_classes)
        self._hidden_size = hidden_size
        self._input_size = input_size

    def forward(self, x):
        x = x.contiguous()
        x = x.view(x.size(0), self._input_size)
        x = self.layers(x)
        x = self.classifier(x)
        return x


class MHTestMLP(TestMLP, MultiTaskModule):

    def __init__(self, num_classes=10, input_size=6, hidden_size=50):
        super().__init__()
        self.classifier = MultiHeadClassifier(self._hidden_size, num_classes)

    def forward(self, x, task_labels):
        x = self.layers(x)
        x = self.classifier(x, task_labels)
        return x


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AbsLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (AbsModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BasicBlock,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BatchRenorm2D,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ConvertImageDtype,
     lambda: ([], {'dtype': torch.float32}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (IcarlNet,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (IdentityShortcut,
     lambda: ([], {'transform_function': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (IncrementalClassifier,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (L2Normalization,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MLP,
     lambda: ([], {'hidden_size': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (RandomPhotometricDistort,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (RandomZoomOut,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (ResidualBlock,
     lambda: ([], {'input_num_filters': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SLDAResNetModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (SimpleCNN,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (SimpleMLP,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 784])], {}),
     True),
    (SimpleSequenceClassifier,
     lambda: ([], {'input_size': 4, 'hidden_size': 4, 'n_classes': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (TestMLP,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 6])], {}),
     True),
    (TrainEvalModel,
     lambda: ([], {'feature_extractor': _mock_layer(), 'train_classifier': _mock_layer(), 'eval_classifier': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (VAEMLPEncoder,
     lambda: ([], {'shape': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (_PlainMLP,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 784])], {}),
     True),
]

class Test_ContinualAI_avalanche(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])


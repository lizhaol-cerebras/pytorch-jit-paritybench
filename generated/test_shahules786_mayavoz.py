import sys
_module = sys.modules[__name__]
del sys
mayavoz = _module
train = _module
data = _module
dataset = _module
fileprocessor = _module
inference = _module
loss = _module
models = _module
complexnn = _module
conv = _module
rnn = _module
utils = _module
dccrn = _module
demucs = _module
model = _module
waveunet = _module
config = _module
io = _module
random = _module
transforms = _module
version = _module
train = _module
train = _module
train = _module
train = _module
setup = _module
loss_function_test = _module
complexnn_test = _module
demucs_test = _module
test_dccrn = _module
test_waveunet = _module
test_inference = _module
transforms_test = _module
utils_test = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from types import MethodType


from torch.optim.lr_scheduler import ReduceLROnPlateau


import math


import warnings


from typing import Optional


import numpy as np


import torch


import torch.nn.functional as F


from torch.utils.data import DataLoader


from torch.utils.data import Dataset


from torch.utils.data import RandomSampler


from typing import Union


from scipy.io import wavfile


from scipy.signal import get_window


import torch.nn as nn


from typing import Tuple


from torch import nn


from typing import List


from typing import Any


from collections import defaultdict


from typing import Text


from torch.optim import Adam


import torchaudio


import random


class mean_squared_error(nn.Module):
    """
    Mean squared error / L1 loss
    """

    def __init__(self, reduction='mean'):
        super().__init__()
        self.loss_fun = nn.MSELoss(reduction=reduction)
        self.higher_better = False
        self.name = 'mse'

    def forward(self, prediction: torch.Tensor, target: torch.Tensor):
        if prediction.size() != target.size() or target.ndim < 3:
            raise TypeError(f"""Inputs must be of the same shape (batch_size,channels,samples)
                    got {prediction.size()} and {target.size()} instead""")
        return self.loss_fun(prediction, target)


class mean_absolute_error(nn.Module):
    """
    Mean absolute error / L2 loss
    """

    def __init__(self, reduction='mean'):
        super().__init__()
        self.loss_fun = nn.L1Loss(reduction=reduction)
        self.higher_better = False
        self.name = 'mae'

    def forward(self, prediction: torch.Tensor, target: torch.Tensor):
        if prediction.size() != target.size() or target.ndim < 3:
            raise TypeError(f"""Inputs must be of the same shape (batch_size,channels,samples)
                            got {prediction.size()} and {target.size()} instead""")
        return self.loss_fun(prediction, target)


class Pesq:

    def __init__(self, sr: int, mode='wb'):
        self.sr = sr
        self.name = 'pesq'
        self.mode = mode
        self.pesq = PerceptualEvaluationSpeechQuality(fs=self.sr, mode=self.mode)

    def __call__(self, prediction: torch.Tensor, target: torch.Tensor):
        pesq_values = []
        for pred, target_ in zip(prediction, target):
            try:
                pesq_values.append(self.pesq(pred.squeeze(), target_.squeeze()))
            except Exception as e:
                warnings.warn(f'{e} error occured while calculating PESQ')
        return torch.tensor(np.mean(pesq_values))


class Si_SDR:
    """
    SI-SDR metric based on SDR â€“ HALF-BAKED OR WELL DONE?(https://arxiv.org/pdf/1811.02508.pdf)
    """

    def __init__(self, reduction: str='mean'):
        if reduction in ['sum', 'mean', None]:
            self.reduction = reduction
        else:
            raise TypeError('Invalid reduction, valid options are sum, mean, None')
        self.higher_better = True
        self.name = 'si-sdr'

    def __call__(self, prediction: torch.Tensor, target: torch.Tensor):
        if prediction.size() != target.size() or target.ndim < 3:
            raise TypeError(f"""Inputs must be of the same shape (batch_size,channels,samples)
                            got {prediction.size()} and {target.size()} instead""")
        target_energy = torch.sum(target ** 2, keepdim=True, dim=-1)
        scaling_factor = torch.sum(prediction * target, keepdim=True, dim=-1) / target_energy
        target_projection = target * scaling_factor
        noise = prediction - target_projection
        ratio = torch.sum(target_projection ** 2, dim=-1) / torch.sum(noise ** 2, dim=-1)
        si_sdr = 10 * torch.log10(ratio).mean(dim=-1)
        if self.reduction == 'sum':
            si_sdr = si_sdr.sum()
        elif self.reduction == 'mean':
            si_sdr = si_sdr.mean()
        else:
            pass
        return si_sdr


class Si_snr(nn.Module):
    """
    SI-SNR
    """

    def __init__(self, **kwargs):
        super().__init__()
        self.loss_fun = ScaleInvariantSignalNoiseRatio(**kwargs)
        self.higher_better = False
        self.name = 'si_snr'

    def forward(self, prediction: torch.Tensor, target: torch.Tensor):
        if prediction.size() != target.size() or target.ndim < 3:
            raise TypeError(f"""Inputs must be of the same shape (batch_size,channels,samples)
                    got {prediction.size()} and {target.size()} instead""")
        return -1 * self.loss_fun(prediction, target)


class Stoi:
    """
    STOI (Short-Time Objective Intelligibility, see [2,3]), a wrapper for the pystoi package [1].
    Note that input will be moved to cpu to perform the metric calculation.
    parameters:
        sr: int
            sampling rate
    """

    def __init__(self, sr: int):
        self.sr = sr
        self.stoi = ShortTimeObjectiveIntelligibility(fs=sr)
        self.name = 'stoi'

    def __call__(self, prediction: torch.Tensor, target: torch.Tensor):
        return self.stoi(prediction, target)


LOSS_MAP = {'mae': mean_absolute_error, 'mse': mean_squared_error, 'si-sdr': Si_SDR, 'pesq': Pesq, 'stoi': Stoi, 'si-snr': Si_snr}


class LossWrapper(nn.Module):
    """
    Combine multiple metics of same nature.
    for example, ["mea","mae"]
    parameters:
        losses : loss function names to be combined
    """

    def __init__(self, losses):
        super().__init__()
        self.valid_losses = nn.ModuleList()
        direction = [getattr(LOSS_MAP[loss](), 'higher_better') for loss in losses]
        if len(set(direction)) > 1:
            raise ValueError('all cost functions should be of same nature, maximize or minimize!')
        self.higher_better = direction[0]
        self.name = ''
        for loss in losses:
            loss = self.validate_loss(loss)
            self.valid_losses.append(loss())
            self.name += f'{loss().name}_'

    def validate_loss(self, loss: str):
        if loss not in LOSS_MAP.keys():
            raise ValueError(f"""Invalid loss function {loss}, available loss functions are
                    {tuple([loss for loss in LOSS_MAP.keys()])}""")
        else:
            return LOSS_MAP[loss]

    def forward(self, prediction: torch.Tensor, target: torch.Tensor):
        loss = 0.0
        for loss_fun in self.valid_losses:
            loss += loss_fun(prediction, target)
        return loss


def init_weights(nnet):
    nn.init.xavier_normal_(nnet.weight.data)
    nn.init.constant_(nnet.bias, 0.0)
    return nnet


class ComplexConv2d(nn.Module):

    def __init__(self, in_channels: int, out_channels: int, kernel_size: Tuple[int, int]=(1, 1), stride: Tuple[int, int]=(1, 1), padding: Tuple[int, int]=(0, 0), groups: int=1, dilation: int=1):
        """
        Complex Conv2d (non-causal)
        """
        super().__init__()
        self.in_channels = in_channels // 2
        self.out_channels = out_channels // 2
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.groups = groups
        self.dilation = dilation
        self.real_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=(self.padding[0], 0), groups=self.groups, dilation=self.dilation)
        self.imag_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=(self.padding[0], 0), groups=self.groups, dilation=self.dilation)
        self.imag_conv = init_weights(self.imag_conv)
        self.real_conv = init_weights(self.real_conv)

    def forward(self, input):
        """
        complex axis should be always 1 dim
        """
        input = F.pad(input, [self.padding[1], 0, 0, 0])
        real, imag = torch.chunk(input, 2, 1)
        real_real = self.real_conv(real)
        real_imag = self.imag_conv(real)
        imag_imag = self.imag_conv(imag)
        imag_real = self.real_conv(imag)
        real = real_real - imag_imag
        imag = real_imag - imag_real
        out = torch.cat([real, imag], 1)
        return out


class ComplexConvTranspose2d(nn.Module):

    def __init__(self, in_channels: int, out_channels: int, kernel_size: Tuple[int, int]=(1, 1), stride: Tuple[int, int]=(1, 1), padding: Tuple[int, int]=(0, 0), output_padding: Tuple[int, int]=(0, 0), groups: int=1):
        super().__init__()
        self.in_channels = in_channels // 2
        self.out_channels = out_channels // 2
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.groups = groups
        self.output_padding = output_padding
        self.real_conv = nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, output_padding=self.output_padding, groups=self.groups)
        self.imag_conv = nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, output_padding=self.output_padding, groups=self.groups)
        self.real_conv = init_weights(self.real_conv)
        self.imag_conv = init_weights(self.imag_conv)

    def forward(self, input):
        real, imag = torch.chunk(input, 2, 1)
        real_real = self.real_conv(real)
        real_imag = self.imag_conv(real)
        imag_imag = self.imag_conv(imag)
        imag_real = self.real_conv(imag)
        real = real_real - imag_imag
        imag = real_imag - imag_real
        out = torch.cat([real, imag], 1)
        return out


class ComplexLSTM(nn.Module):

    def __init__(self, input_size: int, hidden_size: int, num_layers: int=1, projection_size: Optional[int]=None, bidirectional: bool=False):
        super().__init__()
        self.input_size = input_size // 2
        self.hidden_size = hidden_size // 2
        self.num_layers = num_layers
        self.real_lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, bidirectional=bidirectional, batch_first=False)
        self.imag_lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, bidirectional=bidirectional, batch_first=False)
        bidirectional = 2 if bidirectional else 1
        if projection_size is not None:
            self.projection_size = projection_size // 2
            self.real_linear = nn.Linear(self.hidden_size * bidirectional, self.projection_size)
            self.imag_linear = nn.Linear(self.hidden_size * bidirectional, self.projection_size)
        else:
            self.projection_size = None

    def forward(self, input):
        if isinstance(input, List):
            real, imag = input
        else:
            real, imag = torch.chunk(input, 2, 1)
        real_real = self.real_lstm(real)[0]
        real_imag = self.imag_lstm(real)[0]
        imag_imag = self.imag_lstm(imag)[0]
        imag_real = self.real_lstm(imag)[0]
        real = real_real - imag_imag
        imag = imag_real + real_imag
        if self.projection_size is not None:
            real = self.real_linear(real)
            imag = self.imag_linear(imag)
        return [real, imag]


class ComplexBatchNorm2D(nn.Module):

    def __init__(self, num_features: int, eps: float=1e-05, momentum: float=0.1, affine: bool=True, track_running_stats: bool=True):
        """
        Complex batch normalization 2D
        https://arxiv.org/abs/1705.09792


        """
        super().__init__()
        self.num_features = num_features // 2
        self.affine = affine
        self.momentum = momentum
        self.track_running_stats = track_running_stats
        self.eps = eps
        if self.affine:
            self.Wrr = nn.parameter.Parameter(torch.Tensor(self.num_features))
            self.Wri = nn.parameter.Parameter(torch.Tensor(self.num_features))
            self.Wii = nn.parameter.Parameter(torch.Tensor(self.num_features))
            self.Br = nn.parameter.Parameter(torch.Tensor(self.num_features))
            self.Bi = nn.parameter.Parameter(torch.Tensor(self.num_features))
        else:
            self.register_parameter('Wrr', None)
            self.register_parameter('Wri', None)
            self.register_parameter('Wii', None)
            self.register_parameter('Br', None)
            self.register_parameter('Bi', None)
        if self.track_running_stats:
            values = torch.zeros(self.num_features)
            self.register_buffer('Mean_real', values)
            self.register_buffer('Mean_imag', values)
            self.register_buffer('Var_rr', values)
            self.register_buffer('Var_ri', values)
            self.register_buffer('Var_ii', values)
            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))
        else:
            self.register_parameter('Mean_real', None)
            self.register_parameter('Mean_imag', None)
            self.register_parameter('Var_rr', None)
            self.register_parameter('Var_ri', None)
            self.register_parameter('Var_ii', None)
            self.register_parameter('num_batches_tracked', None)
        self.reset_parameters()

    def reset_parameters(self):
        if self.affine:
            self.Wrr.data.fill_(1)
            self.Wii.data.fill_(1)
            self.Wri.data.uniform_(-0.9, 0.9)
            self.Br.data.fill_(0)
            self.Bi.data.fill_(0)
        self.reset_running_stats()

    def reset_running_stats(self):
        if self.track_running_stats:
            self.Mean_real.zero_()
            self.Mean_imag.zero_()
            self.Var_rr.fill_(1)
            self.Var_ri.zero_()
            self.Var_ii.fill_(1)
            self.num_batches_tracked.zero_()

    def extra_repr(self):
        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, track_running_stats={track_running_stats}'.format(**self.__dict__)

    def forward(self, input):
        real, imag = torch.chunk(input, 2, 1)
        exp_avg_factor = 0.0
        training = self.training and self.track_running_stats
        if training:
            self.num_batches_tracked += 1
            if self.momentum is None:
                exp_avg_factor = 1 / self.num_batches_tracked
            else:
                exp_avg_factor = self.momentum
        redux = [i for i in reversed(range(real.dim())) if i != 1]
        vdim = [1] * real.dim()
        vdim[1] = real.size(1)
        if training:
            batch_mean_real, batch_mean_imag = real, imag
            for dim in redux:
                batch_mean_real = batch_mean_real.mean(dim, keepdim=True)
                batch_mean_imag = batch_mean_imag.mean(dim, keepdim=True)
            if self.track_running_stats:
                self.Mean_real.lerp_(batch_mean_real.squeeze(), exp_avg_factor)
                self.Mean_imag.lerp_(batch_mean_imag.squeeze(), exp_avg_factor)
        else:
            batch_mean_real = self.Mean_real.view(vdim)
            batch_mean_imag = self.Mean_imag.view(vdim)
        real = real - batch_mean_real
        imag = imag - batch_mean_imag
        if training:
            batch_var_rr = real * real
            batch_var_ri = real * imag
            batch_var_ii = imag * imag
            for dim in redux:
                batch_var_rr = batch_var_rr.mean(dim, keepdim=True)
                batch_var_ri = batch_var_ri.mean(dim, keepdim=True)
                batch_var_ii = batch_var_ii.mean(dim, keepdim=True)
            if self.track_running_stats:
                self.Var_rr.lerp_(batch_var_rr.squeeze(), exp_avg_factor)
                self.Var_ri.lerp_(batch_var_ri.squeeze(), exp_avg_factor)
                self.Var_ii.lerp_(batch_var_ii.squeeze(), exp_avg_factor)
        else:
            batch_var_rr = self.Var_rr.view(vdim)
            batch_var_ii = self.Var_ii.view(vdim)
            batch_var_ri = self.Var_ri.view(vdim)
        batch_var_rr += self.eps
        batch_var_ii += self.eps
        tau = batch_var_rr + batch_var_ii
        s = batch_var_rr * batch_var_ii - batch_var_ri * batch_var_ri
        t = (tau + 2 * s).sqrt()
        rst = (s * t).reciprocal()
        Urr = (batch_var_ii + s) * rst
        Uri = -batch_var_ri * rst
        Uii = (batch_var_rr + s) * rst
        if self.affine:
            Wrr, Wri, Wii = self.Wrr.view(vdim), self.Wri.view(vdim), self.Wii.view(vdim)
            Zrr = Wrr * Urr + Wri * Uri
            Zri = Wrr * Uri + Wri * Uii
            Zir = Wii * Uri + Wri * Urr
            Zii = Wri * Uri + Wii * Uii
        else:
            Zrr, Zri, Zir, Zii = Urr, Uri, Uri, Uii
        yr = Zrr * real + Zri * imag
        yi = Zir * real + Zii * imag
        if self.affine:
            yr = yr + self.Br.view(vdim)
            yi = yi + self.Bi.view(vdim)
        outputs = torch.cat([yr, yi], 1)
        return outputs


class ComplexRelu(nn.Module):

    def __init__(self):
        super().__init__()
        self.real_relu = nn.PReLU()
        self.imag_relu = nn.PReLU()

    def forward(self, input):
        real, imag = torch.chunk(input, 2, 1)
        real = self.real_relu(real)
        imag = self.imag_relu(imag)
        return torch.cat([real, imag], dim=1)


class DCCRN_ENCODER(nn.Module):

    def __init__(self, in_channels: int, out_channel: int, kernel_size: Tuple[int, int], complex_norm: bool=True, complex_relu: bool=True, stride: Tuple[int, int]=(2, 1), padding: Tuple[int, int]=(2, 1)):
        super().__init__()
        batchnorm = ComplexBatchNorm2D if complex_norm else nn.BatchNorm2d
        activation = ComplexRelu() if complex_relu else nn.PReLU()
        self.encoder = nn.Sequential(ComplexConv2d(in_channels, out_channel, kernel_size=kernel_size, stride=stride, padding=padding), batchnorm(out_channel), activation)

    def forward(self, waveform):
        return self.encoder(waveform)


class DCCRN_DECODER(nn.Module):

    def __init__(self, in_channels: int, out_channels: int, kernel_size: Tuple[int, int], layer: int=0, complex_norm: bool=True, complex_relu: bool=True, stride: Tuple[int, int]=(2, 1), padding: Tuple[int, int]=(2, 0), output_padding: Tuple[int, int]=(1, 0)):
        super().__init__()
        batchnorm = ComplexBatchNorm2D if complex_norm else nn.BatchNorm2d
        activation = ComplexRelu() if complex_relu else nn.PReLU()
        if layer != 0:
            self.decoder = nn.Sequential(ComplexConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding), batchnorm(out_channels), activation)
        else:
            self.decoder = nn.Sequential(ComplexConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding))

    def forward(self, waveform):
        return self.decoder(waveform)


class DemucsLSTM(nn.Module):

    def __init__(self, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool=True):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=bidirectional)
        dim = 2 if bidirectional else 1
        self.linear = nn.Linear(dim * hidden_size, hidden_size)

    def forward(self, x):
        output, (h, c) = self.lstm(x)
        output = self.linear(output)
        return output, (h, c)


class DemucsEncoder(nn.Module):

    def __init__(self, num_channels: int, hidden_size: int, kernel_size: int, stride: int=1, glu: bool=False):
        super().__init__()
        activation = nn.GLU(1) if glu else nn.ReLU()
        multi_factor = 2 if glu else 1
        self.encoder = nn.Sequential(nn.Conv1d(num_channels, hidden_size, kernel_size, stride), nn.ReLU(), nn.Conv1d(hidden_size, hidden_size * multi_factor, 1, 1), activation)

    def forward(self, waveform):
        return self.encoder(waveform)


class DemucsDecoder(nn.Module):

    def __init__(self, num_channels: int, hidden_size: int, kernel_size: int, stride: int=1, glu: bool=False, layer: int=0):
        super().__init__()
        activation = nn.GLU(1) if glu else nn.ReLU()
        multi_factor = 2 if glu else 1
        self.decoder = nn.Sequential(nn.Conv1d(hidden_size, hidden_size * multi_factor, 1, 1), activation, nn.ConvTranspose1d(hidden_size, num_channels, kernel_size, stride))
        if layer > 0:
            self.decoder.add_module('4', nn.ReLU())

    def forward(self, waveform):
        out = self.decoder(waveform)
        return out


class WavenetDecoder(nn.Module):

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int=5, padding: int=2, stride: int=1, dilation: int=1):
        super(WavenetDecoder, self).__init__()
        self.decoder = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation), nn.BatchNorm1d(out_channels), nn.LeakyReLU(negative_slope=0.1))

    def forward(self, waveform):
        return self.decoder(waveform)


class WavenetEncoder(nn.Module):

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int=15, padding: int=7, stride: int=1, dilation: int=1):
        super(WavenetEncoder, self).__init__()
        self.encoder = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation), nn.BatchNorm1d(out_channels), nn.LeakyReLU(negative_slope=0.1))

    def forward(self, waveform):
        return self.encoder(waveform)


class ConvFFT(nn.Module):

    def __init__(self, window_len: int, nfft: Optional[int]=None, window: str='hamming'):
        super().__init__()
        self.window_len = window_len
        self.nfft = nfft if nfft else np.int(2 ** np.ceil(np.log2(window_len)))
        self.window = torch.from_numpy(get_window(window, window_len, fftbins=True).astype('float32'))

    def init_kernel(self, inverse=False):
        fourier_basis = np.fft.rfft(np.eye(self.nfft))[:self.window_len]
        real, imag = np.real(fourier_basis), np.imag(fourier_basis)
        kernel = np.concatenate([real, imag], 1).T
        if inverse:
            kernel = np.linalg.pinv(kernel).T
        kernel = torch.from_numpy(kernel.astype('float32')).unsqueeze(1)
        kernel *= self.window
        return kernel


class ConvSTFT(ConvFFT):

    def __init__(self, window_len: int, hop_size: Optional[int]=None, nfft: Optional[int]=None, window: str='hamming'):
        super().__init__(window_len=window_len, nfft=nfft, window=window)
        self.hop_size = hop_size if hop_size else window_len // 2
        self.register_buffer('weight', self.init_kernel())

    def forward(self, input):
        if input.dim() < 2:
            raise ValueError(f'Expected signal with shape 2 or 3 got {input.dim()}')
        elif input.dim() == 2:
            input = input.unsqueeze(1)
        else:
            pass
        input = F.pad(input, (self.window_len - self.hop_size, self.window_len - self.hop_size))
        output = F.conv1d(input, self.weight, stride=self.hop_size)
        return output


class ConviSTFT(ConvFFT):

    def __init__(self, window_len: int, hop_size: Optional[int]=None, nfft: Optional[int]=None, window: str='hamming'):
        super().__init__(window_len=window_len, nfft=nfft, window=window)
        self.hop_size = hop_size if hop_size else window_len // 2
        self.register_buffer('weight', self.init_kernel(True))
        self.register_buffer('enframe', torch.eye(window_len).unsqueeze(1))

    def forward(self, input, phase=None):
        if phase is not None:
            real = input * torch.cos(phase)
            imag = input * torch.sin(phase)
            input = torch.cat([real, imag], 1)
        out = F.conv_transpose1d(input, self.weight, stride=self.hop_size)
        coeff = self.window.unsqueeze(1).repeat(1, 1, input.size(-1)) ** 2
        coeff = coeff
        coeff = F.conv_transpose1d(coeff, self.enframe, stride=self.hop_size)
        out = out / (coeff + 1e-08)
        pad = self.window_len - self.hop_size
        out = out[..., pad:-pad]
        return out


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ComplexBatchNorm2D,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ComplexConv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ComplexConvTranspose2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ComplexLSTM,
     lambda: ([], {'input_size': 4, 'hidden_size': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (ComplexRelu,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ConvSTFT,
     lambda: ([], {'window_len': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (ConviSTFT,
     lambda: ([], {'window_len': 4}),
     lambda: ([torch.rand([4, 6, 4])], {}),
     True),
    (DCCRN_DECODER,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DCCRN_ENCODER,
     lambda: ([], {'in_channels': 4, 'out_channel': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DemucsDecoder,
     lambda: ([], {'num_channels': 4, 'hidden_size': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (DemucsEncoder,
     lambda: ([], {'num_channels': 4, 'hidden_size': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (DemucsLSTM,
     lambda: ([], {'input_size': 4, 'hidden_size': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (WavenetDecoder,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (WavenetEncoder,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (mean_absolute_error,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (mean_squared_error,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_shahules786_mayavoz(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])


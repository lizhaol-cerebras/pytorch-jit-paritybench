
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch.utils.data as data


import numpy as np


import random


import torch


from torch.utils import model_zoo


from torchvision.models.resnet import BasicBlock


from torchvision.models.resnet import Bottleneck


import torchvision.models as models


import torch.nn as nn


import math


from torch.distributions import VonMises


from torch.distributions.multivariate_normal import _batch_mahalanobis


from torch.distributions.multivariate_normal import _standard_normal


from torch.distributions.multivariate_normal import _batch_mv


from abc import ABCMeta


from abc import abstractmethod


from functools import partial


import torch.nn.functional as F


from typing import Optional


from scipy.linalg import logm


import numpy.linalg as LA


import time


import matplotlib.pyplot as plt


from scipy.spatial.transform import Rotation as R


import torch.utils.data


import matplotlib


import warnings


from torch.nn.modules.batchnorm import _BatchNorm


from torch.utils.cpp_extension import BuildExtension


from torch.utils.cpp_extension import CppExtension


from torch.utils.cpp_extension import CUDAExtension


import copy


class MonteCarloPoseLoss(nn.Module):

    def __init__(self, loss_weight=1.0, init_norm_factor=1.0, momentum=0.01, reduction='mean'):
        super(MonteCarloPoseLoss, self).__init__()
        self.reduction = reduction
        self.loss_weight = loss_weight
        self.register_buffer('norm_factor', torch.tensor(init_norm_factor, dtype=torch.float))
        self.momentum = momentum

    def forward(self, pose_sample_logweights, cost_target, norm_factor, weight=None, avg_factor=None, reduction_override=None):
        """
        Args:
            pose_sample_logweights: Shape (mc_samples, num_obj)
            cost_target: Shape (num_obj, )
            norm_factor: Shape ()
        """
        if self.training:
            with torch.no_grad():
                norm_factor = reduce_mean(norm_factor)
                self.norm_factor.mul_(1 - self.momentum).add_(self.momentum * norm_factor)
        assert reduction_override in (None, 'none', 'mean', 'sum')
        reduction = reduction_override if reduction_override else self.reduction
        loss = monte_carlo_pose_loss(pose_sample_logweights, cost_target, weight=weight, reduction=reduction, avg_factor=avg_factor) * (self.loss_weight / self.norm_factor)
        return loss


class CDPN(nn.Module):

    def __init__(self, backbone, rot_head_net, trans_head_net):
        super(CDPN, self).__init__()
        self.backbone = backbone
        self.rot_head_net = rot_head_net
        self.trans_head_net = trans_head_net
        self.monte_carlo_pose_loss = MonteCarloPoseLoss()

    def forward(self, x):
        features = self.backbone(x)
        cc_maps = self.rot_head_net(features)
        trans = self.trans_head_net(features)
        return cc_maps, trans


class ResNetBackboneNet(nn.Module):

    def __init__(self, block, layers, in_channel=3, freeze=False):
        self.freeze = freeze
        self.inplanes = 64
        super(ResNetBackboneNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, mean=0, std=0.001)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion))
        layers = [block(self.inplanes, planes, stride, downsample)]
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        if self.freeze:
            with torch.no_grad():
                x = self.conv1(x)
                x = self.bn1(x)
                x = self.relu(x)
                x_low_feature = self.maxpool(x)
                x = self.layer1(x_low_feature)
                x = self.layer2(x)
                x = self.layer3(x)
                x_high_feature = self.layer4(x)
                return x_high_feature.detach()
        else:
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x_low_feature = self.maxpool(x)
            x = self.layer1(x_low_feature)
            x = self.layer2(x)
            x = self.layer3(x)
            x_high_feature = self.layer4(x)
            return x_high_feature


class RotHeadNet(nn.Module):

    def __init__(self, in_channels, num_layers=3, num_filters=256, kernel_size=3, output_kernel_size=1, output_dim=5, freeze=False):
        super(RotHeadNet, self).__init__()
        self.freeze = freeze
        assert kernel_size == 2 or kernel_size == 3 or kernel_size == 4, 'Only support kenerl 2, 3 and 4'
        padding = 1
        output_padding = 0
        if kernel_size == 3:
            output_padding = 1
        elif kernel_size == 2:
            padding = 0
        assert output_kernel_size == 1 or output_kernel_size == 3, 'Only support kenerl 1 and 3'
        if output_kernel_size == 1:
            pad = 0
        elif output_kernel_size == 3:
            pad = 1
        self.features = nn.ModuleList()
        for i in range(num_layers):
            _in_channels = in_channels if i == 0 else num_filters
            self.features.append(nn.ConvTranspose2d(_in_channels, num_filters, kernel_size=kernel_size, stride=2, padding=padding, output_padding=output_padding, bias=False))
            self.features.append(nn.BatchNorm2d(num_filters))
            self.features.append(nn.ReLU(inplace=True))
            self.features.append(nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1, bias=False))
            self.features.append(nn.BatchNorm2d(num_filters))
            self.features.append(nn.ReLU(inplace=True))
            self.features.append(nn.Conv2d(num_filters, num_filters, kernel_size=3, stride=1, padding=1, bias=False))
            self.features.append(nn.BatchNorm2d(num_filters))
            self.features.append(nn.ReLU(inplace=True))
        self.out_layer = nn.Conv2d(num_filters, output_dim, kernel_size=output_kernel_size, padding=pad, bias=True)
        self.scale_branch = nn.Linear(256, 2)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, mean=0, std=0.001)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                nn.init.normal_(m.weight, mean=0, std=0.001)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0, std=0.001)

    def forward(self, x):
        if self.freeze:
            with torch.no_grad():
                for i, l in enumerate(self.features):
                    x = l(x)
                x3d, w2d = self.out_layer(x).split([3, 2], dim=1)
                scale = self.scale_branch(x.flatten(2).mean(dim=-1)).exp()
        else:
            for i, l in enumerate(self.features):
                x = l(x)
            x3d, w2d = self.out_layer(x).split([3, 2], dim=1)
            scale = self.scale_branch(x.flatten(2).mean(dim=-1)).exp()
        return x3d, w2d, scale


class TransHeadNet(nn.Module):

    def __init__(self, in_channels, num_layers=3, num_filters=256, kernel_size=3, output_dim=3, freeze=False, with_bias_end=True):
        super(TransHeadNet, self).__init__()
        self.freeze = freeze
        if kernel_size == 3:
            padding = 1
        elif kernel_size == 2:
            padding = 0
        self.features = nn.ModuleList()
        for i in range(num_layers):
            _in_channels = in_channels if i == 0 else num_filters
            self.features.append(nn.Conv2d(_in_channels, num_filters, kernel_size=kernel_size, stride=1, padding=padding, bias=False))
            self.features.append(nn.BatchNorm2d(num_filters))
            self.features.append(nn.ReLU(inplace=True))
        self.linears = nn.ModuleList()
        self.linears.append(nn.Linear(256 * 8 * 8, 4096))
        self.linears.append(nn.ReLU(inplace=True))
        self.linears.append(nn.Linear(4096, 4096))
        self.linears.append(nn.ReLU(inplace=True))
        self.linears.append(nn.Linear(4096, output_dim))
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, mean=0, std=0.001)
                if with_bias_end and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.ConvTranspose2d):
                nn.init.normal_(m.weight, mean=0, std=0.001)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, mean=0, std=0.001)

    def forward(self, x):
        if self.freeze:
            with torch.no_grad():
                for i, l in enumerate(self.features):
                    x = l(x)
                x = x.view(-1, 256 * 8 * 8)
                for i, l in enumerate(self.linears):
                    x = l(x)
                return x.detach()
        else:
            for i, l in enumerate(self.features):
                x = l(x)
            x = x.view(-1, 256 * 8 * 8)
            for i, l in enumerate(self.linears):
                x = l(x)
            return x


def evaluate_pnp(x3d, x2d, w2d, pose, camera, cost_fun, out_jacobian=False, out_residual=False, out_cost=False, **kwargs):
    """
    Args:
        x3d (torch.Tensor): Shape (*, n, 3)
        x2d (torch.Tensor): Shape (*, n, 2)
        w2d (torch.Tensor): Shape (*, n, 2)
        pose (torch.Tensor): Shape (*, 4 or 7)
        camera: Camera object of batch size (*, )
        cost_fun: PnPCost object of batch size (*, )
        out_jacobian (torch.Tensor | bool): When a tensor is passed, treated as the output tensor;
            when True, returns the Jacobian; when False, skip the computation and returns None
        out_residual (torch.Tensor | bool): When a tensor is passed, treated as the output tensor;
            when True, returns the residual; when False, skip the computation and returns None
        out_cost (torch.Tensor | bool): When a tensor is passed, treated as the output tensor;
            when True, returns the cost; when False, skip the computation and returns None

    Returns:
        Tuple:
            residual (torch.Tensor | None): Shape (*, n*2)
            cost (torch.Tensor | None): Shape (*, )
            jacobian (torch.Tensor | None): Shape (*, n*2, 4 or 6)
    """
    x2d_proj, jac_cam = camera.project(x3d, pose, out_jac=out_jacobian.view(x2d.shape[:-1] + (2, out_jacobian.size(-1))) if isinstance(out_jacobian, torch.Tensor) else out_jacobian, **kwargs)
    residual, cost, jacobian = cost_fun.compute(x2d_proj, x2d, w2d, jac_cam=jac_cam, out_residual=out_residual, out_cost=out_cost, out_jacobian=out_jacobian)
    return residual, cost, jacobian


def skew(x):
    """
    Args:
        x (torch.Tensor): shape (*, 3)

    Returns:
        torch.Tensor: (*, 3, 3), skew symmetric matrices
    """
    mat = x.new_zeros(x.shape[:-1] + (3, 3))
    mat[..., [2, 0, 1], [1, 2, 0]] = x
    mat[..., [1, 2, 0], [2, 0, 1]] = -x
    return mat


def quaternion_to_rot_mat(quaternions):
    """
    Args:
        quaternions (torch.Tensor): (*, 4)

    Returns:
        torch.Tensor: (*, 3, 3)
    """
    if quaternions.requires_grad:
        w, i, j, k = torch.unbind(quaternions, -1)
        rot_mats = torch.stack((1 - 2 * (j * j + k * k), 2 * (i * j - k * w), 2 * (i * k + j * w), 2 * (i * j + k * w), 1 - 2 * (i * i + k * k), 2 * (j * k - i * w), 2 * (i * k - j * w), 2 * (j * k + i * w), 1 - 2 * (i * i + j * j)), dim=-1).reshape(quaternions.shape[:-1] + (3, 3))
    else:
        w, v = quaternions.split([1, 3], dim=-1)
        rot_mats = 2 * (w.unsqueeze(-1) * skew(v) + v.unsqueeze(-1) * v.unsqueeze(-2))
        diag = torch.diagonal(rot_mats, dim1=-2, dim2=-1)
        diag += w * w - (v.unsqueeze(-2) @ v.unsqueeze(-1)).squeeze(-1)
    return rot_mats


def yaw_to_rot_mat(yaw):
    """
    Args:
        yaw (torch.Tensor): (*)

    Returns:
        torch.Tensor: (*, 3, 3)
    """
    sin_yaw = torch.sin(yaw)
    cos_yaw = torch.cos(yaw)
    rot_mats = yaw.new_zeros(yaw.shape + (3, 3))
    rot_mats[..., 0, 0] = cos_yaw
    rot_mats[..., 2, 2] = cos_yaw
    rot_mats[..., 0, 2] = sin_yaw
    rot_mats[..., 2, 0] = -sin_yaw
    rot_mats[..., 1, 1] = 1
    return rot_mats


def pnp_denormalize(offset, pose_norm):
    pose = torch.empty_like(pose_norm)
    pose[..., 3:] = pose_norm[..., 3:]
    pose[..., :3] = pose_norm[..., :3] - ((yaw_to_rot_mat(pose_norm[..., 3]) if pose_norm.size(-1) == 4 else quaternion_to_rot_mat(pose_norm[..., 3:])) @ offset.unsqueeze(-1)).squeeze(-1)
    return pose


def pnp_normalize(x3d, pose=None, detach_transformation=True):
    """
    Args:
        x3d (torch.Tensor): Shape (*, n, 3)
        pose (torch.Tensor | None): Shape (*, 4)
        detach_transformation (bool)

    Returns:
        Tuple[torch.Tensor]:
            offset: Shape (*, 1, 3)
            x3d_norm: Shape (*, n, 3), normalized x3d
            pose_norm: Shape (*, ), transformed pose
    """
    offset = torch.mean(x3d.detach() if detach_transformation else x3d, dim=-2)
    x3d_norm = x3d - offset.unsqueeze(-2)
    if pose is not None:
        pose_norm = torch.empty_like(pose)
        pose_norm[..., 3:] = pose[..., 3:]
        pose_norm[..., :3] = pose[..., :3] + ((yaw_to_rot_mat(pose[..., 3]) if pose.size(-1) == 4 else quaternion_to_rot_mat(pose[..., 3:])) @ offset.unsqueeze(-1)).squeeze(-1)
    else:
        pose_norm = None
    return offset, x3d_norm, pose_norm


class EProPnPBase(torch.nn.Module, metaclass=ABCMeta):
    """
    End-to-End Probabilistic Perspective-n-Points.

    Args:
        mc_samples (int): Number of total Monte Carlo samples
        num_iter (int): Number of AMIS iterations
        normalize (bool)
        eps (float)
        solver (dict): PnP solver
    """

    def __init__(self, mc_samples=512, num_iter=4, normalize=False, eps=1e-05, solver=None):
        super(EProPnPBase, self).__init__()
        assert num_iter > 0
        assert mc_samples % num_iter == 0
        self.mc_samples = mc_samples
        self.num_iter = num_iter
        self.iter_samples = self.mc_samples // self.num_iter
        self.eps = eps
        self.normalize = normalize
        self.solver = solver

    @abstractmethod
    def allocate_buffer(self, *args, **kwargs):
        pass

    @abstractmethod
    def initial_fit(self, *args, **kwargs):
        pass

    @abstractmethod
    def gen_new_distr(self, *args, **kwargs):
        pass

    @abstractmethod
    def gen_old_distr(self, *args, **kwargs):
        pass

    @abstractmethod
    def estimate_params(self, *args, **kwargs):
        pass

    def forward(self, *args, **kwargs):
        return self.solver(*args, **kwargs)

    def monte_carlo_forward(self, x3d, x2d, w2d, camera, cost_fun, pose_init=None, force_init_solve=True, **kwargs):
        """
        Monte Carlo PnP forward. Returns weighted pose samples drawn from the probability
        distribution of pose defined by the correspondences {x_{3D}, x_{2D}, w_{2D}}.

        Args:
            x3d (Tensor): Shape (num_obj, num_points, 3)
            x2d (Tensor): Shape (num_obj, num_points, 2)
            w2d (Tensor): Shape (num_obj, num_points, 2)
            camera: Camera object of batch size (num_obj, )
            cost_fun: PnPCost object of batch size (num_obj, )
            pose_init (Tensor | None): Shape (num_obj, 4 or 7), optional. The target pose
                (y_{gt}) can be passed for training with Monte Carlo pose loss
            force_init_solve (bool): Whether to force using the initialization solver when
                pose_init is not None

        Returns:
            Tuple:
                pose_opt (Tensor): Shape (num_obj, 4 or 7), PnP solution y*
                cost (Tensor | None): Shape (num_obj, ), is not None when with_cost=True
                pose_opt_plus (Tensor | None): Shape (num_obj, 4 or 7), y* + Δy, used in derivative
                    regularization loss, is not None when with_pose_opt_plus=True, can be backpropagated
                pose_samples (Tensor): Shape (mc_samples, num_obj, 4 or 7)
                pose_sample_logweights (Tensor): Shape (mc_samples, num_obj), can be backpropagated
                cost_init (Tensor | None): Shape (num_obj, ), is None when pose_init is None, can be
                    backpropagated
        """
        if self.normalize:
            transform, x3d, pose_init = pnp_normalize(x3d, pose_init, detach_transformation=True)
        assert x3d.dim() == x2d.dim() == w2d.dim() == 3
        num_obj = x3d.size(0)
        evaluate_fun = partial(evaluate_pnp, x3d=x3d, x2d=x2d, w2d=w2d, camera=camera, cost_fun=cost_fun, out_cost=True)
        cost_init = evaluate_fun(pose=pose_init)[1] if pose_init is not None else None
        pose_opt, pose_cov, cost, pose_opt_plus = self.solver(x3d, x2d, w2d, camera, cost_fun, pose_init=pose_init, cost_init=cost_init, with_pose_cov=True, force_init_solve=force_init_solve, normalize_override=False, **kwargs)
        if num_obj > 0:
            pose_samples = x3d.new_empty((self.num_iter, self.iter_samples) + pose_opt.size())
            logprobs = x3d.new_empty((self.num_iter, self.num_iter, self.iter_samples, num_obj))
            cost_pred = x3d.new_empty((self.num_iter, self.iter_samples, num_obj))
            distr_params = self.allocate_buffer(num_obj, dtype=x3d.dtype, device=x3d.device)
            with torch.no_grad():
                self.initial_fit(pose_opt, pose_cov, camera, *distr_params)
            for i in range(self.num_iter):
                new_trans_distr, new_rot_distr = self.gen_new_distr(i, *distr_params)
                pose_samples[i, :, :, :3] = new_trans_distr.sample((self.iter_samples,))
                pose_samples[i, :, :, 3:] = new_rot_distr.sample((self.iter_samples,))
                cost_pred[i] = evaluate_fun(pose=pose_samples[i])[1]
                logprobs[i, :i + 1] = new_trans_distr.log_prob(pose_samples[:i + 1, :, :, :3]) + new_rot_distr.log_prob(pose_samples[:i + 1, :, :, 3:]).flatten(2)
                if i > 0:
                    old_trans_distr, old_rot_distr = self.gen_old_distr(i, *distr_params)
                    logprobs[:i, i] = old_trans_distr.log_prob(pose_samples[i, :, :, :3]) + old_rot_distr.log_prob(pose_samples[i, :, :, 3:]).flatten(2)
                mix_logprobs = torch.logsumexp(logprobs[:i + 1, :i + 1], dim=0) - math.log(i + 1)
                pose_sample_logweights = -cost_pred[:i + 1] - mix_logprobs
                if i == self.num_iter - 1:
                    break
                with torch.no_grad():
                    self.estimate_params(i, pose_samples[:i + 1].reshape(((i + 1) * self.iter_samples,) + pose_opt.size()), pose_sample_logweights.reshape((i + 1) * self.iter_samples, num_obj), *distr_params)
            pose_samples = pose_samples.reshape((self.mc_samples,) + pose_opt.size())
            pose_sample_logweights = pose_sample_logweights.reshape(self.mc_samples, num_obj)
        else:
            pose_samples = x2d.new_zeros((self.mc_samples,) + pose_opt.size())
            pose_sample_logweights = x3d.reshape(self.mc_samples, 0) + x2d.reshape(self.mc_samples, 0) + w2d.reshape(self.mc_samples, 0)
        if self.normalize:
            pose_opt = pnp_denormalize(transform, pose_opt)
            pose_samples = pnp_denormalize(transform, pose_samples)
            if pose_opt_plus is not None:
                pose_opt_plus = pnp_denormalize(transform, pose_opt_plus)
        return pose_opt, cost, pose_opt_plus, pose_samples, pose_sample_logweights, cost_init


class VonMisesUniformMix(VonMises):

    def __init__(self, loc, concentration, uniform_mix=0.25, **kwargs):
        super(VonMisesUniformMix, self).__init__(loc, concentration, **kwargs)
        self.uniform_mix = uniform_mix

    @torch.no_grad()
    def sample(self, sample_shape=torch.Size()):
        assert len(sample_shape) == 1
        x = np.empty(tuple(self._extended_shape(sample_shape)), dtype=np.float32)
        uniform_samples = round(sample_shape[0] * self.uniform_mix)
        von_mises_samples = sample_shape[0] - uniform_samples
        x[:uniform_samples] = np.random.uniform(-math.pi, math.pi, size=tuple(self._extended_shape((uniform_samples,))))
        x[uniform_samples:] = np.random.vonmises(self.loc.cpu().numpy(), self.concentration.cpu().numpy(), size=tuple(self._extended_shape((von_mises_samples,))))
        return torch.from_numpy(x)

    def log_prob(self, value):
        von_mises_log_prob = super(VonMisesUniformMix, self).log_prob(value) + np.log(1 - self.uniform_mix)
        log_prob = torch.logaddexp(von_mises_log_prob, torch.full_like(von_mises_log_prob, math.log(self.uniform_mix / (2 * math.pi))))
        return log_prob


def cholesky_wrapper(mat, default_diag=None, force_cpu=True):
    device = mat.device
    if force_cpu:
        mat = mat.cpu()
    try:
        tril = torch.cholesky(mat, upper=False)
    except RuntimeError:
        n_dims = mat.size(-1)
        tril = []
        default_tril_single = torch.diag(mat.new_tensor(default_diag)) if default_diag is not None else torch.eye(n_dims, dtype=mat.dtype, device=mat.device)
        for cov in mat.reshape(-1, n_dims, n_dims):
            try:
                tril.append(torch.cholesky(cov, upper=False))
            except RuntimeError:
                tril.append(default_tril_single)
        tril = torch.stack(tril, dim=0).reshape(mat.shape)
    return tril


class EProPnP4DoF(EProPnPBase):
    """
    End-to-End Probabilistic Perspective-n-Points for 4DoF pose estimation.
    The pose is parameterized as [x, y, z, yaw], where yaw is the
    rotation around the Y-axis in radians.
    Adopted proposal distributions:
        position: multivariate t-distribution, degrees of freedom = 3
        orientation: 0.75 von Mises distribution + 0.25 uniform distribution
    """

    def allocate_buffer(self, num_obj, dtype=torch.float32, device=None):
        trans_mode = torch.empty((self.num_iter, num_obj, 3), dtype=dtype, device=device)
        trans_cov_tril = torch.empty((self.num_iter, num_obj, 3, 3), dtype=dtype, device=device)
        rot_mode = torch.empty((self.num_iter, num_obj, 1), dtype=dtype, device=device)
        rot_kappa = torch.empty((self.num_iter, num_obj, 1), dtype=dtype, device=device)
        return trans_mode, trans_cov_tril, rot_mode, rot_kappa

    def initial_fit(self, pose_opt, pose_cov, camera, trans_mode, trans_cov_tril, rot_mode, rot_kappa):
        trans_mode[0], rot_mode[0] = pose_opt.split([3, 1], dim=-1)
        trans_cov_tril[0] = cholesky_wrapper(pose_cov[:, :3, :3], [1.0, 1.0, 4.0])
        rot_kappa[0] = 0.33 / pose_cov[:, 3, 3, None].clamp(min=self.eps)

    @staticmethod
    def gen_new_distr(iter_id, trans_mode, trans_cov_tril, rot_mode, rot_kappa):
        new_trans_distr = MultivariateStudentT(3, trans_mode[iter_id], trans_cov_tril[iter_id])
        new_rot_distr = VonMisesUniformMix(rot_mode[iter_id], rot_kappa[iter_id])
        return new_trans_distr, new_rot_distr

    @staticmethod
    def gen_old_distr(iter_id, trans_mode, trans_cov_tril, rot_mode, rot_kappa):
        mix_trans_distr = MultivariateStudentT(3, trans_mode[:iter_id, None], trans_cov_tril[:iter_id, None])
        mix_rot_distr = VonMisesUniformMix(rot_mode[:iter_id, None], rot_kappa[:iter_id, None])
        return mix_trans_distr, mix_rot_distr

    def estimate_params(self, iter_id, pose_samples, pose_sample_logweights, trans_mode, trans_cov_tril, rot_mode, rot_kappa):
        sample_weights_norm = torch.softmax(pose_sample_logweights, dim=0)
        trans_mode[iter_id + 1] = (sample_weights_norm[..., None] * pose_samples[..., :3]).sum(dim=0)
        trans_dev = pose_samples[..., :3] - trans_mode[iter_id + 1]
        trans_cov = (sample_weights_norm[..., None, None] * trans_dev.unsqueeze(-1) * trans_dev.unsqueeze(-2)).sum(dim=0)
        trans_cov_tril[iter_id + 1] = cholesky_wrapper(trans_cov, [1.0, 1.0, 4.0])
        mean_vector = pose_samples.new_empty((pose_samples.size(1), 2))
        torch.sum(sample_weights_norm[..., None] * pose_samples[..., 3:].sin(), dim=0, out=mean_vector[:, :1])
        torch.sum(sample_weights_norm[..., None] * pose_samples[..., 3:].cos(), dim=0, out=mean_vector[:, 1:])
        rot_mode[iter_id + 1] = torch.atan2(mean_vector[:, :1], mean_vector[:, 1:])
        r_sq = torch.square(mean_vector).sum(dim=-1, keepdim=True)
        rot_kappa[iter_id + 1] = 0.33 * r_sq.sqrt().clamp(min=self.eps) * (2 - r_sq) / (1 - r_sq).clamp(min=self.eps)


class EProPnP6DoF(EProPnPBase):
    """
    End-to-End Probabilistic Perspective-n-Points for 6DoF pose estimation.
    The pose is parameterized as [x, y, z, w, i, j, k], where [w, i, j, k]
    is the unit quaternion.
    Adopted proposal distributions:
        position: multivariate t-distribution, degrees of freedom = 3
        orientation: angular central Gaussian distribution
    """

    def __init__(self, *args, acg_mle_iter=3, acg_dispersion=0.001, **kwargs):
        super(EProPnP6DoF, self).__init__(*args, **kwargs)
        self.acg_mle_iter = acg_mle_iter
        self.acg_dispersion = acg_dispersion

    def allocate_buffer(self, num_obj, dtype=torch.float32, device=None):
        trans_mode = torch.empty((self.num_iter, num_obj, 3), dtype=dtype, device=device)
        trans_cov_tril = torch.empty((self.num_iter, num_obj, 3, 3), dtype=dtype, device=device)
        rot_cov_tril = torch.empty((self.num_iter, num_obj, 4, 4), dtype=dtype, device=device)
        return trans_mode, trans_cov_tril, rot_cov_tril

    def initial_fit(self, pose_opt, pose_cov, camera, trans_mode, trans_cov_tril, rot_cov_tril):
        trans_mode[0], rot_mode = pose_opt.split([3, 4], dim=-1)
        trans_cov_tril[0] = cholesky_wrapper(pose_cov[:, :3, :3])
        eye_4 = torch.eye(4, dtype=pose_opt.dtype, device=pose_opt.device)
        transform_mat = camera.get_quaternion_transfrom_mat(rot_mode)
        rot_cov = (transform_mat @ pose_cov[:, 3:, 3:].inverse() @ transform_mat.transpose(-1, -2) + eye_4).inverse()
        rot_cov.div_(rot_cov.diagonal(offset=0, dim1=-1, dim2=-2).sum(-1)[..., None, None])
        rot_cov_tril[0] = cholesky_wrapper(rot_cov + rot_cov.det()[:, None, None] ** 0.25 * (self.acg_dispersion * eye_4))

    @staticmethod
    def gen_new_distr(iter_id, trans_mode, trans_cov_tril, rot_cov_tril):
        new_trans_distr = MultivariateStudentT(3, trans_mode[iter_id], trans_cov_tril[iter_id])
        new_rot_distr = AngularCentralGaussian(rot_cov_tril[iter_id])
        return new_trans_distr, new_rot_distr

    @staticmethod
    def gen_old_distr(iter_id, trans_mode, trans_cov_tril, rot_cov_tril):
        mix_trans_distr = MultivariateStudentT(3, trans_mode[:iter_id, None], trans_cov_tril[:iter_id, None])
        mix_rot_distr = AngularCentralGaussian(rot_cov_tril[:iter_id, None])
        return mix_trans_distr, mix_rot_distr

    def estimate_params(self, iter_id, pose_samples, pose_sample_logweights, trans_mode, trans_cov_tril, rot_cov_tril):
        sample_weights_norm = torch.softmax(pose_sample_logweights, dim=0)
        trans_mode[iter_id + 1] = (sample_weights_norm[..., None] * pose_samples[..., :3]).sum(dim=0)
        trans_dev = pose_samples[..., :3] - trans_mode[iter_id + 1]
        trans_cov = (sample_weights_norm[..., None, None] * trans_dev.unsqueeze(-1) * trans_dev.unsqueeze(-2)).sum(dim=0)
        trans_cov_tril[iter_id + 1] = cholesky_wrapper(trans_cov)
        eye_4 = torch.eye(4, dtype=pose_samples.dtype, device=pose_samples.device)
        rot = pose_samples[..., 3:]
        r_r_t = rot[:, :, :, None] * rot[:, :, None, :]
        rot_cov = eye_4.expand(pose_samples.size(1), 4, 4).clone()
        for _ in range(self.acg_mle_iter):
            M = rot[:, :, None, :] @ rot_cov.inverse() @ rot[:, :, :, None]
            invM_weighted = sample_weights_norm[..., None, None] / M.clamp(min=self.eps)
            invM_weighted_norm = invM_weighted / invM_weighted.sum(dim=0)
            rot_cov = (invM_weighted_norm * r_r_t).sum(dim=0) + eye_4 * self.eps
        rot_cov_tril[iter_id + 1] = cholesky_wrapper(rot_cov + rot_cov.det()[:, None, None] ** 0.25 * (self.acg_dispersion * eye_4))


def solve_wrapper(b, A):
    if A.numel() > 0:
        return torch.linalg.solve(A, b)
    else:
        return b + A.reshape_as(b)


class LMSolver(nn.Module):
    """
    Levenberg-Marquardt solver, with fixed number of iterations.

    - For 4DoF case, the pose is parameterized as [x, y, z, yaw], where yaw is the
    rotation around the Y-axis in radians.
    - For 6DoF case, the pose is parameterized as [x, y, z, w, i, j, k], where
    [w, i, j, k] is the unit quaternion.
    """

    def __init__(self, dof=4, num_iter=10, min_lm_diagonal=1e-06, max_lm_diagonal=1e+32, min_relative_decrease=0.001, initial_trust_region_radius=30.0, max_trust_region_radius=1e+16, eps=1e-05, normalize=False, init_solver=None):
        super(LMSolver, self).__init__()
        self.dof = dof
        self.num_iter = num_iter
        self.min_lm_diagonal = min_lm_diagonal
        self.max_lm_diagonal = max_lm_diagonal
        self.min_relative_decrease = min_relative_decrease
        self.initial_trust_region_radius = initial_trust_region_radius
        self.max_trust_region_radius = max_trust_region_radius
        self.eps = eps
        self.normalize = normalize
        self.init_solver = init_solver

    def forward(self, x3d, x2d, w2d, camera, cost_fun, with_pose_opt_plus=False, pose_init=None, normalize_override=None, **kwargs):
        if isinstance(normalize_override, bool):
            normalize = normalize_override
        else:
            normalize = self.normalize
        if normalize:
            transform, x3d, pose_init = pnp_normalize(x3d, pose_init, detach_transformation=True)
        pose_opt, pose_cov, cost = self.solve(x3d, x2d, w2d, camera, cost_fun, pose_init=pose_init, **kwargs)
        if with_pose_opt_plus:
            step = self.gn_step(x3d, x2d, w2d, pose_opt, camera, cost_fun)
            pose_opt_plus = self.pose_add(pose_opt, step, camera)
        else:
            pose_opt_plus = None
        if normalize:
            pose_opt = pnp_denormalize(transform, pose_opt)
            if pose_cov is not None:
                raise NotImplementedError('Normalized covariance unsupported')
            if pose_opt_plus is not None:
                pose_opt_plus = pnp_denormalize(transform, pose_opt_plus)
        return pose_opt, pose_cov, cost, pose_opt_plus

    def solve(self, x3d, x2d, w2d, camera, cost_fun, pose_init=None, cost_init=None, with_pose_cov=False, with_cost=False, force_init_solve=False, fast_mode=False):
        """
        Args:
            x3d (Tensor): Shape (num_obj, num_pts, 3)
            x2d (Tensor): Shape (num_obj, num_pts, 2)
            w2d (Tensor): Shape (num_obj, num_pts, 2)
            camera: Camera object of batch size (num_obj, )
            cost_fun: PnPCost object of batch size (num_obj, )
            pose_init (None | Tensor): Shape (num_obj, 4 or 7) in [x, y, z, yaw], optional
            cost_init (None | Tensor): Shape (num_obj, ), PnP cost of pose_init, optional
            with_pose_cov (bool): Whether to compute the covariance of pose_opt
            with_cost (bool): Whether to compute the cost of pose_opt
            force_init_solve (bool): Whether to force using the initialization solver when
                pose_init is not None
            fast_mode (bool): Fall back to Gauss-Newton for fast inference

        Returns:
            tuple:
                pose_opt (Tensor): Shape (num_obj, 4 or 7)
                pose_cov (Tensor | None): Shape (num_obj, 4, 4) or (num_obj, 6, 6), covariance
                    of local pose parameterization
                cost (Tensor | None): Shape (num_obj, )
        """
        with torch.no_grad():
            num_obj, num_pts, _ = x2d.size()
            tensor_kwargs = dict(dtype=x2d.dtype, device=x2d.device)
            if num_obj > 0:
                evaluate_fun = partial(evaluate_pnp, x3d=x3d, x2d=x2d, w2d=w2d, camera=camera, cost_fun=cost_fun, clip_jac=not fast_mode)
                if pose_init is None or force_init_solve:
                    assert self.init_solver is not None
                    if pose_init is None:
                        pose_init_solve, _, _ = self.init_solver.solve(x3d, x2d, w2d, camera, cost_fun, fast_mode=fast_mode)
                        pose_opt = pose_init_solve
                    else:
                        if cost_init is None:
                            cost_init = evaluate_fun(pose=pose_init, out_cost=True)[1]
                        pose_init_solve, _, cost_init_solve = self.init_solver.solve(x3d, x2d, w2d, camera, cost_fun, with_cost=True, fast_mode=fast_mode)
                        use_init = cost_init < cost_init_solve
                        pose_init_solve[use_init] = pose_init[use_init]
                        pose_opt = pose_init_solve
                else:
                    pose_opt = pose_init.clone()
                jac = torch.empty((num_obj, num_pts * 2, self.dof), **tensor_kwargs)
                residual = torch.empty((num_obj, num_pts * 2), **tensor_kwargs)
                cost = torch.empty((num_obj,), **tensor_kwargs)
                if fast_mode:
                    for i in range(self.num_iter):
                        evaluate_fun(pose=pose_opt, out_jacobian=jac, out_residual=residual, out_cost=cost)
                        jac_t = jac.transpose(-1, -2)
                        jtj = jac_t @ jac
                        diagonal = torch.diagonal(jtj, dim1=-2, dim2=-1)
                        diagonal += self.eps
                        gradient = jac_t @ residual.unsqueeze(-1)
                        if self.dof == 4:
                            pose_opt -= solve_wrapper(gradient, jtj).squeeze(-1)
                        else:
                            step = -solve_wrapper(gradient, jtj).squeeze(-1)
                            pose_opt[..., :3] += step[..., :3]
                            pose_opt[..., 3:] = F.normalize(pose_opt[..., 3:] + (camera.get_quaternion_transfrom_mat(pose_opt[..., 3:]) @ step[..., 3:, None]).squeeze(-1), dim=-1)
                else:
                    evaluate_fun(pose=pose_opt, out_jacobian=jac, out_residual=residual, out_cost=cost)
                    jac_new = torch.empty_like(jac)
                    residual_new = torch.empty_like(residual)
                    cost_new = torch.empty_like(cost)
                    radius = x2d.new_full((num_obj,), self.initial_trust_region_radius)
                    decrease_factor = x2d.new_full((num_obj,), 2.0)
                    step_is_successful = x2d.new_zeros((num_obj,), dtype=torch.bool)
                    i = 0
                    while i < self.num_iter:
                        self._lm_iter(pose_opt, jac, residual, cost, jac_new, residual_new, cost_new, step_is_successful, radius, decrease_factor, evaluate_fun, camera)
                        i += 1
                    if with_pose_cov:
                        jac[step_is_successful] = jac_new[step_is_successful]
                        jtj = jac.transpose(-1, -2) @ jac
                        diagonal = torch.diagonal(jtj, dim1=-2, dim2=-1)
                        diagonal += self.eps
                    if with_cost:
                        cost[step_is_successful] = cost_new[step_is_successful]
                if with_pose_cov:
                    pose_cov = torch.inverse(jtj)
                else:
                    pose_cov = None
                if not with_cost:
                    cost = None
            else:
                pose_opt = torch.empty((0, 4 if self.dof == 4 else 7), **tensor_kwargs)
                pose_cov = torch.empty((0, self.dof, self.dof), **tensor_kwargs) if with_pose_cov else None
                cost = torch.empty((0,), **tensor_kwargs) if with_cost else None
            return pose_opt, pose_cov, cost

    def _lm_iter(self, pose_opt, jac, residual, cost, jac_new, residual_new, cost_new, step_is_successful, radius, decrease_factor, evaluate_fun, camera):
        jac[step_is_successful] = jac_new[step_is_successful]
        residual[step_is_successful] = residual_new[step_is_successful]
        cost[step_is_successful] = cost_new[step_is_successful]
        residual_ = residual.unsqueeze(-1)
        jac_t = jac.transpose(-1, -2)
        jtj = jac_t @ jac
        jtj_lm = jtj.clone()
        diagonal = torch.diagonal(jtj_lm, dim1=-2, dim2=-1)
        diagonal += diagonal.clamp(min=self.min_lm_diagonal, max=self.max_lm_diagonal) / radius[:, None] + self.eps
        gradient = jac_t @ residual_
        step_ = -solve_wrapper(gradient, jtj_lm)
        pose_new = self.pose_add(pose_opt, step_.squeeze(-1), camera)
        evaluate_fun(pose=pose_new, out_jacobian=jac_new, out_residual=residual_new, out_cost=cost_new)
        model_cost_change = -(step_.transpose(-1, -2) @ (jtj @ step_ / 2 + gradient)).flatten()
        relative_decrease = (cost - cost_new) / model_cost_change
        torch.bitwise_and(relative_decrease >= self.min_relative_decrease, model_cost_change > 0.0, out=step_is_successful)
        pose_opt[step_is_successful] = pose_new[step_is_successful]
        radius[step_is_successful] /= (1.0 - (2.0 * relative_decrease[step_is_successful] - 1.0) ** 3).clamp(min=1.0 / 3.0)
        radius.clamp_(max=self.max_trust_region_radius, min=self.eps)
        decrease_factor.masked_fill_(step_is_successful, 2.0)
        radius[~step_is_successful] /= decrease_factor[~step_is_successful]
        decrease_factor[~step_is_successful] *= 2.0
        return

    def gn_step(self, x3d, x2d, w2d, pose, camera, cost_fun):
        residual, _, jac = evaluate_pnp(x3d, x2d, w2d, pose, camera, cost_fun, out_jacobian=True, out_residual=True)
        jac_t = jac.transpose(-1, -2)
        jtj = jac_t @ jac
        jtj = jtj + torch.eye(self.dof, device=jtj.device, dtype=jtj.dtype) * self.eps
        gradient = jac_t @ residual.unsqueeze(-1)
        step = -solve_wrapper(gradient, jtj).squeeze(-1)
        return step

    def pose_add(self, pose_opt, step, camera):
        if self.dof == 4:
            pose_new = pose_opt + step
        else:
            pose_new = torch.cat((pose_opt[..., :3] + step[..., :3], F.normalize(pose_opt[..., 3:] + (camera.get_quaternion_transfrom_mat(pose_opt[..., 3:]) @ step[..., 3:, None]).squeeze(-1), dim=-1)), dim=-1)
        return pose_new


class RSLMSolver(LMSolver):
    """
    Random Sample Levenberg-Marquardt solver, a generalization of RANSAC.
    Used for initialization in ambiguous problems.
    """

    def __init__(self, num_points=16, num_proposals=64, num_iter=3, **kwargs):
        super(RSLMSolver, self).__init__(num_iter=num_iter, **kwargs)
        self.num_points = num_points
        self.num_proposals = num_proposals

    def center_based_init(self, x2d, x3d, camera, eps=1e-06):
        x2dc = solve_wrapper(F.pad(x2d, [0, 1], mode='constant', value=1.0).transpose(-1, -2), camera.cam_mats).transpose(-1, -2)
        x2dc = x2dc[..., :2] / x2dc[..., 2:].clamp(min=eps)
        x2dc_std, x2dc_mean = torch.std_mean(x2dc, dim=-2)
        x3d_std = torch.std(x3d, dim=-2)
        if self.dof == 4:
            t_vec = F.pad(x2dc_mean, [0, 1], mode='constant', value=1.0) * (x3d_std[..., 1] / x2dc_std[..., 1].clamp(min=eps)).unsqueeze(-1)
        else:
            t_vec = F.pad(x2dc_mean, [0, 1], mode='constant', value=1.0) * (math.sqrt(2 / 3) * x3d_std.norm(dim=-1) / x2dc_std.norm(dim=-1).clamp(min=eps)).unsqueeze(-1)
        return t_vec

    def solve(self, x3d, x2d, w2d, camera, cost_fun, **kwargs):
        with torch.no_grad():
            bs, pn, _ = x2d.size()
            if bs > 0:
                mean_weight = w2d.mean(dim=-1).reshape(1, bs, pn).expand(self.num_proposals, -1, -1)
                inds = torch.multinomial(mean_weight.reshape(-1, pn), self.num_points).reshape(self.num_proposals, bs, self.num_points)
                bs_inds = torch.arange(bs, device=inds.device)
                inds += (bs_inds * pn)[:, None]
                x2d_samples = x2d.reshape(-1, 2)[inds]
                x3d_samples = x3d.reshape(-1, 3)[inds]
                w2d_samples = w2d.reshape(-1, 2)[inds]
                pose_init = x2d.new_empty((self.num_proposals, bs, 4 if self.dof == 4 else 7))
                pose_init[..., :3] = self.center_based_init(x2d, x3d, camera)
                if self.dof == 4:
                    pose_init[..., 3] = torch.rand((self.num_proposals, bs), dtype=x2d.dtype, device=x2d.device) * (2 * math.pi)
                else:
                    pose_init[..., 3:] = torch.randn((self.num_proposals, bs, 4), dtype=x2d.dtype, device=x2d.device)
                    q_norm = pose_init[..., 3:].norm(dim=-1)
                    pose_init[..., 3:] /= q_norm.unsqueeze(-1)
                    pose_init.view(-1, 7)[(q_norm < self.eps).flatten(), 3:] = x2d.new_tensor([1, 0, 0, 0])
                camera_expand = camera.shallow_copy()
                camera_expand.repeat_(self.num_proposals)
                cost_fun_expand = cost_fun.shallow_copy()
                cost_fun_expand.repeat_(self.num_proposals)
                pose, _, _ = super(RSLMSolver, self).solve(x3d_samples.reshape(self.num_proposals * bs, self.num_points, 3), x2d_samples.reshape(self.num_proposals * bs, self.num_points, 2), w2d_samples.reshape(self.num_proposals * bs, self.num_points, 2), camera_expand, cost_fun_expand, pose_init=pose_init.reshape(self.num_proposals * bs, pose_init.size(-1)), **kwargs)
                pose = pose.reshape(self.num_proposals, bs, pose.size(-1))
                cost = evaluate_pnp(x3d, x2d, w2d, pose, camera, cost_fun, out_cost=True)[1]
                min_cost, min_cost_ind = cost.min(dim=0)
                pose = pose[min_cost_ind, torch.arange(bs, device=pose.device)]
            else:
                pose = x2d.new_empty((0, 4 if self.dof == 4 else 7))
                min_cost = x2d.new_empty((0,))
            return pose, None, min_cost


class CosineAngleLoss(nn.Module):

    def __init__(self, reduction='mean', loss_weight=1.0):
        super().__init__()
        self.reduction = reduction
        self.loss_weight = loss_weight

    def forward(self, pred, target, weight=None, avg_factor=None, reduction_override=None):
        assert reduction_override in (None, 'none', 'mean', 'sum')
        reduction = reduction_override if reduction_override else self.reduction
        loss = self.loss_weight * cosine_angle_loss(pred, target, weight, reduction=reduction, avg_factor=avg_factor)
        return loss


def get_overlap_boxes(bboxes1, bboxes2):
    overlap_boxes = bboxes1.new_empty((bboxes1.size(0), bboxes2.size(0), 4))
    bboxes1 = bboxes1.unsqueeze(1)
    overlap_boxes[..., :2] = torch.max(bboxes1[..., :2], bboxes2[:, :2])
    overlap_boxes[..., 2:] = torch.min(bboxes1[..., 2:], bboxes2[:, 2:])
    pos_mask = overlap_boxes[..., 2:] - overlap_boxes[..., :2] > 0
    pos_mask = torch.all(pos_mask, dim=2)
    return pos_mask, overlap_boxes


def logsumexp_across_rois(roi_inputs, rois):
    """
    Args:
        roi_inputs (torch.Tensor): shape (bn, chn, rh, rw)
        rois (torch.Tensor): shape (bn, 5)
    Returns:
        Tensor, shape (bn, chn, rh, rw)
    """
    bn, kn, rh, rw = roi_inputs.size()
    rois_logsumexp = roi_inputs.clone()
    if bn > 0:
        roi_ids = rois[:, 0].round().int()
        roi_bboxes = rois[:, 1:]
        for roi_id in range(max(roi_ids) + 1):
            ids = (roi_id == roi_ids).nonzero(as_tuple=False).squeeze(-1)
            if len(ids) <= 1:
                continue
            boxes = roi_bboxes[ids]
            pos_mask, overlap_boxes = get_overlap_boxes(boxes, boxes)
            pos_mask.fill_diagonal_(False)
            for id_self, pos_mask_single, overlap_boxes_single in zip(ids, pos_mask, overlap_boxes):
                if not any(pos_mask_single):
                    continue
                ids_overlap = ids[pos_mask_single]
                num_overlap = ids_overlap.size(0)
                roi_input_self = roi_inputs[id_self]
                roi_inputs_overlap = roi_inputs[ids_overlap]
                bbox_self = roi_bboxes[id_self]
                bbox_overlap = roi_bboxes[ids_overlap]
                overlap_boxes_ = overlap_boxes_single[pos_mask_single]
                wh_bbox_self = bbox_self[2:] - bbox_self[:2]
                wh_bbox_overlap = bbox_overlap[:, 2:] - bbox_overlap[:, :2]
                scale = wh_bbox_self / wh_bbox_overlap
                xy_tl_in_bbox_overlap = 2 * (overlap_boxes_[:, :2] - bbox_overlap[:, :2]) / (bbox_overlap[:, 2:] - bbox_overlap[:, :2]) - 1
                xy_tl_in_bbox_self = 2 * (overlap_boxes_[:, :2] - bbox_self[:2]) / (bbox_self[2:] - bbox_self[:2]) - 1
                affine_mat = wh_bbox_self.new_zeros((num_overlap, 2, 3))
                affine_mat[:, 0, 0] = scale[:, 0]
                affine_mat[:, 1, 1] = scale[:, 1]
                affine_mat[:, :, 2] = xy_tl_in_bbox_overlap - scale * xy_tl_in_bbox_self
                grid = F.affine_grid(affine_mat, (num_overlap, 1, rh, rw), align_corners=False)
                roi_inputs_resample = F.grid_sample(roi_inputs_overlap, grid, padding_mode='border', align_corners=False)
                valid_grid = torch.all((grid > -1) & (grid < 1), dim=3).unsqueeze(1)
                roi_inputs_resampled_cat_self = roi_inputs_resample.new_empty((num_overlap + 1, kn, rh, rw))
                roi_inputs_resampled_cat_self[:-1] = roi_inputs_resample.masked_fill(~valid_grid, float('-inf'))
                roi_inputs_resampled_cat_self[-1] = roi_input_self
                rois_logsumexp[id_self] = roi_inputs_resampled_cat_self.logsumexp(dim=0)
    return rois_logsumexp


class MVDGaussianMixtureNLLLoss(nn.Module):

    def __init__(self, dim=1, reduction='mean', loss_weight=1.0, adaptive_weight=True, momentum=0.1, eps=0.0001):
        super(MVDGaussianMixtureNLLLoss, self).__init__()
        self.reduction = reduction
        self.loss_weight = loss_weight
        self.adaptive_weight = adaptive_weight
        self.momentum = momentum
        self.register_buffer('mean_inv_std', torch.tensor(1, dtype=torch.float))
        self.dim = dim
        self.eps = eps

    def forward(self, pred, target, logstd=None, logmixweight=None, weight=None, avg_factor=None, reduction_override=None, **kwargs):
        assert reduction_override in (None, 'none', 'mean', 'sum')
        reduction = reduction_override if reduction_override else self.reduction
        loss = self.loss_weight * mvd_gaussian_mixture_nll_loss(pred, target, weight, logstd=logstd, logmixweight=logmixweight, adaptive_weight=self.adaptive_weight, momentum=self.momentum, mean_inv_std=self.mean_inv_std, dim=self.dim, eps=self.eps, training=self.training, reduction=reduction, avg_factor=avg_factor, **kwargs)
        return loss


class DeformableAttentionSampler(nn.Module):

    def __init__(self, embed_dims=256, num_heads=8, num_points=32, stride=4, ffn_cfg=dict(type='FFN', embed_dims=256, feedforward_channels=1024, num_fcs=2, ffn_drop=0.1, act_cfg=dict(type='ReLU', inplace=True)), norm_cfg=dict(type='LN'), init_cfg=None):
        super(DeformableAttentionSampler, self).__init__()
        self.embed_dims = embed_dims
        self.num_heads = num_heads
        self.num_points = num_points
        self.stride = stride
        self.ffn_cfg = ffn_cfg
        self.norm_cfg = norm_cfg
        self.sampling_offsets = nn.Linear(self.embed_dims, self.num_heads * self.num_points * 2)
        self.out_proj = nn.Linear(self.embed_dims, self.embed_dims)
        self.layer_norms = nn.ModuleList([build_norm_layer(norm_cfg, self.embed_dims)[1] for _ in range(2)])
        self.ffn = build_feedforward_network(self.ffn_cfg, dict(type='FFN'))
        self.init_weights()

    def init_weights(self):
        xavier_init(self.sampling_offsets, gain=2.5, distribution='uniform')
        for m in [self.layer_norms, self.ffn]:
            if hasattr(m, 'init_weights'):
                m.init_weights()
        self._is_init = True

    def forward(self, query, obj_emb, key, value, img_dense_x2d, img_dense_x2d_mask, obj_xy_point, strides, obj_img_ind):
        """
        Args:
            query: shape (num_obj, num_head, 1, head_emb_dim)
            obj_emb: shape (num_obj, embed_dim)
            key: shape (num_img, embed_dim, h, w)
            value: shape (num_img, embed_dim, h, w)
            img_dense_x2d: shape (num_img, 2, h, w)
            img_dense_x2d_mask: shape (num_img, 1, h, w)
            obj_xy_point: shape (num_obj, 2)
            strides: shape (num_obj, )
            obj_img_ind: shape (num_obj, )

        Returns:
            tuple[tensor]:
                output (num_obj_sample, embed_dim)
                v_samples (num_obj_sample, num_head, head_emb_dim, num_point)
                a_samples (num_obj_sample, num_head, 1, num_point)
                mask_samples (num_obj_sample, num_head, 1, num_point)
                x2d_samples (num_obj_sample, num_head, 2, num_point)
        """
        num_obj_samples = query.size(0)
        num_img, _, h_out, w_out = key.size()
        head_emb_dim = self.embed_dims // self.num_heads
        offsets = self.sampling_offsets(obj_emb).reshape(num_obj_samples, self.num_heads, self.num_points, 2)
        sampling_location = obj_xy_point[:, None, None] + offsets * strides[:, None, None, None]
        hw_img = key.new_tensor(key.shape[-2:]) * self.stride
        sampling_grid = sampling_location * (2 / hw_img[[1, 0]]) - 1
        sampling_grid = sampling_grid.transpose(1, 0).reshape(self.num_heads, num_obj_samples, self.num_points, 1, 2)
        img_ind_grid = (obj_img_ind + 0.5) * (2 / num_img) - 1.0
        sampling_grid = torch.cat((sampling_grid, img_ind_grid[None, :, None, None, None].expand(self.num_heads, -1, self.num_points, 1, 1)), dim=-1)
        k_samples = F.grid_sample(key.reshape(num_img, self.num_heads, head_emb_dim, h_out, w_out).permute(1, 2, 0, 3, 4), sampling_grid, mode='bilinear', padding_mode='border', align_corners=False).squeeze(-1).permute(2, 0, 1, 3)
        v_samples = F.grid_sample(value.reshape(num_img, self.num_heads, head_emb_dim, h_out, w_out).permute(1, 2, 0, 3, 4), sampling_grid, mode='bilinear', padding_mode='border', align_corners=False).squeeze(-1).permute(2, 0, 1, 3)
        x2d_samples = F.grid_sample(img_dense_x2d.transpose(1, 0)[None].expand(self.num_heads, -1, -1, -1, -1), sampling_grid, mode='bilinear', padding_mode='border', align_corners=False).squeeze(-1).permute(2, 0, 1, 3)
        mask_samples = F.grid_sample(img_dense_x2d_mask.transpose(1, 0)[None].expand(self.num_heads, -1, -1, -1, -1), sampling_grid, mode='bilinear', padding_mode='zeros', align_corners=False).squeeze(-1).permute(2, 0, 1, 3)
        a_samples = query @ k_samples / np.sqrt(head_emb_dim)
        a_samples_softmax = a_samples.softmax(dim=-1) * mask_samples
        output = v_samples @ a_samples_softmax.reshape(num_obj_samples, self.num_heads, self.num_points, 1)
        output = output.reshape(num_obj_samples, self.embed_dims)
        output = self.out_proj(output) + obj_emb
        output = self.layer_norms[0](output)
        output = self.ffn(output, output)
        output = self.layer_norms[1](output)
        return output, v_samples, a_samples, mask_samples, x2d_samples


class GroupLinear(nn.Module):

    def __init__(self, in_features, out_features, groups, bias=True):
        super(GroupLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.groups = groups
        self.weight = nn.Parameter(torch.Tensor(groups, out_features // groups, in_features // groups))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(groups, out_features // groups))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        for weight in self.weight:
            nn.init.kaiming_uniform_(weight, nonlinearity='linear')
        if self.bias is not None:
            nn.init.constant_(self.bias, 0)

    def forward(self, input):
        """
        Args:
            input (Tensor): shape (*, in_features)
        """
        batch_size = input.shape[:-1]
        if self.bias is not None:
            output = self.weight @ input.reshape(*batch_size, self.groups, self.in_features // self.groups, 1) + self.bias[..., None]
        else:
            output = self.weight @ input.reshape(*batch_size, self.groups, self.in_features // self.groups, 1)
        return output.reshape(*batch_size, self.out_features)


import torch
from torch.nn import MSELoss, ReLU
from types import SimpleNamespace


TESTCASES = [
    # (nn.Module, init_args, forward_args)
    (CDPN,
     lambda: ([], {'backbone': torch.nn.ReLU(), 'rot_head_net': torch.nn.ReLU(), 'trans_head_net': torch.nn.ReLU()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (GroupLinear,
     lambda: ([], {'in_features': 4, 'out_features': 4, 'groups': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (RotHeadNet,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (TransHeadNet,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
]


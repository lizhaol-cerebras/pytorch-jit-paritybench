import sys
_module = sys.modules[__name__]
del sys
examples = _module
experimental = _module
main = _module
main = _module
classification = _module
bootstrap_nas = _module
bootstrap_nas_search = _module
main = _module
main = _module
main = _module
tensorflow = _module
builder = _module
cifar = _module
imagenet = _module
utils = _module
preprocessing_selector = _module
tfrecords = _module
cifar10 = _module
cifar100 = _module
imagenet2012 = _module
argparser = _module
callbacks = _module
dataset_builder = _module
distributed = _module
logger = _module
model_loader = _module
models = _module
darknet = _module
factory = _module
fpn = _module
heads = _module
nn_ops = _module
resnet = _module
base_model = _module
checkpoint_utils = _module
coco2017 = _module
coco_evaluator = _module
coco_utils = _module
losses = _module
nms = _module
postprocess_ops = _module
roi_ops = _module
spatial_transform_ops = _module
target_ops = _module
anchor = _module
argmax_matcher = _module
balanced_positive_negative_sampler = _module
box_coder = _module
box_list = _module
box_utils = _module
dataloader_utils = _module
faster_rcnn_box_coder = _module
input_utils = _module
mask_utils = _module
matcher = _module
minibatch_sampler = _module
ops = _module
region_similarity_calculator = _module
shape_utils = _module
target_assigner = _module
yolo_v4_utils = _module
optimizer = _module
prepare_checkpoint = _module
sample_config = _module
scheduler = _module
tfrecords_dataset = _module
model_selector = _module
retinanet_config = _module
retinanet_model = _module
yolo_v4_config = _module
yolo_v4_model = _module
yolo_v4_postprocessing = _module
retinanet_preprocessing = _module
yolo_v4_preprocessing = _module
evaluation = _module
maskrcnn_config = _module
maskrcnn_model = _module
maskrcnn_preprocessing = _module
train = _module
main = _module
mobilenet_v2_32x32 = _module
staged_quantization_worker = _module
common = _module
argparser = _module
distributed = _module
example_logger = _module
execution = _module
model_loader = _module
models = _module
efficientnet = _module
inceptionv3_cifar100 = _module
mobilenet_v2_cifar10 = _module
mobilenet_v2_tv_092 = _module
mobilenet_v3_tv_092 = _module
resnet_cifar = _module
resnet_cifar10 = _module
rmnet_cifar = _module
vgg11_bn_cifar10 = _module
segmentation = _module
enet = _module
icnet = _module
unet = _module
optimizer = _module
restricted_pickle_module = _module
utils = _module
object_detection = _module
dataset = _module
datasets = _module
coco = _module
voc0712 = _module
eval = _module
layers = _module
box_utils = _module
extensions = _module
functions = _module
detection = _module
prior_box = _module
modules = _module
l2norm = _module
multibox_loss = _module
ssd_head = _module
main = _module
model = _module
ssd_mobilenet = _module
ssd_vgg = _module
augmentations = _module
camvid = _module
cityscapes = _module
mapillary = _module
main = _module
metric = _module
confusionmatrix = _module
iou = _module
test = _module
train = _module
checkpoint = _module
data = _module
loss_funcs = _module
transforms = _module
nncf = _module
api = _module
compression = _module
statistics = _module
accuracy_aware_training = _module
runner = _module
runner_factory = _module
training_loop = _module
collector = _module
composite_compression = _module
compression = _module
engine = _module
exporter = _module
graph = _module
definitions = _module
graph_matching = _module
layer_attributes = _module
model_transformer = _module
operator_metatypes = _module
patterns = _module
transformations = _module
commands = _module
layout = _module
hardware = _module
config = _module
opset = _module
initialization = _module
batchnorm_adaptation = _module
dataloader = _module
insertion_point_graph = _module
logging = _module
progress_bar = _module
pruning = _module
clusterization = _module
mask_propagation = _module
model_analysis = _module
node_selector = _module
operations = _module
schedulers = _module
shape_pruning_processor = _module
structs = _module
symbolic_mask = _module
tensor_processor = _module
weights_flops_calculator = _module
quantization = _module
collectors = _module
config_assignment = _module
range = _module
quantizer_propagation = _module
graph = _module
grouping = _module
solver = _module
visualizer = _module
quantizer_setup = _module
quantizers = _module
scopes = _module
sparsity = _module
controller = _module
stateful_classes_registry = _module
tensor = _module
tensor_statistics = _module
aggregator = _module
reduction = _module
statistic_point = _module
backend = _module
debug = _module
dot_file_rw = _module
helpers = _module
os = _module
registry = _module
tensorboard = _module
extractors = _module
schema = _module
schemata = _module
accuracy_aware = _module
algo = _module
binarization = _module
const_sparsity = _module
filter_pruning = _module
knowledge_distillation = _module
magnitude_sparsity = _module
rb_sparsity = _module
basic = _module
targeting = _module
defaults = _module
experimental_schema = _module
structures = _module
telemetry_extractors = _module
onnx = _module
metatypes = _module
onnx_metatypes = _module
nncf_graph_builder = _module
onnx_graph = _module
commands = _module
fused_patterns = _module
pattern_operations = _module
model_normalizer = _module
default_quantization = _module
quantizer_parameters = _module
openvino_native = _module
openvino_metatypes = _module
commands = _module
compression_builder = _module
context = _module
argprovider = _module
converter = _module
node_attributes = _module
nncf_network = _module
patch_tf = _module
algorithm = _module
init_range = _module
scope = _module
nas = _module
bootstrapNAS = _module
elasticity = _module
base_handler = _module
elastic_depth = _module
elastic_kernel = _module
elastic_width = _module
elasticity_builder = _module
elasticity_controller = _module
elasticity_dim = _module
filter_reorder = _module
multi_elasticity_handler = _module
visualization = _module
search = _module
evaluator = _module
evaluator_handler = _module
search = _module
training = _module
base_training = _module
lr_scheduler = _module
model_creator_helpers = _module
progressive_shrinking_builder = _module
progressive_shrinking_controller = _module
scheduler = _module
stage_descriptor = _module
training_algorithm = _module
search_building_blocks = _module
search_blocks = _module
search_graph = _module
movement = _module
algo = _module
functions = _module
layers = _module
loss = _module
scheduler = _module
structured_mask_handler = _module
structured_mask_strategy = _module
quantize = _module
openvino = _module
parameters = _module
algorithms = _module
bias_correction = _module
onnx_backend = _module
fast_bias_correction = _module
min_max = _module
post_training = _module
quantize = _module
telemetry = _module
decorator = _module
events = _module
wrapper = _module
keras_model_utils = _module
algorithm_selector = _module
checkpoint_callback = _module
statistics_callback = _module
keras_layers = _module
tf_ops = _module
callback_creation = _module
model_creation = _module
model_manager = _module
custom_objects = _module
data_layout = _module
operation = _module
base_algorithm = _module
magnitude = _module
rb = _module
tf_internals = _module
hook_handle = _module
node = _module
scopes_handle = _module
state = _module
runner = _module
utils = _module
algo_selector = _module
ddpg = _module
memory = _module
quantization_env = _module
batchnorm_adaptation = _module
algo = _module
binarize_functions = _module
extensions = _module
layers = _module
checkpoint_loading = _module
composite_compression = _module
compression_method_api = _module
debug = _module
dynamic_graph = _module
context = _module
graph = _module
graph_tracer = _module
io_handling = _module
op_input_processing = _module
operation_address = _module
patch_pytorch = _module
scope = _module
trace_functions = _module
trace_tensor = _module
transform_graph = _module
wrappers = _module
exporter = _module
extensions = _module
functions = _module
graph = _module
graph_builder = _module
operator_metatypes = _module
config = _module
fused_patterns = _module
initialization = _module
algo = _module
knowledge_distillation_handler = _module
knowledge_distillation_loss = _module
layer_utils = _module
layers = _module
model_creation = _module
module_operations = _module
nested_objects_traversal = _module
nncf_network = _module
base_algo = _module
export_utils = _module
algo = _module
functions = _module
global_ranking = _module
evolutionary_optimization = _module
legr = _module
layers = _module
operations = _module
structs = _module
tensor_processor = _module
utils = _module
adjust_padding = _module
algo = _module
default_quantization = _module
extensions = _module
hessian_trace = _module
init_precision = _module
init_range = _module
layers = _module
metrics = _module
precision_constraints = _module
precision_init = _module
adjacent_quantizers = _module
autoq_init = _module
base_init = _module
bitwidth_graph = _module
compression_ratio = _module
hawq_debug = _module
hawq_init = _module
manual_init = _module
perturbations = _module
traces_order = _module
quantize = _module
quantize_functions = _module
structs = _module
translator = _module
base_algo = _module
collector = _module
const = _module
algo = _module
functions = _module
layers = _module
algo = _module
functions = _module
algo = _module
functions = _module
layers = _module
loss = _module
structures = _module
tensor = _module
algo = _module
collectors = _module
reduction = _module
statistics = _module
utils = _module
version = _module
setup = _module
tests = _module
nx_graph = _module
test_graph_matching = _module
test_graph_pattern = _module
test_utils = _module
dummy_types = _module
test_pruning_operations = _module
test_symbolic_mask_processor = _module
mock_graphs = _module
test_filter_constant_nodes = _module
test_ignore_post_processing = _module
test_quantizer_propagation_graph = _module
test_quantizer_propagation_solver = _module
test_builder_state = _module
test_ctrl_state = _module
test_parameters = _module
test_progress_bar = _module
test_scopes = _module
test_telemetry = _module
cross_fw = _module
install = _module
conftest = _module
install_checks_onnx = _module
install_checks_openvino = _module
install_checks_tf = _module
install_checks_torch = _module
test_install = _module
test_compressed_graph = _module
test_context_independence = _module
test_keras_layer_model = _module
test_models = _module
benchmarking = _module
accuracy_checker = _module
run_ptq = _module
opset_converter = _module
test_calculation_quantizer_params = _module
test_classification_models_graph = _module
test_detection_models_graph = _module
test_graphs = _module
test_ptq_params = _module
test_quantizer_config = _module
test_segmentation_models_graph = _module
test_classification_sanity_sample = _module
test_e2e_ptq = _module
test_metatypes = _module
test_model_transformer = _module
test_nncf_graph_builder = _module
test_statistics_aggregator = _module
test_weightless_model = _module
weightless_model = _module
native = _module
pot = _module
test_helpers = _module
test_engine = _module
model_scope = _module
test_quantize_conformance = _module
case_collection = _module
command = _module
config_factory = _module
helpers = _module
install_fixtures = _module
paths = _module
serialization = _module
test_keras_api = _module
test_algorithm = _module
test_flops_pruning = _module
test_tensor_processor = _module
test_algorithm_quantization = _module
test_overflow_issue = _module
test_range_init = _module
test_statistics = _module
test_unified_scales = _module
test_scheduler = _module
test_components = _module
test_integration = _module
test_weights = _module
test_common = _module
test_tensor_statistics = _module
test_api_behavior = _module
test_bn_adaptation = _module
test_callbacks = _module
test_ignored_scopes = _module
test_init = _module
test_model_converter = _module
densenet = _module
inception_resnet_v2 = _module
inception_v3 = _module
maskrcnn = _module
mobilenet = _module
mobilenet_v2 = _module
mobilenet_v2_slim = _module
mobilenet_v3 = _module
nasnet = _module
resnet_v2 = _module
retinanet = _module
sequential_model = _module
shared_layers_model = _module
vgg16 = _module
xception = _module
yolo_v4 = _module
test_nncf_keras_graph_equality = _module
test_no_compression_algorithm = _module
test_sanity_sample = _module
test_sota_checkpoints = _module
test_transformations = _module
test_weekly = _module
test_wrapper_freeze = _module
test_accuracy_aware_config = _module
test_runner = _module
test_training_loop = _module
test_autograd_functions = _module
test_ddpg = _module
test_memory = _module
test_quantization_env = _module
test_ring_buffer = _module
test_functions = _module
test_composite_scheduler = _module
test_sparsity_quantization = _module
conftest = _module
test_search_building_blocks = _module
test_transformer_blocks = _module
extensions_build_checks = _module
helpers = _module
seq2seq = _module
attention = _module
decoder = _module
encoder = _module
gnmt = _module
seq2seq_base = _module
test_rnn = _module
creators = _module
descriptors = _module
helpers = _module
synthetic = _module
tcn = _module
vgg_k7 = _module
test_all_elasticity = _module
test_elastic_depth = _module
test_elastic_kernel = _module
test_elastic_width = _module
test_flops = _module
test_lr_scheduler = _module
test_onnx_export = _module
test_ps_controller = _module
test_sanity_sample = _module
test_scheduler = _module
test_search = _module
test_search_space = _module
test_state = _module
test_algo = _module
test_evolutionary_optimization = _module
test_flops_pruning = _module
test_functions = _module
test_layers = _module
test_legr = _module
test_set_pruning_rate = _module
helpers = _module
test_common = _module
test_distributed = _module
test_model_pruning_analysis = _module
test_onnx_export = _module
test_schedulers = _module
test_tensor_processor = _module
test_utils = _module
quantization_helpers = _module
test_adjust_padding = _module
test_algo_quantization = _module
test_autoq_precision_init = _module
test_functions = _module
test_hawq_precision_init = _module
test_hw_config = _module
test_logarithm_scale = _module
test_manual_precision_init = _module
test_onnx_export = _module
test_overflow_issue_export = _module
test_quantization_metric = _module
test_range_init = _module
test_sanity_sample = _module
test_scheduler = _module
test_serialize_to_json = _module
test_solver_quantization_traits = _module
test_unified_scales = _module
run_examples_for_test_sota = _module
sample_test_validator = _module
test_algo = _module
test_algo = _module
test_helpers = _module
test_modules = _module
test_scheduler = _module
helpers = _module
config = _module
run_recipe = _module
trainer = _module
utils = _module
test_algo = _module
test_components = _module
test_model_saving = _module
test_scheduler = _module
test_structured_mask = _module
test_training = _module
test_training_with_third_party = _module
run_glue = _module
test_algo = _module
test_components = _module
test_weights = _module
test_common = _module
test_tensor_statistics = _module
test_algo_common = _module
test_api_behavior = _module
test_backward_compat = _module
test_compressed_graph = _module
test_compression_lr_multiplier = _module
test_compression_training = _module
test_config_schema = _module
test_context_independence = _module
test_custom_modules = _module
test_distributed_data_parallel_mode = _module
test_extensions_build = _module
test_frozen_layers = _module
test_get_modules_by_type = _module
test_graph_analysis = _module
test_graph_building = _module
test_init_data_loader = _module
test_input_management = _module
test_knowledge_distillation = _module
test_layer_attributes = _module
test_load_model_state = _module
alexnet = _module
densenet = _module
dpn = _module
googlenet = _module
inceptionv3 = _module
lenet = _module
mobilenet = _module
mobilenet_v3 = _module
pnasnet = _module
preact_resnet = _module
resnet = _module
resnext = _module
senet = _module
shufflenet = _module
shufflenetv2 = _module
squeezenet = _module
sr_small_model = _module
ssd_mobilenet = _module
ssd_vgg = _module
synthetic = _module
unet = _module
vgg = _module
test_nncf_network = _module
test_nncf_utils = _module
test_no_compression_algorithm = _module
test_onnx_export = _module
test_pytorch_patch = _module
test_resume_from_checkpoint = _module
test_sanity_sample = _module
test_sanity_third_party = _module
test_sota_checkpoints = _module
test_telemetry = _module
test_utils = _module
tools = _module
add_new_quantization_parameters = _module
add_qualifiers_to_activation_quantizers = _module
benchmark = _module
benchmark_binarize_layers = _module
benchmark_quantize_layers = _module
clip_dot = _module
convert_mobilenet_v2_to_torchvision = _module
correct_checkpoint = _module
common = _module
compare_accuracy = _module
compare_activations = _module
compare_dump = _module
compare_weights = _module
debug_sample = _module
ir_utils = _module
onnx_tranformer = _module
optimize_tf_graph = _module
pb_to_tb = _module
render_dot_to_svg = _module
update_eval_results = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import Iterable


from typing import Any


import torch


import numpy as np


import warnings


from torch import nn


import re


from typing import Dict


from typing import List


from typing import Optional


from sklearn.metrics import accuracy_score


from torchvision import datasets


from torchvision import transforms


from torchvision import models


import time


from copy import deepcopy


from functools import partial


from torch.backends import cudnn


from torch.cuda.amp.autocast_mode import autocast


import torch.nn.parallel


import torch.optim


import torch.utils.data


import torch.utils.data.distributed


from torch.nn.modules.loss import _Loss


from torch.optim.lr_scheduler import ReduceLROnPlateau


from torchvision.datasets import CIFAR10


from torchvision.datasets import CIFAR100


from torchvision.models import InceptionOutputs


from typing import Callable


from torch import Tensor


import copy


from torch import distributed as dist


from torch.utils.data import Sampler


import torch.multiprocessing as mp


import torchvision.models


from collections import namedtuple


import torch.nn.functional as F


from torch.utils import model_zoo


from torch.hub import load_state_dict_from_url


from torch.nn import functional as F


from typing import Sequence


from collections import OrderedDict


from numpy import lcm


from torch.optim import SGD


from torch.optim import Adam


from torch.optim.lr_scheduler import MultiStepLR


from torch.optim.lr_scheduler import StepLR


from torch.optim.lr_scheduler import LambdaLR


from torch.optim.lr_scheduler import ExponentialLR


import collections


import logging


from typing import Tuple


from torch.utils import data


from torch.utils.tensorboard import SummaryWriter


from torchvision.ops import nms as torch_nms


from torch.utils.cpp_extension import load


from itertools import product


from math import sqrt


from torch.nn import init


import types


from numpy import random


import functools


import torchvision.transforms as T


import torchvision


from torchvision.transforms import ToPILImage


import random


import math


from torchvision import transforms as T


from torchvision.transforms import InterpolationMode


from torchvision.transforms import functional as F


from abc import ABC


from abc import abstractmethod


from typing import TypeVar


from collections import deque


from copy import copy


from typing import Set


from typing import Type


from typing import Union


from collections import Counter


from enum import Enum


from typing import Deque


from itertools import combinations


from torch.nn import Parameter


import inspect


from typing import OrderedDict as OrderedDictType


from typing import NoReturn


from torch.utils.data.dataloader import DataLoader


import matplotlib.pyplot as plt


from functools import cmp_to_key


import torch.distributed as dist


from enum import IntEnum


from functools import reduce


import pandas as pd


from torchvision.transforms import ToTensor


from torch.autograd import Variable


from types import SimpleNamespace


from sklearn.preprocessing import MinMaxScaler


from torch import _C


import torch.nn


from torch.nn import Module


from inspect import Signature


from inspect import Parameter


from itertools import islice


import torch.utils.cpp_extension


from torch.nn import DataParallel


from torch.nn.parallel import DistributedDataParallel


from torch.nn import Conv1d


from torch.nn import Conv2d


from torch.nn import Conv3d


from torch.nn import ConvTranspose1d


from torch.nn import ConvTranspose2d


from torch.nn import ConvTranspose3d


from torch.nn import Linear


from torch.nn import Module as TorchModule


from torch.onnx import OperatorExportTypes


import enum


from torch.utils.cpp_extension import _get_build_directory


from torch.utils.data import DataLoader


import numbers


from torch.nn.utils.rnn import PackedSequence


from torch.nn.utils.weight_norm import WeightNorm


from torch.distributed import barrier


from math import isclose


import queue


from torch import optim


from typing import NamedTuple


from string import Template


from torch import distributed


from itertools import chain


from collections import defaultdict


import itertools


from numpy.testing import assert_allclose


from torch.utils.data import Dataset


from torch.nn.parameter import Parameter


from torch.nn.utils.rnn import pack_padded_sequence


from torch.nn.utils.rnn import pad_packed_sequence


from torch.nn.functional import log_softmax


from enum import auto


from torch.nn.utils import weight_norm


from torchvision.models import resnet50


from torchvision.models import squeezenet1_1


from random import random


from torch.distributions.uniform import Uniform


from numpy.random import random_sample


from torchvision.transforms import transforms


from scipy.special import softmax


import torch.cuda


from torch import cuda


import torch.nn.functional


from torch.nn import AvgPool2d


from torch.nn import BatchNorm2d


from torch.nn import MaxPool2d


from torch.nn import ReLU


from torch.nn import Sequential


from math import ceil


from collections.abc import Iterable


from torch import Size


from torch.nn import Dropout


from torch import randn


class ConvBN(nn.Module):

    def __init__(self, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)
        self.bn = nn.BatchNorm2d(out_channels, momentum=0.05)

    def forward(self, inputs):
        x = self.conv(inputs)
        x = self.bn(x)
        return x


class ConvBNReLU(nn.Module):

    def __init__(self, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, bias=False):
        super().__init__()
        self.convbn = ConvBN(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, inputs):
        x = self.convbn(inputs)
        x = self.relu(x)
        return x


class Conv2dNormActivation(nn.Sequential):

    def __init__(self, in_planes: int, out_planes: int, kernel_size: int=3, stride: int=1, groups: int=1, norm_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None, dilation: int=1) ->None:
        padding = (kernel_size - 1) // 2 * dilation
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if activation_layer is None:
            activation_layer = nn.ReLU6
        super(ConvBNReLU, self).__init__(nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups, bias=False), norm_layer(out_planes), activation_layer(inplace=True))
        self.out_channels = out_planes


class InvertedResidual(nn.Module):

    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, se_layer):
        super().__init__()
        assert stride in [1, 2]
        self.identity = stride == 1 and inp == oup
        self.conv = nn.Sequential(nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False), nn.BatchNorm2d(hidden_dim), nn.ReLU(inplace=True), se_layer(hidden_dim), nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup))

    def forward(self, x):
        if self.identity:
            return x + self.conv(x)
        return self.conv(x)


def _make_divisible(v: float, divisor: int, min_value: Optional[int]=None) ->int:
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class MobileNetV2For32x32(nn.Module):

    def __init__(self, num_classes: int=100, width_mult: float=1.0, inverted_residual_setting: Optional[List[List[int]]]=None, round_nearest: int=8, block: Optional[Callable[..., nn.Module]]=None, norm_layer: Optional[Callable[..., nn.Module]]=None) ->None:
        """
        A special version for MobileNet that is supposed to give solid
        accuracy on CIFAR100 (~68% top1). The differences from the
        regular ImageNet version are:
        1) stride 1, kernel size 1 instead of stride 2, kernel size 3 for the first conv
        2) tcns config [6, 160, 3, 1] for the second-to-last feature block instead of [6, 160, 3, 2]
        (e.g. stride 1 vs stride 2)


        Args:
            num_classes (int): Number of classes
            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount
            inverted_residual_setting: Network structure
            round_nearest (int): Round the number of channels in each layer to be a multiple of this number
            Set to 1 to turn off rounding
            block: Module specifying inverted residual building block for mobilenet
            norm_layer: Module specifying the normalization layer to use

        """
        super().__init__()
        if block is None:
            block = InvertedResidual
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        input_channel = 32
        last_channel = 1280
        if inverted_residual_setting is None:
            inverted_residual_setting = [[1, 16, 1, 1], [6, 24, 2, 2], [6, 32, 3, 2], [6, 64, 4, 2], [6, 96, 3, 1], [6, 160, 3, 1], [6, 320, 1, 1]]
        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:
            raise ValueError('inverted_residual_setting should be non-empty or a 4-element list, got {}'.format(inverted_residual_setting))
        input_channel = _make_divisible(input_channel * width_mult, round_nearest)
        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)
        features: List[nn.Module] = [ConvBNReLU(3, input_channel, kernel_size=1, stride=1, norm_layer=norm_layer)]
        for t, c, n, s in inverted_residual_setting:
            output_channel = _make_divisible(c * width_mult, round_nearest)
            for i in range(n):
                stride = s if i == 0 else 1
                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))
                input_channel = output_channel
        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))
        self.features = nn.Sequential(*features)
        self.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(self.last_channel, num_classes))
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def _forward_impl(self, x: Tensor) ->Tensor:
        x = self.features(x)
        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)).reshape(x.shape[0], -1)
        x = self.classifier(x)
        return x

    def forward(self, x: Tensor) ->Tensor:
        return self._forward_impl(x)


class BasicConv2d(nn.Module):

    def __init__(self, in_channels, out_channels, **kwargs):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)


class InceptionA(nn.Module):

    def __init__(self, in_channels, pool_features):
        super().__init__()
        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)
        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)
        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)
        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)
        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)
        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionAux(nn.Module):

    def __init__(self, in_channels, num_classes):
        super().__init__()
        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)
        self.conv1 = BasicConv2d(128, 768, kernel_size=5)
        self.conv1.stddev = 0.01
        self.fc = nn.Linear(768, num_classes)
        self.fc.stddev = 0.001

    def forward(self, x):
        x = F.avg_pool2d(x, kernel_size=5, stride=3)
        x = self.conv0(x)
        x = self.conv1(x)
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class InceptionB(nn.Module):

    def __init__(self, in_channels):
        super().__init__()
        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)
        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)
        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)
        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)

    def forward(self, x):
        branch3x3 = self.branch3x3(x)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)
        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)
        outputs = [branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionC(nn.Module):

    def __init__(self, in_channels, channels_7x7):
        super().__init__()
        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)
        c7 = channels_7x7
        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)
        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)
        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))
        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)
        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return torch.cat(outputs, 1)


class InceptionD(nn.Module):

    def __init__(self, in_channels):
        super().__init__()
        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)
        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)
        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)
        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))
        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))
        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)

    def forward(self, x):
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)
        branch7x7x3 = self.branch7x7x3_1(x)
        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)
        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)
        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)
        outputs = [branch3x3, branch7x7x3, branch_pool]
        return torch.cat(outputs, 1)


class InceptionE(nn.Module):

    def __init__(self, in_channels):
        super().__init__()
        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)
        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)
        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))
        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)
        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)
        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))
        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))
        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)]
        branch3x3 = torch.cat(branch3x3, 1)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [self.branch3x3dbl_3a(branch3x3dbl), self.branch3x3dbl_3b(branch3x3dbl)]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class Inception3(nn.Module):

    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False):
        super().__init__()
        self.aux_logits = aux_logits
        self.transform_input = transform_input
        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)
        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)
        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)
        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)
        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)
        self.Mixed_5b = InceptionA(192, pool_features=32)
        self.Mixed_5c = InceptionA(256, pool_features=64)
        self.Mixed_5d = InceptionA(288, pool_features=64)
        self.Mixed_6a = InceptionB(288)
        self.Mixed_6b = InceptionC(768, channels_7x7=128)
        self.Mixed_6c = InceptionC(768, channels_7x7=160)
        self.Mixed_6d = InceptionC(768, channels_7x7=160)
        self.Mixed_6e = InceptionC(768, channels_7x7=192)
        if aux_logits:
            self.AuxLogits = InceptionAux(768, num_classes)
        self.Mixed_7a = InceptionD(768)
        self.Mixed_7b = InceptionE(1280)
        self.Mixed_7c = InceptionE(2048)
        self.fc = nn.Linear(2048, num_classes)
        for m in self.modules():
            if isinstance(m, (nn.Conv2d, nn.Linear)):
                from scipy import stats
                stddev = m.stddev if hasattr(m, 'stddev') else 0.1
                X = stats.truncnorm(-2, 2, scale=stddev)
                values = torch.Tensor(X.rvs(m.weight.numel()))
                values = values.view(m.weight.size())
                m.weight.data.copy_(values)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        if self.transform_input:
            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5
            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5
            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5
            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)
        x = self.Conv2d_1a_3x3(x)
        x = self.Conv2d_2a_3x3(x)
        x = self.Conv2d_2b_3x3(x)
        x = F.max_pool2d(x, kernel_size=3, stride=2)
        x = self.Conv2d_3b_1x1(x)
        x = self.Conv2d_4a_3x3(x)
        x = F.max_pool2d(x, kernel_size=3, stride=2)
        x = self.Mixed_5b(x)
        x = self.Mixed_5c(x)
        x = self.Mixed_5d(x)
        x = self.Mixed_6a(x)
        x = self.Mixed_6b(x)
        x = self.Mixed_6c(x)
        x = self.Mixed_6d(x)
        x = self.Mixed_6e(x)
        if self.training and self.aux_logits:
            aux = self.AuxLogits(x)
        x = self.Mixed_7a(x)
        x = self.Mixed_7b(x)
        x = self.Mixed_7c(x)
        x = F.adaptive_avg_pool2d(x, (1, 1))
        x = F.dropout(x, training=self.training)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        if self.training and self.aux_logits:
            return x, aux
        return x


class ConvBNActivation(nn.Sequential):

    def __init__(self, in_planes: int, out_planes: int, kernel_size: int=3, stride: int=1, groups: int=1, norm_layer: Optional[Callable[..., nn.Module]]=None, activation_layer: Optional[Callable[..., nn.Module]]=None, dilation: int=1) ->None:
        padding = (kernel_size - 1) // 2 * dilation
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if activation_layer is None:
            activation_layer = nn.ReLU6
        super(ConvBNReLU, self).__init__(nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups, bias=False), norm_layer(out_planes), activation_layer(inplace=True))
        self.out_channels = out_planes


class SqueezeExcitation(nn.Module):

    def __init__(self, input_channels: int, squeeze_factor: int=4):
        super().__init__()
        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)
        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)

    def _scale(self, input_: Tensor, inplace: bool) ->Tensor:
        scale = F.adaptive_avg_pool2d(input_, 1)
        scale = self.fc1(scale)
        scale = self.relu(scale)
        scale = self.fc2(scale)
        return F.hardsigmoid(scale, inplace=inplace)

    def forward(self, input_: Tensor) ->Tensor:
        scale = self._scale(input_, True)
        return scale * input_


class InvertedResidualConfig:

    def __init__(self, input_channels: int, kernel: int, expanded_channels: int, out_channels: int, use_se: bool, activation: str, stride: int, dilation: int, width_mult: float):
        self.input_channels = self.adjust_channels(input_channels, width_mult)
        self.kernel = kernel
        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)
        self.out_channels = self.adjust_channels(out_channels, width_mult)
        self.use_se = use_se
        self.use_hs = activation == 'HS'
        self.stride = stride
        self.dilation = dilation

    @staticmethod
    def adjust_channels(channels: int, width_mult: float):
        return _make_divisible(channels * width_mult, 8)


class MobileNetV3(nn.Module):

    def __init__(self, inverted_residual_setting: List[InvertedResidualConfig], last_channel: int, num_classes: int=1000, block: Optional[Callable[..., nn.Module]]=None, norm_layer: Optional[Callable[..., nn.Module]]=None) ->None:
        """
        MobileNet V3 main class

        Args:
            inverted_residual_setting (List[InvertedResidualConfig]): Network structure
            last_channel (int): The number of channels on the penultimate layer
            num_classes (int): Number of classes
            block (Optional[Callable[..., nn.Module]]): Module specifying inverted residual building block for mobilenet
            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use
        """
        super().__init__()
        if not inverted_residual_setting:
            raise ValueError('The inverted_residual_setting should not be empty')
        if not (isinstance(inverted_residual_setting, Sequence) and all(isinstance(s, InvertedResidualConfig) for s in inverted_residual_setting)):
            raise TypeError('The inverted_residual_setting should be List[InvertedResidualConfig]')
        if block is None:
            block = InvertedResidual
        if norm_layer is None:
            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)
        layers: List[nn.Module] = []
        firstconv_output_channels = inverted_residual_setting[0].input_channels
        layers.append(ConvBNActivation(3, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer, activation_layer=nn.Hardswish))
        for cnf in inverted_residual_setting:
            layers.append(block(cnf, norm_layer))
        lastconv_input_channels = inverted_residual_setting[-1].out_channels
        lastconv_output_channels = 6 * lastconv_input_channels
        layers.append(ConvBNActivation(lastconv_input_channels, lastconv_output_channels, kernel_size=1, norm_layer=norm_layer, activation_layer=nn.Hardswish))
        self.features = nn.Sequential(*layers)
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Sequential(nn.Linear(lastconv_output_channels, last_channel), nn.Hardswish(inplace=True), nn.Dropout(p=0.2, inplace=True), nn.Linear(last_channel, num_classes))
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def _forward_impl(self, x: Tensor) ->Tensor:
        x = self.features(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

    def forward(self, x: Tensor) ->Tensor:
        return self._forward_impl(x)


class ShuffleBlock(nn.Module):

    def __init__(self, groups=2):
        super().__init__()
        self.groups = groups

    def forward(self, x):
        """Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]"""
        N, C, H, W = x.size()
        g = self.groups
        return x.view(N, g, C // g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)


class SplitBlock(nn.Module):

    def __init__(self, ratio):
        super().__init__()
        self.ratio = ratio

    def forward(self, x):
        c = int(x.size(1) * self.ratio)
        return x[:, :c, :, :], x[:, c:, :, :]


class BasicBlock(nn.Module):

    def __init__(self, in_channels, split_ratio=0.5):
        super().__init__()
        self.split = SplitBlock(split_ratio)
        in_channels = int(in_channels * split_ratio)
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, groups=in_channels, bias=False)
        self.bn2 = nn.BatchNorm2d(in_channels)
        self.conv3 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(in_channels)
        self.shuffle = ShuffleBlock()

    def forward(self, x):
        x1, x2 = self.split(x)
        out = F.relu(self.bn1(self.conv1(x2)))
        out = self.bn2(self.conv2(out))
        out = F.relu(self.bn3(self.conv3(out)))
        out = torch.cat([x1, out], 1)
        out = self.shuffle(out)
        return out


class Bottleneck(nn.Module):

    def __init__(self, in_planes, out_planes, stride, groups):
        super().__init__()
        self.stride = stride
        mid_planes = out_planes // 4
        g = 1 if in_planes == 24 else groups
        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_planes)
        self.shuffle1 = ShuffleBlock(groups=g)
        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_planes)
        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)
        self.bn3 = nn.BatchNorm2d(out_planes)
        self.shortcut = nn.Sequential()
        if stride == 2:
            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.shuffle1(out)
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        res = self.shortcut(x)
        out = F.relu(torch.cat([out, res], 1)) if self.stride == 2 else F.relu(out + res)
        return out


class ResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for block_stride in strides:
            layers.append(block(self.in_planes, planes, block_stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.maxpool(out)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        out = self.linear(out)
        return out


class RMBlock(nn.Module):

    def __init__(self, input_planes, squeeze_planes, output_planes, downsample=False, dropout_ratio=0.1, activation=nn.ELU):
        super().__init__()
        self.downsample = downsample
        self.input_planes = input_planes
        self.output_planes = output_planes
        self.squeeze_conv = nn.Conv2d(input_planes, squeeze_planes, kernel_size=1, bias=False)
        self.squeeze_bn = nn.BatchNorm2d(squeeze_planes)
        self.dw_conv = nn.Conv2d(squeeze_planes, squeeze_planes, groups=squeeze_planes, kernel_size=3, padding=1, stride=2 if downsample else 1, bias=False)
        self.dw_bn = nn.BatchNorm2d(squeeze_planes)
        self.expand_conv = nn.Conv2d(squeeze_planes, output_planes, kernel_size=1, bias=False)
        self.expand_bn = nn.BatchNorm2d(output_planes)
        self.activation = activation(inplace=True)
        self.dropout_ratio = dropout_ratio
        if self.downsample:
            self.skip_conv = nn.Conv2d(input_planes, output_planes, kernel_size=1, bias=False)
            self.skip_conv_bn = nn.BatchNorm2d(output_planes)
        self.init_weights()

    def init_weights(self):
        for m in self.children():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        residual = x
        out = self.activation(self.squeeze_bn(self.squeeze_conv(x)))
        out = self.activation(self.dw_bn(self.dw_conv(out)))
        out = self.expand_bn(self.expand_conv(out))
        if self.dropout_ratio > 0:
            out = F.dropout(out, p=self.dropout_ratio, training=self.training, inplace=True)
        if self.downsample:
            residual = F.max_pool2d(x, kernel_size=2, stride=2, padding=0)
            residual = self.skip_conv(residual)
            residual = self.skip_conv_bn(residual)
        out += residual
        return self.activation(out)


class RMNetBody(nn.Module):

    def __init__(self, block=RMBlock, blocks_per_stage=(None, 4, 8, 10, 11), trunk_width=(32, 32, 64, 128, 256), bottleneck_width=(None, 8, 16, 32, 64)):
        super().__init__()
        assert len(blocks_per_stage) == len(trunk_width) == len(bottleneck_width)
        self.dim_out = trunk_width[-1]
        stages = [nn.Sequential(OrderedDict([('data_bn', nn.BatchNorm2d(3)), ('conv1', nn.Conv2d(3, trunk_width[0], kernel_size=3, stride=2, padding=1, bias=False)), ('bn1', nn.BatchNorm2d(trunk_width[0])), ('relu1', nn.ReLU(inplace=True))]))]
        for i, (blocks_num, w, wb) in enumerate(zip(blocks_per_stage, trunk_width, bottleneck_width)):
            if i == 0:
                continue
            stage = []
            if i > 1:
                stage.append(block(trunk_width[i - 1], wb, w, downsample=True))
            for _ in range(blocks_num):
                stage.append(block(w, wb, w))
            stages.append(nn.Sequential(*stage))
        self.stages = nn.Sequential(OrderedDict([('stage_{}'.format(i), stage) for i, stage in enumerate(stages)]))
        self.init_weights()

    def init_weights(self):
        m = self.stages[0][0]
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)
        m = self.stages[0][1]
        nn.init.kaiming_normal_(m.weight, mode='fan_out')
        m = self.stages[0][2]
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.stages(x)


class RMNetClassifierCifar(nn.Module):

    def __init__(self, num_classes, pretrained=False, body=RMNetBody, dropout_ratio=0.1):
        super().__init__()
        self.dropout_ratio = dropout_ratio
        self.backbone = body()
        self.extra_conv = nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False)
        self.extra_conv_bn = nn.BatchNorm2d(512)
        self.extra_conv_2 = nn.Conv2d(512, 1024, 3, stride=2, padding=1, bias=False)
        self.extra_conv_2_bn = nn.BatchNorm2d(1024)
        self.fc = nn.Conv2d(1024, num_classes, 1, stride=1, padding=0)

    def forward(self, x):
        x = self.backbone(x)
        x = F.elu(self.extra_conv_bn(self.extra_conv(x)))
        x = F.relu(self.extra_conv_2_bn(self.extra_conv_2(x)))
        x = F.dropout(x, p=self.dropout_ratio, training=self.training)
        x = self.fc(x)
        x = x.view(-1, x.size(1))
        return x


VGG_CONFIGS = {'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']}


class VGG(nn.Module):

    def __init__(self, vgg_name):
        super().__init__()
        self.features = self._make_layers(VGG_CONFIGS[vgg_name])
        self.classifier = nn.Linear(512, 10)

    def forward(self, x):
        out = self.features(x)
        out = out.view(out.size(0), -1)
        out = self.classifier(out)
        return out

    def _make_layers(self, config):
        layers = []
        in_channels = 3
        for x in config:
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]
                in_channels = x
        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]
        return nn.Sequential(*layers)


class InitialBlock(nn.Module):
    """The initial block is composed of two branches:
    1. a main branch which performs a regular convolution with stride 2;
    2. an extension branch which performs max-pooling.

    Doing both operations in parallel and concatenating their results
    allows for efficient downsampling and expansion. The main branch
    outputs 13 feature maps while the extension branch outputs 3, for a
    total of 16 feature maps after concatenation.

    Keyword arguments:
    - in_channels (int): the number of input channels.
    - out_channels (int): the number output channels.
    - kernel_size (int, optional): the kernel size of the filters used in
    the convolution layer. Default: 3.
    - padding (int, optional): zero-padding added to both sides of the
    input. Default: 0.
    - bias (bool, optional): Adds a learnable bias to the output if
    ``True``. Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.

    """

    def __init__(self, in_channels, out_channels, kernel_size=3, padding=0, bias=False, relu=True):
        super().__init__()
        if relu:
            activation = nn.ReLU()
        else:
            activation = nn.PReLU()
        self.main_branch = nn.Conv2d(in_channels, out_channels - 3, kernel_size=kernel_size, stride=2, padding=padding, bias=bias)
        self.ext_branch = nn.MaxPool2d(kernel_size, stride=2, padding=padding)
        self.batch_norm = nn.BatchNorm2d(out_channels)
        self.out_prelu = activation

    def forward(self, x):
        main = self.main_branch(x)
        ext = self.ext_branch(x)
        out = torch.cat((main, ext), 1)
        out = self.batch_norm(out)
        return self.out_prelu(out)


class RegularBottleneck(nn.Module):
    """Regular bottlenecks are the main building block of ENet.
    Main branch:
    1. Shortcut connection.

    Extension branch:
    1. 1x1 convolution which decreases the number of channels by
    ``internal_ratio``, also called a projection;
    2. regular, dilated or asymmetric convolution;
    3. 1x1 convolution which increases the number of channels back to
    ``channels``, also called an expansion;
    4. dropout as a regularizer.

    Keyword arguments:
    - channels (int): the number of input and output channels.
    - internal_ratio (int, optional): a scale factor applied to
    ``channels`` used to compute the number of
    channels after the projection. eg. given ``channels`` equal to 128 and
    internal_ratio equal to 2 the number of channels after the projection
    is 64. Default: 4.
    - kernel_size (int, optional): the kernel size of the filters used in
    the convolution layer described above in item 2 of the extension
    branch. Default: 3.
    - padding (int, optional): zero-padding added to both sides of the
    input. Default: 0.
    - dilation (int, optional): spacing between kernel elements for the
    convolution described in item 2 of the extension branch. Default: 1.
    asymmetric (bool, optional): flags if the convolution described in
    item 2 of the extension branch is asymmetric or not. Default: False.
    - dropout_prob (float, optional): probability of an element to be
    zeroed. Default: 0 (no dropout).
    - bias (bool, optional): Adds a learnable bias to the output if
    ``True``. Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.

    """

    def __init__(self, channels, internal_ratio=4, kernel_size=3, padding=0, dilation=1, asymmetric=False, dropout_prob=0, bias=False, relu=True):
        super().__init__()
        if internal_ratio <= 1 or internal_ratio > channels:
            raise RuntimeError('Value out of range. Expected value in the interval [1, {0}], got internal_scale={1}.'.format(channels, internal_ratio))
        internal_channels = channels // internal_ratio
        if relu:
            activation = nn.ReLU()
        else:
            activation = nn.PReLU()
        self.ext_conv1 = nn.Sequential(nn.Conv2d(channels, internal_channels, kernel_size=1, stride=1, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        if asymmetric:
            self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, internal_channels, kernel_size=(kernel_size, 1), stride=1, padding=(padding, 0), dilation=dilation, bias=bias), nn.BatchNorm2d(internal_channels), activation, nn.Conv2d(internal_channels, internal_channels, kernel_size=(1, kernel_size), stride=1, padding=(0, padding), dilation=dilation, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        else:
            self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        self.ext_conv3 = nn.Sequential(nn.Conv2d(internal_channels, channels, kernel_size=1, stride=1, bias=bias), nn.BatchNorm2d(channels), activation)
        self.ext_regul = nn.Dropout2d(p=dropout_prob)
        self.out_prelu = activation

    def forward(self, x):
        main = x
        ext = self.ext_conv1(x)
        ext = self.ext_conv2(ext)
        ext = self.ext_conv3(ext)
        ext = self.ext_regul(ext)
        out = main + ext
        return self.out_prelu(out)


class DownsamplingBottleneck(nn.Module):
    """Downsampling bottlenecks further downsample the feature map size.

    Main branch:
    1. max pooling with stride 2; indices are saved to be used for
    unpooling later.

    Extension branch:
    1. 2x2 convolution with stride 2 that decreases the number of channels
    by ``internal_ratio``, also called a projection;
    2. regular convolution (by default, 3x3);
    3. 1x1 convolution which increases the number of channels to
    ``out_channels``, also called an expansion;
    4. dropout as a regularizer.

    Keyword arguments:
    - in_channels (int): the number of input channels.
    - out_channels (int): the number of output channels.
    - internal_ratio (int, optional): a scale factor applied to ``channels``
    used to compute the number of channels after the projection. eg. given
    ``channels`` equal to 128 and internal_ratio equal to 2 the number of
    channels after the projection is 64. Default: 4.
    - kernel_size (int, optional): the kernel size of the filters used in
    the convolution layer described above in item 2 of the extension branch.
    Default: 3.
    - padding (int, optional): zero-padding added to both sides of the
    input. Default: 0.
    - dilation (int, optional): spacing between kernel elements for the
    convolution described in item 2 of the extension branch. Default: 1.
    - asymmetric (bool, optional): flags if the convolution described in
    item 2 of the extension branch is asymmetric or not. Default: False.
    - return_indices (bool, optional):  if ``True``, will return the max
    indices along with the outputs. Useful when unpooling later.
    - dropout_prob (float, optional): probability of an element to be
    zeroed. Default: 0 (no dropout).
    - bias (bool, optional): Adds a learnable bias to the output if
    ``True``. Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.

    """

    def __init__(self, in_channels, out_channels, internal_ratio=4, kernel_size=3, padding=0, return_indices=False, dropout_prob=0, bias=False, relu=True):
        super().__init__()
        self.return_indices = return_indices
        if internal_ratio <= 1 or internal_ratio > in_channels:
            raise RuntimeError('Value out of range. Expected value in the interval [1, {0}], got internal_scale={1}. '.format(in_channels, internal_ratio))
        internal_channels = in_channels // internal_ratio
        if relu:
            activation = nn.ReLU()
        else:
            activation = nn.PReLU()
        self.main_max1 = nn.MaxPool2d(kernel_size, stride=2, padding=padding, return_indices=return_indices)
        self.ext_conv1 = nn.Sequential(nn.Conv2d(in_channels, internal_channels, kernel_size=2, stride=2, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        self.ext_conv3 = nn.Sequential(nn.Conv2d(internal_channels, out_channels, kernel_size=1, stride=1, bias=bias), nn.BatchNorm2d(out_channels), activation)
        self.ext_regul = nn.Dropout2d(p=dropout_prob)
        self.out_prelu = activation

    def forward(self, x):
        if self.return_indices:
            main, max_indices = self.main_max1(x)
        else:
            main = self.main_max1(x)
        ext = self.ext_conv1(x)
        ext = self.ext_conv2(ext)
        ext = self.ext_conv3(ext)
        ext = self.ext_regul(ext)
        n, ch_ext, h, w = ext.size()
        ch_main = main.size()[1]
        padding = torch.zeros(n, ch_ext - ch_main, h, w)
        if main.is_cuda:
            padding = padding
        main = torch.cat((main, padding), 1)
        out = main + ext
        return self.out_prelu(out), max_indices


class UpsamplingBottleneck(nn.Module):
    """The upsampling bottlenecks upsample the feature map resolution using max
    pooling indices stored from the corresponding downsampling bottleneck.

    Main branch:
    1. 1x1 convolution with stride 1 that decreases the number of channels by
    ``internal_ratio``, also called a projection;
    2. max unpool layer using the max pool indices from the corresponding
    downsampling max pool layer.

    Extension branch:
    1. 1x1 convolution with stride 1 that decreases the number of channels by
    ``internal_ratio``, also called a projection;
    2. transposed convolution (by default, 3x3);
    3. 1x1 convolution which increases the number of channels to
    ``out_channels``, also called an expansion;
    4. dropout as a regularizer.

    Keyword arguments:
    - in_channels (int): the number of input channels.
    - out_channels (int): the number of output channels.
    - internal_ratio (int, optional): a scale factor applied to ``in_channels``
     used to compute the number of channels after the projection. eg. given
     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number
     of channels after the projection is 64. Default: 4.
    - kernel_size (int, optional): the kernel size of the filters used in the
    convolution layer described above in item 2 of the extension branch.
    Default: 3.
    - padding (int, optional): zero-padding added to both sides of the input.
    Default: 0.
    - dropout_prob (float, optional): probability of an element to be zeroed.
    Default: 0 (no dropout).
    - bias (bool, optional): Adds a learnable bias to the output if ``True``.
    Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.

    """

    def __init__(self, in_channels, out_channels, internal_ratio=4, kernel_size=3, padding=0, dropout_prob=0, bias=False, relu=True):
        super().__init__()
        if internal_ratio <= 1 or internal_ratio > in_channels:
            raise RuntimeError('Value out of range. Expected value in the interval [1, {0}], got internal_scale={1}. '.format(in_channels, internal_ratio))
        internal_channels = in_channels // internal_ratio
        if relu:
            activation = nn.ReLU()
        else:
            activation = nn.PReLU()
        self.main_conv1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(out_channels))
        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)
        self.ext_conv1 = nn.Sequential(nn.Conv2d(in_channels, internal_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        self.ext_conv2 = nn.Sequential(nn.ConvTranspose2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=2, padding=padding, output_padding=1, bias=bias), nn.BatchNorm2d(internal_channels), activation)
        self.ext_conv3 = nn.Sequential(nn.Conv2d(internal_channels, out_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(out_channels), activation)
        self.ext_regul = nn.Dropout2d(p=dropout_prob)
        self.out_prelu = activation

    def forward(self, x, max_indices):
        main = self.main_conv1(x)
        main = self.main_unpool1(main, max_indices)
        ext = self.ext_conv1(x)
        ext = self.ext_conv2(ext)
        ext = self.ext_conv3(ext)
        ext = self.ext_regul(ext)
        out = main + ext
        return self.out_prelu(out)


class ENet(nn.Module):
    """Generate the ENet model.

    Keyword arguments:
    - num_classes (int): the number of classes to segment.
    - encoder_relu (bool, optional): When ``True`` ReLU is used as the
    activation function in the encoder blocks/layers; otherwise, PReLU
    is used. Default: False.
    - decoder_relu (bool, optional): When ``True`` ReLU is used as the
    activation function in the decoder blocks/layers; otherwise, PReLU
    is used. Default: True.

    """

    def __init__(self, num_classes, encoder_relu=False, decoder_relu=True):
        super().__init__()
        self.initial_block = InitialBlock(3, 16, padding=1, relu=encoder_relu)
        self.downsample1_0 = DownsamplingBottleneck(16, 64, padding=1, return_indices=True, dropout_prob=0.01, relu=encoder_relu)
        self.regular1_1 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)
        self.regular1_2 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)
        self.regular1_3 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)
        self.regular1_4 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)
        self.downsample2_0 = DownsamplingBottleneck(64, 128, padding=1, return_indices=True, dropout_prob=0.1, relu=encoder_relu)
        self.regular2_1 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)
        self.dilated2_2 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)
        self.asymmetric2_3 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)
        self.dilated2_4 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)
        self.regular2_5 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)
        self.dilated2_6 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)
        self.asymmetric2_7 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)
        self.dilated2_8 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)
        self.regular3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)
        self.dilated3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)
        self.asymmetric3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)
        self.dilated3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)
        self.regular3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)
        self.dilated3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)
        self.asymmetric3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)
        self.dilated3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)
        self.upsample4_0 = UpsamplingBottleneck(128, 64, padding=1, dropout_prob=0.1, relu=decoder_relu)
        self.regular4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)
        self.regular4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)
        self.upsample5_0 = UpsamplingBottleneck(64, 16, padding=1, dropout_prob=0.1, relu=decoder_relu)
        self.regular5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)
        self.transposed_conv = nn.ConvTranspose2d(16, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)

    def forward(self, x):
        x = self.initial_block(x)
        x, max_indices1_0 = self.downsample1_0(x)
        x = self.regular1_1(x)
        x = self.regular1_2(x)
        x = self.regular1_3(x)
        x = self.regular1_4(x)
        x, max_indices2_0 = self.downsample2_0(x)
        x = self.regular2_1(x)
        x = self.dilated2_2(x)
        x = self.asymmetric2_3(x)
        x = self.dilated2_4(x)
        x = self.regular2_5(x)
        x = self.dilated2_6(x)
        x = self.asymmetric2_7(x)
        x = self.dilated2_8(x)
        x = self.regular3_0(x)
        x = self.dilated3_1(x)
        x = self.asymmetric3_2(x)
        x = self.dilated3_3(x)
        x = self.regular3_4(x)
        x = self.dilated3_5(x)
        x = self.asymmetric3_6(x)
        x = self.dilated3_7(x)
        x = self.upsample4_0(x, max_indices2_0)
        x = self.regular4_1(x)
        x = self.regular4_2(x)
        x = self.upsample5_0(x, max_indices1_0)
        x = self.regular5_1(x)
        x = self.transposed_conv(x)
        return x


class ResNetBlock(nn.Module):

    def __init__(self, in_channels, reduce_channels, increase_channels, dilation=1, stride=1):
        super().__init__()
        nonshrinking_padding = dilation
        self.conv_1x1_reduce_bnrelu = ConvBNReLU(in_channels, out_channels=reduce_channels, kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)
        self.conv_3x3_bnrelu = ConvBNReLU(in_channels=reduce_channels, out_channels=reduce_channels, kernel_size=3, stride=1, padding=nonshrinking_padding, dilation=dilation, bias=False)
        self.conv_1x1_increase_bn = ConvBN(in_channels=reduce_channels, out_channels=increase_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)
        self.need_proj = in_channels != increase_channels
        if self.need_proj:
            self.conv_1x1_proj_bn = ConvBN(in_channels, out_channels=increase_channels, kernel_size=1, stride=stride, padding=0, bias=False)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, inputs):
        fx = self.conv_1x1_reduce_bnrelu(inputs)
        fx = self.conv_3x3_bnrelu(fx)
        fx = self.conv_1x1_increase_bn(fx)
        x = inputs
        if self.need_proj:
            x = self.conv_1x1_proj_bn(x)
        out = fx + x
        out = self.relu(out)
        return out


class ICNetBackbone(nn.Module):

    def __init__(self, in_channels):
        super().__init__()
        self.conv1 = nn.Sequential(OrderedDict([('conv1_1_3x3_s2', ConvBNReLU(in_channels, out_channels=32, kernel_size=3, stride=2, padding=1, dilation=1, bias=False)), ('conv1_2_3x3', ConvBNReLU(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)), ('conv1_3_3x3', ConvBNReLU(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, bias=False))]))
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Sequential(OrderedDict([('conv2_1', ResNetBlock(64, 32, 128)), ('conv2_2', ResNetBlock(128, 32, 128)), ('conv2_3', ResNetBlock(128, 32, 128))]))
        self.conv3_1 = ResNetBlock(128, 64, 256, stride=2)
        self.conv3_rest = nn.Sequential(OrderedDict([('conv3_2', ResNetBlock(256, 64, 256)), ('conv3_3', ResNetBlock(256, 64, 256)), ('conv3_4', ResNetBlock(256, 64, 256))]))
        self.conv4 = nn.Sequential(OrderedDict([('conv4_1', ResNetBlock(256, 128, 512, dilation=2)), ('conv4_2', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_3', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_4', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_5', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_6', ResNetBlock(512, 128, 512, dilation=2))]))
        self.conv4 = nn.Sequential(OrderedDict([('conv4_1', ResNetBlock(256, 128, 512, dilation=2)), ('conv4_2', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_3', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_4', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_5', ResNetBlock(512, 128, 512, dilation=2)), ('conv4_6', ResNetBlock(512, 128, 512, dilation=2))]))
        self.conv5 = nn.Sequential(OrderedDict([('conv5_1', ResNetBlock(512, 256, 1024, dilation=4)), ('conv5_2', ResNetBlock(1024, 256, 1024, dilation=4)), ('conv5_3', ResNetBlock(1024, 256, 1024, dilation=4))]))

    def forward(self):
        pass


class PyramidPooling(nn.Module):

    def __init__(self, input_size_hw, bin_dimensions=None, mode='sum'):
        super().__init__()
        if mode not in ['sum', 'cat']:
            raise NotImplementedError
        self.mode = mode
        self.input_size_hw = input_size_hw
        self.sampling_params = {'mode': 'nearest'}
        if bin_dimensions is None:
            self.bin_dimensions = [1, 2, 3, 6]
        else:
            self.bin_dimensions = bin_dimensions
        self.paddings = {}
        for dim in self.bin_dimensions:
            pad_h = (dim - input_size_hw[0] % dim) % dim
            pad_w = (dim - input_size_hw[1] % dim) % dim
            self.paddings[dim] = 0, pad_w, 0, pad_h

    def forward(self, inputs):
        x = inputs.clone()
        for dim in self.bin_dimensions:
            pooled_feature = F.adaptive_avg_pool2d(inputs, dim)
            pooled_feature = F.interpolate(pooled_feature, self.input_size_hw, **self.sampling_params)
            if self.mode == 'sum':
                x += pooled_feature
            elif self.mode == 'cat':
                x = torch.cat(pooled_feature)
            else:
                raise NotImplementedError
        return x


class CascadeFeatureFusion(nn.Module):

    def __init__(self, in_channels_lowres, in_channels_highres, highres_size_hw, num_classes):
        super().__init__()
        self.sampling_params = {'mode': 'nearest'}
        self.conv = ConvBN(in_channels_lowres, out_channels=128, kernel_size=3, padding=2, dilation=2, bias=False)
        self.conv_proj = ConvBN(in_channels_highres, out_channels=128, kernel_size=1, padding=0, dilation=1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.classifier = nn.Conv2d(in_channels_lowres, out_channels=num_classes, kernel_size=1, padding=0, dilation=1, bias=True)
        self.highres_size_hw = highres_size_hw

    def forward(self, lowres_input, highres_input):
        upsampled = F.interpolate(lowres_input, self.highres_size_hw, **self.sampling_params)
        lr = self.conv(upsampled)
        hr = self.conv_proj(highres_input)
        x = lr + hr
        x = self.relu(x)
        if self.training:
            aux_labels = self.classifier(upsampled)
            return x, aux_labels
        return x


def get_backbone(backbone, in_channels):
    if backbone == 'icnet':
        return ICNetBackbone(in_channels)
    raise NotImplementedError


def is_tracing_state():
    return torch._C._get_tracing_state() is not None


class ICNet(nn.Module):

    def __init__(self, input_size_hw, in_channels=3, n_classes=20, backbone='icnet'):
        super().__init__()
        self._input_size_hw = input_size_hw
        self._input_size_hw_ds2 = self._input_size_hw[0] // 2, self._input_size_hw[1] // 2
        self._input_size_hw_ds4 = self._input_size_hw[0] // 4, self._input_size_hw[1] // 4
        self._input_size_hw_ds8 = self._input_size_hw[0] // 8, self._input_size_hw[1] // 8
        self._input_size_hw_ds16 = self._input_size_hw[0] // 16, self._input_size_hw[1] // 16
        self._input_size_hw_ds32 = self._input_size_hw[0] // 32, self._input_size_hw[1] // 32
        self.sampling_params = {'mode': 'nearest'}
        self.backbone = get_backbone(backbone, in_channels)
        self.highres_conv = nn.Sequential(OrderedDict([('conv1_sub1', ConvBNReLU(in_channels, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False)), ('conv2_sub1', ConvBNReLU(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False)), ('conv3_sub1', ConvBNReLU(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False))]))
        self.conv5_4_k1 = ConvBNReLU(in_channels=1024, out_channels=256, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)
        self.ppm = PyramidPooling(self._input_size_hw_ds32)
        self.cff42 = CascadeFeatureFusion(in_channels_lowres=256, in_channels_highres=256, highres_size_hw=self._input_size_hw_ds16, num_classes=n_classes)
        self.cff421 = CascadeFeatureFusion(in_channels_lowres=128, in_channels_highres=32, highres_size_hw=self._input_size_hw_ds8, num_classes=n_classes)
        self.conv6_cls = nn.Conv2d(128, out_channels=n_classes, kernel_size=1, padding=0, dilation=1, bias=True)
        required_alignment = 32
        for bin_dim in self.ppm.bin_dimensions:
            required_alignment = lcm(required_alignment, bin_dim)
        if input_size_hw[0] % required_alignment or input_size_hw[1] % required_alignment:
            raise ValueError('ICNet may only operate on {}-aligned input resolutions'.format(required_alignment))

    def highres_branch(self, inputs):
        x = self.highres_conv(inputs)
        return x

    def mediumres_branch(self, inputs):
        x = self.backbone.conv1(inputs)
        x = self.backbone.maxpool(x)
        x = self.backbone.conv2(x)
        x = self.backbone.conv3_1(x)
        return x

    def lowres_branch(self, inputs):
        x = self.backbone.conv3_rest(inputs)
        x = self.backbone.conv4(x)
        x = self.backbone.conv5(x)
        x = self.ppm(x)
        x = self.conv5_4_k1(x)
        return x

    def forward(self, inputs):
        data_sub1 = inputs
        features_sub1 = self.highres_branch(data_sub1)
        data_sub2 = F.interpolate(data_sub1, self._input_size_hw_ds2, **self.sampling_params)
        features_sub2 = self.mediumres_branch(data_sub2)
        data_sub4 = F.interpolate(features_sub2, self._input_size_hw_ds32, **self.sampling_params)
        features_sub4 = self.lowres_branch(data_sub4)
        if self.training:
            fused_features_sub42, label_scores_ds16 = self.cff42(features_sub4, features_sub2)
            fused_features_sub421, label_scores_ds8 = self.cff421(fused_features_sub42, features_sub1)
            fused_features_ds4 = F.interpolate(fused_features_sub421, self._input_size_hw_ds4, **self.sampling_params)
            label_scores_ds4 = self.conv6_cls(fused_features_ds4)
            return OrderedDict([('ds4', label_scores_ds4), ('ds8', label_scores_ds8), ('ds16', label_scores_ds16)])
        fused_features_sub42 = self.cff42(features_sub4, features_sub2)
        fused_features_sub421 = self.cff421(fused_features_sub42, features_sub1)
        fused_features_ds4 = F.interpolate(fused_features_sub421, self._input_size_hw_ds4, **self.sampling_params)
        label_scores_ds4 = self.conv6_cls(fused_features_ds4)
        label_scores = F.interpolate(label_scores_ds4, self._input_size_hw, **self.sampling_params)
        if is_tracing_state() and parse_version(torch.__version__) >= parse_version('1.1.0'):
            softmaxed = F.softmax(label_scores, dim=1)
            return softmaxed
        return label_scores


class UNetConvBlock(nn.Module):

    def __init__(self, in_size, out_size, padding, batch_norm):
        super().__init__()
        block = []
        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))
        if batch_norm:
            block.append(nn.BatchNorm2d(out_size))
        block.append(nn.ReLU())
        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))
        if batch_norm:
            block.append(nn.BatchNorm2d(out_size))
        block.append(nn.ReLU())
        self.block = nn.Sequential(*block)

    def forward(self, x):
        out = self.block(x)
        return out


def center_crop(layer, target_size):
    if layer.dim() == 4:
        _, _, layer_height, layer_width = layer.size()
        diff_y = (layer_height - target_size[0]) // 2
        diff_x = (layer_width - target_size[1]) // 2
        return layer[:, :, diff_y:diff_y + target_size[0], diff_x:diff_x + target_size[1]]
    assert layer.dim() == 3
    _, layer_height, layer_width = layer.size()
    diff_y = (layer_height - target_size[0]) // 2
    diff_x = (layer_width - target_size[1]) // 2
    return layer[:, diff_y:diff_y + target_size[0], diff_x:diff_x + target_size[1]]


class UNetUpBlock(nn.Module):

    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):
        super().__init__()
        if up_mode == 'upconv':
            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)
        elif up_mode == 'upsample':
            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2), nn.Conv2d(in_size, out_size, kernel_size=1))
        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)

    def forward(self, x, bridge):
        up = self.up(x)
        crop1 = center_crop(bridge, up.shape[2:])
        out = torch.cat([up, crop1], 1)
        out = self.conv_block(out)
        return out


class UNet(nn.Module):

    def __init__(self, in_channels=3, n_classes=12, depth=5, wf=6, padding=False, batch_norm=True, up_mode='upconv'):
        """
        Implementation of
        U-Net: Convolutional Networks for Biomedical Image Segmentation
        (Ronneberger et al., 2015)
        https://arxiv.org/abs/1505.04597
        Using the default arguments will yield the exact version used
        in the original paper
        Args:
            in_channels (int): number of input channels
            n_classes (int): number of output channels
            depth (int): depth of the network
            wf (int): number of filters in the first layer is 2**wf
            padding (bool): if True, apply padding such that the input shape
                            is the same as the output.
                            This may introduce artifacts
            batch_norm (bool): Use BatchNorm prior to layers with an
                               activation function
            up_mode (str): one of 'upconv' or 'upsample'.
                           'upconv' will use transposed convolutions for
                           learned upsampling.
                           'upsample' will use bilinear upsampling.
        """
        super().__init__()
        assert up_mode in ('upconv', 'upsample')
        self.padding = padding
        self.depth = depth
        prev_channels = in_channels
        self.down_path = nn.ModuleList()
        for i in range(depth):
            self.down_path.append(UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm))
            prev_channels = 2 ** (wf + i)
        self.up_path = nn.ModuleList()
        for i in reversed(range(depth - 1)):
            self.up_path.append(UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm))
            prev_channels = 2 ** (wf + i)
        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)

    def forward(self, x):
        blocks = []
        for i, down in enumerate(self.down_path):
            x = down(x)
            if i != len(self.down_path) - 1:
                blocks.append(x)
                x = F.max_pool2d(x, 2)
        for i, up in enumerate(self.up_path):
            x = up(x, blocks[-i - 1])
        x = self.last(x)
        return x


def add_domain(name_operator: str) ->str:
    return DOMAIN_CUSTOM_OPS_NAME + '::' + name_operator


def decode(loc, priors):
    """Decode locations from predictions using priors to undo
    the encoding we did for offset regression at train time.
    Args:
        loc (tensor): location predictions for loc layers,
            Shape: [num_priors,4]
        priors (tensor): Prior boxes in point-form with variances.
            Shape: [2, num_priors,4].
    Return:
        decoded bounding box predictions
    """
    variances = priors[1].squeeze(0)
    priors = priors[0].squeeze(0)
    decoded_boxes_cx_cy = variances[:, :2] * loc[:, :2] * (priors[:, 2:] - priors[:, :2]) + (priors[:, :2] + priors[:, 2:]) / 2
    decoded_boxes_w_h = torch.exp(variances[:, 2:] * loc[:, 2:]) * (priors[:, 2:] - priors[:, :2])
    decoded_boxes_xmin_ymin = decoded_boxes_cx_cy - decoded_boxes_w_h / 2
    decoded_boxes_xmax_ymax = decoded_boxes_cx_cy + decoded_boxes_w_h / 2
    encoded_boxes = torch.cat((decoded_boxes_xmin_ymin, decoded_boxes_xmax_ymax), 1)
    return encoded_boxes


class Registry:
    REGISTERED_NAME_ATTR = '_registered_name'

    def __init__(self, name, add_name_as_attr=False):
        self._name = name
        self._registry_dict = {}
        self._add_name_as_attr = add_name_as_attr

    @property
    def registry_dict(self):
        return self._registry_dict

    def values(self):
        return self._registry_dict.values()

    def _register(self, obj, name):
        if name in self._registry_dict:
            raise KeyError('{} is already registered in {}'.format(name, self._name))
        self._registry_dict[name] = obj

    def register(self, name=None):

        def wrap(obj):
            cls_name = name
            if cls_name is None:
                cls_name = obj.__name__
            if self._add_name_as_attr:
                setattr(obj, self.REGISTERED_NAME_ATTR, name)
            self._register(obj, cls_name)
            return obj
        return wrap

    def get(self, name):
        if name not in self._registry_dict:
            self._key_not_found(name)
        return self._registry_dict[name]

    def _key_not_found(self, name):
        raise KeyError('{} is unknown type of {} '.format(name, self._name))

    def __contains__(self, item):
        return item in self._registry_dict.values()


EXTENSIONS = Registry('extensions')


class NMSFunction(torch.autograd.Function):

    @staticmethod
    def forward(ctx, boxes, scores, threshold, top_k=200):
        if scores.size(0) == 0:
            return torch.tensor([], dtype=torch.int), torch.tensor(0)
        if scores.dim() == 1:
            scores = scores.unsqueeze(1)
        if not boxes.is_cuda:
            keep = torch_nms(boxes, scores.flatten(), threshold)
        else:
            keep = EXTENSIONS.nms(torch.cat((boxes, scores), dim=1), threshold, top_k)
        return keep, torch.tensor(keep.size(0))

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        raise NotImplementedError


nms = NMSFunction.apply


class no_jit_trace:

    def __enter__(self):
        self.state = torch._C._get_tracing_state()
        torch._C._set_tracing_state(None)

    def __exit__(self, *args):
        torch._C._set_tracing_state(self.state)
        self.state = None


class BaseLayerAttributes(ABC):
    """
    This class stores base useful for some algorithms attributes
    of modules/layers.
    """


class CopySafeThreadingVars:
    """ A class holding variables that are related to threading and
    thus impossible to deepcopy. The deepcopy will simply return a
    new object without copying, but won't fail."""

    def __init__(self):
        self.thread_local = threading.local()
        self.cond = threading.Condition()

    def __deepcopy__(self, memo):
        return CopySafeThreadingVars()


class Dtype(Enum):
    FLOAT = 'float'
    INTEGER = 'int'


class DynamicGraphEdge:

    def __init__(self, from_node_id: int, to_node_id: int, activation_shape: List[int], input_port_id: int, output_port_id: int, dtype: Dtype):
        self.from_node_id = from_node_id
        self.to_node_id = to_node_id
        self.activation_shape = activation_shape
        self.input_port_id = input_port_id
        self.output_port_id = output_port_id
        self.dtype = dtype


class TensorMeta:

    @staticmethod
    def default_comparator(lhs: 'TensorMeta', rhs: 'TensorMeta'):
        return lhs.index == rhs.index and lhs.creator_id == rhs.creator_id and lhs.shape[1:] == rhs.shape[1:]

    def __init__(self, creator_id: int, index: int, shape: Union[List[int], Tuple[torch.Tensor, ...]], dtype: Dtype=Dtype.FLOAT):
        """
        :param creator_id: An ID of the node in DynamicGraph that corresponds to an operation that created the
            tensor.
        :param index: The index of this tensor in the creator operation's output.
        :param shape: The shape of the tensor.
        """
        self.creator_id = creator_id
        self.index = index
        self.shape = tuple(int(dim) for dim in shape)
        self.dtype = dtype

    def __eq__(self, other):
        if not isinstance(other, TensorMeta):
            return False
        return self.default_comparator(self, other)

    def __hash__(self):
        return hash((self.creator_id, self.index, self.shape))

    def __str__(self):
        return 'C{}_I{}_'.format(self.creator_id, self.index) + 'S' + 'x'.join([str(s) for s in self.shape])


class TensorMetaComparator:

    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) ->bool:
        raise NotImplementedError


class InputsMatcher:

    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta], tm_comparators: List[TensorMetaComparator]) ->bool:
        raise NotImplementedError


NNCF_LOGGER_NAME = 'nncf'


nncf_logger = logging.getLogger(NNCF_LOGGER_NAME)


class DefaultInputsMatcher(InputsMatcher):

    def __call__(self, saved_inputs: List[TensorMeta], actual_inputs: List[TensorMeta], tm_comparators: List[TensorMetaComparator]) ->bool:
        if saved_inputs is None and actual_inputs:
            return False
        matched_with_unexpected_tensors = False
        for saved_input, actual_input in zip(saved_inputs, actual_inputs):
            if saved_input is None and actual_input is None:
                continue
            if saved_input is None and actual_input is not None:
                matched_with_unexpected_tensors = True
                continue
            if saved_input is not None and actual_input is None:
                return False
            for tm_comparator in tm_comparators:
                if not tm_comparator(saved_input, actual_input):
                    return False
        if matched_with_unexpected_tensors:
            nncf_logger.debug(f'Had to match a node to an op which has tensors at positions where there were no tensors at graph building time:\nNode input metas: {saved_inputs}, but op input metas: {actual_inputs}')
        return True


class DefaultTensorMetaComparator(TensorMetaComparator):

    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) ->bool:
        return TensorMeta.default_comparator(lhs, rhs)


class ScopeElement:

    def __init__(self, calling_module_class_name: str, calling_field_name: str=None):
        self.calling_module_class_name = calling_module_class_name
        self.calling_field_name = calling_field_name

    def __str__(self):
        if self.calling_field_name is None:
            return self.calling_module_class_name
        return '{cls}[{name}]'.format(cls=self.calling_module_class_name, name=self.calling_field_name)

    def __eq__(self, other: 'ScopeElement'):
        return self.calling_module_class_name == other.calling_module_class_name and self.calling_field_name == other.calling_field_name

    def __hash__(self):
        return hash((self.calling_module_class_name, self.calling_field_name))

    @staticmethod
    def from_str(string: str):
        matches = re.search('(.*)\\[(.*)\\]|(.*)', string)
        if matches is None:
            raise RuntimeError('Invalid scope element string')
        if matches.groups()[0] is None and matches.groups()[1] is None:
            return ScopeElement(matches.groups()[2])
        if matches.groups()[0] is not None and matches.groups()[1] is not None:
            return ScopeElement(matches.groups()[0], matches.groups()[1])
        raise RuntimeError('Could not parse the scope element string')


class Scope:

    def __init__(self, scope_elements: List[ScopeElement]=None):
        if scope_elements is not None:
            self.scope_elements = scope_elements
        else:
            self.scope_elements = []

    def __str__(self):
        return '/'.join([str(scope_el) for scope_el in self.scope_elements])

    def __hash__(self):
        return hash(str(self))

    def __eq__(self, other: 'Scope'):
        return self.scope_elements == other.scope_elements

    def __getitem__(self, key):
        return self.scope_elements[key]

    def __contains__(self, item: 'Scope'):
        """Idiom: ('A/B/C' in 'A/B') == True"""
        if len(self.scope_elements) > len(item.scope_elements):
            return False
        for i, element in enumerate(self.scope_elements):
            if element != item.scope_elements[i]:
                return False
        return True

    def __add__(self, rhs):
        init_list = self.scope_elements + rhs.scope_elements
        return Scope(init_list)

    def copy(self):
        return Scope(deepcopy(self.scope_elements))

    def push(self, scope_element: ScopeElement):
        self.scope_elements.append(scope_element)

    def pop(self) ->ScopeElement:
        return self.scope_elements.pop()

    @staticmethod
    def from_str(string: str) ->'Scope':
        if string:
            elts = string.split('/')
        else:
            elts = []
        return Scope([ScopeElement.from_str(s) for s in elts])

    def get_iteration_scopes(self) ->List[str]:
        results = []
        scope_name = str(self)
        for iter_scope in ITERATION_MODULES.registry_dict:
            if iter_scope in scope_name:
                results.append(iter_scope)
        return results


class OperationAddress:

    def __init__(self, operator_name: str, scope_in_model: Scope, call_order: int):
        self.operator_name = operator_name
        self.scope_in_model = scope_in_model
        self.call_order = call_order

    def __eq__(self, other: 'OperationAddress'):
        return isinstance(other, OperationAddress) and self.operator_name == other.operator_name and self.scope_in_model == other.scope_in_model and self.call_order == other.call_order

    def __str__(self):
        return str(self.scope_in_model) + '/' + self.operator_name + '_' + str(self.call_order)

    def __hash__(self):
        return hash((self.operator_name, self.scope_in_model, self.call_order))

    @staticmethod
    def from_str(s: str):
        scope_and_op, _, call_order_str = s.rpartition('_')
        scope_str, _, op_name = scope_and_op.rpartition('/')
        return OperationAddress(op_name, Scope.from_str(scope_str), int(call_order_str))


class OperationExecutionContext:
    """
    Information that allows to uniquely identify an operation inside the NNCF graph,
    i.e. determine whether an execution of the operator inside the module has already been
    registered as a node in the graph or not (in the latter case a new node would have to
    be created
    """

    def __init__(self, operator_name: str, scope_in_model: Scope, call_order: int, tensor_metas: List[TensorMeta], tm_comparators: List[TensorMetaComparator]=None, input_matcher: InputsMatcher=None):
        self.op_address = OperationAddress(operator_name, scope_in_model, call_order)
        self.tensor_metas = tensor_metas
        self.tm_comparators = tm_comparators if tm_comparators else [DefaultTensorMetaComparator()]
        self.input_matcher = input_matcher if input_matcher else DefaultInputsMatcher()

    def __eq__(self, other):
        return self.op_address == other.op_address and Counter(self.tensor_metas) == Counter(other.tensor_metas)

    def matches_saved_inputs_from(self, other: 'OperationExecutionContext'):
        return self.op_address == other.op_address and self.input_matcher(other.tensor_metas, self.tensor_metas, self.tm_comparators)

    def __hash__(self):
        return hash((self.operator_name, tuple(self.scope_in_model), self.call_order, tuple(self.tensor_metas)))

    def __str__(self):
        input_info_str = ''
        for meta in self.tensor_metas:
            if meta is None:
                input_info_str += 'N;'
            else:
                input_info_str += str(meta) + ';'
        return super().__str__() + '(' + input_info_str + ')'

    @property
    def operator_name(self):
        return self.op_address.operator_name

    @property
    def scope_in_model(self) ->Scope:
        return self.op_address.scope_in_model

    @property
    def call_order(self):
        return self.op_address.call_order


class DynamicGraphNode:

    def __init__(self, node_id: int, node_key: str, layer_attributes: BaseLayerAttributes, op_exec_context: OperationExecutionContext, ignored_algorithms: List[str], is_called_inside_nncf_module: bool, is_in_iteration_scope: bool):
        self.node_id = node_id
        self.node_key = node_key
        self.layer_attributes = layer_attributes
        self.op_exec_context = op_exec_context
        self.ignored_algorithms = ignored_algorithms
        self.is_called_inside_nncf_module = is_called_inside_nncf_module
        self.is_in_iteration_scope = is_in_iteration_scope

    @classmethod
    def build_from_nx_node_dict(cls, nx_node_dict: Dict[str, Any]) ->'DynamicGraphNode':
        return cls(node_id=nx_node_dict[DynamicGraph.ID_NODE_ATTR], node_key=nx_node_dict[DynamicGraph.KEY_NODE_ATTR], layer_attributes=nx_node_dict.get(DynamicGraph.LAYER_ATTRIBUTES), op_exec_context=nx_node_dict[DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR], ignored_algorithms=nx_node_dict[DynamicGraph.IGNORED_ALGOS_NODE_ATTR], is_called_inside_nncf_module=nx_node_dict[DynamicGraph.IS_CALLED_INSIDE_NNCF_MODULE], is_in_iteration_scope=nx_node_dict[DynamicGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR])

    def __eq__(self, other: 'DynamicGraphNode') ->bool:
        return self.__dict__ == other.__dict__

    def __str__(self):
        return self.node_key


class DynamicGraphNodeParameters:

    def __init__(self, layer_attributes: BaseLayerAttributes, ignored_algorithms: List[str], is_called_inside_nncf_module: bool):
        self.layer_attributes = layer_attributes
        self.ignored_algorithms = ignored_algorithms
        self.is_called_inside_nncf_module = is_called_inside_nncf_module


class DefaultScopeNodeMatcher:

    def __init__(self, node_id_to_key_dict, nx_graph):
        self._node_id_to_key_dict = node_id_to_key_dict
        self._nx_graph = nx_graph
        self._inputless_nodes = {}

    def get_node_by_id(self, node_id):
        return self._nx_graph.nodes[self._node_id_to_key_dict[node_id]]

    def _find_nodes_with_matching_context_among_inputless(self, op_exec_context: OperationExecutionContext) ->Dict[str, DynamicGraphNode]:
        node_candidates = {}
        for nx_node_key, node in self._inputless_nodes.items():
            if op_exec_context.matches_saved_inputs_from(node.op_exec_context):
                node_candidates[nx_node_key] = node
        return node_candidates

    def _find_nodes_with_matching_context_and_inputs(self, op_exec_context: OperationExecutionContext) ->Dict[str, DynamicGraphNode]:
        nx_node_candidates = {}
        for info in op_exec_context.tensor_metas:
            if info is None or info.creator_id is None:
                continue
            creator_id = info.creator_id
            for successor_node_key in self._nx_graph.successors(self._node_id_to_key_dict[creator_id]):
                successor_node = self._nx_graph.nodes[successor_node_key]
                if op_exec_context.matches_saved_inputs_from(successor_node[DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR]):
                    nx_node_candidates[successor_node_key] = successor_node
        node_candidates = {}
        for nx_node_key, nx_node_dict in nx_node_candidates.items():
            node_candidates[nx_node_key] = DynamicGraphNode.build_from_nx_node_dict(nx_node_dict)
        return node_candidates

    def add_node(self, op_exec_context: OperationExecutionContext, inputs, node_parameters: DynamicGraphNodeParameters, is_in_iteration_scope: bool=False) ->DynamicGraphNode:
        node_id = len(self._node_id_to_key_dict)
        name_parts = str(op_exec_context.scope_in_model), op_exec_context.operator_name
        node_key = '{idx} {uri}'.format(uri='/'.join(name_parts), idx=node_id)
        nncf_logger.debug(f'New node added to NNCF graph: {node_key}')
        self._node_id_to_key_dict[node_id] = node_key
        attrs = {DynamicGraph.ID_NODE_ATTR: node_id, DynamicGraph.KEY_NODE_ATTR: node_key, DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR: op_exec_context, DynamicGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR: is_in_iteration_scope}
        if node_parameters.layer_attributes is not None:
            attrs[DynamicGraph.LAYER_ATTRIBUTES] = node_parameters.layer_attributes
        if node_parameters.ignored_algorithms is not None:
            attrs[DynamicGraph.IGNORED_ALGOS_NODE_ATTR] = node_parameters.ignored_algorithms
        else:
            attrs[DynamicGraph.IGNORED_ALGOS_NODE_ATTR] = []
        attrs[DynamicGraph.IS_CALLED_INSIDE_NNCF_MODULE] = node_parameters.is_called_inside_nncf_module
        self._nx_graph.add_node(node_key, **attrs)
        has_traced_inputs = False
        for i, info in enumerate(op_exec_context.tensor_metas):
            if info is None or info.creator_id is None:
                continue
            parent = self._node_id_to_key_dict[info.creator_id]
            self._nx_graph.add_edge(parent, node_key)
            has_traced_inputs = True
            self._nx_graph.edges[parent, node_key][DynamicGraph.ACTIVATION_SHAPE_EDGE_ATTR] = info.shape
            self._nx_graph.edges[parent, node_key][DynamicGraph.INPUT_PORT_ID_EDGE_ATTR] = i
            self._nx_graph.edges[parent, node_key][DynamicGraph.OUTPUT_PORT_ID_EDGE_ATTR] = info.index
            self._nx_graph.edges[parent, node_key][DynamicGraph.ACTIVATION_DTYPE_EDGE_ATTR] = info.dtype
        nx_node_dict = self._nx_graph.nodes[node_key]
        node = DynamicGraphNode.build_from_nx_node_dict(nx_node_dict)
        if not has_traced_inputs:
            self._inputless_nodes[node_key] = node
        return node

    def find_node(self, op_address: OperationAddress, tensor_metas: List[TensorMeta], tm_comparators: List[TensorMetaComparator]) ->DynamicGraphNode:
        op_exec_context = OperationExecutionContext(op_address.operator_name, op_address.scope_in_model, op_address.call_order, tensor_metas, tm_comparators=tm_comparators)
        node_candidates = self._find_nodes_with_matching_context_and_inputs(op_exec_context)
        if not node_candidates:
            node_candidates = self._find_nodes_with_matching_context_among_inputless(op_exec_context)
        node_candidates = list(node_candidates.values())
        result = None
        if len(node_candidates) == 1:
            result = node_candidates[0]
        if len(node_candidates) > 1:
            nncf_logger.debug(f'More than one node was matched against context {op_exec_context}')
            result = node_candidates[0]
        return result


class FirstInputsMatcher(InputsMatcher):

    def __call__(self, node_inputs: List[TensorMeta], real_inputs: List[TensorMeta], tm_comparators: List[TensorMetaComparator]) ->bool:
        if not node_inputs or not real_inputs:
            return False
        if not node_inputs[0] or not real_inputs[0]:
            return False
        for tm_comparator in tm_comparators:
            if not tm_comparator(node_inputs[0], real_inputs[0]):
                return False
        return True


class ShapeOnlyTensorMetaComparator(TensorMetaComparator):

    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) ->bool:
        return lhs.shape[1:] == rhs.shape[1:]


class TracedTensor(torch.Tensor):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.tensor_meta = None

    @staticmethod
    def from_torch_tensor(tensor, tensor_meta: TensorMeta):
        tensor.tensor_meta = tensor_meta
        tensor.__class__ = TracedTensor
        return tensor

    def as_subclass(self, cls: 'TracedTensor') ->'TracedTensor':
        """
        Required for PyTorch 1.7.0 compatibility - the handle_torch_function and __torch_function__
        API in general calls this after a wrapped function call; need to preserve the tensor_meta extensions
        """
        return self
    if hasattr(torch._C, '_disabled_torch_function_impl'):
        __torch_function__ = torch._C._disabled_torch_function_impl


class IterationScopeNodeMatcher(DefaultScopeNodeMatcher):

    def __init__(self, node_id_to_key_dict, nx_graph):
        super().__init__(node_id_to_key_dict, nx_graph)
        self._first_iteration_nodes = {}

    def save_first_iteration_node(self, inputs: 'OperatorInput', node: DynamicGraphNode):
        """
        It finds and saves "starting" points of iteration for further matching with them on next iteration,
        instead of adding new nodes for each iteration. "Starting" points of iteration are nodes
            * that have at least one input node, which is outside of iteration scope
            * or whose all inputs are not TracedTensor
        """
        op_exec_context = node.op_exec_context
        name = str(node)
        iter_scopes = op_exec_context.scope_in_model.get_iteration_scopes()
        if iter_scopes:
            for iter_scope in iter_scopes:
                if iter_scope not in self._first_iteration_nodes:
                    self._first_iteration_nodes[iter_scope] = {}
                first_nodes = self._first_iteration_nodes[iter_scope]
                has_input_outside_iteration = False
                untraced_tensor_inputs = []
                traced_tensor_inputs = []
                non_tensor_inputs = []
                for i in inputs:
                    input_obj = i.getter()
                    if isinstance(input_obj, Tensor):
                        if not isinstance(input_obj, TracedTensor):
                            untraced_tensor_inputs.append(input_obj)
                        else:
                            traced_tensor_inputs.append(input_obj)
                    else:
                        non_tensor_inputs.append(input_obj)
                for i in traced_tensor_inputs:
                    creator_id = i.tensor_meta.creator_id
                    creator_node = self.get_node_by_id(creator_id)
                    creator_node_op_exec_ctx = creator_node[DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR]
                    within_scopes = creator_node_op_exec_ctx.scope_in_model.get_iteration_scopes()
                    if iter_scope not in within_scopes:
                        has_input_outside_iteration = True
                if len(untraced_tensor_inputs) == len(inputs) - len(non_tensor_inputs):
                    has_input_outside_iteration = True
                if has_input_outside_iteration:
                    node_name = str(op_exec_context.op_address)
                    first_nodes[node_name] = node
                    nncf_logger.debug(f'Found first iteration node: {name} in scope: {iter_scope}')

    def add_node(self, op_exec_context: OperationExecutionContext, inputs, node_parameters: DynamicGraphNodeParameters, is_in_iteration_scope: bool=True) ->DynamicGraphNode:
        node = super().add_node(op_exec_context, inputs, node_parameters, is_in_iteration_scope=True)
        self.save_first_iteration_node(inputs, node)
        return node

    def find_node(self, op_address: OperationAddress, tensor_metas: List[TensorMeta], tm_comparators: List[TensorMetaComparator]) ->Optional[DynamicGraphNode]:
        iter_scopes = op_address.scope_in_model.get_iteration_scopes()
        input_matcher = FirstInputsMatcher()
        op_exec_context = OperationExecutionContext(op_address.operator_name, op_address.scope_in_model, op_address.call_order, tensor_metas, input_matcher=input_matcher, tm_comparators=tm_comparators)
        node_candidates = self._find_nodes_with_matching_context_and_inputs(op_exec_context)
        if not node_candidates:
            op_exec_context = OperationExecutionContext(op_address.operator_name, op_address.scope_in_model, op_address.call_order, tensor_metas, tm_comparators=tm_comparators)
            node_candidates = self._find_nodes_with_matching_context_among_inputless(op_exec_context)
            if not node_candidates and iter_scopes:
                comparators = tm_comparators + [ShapeOnlyTensorMetaComparator()]
                op_exec_context = OperationExecutionContext(op_address.operator_name, op_address.scope_in_model, op_address.call_order, tensor_metas, tm_comparators=comparators)
                iter_nodes = self._match_first_iteration_nodes(op_exec_context, iter_scopes)
                for node_key, node in iter_nodes.items():
                    node_candidates[node_key] = node
        node_candidates = list(node_candidates.values())
        result = None
        if len(node_candidates) == 1:
            result = node_candidates[0]
        if len(node_candidates) > 1:
            nncf_logger.debug(f'More than one node was matched against context {op_exec_context}')
            result = node_candidates[0]
        return result

    def _match_first_iteration_nodes(self, op_exec_context: OperationExecutionContext, iter_scopes):
        node_candidates = {}
        for iter_scope in iter_scopes:
            if iter_scope in self._first_iteration_nodes:
                for name, node in self._first_iteration_nodes[iter_scope].items():
                    if op_exec_context.matches_saved_inputs_from(node.op_exec_context):
                        node_candidates[name] = node
                        break
                if node_candidates:
                    break
        return node_candidates

    def get_first_iteration_modules(self) ->Dict:
        return self._first_iteration_nodes


class NodeManager:

    def __init__(self, node_id_to_key_dict, nx_graph):
        self.base_matcher = DefaultScopeNodeMatcher(node_id_to_key_dict, nx_graph)
        self.iteration_matcher = IterationScopeNodeMatcher(node_id_to_key_dict, nx_graph)

    @staticmethod
    def _within_iteration(scope: Scope):
        scope_name = str(scope)
        for iter_scope in ITERATION_MODULES.registry_dict:
            if iter_scope in scope_name:
                return True
        return False

    def choose_matcher(self, op_address: OperationAddress) ->DefaultScopeNodeMatcher:
        if self._within_iteration(op_address.scope_in_model):
            return self.iteration_matcher
        return self.base_matcher

    @staticmethod
    def choose_tm_comparators(op_address: OperationAddress, input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) ->List[TensorMetaComparator]:
        result = []
        for pairs in input_comparators_per_scope:
            comparator, scopes = pairs
            for scope in scopes:
                if scope in str(op_address):
                    result.append(comparator)
        return result

    def find_node(self, op_address: OperationAddress, tensor_metas: List[TensorMeta], input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) ->DynamicGraphNode:
        matcher = self.choose_matcher(op_address)
        comparators = self.choose_tm_comparators(op_address, input_comparators_per_scope)
        return matcher.find_node(op_address, tensor_metas, comparators)

    def add_node(self, op_address: OperationAddress, tensor_metas: List[TensorMeta], tm_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]], inputs, node_parameters: DynamicGraphNodeParameters) ->DynamicGraphNode:
        matcher = self.choose_matcher(op_address)
        tm_comparators = self.choose_tm_comparators(op_address, tm_comparators_per_scope)
        op_exec_context = OperationExecutionContext(op_address.operator_name, op_address.scope_in_model, op_address.call_order, tensor_metas, tm_comparators=tm_comparators)
        return matcher.add_node(op_exec_context, inputs, node_parameters)


class DynamicGraph:
    """
    The class for representing a graph dynamically built during a PyTorch model's `forward` method execution
    within a nncf.torch.dynamic_graph.context.TracingContext. This graph may change from a forward call to a
    forward call if the execution paths of the model change between the calls - this sets DynamicGraph apart from
    NNCFGraph which is a static representation of the model's structure. The DynamicGraph has limited support for
    RNN tracing and is rather suited to regular DNN tracing.
    """
    ID_NODE_ATTR = 'id'
    KEY_NODE_ATTR = 'key'
    LAYER_ATTRIBUTES = 'layer_attributes'
    OP_EXEC_CONTEXT_NODE_ATTR = 'op_exec_context'
    ACTIVATION_SHAPE_EDGE_ATTR = 'activation_shape'
    ACTIVATION_DTYPE_EDGE_ATTR = 'activation_dtype'
    INPUT_PORT_ID_EDGE_ATTR = 'input_port_id'
    OUTPUT_PORT_ID_EDGE_ATTR = 'output_port_id'
    IGNORED_ALGOS_NODE_ATTR = 'ignored_algos'
    IS_CALLED_INSIDE_NNCF_MODULE = 'is_called_inside_nncf_module'
    IS_IN_ITERATION_SCOPE_NODE_ATTR = 'is_in_iteration_scope'

    def __init__(self):
        self._nx_graph = nx.DiGraph()
        self._node_id_to_key_dict = {}
        self.match_manager = NodeManager(self._node_id_to_key_dict, self._nx_graph)
        self._input_nncf_nodes = []
        self._output_nncf_nodes = []

    def __eq__(self, other: 'DynamicGraph'):
        nm = iso.categorical_node_match([DynamicGraph.ID_NODE_ATTR, DynamicGraph.KEY_NODE_ATTR, DynamicGraph.OP_EXEC_CONTEXT_NODE_ATTR, DynamicGraph.LAYER_ATTRIBUTES], [None, None, None])
        em = iso.categorical_edge_match([DynamicGraph.ACTIVATION_SHAPE_EDGE_ATTR, DynamicGraph.INPUT_PORT_ID_EDGE_ATTR], [None, None])
        return nx.is_isomorphic(self._nx_graph, other._nx_graph, node_match=nm, edge_match=em)

    def find_node(self, op_address: OperationAddress, tensor_metas: List[TensorMeta], input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]]) ->DynamicGraphNode:
        return self.match_manager.find_node(op_address, tensor_metas, input_comparators_per_scope)

    def add_node(self, op_address: OperationAddress, tensor_metas: List[TensorMeta], input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[str]]], inputs, node_parameters: DynamicGraphNodeParameters) ->DynamicGraphNode:
        node = self.match_manager.add_node(op_address, tensor_metas, input_comparators_per_scope, inputs, node_parameters)
        if node.op_exec_context.operator_name == MODEL_INPUT_OP_NAME:
            self._input_nncf_nodes.append(node)
        if node.op_exec_context.operator_name == MODEL_OUTPUT_OP_NAME:
            self._output_nncf_nodes.append(node)
        return node

    def get_input_nodes(self) ->List[DynamicGraphNode]:
        return self._input_nncf_nodes

    def get_output_nodes(self) ->List[DynamicGraphNode]:
        return self._output_nncf_nodes

    def get_nodes_count(self) ->int:
        return self._nx_graph.number_of_nodes()

    def get_all_nodes(self) ->List[DynamicGraphNode]:
        all_nodes = []
        for node_key in self._node_id_to_key_dict.values():
            dynamic_graph_node = DynamicGraphNode.build_from_nx_node_dict(self._nx_graph.nodes[node_key])
            all_nodes.append(dynamic_graph_node)
        return all_nodes

    def get_all_edges(self) ->List[DynamicGraphEdge]:
        all_edges = []
        for from_node_key, to_node_key in self._nx_graph.edges:
            nx_edge_attrs = self._nx_graph.edges[from_node_key, to_node_key]
            from_node = self._nx_graph.nodes[from_node_key]
            to_node = self._nx_graph.nodes[to_node_key]
            from_node_id = from_node[DynamicGraph.ID_NODE_ATTR]
            to_node_id = to_node[DynamicGraph.ID_NODE_ATTR]
            dynamic_graph_edge = DynamicGraphEdge(from_node_id=from_node_id, to_node_id=to_node_id, activation_shape=nx_edge_attrs[DynamicGraph.ACTIVATION_SHAPE_EDGE_ATTR], input_port_id=nx_edge_attrs[DynamicGraph.INPUT_PORT_ID_EDGE_ATTR], output_port_id=nx_edge_attrs[DynamicGraph.OUTPUT_PORT_ID_EDGE_ATTR], dtype=nx_edge_attrs[DynamicGraph.ACTIVATION_DTYPE_EDGE_ATTR])
            all_edges.append(dynamic_graph_edge)
        return all_edges

    def is_graph_with_iteration_modules(self) ->bool:
        return len(self.match_manager.iteration_matcher.get_first_iteration_modules()) > 0


class InputIndexEntry:

    def __init__(self, path: Tuple[Union[int, str], ...], getter: Callable, setter: Callable):
        self.path = path
        self.getter = getter
        self.setter = setter


class TupleRebuildingSetter:

    def __init__(self, idx_to_set, current_tuple, previous_level_setter_for_current_tuple):
        self._previous_level_setter = previous_level_setter_for_current_tuple
        self._current_tuple = current_tuple
        self._idx_to_set = idx_to_set

    def __call__(self, value):
        tmp_list = list(self._current_tuple)
        tmp_list[self._idx_to_set] = value
        new_tuple = tuple(tmp_list)
        self._current_tuple = new_tuple
        self._previous_level_setter(new_tuple)


iteritems = lambda mapping: getattr(mapping, 'iteritems', mapping.items)()


string_types = str, bytes


def maybe_get_iterator(obj):
    it = None
    if isinstance(obj, Mapping):
        it = iteritems
    elif isinstance(obj, (Sequence, Set)) and not isinstance(obj, string_types):
        it = enumerate
    return it


class NestedObjectIndex:

    def __init__(self, obj, path=(), memo=None, previous_level_setter=None):
        self._flat_nested_obj_indexing = []
        self._nested_object_paths_generator(obj, self._flat_nested_obj_indexing, path, memo, previous_level_setter)

    @staticmethod
    def _nested_object_paths_generator(obj, out_entries_list, path=(), memo=None, previous_level_setter=None):
        if memo is None:
            memo = set()
        iterator = maybe_get_iterator(obj)
        if iterator is not None:
            if id(obj) not in memo:
                memo.add(id(obj))
                current_level_getters = []
                current_level_setters = []
                for idx, iterval in enumerate(iterator(obj)):
                    path_component, value = iterval
                    current_level_getters.append(partial(obj.__getitem__, path_component))
                    if not isinstance(obj, tuple):
                        if hasattr(obj, '__setitem__'):
                            current_level_setters.append(partial(obj.__setitem__, path_component))
                        else:
                            current_level_setters.append(None)
                    else:
                        current_level_setters.append(TupleRebuildingSetter(idx, obj, previous_level_setter))
                for idx, iterval in enumerate(iterator(obj)):
                    path_component, value = iterval
                    retval = NestedObjectIndex._nested_object_paths_generator(value, out_entries_list, path + (path_component,), memo, current_level_setters[idx])
                    was_leaf = retval[1]
                    if was_leaf:
                        leaf_entry_path = retval
                        getter = current_level_getters[idx]
                        setter = current_level_setters[idx]
                        if setter is not None:
                            out_entries_list.append(InputIndexEntry(leaf_entry_path, getter, setter))
                memo.remove(id(obj))
            is_leaf = False
            return path, is_leaf
        is_leaf = True
        return path, is_leaf

    def get_flat_nested_obj_indexing(self) ->List[InputIndexEntry]:
        return self._flat_nested_obj_indexing


class OperatorInput:

    def __init__(self, op_args, op_kwargs):
        self.op_args = op_args
        self.op_kwargs = op_kwargs
        self._index = OrderedDict()
        op_args_index_entries = NestedObjectIndex(self.op_args, previous_level_setter=partial(setattr, self, 'op_args'))
        op_kwargs_index_entries = NestedObjectIndex(self.op_kwargs)
        self._index = {idx: entry for idx, entry in enumerate(op_args_index_entries.get_flat_nested_obj_indexing() + op_kwargs_index_entries.get_flat_nested_obj_indexing())}

    def __iter__(self):
        return iter(self._index.values())

    def __getitem__(self, n):
        return self._index[n].getter()

    def __setitem__(self, n, value):
        self._index[n].setter(value)

    def __len__(self):
        return len(self._index)


class PreHookId:

    def __init__(self, op_address: OperationAddress, input_port_id: int):
        self.op_address = op_address
        self.input_port_id = input_port_id

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __str__(self):
        return str(self.op_address) + '|INPUT{}'.format(self.input_port_id)

    def __hash__(self):
        return hash(str(self))


def is_debug():
    return nncf_logger.getEffectiveLevel() == logging.DEBUG


class TracingContext:

    def __init__(self):
        self.graph = DynamicGraph()
        self._save_context = None
        self._post_hooks = {}
        self._pre_hooks = {}
        self._num_nested_hooks = 0
        self._threading = CopySafeThreadingVars()
        self._n_instances_searching_graph = 0
        self._is_tracing = True
        self._is_forwarding = False
        self._may_add_nodes = True
        self._input_comparators_per_scope = []
        self.global_buffer_store = {}
        self._trace_dynamic_graph = False
        self.elastic_depth = False
        self.skipped_blocks = []
        self.in_skipped_block = False
        self.start_node_name_of_skipped_block = []
        self.end_node_name_of_skipped_block = []
        self.active_block_indexes = None
        self.tensor_cache = None
        self._ordinals_ids = None

    def __enter__(self):
        global _CURRENT_CONTEXT
        self._save_context = _CURRENT_CONTEXT
        _CURRENT_CONTEXT = self
        self._reset_thread_local()
        if is_debug():
            self.reset_node_call_counters()
        return self

    def __exit__(self, *args):
        self._reset_thread_local()
        global _CURRENT_CONTEXT
        _CURRENT_CONTEXT = self._save_context
        self._save_context = None

    def find_operator_node(self, tensor_metas: List[Optional[TensorMeta]], op_address: OperationAddress) ->Optional[DynamicGraphNode]:
        with self._threading.cond:
            self._n_instances_searching_graph += 1
        node = self.graph.find_node(op_address, tensor_metas, self._input_comparators_per_scope)
        with self._threading.cond:
            self._n_instances_searching_graph -= 1
            self._threading.cond.notify_all()
        return node

    def register_global_buffer(self, name: str, buffer):
        self.global_buffer_store[name] = buffer

    def maybe_add_node(self, inputs: OperatorInput, tensor_metas: List[Optional[TensorMeta]], op_address: OperationAddress, module_attrs: BaseLayerAttributes=None, ignored_algorithms: List[str]=None, is_called_inside_nncf_module: bool=False) ->Optional[DynamicGraphNode]:
        if not self._may_add_nodes:
            return None
        with self._threading.cond:
            while self._n_instances_searching_graph > 0:
                self._threading.cond.wait()
            node = self.graph.find_node(op_address, tensor_metas, self._input_comparators_per_scope)
            if node is None:
                node = self.graph.add_node(op_address, tensor_metas, self._input_comparators_per_scope, inputs, DynamicGraphNodeParameters(module_attrs, ignored_algorithms, is_called_inside_nncf_module))
        return node

    def get_caller_context(self, operator_name: str) ->OperationAddress:
        """
        Designed to work in the following way - for each scope the context will track the number of the calls to the
        operators with the name operator_name (call_order). The counter values are preserved until reset by a
        corresponding member function of the context, which must be called after each model iteration - this is
        usually handled inside NNCF. This mechanism allows to discern between multiple function calls inside the same
        module that would each require their own instance of compression layers - for instance, multiple `relu`
        function calls (either on their own or inside a `for` cycle), and at the same moment allow the checkpoints to
        be loaded if the model had changed in the meantime in a way that does not impact the major function call
        order (e.g. if comments were added to the .py file with the model)
        """
        call_order = self.get_operator_call_count_in_scope(operator_name, self.scope)
        op_address = OperationAddress(operator_name, self.scope, call_order)
        return op_address

    def reset_scope_operator_call_counters(self):
        """
        Must be called after each "forward" operation of the model that is made
        within this context
        """
        self._threading.thread_local.operator_counters = {}

    @staticmethod
    def _get_operator_counter_key(operator_name: str, scope: Scope):
        return '{}_{}'.format(str(scope), operator_name)

    def register_operator_call(self, operator_name: str, scope: Scope):
        key = self._get_operator_counter_key(operator_name, scope)
        if key in self._threading.thread_local.operator_counters:
            self._threading.thread_local.operator_counters[key] += 1
        else:
            self._threading.thread_local.operator_counters[key] = 1

    def get_operator_call_count_in_scope(self, operator_name: str, scope: Scope):
        key = self._get_operator_counter_key(operator_name, scope)
        if key in self._threading.thread_local.operator_counters:
            return self._threading.thread_local.operator_counters[key]
        return 0

    def reset_operator_call_count_in_scope(self, scope):
        scoped_op_name = str(scope)
        for key in self._threading.thread_local.operator_counters.keys():
            if scoped_op_name in key:
                self._threading.thread_local.operator_counters[key] = 0

    def push_scope(self, called_module: torch.nn.Module):
        relative_scopes_list = self._get_scope_relative_to_last_registered_module_call(called_module)
        self.module_call_stack.append(called_module)
        self.relative_scopes_stack.append(relative_scopes_list)

    def pop_scope(self):
        self.relative_scopes_stack.pop()
        self.module_call_stack.pop()

    def register_pre_hooks(self, fn_list: List[Callable], op_address: OperationAddress, input_port_id: int):
        pre_hook_id = PreHookId(op_address, input_port_id)
        if pre_hook_id in self._pre_hooks:
            raise KeyError('Pre hook for context {} is already registered'.format(str(pre_hook_id)))
        self._pre_hooks[pre_hook_id] = fn_list

    def execute_pre_hooks(self, op_address: OperationAddress, op_inputs: OperatorInput) ->OperatorInput:
        in_op = getattr(self, 'in_operator', False)
        self.in_operator = False
        self._threading.thread_local.num_nested_hooks += 1
        pre_hook_ids_for_curr_op = [x for x in self._pre_hooks if x.op_address == op_address]
        pre_hook_ids_for_curr_op = sorted(pre_hook_ids_for_curr_op, key=lambda x: x.input_port_id)
        for pre_hook_id in pre_hook_ids_for_curr_op:
            hook_list_for_current_input_port = self._pre_hooks[pre_hook_id]
            input_arg_to_process = pre_hook_id.input_port_id
            for hook in hook_list_for_current_input_port:
                op_inputs[input_arg_to_process] = hook(op_inputs[input_arg_to_process])
        self._threading.thread_local.num_nested_hooks -= 1
        self.in_operator = in_op
        return op_inputs

    def register_post_hooks(self, fn_list: List[Callable], op_address: OperationAddress):
        if op_address in self._post_hooks:
            raise KeyError('Post hook for context {} is already registered'.format(str(op_address)))
        self._post_hooks[op_address] = fn_list

    def execute_post_hooks(self, op_address: OperationAddress, outputs):
        in_op = getattr(self, 'in_operator', False)
        self.in_operator = False
        self._threading.thread_local.num_nested_hooks += 1
        if op_address in self._post_hooks:
            for hook in self._post_hooks[op_address]:
                outputs = hook(outputs)
        self._threading.thread_local.num_nested_hooks -= 1
        self.in_operator = in_op
        return outputs

    @property
    def is_tracing(self) ->bool:
        return self._is_tracing

    def disable_tracing(self):
        self._is_tracing = False

    def enable_tracing(self):
        self._is_tracing = True

    @property
    def is_forwarding(self) ->bool:
        return self._is_forwarding

    def disable_forwarding(self):
        self._is_forwarding = False

    def enable_forwarding(self):
        self._is_forwarding = True

    def enable_node_additions(self):
        self._may_add_nodes = True

    def disable_node_additions(self):
        self._may_add_nodes = False

    def add_node_comparators(self, scopes_to_apply: List[str], node_input_comparator: 'TensorMetaComparator'=None):
        self._input_comparators_per_scope.append((node_input_comparator, scopes_to_apply))

    @property
    def base_module_thread_local_replica(self) ->torch.nn.Module:
        return self._threading.thread_local.base_module_replica

    @base_module_thread_local_replica.setter
    def base_module_thread_local_replica(self, value: torch.nn.Module):
        self._threading.thread_local.base_module_replica = value

    @property
    def in_operator(self):
        return self._threading.thread_local.in_operator

    @in_operator.setter
    def in_operator(self, val):
        self._threading.thread_local.in_operator = val

    @property
    def module_call_stack(self) ->List[torch.nn.Module]:
        return self._threading.thread_local.module_call_stack

    def get_current_module(self) ->Optional[torch.nn.Module]:
        if self.module_call_stack:
            return self.module_call_stack[-1]
        return None

    @property
    def relative_scopes_stack(self) ->List[Scope]:
        return self._threading.thread_local.scopes

    @property
    def trace_dynamic_graph(self) ->bool:
        return self._trace_dynamic_graph

    def disable_trace_dynamic_graph(self):
        if not self.graph.is_graph_with_iteration_modules():
            self._trace_dynamic_graph = False

    def enable_trace_dynamic_graph(self):
        self._trace_dynamic_graph = True

    def _reset_thread_local(self):
        tl = self._threading.thread_local
        tl.scopes = []
        tl.module_call_stack = []
        tl.in_operator = False
        tl.num_nested_hooks = 0
        tl.base_module_replica = None
        tl.operator_counters = {}
        tl.node_call_tracker = {}

    def register_node_call(self, node: DynamicGraphNode):
        if node.node_id in self._threading.thread_local.node_call_tracker:
            self._threading.thread_local.node_call_tracker[node.node_id] += 1
        else:
            self._threading.thread_local.node_call_tracker[node.node_id] = 1

    def reset_node_call_counters(self):
        for k, _ in self._threading.thread_local.node_call_tracker.items():
            self._threading.thread_local.node_call_tracker[k] = 0

    def get_node_call_counter_dict(self):
        return self._threading.thread_local.node_call_tracker

    def _get_scope_relative_to_last_registered_module_call(self, module) ->Scope:
        module_class = module.__class__.__name__
        if not self.module_call_stack:
            return Scope([ScopeElement(module_class)])
        q = deque([(tuple(), self.module_call_stack[-1])])
        while q:
            scope_parts, top = q.popleft()
            if module is top:
                return Scope(list(scope_parts))
            for name, child in top.named_children():
                scope_element = ScopeElement(child.__class__.__name__, name)
                q.append((scope_parts + (scope_element,), child))
        return Scope([ScopeElement(module_class)])

    @property
    def scope(self) ->Scope:
        stack_copy = self.relative_scopes_stack.copy()
        scope_el_list = []
        for relative_scope in stack_copy:
            for scope_element in relative_scope.scope_elements:
                scope_el_list.append(scope_element)
        return Scope(scope_el_list)

    def reset_graph(self):
        self.graph = DynamicGraph()

    def set_active_skipped_block(self, block_indexes: List[int]):
        if self.active_block_indexes is not None:
            self.start_node_name_of_skipped_block = []
            self.end_node_name_of_skipped_block = []
        self.active_block_indexes = block_indexes
        if self.elastic_depth and len(block_indexes) > 0:
            for block_index in block_indexes:
                self.start_node_name_of_skipped_block.append(self.skipped_blocks[block_index].start_node_name)
                self.end_node_name_of_skipped_block.append(self.skipped_blocks[block_index].end_node_name)

    def set_elastic_blocks(self, blocks: List['BuildingBlock']=None):
        if blocks is not None:
            if isinstance(blocks, list):
                if len(blocks) == 0:
                    self.skipped_blocks = []
                elif isinstance(blocks[0], str):
                    self.skipped_blocks = [blocks]
                else:
                    self.skipped_blocks = blocks


_CURRENT_CONTEXT = None


def get_current_context() ->TracingContext:
    return _CURRENT_CONTEXT


class DetectionOutputFunction(torch.autograd.Function):
    """At test time, Detect is the final layer of SSD.  Decode location preds,
    apply non-maximum suppression to location predictions based on conf
    scores and threshold to a top_k number of output predictions for both
    confidence score and locations.
    """

    @staticmethod
    def symbolic(g, loc_data, conf_data, prior_data, detection_output_params):
        return g.op(add_domain('DetectionOutput'), loc_data, conf_data, prior_data, num_classes_i=detection_output_params.num_classes, background_label_id_i=detection_output_params.background_label_id, top_k_i=detection_output_params.top_k, keep_top_k_i=detection_output_params.keep_top_k, confidence_threshold_f=detection_output_params.confidence_threshold, nms_threshold_f=detection_output_params.nms_threshold, eta_f=detection_output_params.eta, share_location_i=detection_output_params.share_location, code_type_s=detection_output_params.code_type, variance_encoded_in_target_i=detection_output_params.variance_encoded_in_target)

    @staticmethod
    def forward(ctx, loc_data, conf_data, prior_data, detection_output_params):
        """
        Args:
            loc_data: (tensor) Loc preds from loc layers
                Shape: [batch,num_priors*4]
            conf_data: (tensor) Shape: Conf preds from conf layers
                Shape: [batch,num_priors*num_classes]
            prior_data: (tensor) Prior boxes and variances from priorbox layers
                Shape: [1,2,num_priors*4]
        """
        with no_jit_trace(), no_nncf_trace():
            if detection_output_params.nms_threshold <= 0:
                raise ValueError('nms_threshold must be non negative.')
            device = loc_data.device
            batch_size = loc_data.size(0)
            num_priors = int(loc_data.size(1) / 4)
            loc_data = loc_data.view(batch_size, num_priors, 4)
            conf_data = conf_data.view(batch_size, num_priors, -1)
            prior_data = prior_data.view(1, 2, num_priors, 4)
            output = torch.zeros(batch_size, 1, detection_output_params.keep_top_k, 7)
            conf_preds = conf_data.view(batch_size, num_priors, detection_output_params.num_classes).transpose(2, 1)
            for i in range(batch_size):
                output_for_img = torch.zeros(0, 7)
                decoded_boxes = decode(loc_data[i], prior_data[0])
                conf_scores = conf_preds[i].clone()
                total_detections_count = 0
                all_indices = {}
                boxes = {}
                for cl in range(0, detection_output_params.num_classes):
                    if cl == detection_output_params.background_label_id:
                        continue
                    c_mask = conf_scores[cl].gt(detection_output_params.confidence_threshold)
                    scores = conf_scores[cl][c_mask]
                    if scores.dim() == 0:
                        continue
                    conf_scores[cl, :scores.size()[0]] = scores
                    conf_scores[cl, scores.size()[0]:] = 0
                    l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)
                    boxes[cl] = decoded_boxes[l_mask].view(-1, 4)
                    all_indices[cl], count = nms(boxes[cl], scores, detection_output_params.nms_threshold, detection_output_params.top_k)
                    all_indices[cl] = all_indices[cl][:count]
                    total_detections_count += count
                score_index_pairs = []
                for label, indices in all_indices.items():
                    indices = indices.cpu().numpy()
                    for idx in indices:
                        score_index_pairs.append((conf_scores[label, idx], label, idx))
                score_index_pairs.sort(key=lambda tup: tup[0], reverse=True)
                score_index_pairs = score_index_pairs[:detection_output_params.keep_top_k]
                all_indices_new = {}
                for _, label, idx in score_index_pairs:
                    if label not in all_indices_new:
                        all_indices_new[label] = [idx]
                    else:
                        all_indices_new[label].append(idx)
                for label, indices in all_indices_new.items():
                    out = torch.cat((torch.zeros((len(indices), 1), dtype=torch.float).new_full((len(indices), 1), i), torch.zeros((len(indices), 1), dtype=torch.float).new_full((len(indices), 1), label), conf_scores[label, indices].unsqueeze(1), boxes[label][indices]), 1)
                    output_for_img = torch.cat((output_for_img, out), 0)
                output[i, 0, :output_for_img.size()[0]] = output_for_img
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


class DetectionOutput(nn.Module):

    def __init__(self, num_classes, background_label_id, top_k, keep_top_k, confidence_threshold, nms_threshold, eta=1, share_location=1, code_type='CENTER_SIZE', variance_encoded_in_target=0):
        super().__init__()
        self.num_classes = num_classes
        self.background_label_id = background_label_id
        self.top_k = top_k
        self.keep_top_k = keep_top_k
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.eta = eta
        self.share_location = share_location
        self.code_type = code_type
        self.variance_encoded_in_target = variance_encoded_in_target

    def forward(self, loc_data, conf_data, prior_data):
        return DetectionOutputFunction.apply(loc_data, conf_data, prior_data, self)

    @staticmethod
    def backward(ctx, grad_out):
        return grad_out


class PriorBoxFunction(torch.autograd.Function):
    """Compute priorbox coordinates in point form for each source
    feature map.
    """

    @staticmethod
    def symbolic(g, input_fm, img_tensor, priorbox_params):
        return g.op(add_domain('PriorBox'), input_fm, img_tensor, min_size_f=[priorbox_params.min_size], max_size_f=[priorbox_params.max_size], aspect_ratio_f=priorbox_params.aspect_ratio, flip_i=priorbox_params.flip, clip_i=priorbox_params.clip, variance_f=priorbox_params.variance, step_f=priorbox_params.step, offset_f=priorbox_params.offset, step_h_f=priorbox_params.step_h, step_w_f=priorbox_params.step_w, img_size_i=priorbox_params.img_size, img_h_i=priorbox_params.img_h, img_w_i=priorbox_params.img_w)

    @staticmethod
    def forward(ctx, input_fm, img_tensor, priorbox_params):
        for v in priorbox_params.variance:
            if v <= 0:
                raise ValueError('Variances must be greater than 0')
        mean = []
        variance_channel = []
        f_h = input_fm.size()[2]
        f_w = input_fm.size()[3]
        img_height = img_tensor.size()[2]
        img_width = img_tensor.size()[3]
        box_widths_heights = [(priorbox_params.min_size, priorbox_params.min_size), (sqrt(priorbox_params.min_size * priorbox_params.max_size), sqrt(priorbox_params.min_size * priorbox_params.max_size))]
        for ar in priorbox_params.aspect_ratio:
            box_widths_heights.append((priorbox_params.min_size * sqrt(ar), priorbox_params.min_size / sqrt(ar)))
            if priorbox_params.flip:
                box_widths_heights.append((priorbox_params.min_size / sqrt(ar), priorbox_params.min_size * sqrt(ar)))
        for i, j in product(range(f_h), range(f_w)):
            cx = (j + priorbox_params.offset) * priorbox_params.step
            cy = (i + priorbox_params.offset) * priorbox_params.step
            for box_width, box_height in box_widths_heights:
                mean += [(cx - box_width / 2.0) / img_width, (cy - box_height / 2.0) / img_height, (cx + box_width / 2.0) / img_width, (cy + box_height / 2.0) / img_height]
                variance_channel += priorbox_params.variance
        mean = torch.Tensor(mean).unsqueeze(0)
        if priorbox_params.clip:
            mean.clamp_(max=1, min=0)
        variance_channel = torch.Tensor(variance_channel).unsqueeze(0)
        output = torch.stack((mean, variance_channel), dim=1)
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


class PriorBox(nn.Module):

    def __init__(self, min_size, max_size, aspect_ratio, flip, clip, variance, step, offset, step_h, step_w, img_size, img_h, img_w):
        super().__init__()
        self.min_size = min_size
        self.max_size = max_size
        self.aspect_ratio = aspect_ratio
        self.flip = flip
        self.clip = clip
        self.variance = variance
        self.step = step
        self.offset = offset
        self.step_h = step_h
        self.step_w = step_w
        self.img_size = img_size
        self.img_h = img_h
        self.img_w = img_w

    def forward(self, input_fm, img_tensor):
        return PriorBoxFunction.apply(input_fm, img_tensor, self)


class L2NormFunction(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, weight, l2NormParams):
        with no_jit_trace():
            norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + l2NormParams.eps
            x = torch.div(x, norm)
            out = weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x
            return out

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]

    @staticmethod
    def symbolic(g, x, weight, l2NormParams):
        return g.op(add_domain('Normalize'), x, weight, eps_f=l2NormParams.eps, across_spatial_i=l2NormParams.across_spatial, channel_shared_i=l2NormParams.channel_shared)


class WeightedLayerAttributes(BaseLayerAttributes):
    """
    Represents a layer with weights.
    """

    def __init__(self, weight_requires_grad: bool, dtype: Dtype=Dtype.FLOAT):
        self.weight_requires_grad = weight_requires_grad
        self.dtype = dtype

    def __eq__(self, other: Any):
        return isinstance(other, WeightedLayerAttributes) and self.weight_requires_grad == other.weight_requires_grad

    @abstractmethod
    def get_weight_shape(self) ->List[int]:
        pass

    def get_num_filters(self) ->int:
        weight_shape = self.get_weight_shape()
        return weight_shape[self.get_target_dim_for_compression()]

    @abstractmethod
    def get_target_dim_for_compression(self) ->int:
        pass


class GenericWeightedLayerAttributes(WeightedLayerAttributes):
    """
    Represents a weighted layer for which there is no information ahead of time
    of the exact meaning of the weight indices.
    """

    def __init__(self, weight_requires_grad: bool, weight_shape: List[int], filter_dimension_idx: int=0):
        super().__init__(weight_requires_grad)
        self.weight_shape = weight_shape
        self.filter_dimension_idx = filter_dimension_idx

    def get_weight_shape(self) ->List[int]:
        return self.weight_shape

    def get_target_dim_for_compression(self) ->int:
        return 0


NNCF_WRAPPED_USER_MODULES_DICT = {}


UNWRAPPED_USER_MODULES = Registry('user_modules')


class ProxyModule:

    def __init__(self, module):
        self._module = module

    def __getattr__(self, name):
        return getattr(self._module, name)


class _NNCFModuleMixin:
    """
    Default class for modules that will be optimized by NNCF.

        Attributes:
            op_func_name    Name of corresponding torch function.
            target_weight_dim_for_compression   Target dimension of weights that will be compressed in some algorithms.
            ignored_algorithms   List of algorithms that will skip the module.
            _custom_forward_fn  wrapper of the custom forward function that is called with `self` argument equals to the
                ProxyModule
    """
    op_func_name = ''
    target_weight_dim_for_compression = 0
    _custom_forward_fn = None
    ignored_algorithms = []

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        _NNCFModuleMixin.add_mixin_fields(self)

    @staticmethod
    def add_mixin_fields(obj):
        obj.pre_ops = nn.ModuleDict()
        obj.post_ops = nn.ModuleDict()

    def get_pre_op(self, key):
        return self.pre_ops[key]

    def get_post_op(self, key):
        return self.post_ops[key]

    def register_pre_forward_operation(self, op):
        key = str(len(self.pre_ops))
        self.pre_ops[key] = op
        return key

    def remove_pre_forward_operation(self, key):
        return self.pre_ops.pop(key)

    def register_post_forward_operation(self, op):
        key = str(len(self.post_ops))
        self.post_ops[key] = op
        return key

    def remove_post_forward_operation(self, key):
        return self.post_ops.pop(key)

    def reset(self):
        self.pre_ops.clear()
        self.post_ops.clear()

    def forward(self, *args):
        proxy_module = ProxyModule(self)
        for op in self.pre_ops.values():
            op_args = op(proxy_module, args)
            if op_args is not None:
                if not isinstance(op_args, tuple):
                    op_args = tuple([op_args])
                args = op_args
        forward_fn = self._custom_forward_fn.__func__ if self._custom_forward_fn else super().forward.__func__
        results = forward_fn(proxy_module, *args)
        for op in self.post_ops.values():
            op_results = op(proxy_module, results)
            if op_results is not None:
                results = op_results
        return results


SPARSITY_TYPES = ['magnitude', 'rb', 'const']


SPARSITY_ALGOS = {'_'.join([type, 'sparsity']) for type in SPARSITY_TYPES}


def register_module(*quantizable_field_names: str, ignored_algorithms: list=None):

    def wrap(cls):
        UNWRAPPED_USER_MODULES.registry_dict[cls.__name__] = cls
        nncf_wrapped_module_class_name = 'NNCFUser{}'.format(cls.__name__)
        NNCF_WRAPPED_USER_MODULES_DICT[cls] = type(nncf_wrapped_module_class_name, (_NNCFModuleMixin, cls), {})
        get_base_attributes_fn = lambda self: GenericWeightedLayerAttributes(self.weight.requires_grad, self.weight.shape)
        setattr(NNCF_WRAPPED_USER_MODULES_DICT[cls], 'get_weight_shape', get_base_attributes_fn)
        if ignored_algorithms:
            setattr(NNCF_WRAPPED_USER_MODULES_DICT[cls], 'ignored_algorithms', ignored_algorithms)
        return cls
    return wrap


class L2Norm(nn.Module):

    def __init__(self, n_channels, scale, eps, across_spatial=0, channel_shared=0):
        super().__init__()
        self.n_channels = n_channels
        self.scale = scale or None
        self.eps = eps
        self.across_spatial = across_spatial
        self.channel_shared = channel_shared
        self.weight = nn.Parameter(torch.Tensor(self.n_channels))
        self.reset_parameters()

    def reset_parameters(self):
        init.constant_(self.weight, self.scale)

    def forward(self, x):
        if self.training:
            norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps
            x = torch.div(x, norm)
            out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x
            return out
        return L2NormFunction.apply(x, self.weight, self)


def log_sum_exp(x):
    """Utility function for computing log_sum_exp while determining
    This will be used to determine unaveraged confidence loss across
    all examples in a batch.
    Args:
        x (Variable(tensor)): conf_preds from conf layers
    """
    x_max = x.data.max()
    return torch.log(torch.sum(torch.exp(x - x_max), 1, keepdim=True)) + x_max


def encode(matched, priors):
    """Encode the variances from the priorbox layers into the ground truth boxes
    we have matched (based on jaccard overlap) with the prior boxes.
    Args:
        matched: (tensor) Coords of ground truth for each prior in point-form
            Shape: [num_priors, 4].
        priors: (tensor) Prior boxes in point-form with variances
            Shape: [2, num_priors,4].
    Return:
        encoded boxes (tensor), Shape: [num_priors, 4]
    """
    variances = priors[1]
    priors = priors[0]
    g_cxcy = ((matched[:, :2] + matched[:, 2:]) / 2 - (priors[:, :2] + priors[:, 2:]) / 2) / ((priors[:, 2:] - priors[:, :2]) * variances[:, :2])
    g_wh = torch.log((matched[:, 2:] - matched[:, :2]) / (priors[:, 2:] - priors[:, :2])) / variances[:, 2:]
    return torch.cat([g_cxcy, g_wh], 1)


def intersect(box_a: np.ndarray, box_b: np.ndarray) ->np.ndarray:
    max_xy = np.minimum(box_a[:, 2:], box_b[2:])
    min_xy = np.maximum(box_a[:, :2], box_b[:2])
    inter = np.clip(max_xy - min_xy, a_min=0, a_max=np.inf)
    return inter[:, 0] * inter[:, 1]


def jaccard(box_a, box_b):
    """Compute the jaccard overlap of two sets of boxes.  The jaccard overlap
    is simply the intersection over union of two boxes.  Here we operate on
    ground truth boxes and default boxes.
    E.g.:
        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)
    Args:
        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]
        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]
    Return:
        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]
    """
    inter = intersect(box_a, box_b)
    area_a = ((box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])).unsqueeze(1).expand_as(inter)
    area_b = ((box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])).unsqueeze(0).expand_as(inter)
    union = area_a + area_b - inter
    return inter / union


def match(threshold, truths, priors, labels, loc_t, conf_t, idx):
    """Match each prior box with the ground truth box of the highest jaccard
    overlap, encode the bounding boxes, then return the matched indices
    corresponding to both confidence and location preds.
    Args:
        threshold: (float) The overlap threshold used when mathing boxes.
        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].
        priors: (tensor) Prior boxes from priorbox layers with variances, Shape: [2,n_priors,4].
        labels: (tensor) All the class labels for the image, Shape: [num_obj].
        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.
        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.
        idx: (int) current batch index
    Return:
        The matched indices corresponding to 1)location and 2)confidence preds.
    """
    overlaps = jaccard(truths, priors[0])
    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)
    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)
    best_truth_idx.squeeze_(0)
    best_truth_overlap.squeeze_(0)
    best_prior_idx.squeeze_(1)
    best_prior_overlap.squeeze_(1)
    best_truth_overlap.index_fill_(0, best_prior_idx, 2)
    for j in range(best_prior_idx.size(0)):
        best_truth_idx[best_prior_idx[j]] = j
    matches = truths[best_truth_idx]
    conf = labels[best_truth_idx] + 1
    conf[best_truth_overlap < threshold] = 0
    loc = encode(matches, priors)
    loc_t[idx] = loc
    conf_t[idx] = conf


class MultiBoxLoss(nn.Module):
    """SSD Weighted Loss Function
    Compute Targets:
        1) Produce Confidence Target Indices by matching  ground truth boxes
           with (default) 'priorboxes' that have jaccard index > threshold parameter
           (default threshold: 0.5).
        2) Produce localization target by 'encoding' variance into offsets of ground
           truth boxes and their matched  'priorboxes'.
        3) Hard negative mining to filter the excessive number of negative examples
           that comes with using a large number of default bounding boxes.
           (default negative:positive ratio 3:1)
    Objective Loss:
        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N
        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss
        weighted by α which is set to 1 by cross val.
        Args:
            c: class confidences,
            l: predicted boxes,
            g: ground truth boxes
            N: number of matched default boxes
        See: https://arxiv.org/pdf/1512.02325.pdf for more details.
    """

    def __init__(self, cfg, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, device=None):
        super().__init__()
        self.device = device
        self.num_classes = num_classes
        self.threshold = overlap_thresh
        self.background_label = bkg_label
        self.encode_target = encode_target
        self.use_prior_for_matching = prior_for_matching
        self.do_neg_mining = neg_mining
        self.negpos_ratio = neg_pos
        self.neg_overlap = neg_overlap

    def forward(self, predictions, targets):
        """Multibox Loss
        Args:
            predictions (tuple): A tuple containing loc preds, conf preds,
            and prior boxes from SSD net.
                conf shape: torch.size(batch_size,num_priors,num_classes)
                loc shape: torch.size(batch_size,num_priors,4)
                priors shape: torch.size(num_priors,4)

            ground_truth (tensor): Ground truth boxes and labels for a batch,
                shape: [batch_size,num_objs,5] (last idx is the label).
        """
        loc_data, conf_data, priors = predictions
        batch = loc_data.size(0)
        num_priors = loc_data.size(1)
        loc_t = torch.Tensor(batch, num_priors, 4)
        conf_t = torch.LongTensor(batch, num_priors)
        for idx in range(batch):
            truths = targets[idx][:, :-1].data
            labels = targets[idx][:, -1].data
            defaults = priors.data
            match(self.threshold, truths, defaults[0], labels, loc_t, conf_t, idx)
        pos = conf_t > 0
        num_pos = pos.sum(dim=1, keepdim=True)
        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)
        loc_p = loc_data[pos_idx].view(-1, 4)
        loc_t = loc_t[pos_idx].view(-1, 4)
        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')
        batch_conf = conf_data.view(-1, self.num_classes)
        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))
        loss_c = loss_c.view(batch, -1)
        loss_c[pos] = 0
        _, loss_idx = loss_c.sort(1, descending=True)
        _, idx_rank = loss_idx.sort(1)
        num_pos = pos.long().sum(1, keepdim=True)
        num_neg = torch.clamp(self.negpos_ratio * num_pos, max=pos.size(1) - 1)
        neg = idx_rank < num_neg.expand_as(idx_rank)
        pos_idx = pos.unsqueeze(2).expand_as(conf_data)
        neg_idx = neg.unsqueeze(2).expand_as(conf_data)
        conf_p = conf_data[(pos_idx + neg_idx).gt(0)].view(-1, self.num_classes)
        targets_weighted = conf_t[(pos + neg).gt(0)]
        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')
        N = num_pos.data.sum()
        loss_l /= N
        loss_c /= N
        return loss_l, loss_c


class SSDHead(nn.Module):

    def __init__(self, num_features, num_classes, min_size, max_size, aspect_ratios, steps, varience, flip, clip):
        super().__init__()
        self.num_classes = num_classes
        self.clip = clip
        self.flip = flip
        self.varience = varience
        self.steps = steps
        self.aspect_ratios = aspect_ratios
        self.max_size = max_size
        self.min_size = min_size
        self.input_features = num_features
        num_prior_per_cell = 2 + 2 * len(aspect_ratios)
        self.loc = nn.Conv2d(num_features, num_prior_per_cell * 4, kernel_size=3, padding=1)
        self.conf = nn.Conv2d(num_features, num_prior_per_cell * num_classes, kernel_size=3, padding=1)
        self.prior_box = PriorBox(min_size, max_size, aspect_ratios, flip, clip, varience, steps, 0.5, 0, 0, 0, 0, 0)

    def forward(self, features, image_tensor):
        loc = self.loc(features)
        conf = self.conf(features)
        with no_nncf_trace():
            priors = self.prior_box(features, image_tensor)
        priors = priors.detach()
        loc = loc.permute(0, 2, 3, 1).contiguous()
        conf = conf.permute(0, 2, 3, 1).contiguous()
        return loc, conf, priors


class SSDDetectionOutput(nn.Module):

    def __init__(self, num_input_features, num_classes, config):
        super().__init__()
        self.config = config
        self.num_classes = num_classes
        self.loss_inference = config.get('loss_inference', False)
        self.heads = nn.ModuleList()
        for i, num_features in enumerate(num_input_features):
            self.heads.append(SSDHead(num_features, num_classes, config.min_sizes[i], config.max_sizes[i], config.aspect_ratios[i], config.steps[i], config.variance, config.flip, config.clip))
        self.detection_output = DetectionOutput(num_classes, 0, config.get('top_k', 200), config.get('keep_top_k', 200), 0.01, 0.45, 1, 1, 'CENTER_SIZE', 0)

    def forward(self, source_features, img_tensor):
        locs = []
        confs = []
        priors = []
        for features, head in zip(source_features, self.heads):
            loc, conf, prior = head(features, img_tensor)
            locs.append(loc)
            confs.append(conf)
            priors.append(prior)
        batch = source_features[0].size(0)
        loc = torch.cat([o.view(batch, -1) for o in locs], 1)
        conf = torch.cat([o.view(batch, -1) for o in confs], 1)
        conf_softmax = F.softmax(conf.view(conf.size(0), -1, self.num_classes), dim=-1)
        with no_nncf_trace():
            priors = torch.cat(priors, dim=2)
        if self.training:
            return loc.view(batch, -1, 4), conf.view(batch, -1, self.num_classes), priors.view(1, 2, -1, 4)
        with no_nncf_trace():
            if self.loss_inference is True:
                return loc.view(batch, -1, 4), conf.view(batch, -1, self.num_classes), priors.view(1, 2, -1, 4)
            return self.detection_output(loc, conf_softmax.view(batch, -1), priors)


class MultiOutputSequential(nn.Sequential):

    def __init__(self, outputs, modules):
        super().__init__(*modules)
        self.outputs = [str(o) for o in outputs]

    def forward(self, x):
        outputs = []
        for name, module in self._modules.items():
            x = module(x)
            if name in self.outputs:
                outputs.append(x)
        return outputs, x


def conv_bn(inp, oup, kernel, stride, padding):
    return nn.Sequential(nn.Conv2d(inp, oup, kernel, stride, padding, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True))


def extra_layers(start_input_channels):
    return MultiOutputSequential([1, 3, 5, 7], [conv_bn(start_input_channels, 256, 1, 1, 0), conv_bn(256, 512, 3, 2, 1), conv_bn(512, 128, 1, 1, 0), conv_bn(128, 256, 3, 2, 1), conv_bn(256, 128, 1, 1, 0), conv_bn(128, 256, 3, 2, 1), conv_bn(256, 64, 1, 1, 0), conv_bn(64, 128, 3, 2, 1)])


def conv_dw(inp, oup, stride):
    return nn.Sequential(nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), nn.ReLU(inplace=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True))


def mobilenet(start_input_channels=3):
    model = MultiOutputSequential([11, 13], [conv_bn(start_input_channels, 32, 3, 2, 1), conv_dw(32, 64, 1), conv_dw(64, 128, 2), conv_dw(128, 128, 1), conv_dw(128, 256, 2), conv_dw(256, 256, 1), conv_dw(256, 512, 2), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 1024, 2), conv_dw(1024, 1024, 1)])
    return model


class MobileNetSSD(nn.Module):

    def __init__(self, num_classes, cfg):
        super().__init__()
        self.cfg = cfg
        self.num_classes = num_classes
        self.basenet = mobilenet()
        self.extras = extra_layers(1024)
        NUM_INPUT_FEATURES = [512, 1024, 512, 256, 256, 128]
        self.detection_head = SSDDetectionOutput(NUM_INPUT_FEATURES, num_classes, cfg)

    def forward(self, x):
        img_tensor = x[0].clone().unsqueeze(0)
        sources, x = self.basenet(x)
        extra_sources, x = self.extras(x)
        return self.detection_head(sources + extra_sources, img_tensor)


BASE_NUM_OUTPUTS = {(300): [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], (512): [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512]}


BASE_OUTPUT_INDICES = {(300): [12], (512): [12]}


EXTRAS_NUM_OUTPUTS = {(300): [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], (512): [256, 'S', 512, 128, 'S', 256, 128, 'S', 256, 128, 'S', 256, 128, 'K', 256]}


EXTRA_OUTPUT_INDICES = {(300): [2, 5, 7, 9], (512): [2, 5, 8, 11, 14]}


def make_ssd_vgg_layer(input_features, output_features, kernel=3, padding=1, dilation=1, modifier=None, batch_norm=False):
    stride = 1
    if modifier == 'S':
        stride = 2
        padding = 1
    elif modifier == 'K':
        kernel = 4
        padding = 1
    layer = [nn.Conv2d(input_features, output_features, kernel_size=kernel, stride=stride, padding=padding, dilation=dilation)]
    if batch_norm:
        layer.append(nn.BatchNorm2d(output_features))
    layer.append(nn.ReLU(inplace=True))
    return layer


def build_vgg_ssd_extra(num_outputs, output_indices, statrt_input_channels=1024, batch_norm=False):
    extra_layers = []
    output_num_features = []
    source_indices = []
    in_planes = statrt_input_channels
    modifier = None
    kernel_sizes = 1, 3
    for i, out_planes in enumerate(num_outputs):
        if isinstance(out_planes, str):
            modifier = out_planes
            continue
        kernel = kernel_sizes[len(extra_layers) % 2]
        extra_layers.extend(make_ssd_vgg_layer(in_planes, out_planes, modifier=modifier, kernel=kernel, padding=0, batch_norm=batch_norm))
        modifier = None
        in_planes = out_planes
        if i in output_indices:
            source_indices.append(len(extra_layers) - 1)
            output_num_features.append(out_planes)
    return extra_layers, source_indices, output_num_features


def build_vgg_ssd_layers(num_outputs, output_inddices, start_input_channels=3, batch_norm=False):
    vgg_layers = []
    output_num_features = []
    source_indices = []
    in_planes = start_input_channels
    modifier = None
    for i, out_planes in enumerate(num_outputs):
        if out_planes in ('M', 'C'):
            vgg_layers.append(nn.MaxPool2d(kernel_size=2, stride=2, padding=1 if modifier == 'C' else 0))
            continue
        if isinstance(out_planes, str):
            modifier = out_planes
            continue
        vgg_layers.extend(make_ssd_vgg_layer(in_planes, out_planes, modifier=modifier, batch_norm=batch_norm))
        modifier = None
        in_planes = out_planes
        if i in output_inddices:
            source_indices.append(len(vgg_layers) - 1)
            output_num_features.append(out_planes)
    vgg_layers.append(nn.MaxPool2d(kernel_size=3, stride=1, padding=1))
    vgg_layers.extend(make_ssd_vgg_layer(in_planes, 1024, kernel=3, padding=6, dilation=6, batch_norm=batch_norm))
    vgg_layers.extend(make_ssd_vgg_layer(1024, 1024, kernel=1, batch_norm=batch_norm))
    source_indices.append(len(vgg_layers) - 1)
    output_num_features.append(1024)
    return vgg_layers, source_indices, output_num_features


class SSD_VGG(nn.Module):

    def __init__(self, cfg, size, num_classes, batch_norm=False):
        super().__init__()
        self.config = cfg
        self.num_classes = num_classes
        self.size = size
        self.enable_batchmorm = batch_norm
        base_layers, base_outs, base_feats = build_vgg_ssd_layers(BASE_NUM_OUTPUTS[size], BASE_OUTPUT_INDICES[size], batch_norm=batch_norm)
        extra_layers, extra_outs, extra_feats = build_vgg_ssd_extra(EXTRAS_NUM_OUTPUTS[size], EXTRA_OUTPUT_INDICES[size], batch_norm=batch_norm)
        self.basenet = MultiOutputSequential(base_outs, base_layers)
        self.extras = MultiOutputSequential(extra_outs, extra_layers)
        self.detection_head = SSDDetectionOutput(base_feats + extra_feats, num_classes, cfg)
        self.L2Norm = L2Norm(512, 20, 1e-10)

    def forward(self, x):
        img_tensor = x[0].clone().unsqueeze(0)
        sources, x = self.basenet(x)
        sources[0] = self.L2Norm(sources[0])
        extra_sources, x = self.extras(x)
        return self.detection_head(sources + extra_sources, img_tensor)

    def load_weights(self, base_file):
        _, ext = os.path.splitext(base_file)
        if ext in ['.pkl', '.pth']:
            None
            self.load_state_dict(torch.load(base_file, map_location=lambda storage, loc: storage))
            None
        else:
            None


KernelSizeType = int


NNCFNodeName = str


class ElasticKernelOp:
    """
    Base class for introducing elastic kernel for the operations. On the forward pass it takes parameters of operations
    and modifies in the way that kernel size is changing to a given value.
    """

    def __init__(self, max_kernel_size: KernelSizeType, node_name: NNCFNodeName):
        """
        Constructor.

        :param max_kernel_size: maximum kernel size value in the original operation.
        :param node_name: string representation of operation address. It's used for more informative messages only.
        """
        super().__init__()
        self._kernel_size_list = []
        self._active_kernel_size = max_kernel_size
        self._max_kernel_size = max_kernel_size
        self._node_name = node_name

    def get_active_kernel_size(self) ->KernelSizeType:
        """
        :return: a target kernel size that operation's parameters should have after forward's call
        """
        return self._active_kernel_size

    def set_active_kernel_size(self, kernel_size: KernelSizeType) ->None:
        """
        Sets current level of elasticity for the operation - kernel size value that parameters should have.
        The actual modification of parameters happens on forward call.
        The value should be less the original kernel size and more than one.

        :param kernel_size: kernel size value
        """
        if kernel_size is None or kernel_size > self.max_kernel_size or kernel_size < 1:
            raise AttributeError('Invalid kernel size={} in scope={}.\nIt should be within the range: [1, {}]'.format(kernel_size, self.node_name, self.max_kernel_size))
        self._active_kernel_size = kernel_size

    @property
    def kernel_size_list(self) ->List[KernelSizeType]:
        """
        Gets list of all available kernel sizes to select from. Each value corresponds to a single element in the
        search space of operation. The search space of the model is cartesian product of search spaces of operation.

        :return: list of kernel sizes
        """
        return self._kernel_size_list

    @property
    def node_name(self) ->NNCFNodeName:
        """
        :return: node name that corresponds to operation with elastic kernel
        """
        return self._node_name

    @property
    def max_kernel_size(self) ->KernelSizeType:
        """
        :return: kernel size in the original operation
        """
        return self._max_kernel_size


class BaseElasticityParams:

    @classmethod
    def from_config(cls, config: Dict[str, Any]) ->'BaseElasticityParams':
        """
        Creates the object from its config.
        """

    @classmethod
    @abstractmethod
    def from_state(cls, state: Dict[str, Any]) ->'BaseElasticityParams':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """

    @abstractmethod
    def get_state(self) ->Dict[str, Any]:
        """
        Returns the compression loss state.

        :return: The compression loss state.
        """


class EKParamsStateNames:
    MAX_NUM_KERNELS = 'max_num_kernels'


ELASTICITY_PARAMS = Registry('Elasticity builder', add_name_as_attr=True)


class ElasticityDim(Enum):
    """
    Defines elasticity dimension or type of elasticity applied to the model
    """
    KERNEL = 'kernel'
    WIDTH = 'width'
    DEPTH = 'depth'


class ElasticKernelParams(BaseElasticityParams):
    _state_names = EKParamsStateNames

    def __init__(self, max_num_kernels: int=-1):
        """
        Constructor

        :param max_num_kernels: Restricts total number of different elastic kernel values for each layer.
        The default value is -1 means that there's no restrictions.
        """
        self.max_num_kernels = max_num_kernels

    @classmethod
    def from_config(cls, config: Dict[str, Any]) ->'ElasticKernelParams':
        """
        Creates the object from its config.
        """
        kwargs = {cls._state_names.MAX_NUM_KERNELS: config.get(cls._state_names.MAX_NUM_KERNELS, -1)}
        return cls(**kwargs)

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'ElasticKernelParams':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        return cls(**state)

    def get_state(self) ->Dict[str, Any]:
        """
        Returns the compression loss state.

        :return: The compression loss state.
        """
        return {self._state_names.MAX_NUM_KERNELS: self.max_num_kernels}

    def __eq__(self, other: 'ElasticKernelParams') ->bool:
        return self.__dict__ == other.__dict__


class ElasticKernelConv2DOp(ElasticKernelOp, nn.Module):
    """
    Introduces elastic kernel for the 2D convolution. On the forward pass it takes parameters of operations
    and modifies in the way that kernel size is changing to a given value.
    """

    def __init__(self, max_kernel_size: KernelSizeType, node_name: NNCFNodeName, params: ElasticKernelParams):
        """
        Constructor.

        :param max_kernel_size: maximum kernel size value in the original operation.
        :param node_name: string representation of operation address. It's used for more informative messages only.
        :param params: parameters to configure elastic kernel for the operation.
        """
        super().__init__(max_kernel_size=max_kernel_size, node_name=node_name)
        self._max_num_params = params.max_num_kernels
        self._kernel_size_list = self.generate_kernel_size_list(max_kernel_size)
        self._ks_set = list(set(self.kernel_size_list))
        self._ks_set.sort()
        scale_params = {}
        for i in range(len(self._ks_set) - 1):
            ks_small = self._ks_set[i]
            ks_larger = self._ks_set[i + 1]
            param_name = '%dto%d' % (ks_larger, ks_small)
            scale_params['%s_matrix' % param_name] = Parameter(torch.eye(ks_small ** 2))
        for name, param in scale_params.items():
            self.register_parameter(name, param)

    def generate_kernel_size_list(self, max_kernel_size: KernelSizeType) ->List[KernelSizeType]:
        """
        Generates list of available kernel size values.

        :param max_kernel_size: maximum value of kernel size, it's supposed to be odd
        :return: list of kernel size values.
        """
        DEFAULT_KERNEL_SIZE_STEP = 2
        assert max_kernel_size % 2 > 0, 'kernel size should be odd number'
        if max_kernel_size == 1:
            return [1]
        kernel = max_kernel_size
        ks_list = []
        while kernel > 1:
            ks_list.append(kernel)
            kernel -= DEFAULT_KERNEL_SIZE_STEP
            if self._max_num_params == len(ks_list):
                break
        return ks_list

    def forward(self, weight: torch.Tensor) ->torch.Tensor:
        """
        Modifies weight to have kernel size equals to active kernel size value.

        :param weight: weight tensor to be modified
        :return: modified weight
        """
        kernel_size = self.get_active_kernel_size()
        nncf_logger.debug(f'Conv2d with active kernel size={kernel_size} in scope={self.node_name}')
        result = weight
        if is_tracing_state():
            with no_jit_trace():
                if kernel_size > 1:
                    result = self._get_active_filter(kernel_size, weight).detach()
        elif kernel_size > 1:
            result = self._get_active_filter(kernel_size, weight)
        return result

    def set_active_kernel_size(self, kernel_size: KernelSizeType) ->None:
        """
        Sets current level of elasticity for the operation - kernel size value that parameters should have.
        The actual modification of parameters happens on forward call.
        The value should be less the original kernel size and more than one.

        :param kernel_size: kernel size value
        """
        nncf_logger.debug(f'set active elastic_kernel={kernel_size} in scope={self.node_name}')
        assert kernel_size % 2 > 0, 'kernel size should be odd number'
        if kernel_size not in self.kernel_size_list and kernel_size != self.max_kernel_size:
            raise ValueError('invalid kernel size to set. Should be a number in {}'.format(self.kernel_size_list))
        super().set_active_kernel_size(kernel_size)

    @staticmethod
    def _sub_filter_start_end(kernel_size, sub_kernel_size):
        center = kernel_size // 2
        dev = sub_kernel_size // 2
        start, end = center - dev, center + dev + 1
        assert end - start == sub_kernel_size
        return start, end

    def _get_active_filter(self, kernel_size, weight):
        max_kernel_size = max(self.kernel_size_list)
        start, end = self._sub_filter_start_end(max_kernel_size, kernel_size)
        filters = weight[:, :, start:end, start:end]
        if kernel_size < max_kernel_size:
            start_filter = weight
            for i in range(len(self._ks_set) - 1, 0, -1):
                src_ks = self._ks_set[i]
                if src_ks <= kernel_size:
                    break
                target_ks = self._ks_set[i - 1]
                start, end = self._sub_filter_start_end(src_ks, target_ks)
                _input_filter = start_filter[:, :, start:end, start:end]
                _input_filter = _input_filter.contiguous()
                _input_filter = _input_filter.view(_input_filter.size(0), _input_filter.size(1), -1)
                _input_filter = _input_filter.view(-1, _input_filter.size(2))
                _input_filter = F.linear(_input_filter, self.__getattr__('%dto%d_matrix' % (src_ks, target_ks)))
                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks ** 2)
                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks, target_ks)
                start_filter = _input_filter
            filters = start_filter
        return filters


WidthType = int


class ElasticWidthOp:
    """
    Base class for introducing elastic width for the operations. On the forward pass it takes parameters of operations
    and trims its input channels (elastic input width) or output channels (elastic output width). This class produces
    2 groups of classes with prefixes ElasticOutputWidth and ElasticInputWidth correspondingly.
    """

    def __init__(self, max_width: int, node_name: str):
        """
        Constructor.

        :param max_width: maximum number of channels in the original layer.
        :param node_name: string representation of operation address. It's used for more informative messages only.
        """
        super().__init__()
        self._active_width = max_width
        self._max_width = max_width
        self._node_name = node_name

    @property
    def max_width(self) ->WidthType:
        """
        :return: maximum number of channels specified on creation of the object.
        """
        return self._max_width

    def get_active_width(self) ->WidthType:
        """
        :return: number of channels to trim on forward call
        """
        return self._active_width

    def set_active_width(self, width: WidthType) ->None:
        """
        Sets current level of elasticity for the operation - number of channels to trim.
        The actual trimming of specified number of channels happens on forward call.
        The value should be less the original width and more than one. Zero number of channels is
        supported through Dynamic Depth feature.

        :param width: number of channels
        """
        if width is None or width > self._max_width or width < 1:
            raise AttributeError('Invalid width={} in scope={}.\nIt should be within the range: [1, {}]'.format(width, self._node_name, self._max_width))
        self._active_width = width


class ElasticInputWidthLinearOp(ElasticWidthOp, nn.Module):
    """
    Introduces elastic input width for linear layer.
    """

    def forward(self, weight: torch.Tensor) ->torch.Tensor:
        """
        Trims weight according to active number of input channels

        :param weight: weight tensor to be trimmed
        :return: trimmed weight
        """
        return weight[:, :self._active_width]


class ElasticInputWidthConvOp(ElasticWidthOp, nn.Module):
    """
    Introduces elastic input width for 2D convolution.
    """

    def forward(self, weight: torch.Tensor) ->torch.Tensor:
        """
        Trims weight according to active number of input channels

        :param weight: weight tensor to be trimmed
        :return: trimmed weight
        """
        return weight[:, :self._active_width, :, :]


class ElasticInputWidthDWConvOp(ElasticWidthOp, nn.Module):
    """
    Introduces elastic input width for depthwise convolution.
    """

    def forward(self, _) ->int:
        """
        :return: number of input channels to be trimmed. In case of depthwise convolution no need to trim weights, just
        need to change number of group accordingly.
        """
        return self._active_width


class ElasticInputWidthBatchNormOp(ElasticWidthOp, nn.Module):
    """
    Introduces elastic input width for batchnorm layer.
    """
    SET_RUNNING_STATISTICS = False

    def forward(self, **bn_params: torch.Tensor) ->List[torch.Tensor]:
        """
        Trims batchnorm parameters according to active number of input channels.

        :param bn_params: map of name and tensor to be trimmed
        :return: trimmed batchnorm parameters
        """
        return [param[:self._active_width] for param in bn_params.values()]


class ElasticInputWidthLayerNormOp(ElasticWidthOp, nn.Module):
    """
    Introduces elastic input width for layernorm layer.
    """

    def forward(self, weight: torch.Tensor, bias: torch.Tensor, normalized_shape: torch.Tensor) ->List[torch.Tensor]:
        """
        Trims layernorm parameters according to active number of input channels.
        :param weight: weight tensor to be trimmed
        :param bias: bias tensor to be trimmed
        :param normalized_shape: normalized_shape to be trimmed
        :return: list of trimmed layernorm parameters
        """
        assert len(normalized_shape) == 1, 'Currently only 1-dimensional shape is supported.'
        return [weight[:self._active_width], bias[:self._active_width], (self._active_width,)]


class EWParamsStateNames:
    MIN_WIDTH = 'min_width'
    MAX_NUM_WIDTHS = 'max_num_widths'
    WIDTH_STEP = 'width_step'
    WIDTH_MULTIPLIERS = 'width_multipliers'
    FILTER_IMPORTANCE = 'filter_importance'
    OVERWRITE_GROUPS = 'overwrite_groups'
    OVERWRITE_GROUPS_WIDTHS = 'overwrite_groups_widths'
    ADD_DYNAMIC_INPUTS = 'add_dynamic_inputs'


class ElasticWidthParams(BaseElasticityParams):
    _state_names = EWParamsStateNames

    def __init__(self, min_width: int, max_num_widths: int, width_step: int, width_multipliers: List[float], filter_importance: str, overwrite_groups: Optional[List[str]]=None, overwrite_groups_widths: Optional[List[str]]=None, add_dynamic_inputs: Optional[List[str]]=None):
        """
        Constructor

        :param min_width: Minimal number of output channels that can be activated for each layers with elastic width.
        Default value is 32.
        :param max_num_widths: Restricts total number of different elastic width values for each layer.
        The default value is -1 means that there's no restrictions.
        :param width_step: Defines a step size for a generation of the elastic width search space - the list of all
        possible width values for each layer. The generation starts from the number of output channels in the original
        model and stops when it reaches whether a `min_width` width value or number of generated width values
        equal to `max_num_widths`.
        This parameter is mutually exclusive with `width_multipliers`.
        :param width_multipliers: Defines elastic width search space via a list of multipliers. All possible width
        values are obtained by multiplying the original width value with the values in the given list.
        The obtained values are rounded to the nearest smaller value divisible by alignment constant (e.g. 8).
        This parameter is mutually exclusive with `width_step`.
        :param filter_importance: The type of filter importance metric. Can be one of `L1`, `L2`, `geometric_median`.
        `L1` by default.
        """
        self.min_width = min_width
        self.max_num_widths = max_num_widths
        self.width_step = width_step
        self.width_multipliers = width_multipliers
        self.filter_importance = filter_importance
        self.overwrite_groups = overwrite_groups
        self.overwrite_groups_widths = overwrite_groups_widths
        self.add_dynamic_inputs = add_dynamic_inputs

    @classmethod
    def from_config(cls, config: Dict[str, Any]) ->'ElasticWidthParams':
        """
        Creates the object from its config.
        """
        kwargs = {cls._state_names.MIN_WIDTH: config.get(cls._state_names.MIN_WIDTH, 32), cls._state_names.MAX_NUM_WIDTHS: config.get(cls._state_names.MAX_NUM_WIDTHS, -1), cls._state_names.WIDTH_STEP: config.get(cls._state_names.WIDTH_STEP, 32), cls._state_names.WIDTH_MULTIPLIERS: config.get(cls._state_names.WIDTH_MULTIPLIERS), cls._state_names.FILTER_IMPORTANCE: config.get(cls._state_names.FILTER_IMPORTANCE, 'L1'), cls._state_names.OVERWRITE_GROUPS: config.get(cls._state_names.OVERWRITE_GROUPS, None), cls._state_names.OVERWRITE_GROUPS_WIDTHS: config.get(cls._state_names.OVERWRITE_GROUPS_WIDTHS, None), cls._state_names.ADD_DYNAMIC_INPUTS: config.get(cls._state_names.ADD_DYNAMIC_INPUTS, None)}
        return cls(**kwargs)

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'ElasticWidthParams':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        return cls(**state)

    def get_state(self) ->Dict[str, Any]:
        """
        Returns the compression loss state.

        :return: The compression loss state.
        """
        return {self._state_names.MIN_WIDTH: self.min_width, self._state_names.MAX_NUM_WIDTHS: self.max_num_widths, self._state_names.WIDTH_STEP: self.width_step, self._state_names.WIDTH_MULTIPLIERS: self.width_multipliers, self._state_names.FILTER_IMPORTANCE: self.filter_importance, self._state_names.OVERWRITE_GROUPS: self.overwrite_groups, self._state_names.OVERWRITE_GROUPS_WIDTHS: self.overwrite_groups_widths, self._state_names.ADD_DYNAMIC_INPUTS: self.add_dynamic_inputs}

    def __eq__(self, other: 'ElasticWidthParams') ->bool:
        return self.__dict__ == other.__dict__

    def __str__(self):
        return f'{self.__class__.__name__}: width_step: {self.width_step} min_width: {self.min_width} width_multipliers: {self.width_multipliers} max_num_widths: {self.max_num_widths} overwrite_groups: {self.overwrite_groups} overwrite_group_widths: {self.overwrite_groups_widths}'


class ElasticOutputWidthOp(ElasticWidthOp):
    """
    Base class for trimming output channels (output width) of the operations.
    """

    def __init__(self, max_width: int, node_name: str, params: ElasticWidthParams, fixed_width_list: Optional[List[int]]=None):
        """
        Constructor.

        :param max_width: maximum number of output channels in the original operation.
        :param node_name: string representation of operation address. It's used for more informative messages only.
        :param params: parameters to configure elastic width for the operation.
        """
        super().__init__(max_width=max_width, node_name=node_name)
        if fixed_width_list is None:
            fixed_width_list = []
        if fixed_width_list:
            fixed_width_list.sort(reverse=True)
            if fixed_width_list[0] > max_width:
                raise RuntimeError(f'Width list for {node_name} contains invalid values: {fixed_width_list}, {max_width}')
            if fixed_width_list[0] != max_width:
                raise RuntimeError(f'Max width for {node_name} is not aligned with pre-trained model')
            self._width_list = fixed_width_list
        else:
            self._width_list = self._generate_width_list(self._max_width, params)

    @property
    def width_list(self) ->List[int]:
        """
        list of all available widths to select from. Each value corresponds to a single element in the search space of
        operation. The search space of the model is cartesian product of search spaces of operation.
        If all widths starting from 1 to maximum number of channels with step size 1 are available, the search space
        would be prohibitively large to efficiently train and search.

        That's why there are elastic width parameters that constraint number of all available widths.

        :return: list of widths
        """
        return self._width_list

    def set_active_width(self, width: int) ->None:
        """
        Sets current level of elasticity for the operation - number of output channels to trim.
        The actual trimming of specified number of channels happens on forward call.
        The value should be less the original width and more than one. Zero number of channels is
        supported through Dynamic Depth feature.

        :param width: number of output channels
        """
        if width not in self.width_list and width != self.max_width:
            raise ValueError(f'Invalid number of output channels to set: {width} in scope={self._node_name}. Should be a number in {self.width_list}')
        super().set_active_width(width)

    @staticmethod
    def _generate_width_list(max_width: int, params: ElasticWidthParams) ->List[int]:
        """
        Generates list of available width values.
        There are two mutually exclusive modes: using `width_step` and using `width_multipliers`.
        The mode with `width_step` defines a step size for a generation of the elastic width search space - the list of
        all possible width values for each layer. The generation starts from the number of output channels in the
        original model and stops when it reaches whether a `min_width` width value or number of generated width values
        equal to `max_num_widths`.
        The mode with `width_multipliers` defines elastic width search space via a list of multipliers. All possible
        width values are obtained by multiplying the original width value with the values in the given list.
        The obtained values are rounded to the nearest smaller value divisible by alignment constant (e.g. 8).

        :param max_width: maximum value of width
        :param params: parameters to configure elastic width for the operation.
        :return: list of available width values.
        """
        ALIGNMENT_CONSTANT_FOR_MULTIPLIERS = 8
        width_list = []
        p = params
        if max_width <= p.min_width:
            width_list.append(max_width)
        elif not p.width_multipliers:
            width = max_width
            width_list.append(width)
            width -= p.width_step
            while width >= p.min_width:
                if p.max_num_widths == len(width_list):
                    break
                width_list.append(width)
                width -= p.width_step
        else:
            p.width_multipliers.sort(reverse=True)
            if p.width_multipliers[0] < 1:
                width_list.append(max_width)
            for multiplier in p.width_multipliers:
                if p.max_num_widths == len(width_list):
                    break
                if 0 >= multiplier > 1:
                    raise RuntimeError(f'Wrong value for multiplier: {multiplier}')
                w = int(max_width * multiplier)
                w = w - w % ALIGNMENT_CONSTANT_FOR_MULTIPLIERS
                w = max(w, p.min_width)
                if w in width_list:
                    continue
                width_list.append(w)
        return width_list


class ElasticOutputWidthConv2DOp(ElasticOutputWidthOp, nn.Module):
    """
    Introduces elastic output width for 2D convolution.
    """

    def forward(self, weight: torch.Tensor, bias: Optional[torch.Tensor]) ->List[torch.Tensor]:
        """
        Trims convolution parameters according to active number of output channels.

        :param weight: weight tensor to be trimmed
        :param bias: bias tensor to be trimmed
        :return: list of trimmed convolution parameters
        """
        nncf_logger.debug(f'Conv2d with active width={self._active_width} in scope={self._node_name}')
        num_out_channels = self._active_width
        new_bias = None if bias is None else bias[:num_out_channels]
        new_weights = weight[:num_out_channels, :, :, :]
        return [new_weights, new_bias]


class ElasticOutputWidthLinearOp(ElasticOutputWidthOp, nn.Module):
    """
    Introduces elastic output width for linear layer.
    """

    def forward(self, weight: torch.Tensor, bias: Optional[torch.Tensor]) ->List[torch.Tensor]:
        """
        Trims linear layer's parameters according to active number of output channels.

        :param weight: weight tensor to be trimmed
        :param bias: bias tensor to be trimmed
        :return: list of trimmed linear parameters
        """
        new_bias = None if bias is None else bias[:self._active_width]
        return [weight[:self._active_width, :], new_bias]


COMPRESSION_MODULES = Registry('compression modules')


class BinaryMask(nn.Module):

    def __init__(self, shape: List[int]):
        super().__init__()
        self.register_buffer('_binary_mask', torch.ones(shape))
        self.frozen = False

    @property
    def binary_mask(self):
        return self._binary_mask

    @binary_mask.setter
    def binary_mask(self, tensor):
        with torch.no_grad():
            self._binary_mask.set_(tensor)

    def forward(self, weight):
        if is_tracing_state():
            return weight.mul(self.binary_mask)
        tmp_tensor = self._calc_training_binary_mask(weight)
        return apply_binary_mask_impl(tmp_tensor, weight)

    def _calc_training_binary_mask(self, weight):
        return self.binary_mask

    def apply_binary_mask(self, weight):
        return apply_binary_mask_impl(self.binary_mask, weight)


class CompressionParameter(nn.Parameter):
    """
    The class that should be used in all compression algorithms instead of torch.nn.Parameter.

    This class utilize `compression_lr_multiplier` parameter from :class:`nncf.NNCFConfig`
    to increase/decrease gradients for compression algorithms' parameters.
    """

    def __new__(cls, data: torch.Tensor=None, requires_grad: bool=True, compression_lr_multiplier: float=None):
        return super().__new__(cls, data, requires_grad=requires_grad)

    def __init__(self, data: torch.Tensor=None, requires_grad: bool=True, compression_lr_multiplier: float=None):
        """

        Args:
            data: Parameter tensor
            requires_grad: If the parameter requires gradient
            compression_lr_multiplier: Multiplier for gradient values
        """
        super().__init__()
        if compression_lr_multiplier is not None and self.dtype.is_floating_point:
            self.requires_grad = True
            self.register_hook(lambda grad: compression_lr_multiplier * grad)
            self.requires_grad = requires_grad


class STThreshold(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_, threshold: float=0.5):
        output = (input_ > threshold).type(input_.dtype)
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0], None


def binary_mask(mask):
    return STThreshold.apply(torch.sigmoid(mask))


class MaskCalculationHook:

    def __init__(self, module):
        self.hook = module._register_state_dict_hook(self.hook_fn)

    def hook_fn(self, module, destination, prefix, local_metadata):
        module.binary_mask = binary_mask(module.mask)
        destination[prefix + '_binary_mask'] = module.binary_mask
        return destination

    def close(self):
        self.hook.remove()


LayerName = str


INPUT_NOOP_METATYPES = Registry('input_noop_metatypes')


OUTPUT_NOOP_METATYPES = Registry('output_noop_metatypes')


class OperatorMetatype:
    """
    Base class for grouping framework operators based on their semantic meaning.
    """
    name = ''
    hw_config_names = []

    @classmethod
    def get_all_aliases(cls) ->List[str]:
        """
        Returns a list of the framework operator aliases.

        :return: A list of the framework operator aliases.
        """
        return []

    @classmethod
    def get_subtypes(cls) ->List[Type['OperatorMetatype']]:
        """
        Returns a list of 'OperatorMetatype' that are subtypes.

        :return: A subtype list.
        """
        return []

    @classmethod
    def subtype_check(cls, metatype: Type['OperatorMetatype']) ->bool:
        """
        Check if a metatype is a subtype.

        :param metatype: An operator metatype.
        :return: True if metatype is a subtype otherwise False
        """
        subtypes = cls.get_subtypes()
        if metatype == cls or metatype in subtypes:
            return True
        return any(subtype.subtype_check(metatype) for subtype in subtypes)


class NNCFNode:
    """
    Class describing nodes used in NNCFGraph.
    """

    def __init__(self, node_id: int, node_name: NNCFNodeName, data: dict=None):
        self.node_id = node_id
        self.data = data if data else {}
        self.data[NNCFGraph.NODE_NAME_ATTR] = node_name

    @property
    def node_name(self) ->NNCFNodeName:
        return self.data.get(NNCFGraph.NODE_NAME_ATTR)

    @property
    def metatype(self) ->Type[OperatorMetatype]:
        return self.data.get(NNCFGraph.METATYPE_ATTR)

    @property
    def node_type(self) ->str:
        return self.data.get(NNCFGraph.NODE_TYPE_ATTR)

    @property
    def layer_name(self) ->LayerName:
        return self.data.get(NNCFGraph.LAYER_NAME_ATTR)

    @property
    def layer_attributes(self) ->BaseLayerAttributes:
        return self.data.get(NNCFGraph.LAYER_ATTRIBUTES)

    @layer_attributes.setter
    def layer_attributes(self, data: Any) ->None:
        self.data[NNCFGraph.LAYER_ATTRIBUTES] = data

    @property
    def ignored_algorithms(self) ->List[str]:
        return self.data.get(NNCFGraph.IGNORED_ALGOS_ATTR, [])

    def is_in_iteration_scope(self) ->bool:
        return self.data.get(NNCFGraph.IS_IN_ITERATION_SCOPE_NODE_ATTR, False)

    def is_integer_input(self) ->bool:
        return self.data.get(NNCFGraph.IS_INTEGER_INPUT_NODE_ATTR, False)

    def is_shared(self) ->bool:
        return self.data.get(NNCFGraph.IS_SHARED_ATTR, False)

    def __repr__(self):
        return str(self)

    def __str__(self):
        return ' '.join([str(self.node_id), self.node_name, self.node_type])

    def __hash__(self):
        return hash(str(self))

    def __eq__(self, other):
        return isinstance(other, NNCFNode) and self.node_id == other.node_id and self.data == other.data and self.node_type == other.node_type and self.layer_attributes == other.layer_attributes


class SparseStructure(Enum):
    FINE = 'fine'
    BLOCK = 'block'
    PER_DIM = 'per_dim'


class SparseConfig:
    """
    Defines the sparse structure config with required options for a certain supported layer.
    """

    def __init__(self, mode: SparseStructure, sparse_factors: Optional[Tuple[int, int]]=None, sparse_axis: Optional[int]=None):
        """
        Parses and validates the sparse structure of a certain layer for movement sparsity.

        :param mode: The sparse structure mode.
        :param sparse_factors: Block shape to sparsify as a whole in a weight. Required when `mode` is "block".
        :param sparse_axis: The dimension to sparsify in a weight. Required when `mode` is "per_dim".
        """
        error_prefix = 'Invalid sparse config.'
        self.sparse_factors = None
        self.sparse_axis = None
        self.mode = mode
        if self.mode == SparseStructure.FINE:
            if not (isinstance(sparse_factors, (tuple, list)) and tuple(sparse_factors) == (1, 1) or sparse_factors is None):
                raise ValueError(f'{error_prefix} Fine sparse structure expects `sparse_factors` to be [1, 1] or unspecified.')
            if sparse_axis is not None:
                raise ValueError(f'{error_prefix} Fine sparse structure does not expect specified `axis`.')
            self.sparse_factors = 1, 1
        if self.mode == SparseStructure.BLOCK:
            if sparse_factors is None:
                raise ValueError(f'{error_prefix} Missing `sparse_factors`. Block sparsity structure expects it specified.')
            if not (isinstance(sparse_factors, (tuple, list)) and len(sparse_factors) == 2):
                raise ValueError(f'{error_prefix} Invalid format of `sparse_factors. Block sparsity structure expects tuple of two numbers.')
            if sparse_axis is not None:
                raise ValueError(f'{error_prefix} Block sparse structure does not expect specified `axis`.')
            self.sparse_factors = tuple(sparse_factors)
        if self.mode == SparseStructure.PER_DIM:
            if sparse_axis is None:
                raise ValueError(f'{error_prefix} Missing `axis`. Per-dim sparsity structure expects it to be specified.')
            if sparse_factors is not None:
                raise ValueError(f'{error_prefix} Per-dim sparsity structure does not expect specified `sparse_factors`.')
            self.sparse_axis = int(sparse_axis)

    @classmethod
    def from_config(cls, config: Dict[str, Any]) ->'SparseConfig':
        """
        Creates the object from its config.

        :param config: A dict that describes the sparse structure.
        """
        mode_str = config.get('mode', SparseStructure.FINE.value)
        mode = SparseStructure(mode_str)
        sparse_factors = config.get('sparse_factors')
        axis = config.get('axis')
        return cls(mode, sparse_factors, axis)

    def __str__(self) ->str:
        return f'{self.mode.value, self.sparse_factors}'


class NamespaceTarget(Enum):
    """
    NamespaceTarget stores modules from which patched operators were obtained.
    """
    TORCH_NN_FUNCTIONAL = 'torch.nn.functional'
    TORCH_TENSOR = 'torch.tensor'
    TORCH = 'torch'
    EXTERNAL = 'external_function'


class PatchedOperatorInfo:

    def __init__(self, name: str, operator_namespace: NamespaceTarget, skip_trace: bool=False):
        """
        Information about patched operator.
        :param name: Operator name
        :param operator_namespace: Python module, from which operator was gotten.
        :param skip_trace: If it is set to True, the both operator and its internal calls
         to otherwise traced functions do not appear into the model graph.
        """
        self.name = name
        self.operator_namespace = operator_namespace
        self.skip_trace = skip_trace


def dict_update(src: Dict, dst: Dict, recursive: bool=True):
    for name, value in src.items():
        if recursive and name in dst and isinstance(value, dict):
            dict_update(value, dst[name], recursive)
        else:
            dst[name] = value


def maybe_reapply_weight_norm(src: torch.nn.Module, dst: torch.nn.Module) ->torch.nn.Module:
    for k, hook in src._forward_pre_hooks.items():
        if isinstance(hook, WeightNorm):
            hook.remove(dst)
            del dst._forward_pre_hooks[k]
            name = hook.name
            dim = hook.dim
            WeightNorm.apply(dst, name=name, dim=dim)
    return dst


def align_module_internals(src: torch.nn.Module, dst: torch.nn.Module) ->torch.nn.Module:
    dict_update(src.__dict__, dst.__dict__)
    dst = maybe_reapply_weight_norm(src, dst)
    return dst


class NNCFBatchNorm1d(_NNCFModuleMixin, nn.BatchNorm1d):
    op_func_name = 'batch_norm'
    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.BatchNorm1d.__name__
        nncf_bn = NNCFBatchNorm1d(module.num_features)
        nncf_bn = align_module_internals(module, nncf_bn)
        return nncf_bn


class NNCFBatchNorm2d(_NNCFModuleMixin, nn.BatchNorm2d):
    op_func_name = 'batch_norm'
    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.BatchNorm2d.__name__
        nncf_bn = NNCFBatchNorm2d(module.num_features)
        nncf_bn = align_module_internals(module, nncf_bn)
        return nncf_bn


class NNCFBatchNorm3d(_NNCFModuleMixin, nn.BatchNorm3d):
    op_func_name = 'batch_norm'
    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.BatchNorm3d.__name__
        nncf_bn = NNCFBatchNorm3d(module.num_features)
        nncf_bn = align_module_internals(module, nncf_bn)
        return nncf_bn


class NNCFConv1d(_NNCFModuleMixin, nn.Conv1d):
    op_func_name = 'conv1d'

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.Conv1d.__name__
        nncf_conv = NNCFConv1d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups, hasattr(module, 'bias'))
        nncf_conv = align_module_internals(module, nncf_conv)
        return nncf_conv


NNCF_PADDING_VALUE_ATTR_NAME = 'nncf_padding_value'


class NNCFConv2d(_NNCFModuleMixin, nn.Conv2d):
    op_func_name = 'conv2d'

    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride=1, padding=0, dilation=1, groups: int=1, bias: bool=True, padding_mode: str='zeros'):
        super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)
        self.register_buffer(NNCF_PADDING_VALUE_ATTR_NAME, torch.zeros([1]))

    def get_padding_value_ref(self):
        return getattr(self, NNCF_PADDING_VALUE_ATTR_NAME)

    def _set_padding_value(self, value):
        setattr(self, NNCF_PADDING_VALUE_ATTR_NAME, value)

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.Conv2d.__name__
        nncf_conv = NNCFConv2d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups, hasattr(module, 'bias'))
        nncf_conv = align_module_internals(module, nncf_conv)
        return nncf_conv

    def _custom_forward_fn(self, input_):
        proxy_padding_value = getattr(self, NNCF_PADDING_VALUE_ATTR_NAME)
        proxy_weight = self.weight
        proxy_bias = self.bias
        proxy_padding = self.padding
        proxy_num_groups = self.groups
        return self._conv_forward_proxy(input_, proxy_weight, proxy_bias, proxy_padding_value, proxy_padding, proxy_num_groups)

    def _conv_forward_proxy(self, input_, weight, bias, padding_value, padding, num_groups):
        with no_jit_trace():
            padding_val = padding_value.item()
        self.get_padding_value_ref().data.fill_(padding_val)
        self.groups = num_groups

        def _reverse_repeat_tuple(t, n):
            """Reverse the order of `t` and repeat each element for `n` times.

            This can be used to translate padding arg used by Conv and Pooling modules
            to the ones used by `F.pad`.
            """
            return tuple(x for x in reversed(t) for _ in range(n))
        reversed_padding_repeated_twice = _reverse_repeat_tuple(self.padding, 2)
        if self.padding_mode != 'zeros':
            return F.conv2d(F.pad(input_, reversed_padding_repeated_twice, mode=self.padding_mode, value=padding_val), weight, bias, self.stride, (0, 0), self.dilation, self.groups)
        if padding_val == 0:
            return F.conv2d(input_, weight, bias, self.stride, padding, self.dilation, self.groups)
        return F.conv2d(F.pad(input_, reversed_padding_repeated_twice, value=padding_val), weight, bias, self.stride, (0, 0), self.dilation, self.groups)


class NNCFConv3d(_NNCFModuleMixin, nn.Conv3d):
    op_func_name = 'conv3d'

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.Conv3d.__name__
        nncf_conv3d = NNCFConv3d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups, hasattr(module, 'bias'))
        nncf_conv3d = align_module_internals(module, nncf_conv3d)
        return nncf_conv3d


class NNCFConvTranspose1d(_NNCFModuleMixin, nn.ConvTranspose1d):
    op_func_name = 'conv_transpose1d'
    target_weight_dim_for_compression = 1

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.ConvTranspose1d.__name__
        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.output_padding, module.groups, hasattr(module, 'bias'), module.dilation]
        if hasattr(module, 'padding_mode'):
            args.append(module.padding_mode)
        nncf_conv_transpose1d = NNCFConvTranspose1d(*args)
        nncf_conv_transpose1d = align_module_internals(module, nncf_conv_transpose1d)
        return nncf_conv_transpose1d


class NNCFConvTranspose2d(_NNCFModuleMixin, nn.ConvTranspose2d):
    op_func_name = 'conv_transpose2d'
    target_weight_dim_for_compression = 1

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.ConvTranspose2d.__name__
        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.output_padding, module.groups, hasattr(module, 'bias'), module.dilation]
        if hasattr(module, 'padding_mode'):
            args.append(module.padding_mode)
        nncf_conv_transpose2d = NNCFConvTranspose2d(*args)
        nncf_conv_transpose2d = align_module_internals(module, nncf_conv_transpose2d)
        return nncf_conv_transpose2d


class NNCFConvTranspose3d(_NNCFModuleMixin, nn.ConvTranspose3d):
    op_func_name = 'conv_transpose3d'
    target_weight_dim_for_compression = 1

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.ConvTranspose3d.__name__
        args = [module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.output_padding, module.groups, hasattr(module, 'bias'), module.dilation]
        if hasattr(module, 'padding_mode'):
            args.append(module.padding_mode)
        nncf_conv_transpose3d = NNCFConvTranspose3d(*args)
        nncf_conv_transpose3d = align_module_internals(module, nncf_conv_transpose3d)
        return nncf_conv_transpose3d


class NNCFEmbedding(_NNCFModuleMixin, nn.Embedding):
    op_func_name = 'embedding'

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.Embedding.__name__
        args = [module.num_embeddings, module.embedding_dim, module.padding_idx, module.max_norm, module.norm_type, module.scale_grad_by_freq, module.sparse, module.weight]
        nncf_embedding = NNCFEmbedding(*args)
        nncf_embedding = align_module_internals(module, nncf_embedding)
        return nncf_embedding


class NNCFEmbeddingBag(_NNCFModuleMixin, nn.EmbeddingBag):
    op_func_name = 'embedding_bag'

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.EmbeddingBag.__name__
        args = [module.num_embeddings, module.embedding_dim, module.max_norm, module.norm_type, module.scale_grad_by_freq, module.mode, module.sparse, module.weight, module.include_last_offset]
        nncf_embedding_bag = NNCFEmbeddingBag(*args)
        nncf_embedding_bag = align_module_internals(module, nncf_embedding_bag)
        return nncf_embedding_bag


class NNCFGroupNorm(_NNCFModuleMixin, nn.GroupNorm):
    op_func_name = 'group_norm'
    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.GroupNorm.__name__
        nncf_bn = NNCFGroupNorm(num_groups=module.num_groups, num_channels=module.num_channels, eps=module.eps, affine=module.affine)
        nncf_bn = align_module_internals(module, nncf_bn)
        return nncf_bn


class NNCFLayerNorm(_NNCFModuleMixin, nn.LayerNorm):
    op_func_name = 'layer_norm'
    ignored_algorithms = ['magnitude_sparsity', 'rb_sparsity', 'const_sparsity', 'quantization']

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.LayerNorm.__name__
        nncf_ln = NNCFLayerNorm(module.normalized_shape, hasattr(module, 'eps'))
        nncf_ln = align_module_internals(module, nncf_ln)
        return nncf_ln


class NNCFLinear(_NNCFModuleMixin, nn.Linear):
    op_func_name = 'linear'

    @staticmethod
    def from_module(module):
        assert module.__class__.__name__ == nn.Linear.__name__
        nncf_linear = NNCFLinear(module.in_features, module.out_features, hasattr(module, 'bias'))
        nncf_linear = align_module_internals(module, nncf_linear)
        return nncf_linear


NNCF_MODULES_DICT = {NNCFConv1d: nn.Conv1d, NNCFConv2d: nn.Conv2d, NNCFConv3d: nn.Conv3d, NNCFLinear: nn.Linear, NNCFBatchNorm1d: nn.BatchNorm1d, NNCFBatchNorm2d: nn.BatchNorm2d, NNCFBatchNorm3d: nn.BatchNorm3d, NNCFGroupNorm: nn.GroupNorm, NNCFLayerNorm: nn.LayerNorm, NNCFConvTranspose1d: nn.ConvTranspose1d, NNCFConvTranspose2d: nn.ConvTranspose2d, NNCFConvTranspose3d: nn.ConvTranspose3d, NNCFEmbedding: nn.Embedding, NNCFEmbeddingBag: nn.EmbeddingBag}


OP_NAMES_REQUIRING_MODULE_ATTRS = [v.op_func_name for v in NNCF_MODULES_DICT]


class ConvolutionLayerAttributes(WeightedLayerAttributes):
    """
    This class stores attributes of convolution modules/layers
    that are useful for some algorithms.
    """

    def __init__(self, weight_requires_grad: bool, in_channels: int, out_channels: int, kernel_size: Tuple[int, ...], stride: Tuple[int, ...], groups: int, transpose: bool, padding_values: Tuple[int, ...]):
        super().__init__(weight_requires_grad)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.stride = stride
        self.groups = groups
        self.transpose = transpose
        self.padding_values = padding_values

    def __eq__(self, other: Any):
        return isinstance(other, ConvolutionLayerAttributes) and super().__eq__(other) and self.in_channels == other.in_channels and self.out_channels == other.out_channels and self.kernel_size == other.kernel_size and self.stride == other.stride and self.groups == other.groups and self.transpose == other.transpose

    def get_weight_shape(self) ->List[int]:
        if not self.transpose:
            return [self.out_channels, self.in_channels // self.groups, *self.kernel_size]
        return [self.in_channels, self.out_channels // self.groups, *self.kernel_size]

    def get_target_dim_for_compression(self) ->int:
        if self.transpose:
            return 1
        return 0


class GroupNormLayerAttributes(WeightedLayerAttributes):
    """
    This class stores attributes of group normalization modules/layers
    that are useful for some algorithms.
    """

    def __init__(self, weight_requires_grad: bool, num_channels: int, num_groups: int):
        super().__init__(weight_requires_grad)
        self.num_channels = num_channels
        self.num_groups = num_groups

    def __eq__(self, other: Any):
        return isinstance(other, GroupNormLayerAttributes) and super().__eq__(other) and self.num_channels == other.num_channels and self.num_groups == other.num_groups

    def get_weight_shape(self) ->List[int]:
        return [self.num_channels]

    def get_target_dim_for_compression(self) ->int:
        return 0


class LinearLayerAttributes(WeightedLayerAttributes):

    def __init__(self, weight_requires_grad: bool, in_features: int, out_features: int, bias: bool=True):
        super().__init__(weight_requires_grad)
        self.in_features = in_features
        self.out_features = out_features
        self.bias = bias

    def get_weight_shape(self) ->List[int]:
        return [self.out_features, self.in_features]

    def get_bias_shape(self) ->int:
        return self.out_features if self.bias is True else 0

    def get_target_dim_for_compression(self) ->int:
        return 0


def _get_layer_attributes(module: TorchModule, operator_name: str) ->BaseLayerAttributes:
    if operator_name == 'group_norm':
        return GroupNormLayerAttributes(module.weight.requires_grad, module.num_channels, module.num_groups)
    is_weight_norm_applied = hasattr(module, 'weight_g') and hasattr(module, 'weight_v')
    weight_attr = 'weight_g' if is_weight_norm_applied else 'weight'
    if isinstance(module, (Conv1d, Conv2d, Conv3d)):
        return ConvolutionLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad, in_channels=module.in_channels, out_channels=module.out_channels, kernel_size=module.kernel_size, stride=module.stride, groups=module.groups, transpose=False, padding_values=module.padding)
    if isinstance(module, (ConvTranspose1d, ConvTranspose2d, ConvTranspose3d)):
        return ConvolutionLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad, in_channels=module.in_channels, out_channels=module.out_channels, kernel_size=module.kernel_size, stride=module.stride, groups=module.groups, transpose=True, padding_values=module.padding)
    if isinstance(module, Linear):
        return LinearLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad, in_features=module.in_features, out_features=module.out_features, bias=module.bias is not None)
    if hasattr(module, 'weight') or is_weight_norm_applied:
        return GenericWeightedLayerAttributes(weight_requires_grad=getattr(module, weight_attr).requires_grad, weight_shape=module.weight.shape)
    return GenericWeightedLayerAttributes(weight_requires_grad=False, weight_shape=[1, 1])


def _collect_module_attrs_and_ignored_algorithms(ctx: TracingContext, op_name: str) ->Tuple[BaseLayerAttributes, List[str]]:
    layer_attrs = None
    ignored_algos = []
    if op_name in OP_NAMES_REQUIRING_MODULE_ATTRS:
        curr_module = ctx.get_current_module()
        if curr_module is None:
            raise RuntimeError('Operation {} requires module attributes, but it was executed outside any module'.format(op_name))
        layer_attrs = _get_layer_attributes(curr_module, op_name)
        if isinstance(curr_module, _NNCFModuleMixin):
            ignored_algos = deepcopy(curr_module.ignored_algorithms)
    return layer_attrs, ignored_algos


def make_tensor_metas(inputs: 'OperatorInput') ->List[Optional[TensorMeta]]:
    tensor_metas = []
    for i, node_input_index_entry in enumerate(inputs):
        node_input = node_input_index_entry.getter()
        if isinstance(node_input, TracedTensor):
            tensor_metas.append(node_input.tensor_meta)
        elif isinstance(node_input, torch.Tensor) and not isinstance(node_input, TracedTensor):
            meta = TensorMeta(None, i, node_input.shape)
            tensor_metas.append(meta)
        else:
            tensor_metas.append(None)
    return tensor_metas


def get_dtype(x: torch.Tensor) ->Dtype:
    if x.dtype in [torch.float, torch.float16, torch.float32, torch.float64]:
        return Dtype.FLOAT
    return Dtype.INTEGER


def trace_tensors(operator_output, node: 'DynamicGraphNode'):
    if isinstance(operator_output, (list, tuple)):
        output_ = []
        for i, x in enumerate(operator_output):
            meta = None
            if node is not None:
                meta = TensorMeta(node.node_id, i, x.shape, get_dtype(x))
            output_.append(TracedTensor.from_torch_tensor(x, meta))
        return operator_output.__class__(output_)
    if isinstance(operator_output, torch.Tensor):
        meta = None
        if node is not None:
            meta = TensorMeta(node.node_id, 0, operator_output.shape, get_dtype(operator_output))
        return TracedTensor.from_torch_tensor(operator_output, meta)
    raise ValueError('Unknown return type. Can not trace function call')


def _execute_op(op_address: 'OperationAddress', operator_info: 'PatchedOperatorInfo', operator: Callable, ctx: 'TracingContext', *args, **kwargs):
    op_name = operator_info.name
    op_input = OperatorInput(list(args), kwargs)
    processed_input = ctx.execute_pre_hooks(op_address, op_input)
    args = tuple(processed_input.op_args)
    kwargs = processed_input.op_kwargs
    result = operator(*args, **kwargs)
    node = None
    if isinstance(result, type(NotImplemented)):
        nncf_logger.debug('Operation {} returned NotImplemented'.format(op_name))
    elif ctx.trace_dynamic_graph:
        tensor_metas = make_tensor_metas(processed_input)
        node = ctx.find_operator_node(tensor_metas, op_address)
        if node is None:
            layer_attrs, ignored_algos = _collect_module_attrs_and_ignored_algorithms(ctx, op_name)
            is_called_inside_nncf_module = isinstance(ctx.get_current_module(), _NNCFModuleMixin)
            node = ctx.maybe_add_node(processed_input, tensor_metas, op_address, layer_attrs, ignored_algos, is_called_inside_nncf_module)
        if is_debug() and node is not None:
            ctx.register_node_call(node)
    result = trace_tensors(result, node)
    result = ctx.execute_post_hooks(op_address, result)
    return result


def wrap_operator(operator, operator_info: 'PatchedOperatorInfo'):
    """
    Wraps the input callable object (`operator`) with the functionality that allows the calls to this object
    to be tracked by the currently set global TracingContext. The wrapped functions can be then intercepted,
    their arguments and return values modified arbitrarily and, for functions that correspond to operations on
    tensors in a DNN,  their general position and address in the DNN's model control flow graph can be established.

    :param: operator: A callable object to be wrapped.
    :param: operator_info (PatchedOperatorInfo): An informational struct containing the specifics of wrapping
            the `operator` in question.

    :return: The wrapped version of `operator` that, without a TracingContext, performs functionally the same as
             the unwrapped version, but within a TracingContext is able to be tracked and hooked.
    """
    _orig_op = getattr(operator, '_original_op', None)
    if _orig_op is not None:
        nncf_logger.debug('Operator: {} is already wrapped'.format(_orig_op.__name__))
        return operator

    def wrapped(*args, **kwargs):
        ctx = get_current_context()
        if not ctx or getattr(ctx, 'in_operator', False) or not ctx.is_tracing:
            op1 = operator(*args, **kwargs)
            return op1
        ctx.in_operator = True
        try:
            if operator_info.skip_trace:
                result = operator(*args, **kwargs)
            elif ctx.is_forwarding:
                result = forward_trace_only(operator, *args, **kwargs)
            else:
                op_name = operator_info.name
                op_address = ctx.get_caller_context(op_name)
                ctx.register_operator_call(op_address.operator_name, op_address.scope_in_model)
                if ctx.elastic_depth and ctx.in_skipped_block:
                    result = ctx.tensor_cache
                else:
                    result = _execute_op(op_address, operator_info, operator, ctx, *args, **kwargs)
                str_op_address = str(op_address)
                if str_op_address in ctx.end_node_name_of_skipped_block:
                    assert ctx.in_skipped_block is True
                    ctx.in_skipped_block = False
                if str_op_address in ctx.start_node_name_of_skipped_block:
                    assert ctx.in_skipped_block is False, 'skipping of overlapping blocks'
                    ctx.in_skipped_block = True
                    ctx.tensor_cache = result
        except:
            ctx.in_operator = False
            raise
        ctx.in_operator = False
        return result
    wrapped._original_op = operator
    wrapped._operator_namespace = operator_info.operator_namespace
    return wrapped


def register_operator(name=None):

    def wrap(operator):
        op_name = name
        if op_name is None:
            op_name = operator.__name__
        return wrap_operator(operator, PatchedOperatorInfo(op_name, NamespaceTarget.EXTERNAL))
    return wrap


@register_operator()
def binary_mask_by_threshold(input_tensor: torch.Tensor, threshold: float=0.5, max_percentile: float=0.98) ->torch.Tensor:
    """
    Conduct straight-through thresholding function while limiting the maximum threshold.

    :param input_tensor: The tensor to conduct thresholding.
    :param threshold: The criterion for thresholding.
    :param max_percentile: Specifies the `q`-th quantiles of the input tensor.
    :return: The mask for the input tensor with the threshold limited by `max_percentile`.
    """
    with torch.no_grad():
        max_threshold = torch.quantile(input_tensor, q=max_percentile).item()
    return STThreshold.apply(input_tensor, min(threshold, max_threshold))


class MovementSparsifier(nn.Module):
    """
    Defines the operand of movement sparsity for supported layers.
    """

    def __init__(self, target_module_node: NNCFNode, sparse_cfg: SparseConfig=SparseConfig(mode=SparseStructure.FINE), frozen: bool=True, compression_lr_multiplier: Optional[float]=None, layerwise_loss_lambda: float=0.5):
        """
        Initializes the operand of movement sparsity for a certain layer.

        :param target_module_node: Node name of the module this operand is related to.
        :param sparse_cfg: Sparse structure config for the module to sparsify.
        :param frozen: Whether the operand is frozen, i.e., binary masks are fixed.
        :param compression_lr_multiplier: The value of gradient multiplier for learnable parameters in the operand.
        :param layerwise_loss_lambda: The extra factor of compression loss for this specific layer.
        """
        super().__init__()
        self.target_module_node = target_module_node
        self.prune_bias = bool(target_module_node.layer_attributes.bias)
        self.frozen = frozen
        self.layerwise_loss_lambda = layerwise_loss_lambda
        self._importance_threshold = -math.inf
        self._importance_regularization_factor = 0.0
        weight_shape: List[int] = target_module_node.layer_attributes.get_weight_shape()
        assert len(weight_shape) == 2, 'Unsupported module with weight shape not in 2D.'
        self.weight_ctx = BinaryMask(weight_shape)
        self.sparse_factors = self._get_sparse_factors(weight_shape, sparse_cfg)
        self.sparse_structure = sparse_cfg.mode
        weight_importance_shape = self._get_weight_importance_shape(weight_shape, self.sparse_factors, self.sparse_structure)
        self.weight_importance = CompressionParameter(torch.zeros(weight_importance_shape), requires_grad=not self.frozen, compression_lr_multiplier=compression_lr_multiplier)
        self.weight_ctx.binary_mask = self._calc_training_binary_mask()
        if self.prune_bias:
            bias_shape = target_module_node.layer_attributes.get_bias_shape()
            self.bias_ctx = BinaryMask(bias_shape)
            bias_importance_shape = weight_importance_shape[0]
            self.bias_importance = CompressionParameter(torch.zeros(bias_importance_shape), requires_grad=not self.frozen, compression_lr_multiplier=compression_lr_multiplier)
            self.bias_ctx.binary_mask = self._calc_training_binary_mask(is_bias=True)
        self.mask_calculation_hook = MaskCalculationHook(self)

    @property
    def importance_threshold(self):
        return self._importance_threshold

    @importance_threshold.setter
    def importance_threshold(self, value: float):
        self._importance_threshold = value

    @property
    def importance_regularization_factor(self):
        return self._importance_regularization_factor

    @importance_regularization_factor.setter
    def importance_regularization_factor(self, value: float):
        self._importance_regularization_factor = value

    def forward(self, weight: torch.Tensor, bias: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, Optional[torch.Tensor]]:
        if is_tracing_state():
            masked_weight = weight.mul(self.weight_ctx.binary_mask)
            masked_bias = None if bias is None else bias.mul(self.bias_ctx.binary_mask)
        else:
            weight_mask = self._calc_training_binary_mask(is_bias=False)
            masked_weight = apply_binary_mask_impl(weight_mask, weight)
            masked_bias = None
            if bias is not None:
                bias_mask = self._calc_training_binary_mask(is_bias=True)
                masked_bias = apply_binary_mask_impl(bias_mask, bias)
        return masked_weight, masked_bias

    def apply_binary_mask(self, param_tensor: torch.Tensor, is_bias: bool=False) ->torch.Tensor:
        ctx = self.bias_ctx if is_bias else self.weight_ctx
        return ctx.apply_binary_mask(param_tensor)

    def get_importance(self, is_bias: bool=False, expanded: bool=True) ->torch.Tensor:
        """
        Gets the importance score parameter of the operand.

        :param is_bias: If true, will return the bias importance. Otherwise will return the weight importance.
        :param expanded: Whether should expand the importance to the same shape as module weight or bias.
        """
        if is_bias and not self.prune_bias:
            raise ValueError('The layer to sparsify does not contain bias.')
        importance = self.bias_importance if is_bias else self.weight_importance
        if not expanded or self.sparse_factors == [1, 1]:
            return importance
        expand_factors = [self.sparse_factors[0]] if is_bias else self.sparse_factors
        for dim, factor in enumerate(expand_factors):
            importance = importance.repeat_interleave(factor, dim=dim)
        return importance

    def loss(self) ->torch.Tensor:
        if self.frozen or self.importance_regularization_factor == 0.0:
            return torch.tensor(0.0, device=self._get_device())
        layer_loss = torch.mean(torch.sigmoid(self.weight_importance)) * self.layerwise_loss_lambda * math.prod(self.sparse_factors)
        if self.prune_bias:
            layer_loss += torch.mean(torch.sigmoid(self.bias_importance)) * self.layerwise_loss_lambda * float(self.sparse_factors[0])
        return layer_loss * self.importance_regularization_factor

    def requires_grad_(self, requires_grad: bool=True):
        super().requires_grad_(requires_grad)
        if not requires_grad:
            self.zero_grad(set_to_none=True)
        self.frozen = not requires_grad

    def extra_repr(self) ->str:
        return f'sparse_structure: {self.sparse_structure.value} {self.sparse_factors}'

    def _get_device(self) ->torch.device:
        return self.weight_importance.device

    def _calc_training_binary_mask(self, is_bias: bool=False):
        ctx = self.bias_ctx if is_bias else self.weight_ctx
        if not self.training or self.frozen:
            return ctx.binary_mask
        mask = binary_mask_by_threshold(input_tensor=self.get_importance(is_bias, expanded=True), threshold=self.importance_threshold)
        ctx.binary_mask = mask
        return mask

    @staticmethod
    def _get_weight_importance_shape(weight_shape: List[int], sparse_factors: Tuple[int, int], sparse_structure: SparseStructure) ->Tuple[int, int]:
        if sparse_structure == SparseStructure.FINE:
            return weight_shape
        if sparse_structure == SparseStructure.BLOCK:
            r, c = sparse_factors
            return weight_shape[0] // r, weight_shape[1] // c
        if sparse_structure == SparseStructure.PER_DIM:
            score_shape = []
            for axis, (dim, factor) in enumerate(zip(weight_shape, sparse_factors)):
                assert dim % factor == 0, f'{factor} is not a factor of axis {axis} with dim size {dim}.'
                score_shape.append(dim // factor)
            return tuple(score_shape)
        raise RuntimeError('Unknown sparse structure.')

    @staticmethod
    def _get_sparse_factors(weight_shape: List[int], sparse_config: SparseConfig) ->Tuple[int, int]:
        sparse_factors = sparse_config.sparse_factors
        if sparse_config.mode == SparseStructure.BLOCK:
            r, c = sparse_factors
            assert weight_shape[0] % r == 0, f'r: {r} is not a factor of dim axis 0.'
            assert weight_shape[1] % c == 0, f'c: {c} is not a factor of dim axis 1.'
        if sparse_config.mode == SparseStructure.PER_DIM:
            if sparse_config.sparse_axis < 0 or sparse_config.sparse_axis >= len(weight_shape):
                raise ValueError('Invalid axis id {}, axes range is [0, {}]'.format(sparse_config.sparse_axis, len(weight_shape)))
            sparse_factors = deepcopy(weight_shape)
            sparse_factors[sparse_config.sparse_axis] = 1
            sparse_factors = tuple(sparse_factors)
        return sparse_factors


class Actor(nn.Module):

    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=0.003):
        super().__init__()
        self.fc1 = nn.Linear(nb_states, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.fc3 = nn.Linear(hidden2, nb_actions)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        out = self.sigmoid(out)
        return out


class Critic(nn.Module):

    def __init__(self, nb_states, nb_actions, hidden1=400, hidden2=300, init_w=0.003):
        super().__init__()
        self.fc11 = nn.Linear(nb_states, hidden1)
        self.fc12 = nn.Linear(nb_actions, hidden1)
        self.fc2 = nn.Linear(hidden1, hidden2)
        self.fc3 = nn.Linear(hidden2, 1)
        self.relu = nn.ReLU()

    def forward(self, xs):
        x, a = xs
        out = self.fc11(x) + self.fc12(a)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        return out


class BaseBinarizer(nn.Module):

    def __init__(self, enabled=False):
        super().__init__()
        self.register_buffer('enabled', torch.IntTensor([0]))
        if enabled:
            self.enable()

    def forward(self, x):
        if self.is_enabled():
            return self.binarize(x)
        return x

    def binarize(self, x):
        raise NotImplementedError

    def enable(self):
        self.enabled[0] = 1

    def disable(self):
        self.enabled[0] = 0

    def is_enabled(self):
        return self.enabled[0] == 1


class WeightBinarizer(BaseBinarizer):

    def binarize(self, x):
        raise NotImplementedError


class ActivationBinarizer(BaseBinarizer):

    def binarize(self, x):
        raise NotImplementedError


BINARIZATION_MODULES = Registry('binarization_modules')


class BinarizationMode:
    XNOR = 'xnor'
    DOREFA = 'dorefa'


def _is_value(x: Any) ->bool:
    return isinstance(x, _C.Value)


def _is_constant(value: Any) ->bool:
    return not _is_value(value) or value.node().kind() in {'onnx::Constant', 'prim::Constant'}


def _unsqueeze_helper(g, input_, axes_i):
    if _is_constant(axes_i[0]):
        axes = g.op('Constant', value_t=torch.tensor(axes_i, dtype=torch.long))
        return g.op('Unsqueeze', input_, axes)
    return g.op('Unsqueeze', input_, axes_i=axes_i[0])


class XNORBinarizeFn(torch.autograd.Function):
    """ Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based
        on whether the x element value is >0 or <0. `scale` is determined as mean of absolute
        values, per input channel (0-th dimension of x). """

    @staticmethod
    def symbolic(g, x):
        zero = g.constant(0, [1], 'float')
        zero = _unsqueeze_helper(g, zero, [1, 2, 3])
        scale = g.op('Abs', x)
        scale = g.op('ReduceMean', scale, axes_i=[1, 2, 3])
        scale_neg = g.op('Neg', scale)
        return g.op(add_domain('FakeQuantize'), x, zero, zero, scale_neg, scale, levels_i=2)

    @staticmethod
    def forward(ctx, x):
        if x.is_cuda:
            output = BinarizedFunctionsCUDA.WeightBinarize_forward(x, True)
        else:
            norm = x.abs().mean([1, 2, 3], keepdim=True)
            sign = (x > 0).type(x.dtype) * 2 - 1
            output = sign * norm
            return output
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


@register_operator()
def xnor_binarize_op(x):
    return XNORBinarizeFn.apply(x)


class XNORBinarize(WeightBinarizer):

    def binarize(self, x):
        return xnor_binarize_op(x)


class DOREFABinarizeFn(torch.autograd.Function):
    """ Binarizes x into `scale` * { +1; -1}, where +1 or -1 are chosen based
        on whether the x element value is >0 or <0. `scale` is determined as mean of absolute
        values of the entire x tensor. """

    @staticmethod
    def symbolic(g, x):
        zero = g.constant(0, [1], 'float')
        zero = _unsqueeze_helper(g, zero, [1, 2, 3])
        scale = g.op('Abs', x)
        scale = g.op('ReduceMean', scale, axes_i=[0, 1, 2, 3])
        scale_neg = g.op('Neg', scale)
        return g.op(add_domain('FakeQuantize'), x, zero, zero, scale_neg, scale, levels_i=2)

    @staticmethod
    def forward(ctx, x):
        if x.is_cuda:
            output = BinarizedFunctionsCUDA.WeightBinarize_forward(x, False)
        else:
            norm = x.abs().mean()
            sign = (x > 0).type(x.dtype) * 2 - 1
            output_flat = sign * norm
            return output_flat.view_as(x)
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


@register_operator()
def dorefa_binarize_op(x):
    return DOREFABinarizeFn.apply(x)


class DOREFABinarize(WeightBinarizer):

    def binarize(self, x):
        return dorefa_binarize_op(x)


class ActivationBinarizationScaleThresholdFn(torch.autograd.Function):

    @staticmethod
    def symbolic(g, x, scale, threshold):
        zero = g.constant(0, [1], 'float')
        zero = _unsqueeze_helper(g, zero, [0, 2, 3])
        threshold = g.op('Mul', threshold, scale)
        scale = _unsqueeze_helper(g, scale, [0, 2, 3])
        return g.op(add_domain('FakeQuantize'), x, threshold, threshold, zero, scale, levels_i=2)

    @staticmethod
    def forward(ctx, input_, scale, threshold):
        if input_.is_cuda:
            output = BinarizedFunctionsCUDA.ActivationBinarize_forward(input_, scale, threshold)
        else:
            shape = [(1) for s in input_.shape]
            shape[1] = input_.shape[1]
            t = (threshold * scale).view(shape)
            output = (input_ > t).type(input_.dtype) * scale
            ctx.save_for_backward(input_, scale, output)
        ctx.save_for_backward(input_, scale, output)
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        grad_output = grad_outputs[0]
        if grad_output.is_cuda:
            if not grad_output.is_contiguous():
                nncf_logger.debug('grad_output is not contiguous!')
                grad_output = grad_output.contiguous()
        input_, scale, output = ctx.saved_variables
        if input_.is_cuda:
            grad_input, grad_scale, grad_threshold = BinarizedFunctionsCUDA.ActivationBinarize_backward(grad_output, input_, scale, output)
        else:
            mask_lower = (input_ <= scale).type(input_.dtype)
            grad_input = grad_output * (input_ >= 0).type(input_.dtype) * mask_lower
            err = (output - input_) * scale.reciprocal()
            grad_scale = grad_output * (mask_lower * err + (1 - mask_lower))
            grad_scale = grad_scale.sum().view(1)
            grad_threshold = -grad_output * (input_ > 0).type(input_.dtype) * (input_ < scale).type(input_.dtype)
            for idx, _ in enumerate(input_.shape):
                if idx != 1:
                    grad_threshold = grad_threshold.sum(idx, keepdim=True)
        return grad_input, grad_scale, grad_threshold


@register_operator()
def activation_bin_scale_threshold_op(x, scale, threshold):
    return ActivationBinarizationScaleThresholdFn.apply(x, scale, threshold)


def get_per_channel_scale_shape(input_shape, is_weights, channel_idx: int=None):
    scale_shape = [(1) for _ in input_shape]
    if channel_idx is None:
        if is_weights:
            channel_idx = 0
        else:
            channel_idx = 1
    scale_shape[channel_idx] = input_shape[channel_idx]
    return scale_shape


class ActivationBinarizationScaleThreshold(ActivationBinarizer):

    def __init__(self, input_shape, enabled=False, compression_lr_multiplier=None, desc=''):
        super().__init__(enabled)
        self.input_shape = input_shape
        self.scale = CompressionParameter(torch.Tensor([0]), requires_grad=enabled, compression_lr_multiplier=compression_lr_multiplier)
        self.scale.data.zero_()
        self.register_buffer('scale_initialized', torch.IntTensor([0]))
        threshold_shape = get_per_channel_scale_shape(self.input_shape, is_weights=False)
        self.threshold = CompressionParameter(torch.ones(threshold_shape), requires_grad=enabled, compression_lr_multiplier=compression_lr_multiplier)
        self.threshold.data.zero_()
        self.bin = activation_bin_scale_threshold_op

    @property
    def is_scale_initialized(self):
        return self.scale_initialized[0] == 1

    @is_scale_initialized.setter
    def is_scale_initialized(self, value: bool):
        self.scale_initialized[0] = 1 if value else 0

    def binarize(self, x):
        if self.training and not self.is_scale_initialized:
            d = x.detach().data.contiguous().view(-1)
            top_num = max(1, round(d.shape[0] * 0.001))
            topk_res = d.topk(top_num)
            scale = topk_res[0].min()
            nncf_logger.debug(f'Binarized activation scale set to: {scale.item()}')
            self.scale.data[:] = scale.log()
            self.is_scale_initialized = True
        x = self.bin(x, self.scale.exp(), self.threshold.sigmoid())
        return x

    def enable(self):
        super().enable()
        self.scale.requires_grad_(True)
        self.threshold.requires_grad_(True)


class CompressionLoss(ABC):
    """
    Used to calculate the additional loss to be added to the base loss during the
    training process. It uses the model graph to measure variables and activations
    values of the layers during the loss construction. For example, the $L_0$-based
    sparsity algorithm calculates the number of non-zero weights in convolutional
    and fully-connected layers to construct the loss function.
    """

    @abstractmethod
    def calculate(self, *args, **kwargs) ->Any:
        """
        Calculates the compression loss value.

        :return: The compression loss value.
        """

    @abstractmethod
    def load_state(self, state: Dict[str, Any]) ->None:
        """
        Loads the compression loss state.

        :param state: Output of `get_state()` method.
        """

    @abstractmethod
    def get_state(self) ->Dict[str, Any]:
        """
        Returns the compression loss state.

        :return: The compression loss state.
        """

    def __call__(self, *args, **kwargs) ->Any:
        """
        Invokes the `CompressionLoss` instance.

        :return: The compression loss value.
        """
        return self.calculate(*args, **kwargs)


class PTCompressionLoss(nn.Module, CompressionLoss):
    """
    Used to calculate additional loss to be added to the base loss during the
    training process. It uses the model graph to measure variables and activations
    values of the layers during the loss construction. For example, the $L_0$-based
    sparsity algorithm calculates the number of non-zero weights in convolutional
    and fully-connected layers to construct the loss function.
    """

    def calculate(self) ->torch.Tensor:
        """
        Calculates the compression loss value.

        :return: The compression loss value.
        """
        return torch.zeros([])

    def forward(self) ->torch.Tensor:
        """
        Overriding  forward function of the base nn.Module class

        :return: The compression loss value.
        """
        return self.calculate()

    def load_state(self, state: Dict[str, Any]) ->None:
        """
        Loads the compression loss state.

        :param state: Output of `get_state()` method.
        """

    def get_state(self) ->None:
        """
        Returns the compression loss state.

        :return: The compression loss state.
        """


class KnowledgeDistillationLossHandler(nn.Module):
    """
    Encapsulates knowledge distillation logic. Controls knowledge distillation loss calculation. Notice that knowledge
    distillation loss is computed between results of original model and compressed model inferences only with latest
    inputs. And storages loss values in context at storage_device for further access. Such complex method of storage
    is required for DataParallel model replication logic.
    """
    KD_LOSS_STORAGE_NAME = 'kd_loss'
    KD_STORAGE_DEVICE = 'kd_storage_device'

    def __init__(self, context: TracingContext, kd_original_model: nn.Module, calculate_kd_loss_fn, storage_device: torch.device):
        super().__init__()
        self._compressed_context = context
        self._kd_original_model = kd_original_model
        self._calculate_kd_loss_fn = calculate_kd_loss_fn
        self._compressed_context.register_global_buffer(self.KD_LOSS_STORAGE_NAME, [])
        self._compressed_context.register_global_buffer(self.KD_STORAGE_DEVICE, storage_device)

    def zero_kd_loss(self):
        """
            Frees storage space for further next iteration loss value storage.
        """
        self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME] = []

    def get_kd_loss(self) ->List[torch.Tensor]:
        if len(self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME]) == 0:
            return [torch.zeros([], device=self._compressed_context.global_buffer_store[self.KD_STORAGE_DEVICE])]
        return self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME]

    def forward(self, inputs, *args, **kwargs):
        """
        Infers kd original model with latest NNCFNetwork forward inputs (*args, **kwargs) and computes distillation loss
        between results of kd original model forward and compressed model forward (inputs). Then stores loss values
        in context at storage device.

        :param inputs: Results of compressed model forward used for knowledge distillation loss calculations.
        """
        self.zero_kd_loss()
        with torch.no_grad():
            kd_outputs = self._kd_original_model(*args, **kwargs)
        kd_loss = self._calculate_kd_loss_fn(inputs, kd_outputs)
        self._compressed_context.global_buffer_store[self.KD_LOSS_STORAGE_NAME].append(kd_loss)


class DebugInterface:

    def pre_forward_actions(self, module: Module):
        raise NotImplementedError

    def post_forward_actions(self, module: Module):
        raise NotImplementedError

    def init_actual(self, owner_model):
        raise NotImplementedError


class CombinedDebugInterface(DebugInterface):

    def __init__(self):
        self._interfaces = []

    def add_interface(self, interface: 'DebugInterface'):
        self._interfaces.append(interface)

    def init_actual(self, owner_model: 'NNCFNetwork'):
        for interface in self._interfaces:
            interface.init_actual(owner_model)

    def pre_forward_actions(self, module: Module):
        for interface in self._interfaces:
            interface.pre_forward_actions(module)

    def post_forward_actions(self, module: Module):
        for interface in self._interfaces:
            interface.post_forward_actions(module)


EXTERNAL_QUANTIZERS_STORAGE_NAME = 'external_quantizers'


class ExtraCompressionModuleType(Enum):
    EXTERNAL_QUANTIZER = 0


class ModelInputInfo:
    FILLER_TYPE_ONES = 'ones'
    FILLER_TYPE_ZEROS = 'zeros'
    FILLER_TYPE_RANDOM = 'random'
    FILLER_TYPES = [FILLER_TYPE_ONES, FILLER_TYPE_ZEROS, FILLER_TYPE_RANDOM]

    def __init__(self, shape: List[int], type_str: str='float', keyword=None, filler=None):
        self.shape = shape
        self.type = self._string_to_torch_type(type_str)
        self.keyword = keyword
        if filler is None:
            self.filler = self.FILLER_TYPE_ONES
        else:
            self.filler = filler
            if self.filler not in self.FILLER_TYPES:
                raise RuntimeError('Unknown input filler type: {}'.format(filler))

    @staticmethod
    def _string_to_torch_type(string):
        if string == 'long':
            return torch.long
        return torch.float32

    @staticmethod
    def torch_type_to_string(dtype: torch.dtype):
        if dtype is torch.long:
            return 'long'
        return 'float'

    def is_integer_input(self):
        return self.type != torch.float32

    def __eq__(self, other):
        return self.type == other.type and self.keyword == other.keyword


class MultipleInputLayerAttributes(BaseLayerAttributes):
    """
    Represents a layer with multiple inputs.
    """

    def __init__(self, axis: int):
        self.axis = axis

    def __eq__(self, other: Any):
        return isinstance(other, MultipleInputLayerAttributes) and self.axis == other.axis


class MultipleOutputLayerAttributes(BaseLayerAttributes):
    """
    Represents a layer with multiple outputs.
    """

    def __init__(self, chunks: Union[int, List], axis: int):
        self.chunks = chunks
        self.axis = axis

    def __eq__(self, other: Any):
        return isinstance(other, MultipleOutputLayerAttributes) and self.chunks == other.chunks and self.axis == other.axis


HWConfigOpName = str


class PTOperatorMetatype(OperatorMetatype):
    """
    Base class for grouping PyTorch operators based on their semantic meaning.
    Each derived class represents a single semantic group - for example, AddMetatype would
    group together '__iadd__', '__add__' and '__radd__' operations which all define nodewise
    tensor addition.
    Derived classes also specify which PyTorch functions in which modules should be patched
    so that the entire group of operations is visible in the internal graph.
    Grouping also allows efficient application of HW specifics to compression of
    certain operation groups.
    """
    external_op_names = []
    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: [], NamespaceTarget.TORCH_TENSOR: [], NamespaceTarget.TORCH: []}
    subtypes = []

    @classmethod
    def get_subtypes(cls) ->List[Type['PTOperatorMetatype']]:
        return cls.subtypes.copy()

    @classmethod
    def get_all_namespace_to_function_names(cls) ->Dict[NamespaceTarget, List[str]]:
        output = dict(cls.module_to_function_names)
        output[NamespaceTarget.EXTERNAL] = cls.external_op_names
        return output

    @classmethod
    def get_all_aliases(cls) ->List[str]:
        output = set()
        for _, function_names in cls.module_to_function_names.items():
            output = output.union(function_names)
        if cls.external_op_names is not None:
            output = output.union(cls.external_op_names)
        return output

    @classmethod
    def determine_subtype(cls, layer_attributes: Optional[BaseLayerAttributes]=None, function_args=None, functions_kwargs=None) ->Optional['PTOperatorSubtype']:
        matches = []
        for subtype in cls.get_subtypes():
            if subtype.matches(layer_attributes, function_args, functions_kwargs):
                matches.append(subtype)
        assert len(matches) <= 1, 'Multiple subtypes match operator call - cannot determine single subtype.'
        if not matches:
            return None
        subtype = matches[0]
        nested_subtype = subtype.determine_subtype(layer_attributes, function_args, functions_kwargs)
        if nested_subtype:
            return nested_subtype
        return subtype


class UnknownMetatype(OperatorMetatype):
    """
    UnknownMetatype is mapped to operations in NNCFGraph, which are unknown for algorithms,
    typically these are the operations that haven't been discovered before.
    Algorithms should avoid processing graph nodes with this metatype.
    """
    name = 'unknown'

    @classmethod
    def get_all_aliases(cls) ->List[str]:
        return [cls.name]


class OperatorMetatypeRegistry(Registry):
    """
    Operator Metatypes Registry.
    """

    def __init__(self, name: str):
        """
        Initialize registry state.

        :param name: The registry name.
        """
        super().__init__(name)
        self._op_name_to_op_meta_dict = {}

    def register(self, name: Optional[str]=None):
        """
        Decorator for registering operator metatypes.

        :param name: The registration name.
        :return: The inner function for registering operator metatypes.
        """
        name_ = name
        super_register = super()._register

        def wrap(obj: Type[OperatorMetatype]):
            """
            Inner function for registering operator metatypes.

            :param obj: The operator metatype.
            :return: The input operator metatype.
            """
            cls_name = name_
            if cls_name is None:
                cls_name = obj.__name__
            super_register(obj, cls_name)
            op_names = obj.get_all_aliases()
            for name in op_names:
                if name in self._op_name_to_op_meta_dict and not obj.subtype_check(self._op_name_to_op_meta_dict[name]):
                    raise RuntimeError('Inconsistent operator metatype registry - single patched op name maps to multiple metatypes!')
                self._op_name_to_op_meta_dict[name] = obj
            return obj
        return wrap

    def get_operator_metatype_by_op_name(self, op_name: str) ->Type[OperatorMetatype]:
        """
        Returns the operator metatype by operator name.

        :param op_name: The operator name.
        :return: The operator metatype.
        """
        if op_name not in self._op_name_to_op_meta_dict:
            return UnknownMetatype
        return self._op_name_to_op_meta_dict[op_name]


PT_OPERATOR_METATYPES = OperatorMetatypeRegistry('operator_metatypes')


class PTTargetPointStateNames:
    TARGET_NODE_NAME = 'target_node_name'
    INPUT_PORT = 'input_port_id'
    TARGET_TYPE = 'target_type'


class StatefulClassesRegistry:
    """
    Registry for the stateful classes  - classes that can be restored from their state by `from_state` method.
    """
    REQUIRED_METHOD_NAME = 'from_state'

    def __init__(self):
        self._name_vs_class_map = {}
        self._class_vs_name_map = {}

    def register(self, name: str=None) ->Callable:
        """
        Decorator to map class with some name - specified in the argument or name of the class.

        :param name: The registration name. By default, it's name of the class.
        :return: The inner function for registration.
        """

        def decorator(cls):
            registered_name = name if name is not None else cls.__name__
            if registered_name in self._name_vs_class_map:
                raise ValueError('{} has already been registered to {}'.format(registered_name, self._name_vs_class_map[registered_name]))
            if cls in self._class_vs_name_map:
                raise ValueError('{} has already been registered to {}'.format(cls, self._class_vs_name_map[cls]))
            if inspect.isclass(cls) and not hasattr(cls, self.REQUIRED_METHOD_NAME):
                raise ValueError('Cannot register a class ({}) that does not have {}() method.'.format(registered_name, self.REQUIRED_METHOD_NAME))
            self._class_vs_name_map[cls] = registered_name
            self._name_vs_class_map[registered_name] = cls
            return cls
        return decorator

    def get_registered_class(self, registered_name: str) ->object:
        """
        Provides a class that was registered with the given name.

        :param registered_name: name
        :return: class that was registered with the given name
        """
        if registered_name in self._name_vs_class_map:
            return self._name_vs_class_map[registered_name]
        raise KeyError('No registered stateful classes with {} name'.format(registered_name))

    def get_registered_name(self, stateful_cls: object) ->str:
        """
        Provides a name that was used to register the given stateful class.

        :param stateful_cls: class
        :return: name that was used on registration of the given class
        """
        if stateful_cls in self._class_vs_name_map:
            return self._class_vs_name_map[stateful_cls]
        raise KeyError('The class {} was not registered.'.format(stateful_cls.__name__))


PT_STATEFUL_CLASSES = StatefulClassesRegistry()


TF_STATEFUL_CLASSES = StatefulClassesRegistry()


class CommonStatefulClassesRegistry:
    """
    Common for TF and PT registry for the stateful classes.
    """

    @staticmethod
    def register(name: str=None) ->Callable:
        """
        Decorator to map class with some name - specified in the argument or name of the class.

        :param name: The registration name. By default, it's name of the class.
        :return: The inner function for registration.
        """

        def decorator(cls):
            PT_STATEFUL_CLASSES.register(name)(cls)
            TF_STATEFUL_CLASSES.register(name)(cls)
            return cls
        return decorator

    @staticmethod
    def get_registered_class(registered_name: str) ->object:
        """
        Provides a class that was registered with the given name.

        :param registered_name: name
        :return: class that was registered with the given name
        """
        return PT_STATEFUL_CLASSES.get_registered_class(registered_name)

    @staticmethod
    def get_registered_name(stateful_cls: object) ->str:
        """
        Provides a name that was used to register the given stateful class.

        :param stateful_cls: class
        :return: name that was used on registration of the given class
        """
        return PT_STATEFUL_CLASSES.get_registered_name(stateful_cls)


class TargetPointStateNames:
    TARGET_TYPE = 'target_type'


TARGET_TYPE_STATE_ATTR = 'name'


class TargetType(IntEnum):
    """
    Describes the types of locations in the model that can be modified using NNCF
    in order to create a compressed model.

    `LAYER` - a location corresponding directly to an existing layer in the model
    `BEFORE_LAYER` - a location before the associated model layer,
                     implemented by inserting an additional layer in the TF model
    `AFTER_LAYER` - a location after the associated model layer,
                    implemented by inserting an additional layer in the TF model
    `PRE_LAYER_OPERATION` - a location before the associated PT-module or TF-layer
                            execution, for which the local attributes of said
                            PT-module or TF-layer are accessible
    `POST_LAYER_OPERATION` - a location before the associated PT-module or TF-layer
                             execution, for which the local attributes of said
                             PT-module or TF-layer are accessible
    `OPERATION_WITH_WEIGHTS` - same as PRE_LAYER_OPERATION, but targets weights
                               of the layer/module specifically
    `OPERATOR_PRE_HOOK` - a location before a function call in PT without access to
                          specific module attributes - N/A in TF
    `OPERATOR_POST_HOOK` - a location after a function call in PT without access to
                           specific module attributes - N/A in TF

    Notes: Adding operations to a PT-module or TF-layer implemented by wrapping
    the original PT-module or TF-layer and registering operations that are executed
    before/after calling the original PT-module or TF-layer according to the
    registration location:`PRE_LAYER_OPERATION`, `POST_LAYER_OPERATION` and
    `OPERATION_WITH_WEIGHTS`.
    """
    LAYER = 0
    BEFORE_LAYER = 1
    AFTER_LAYER = 2
    PRE_LAYER_OPERATION = 3
    POST_LAYER_OPERATION = 4
    OPERATION_WITH_WEIGHTS = 5
    OPERATOR_PRE_HOOK = 6
    OPERATOR_POST_HOOK = 7

    def get_state(self) ->Dict[str, Any]:
        """
        Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
        represents state of the object.

        :return: state of the object
        """
        return {TARGET_TYPE_STATE_ATTR: self.name}

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'TargetType':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        return TargetType[state[TARGET_TYPE_STATE_ATTR]]


class TargetPoint:
    """
    The base class for all target points.

    A target point is an object or spot in the model graph. It can be a layer,
    weights, position before or after layer and etc.

    For example, the transformation commands use `TargetPoint` to specify
    the target point in the model graph to which the transformation command
    will be applied.
    """
    _state_names = TargetPointStateNames

    def __init__(self, target_type: TargetType):
        """
        Constructor.

        :param target_type: Type of the target point.
        """
        self._target_type = target_type

    @property
    def type(self) ->TargetType:
        return self._target_type

    def __eq__(self, other: Any) ->bool:
        return isinstance(other, TargetPoint) and self.type == other.type

    def __str__(self) ->str:
        return str(self.type)

    def __hash__(self) ->int:
        return hash(str(self))

    def get_state(self) ->Dict[str, Any]:
        """
        Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
        represents state of the object.

        :return: state of the object
        """
        return {self._state_names.TARGET_TYPE: self._target_type.get_state()}

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'TargetPoint':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        kwargs = {cls._state_names.TARGET_TYPE: TargetType.from_state(state[cls._state_names.TARGET_TYPE])}
        return cls(**kwargs)


class PTTargetPoint(TargetPoint):
    _OPERATION_TYPES = [TargetType.PRE_LAYER_OPERATION, TargetType.POST_LAYER_OPERATION, TargetType.OPERATION_WITH_WEIGHTS]
    _HOOK_TYPES = [TargetType.OPERATOR_PRE_HOOK, TargetType.OPERATOR_POST_HOOK]
    _state_names = PTTargetPointStateNames

    def __init__(self, target_type: TargetType, target_node_name: NNCFNodeName, *, input_port_id: int=None):
        super().__init__(target_type)
        self.target_node_name = target_node_name
        self.target_type = target_type
        if self.target_type not in self._OPERATION_TYPES + self._HOOK_TYPES:
            raise NotImplementedError('Unsupported target type: {}'.format(target_type))
        self.input_port_id = input_port_id

    def __eq__(self, other: 'PTTargetPoint'):
        return isinstance(other, PTTargetPoint) and self.target_type == other.target_type and self.target_node_name == other.target_node_name

    def __str__(self):
        prefix = str(self.target_type)
        retval = prefix
        if self.target_type in self._OPERATION_TYPES:
            retval += ' {}'.format(self.target_node_name)
        elif self.target_type in self._HOOK_TYPES:
            if self.input_port_id is not None:
                retval += ' {}'.format(self.input_port_id)
            retval += ' ' + str(self.target_node_name)
        return retval

    def __hash__(self):
        return hash(str(self))

    def get_state(self) ->Dict[str, Any]:
        """
        Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
        represents state of the object.

        :return: state of the object
        """
        return {self._state_names.TARGET_TYPE: self.target_type.get_state(), self._state_names.INPUT_PORT: self.input_port_id, self._state_names.TARGET_NODE_NAME: self.target_node_name}

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'PTTargetPoint':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        kwargs = {cls._state_names.TARGET_TYPE: TargetType.from_state(state[cls._state_names.TARGET_TYPE]), cls._state_names.INPUT_PORT: state[cls._state_names.INPUT_PORT], cls._state_names.TARGET_NODE_NAME: state[cls._state_names.TARGET_NODE_NAME]}
        return cls(**kwargs)


class ReshapeLayerAttributes(BaseLayerAttributes):
    """
    This class stores attributes of reshape modules/layers
    that are useful for some algorithms.
    """

    def __init__(self, input_shape: List[int], output_shape: List[int]):
        self.input_shape = input_shape
        self.output_shape = output_shape


def get_concat_axis(input_shapes: List[List[int]], output_shapes: List[List[int]]) ->int:
    """
    Returns concatenation axis by given input and output shape of concat node.

    :param input_shapes: Input_shapes of given concat node.
    :param output_shapes: Output_shapes of given concat node.
    :returns: Concatenation axis of given concat node.
    """
    axis = None
    none_dim = None
    for idx, (dim_in, dim_out) in enumerate(zip(input_shapes[0], output_shapes[0])):
        if dim_in != dim_out:
            axis = idx
            break
        if dim_in is None:
            none_dim = idx
    if axis is None:
        if none_dim is None:
            axis = -1
            nncf_logger.debug('Identity concat node detected')
        else:
            axis = none_dim
    return axis


def get_split_axis(input_shapes: List[List[int]], output_shapes: List[List[int]]) ->int:
    """
    Returns split/chunk axis by given input and output shape of split/chunk node.

    :param input_shapes: Input_shapes of given split/chunk node.
    :param output_shapes: Output_shapes of given split/chunk node.
    :returns: Split/Chunk axis of given split/chunk node.
    """
    axis = None
    for idx, (dim_in, dim_out) in enumerate(zip(input_shapes[0], output_shapes[0])):
        if dim_in != dim_out:
            axis = idx
            break
    if axis is None:
        axis = -1
        nncf_logger.debug('Identity split/concat node detected')
    return axis


class PostGraphBuildActing:

    def post_build_graph_actions(self):
        pass


class GraphTracer:

    def __init__(self, custom_forward_fn: Callable[[torch.nn.Module], Any]):
        self.custom_forward_fn = custom_forward_fn

    def trace_graph(self, model: torch.nn.Module, context_to_use: Optional['TracingContext']=None, as_eval: bool=False) ->DynamicGraph:
        sd = deepcopy(model.state_dict())
        if context_to_use is None:
            context_to_use = TracingContext()
        context_to_use.enable_trace_dynamic_graph()
        with context_to_use as _ctx:
            _ctx.base_module_thread_local_replica = model
            with torch.no_grad():
                if as_eval:
                    with training_mode_switcher(model, is_training=False):
                        self.custom_forward_fn(model)
                else:
                    self.custom_forward_fn(model)
        model.load_state_dict(sd)
        if isinstance(model, PostGraphBuildActing):
            model.post_build_graph_actions()
        context_to_use.disable_trace_dynamic_graph()
        return context_to_use.graph


def create_mock_tensor(input_info: ModelInputInfo, device: str):
    args = {'size': input_info.shape, 'dtype': input_info.type, 'device': device}
    if input_info.filler == ModelInputInfo.FILLER_TYPE_ZEROS:
        return torch.zeros(**args)
    if input_info.filler == ModelInputInfo.FILLER_TYPE_ONES:
        return torch.ones(**args)
    if input_info.filler == ModelInputInfo.FILLER_TYPE_RANDOM:
        return torch.rand(**args)
    raise RuntimeError


def get_model_device(model: torch.nn.Module) ->torch.device:
    try:
        device = next(model.parameters()).device
    except StopIteration:
        device = torch.device('cpu')
    return device


MODEL_INPUT_OP_NAME = 'nncf_model_input'


@register_operator(name=MODEL_INPUT_OP_NAME)
def nncf_model_input(tensor: 'torch.Tensor'):
    return tensor


class InputInfoWrapManager:
    INPUTS_MISMATCH_WARNING_TEXT = "Compression with regards to this input may occur incorrectly. Make sure you call the compressed model with inputs that correspond to what NNCF was configured to expect (either via NNCF config's input_infos, or customdummy_forward_fn/wrap_inputs_fn parameters), or that you know what you are doing. This warning will not be shown again."
    ARGS_INPUTS_MISMATCH_FORMAT_STRING = 'Inputs mismatch - could not find arg with idx {} in NNCF-wrapped model input args! ' + INPUTS_MISMATCH_WARNING_TEXT
    KWARGS_INPUTS_MISMATCH_FORMAT_STRING = "Inputs mismatch - could not find kwarg '{}' in NNCF-wrapped model input kwargs! " + INPUTS_MISMATCH_WARNING_TEXT

    def __init__(self, input_infos: List[ModelInputInfo], fwd_signature: Signature, module_ref_for_device: torch.nn.Module=None):
        self._module_ref_for_device = module_ref_for_device
        arg_iis_list = [ii for ii in input_infos if ii.keyword is None]
        kwarg_iis_list = [(ii.keyword, ii) for ii in input_infos if ii.keyword is not None]
        kwarg_iis = OrderedDict()
        arg_iis = tuple(arg_iis_list)
        for kw, ii in kwarg_iis_list:
            kwarg_iis[kw] = ii
        bound_params = fwd_signature.bind(*arg_iis, **kwarg_iis)
        self._fwd_params_to_input_infos_odict = bound_params.arguments
        self._fwd_signature = fwd_signature

    def set_device(self, device: str):
        self._device = device

    def wrap_inputs(self, model_args, model_kwargs):
        bound_model_params = self._fwd_signature.bind(*model_args, **model_kwargs)
        for param_name in self._fwd_params_to_input_infos_odict:
            param_kind = self._fwd_signature.parameters[param_name].kind
            if param_kind is Parameter.VAR_POSITIONAL or param_kind is Parameter.VAR_KEYWORD:
                nncf_logger.warning("An input_info tensor was bound to a *args or **kwargs variadic parameter in theforward's signature! This is currently unsupported by NNCF. Input compression may be incorrect.")
                continue
            if param_name not in bound_model_params.arguments:
                nncf_logger.warning("A call to a compressed model's forward occurred without one of the paramsspecified in input_infos! Input compression may be incorrect. Trying to recover by wrapping the default value for the parameter.")
                bound_model_params.apply_defaults()
            potential_tensor = bound_model_params.arguments[param_name]
            if potential_tensor is not None:
                bound_model_params.arguments[param_name] = nncf_model_input(bound_model_params.arguments[param_name])
            else:
                nncf_logger.info('Wrapping a dummy tensor for input {}'.format(param_name))
                info_for_missing_input = self._fwd_params_to_input_infos_odict[param_name]
                device = 'cuda'
                if self._module_ref_for_device is not None:
                    device = get_model_device(self._module_ref_for_device)
                dummy_tensor = create_mock_tensor(info_for_missing_input, device)
                _ = nncf_model_input(dummy_tensor)
        return bound_model_params.args, bound_model_params.kwargs


class InsertionPointGraphNodeType(Enum):
    PRE_HOOK = 0
    POST_HOOK = 1
    OPERATOR = 2


class PostHookInsertionPoint:

    def __init__(self, target_node_name: str):
        self.target_node_name = target_node_name

    def __str__(self):
        return self.target_node_name


class PreHookInsertionPoint:

    def __init__(self, target_node_name: str, input_port_id: int):
        self.target_node_name = target_node_name
        self.input_port_id = input_port_id

    def __str__(self):
        return str(self.input_port_id) + ' ' + self.target_node_name


MODEL_OUTPUT_OP_NAME = 'nncf_model_output'


MODEL_WRAPPED_BY_NNCF_ATTR_NAME = 'nncf_module'


NNCF_MODULES_MAP = {k.__name__: v.__name__ for k, v in NNCF_MODULES_DICT.items()}


NNCF_MODULES = list(NNCF_MODULES_MAP.keys())


class PTOperatorSubtype(PTOperatorMetatype):
    """
    Exact specialization of PTOperatorMetatype that can only be determined via operator argument
    inspection or owning module attribute inspection, and that may have specialized compression method
    configuration other than the one used for general operations having the type of PTOperatorMetatype.
    """

    @classmethod
    def matches(cls, layer_attributes: Optional[BaseLayerAttributes]=None, function_args=None, functions_kwargs=None) ->bool:
        raise NotImplementedError


class PTDepthwiseConvOperatorSubtype(PTOperatorSubtype):

    @classmethod
    def matches(cls, layer_attributes: Optional[BaseLayerAttributes]=None, function_args=None, functions_kwargs=None) ->bool:
        if not isinstance(layer_attributes, ConvolutionLayerAttributes):
            return False
        if layer_attributes.groups == layer_attributes.in_channels and layer_attributes.in_channels > 1:
            return True
        return False


class PTModuleOperatorSubtype(PTOperatorSubtype):

    @classmethod
    def matches(cls, layer_attributes: Optional[BaseLayerAttributes]=None, function_args=None, functions_kwargs=None) ->bool:
        key = DynamicGraph.IS_CALLED_INSIDE_NNCF_MODULE
        if functions_kwargs is None or key not in functions_kwargs:
            return False
        return functions_kwargs[key]


class PTModuleBatchNormMetatype(PTModuleOperatorSubtype):
    name = 'BatchNormOp'
    module_to_function_names = {NamespaceTarget.TORCH_NN_FUNCTIONAL: ['batch_norm']}


class PTInsertionType(IntEnum):
    NNCF_MODULE_PRE_OP = 0
    NNCF_MODULE_POST_OP = 1
    OPERATOR_PRE_HOOK = 2
    OPERATOR_POST_HOOK = 3


class PTInsertionPoint:
    TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT = {TargetType.PRE_LAYER_OPERATION: PTInsertionType.NNCF_MODULE_PRE_OP, TargetType.POST_LAYER_OPERATION: PTInsertionType.NNCF_MODULE_POST_OP, TargetType.OPERATION_WITH_WEIGHTS: PTInsertionType.NNCF_MODULE_PRE_OP, TargetType.OPERATOR_PRE_HOOK: PTInsertionType.OPERATOR_PRE_HOOK, TargetType.OPERATOR_POST_HOOK: PTInsertionType.OPERATOR_POST_HOOK}

    def _get_pt_insertion_type(self, target_type: TargetType) ->PTInsertionType:
        if not isinstance(target_type, TargetType) or target_type not in PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT:
            raise RuntimeError('Unsupported target type for PyTorch: {}'.format(target_type))
        return PTInsertionPoint.TARGET_TYPE_VS_PT_INSERTION_TYPE_DICT[target_type]

    def __init__(self, target_type: TargetType, op_address: OperationAddress, input_port_id: int=None):
        self.insertion_type = self._get_pt_insertion_type(target_type)
        self.op_address = op_address
        self.module_scope = op_address.scope_in_model
        self.input_port_id = input_port_id

    def __eq__(self, other: 'PTInsertionPoint'):
        return self.insertion_type == other.insertion_type and self.op_address == other.op_address and self.module_scope == other.module_scope and self.input_port_id == other.input_port_id

    def __str__(self):
        return ' '.join([str(v) for v in self.__dict__.values()])

    def __hash__(self):
        return hash(str(self))


QUANTIZATION_MODULES = Registry('quantization_modules')


class ShapeIgnoringTensorMetaComparator(TensorMetaComparator):

    def __call__(self, lhs: TensorMeta, rhs: TensorMeta) ->bool:
        return lhs.creator_id == rhs.creator_id and lhs.index == rhs.index


def compute_FLOPs_hook(module, input_, output, dict_to_save, module_node_name: NNCFNodeName):
    if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d, nn.Conv2d, nn.ConvTranspose2d, nn.Conv3d, nn.ConvTranspose3d)):
        ks = module.weight.data.shape
        mac_count = np.prod(ks) * np.prod(output.shape[2:])
    elif isinstance(module, nn.Linear):
        if len(input_[0].shape) == 1:
            mac_count = input_[0].shape[0] * output.shape[-1]
        else:
            mac_count = np.prod(input_[0].shape[1:]) * output.shape[-1]
    else:
        return
    dict_to_save[module_node_name] = 2 * mac_count


def create_dummy_forward_fn(input_infos: List[ModelInputInfo], with_input_tracing=False, wrap_inputs_fn=None, wrap_outputs_fn=None, with_output_tracing=False):

    def default_dummy_forward_fn(model):
        device = get_model_device(model)
        args_list = [create_mock_tensor(info, device) for info in input_infos if info.keyword is None]
        kwargs = OrderedDict()
        for info in input_infos:
            if info.keyword is not None:
                kwargs[info.keyword] = create_mock_tensor(info, device)
        args = tuple(args_list)
        if with_input_tracing:
            if wrap_inputs_fn is None:
                args, kwargs = wrap_nncf_model_inputs_with_objwalk(args, kwargs)
            else:
                args, kwargs = wrap_inputs_fn(args, kwargs)
        retval = model(*args, **kwargs)
        if with_output_tracing:
            retval = replicate_same_tensors(retval)
            if wrap_outputs_fn is not None:
                return wrap_outputs_fn(retval)
            return wrap_nncf_model_outputs_with_objwalk(retval)
        return retval
    return default_dummy_forward_fn


def debuggable_forward(forward_func):

    def decorated(self, *args, **kwargs):
        if self.debug_interface is not None:
            self.debug_interface.pre_forward_actions(module=self)
        retval = forward_func(self, *args, **kwargs)
        if self.debug_interface is not None:
            self.debug_interface.post_forward_actions(module=self)
        return retval
    return decorated


def matches_any(tested_str: str, str_or_list_to_match_to: Union[List[str], str]) ->bool:
    """
    Return True if tested_str matches at least one element in str_or_list_to_match_to.

    :param tested_str: One of the supported entity types to be matched - currently possible to pass either
        NNCFNodeName (to refer to the original model operations) or QuantizerId (to refer to specific quantizers).
    :param str_or_list_to_match_to: A list of strings specifying for the serializable_id. Entries of the strings
        may be prefixed with `{re}` to enable regex matching.

    :return: A boolean value specifying whether a tested_str should matches at least one element
        in str_or_list_to_match_to.
    """
    if str_or_list_to_match_to is None:
        return False
    str_list = [str_or_list_to_match_to] if isinstance(str_or_list_to_match_to, str) else str_or_list_to_match_to
    for item in str_list:
        if '{re}' in item:
            regex = item.replace('{re}', '')
            if re.search(regex, tested_str):
                return True
        elif tested_str == item:
            return True
    return False


def get_all_modules_by_type(model, module_types=None, current_scope=None, ignored_scopes=None, target_scopes=None) ->Dict['Scope', Module]:
    if isinstance(module_types, str):
        module_types = [module_types]
    found = OrderedDict()
    if current_scope is None:
        current_scope = Scope()
        current_scope.push(ScopeElement(model.__class__.__name__))
    for name, module in model.named_children():
        child_scope_element = ScopeElement(module.__class__.__name__, name)
        child_scope = current_scope.copy()
        child_scope.push(child_scope_element)
        if matches_any(str(child_scope), ignored_scopes):
            continue
        if target_scopes is None or matches_any(str(child_scope), target_scopes):
            if module_types is None or module_types.count(str(type(module).__name__)) != 0:
                found[child_scope] = module
            sub_found = get_all_modules_by_type(module, module_types, current_scope=child_scope, ignored_scopes=ignored_scopes, target_scopes=target_scopes)
            if sub_found:
                found.update(sub_found)
    return found


def get_state_dict_names_with_modules(model: 'NNCFNetwork', str_types: List[str]=None, prefix='') ->Dict[str, torch.nn.Module]:
    found = OrderedDict()
    for name, module in model.named_children():
        full_node_name = '{}{}'.format(prefix, name)
        if str_types is not None and type(module).__name__ in str_types:
            found[full_node_name] = module
        sub_found = get_state_dict_names_with_modules(module, str_types, prefix=full_node_name + '.')
        if sub_found:
            found.update(sub_found)
    return found


_IGNORED_SCOPES = []


def ignore_scope(cls):
    if cls not in _IGNORED_SCOPES:
        _IGNORED_SCOPES.append(cls)
    return cls


def is_tuple(obj) ->bool:
    return isinstance(obj, tuple)


def is_named_tuple(obj) ->bool:
    return is_tuple(obj) and obj.__class__ != tuple


def to_tuple(lst: List, named_tuple_class: Type=None, named_tuple_fields: List[str]=None) ->Tuple:
    if named_tuple_fields is None:
        return tuple(lst)
    return named_tuple_class(*lst)


def objwalk(obj, unary_predicate: Callable[[Any], bool], apply_fn: Callable, memo=None):
    """
    Walks through the indexable container hierarchy of obj and replaces all sub-objects matching a criterion
    with the result of a given function application.
    """
    if memo is None:
        memo = set()
    named_tuple_class = None
    named_tuple_fields = None
    if is_named_tuple(obj):
        named_tuple_class = obj.__class__
        named_tuple_fields = obj._fields
    was_tuple = is_tuple(obj)
    if was_tuple:
        obj = list(obj)
    iterator = maybe_get_iterator(obj)
    if iterator is not None:
        if id(obj) not in memo:
            memo.add(id(obj))
            indices_to_apply_fn_to = set()
            indices_vs_named_tuple_data = {}
            for idx, value in iterator(obj):
                next_level_it = maybe_get_iterator(value)
                if next_level_it is None:
                    if unary_predicate(value):
                        indices_to_apply_fn_to.add(idx)
                elif is_tuple(value):
                    processed_tuple = objwalk(value, unary_predicate, apply_fn, memo)
                    if is_named_tuple(value):
                        indices_vs_named_tuple_data[idx] = processed_tuple, value.__class__, value._fields
                    else:
                        indices_vs_named_tuple_data[idx] = processed_tuple, None, None
                else:
                    objwalk(value, unary_predicate, apply_fn)
            for idx in indices_to_apply_fn_to:
                obj[idx] = apply_fn(obj[idx])
            for idx, tpl_data in indices_vs_named_tuple_data.items():
                tpl, n_tpl_class, n_tpl_fields = tpl_data
                obj[idx] = to_tuple(tpl, n_tpl_class, n_tpl_fields)
            memo.remove(id(obj))
    elif unary_predicate(obj):
        return apply_fn(obj)
    if was_tuple:
        return to_tuple(obj, named_tuple_class, named_tuple_fields)
    return obj


def is_nncf_module(module: nn.Module) ->bool:
    """
    Checks weather given module is an instance of
    a nncf layer or not.

    :param module: Module to check.
    :returns: True if module is an instance of a nncf layer
        otherwise False.
    """
    for nncf_module_name in NNCF_MODULES:
        if module.__class__.__name__ == nncf_module_name:
            return True
    for nncf_user_wrapped_class in NNCF_WRAPPED_USER_MODULES_DICT.values():
        if module.__class__.__name__ == nncf_user_wrapped_class.__name__:
            return True
    return False


def add_nncf_functionality_to_user_module(module: torch.nn.Module):
    user_class = module.__class__
    assert user_class.__name__ in UNWRAPPED_USER_MODULES.registry_dict
    module.__class__ = NNCF_WRAPPED_USER_MODULES_DICT[user_class]
    _NNCFModuleMixin.add_mixin_fields(module)
    return module


def replace_module_by_nncf_module(module: nn.Module) ->nn.Module:
    """
    Returns updated modules for modules that could be replaced by a nncf layer.

    :param module: Candidate module for the replacement
    :returns: A correspondent nncf layer if it is possible
        and given module otherwise.
    """
    for nncf_module_type, module_type in NNCF_MODULES_DICT.items():
        if module.__class__.__name__ == module_type.__name__:
            nncf_module = module
            if not module.__class__.__name__ == nncf_module_type.__name__:
                nncf_module = nncf_module_type.from_module(module)
            return nncf_module
    for _, user_module_type in UNWRAPPED_USER_MODULES.registry_dict.items():
        if module.__class__ == user_module_type:
            nncf_module = deepcopy(module)
            nncf_module = add_nncf_functionality_to_user_module(nncf_module)
            return nncf_module
    return module


def set_replaced_module_by_name(model: nn.Module, name: str, replaced_module: nn.Module) ->None:
    """
    Replaces `model` nested module with name `name` by the `replaced_module`.

    :param model: Module to replace nested module.
    :param name: Name of the target nested module to replace.
    :param replaced_module: Module to be placed instead of replaced one.
    """
    if isinstance(model, nn.Sequential):
        model._modules[name] = replaced_module
    else:
        setattr(model, name, replaced_module)


def replace_modules(model: nn.Module, replace_fn: Callable[[nn.Module], Optional[nn.Module]], stop_branching_fn: Callable[[nn.Module], bool], affected_scopes: List[ScopeElement], ignored_scopes: Optional[List[str]]=None, target_scopes: Optional[List[str]]=None, memo: Optional[Set[ScopeElement]]=None, current_scope: Optional[List[ScopeElement]]=None, eval_op_scopes: Optional[List[Scope]]=None, reset: Optional[bool]=False) ->Tuple[nn.Module, List[Scope]]:
    """
    Recursive function to replace target children modules inside given `model` according to
    given `replace_fn` function. Does not replace modules inside already replaced modules.
    Does not replace target module if it is not inside non target module.

    :param model: Model to replace internal modules.
    :param replace_fn: Callable that returns updated modules for modules
        that should be replaced.
    :param stop_branching_fn: Condition whether to stop recursive call for given model.
    :param affected_scopes: Recursive buffer for a list of scopes that contain modules
        that was replaced during function execution.
    :param ignored_scopes: List of scope all modules from that should be ignored.
    :param target_scopes: List of scope all modules from that could be replaced.
        If present, all other scopes become ignored. Has lower priority that ignored_scopes.
    :param memo: Set of all seen before modules.
    :param current_scope: List of all scope elements of given module.
    :param eval_op_scopes: List of scopes of modules that are executed in evaluation mode only.
    :param reset: Should reset wrapped nncf modules after replacement or not.
    :returns: Tuple of a module with target children modules replaced according to `replace_fn`
        and list of scopes that contain replaced target children modules.
    """
    if memo is None:
        memo = set()
        current_scope = Scope()
        current_scope.push(ScopeElement(model.__class__.__name__))
    if model in memo:
        return model, affected_scopes
    memo.add(model)
    for name, module in model.named_children():
        if module is None:
            continue
        child_scope_element = ScopeElement(module.__class__.__name__, name)
        child_scope = current_scope.copy()
        child_scope.push(child_scope_element)
        replaced_module = replace_fn(module)
        if replaced_module is not None:
            replaced_scope_element = ScopeElement(replaced_module.__class__.__name__, name)
            replaced_scope = current_scope.copy()
            replaced_scope.push(replaced_scope_element)
            if module is not replaced_module:
                if matches_any(str(child_scope), ignored_scopes):
                    nncf_logger.info(f'Not processing a module that matched to ignored scope in config: {child_scope}')
                    continue
                if eval_op_scopes is None:
                    eval_op_scopes = []
                is_ignored = True
                for eval_op_scope in eval_op_scopes:
                    if eval_op_scope in child_scope:
                        is_ignored = False
                        break
                if is_ignored and eval_op_scopes:
                    nncf_logger.info(f'Not processing a module not called in eval mode: {child_scope}')
                    continue
                if target_scopes is None or matches_any(str(child_scope), target_scopes):
                    nncf_logger.debug(f'Wrapping module {str(child_scope)} by {str(replaced_scope)}')
                    set_replaced_module_by_name(model, name, replaced_module)
                    affected_scopes.append(replaced_scope)
            elif is_nncf_module(replaced_module):
                affected_scopes.append(replaced_scope)
                if reset:
                    replaced_module.reset()
        if stop_branching_fn(module):
            continue
        if replaced_module is None or module is replaced_module:
            _, affected_scopes = replace_modules(module, replace_fn, stop_branching_fn, affected_scopes, ignored_scopes, target_scopes, memo, child_scope, eval_op_scopes, reset=reset)
    return model, affected_scopes


def replace_modules_by_nncf_modules(model: nn.Module, ignored_scopes: Optional[List[str]]=None, target_scopes: Optional[List[str]]=None, eval_op_scopes: Optional[List[Scope]]=None, reset: Optional[bool]=False) ->Tuple[nn.Module, List[Scope]]:
    """
    Function to replace target children modules inside given `model` to
    the nncf modules enumerated in NNCF_MODULE_DICT and UNWRAPPED_USER_MODULES.
    Does not replace modules inside already replaced modules.
    Does not replace target module if it is not inside non target module.

    :param model: Model to replace internal modules.
    :param ignored_scopes: List of scope all modules from that should be ignored.
    :param target_scopes: List of scope all modules from that could be replaced.
        If present, all other scopes become ignored. Has lower priority that ignored_scopes.
    :param eval_op_scopes: List of scopes of modules that are executed in evaluation mode only.
    :param reset: Should reset wrapped nncf modules after replacement or not.
    :returns: Tuple of a module with target children modules replaced according to `replace_fn`
        and list of scopes that contain replaced target children modules.
    """
    replace_fn = partial(replace_module_by_nncf_module)
    affected_scopes = []
    return replace_modules(model, replace_fn, is_nncf_module, affected_scopes, ignored_scopes=ignored_scopes, target_scopes=target_scopes, eval_op_scopes=eval_op_scopes, reset=reset)


def is_tensor(obj):
    return isinstance(obj, torch.Tensor)


def replicate_same_tensors(obj: Any) ->Any:
    """
    Required to handle the situation when multiple references to one and the
    same tensor are present in the input. If tensor replication is not done, then
    at runtime one and the same tensor could be wrapped by input/output wrappers twice,
    which will disrupt the traced graph structure and possibly hook calls.
    """
    observed_tensor_object_ids = set()

    def replicate_fn(tensor: torch.Tensor) ->torch.Tensor:
        tensor_object_id = id(tensor)
        if tensor_object_id in observed_tensor_object_ids:
            with forward_nncf_trace():
                return tensor.clone()
        observed_tensor_object_ids.add(tensor_object_id)
        return tensor
    obj = objwalk(obj, is_tensor, replicate_fn)
    return obj


def is_traced_tensor(obj):
    return isinstance(obj, TracedTensor)


@register_operator(name=MODEL_OUTPUT_OP_NAME)
def nncf_model_output(tensor: 'torch.Tensor'):
    return tensor


def wrap_nncf_model_outputs_with_objwalk(model_outputs):
    model_outputs = objwalk(model_outputs, is_traced_tensor, nncf_model_output)
    return model_outputs


class RNNCellBaseNNCF(nn.Module):
    __constants__ = ['input_size', 'hidden_size', 'bias']

    def __init__(self, input_size, hidden_size, bias, num_chunks):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bias = bias
        linear_ih = nn.Linear(input_size, num_chunks * hidden_size, self.bias)
        linear_hh = nn.Linear(hidden_size, num_chunks * hidden_size, self.bias)
        self.weight_ih = linear_ih.weight
        self.weight_hh = linear_hh.weight
        self.bias_ih = linear_ih.bias
        self.bias_hh = linear_hh.bias
        self.linear_list = [linear_ih, linear_hh]
        self.reset_parameters()

    def extra_repr(self):
        s = '{input_size}, {hidden_size}'
        if 'bias' in self.__dict__ and self.bias is not True:
            s += ', bias={bias}'
        if 'nonlinearity' in self.__dict__ and self.nonlinearity != 'tanh':
            s += ', nonlinearity={nonlinearity}'
        return s.format(**self.__dict__)

    def check_forward_input(self, input_):
        if input_.size(1) != self.input_size:
            raise RuntimeError('input_ has inconsistent input_size: got {}, expected {}'.format(input_.size(1), self.input_size))

    def check_forward_hidden(self, input_, hx, hidden_label=''):
        if input_.size(0) != hx.size(0):
            raise RuntimeError("Input batch size {} doesn't match hidden{} batch size {}".format(input_.size(0), hidden_label, hx.size(0)))
        if hx.size(1) != self.hidden_size:
            raise RuntimeError('hidden{} has inconsistent hidden_size: got {}, expected {}'.format(hidden_label, hx.size(1), self.hidden_size))

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            init.uniform_(weight, -stdv, stdv)

    def forward(self, input_, hidden):
        raise NotImplementedError


ITERATION_MODULES = Registry('iteration_modules')


class LSTMCellForwardNNCF(nn.Module):

    def __init__(self, input_linear, hidden_linear):
        super().__init__()
        self.input_linear = input_linear
        self.hidden_linear = hidden_linear

    def forward(self, input_, hidden):
        hx, cx = hidden
        gates = self.input_linear(input_) + self.hidden_linear(hx)
        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)
        ingate = F.sigmoid(ingate)
        forgetgate = F.sigmoid(forgetgate)
        cellgate = F.tanh(cellgate)
        outgate = F.sigmoid(outgate)
        cy = forgetgate * cx + ingate * cellgate
        hy = outgate * F.tanh(cy)
        return hy, cy


class LSTMCellNNCF(RNNCellBaseNNCF):

    def __init__(self, input_size=1, hidden_size=1, bias=True):
        super().__init__(input_size, hidden_size, bias, num_chunks=4)
        self.cell = LSTMCellForwardNNCF(self.linear_list[0], self.linear_list[1])

    def forward(self, input_, hidden=None):
        self.check_forward_input(input_)
        if hidden is None:
            zeros = torch.zeros(input_.size(0), self.hidden_size, dtype=input_.dtype, device=input_.device)
            hidden = zeros, zeros
        self.check_forward_hidden(input_, hidden[0], '[0]')
        self.check_forward_hidden(input_, hidden[1], '[1]')
        return self.cell(input_, hidden)


class StackedRNN(nn.Module):


    class StackedRNNResetPoint(nn.Module):
        """
        Intentionally wrap concat, which is called inside nested loops, as a separate module.
        It allows not to add new node to nncf graph on each iteration of the loops.
        """

        def forward(self, all_output, input_):
            input_ = torch.cat(all_output, input_.dim() - 1)
            return input_

    def __init__(self, inners, num_layers, lstm=False, dropout=0):
        super().__init__()
        self.lstm = lstm
        self.num_layers = num_layers
        self.num_directions = int(len(inners) / num_layers)
        self.inners = nn.ModuleList(inners)
        self.total_layers = self.num_layers * self.num_directions
        self.dropout = dropout

    def forward(self, input_, hidden, batch_sizes):
        next_hidden = []
        if self.lstm:
            hidden = list(zip(*hidden))
        for i in range(self.num_layers):
            all_output = []
            for j in range(self.num_directions):
                l = i * self.num_directions + j
                hy, output = self.inners[l](input_, hidden[l], batch_sizes)
                next_hidden.append(hy)
                all_output.append(output)
            input_ = self.StackedRNNResetPoint()(all_output, input_)
            if self.dropout != 0 and i < self.num_layers - 1:
                input_ = F.dropout(input_, p=self.dropout, training=self.training, inplace=False)
        if self.lstm:
            next_h, next_c = zip(*next_hidden)
            next_hidden = torch.cat(next_h, 0).view(self.total_layers, *next_h[0].size()), torch.cat(next_c, 0).view(self.total_layers, *next_c[0].size())
        else:
            next_hidden = torch.cat(next_hidden, 0).view(self.total_layers, *next_hidden[0].size())
        return next_hidden, input_


class Recurrent(nn.Module):

    def __init__(self, cell, reverse=False):
        super().__init__()
        self.reverse = reverse
        self.cell = cell

    def forward(self, input_, hidden, batch_sizes=None):
        output = []
        steps = range(input_.size(0) - 1, -1, -1) if self.reverse else range(input_.size(0))
        for i in steps:
            with forward_nncf_trace():
                hidden_input = input_[i]
            hidden = self.cell(hidden_input, hidden)
            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)
        if self.reverse:
            output.reverse()
        output = torch.cat(output, 0).view(input_.size(0), *output[0].size())
        return hidden, output


class VariableRecurrent(nn.Module):

    def __init__(self, cell):
        super().__init__()
        self.cell = cell

    def forward(self, input_, hidden, batch_sizes):
        output = []
        input_offset = 0
        last_batch_size = batch_sizes[0]
        hiddens = []
        flat_hidden = not isinstance(hidden, tuple)
        if flat_hidden:
            hidden = hidden,
        with forward_nncf_trace():
            batch_size_elements = [b for b in batch_sizes]
        for batch_size in batch_size_elements:
            step_input = input_[input_offset:input_offset + batch_size]
            input_offset += batch_size
            bs_decrease = last_batch_size - batch_size
            if bs_decrease > 0:
                hidden_len = len(hidden)
                hidden_offset_elts = []
                hidden_offset_elts_reversed = []
                for i in range(hidden_len):
                    with forward_nncf_trace():
                        hidden_offset_elts.append(hidden[i][-bs_decrease:])
                        hidden_offset_elts_reversed.append(hidden[i][:-bs_decrease])
                hiddens.append(tuple(hidden_offset_elts))
                hidden = tuple(hidden_offset_elts_reversed)
            last_batch_size = batch_size
            if flat_hidden:
                hidden = self.cell(step_input, hidden[0]),
            else:
                hidden = self.cell(step_input, hidden)
            output.append(hidden[0])
        hiddens.append(hidden)
        hiddens.reverse()
        hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))
        assert hidden[0].size(0) == batch_sizes[0]
        if flat_hidden:
            hidden = hidden[0]
        output = torch.cat(output, 0)
        return hidden, output


class VariableRecurrentReverse(nn.Module):

    def __init__(self, cell):
        super().__init__()
        self.cell = cell

    def forward(self, input_, hidden, batch_sizes):
        output = []
        input_offset = input_.size(0)
        last_batch_size = batch_sizes[-1]
        initial_hidden = hidden
        flat_hidden = not isinstance(hidden, tuple)
        if flat_hidden:
            hidden = hidden,
            initial_hidden = initial_hidden,
        hidden = tuple(h[:batch_sizes[-1]] for h in hidden)
        for batch_size in reversed(batch_sizes):
            inc = batch_size - last_batch_size
            hidden = self.ReverseResetPoint()(batch_size, hidden, inc, initial_hidden, last_batch_size)
            last_batch_size = batch_size
            step_input = input_[input_offset - batch_size:input_offset]
            input_offset -= batch_size
            if flat_hidden:
                hidden = self.cell(step_input, hidden[0]),
            else:
                hidden = self.cell(step_input, hidden)
            output.append(hidden[0])
        output.reverse()
        output = torch.cat(output, 0)
        if flat_hidden:
            hidden = hidden[0]
        return hidden, output


    class ReverseResetPoint(nn.Module):
        """
        Intentionally wrap concat undef if condition as a separate module
        to prevent adding new node to nncf graph on each iteration
        """

        def forward(self, batch_size, hidden, inc, initial_hidden, last_batch_size):
            if inc > 0:
                hidden = tuple(torch.cat((h, ih[last_batch_size:batch_size]), 0) for h, ih in zip(hidden, initial_hidden))
            return hidden


def variable_recurrent_factory():

    def factory(cell, reverse=False):
        if reverse:
            return VariableRecurrentReverse(cell)
        return VariableRecurrent(cell)
    return factory


class NNCF_RNN(nn.Module):
    """Common class for RNN modules. Currently, LSTM is supported only"""

    def __init__(self, mode='LSTM', input_size=1, hidden_size=1, num_layers=1, batch_first=False, dropout=0, bidirectional=False, bias=True):
        super().__init__()
        self.mode = mode
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.num_directions = 2 if bidirectional else 1
        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or isinstance(dropout, bool):
            raise ValueError('dropout should be a number in range [0, 1] representing the probability of an element being zeroed')
        if dropout > 0 and num_layers == 1:
            nncf_logger.debug(f'dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout={dropout} and num_layers={num_layers}')
        if mode == 'LSTM':
            gate_size = 4 * hidden_size
            self.cell_type = LSTMCellForwardNNCF
        else:
            raise ValueError('Unrecognized RNN mode: ' + mode)
        self._all_weights = []
        self.cells = []
        for layer in range(num_layers):
            for direction in range(self.num_directions):
                layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions
                linear_ih = nn.Linear(layer_input_size, gate_size, bias)
                linear_hh = nn.Linear(hidden_size, gate_size, bias)
                self.cells.append(self.cell_type(linear_ih, linear_hh))
                params = linear_ih.weight, linear_hh.weight, linear_ih.bias, linear_hh.bias
                suffix = '_reverse' if direction == 1 else ''
                weight_names = ['weight_ih_l{}{}', 'weight_hh_l{}{}']
                if bias:
                    weight_names += ['bias_ih_l{}{}', 'bias_hh_l{}{}']
                weight_names = [x.format(layer, suffix) for x in weight_names]
                for name, param in zip(weight_names, params):
                    setattr(self, name, param)
                self._all_weights.append(weight_names)
        self.reset_parameters()
        self.variable_length = True
        self.rnn_impl = self.get_rnn_impl(self.variable_length, self.cells)

    def get_rnn_impl(self, variable_length, cells):
        if variable_length:
            rec_factory = variable_recurrent_factory()
        else:
            rec_factory = Recurrent
        inners = []
        for layer_idx in range(self.num_layers):
            idx = layer_idx * self.num_directions
            if self.bidirectional:
                layer_inners = [rec_factory(cells[idx]), rec_factory(cells[idx + 1], reverse=True)]
            else:
                layer_inners = [rec_factory(cells[idx])]
            inners.extend(layer_inners)
        return StackedRNN(inners, self.num_layers, self.mode == 'LSTM', dropout=self.dropout)

    def check_forward_args(self, input_, hidden, batch_sizes):
        is_input_packed = batch_sizes is not None
        expected_input_dim = 2 if is_input_packed else 3
        if input_.dim() != expected_input_dim:
            raise RuntimeError('input_ must have {} dimensions, got {}'.format(expected_input_dim, input_.dim()))
        if self.input_size != input_.size(-1):
            raise RuntimeError('input_.size(-1) must be equal to input_size. Expected {}, got {}'.format(self.input_size, input_.size(-1)))
        if is_input_packed:
            mini_batch = int(batch_sizes[0])
        else:
            mini_batch = input_.size(0) if self.batch_first else input_.size(1)
        expected_hidden_size = mini_batch, self.hidden_size

        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):
            expected_size = self.num_layers * self.num_directions
            if expected_size != len(hx):
                raise RuntimeError('Expected number of hidden states {}, got {}'.format(expected_size, len(hx)))
            for element in hx:
                if tuple(element.size()) != expected_hidden_size:
                    raise RuntimeError(msg.format(expected_hidden_size, tuple(element.size())))
        if self.mode == 'LSTM':
            check_hidden_size(hidden[0], expected_hidden_size, 'Expected hidden[0] size {}, got {}')
            check_hidden_size(hidden[1], expected_hidden_size, 'Expected hidden[1] size {}, got {}')
        else:
            check_hidden_size(hidden, expected_hidden_size)

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            init.uniform_(weight, -stdv, stdv)

    @property
    def all_weights(self):
        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]

    @staticmethod
    def apply_permutation(tensor, permutation, dim=1):
        return tensor.index_select(dim, permutation)

    def permute_hidden(self, hx, permutation):
        if permutation is None:
            return hx
        return self.apply_permutation(hx[0], permutation), self.apply_permutation(hx[1], permutation)

    def prepare_hidden(self, hx, permutation):
        if permutation is None:
            return hx
        split_size = len(hx[0])
        concat_hx = torch.cat([torch.unsqueeze(t, 0) for t in hx[0]])
        concat_cx = torch.cat([torch.unsqueeze(t, 0) for t in hx[1]])
        permuted_hidden = self.apply_permutation(concat_hx, permutation), self.apply_permutation(concat_cx, permutation)
        hc = permuted_hidden[0].chunk(split_size, 0)
        cc = permuted_hidden[1].chunk(split_size, 0)
        hidden = tuple(torch.squeeze(c, 0) for c in hc), tuple(torch.squeeze(c, 0) for c in cc)
        return hidden

    def forward(self, input_, hidden=None):
        is_packed = isinstance(input_, PackedSequence)
        sorted_indices = None
        unsorted_indices = None
        if is_packed:
            input_, batch_sizes, sorted_indices, unsorted_indices = input_
            max_batch_size = int(batch_sizes[0])
        else:
            batch_sizes = None
            max_batch_size = input_.size(0) if self.batch_first else input_.size(1)
        if hidden is None:
            num_directions = 2 if self.bidirectional else 1
            hidden = torch.zeros(self.num_layers * num_directions, max_batch_size, self.hidden_size, requires_grad=False, device=input_.device)
            if self.mode == 'LSTM':
                hidden = hidden, hidden
        else:
            hidden = self.prepare_hidden(hidden, sorted_indices)
        self.check_forward_args(input_, hidden, batch_sizes)
        is_currently_variable = batch_sizes is not None
        if self.variable_length and not is_currently_variable or not self.variable_length and is_currently_variable:
            self.rnn_impl = self.get_rnn_impl(is_currently_variable, self.cells)
        if self.batch_first and batch_sizes is None:
            input_ = input_.transpose(0, 1)
        hidden, output = self.rnn_impl(input_, hidden, batch_sizes)
        if self.batch_first and batch_sizes is None:
            output = output.transpose(0, 1)
        if is_packed:
            output = PackedSequence(output, batch_sizes, sorted_indices, unsorted_indices)
        return output, self.permute_hidden(hidden, unsorted_indices)


class BaseOp(nn.Module):

    def __init__(self, op):
        super().__init__()
        self.op = op

    @property
    def operand(self):
        return self.op

    def forward(self, *inputs, **kwargs):
        return self.op(*inputs, **kwargs)


class UpdateInputs(BaseOp):
    """
    A module which updates inputs for a module
    fed to forward method call by operand call.
    """

    def __call__(self, _, inputs):
        return super().__call__(*inputs)


class UpdateParameter(BaseOp):
    """
    A module which updates the attribute by a given of a module
    fed to forward method call by operand call.
    """

    def __init__(self, param_name, op):
        super().__init__(op)
        self._param_name = param_name

    def __call__(self, module, _):
        if not hasattr(module, self._param_name):
            raise TypeError('{} should have {} attribute'.format(type(module), self._param_name))
        value = getattr(module, self._param_name)
        result = super().__call__(value)
        setattr(module, self._param_name, result)


class UpdateWeight(UpdateParameter):
    """
    A module which updates `weight` attributes of a module
    fed to forward method call by operand call.
    """

    def __init__(self, op):
        super().__init__('weight', op)


class UpdateParameterList(BaseOp):
    """
    A module which updates attributes by a given list of names of a module fed to
    forward method call by operand call.
    """

    def __init__(self, param_names: List[str], op: Callable, is_optional_list: Optional[List[bool]]=None):
        super().__init__(op)
        self._param_names = param_names
        if is_optional_list is None:
            is_optional_list = [(False) for _ in param_names]
        self._is_optional_list = is_optional_list

    def __call__(self, module, _):
        param_values = []
        for param_name, is_optional in zip(self._param_names, self._is_optional_list):
            if not hasattr(module, param_name):
                if is_optional:
                    param_values.append(None)
                    continue
                raise TypeError('{} should have {} attribute'.format(type(module), param_name))
            param_values.append(getattr(module, param_name))
        updated_kwargs = dict(zip(self._param_names, param_values))
        updated_values = super().__call__(**updated_kwargs)
        for param_name, updated_value in zip(self._param_names, updated_values):
            setattr(module, param_name, updated_value)


class UpdateWeightAndBias(UpdateParameterList):
    """
    A module which updates `weight` and `bias` attributes of a module
    fed to forward method call by operand call.
    """

    def __init__(self, op):
        super().__init__(['weight', 'bias'], op)


class UpdateWeightAndOptionalBias(UpdateParameterList):
    """
    A module which updates `weight` and optionally `bias` attributes of a module
    fed to forward method call by operand call. If the module doesn't have bias attribute, None will be passed instead
    of it.
    """

    def __init__(self, op):
        super().__init__(['weight', 'bias'], op, [False, True])


class UpdatePaddingValue(UpdateParameter):
    """
    A module which updates `nncf_padding` attributes of a module
    fed to forward method call by operand call. Eventually, that will be used to apply a custom padding value.
    """

    def __init__(self, op):
        super().__init__(NNCF_PADDING_VALUE_ATTR_NAME, op)


class UpdateNumGroups(UpdateParameter):
    """
    A module which updates `groups` attribute of a module
    fed to forward method call by operand call.
    """

    def __init__(self, op):
        super().__init__('groups', op)


class UpdatePadding(UpdateParameter):
    """
    A module which updates `padding` attribute of a module
    fed to forward method call by operand call.
    """

    def __init__(self, op):
        super().__init__('padding', op)


class UpdateBatchNormParams(UpdateParameterList):
    """
    A module which updates attribute of batch norm module
    fed to forward method call by operand call.
    """

    def __init__(self, op):
        super().__init__(['weight', 'bias', 'running_mean', 'running_var'], op)


class UpdateLayerNormParams(UpdateParameterList):
    """
    A module which updates attribute of layer norm module
    fed to forward method call by operand call.
    """

    def __init__(self, op):
        super().__init__(['weight', 'bias', 'normalized_shape'], op)


def broadcast_filter_mask(filter_mask, shape, dim=0):
    broadcasted_shape = np.ones(len(shape), dtype=np.int64)
    broadcasted_shape[dim] = filter_mask.size(0)
    broadcasted_filter_mask = torch.reshape(filter_mask, tuple(broadcasted_shape))
    return broadcasted_filter_mask


def apply_filter_binary_mask(filter_mask: torch.Tensor, module_parameter: torch.nn.Parameter, node_name_for_logging: NNCFNodeName='', dim: int=0):
    """
    Applying binary filter mask to parameter of the module - usually to weight/bias of convolution or linear layer.
    Mask is applied to a given dimension without overriding parameter's values.
    :param filter_mask: binary mask (should have the same shape as conv weight on the given dimension)
    :param module_parameter: a tensor representing a module parameter (e.g. weight or bias of convolution)
    :param node_name_for_logging: name of the module to which the mask is applied
    :param dim: a dimension to apply the mask (0 by default)
    :return: result with applied mask
    """
    if filter_mask.size(0) != module_parameter.size(dim):
        raise RuntimeError("Shape of mask = {} for module {} isn't broadcastable to weight shape={}. ".format(filter_mask.shape, node_name_for_logging, module_parameter.shape))
    broadcasted_filter_mask = broadcast_filter_mask(filter_mask, module_parameter.shape, dim)
    return module_parameter.mul(broadcasted_filter_mask)


class FilterPruningMask(nn.Module):
    """
    A module contains the mask for pruning.
    On forward pass applying the mask to weight and bias of the module.
    """

    def __init__(self, size, node_name, dim=0):
        super().__init__()
        self.register_buffer('_binary_filter_pruning_mask', torch.ones(size))
        self.mask_applying_dim = dim
        self.node_name = node_name

    @property
    def binary_filter_pruning_mask(self):
        return self._binary_filter_pruning_mask

    @binary_filter_pruning_mask.setter
    def binary_filter_pruning_mask(self, mask):
        with torch.no_grad():
            self._binary_filter_pruning_mask.set_(mask)

    def forward(self, **params):
        new_params = []
        for param_name, param_value in params.items():
            if param_value is None:
                new_params.append(param_value)
                continue
            dim = 0 if param_name == 'bias' else self.mask_applying_dim
            new_params.append(apply_filter_binary_mask(self.binary_filter_pruning_mask, param_value, node_name_for_logging=self.node_name, dim=dim))
        return new_params


class ExportQuantizeToFakeQuantize(torch.autograd.Function):

    @staticmethod
    def symbolic(g, input_, levels, input_low, input_high, output_low, output_high):
        output = g.op(add_domain('FakeQuantize'), input_, input_low, input_high, output_low, output_high, levels_i=levels)
        output.setType(input_.type())
        return output

    @staticmethod
    def forward(ctx, input_, levels, input_low, input_high, output_low, output_high):
        return torch.clone(input_)

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


class ExportQuantizeToONNXQuantDequant(torch.autograd.Function):

    @staticmethod
    def symbolic(g, input_, y_scale, y_zero_point, axis):
        quantized = g.op('QuantizeLinear', input_, y_scale, y_zero_point, axis_i=axis)
        dequantized = g.op('DequantizeLinear', quantized, y_scale, y_zero_point, axis_i=axis)
        return dequantized

    @staticmethod
    def forward(ctx, input_, y_scale, y_zero_point, axis):
        return torch.clone(input_)

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


class ParametersRegistry:
    """
    Provides an interface to register parameters and get access to all of them.
    """

    def __init__(self):
        self._parameters_names = set()

    def register(self, parameter_name: str):
        self._parameters_names.add(parameter_name)

    def get_parameters_names(self) ->Set[str]:
        return self._parameters_names


OPTIONAL_PARAMETERS_REGISTRY = ParametersRegistry()


class PTQSpecStateNames:
    NUM_BITS = 'num_bits'
    MODE = 'mode'
    SIGNED_TO_FORCE = 'signedness_to_force'
    NARROW_RANGE = 'narrow_range'
    HALF_RANGE = 'half_range'
    SCALE_SHAPE = 'scale_shape'
    LOGARITHM_SCALE = 'logarithm_scale'
    IS_QUANTIZED_ON_EXPORT = 'is_quantized_on_export'
    COMPRESSION_LR_MULTIPLIER = 'compression_lr_multiplier'


class QuantizationMode:
    SYMMETRIC = 'symmetric'
    ASYMMETRIC = 'asymmetric'


QUANTIZATION_BITS = 8


QUANTIZATION_PER_CHANNEL = False


class QuantizerConfig:
    """
    A generic, framework-agnostic information on a configuration of a quantizer for abstract reasoning
    and determination of a quantizer setup scheme for a given model.
    """

    def __init__(self, num_bits: int=QUANTIZATION_BITS, mode: QuantizationMode=QuantizationMode.SYMMETRIC, signedness_to_force: Optional[bool]=None, per_channel: bool=QUANTIZATION_PER_CHANNEL):
        """
        :param num_bits: Bitwidth of the quantization.
        :param mode: The mode of quantization (symmetric or asymmetric).
        :param signedness_to_force: True if the quantizer *must* be signed, False if *must* be unsigned,
            None if the signed/unsigned attribute should be determined based on the incoming activation
            statistics during range initialization.
        :param per_channel: True for per-channel quantization, False for per-tensor.
        """
        self.num_bits = num_bits
        self.mode = mode
        self.signedness_to_force = signedness_to_force
        self.per_channel = per_channel

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __str__(self):
        return 'B:{bits} M:{mode} SGN:{signedness} PC:{per_channel}'.format(bits=self.num_bits, mode='S' if self.mode == QuantizationMode.SYMMETRIC else 'A', signedness='ANY' if self.signedness_to_force is None else 'S' if self.signedness_to_force else 'U', per_channel='Y' if self.per_channel else 'N')

    def __hash__(self):
        return hash(str(self))

    def is_valid_requantization_for(self, other: 'QuantizerConfig') ->bool:
        """
        Quantizer config A is a valid requantization for quantizer config B if A is more strict -
        specifically, it might be reasonable to put quantizer A after quantizer B in tensor data control flow, so that
        the requantization will further constrain the input tensor data w.r.t. values it can take, but
        putting quantizer A after quantizer B would be unreasonable.

        :param other: The "primary" QuantizerConfig, i.e. the one that defines an already present quantization.
        :return: True if the current config is a valid requantization for `other`, False otherwise.
        """
        fail_conditions = [self.num_bits > other.num_bits, self.mode is QuantizationMode.ASYMMETRIC and other.mode is QuantizationMode.SYMMETRIC, self.signedness_to_force is None and other.signedness_to_force is not None, self.signedness_to_force is True and other.signedness_to_force is False]
        if any(fail_conditions):
            return False
        return True

    def compatible_with_a_unified_scale_linked_qconfig(self, linked_qconfig: 'QuantizerConfig'):
        """
        For two configs to be compatible in a unified scale scenario, all of their fundamental parameters
        must be aligned.

        :param linked_qconfig: A QuantizerConfig that is compared against the current config.
        :return: A boolean value specifying whether `linked_qconfig` is compatible with the current config in terms
            of scale unification.
        """
        return self.num_bits == linked_qconfig.num_bits and self.mode == linked_qconfig.mode and self.signedness_to_force == linked_qconfig.signedness_to_force and self.per_channel == linked_qconfig.per_channel

    def is_a_bitwidth_variant(self, other_qconfig: 'QuantizerConfig') ->bool:
        """
        :param other_qconfig: A QuantizerConfig to be compared against the current config.
        :return: A boolean value specifying whether `other_config` is identical to the current config
            in everything except the bitwidth.
        """
        return self.per_channel == other_qconfig.per_channel and self.signedness_to_force == other_qconfig.signedness_to_force and self.mode == other_qconfig.mode

    def get_state(self) ->Dict[str, Any]:
        """
        Returns a dictionary with Python data structures (dict, list, tuple, str, int, float, True, False, None) that
        represents state of the object.

        :return: state of the object
        """
        return {'num_bits': self.num_bits, 'mode': self.mode, 'signedness_to_force': self.signedness_to_force, 'per_channel': self.per_channel}

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'QuantizerConfig':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        return cls(**state)


class QuantizerSpec:
    """
    A specific (potentially framework-aware) parameter struct required to initialize a
    given object that performs quantization of an input tensor.
    """

    def __init__(self, num_bits: int, mode: QuantizationMode, signedness_to_force: bool, narrow_range: bool, half_range: bool):
        """
        :param num_bits: Bitwidth of the quantization.
        :param mode: The mode of quantization (symmetric or asymmetric).
        :param signedness_to_force: True if the quantizer *must* be signed, False if *must* be unsigned,
            None if the signed/unsigned attribute should be determined based on the incoming activation
            statistics during range initialization.
        :param narrow_range: True if the range of quantized values should be narrowed as compared to the
            naive case, False if all 2^`num_bits` quantizations should be used.
        :param half_range: If ``True`` effectively only a half of an quantizer range are used.
            False - the full range are used.
        """
        self.num_bits = num_bits
        self.mode = mode
        self.signedness_to_force = signedness_to_force
        self.narrow_range = narrow_range
        self.half_range = half_range

    def __eq__(self, other: 'QuantizerSpec'):
        return self.__dict__ == other.__dict__

    @classmethod
    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool, half_range: bool) ->'QuantizerSpec':
        return cls(qconfig.num_bits, qconfig.mode, qconfig.signedness_to_force, narrow_range, half_range)


class PTQuantizerSpec(QuantizerSpec):
    _state_names = PTQSpecStateNames

    def __init__(self, num_bits: int, mode: QuantizationMode, signedness_to_force: Optional[bool], narrow_range: bool, half_range: bool, scale_shape: Tuple[int, ...], logarithm_scale: bool, is_quantized_on_export: bool=False, compression_lr_multiplier: float=None):
        """
        :param scale_shape: Shape of quantizer scale parameters
        :param logarithm_scale: Whether to use log of scale as optimized parameter instead of scale itself.
        :param compression_lr_multiplier: Used to increase/decrease gradients for quantization parameters.
        :param is_quantized_on_export: Export to onnx weights quantized or non quantized. Should not be True for
            activation quantizers.
        """
        super().__init__(num_bits, mode, signedness_to_force, narrow_range, half_range)
        self.per_channel = scale_shape != [1]
        self.scale_shape = scale_shape
        self.logarithm_scale = logarithm_scale
        self.compression_lr_multiplier = compression_lr_multiplier
        self.is_quantized_on_export = is_quantized_on_export

    @classmethod
    def from_config(cls, qconfig: QuantizerConfig, narrow_range: bool, half_range: bool, scale_shape: Tuple[int], logarithm_scale: bool, is_quantized_on_export: bool, compression_lr_multiplier: float) ->'PTQuantizerSpec':
        return cls(qconfig.num_bits, qconfig.mode, qconfig.signedness_to_force, narrow_range, half_range, scale_shape, logarithm_scale, is_quantized_on_export, compression_lr_multiplier)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    @classmethod
    def from_state(cls, state: Dict[str, Any]) ->'PTQuantizationPoint':
        """
        Creates the object from its state.

        :param state: Output of `get_state()` method.
        """
        kwargs = {cls._state_names.NUM_BITS: state['num_bits'], cls._state_names.MODE: state['mode'], cls._state_names.SIGNED_TO_FORCE: state['signedness_to_force'], cls._state_names.NARROW_RANGE: state['narrow_range'], cls._state_names.HALF_RANGE: state['half_range'], cls._state_names.SCALE_SHAPE: state['scale_shape'], cls._state_names.LOGARITHM_SCALE: state['logarithm_scale'], cls._state_names.IS_QUANTIZED_ON_EXPORT: state['is_quantized_on_export'], cls._state_names.COMPRESSION_LR_MULTIPLIER: state['compression_lr_multiplier']}
        return cls(**kwargs)

    def get_state(self):
        return {self._state_names.NUM_BITS: self.num_bits, self._state_names.MODE: self.mode, self._state_names.SIGNED_TO_FORCE: self.signedness_to_force, self._state_names.NARROW_RANGE: self.narrow_range, self._state_names.HALF_RANGE: self.half_range, self._state_names.SCALE_SHAPE: self.scale_shape, self._state_names.LOGARITHM_SCALE: self.logarithm_scale, self._state_names.IS_QUANTIZED_ON_EXPORT: self.is_quantized_on_export, self._state_names.COMPRESSION_LR_MULTIPLIER: self.compression_lr_multiplier}


class QuantizerExportMode(Enum):
    FAKE_QUANTIZE = 'fake_quantize'
    ONNX_QUANTIZE_DEQUANTIZE_PAIRS = 'quantize_dequantize'


def get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high):
    levels = level_high - level_low + 1
    assert levels in [255, 256], 'Can only export to INT8 256-level ONNX Quantize/Dequantize pairs'
    y_scale = (input_high - input_low) / (level_high - level_low)
    y_zero_point = (level_low * input_high - level_high * input_low) / (input_high - input_low)
    type_ = torch.int8 if level_low < 0 else torch.uint8
    level_low *= torch.ones_like(y_zero_point)
    level_high *= torch.ones_like(y_zero_point)
    level_low = level_low
    level_high = level_high
    y_zero_point = torch.min(torch.max(level_low, y_zero_point), level_high)
    y_scale = torch.squeeze(y_scale)
    y_zero_point = torch.squeeze(y_zero_point)
    return y_scale, y_zero_point


class BaseQuantizer(nn.Module):

    def __init__(self, qspec: PTQuantizerSpec):
        super().__init__()
        self._narrow_range = qspec.narrow_range
        self._signedness_to_force = qspec.signedness_to_force
        self._is_using_log_scale_storage = qspec.logarithm_scale
        self._half_range = qspec.half_range
        self._is_quantized_on_export = qspec.is_quantized_on_export
        self._num_bits = CompressionParameter(torch.IntTensor([qspec.num_bits]), requires_grad=False, compression_lr_multiplier=qspec.compression_lr_multiplier)
        OPTIONAL_PARAMETERS_REGISTRY.register('_num_bits')
        self.level_high = None
        self.level_low = None
        self.levels = 0
        ENABLED_VAR_NAME = 'enabled'
        self.register_buffer(ENABLED_VAR_NAME, torch.IntTensor([1]))
        OPTIONAL_PARAMETERS_REGISTRY.register(ENABLED_VAR_NAME)
        self.initialized = False
        self.call_count = 0
        self._scale_shape = qspec.scale_shape
        self._export_mode = QuantizerExportMode.FAKE_QUANTIZE


        class LoadStateListener:
            """
               Check whether a quantization module are going to be updated by new values from state_dict or checkpoint.
            """

            def __init__(self, module):
                self.hook = module._register_load_state_dict_pre_hook(partial(self.hook_fn, module=module))

            def hook_fn(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs, module):
                for module_key in module.state_dict().keys():
                    candidate = prefix + module_key
                    if candidate in state_dict:
                        module.initialized = True

            def close(self):
                self.hook.remove()
        self.load_listener = LoadStateListener(self)

    def enable_gradients(self):
        raise NotImplementedError

    def disable_gradients(self):
        raise NotImplementedError

    def is_enabled_quantization(self):
        with no_jit_trace():
            return self.enabled[0].item() == 1

    def enable_quantization(self):
        self.enabled[0] = 1
        self.enable_gradients()

    def disable_quantization(self):
        self.enabled[0] = 0
        self.disable_gradients()

    def forward(self, x):
        if is_debug():
            self.call_count += 1
        if not self.is_enabled_quantization():
            return x
        self.set_level_ranges()
        is_exporting = is_tracing_state()
        if is_exporting:
            with no_nncf_trace():
                x = self.run_export_quantization(x)
            return self.quantize(x, execute_traced_op_as_identity=True)
        return self.quantize(x, execute_traced_op_as_identity=False)

    def quantize(self, x, execute_traced_op_as_identity: bool=False):
        raise NotImplementedError

    def reset_call_counter(self):
        self.call_count = 0

    def get_trainable_params(self) ->Dict[str, torch.Tensor]:
        raise NotImplementedError

    def apply_minmax_init(self, min_values: torch.Tensor, max_values: torch.Tensor, log_module_name: str=None):
        """min_values and max_values must have the same shape as specified in self.scale_shape"""
        if self.initialized:
            nncf_logger.debug(f'Skipped initializing {log_module_name} - loaded from checkpoint')
            return
        if torch.all(torch.isinf(min_values)) or torch.all(torch.isinf(max_values)):
            raise ValueError(f'Statistics are not collected for {log_module_name}')
        if torch.any(torch.eq(min_values, np.inf)) or torch.any(torch.eq(max_values, -np.inf)):
            raise ValueError(f'Some of the values in statistics have infinite value for {log_module_name}')
        own_device = get_model_device(self)
        min_values = min_values
        max_values = max_values
        self._apply_minmax_init(min_values, max_values, log_module_name)

    def _apply_minmax_init(self, min_values: torch.Tensor, max_values: torch.Tensor, log_module_name: str=None):
        raise NotImplementedError

    def set_level_ranges(self):
        raise NotImplementedError

    @property
    def is_using_log_scale_storage(self):
        return self._is_using_log_scale_storage

    @property
    def signed(self):
        raise NotImplementedError

    @property
    def num_bits(self):
        with no_jit_trace():
            return self._num_bits.item()

    @num_bits.setter
    def num_bits(self, num_bits: int):
        self._num_bits.fill_(num_bits)

    @property
    def narrow_range(self) ->bool:
        return self._narrow_range

    @property
    def scale_shape(self) ->Tuple[int, ...]:
        return self._scale_shape

    def broadcast_initialized_params(self, src: int=0):
        distributed.broadcast(self._num_bits, src=src)

    def set_export_mode(self, mode: QuantizerExportMode):
        self._export_mode = mode

    def _get_input_low_input_high(self):
        raise NotImplementedError

    def _prepare_export_quantization(self, x: torch.Tensor):
        raise NotImplementedError

    def _prepare_fq_export_quantization(self, x: torch.Tensor):
        x, level_high, level_low, input_low, input_high = self._prepare_export_quantization(x)
        with no_jit_trace():
            levels = level_high - level_low + 1
        return x, levels, input_low, input_high

    def _prepare_qdq_export_quantization(self, x: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:
        x, level_high, level_low, input_low, input_high = self._prepare_export_quantization(x)
        with no_jit_trace():
            y_scale, y_zero_point = get_scale_zp_from_input_low_input_high(level_low, level_high, input_low, input_high)
            possible_axes = self._possible_per_channel_dimensions()
            if len(possible_axes) > 1:
                raise RuntimeError(f'Impossible to determine the per-channel axis for a scale shape {self.scale_shape} - more than one dimension is >1')
            if not possible_axes:
                axis = 1
                y_scale = y_scale.flatten()[0]
                y_zero_point = y_zero_point.flatten()[0]
            else:
                axis = possible_axes[0]
        return x, y_scale, y_zero_point, axis

    def _possible_per_channel_dimensions(self) ->List[int]:
        return [i for i in range(len(self.scale_shape)) if self.scale_shape[i] > 1]

    def run_export_quantization(self, x: torch.Tensor):
        with torch.no_grad():
            if self._export_mode == QuantizerExportMode.FAKE_QUANTIZE:
                x, levels, input_low, input_high = self._prepare_fq_export_quantization(x)
                return ExportQuantizeToFakeQuantize.apply(x, levels, input_low, input_high, input_low, input_high)
            if self._export_mode == QuantizerExportMode.ONNX_QUANTIZE_DEQUANTIZE_PAIRS:
                x, y_scale, y_zero_point, axis = self._prepare_qdq_export_quantization(x)
                return ExportQuantizeToONNXQuantDequant.apply(x, y_scale, y_zero_point, axis)
        raise RuntimeError('Unknown export mode')

    def extra_repr(self):
        return 'bit={}, ch={}'.format(self.num_bits, self.per_channel)

    def get_quantizer_config(self) ->QuantizerConfig:
        raise NotImplementedError

    @property
    def per_channel(self) ->bool:
        numel = 1
        for el in self.scale_shape:
            numel *= el
        is_per_tensor = numel == 1 and len(self.scale_shape) == 1
        return not is_per_tensor


class StorageRedirectingLoadStateDictHook:

    def __init__(self, storage_attribute_in_module: str, name_in_state_dict: str, use_log_storage_in_module: bool=False):
        self._storage_attribute_in_module = storage_attribute_in_module
        self._name_in_state_dict = name_in_state_dict
        self._use_log_storage_in_module = use_log_storage_in_module

    def __call__(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) ->None:
        state_dict_key = prefix + self._name_in_state_dict
        if state_dict_key in state_dict:
            v = state_dict.pop(state_dict_key)
            if self._use_log_storage_in_module:
                v = v.abs().log().detach()
            state_dict[prefix + self._storage_attribute_in_module] = v
        else:
            missing_keys.append(state_dict_key)


class StorageRedirectingStateDictHook:

    def __init__(self, storage_attribute_in_module: str, name_in_state_dict: str, use_log_storage_in_module: bool=False):
        self._storage_attribute_in_module = storage_attribute_in_module
        self._name_in_state_dict = name_in_state_dict
        self._use_log_storage_in_module = use_log_storage_in_module

    def __call__(self, module, state_dict, prefix, local_metadata) ->None:
        v = state_dict.pop(prefix + self._storage_attribute_in_module)
        if self._use_log_storage_in_module:
            v = v.exp().detach()
        state_dict[prefix + self._name_in_state_dict] = v


def calculate_symmetric_level_ranges(num_bits: int, signed: bool, narrow_range: bool=False) ->Tuple[int, int, int]:
    """
    Calculates the numbers of the low and high quant and the number of
    quantization levels for the symmetric quantization scheme.

    :param num_bits: The bitwidth of the quantization.
    :param signed: The flag specifying type of the symmetric quantization scheme
        if it is True then the symmetric quantization scheme is the signed and
        the un-signed otherwise.
    :param narrow_range: The flag specifying quantization range if it is True
        then [1; 2^num_bits - 1] and [0; 2^num_bits - 1] otherwise.
    :return: A Tuple
        level_low - the low quant number
        level_high - the high quant number
        levels - the number of quantization levels
    """
    levels = 2 ** num_bits
    if signed:
        level_high = levels // 2 - 1
        level_low = -(levels // 2)
    else:
        level_high = levels - 1
        level_low = 0
    if narrow_range:
        level_low = level_low + 1
        levels = levels - 1
    return level_low, level_high, levels


def get_flat_tensor_contents_string(input_tensor):
    retval = '['
    for idx, el in enumerate(input_tensor.view(-1)):
        if idx >= 10:
            retval += '... (first 10/{} elements shown only) '.format(len(input_tensor.view(-1)))
            break
        retval += '{:.4f}, '.format(el.item())
    retval += ']'
    return retval


class QuantizeSymmetric(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_, scale, level_low, level_high, levels):
        input_low = scale * (level_low / level_high)
        input_range = scale - input_low
        if input_.is_cuda:
            if not input_.is_contiguous():
                nncf_logger.debug('input_ is not contiguous!')
                input_ = input_.contiguous()
            if input_.dtype == torch.float16:
                input_low = input_low.type(torch.float16)
                input_range = input_range.type(torch.float16)
            output = QuantizedFunctionsCUDA.Quantize_forward(input_, input_low, input_range, levels)
        else:
            output = QuantizedFunctionsCPU.Quantize_forward(input_, input_low, input_range, levels)
        ctx.save_for_backward(input_, input_low, input_range)
        ctx.levels = levels
        ctx.level_low = level_low
        ctx.level_high = level_high
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        grad_output = grad_outputs[0]
        input_, input_low, input_range = ctx.saved_tensors
        levels = ctx.levels
        level_low = ctx.level_low
        level_high = ctx.level_high
        if grad_output.is_cuda:
            if not grad_output.is_contiguous():
                nncf_logger.debug('grad_output is not contiguous!')
                grad_output = grad_output.contiguous()
            grad_input, _, grad_scale = QuantizedFunctionsCUDA.Quantize_backward(grad_output, input_, input_low, input_range, levels, level_low, level_high)
        else:
            grad_input, _, grad_scale = QuantizedFunctionsCPU.Quantize_backward(grad_output, input_, input_low, input_range, levels, level_low, level_high, False)
        return grad_input, grad_scale, None, None, None


@register_operator()
def symmetric_quantize(input_, levels, level_low, level_high, scale, eps, skip: bool=False):
    if skip:
        return input_
    scale = scale
    scale_safe = abs(scale) + eps
    return QuantizeSymmetric.apply(input_, scale_safe, level_low, level_high, levels)


class SymmetricQuantizer(BaseQuantizer):
    SCALE_PARAM_NAME = 'scale'
    _SCALE_PARAM_STORAGE_ATTR = '_scale_param_storage'

    def __init__(self, qspec: PTQuantizerSpec):
        super().__init__(qspec)
        self.signed_tensor = CompressionParameter(torch.IntTensor([0]), requires_grad=False, compression_lr_multiplier=qspec.compression_lr_multiplier)
        self.collect_scale_statistics = False
        setattr(self, self._SCALE_PARAM_STORAGE_ATTR, CompressionParameter(torch.ones(self.scale_shape), requires_grad=True, compression_lr_multiplier=qspec.compression_lr_multiplier))
        if self._is_using_log_scale_storage:
            self._scale_param_storage.data.log_()
            self.eps = 0
        else:
            self.eps = 1e-16
        if qspec.signedness_to_force is not None:
            self.signed = int(qspec.signedness_to_force)
        self.set_level_ranges()
        self._register_load_state_dict_pre_hook(StorageRedirectingLoadStateDictHook(storage_attribute_in_module=self._SCALE_PARAM_STORAGE_ATTR, name_in_state_dict=self.SCALE_PARAM_NAME, use_log_storage_in_module=self._is_using_log_scale_storage))
        self._register_state_dict_hook(StorageRedirectingStateDictHook(storage_attribute_in_module=self._SCALE_PARAM_STORAGE_ATTR, name_in_state_dict=self.SCALE_PARAM_NAME, use_log_storage_in_module=self._is_using_log_scale_storage))

    @property
    def scale(self):
        return self._scale_param_storage.exp() if self._is_using_log_scale_storage else self._scale_param_storage

    @scale.setter
    def scale(self, v):
        self._scale_param_storage = v
        if self._is_using_log_scale_storage:
            self._scale_param_storage.data.log_()

    def __setattr__(self, key, value):
        """
        Need to handle the redirect-storage attributes (which are implemented using Python properties
        here) specially - otherwise the torch.nn.Module's __setattr__ will try to set them during
        assignment.
        """
        if key == self.SCALE_PARAM_NAME:
            object.__setattr__(self, key, value)
        else:
            super().__setattr__(key, value)

    def enable_gradients(self):
        self._scale_param_storage.requires_grad = True

    def disable_gradients(self):
        self._scale_param_storage.requires_grad = False

    def set_level_ranges(self):
        scaled_num_bits = 1 if self._half_range else 0
        self.level_low, self.level_high, self.levels = self.calculate_level_ranges(self.num_bits - scaled_num_bits, self.signed)

    @staticmethod
    def calculate_level_ranges(num_bits, signed):
        return calculate_symmetric_level_ranges(num_bits, signed)

    @property
    def signed(self):
        with no_jit_trace():
            return self.signed_tensor.item() == 1

    @signed.setter
    def signed(self, signed: bool):
        self.signed_tensor.fill_(signed)

    def quantize(self, x, execute_traced_op_as_identity: bool=False):
        return symmetric_quantize(x, self.levels, self.level_low, self.level_high, self.scale, self.eps, skip=execute_traced_op_as_identity)

    def get_trainable_params(self) ->Dict[str, torch.Tensor]:
        return {self.SCALE_PARAM_NAME: self.scale.detach()}

    def _apply_minmax_init(self, min_values, max_values, log_module_name: str=None):
        sign = torch.any(torch.lt(min_values, 0))
        if self._signedness_to_force is not None and sign != self._signedness_to_force:
            nncf_logger.debug(f'Forcing signed to {self._signedness_to_force} for module {log_module_name}')
            sign = self._signedness_to_force
        self.signed = int(sign)
        abs_max = torch.max(torch.abs(max_values), torch.abs(min_values))
        SCALE_LOWER_THRESHOLD = 0.1
        mask = torch.gt(abs_max, SCALE_LOWER_THRESHOLD)
        self._scale_param_storage.data = torch.where(mask, abs_max, SCALE_LOWER_THRESHOLD * torch.ones_like(self._scale_param_storage))
        if self._is_using_log_scale_storage:
            self._scale_param_storage.data.log_()
        nncf_logger.debug(f'Set sign: {self.signed} and scale: {get_flat_tensor_contents_string(self.scale)} for {log_module_name}')

    def broadcast_initialized_params(self, src: int=0):
        super().broadcast_initialized_params(src)
        distributed.broadcast(self._scale_param_storage, src=src)
        distributed.broadcast(self.signed_tensor, src=src)

    def _get_input_low_input_high(self, scale, level_low, level_high, eps):
        input_range = abs(scale) + eps
        input_low = input_range * level_low / level_high
        input_high = input_range
        return input_low, input_high

    def _prepare_export_quantization(self, x: torch.Tensor):
        with no_jit_trace():
            input_low, input_high = self._get_input_low_input_high(self.scale, self.level_low, self.level_high, self.eps)
            level_low = self.level_low
            level_high = self.level_high
            if self._half_range:
                x = torch.min(torch.max(x, input_low), input_high)
                level_low = 2 * self.level_low
                level_high = 2 * self.level_high + 1
                input_low, input_high = self._get_input_low_input_high(level_high / self.level_high * self.scale, level_low, level_high, self.eps)
            if self._is_quantized_on_export:
                x = self.quantize(x, execute_traced_op_as_identity=False)
        return x, level_high, level_low, input_low, input_high

    def get_quantizer_config(self) ->QuantizerConfig:
        return QuantizerConfig(num_bits=self.num_bits, mode=QuantizationMode.SYMMETRIC, signedness_to_force=self.signed, per_channel=self.per_channel)


class TuneRange(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_low, input_range, levels):
        input_high = input_range + input_low
        input_low_copy = input_low.clone()
        input_low_copy[input_low_copy > 0] = 0
        input_high[input_high < 0] = 0
        n = levels - 1
        scale = levels / (input_high - input_low_copy)
        zp = torch.round(-input_low_copy * scale)
        new_input_low = torch.where(zp < n, zp / (zp - n) * input_high, input_low_copy)
        new_input_high = torch.where(zp > 0.0, (zp - n) / zp * input_low_copy, input_high)
        range_1 = input_high - new_input_low
        range_2 = new_input_high - input_low_copy
        mask = range_1 > range_2
        inv_mask = (1 - mask).abs()
        new_input_low = mask * new_input_low + inv_mask * input_low_copy
        new_input_range = inv_mask * new_input_high + mask * input_high - new_input_low
        return new_input_low, new_input_range

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        grad_input_low = grad_outputs[0]
        grad_input_range = grad_outputs[1]
        return grad_input_low, grad_input_range, None


class QuantizeAsymmetric(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_, input_low, input_range, level_low, level_high, levels):
        if input_.is_cuda:
            if not input_.is_contiguous():
                nncf_logger.debug('input_ is not contiguous!')
                input_ = input_.contiguous()
            if input_.dtype == torch.float16:
                input_low = input_low.type(torch.float16)
                input_range = input_range.type(torch.float16)
            output = QuantizedFunctionsCUDA.Quantize_forward(input_, input_low, input_range, levels)
        else:
            output = QuantizedFunctionsCPU.Quantize_forward(input_, input_low, input_range, levels)
        ctx.save_for_backward(input_, input_low, input_range)
        ctx.levels = levels
        ctx.level_low = level_low
        ctx.level_high = level_high
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        grad_output = grad_outputs[0]
        input_, input_low, input_range = ctx.saved_tensors
        levels = ctx.levels
        level_low = ctx.level_low
        level_high = ctx.level_high
        if grad_output.is_cuda:
            if not grad_output.is_contiguous():
                nncf_logger.debug('grad_output is not contiguous!')
                grad_output = grad_output.contiguous()
            grad_input, grad_input_low, grad_input_range = QuantizedFunctionsCUDA.Quantize_backward(grad_output, input_, input_low, input_range, levels, level_low, level_high)
        else:
            grad_input, grad_input_low, grad_input_range = QuantizedFunctionsCPU.Quantize_backward(grad_output, input_, input_low, input_range, levels, level_low, level_high, True)
        return grad_input, grad_input_low, grad_input_range, None, None, None


@register_operator()
def asymmetric_quantize(input_, levels, level_low, level_high, input_low, input_range, eps, skip: bool=False):
    if skip:
        return input_
    input_range_safe = abs(input_range) + eps
    input_low_tuned, input_range_tuned = TuneRange.apply(input_low, input_range_safe, levels)
    return QuantizeAsymmetric.apply(input_, input_low_tuned, input_range_tuned, level_low, level_high, levels)


def calculate_asymmetric_level_ranges(num_bits: int, narrow_range: bool=False) ->Tuple[int, int, int]:
    """
    Calculates the numbers of the low and high quant and the number of
    quantization levels for the asymmetric quantization scheme.

    :param num_bits: The bitwidth of the quantization
    :param narrow_range: The flag specifying quantization range if it is True
        then [1; 2^num_bits - 1] and [0; 2^num_bits - 1] otherwise
    :return: A Tuple
        level_low - the low quant number
        level_high - the high quant number
        levels - the number of quantization levels
    """
    levels = 2 ** num_bits
    level_high = levels - 1
    level_low = 0
    if narrow_range:
        level_low = level_low + 1
        levels = levels - 1
    return level_low, level_high, levels


def clamp(x, low, high):
    return torch.max(torch.min(x, high), low)


class AsymmetricQuantizer(BaseQuantizer):
    INPUT_LOW_PARAM_NAME = 'input_low'
    INPUT_RANGE_PARAM_NAME = 'input_range'
    _INPUT_RANGE_PARAM_STORAGE_ATTR = '_input_range_param_storage'

    def __init__(self, qspec: PTQuantizerSpec):
        super().__init__(qspec)
        self.input_low = CompressionParameter(torch.zeros(self.scale_shape), requires_grad=True, compression_lr_multiplier=qspec.compression_lr_multiplier)
        setattr(self, self._INPUT_RANGE_PARAM_STORAGE_ATTR, CompressionParameter(torch.ones(self.scale_shape), requires_grad=True, compression_lr_multiplier=qspec.compression_lr_multiplier))
        if self._is_using_log_scale_storage:
            self._input_range_param_storage.data.log_()
            self.eps = 0
        else:
            self.eps = 1e-16
        self.set_level_ranges()
        self._register_load_state_dict_pre_hook(StorageRedirectingLoadStateDictHook(storage_attribute_in_module=self._INPUT_RANGE_PARAM_STORAGE_ATTR, name_in_state_dict=self.INPUT_RANGE_PARAM_NAME, use_log_storage_in_module=self._is_using_log_scale_storage))
        self._register_state_dict_hook(StorageRedirectingStateDictHook(storage_attribute_in_module=self._INPUT_RANGE_PARAM_STORAGE_ATTR, name_in_state_dict=self.INPUT_RANGE_PARAM_NAME, use_log_storage_in_module=self._is_using_log_scale_storage))

    @property
    def input_range(self):
        if self._is_using_log_scale_storage:
            return self._input_range_param_storage.exp()
        return self._input_range_param_storage

    @input_range.setter
    def input_range(self, v: torch.Tensor):
        self._input_range_param_storage = v
        if self._is_using_log_scale_storage:
            self._input_range_param_storage.data.log_()

    def __setattr__(self, key, value):
        """
        Need to handle the redirect-storage attributes (which are implemented using Python properties
        here) specially - otherwise the torch.nn.Module's __setattr__ will try to set them during
        assignment.
        """
        if key == self.INPUT_RANGE_PARAM_NAME:
            object.__setattr__(self, key, value)
        else:
            super().__setattr__(key, value)

    def enable_gradients(self):
        self.input_low.requires_grad = True
        self._input_range_param_storage.requires_grad = True

    def disable_gradients(self):
        self.input_low.requires_grad = False
        self._input_range_param_storage.requires_grad = False

    @property
    def signed(self):
        return True

    def set_level_ranges(self):
        scaled_num_bits = 1 if self._half_range else 0
        self.level_low, self.level_high, self.levels = self.calculate_level_ranges(self.num_bits - scaled_num_bits)

    @staticmethod
    def calculate_level_ranges(num_bits):
        return calculate_asymmetric_level_ranges(num_bits)

    def quantize(self, x, execute_traced_op_as_identity: bool=False):
        return asymmetric_quantize(x, self.levels, self.level_low, self.level_high, self.input_low, self.input_range, self.eps, skip=execute_traced_op_as_identity)

    def get_trainable_params(self) ->Dict[str, torch.Tensor]:
        return {self.INPUT_LOW_PARAM_NAME: self.input_low.detach(), self.INPUT_RANGE_PARAM_NAME: self.input_range.detach()}

    def _apply_minmax_init(self, min_values, max_values, log_module_name: str=None):
        ranges = max_values - min_values
        max_range = torch.max(max_values - min_values)
        eps = 0.01
        correction = (clamp(ranges, low=eps * max_range, high=max_range) - ranges) * 0.5
        self._input_range_param_storage.data = (ranges + 2 * correction).data
        if self._is_using_log_scale_storage:
            self._input_range_param_storage.data.log_()
        self.input_low.data = (min_values - correction).data
        nncf_logger.debug(f'Set input_low: {get_flat_tensor_contents_string(self.input_low)} and input_range: {get_flat_tensor_contents_string(self.input_range)} for {log_module_name}')

    def broadcast_initialized_params(self, src: int=0):
        super().broadcast_initialized_params(src)
        distributed.broadcast(self.input_low, src)
        distributed.broadcast(self._input_range_param_storage, src)

    def _get_input_low_input_high(self, input_range, input_low, levels, eps):
        input_range_safe = abs(input_range) + eps
        input_low, input_range_tuned = TuneRange.apply(input_low, input_range_safe, levels)
        input_high = input_low + input_range_tuned
        return input_low, input_high

    def _prepare_export_quantization(self, x: torch.Tensor):
        with no_jit_trace():
            input_low, input_high = self._get_input_low_input_high(self.input_range, self.input_low, self.levels, self.eps)
            level_low = self.level_low
            level_high = self.level_high
            if self._half_range:
                x = torch.min(torch.max(x, input_low), input_high)
                level_low = 2 * level_low
                level_high = 2 * level_high + 1
                input_low, input_high = self._get_input_low_input_high(level_high / self.level_high * self.input_range, self.input_low, self.levels, self.eps)
            if self._is_quantized_on_export:
                x = self.quantize(x, execute_traced_op_as_identity=False)
        return x, level_high, level_low, input_low, input_high

    def get_quantizer_config(self) ->QuantizerConfig:
        return QuantizerConfig(num_bits=self.num_bits, mode=QuantizationMode.ASYMMETRIC, signedness_to_force=self.signed, per_channel=self.per_channel)


def logit(x):
    return torch.log(x / (1 - x))


@register_operator()
def calc_rb_binary_mask(mask, uniform_buffer, eps):
    if uniform_buffer is not None:
        uniform_buffer.uniform_()
        mask = mask + logit(uniform_buffer.clamp(eps, 1 - eps))
    return binary_mask(mask)


class RBSparsifyingWeight(BinaryMask):

    def __init__(self, weight_shape: List[int], frozen=True, compression_lr_multiplier=None, eps=1e-06):
        super().__init__(weight_shape)
        self.frozen = frozen
        self.eps = eps
        self._mask = CompressionParameter(logit(torch.ones(weight_shape) * 0.99), requires_grad=not self.frozen, compression_lr_multiplier=compression_lr_multiplier)
        self.binary_mask = binary_mask(self._mask)
        self.register_buffer('uniform', torch.zeros(weight_shape))
        self.mask_calculation_hook = MaskCalculationHook(self)

    @property
    def mask(self):
        return self._mask

    @mask.setter
    def mask(self, tensor):
        self._mask.data = tensor
        self.binary_mask = binary_mask(self._mask)

    def _calc_training_binary_mask(self, weight):
        u = self.uniform if self.training and not self.frozen else None
        return calc_rb_binary_mask(self._mask, u, self.eps)

    def loss(self):
        return binary_mask(self._mask)


class SparseLoss(PTCompressionLoss):

    def __init__(self, sparse_layers=None, target=1.0, p=0.05):
        super().__init__()
        self._sparse_layers = sparse_layers
        self.target = target
        self.p = p
        self.disabled = False
        self.current_sparsity = 0
        self.mean_sparse_prob = 0

    def set_layers(self, sparse_layers):
        self._sparse_layers = sparse_layers

    def disable(self):
        if not self.disabled:
            self.disabled = True
            for sparse_layer in self._sparse_layers:
                sparse_layer.frozen = True

    def calculate(self) ->torch.Tensor:
        if self.disabled:
            return 0
        params = 0
        loss = 0
        sparse_prob_sum = 0
        for sparse_layer in self._sparse_layers:
            if not self.disabled and sparse_layer.frozen:
                raise AssertionError('Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss')
            if not sparse_layer.frozen:
                sw_loss = sparse_layer.loss()
                params = params + sw_loss.view(-1).size(0)
                loss = loss + sw_loss.sum()
                sparse_prob_sum += torch.sigmoid(sparse_layer.mask).sum()
        self.mean_sparse_prob = (sparse_prob_sum / params).item()
        self.current_sparsity = 1 - loss / params
        return ((loss / params - self.target) / self.p).pow(2)

    @property
    def target_sparsity_rate(self):
        rate = 1 - self.target
        if rate < 0 or rate > 1:
            raise IndexError('Target is not within range(0,1)')
        return rate

    def set_target_sparsity_loss(self, sparsity_level):
        self.target = 1 - sparsity_level


class SparseLossForPerLayerSparsity(SparseLoss):

    def __init__(self, sparse_layers=None, target=1.0, p=0.05):
        super().__init__(sparse_layers, target, p)
        self.per_layer_target = {}
        for sparse_layer in self._sparse_layers:
            self.per_layer_target[sparse_layer] = self.target

    def calculate(self) ->torch.Tensor:
        if self.disabled:
            return 0
        params = 0
        sparse_prob_sum = 0
        sparse_layers_loss = 0
        for sparse_layer in self._sparse_layers:
            if not self.disabled and not sparse_layer.sparsify:
                raise AssertionError('Invalid state of SparseLoss and SparsifiedWeight: mask is frozen for enabled loss')
            if sparse_layer.sparsify:
                sw_loss = sparse_layer.loss()
                params_layer = sw_loss.view(-1).size(0)
                params += params_layer
                sparse_layers_loss -= torch.abs(sw_loss.sum() / params_layer - self.per_layer_target[sparse_layer])
                sparse_prob_sum += torch.sigmoid(sparse_layer.mask).sum()
        self.mean_sparse_prob = (sparse_prob_sum / params).item()
        return (sparse_layers_loss / self.p).pow(2)

    def set_target_sparsity_loss(self, target, sparse_layer):
        self.per_layer_target[sparse_layer] = 1 - target


class SelfAttention(nn.Module):

    def __init__(self):
        super().__init__()
        self.q = nn.Linear(768, 768)
        self.k = nn.Linear(768, 768)
        self.v = nn.Linear(768, 768)
        self.o = nn.Linear(768, 768)
        self.sm = nn.Softmax()

    def forward(self, x):
        k = self.k(x)
        q = self.q(x)
        v = self.v(x)
        k = k.view(-1, 12, 64).permute(1, 0, 2)
        q = q.view(-1, 12, 64).permute(1, 2, 0)
        v = v.view(-1, 12, 64).permute(1, 0, 2)
        x = self.sm(torch.matmul(k, q)) / np.sqrt(1 / 384)
        x = torch.matmul(x, v)
        x = x.permute(1, 0, 2).contiguous().view(-1, 768)
        return self.o(x)


def fill_bias(module, value):
    module.bias.data.fill_(value)


def fill_conv_weight(conv, value, dim=2):
    conv.weight.data.fill_(value)
    if dim in [2, 3]:
        with torch.no_grad():
            mask = torch.eye(conv.kernel_size[0])
            conv.weight += mask


def create_conv(in_channels, out_channels, kernel_size, weight_init=1, bias_init=0, padding=0, stride=1, dim=2, bias=True):
    conv_dim_map = {(1): nn.Conv1d, (2): nn.Conv2d, (3): nn.Conv3d}
    conv = conv_dim_map[dim](in_channels, out_channels, kernel_size, padding=padding, stride=stride, bias=bias)
    fill_conv_weight(conv, weight_init, dim)
    if bias:
        fill_bias(conv, bias_init)
    return conv


class BasicConvTestModel(nn.Module):
    INPUT_SIZE = [1, 1, 4, 4]

    def __init__(self, in_channels=1, out_channels=2, kernel_size=2, weight_init=-1, bias_init=-2):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.weight_init = weight_init
        self.bias_init = bias_init
        self.conv = create_conv(in_channels, out_channels, kernel_size, weight_init, bias_init)
        self.wq_scale_shape_per_channel = out_channels, 1, 1, 1
        self.aq_scale_shape_per_channel = 1, in_channels, 1, 1

    @staticmethod
    def default_weight():
        return torch.tensor([[[[0.0, -1.0], [-1.0, 0.0]]], [[[0.0, -1.0], [-1.0, 0.0]]]])

    @staticmethod
    def default_bias():
        return torch.tensor([-2.0, -2])

    def forward(self, x):
        return self.conv(x)

    @property
    def weights_num(self):
        return self.out_channels * self.kernel_size ** 2

    @property
    def bias_num(self):
        return self.kernel_size

    @property
    def nz_weights_num(self):
        return self.kernel_size * self.out_channels

    @property
    def nz_bias_num(self):
        return self.kernel_size


class TwoConvTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.features = []
        self.features.append(nn.Sequential(create_conv(1, 2, 2, -1, -2)))
        self.features.append(nn.Sequential(create_conv(2, 1, 3, 0, 0)))
        self.features = nn.Sequential(*self.features)

    def forward(self, x):
        return self.features(x)

    @property
    def weights_num(self):
        return 8 + 18

    @property
    def bias_num(self):
        return 2 + 1

    @property
    def nz_weights_num(self):
        return 4 + 6

    @property
    def nz_bias_num(self):
        return 2


class LeNet(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        out = F.relu(self.conv1(x))
        out = F.max_pool2d(out, 2)
        out = F.relu(self.conv2(out))
        out = F.max_pool2d(out, 2)
        out = out.view(out.size(0), -1)
        out = F.relu(self.fc1(out))
        out = F.relu(self.fc2(out))
        out = self.fc3(out)
        return out


class MockModel(torch.nn.Module):

    def __init__(self, stub_forward):
        super().__init__()
        self.param = torch.nn.Parameter(torch.ones([1]))
        self.stub_forward = stub_forward

    def forward(self, *args, **kwargs):
        return self.stub_forward(*args, **kwargs)


class BahdanauAttention(nn.Module):
    """
    Bahdanau Attention (https://arxiv.org/abs/1409.0473)
    Implementation is very similar to tf.contrib.seq2seq.BahdanauAttention
    """

    def __init__(self, query_size, key_size, num_units, normalize=False, batch_first=False, init_weight=0.1):
        """
        Constructor for the BahdanauAttention.

        :param query_size: feature dimension for query
        :param key_size: feature dimension for keys
        :param num_units: internal feature dimension
        :param normalize: whether to normalize energy term
        :param batch_first: if True batch size is the 1st dimension, if False
            the sequence is first and batch size is second
        :param init_weight: range for uniform initializer used to initialize
            Linear key and query transform layers and linear_att vector
        """
        super().__init__()
        self.normalize = normalize
        self.batch_first = batch_first
        self.num_units = num_units
        self.linear_q = nn.Linear(query_size, num_units, bias=False)
        self.linear_k = nn.Linear(key_size, num_units, bias=False)
        nn.init.uniform_(self.linear_q.weight.data, -init_weight, init_weight)
        nn.init.uniform_(self.linear_k.weight.data, -init_weight, init_weight)
        self.linear_att = Parameter(torch.Tensor(num_units))
        self.mask = None
        if self.normalize:
            self.normalize_scalar = Parameter(torch.Tensor(1))
            self.normalize_bias = Parameter(torch.Tensor(num_units))
        else:
            self.register_parameter('normalize_scalar', None)
            self.register_parameter('normalize_bias', None)
        self.reset_parameters(init_weight)

    def reset_parameters(self, init_weight):
        """
        Sets initial random values for trainable parameters.
        """
        stdv = 1.0 / math.sqrt(self.num_units)
        self.linear_att.data.uniform_(-init_weight, init_weight)
        if self.normalize:
            self.normalize_scalar.data.fill_(stdv)
            self.normalize_bias.data.zero_()

    def set_mask(self, context_len, context):
        """
        sets self.mask which is applied before softmax
        ones for inactive context fields, zeros for active context fields

        :param context_len: b
        :param context: if batch_first: (b x t_k x n) else: (t_k x b x n)

        self.mask: (b x t_k)
        """
        if self.batch_first:
            max_len = context.size(1)
        else:
            max_len = context.size(0)
        indices = torch.arange(0, max_len, dtype=torch.int64, device=context.device)
        with no_nncf_trace():
            self.mask = indices >= context_len.unsqueeze(1)

    def calc_score(self, att_query, att_keys):
        """
        Calculate Bahdanau score

        :param att_query: b x t_q x n
        :param att_keys: b x t_k x n

        returns: b x t_q x t_k scores
        """
        b, t_k, n = att_keys.size()
        t_q = att_query.size(1)
        att_query = att_query.unsqueeze(2).expand(b, t_q, t_k, n)
        att_keys = att_keys.unsqueeze(1).expand(b, t_q, t_k, n)
        sum_qk = att_query + att_keys
        linear_att = self.linear_att
        out = torch.tanh(sum_qk).matmul(linear_att)
        return out

    def forward(self, query, keys):
        """

        :param query: if batch_first: (b x t_q x n) else: (t_q x b x n)
        :param keys: if batch_first: (b x t_k x n) else (t_k x b x n)

        :returns: (context, scores_normalized)
        context: if batch_first: (b x t_q x n) else (t_q x b x n)
        scores_normalized: if batch_first (b x t_q x t_k) else (t_q x b x t_k)
        """
        if not self.batch_first:
            keys = keys.transpose(0, 1)
            if query.dim() == 3:
                query = query.transpose(0, 1)
        if query.dim() == 2:
            single_query = True
            query = query.unsqueeze(1)
        else:
            single_query = False
        b = query.size(0)
        t_k = keys.size(1)
        t_q = query.size(1)
        processed_query = self.linear_q(query)
        processed_key = self.linear_k(keys)
        scores = self.calc_score(processed_query, processed_key)
        if self.mask is not None:
            mask = self.mask.unsqueeze(1).expand(b, t_q, t_k)
            scores.masked_fill_(mask, -65504.0)
        scores_normalized = F.softmax(scores, dim=-1)
        context = torch.bmm(scores_normalized, keys)
        if single_query:
            context = context.squeeze(1)
            scores_normalized = scores_normalized.squeeze(1)
        elif not self.batch_first:
            context = context.transpose(0, 1)
            scores_normalized = scores_normalized.transpose(0, 1)
        return context, scores_normalized


class RecurrentAttention(nn.Module):
    """
    LSTM wrapped with an attention module.
    """

    def __init__(self, input_size=1024, context_size=1024, hidden_size=1024, num_layers=1, batch_first=False, dropout=0.2, init_weight=0.1):
        """
        Constructor for the RecurrentAttention.

        :param input_size: number of features in input tensor
        :param context_size: number of features in output from encoder
        :param hidden_size: internal hidden size
        :param num_layers: number of layers in LSTM
        :param batch_first: if True the model uses (batch,seq,feature) tensors,
            if false the model uses (seq, batch, feature)
        :param dropout: probability of dropout (on input to LSTM layer)
        :param init_weight: range for the uniform initializer
        """
        super().__init__()
        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, bias=True, batch_first=batch_first)
        self.attn = BahdanauAttention(hidden_size, context_size, context_size, normalize=True, batch_first=batch_first)
        self.dropout = nn.Dropout(dropout)

    def forward(self, inputs, hidden, context, context_len):
        """
        Execute RecurrentAttention.

        :param inputs: tensor with inputs
        :param hidden: hidden state for LSTM layer
        :param context: context tensor from encoder
        :param context_len: vector of encoder sequence lengths

        :returns (rnn_outputs, hidden, attn_output, attn_scores)
        """
        self.attn.set_mask(context_len, context)
        inputs = self.dropout(inputs)
        rnn_outputs, hidden = self.rnn(inputs, hidden)
        attn_outputs, scores = self.attn(rnn_outputs, context)
        return rnn_outputs, hidden, attn_outputs, scores


class Classifier(nn.Module):
    """
    Fully-connected classifier
    """

    def __init__(self, in_features, out_features, init_weight=0.1):
        """
        Constructor for the Classifier.

        :param in_features: number of input features
        :param out_features: number of output features (size of vocabulary)
        :param init_weight: range for the uniform initializer
        """
        super().__init__()
        self.classifier = nn.Linear(in_features, out_features)
        nn.init.uniform_(self.classifier.weight.data, -init_weight, init_weight)
        nn.init.uniform_(self.classifier.bias.data, -init_weight, init_weight)

    def forward(self, x):
        """
        Execute the classifier.

        :param x: output from decoder
        """
        out = self.classifier(x)
        return out


PAD = 0


class ResidualRecurrentDecoder(nn.Module):
    """
    Decoder with Embedding, LSTM layers, attention, residual connections and
    optinal dropout.

    Attention implemented in this module is different than the attention
    discussed in the GNMT arxiv paper. In this model the output from the first
    LSTM layer of the decoder goes into the attention module, then the
    re-weighted context is concatenated with inputs to all subsequent LSTM
    layers in the decoder at the current timestep.

    Residual connections are enabled after 3rd LSTM layer, dropout is applied
    on inputs to LSTM layers.
    """

    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2, batch_first=False, embedder=None, init_weight=0.1):
        """
        Constructor of the ResidualRecurrentDecoder.

        :param vocab_size: size of vocabulary
        :param hidden_size: hidden size for LSMT layers
        :param num_layers: number of LSTM layers
        :param dropout: probability of dropout (on input to LSTM layers)
        :param batch_first: if True the model uses (batch,seq,feature) tensors,
            if false the model uses (seq, batch, feature)
        :param embedder: instance of nn.Embedding, if None constructor will
            create new embedding layer
        :param init_weight: range for the uniform initializer
        """
        super().__init__()
        self.num_layers = num_layers
        self.att_rnn = RecurrentAttention(hidden_size, hidden_size, hidden_size, num_layers=1, batch_first=batch_first, dropout=dropout)
        self.rnn_layers = nn.ModuleList()
        for _ in range(num_layers - 1):
            self.rnn_layers.append(nn.LSTM(2 * hidden_size, hidden_size, num_layers=1, bias=True, batch_first=batch_first))
        if embedder is not None:
            self.embedder = embedder
        else:
            self.embedder = nn.Embedding(vocab_size, hidden_size, padding_idx=PAD)
            nn.init.uniform_(self.embedder.weight.data, -init_weight, init_weight)
        self.classifier = Classifier(hidden_size, vocab_size)
        self.dropout = nn.Dropout(p=dropout)

    def init_hidden(self, hidden):
        """
        Converts flattened hidden state (from sequence generator) into a tuple
        of hidden states.

        :param hidden: None or flattened hidden state for decoder RNN layers
        """
        if hidden is not None:
            hidden = hidden.chunk(self.num_layers)
            hidden = tuple(i.chunk(2) for i in hidden)
        else:
            hidden = [None] * self.num_layers
        self.next_hidden = []
        return hidden

    def append_hidden(self, h):
        """
        Appends the hidden vector h to the list of internal hidden states.

        :param h: hidden vector
        """
        if self.inference:
            self.next_hidden.append(h)

    def package_hidden(self):
        """
        Flattens the hidden state from all LSTM layers into one tensor (for
        the sequence generator).
        """
        if self.inference:
            hidden = torch.cat(tuple(itertools.chain(*self.next_hidden)))
        else:
            hidden = None
        return hidden

    def forward(self, inputs, context, inference=False):
        """
        Execute the decoder.

        :param inputs: tensor with inputs to the decoder
        :param context: state of encoder, encoder sequence lengths and hidden
            state of decoder's LSTM layers
        :param inference: if True stores and repackages hidden state
        """
        self.inference = inference
        enc_context, enc_len, hidden = context
        hidden = self.init_hidden(hidden)
        x = self.embedder(inputs)
        x, h, attn, scores = self.att_rnn(x, hidden[0], enc_context, enc_len)
        self.append_hidden(h)
        x = torch.cat((x, attn), dim=2)
        x = self.dropout(x)
        x, h = self.rnn_layers[0](x, hidden[1])
        self.append_hidden(h)
        for i in range(1, len(self.rnn_layers)):
            residual = x
            x = torch.cat((x, attn), dim=2)
            x = self.dropout(x)
            x, h = self.rnn_layers[i](x, hidden[i + 1])
            self.append_hidden(h)
            x = x + residual
        x = self.classifier(x)
        hidden = self.package_hidden()
        return x, scores, [enc_context, enc_len, hidden]


class ResidualRecurrentEncoder(nn.Module):
    """
    Encoder with Embedding, LSTM layers, residual connections and optional
    dropout.

    The first LSTM layer is bidirectional and uses variable sequence length
    API, the remaining (num_layers-1) layers are unidirectional. Residual
    connections are enabled after third LSTM layer, dropout is applied on
    inputs to LSTM layers.
    """

    def __init__(self, vocab_size, hidden_size=1024, num_layers=4, dropout=0.2, batch_first=False, embedder=None, init_weight=0.1):
        """
        Constructor for the ResidualRecurrentEncoder.

        :param vocab_size: size of vocabulary
        :param hidden_size: hidden size for LSTM layers
        :param num_layers: number of LSTM layers, 1st layer is bidirectional
        :param dropout: probability of dropout (on input to LSTM layers)
        :param batch_first: if True the model uses (batch,seq,feature) tensors,
            if false the model uses (seq, batch, feature)
        :param embedder: instance of nn.Embedding, if None constructor will
            create new embedding layer
        :param init_weight: range for the uniform initializer
        """
        super().__init__()
        self.batch_first = batch_first
        self.rnn_layers = nn.ModuleList()
        self.rnn_layers.append(nn.LSTM(hidden_size, hidden_size, num_layers=1, bias=True, batch_first=batch_first, bidirectional=True))
        self.rnn_layers.append(nn.LSTM(2 * hidden_size, hidden_size, num_layers=1, bias=True, batch_first=batch_first))
        for _ in range(num_layers - 2):
            self.rnn_layers.append(nn.LSTM(hidden_size, hidden_size, num_layers=1, bias=True, batch_first=batch_first))
        self.dropout = nn.Dropout(p=dropout)
        if embedder is not None:
            self.embedder = embedder
        else:
            self.embedder = nn.Embedding(vocab_size, hidden_size, padding_idx=PAD)
            nn.init.uniform_(self.embedder.weight.data, -init_weight, init_weight)

    def forward(self, inputs, lengths):
        """
        Execute the encoder.

        :param inputs: tensor with indices from the vocabulary
        :param lengths: vector with sequence lengths (excluding padding)

        returns: tensor with encoded sequences
        """
        x = self.embedder(inputs)
        x = self.dropout(x)
        x = pack_padded_sequence(x, lengths.cpu().numpy(), batch_first=self.batch_first)
        x, _ = self.rnn_layers[0](x)
        x, _ = pad_packed_sequence(x, batch_first=self.batch_first)
        x = self.dropout(x)
        x, _ = self.rnn_layers[1](x)
        for i in range(2, len(self.rnn_layers)):
            residual = x
            x = self.dropout(x)
            x, _ = self.rnn_layers[i](x)
            x = x + residual
        return x


class Seq2Seq(nn.Module):
    """
    Generic Seq2Seq module, with an encoder and a decoder.
    """

    def __init__(self, encoder=None, decoder=None, batch_first=False):
        """
        Constructor for the Seq2Seq module.

        :param encoder: encoder module
        :param decoder: decoder module
        :param batch_first: if True the model uses (batch, seq, feature)
            tensors, if false the model uses (seq, batch, feature) tensors
        """
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.batch_first = batch_first

    def encode(self, inputs, lengths):
        """
        Applies the encoder to inputs with a given input sequence lengths.

        :param inputs: tensor with inputs (batch, seq_len) if 'batch_first'
            else (seq_len, batch)
        :param lengths: vector with sequence lengths (excluding padding)
        """
        return self.encoder(inputs, lengths)

    def decode(self, inputs, context, inference=False):
        """
        Applies the decoder to inputs, given the context from the encoder.

        :param inputs: tensor with inputs (batch, seq_len) if 'batch_first'
            else (seq_len, batch)
        :param context: context from the encoder
        :param inference: if True inference mode, if False training mode
        """
        return self.decoder(inputs, context, inference)

    def generate(self, inputs, context, beam_size):
        """
        Autoregressive generator, works with SequenceGenerator class.
        Executes decoder (in inference mode), applies log_softmax and topK for
        inference with beam search decoding.

        :param inputs: tensor with inputs to the decoder
        :param context: context from the encoder
        :param beam_size: beam size for the generator

        returns: (words, logprobs, scores, new_context)
            words: indices of topK tokens
            logprobs: log probabilities of topK tokens
            scores: scores from the attention module (for coverage penalty)
            new_context: new decoder context, includes new hidden states for
                decoder RNN cells
        """
        logits, scores, new_context = self.decode(inputs, context, True)
        logprobs = log_softmax(logits, dim=-1)
        logprobs, words = logprobs.topk(beam_size, dim=-1)
        return words, logprobs, scores, new_context

    def forward(self, input_encoder, input_enc_len, input_decoder):
        raise NotImplementedError


class TwoConvModel(nn.Module):
    INPUT_SIZE = [1, 1, 10, 10]

    def __init__(self, in_channels=1, out_channels=3, kernel_size=5, weight_init=1, bias_init=0):
        super().__init__()
        self.conv1 = create_conv(in_channels, out_channels, kernel_size, weight_init, bias_init)
        self.last_conv = create_conv(out_channels, 3, 1)

    def forward(self, x):
        return self.last_conv(self.conv1(x))


class ThreeConvModelMode(Enum):
    ORIGINAL = auto()
    SUPERNET = auto()
    KERNEL_STAGE = auto()
    WIDTH_STAGE = auto()
    DEPTH_STAGE = auto()


def do_conv2d(conv, input_, *, padding=None, weight=None, bias=None):
    weight = conv.weight if weight is None else weight
    bias = conv.bias if bias is None else bias
    padding = conv.padding if padding is None else padding
    return F.conv2d(input_, weight, bias, conv.stride, padding, conv.dilation, conv.groups)


def ref_kernel_transform(weights, target_kernel_size=3, start=1, end=4, transition_matrix=None):
    if transition_matrix is None:
        transition_matrix = torch.eye(target_kernel_size ** 2)
    weights = weights[:, :, start:end, start:end]
    out_channels = weights.size(0)
    in_channels = weights.size(1)
    weights = weights.reshape(weights.size(0), weights.size(1), -1)
    weights = weights.view(-1, weights.size(2))
    weights = F.linear(weights, transition_matrix)
    weights = weights.view(out_channels, in_channels, target_kernel_size ** 2)
    weights = weights.view(out_channels, in_channels, target_kernel_size, target_kernel_size)
    return weights


class ThreeConvModel(nn.Module):
    INPUT_SIZE = [1, 1, 10, 10]

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 5, bias=False)
        self.conv_to_skip = create_conv(3, 3, 1, bias=False)
        self.last_conv = create_conv(3, 1, 1)
        self.mode = ThreeConvModelMode.ORIGINAL
        self._forward_fn_per_mode = {ThreeConvModelMode.ORIGINAL: self.original_forward, ThreeConvModelMode.SUPERNET: self.supernet_forward, ThreeConvModelMode.WIDTH_STAGE: self.forward_min_subnet_on_width_stage, ThreeConvModelMode.KERNEL_STAGE: self.forward_min_subnet_on_kernel_stage, ThreeConvModelMode.DEPTH_STAGE: self.forward_min_subnet_on_depth_stage}
        self._transition_matrix = nn.Parameter(torch.eye(3 ** 2))

    def assert_weights_equal(self, model: 'ThreeConvModel'):
        params = dict(model.named_parameters())
        for name, ref_param in self.named_parameters():
            if name.endswith('transition_matrix'):
                continue
            param = params[name]
            assert torch.equal(ref_param, param)

    def assert_transition_matrix_equals(self, matrix_to_cmp: Tensor):
        assert torch.equal(self._transition_matrix, matrix_to_cmp)

    def original_forward(self, x):
        o1 = self.conv1(x)
        o2 = self.conv_to_skip(o1)
        o3 = o1 + o2
        return self.last_conv(o3)

    def supernet_forward(self, x):
        o1 = do_conv2d(self.conv1, x, padding=2)
        o2 = self.conv_to_skip(o1)
        o3 = o1 + o2
        return self.last_conv(o3)

    def forward_min_subnet_on_kernel_stage(self, x):
        ref_weights = ref_kernel_transform(self.conv1.weight, transition_matrix=self._transition_matrix)
        o1 = do_conv2d(self.conv1, x, padding=1, weight=ref_weights)
        o2 = self.conv_to_skip(o1)
        o3 = o1 + o2
        return self.last_conv(o3)

    def forward_min_subnet_on_depth_stage(self, x):
        ref_weights = ref_kernel_transform(self.conv1.weight, transition_matrix=self._transition_matrix)
        o1 = do_conv2d(self.conv1, x, padding=1, weight=ref_weights)
        o3 = o1 + o1
        return self.last_conv(o3)

    def forward_min_subnet_on_width_stage(self, x):
        ref_weights = self.conv1.weight[:1, :, :, :]
        ref_weights = ref_kernel_transform(ref_weights, transition_matrix=self._transition_matrix)
        o1 = do_conv2d(self.conv1, x, padding=1, weight=ref_weights)
        o3 = o1 + o1
        ref_weights_last = self.last_conv.weight[:, :1, :, :]
        ref_bias_last = self.last_conv.bias[:1]
        return do_conv2d(self.last_conv, o3, weight=ref_weights_last, bias=ref_bias_last)

    def forward(self, x):
        fn = self._forward_fn_per_mode[self.mode]
        return fn(x)


class TwoSequentialConvBNTestModel(nn.Module):
    INPUT_SIZE = [1, 1, 1, 1]
    w1_max = 3
    w2_max = 2
    w2_min = 1

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 1)
        self.bn1 = nn.BatchNorm2d(3)
        bias = torch.Tensor([self.w1_max, 1, 2])
        weights = bias.reshape(3, 1, 1, 1)
        self.set_params(bias, weights, self.conv1, self.bn1)
        self.conv2 = create_conv(3, 2, 1)
        self.bn2 = nn.BatchNorm2d(2)
        weight = torch.Tensor([[self.w2_min, 2, 0], [self.w2_max, 3, 0]]).reshape(2, 3, 1, 1)
        bias = torch.Tensor([1, self.w2_max])
        self.set_params(bias, weight, self.conv2, self.bn2)
        self.last_conv = create_conv(2, 1, 1)
        self.all_layers = nn.Sequential(self.conv1, self.bn1, nn.ReLU(), self.conv2, self.bn2, nn.ReLU(), self.last_conv)

    @staticmethod
    def set_params(bias, weight, conv, bn):
        conv.weight.data = weight
        list_params = TwoSequentialConvBNTestModel.get_bias_like_params(bn, conv)
        for param in list_params:
            param.data = bias

    @staticmethod
    def compare_params(bias, weight, conv, bn):
        device = conv.weight.device
        weight
        assert torch.equal(conv.weight, weight)
        list_params = TwoSequentialConvBNTestModel.get_bias_like_params(bn, conv)
        for param in list_params:
            param
            assert torch.equal(param, bias)

    @staticmethod
    def get_bias_like_params(bn, conv):
        list_params = [conv.bias, bn.weight, bn.bias, bn.running_mean, bn.running_var]
        return list_params

    def check_reorg(self):
        device = next(self.parameters()).device
        ref_bias_1 = torch.Tensor([self.w1_max, 2, 1])
        ref_weights_1 = ref_bias_1.reshape(3, 1, 1, 1)
        ref_bias_2 = torch.Tensor([self.w2_max, 1])
        ref_weights_2 = torch.Tensor([[self.w2_max, 0, 3], [self.w2_min, 0, 2]]).reshape(2, 3, 1, 1)
        TwoSequentialConvBNTestModel.compare_params(ref_bias_1, ref_weights_1, self.conv1, self.bn1)
        TwoSequentialConvBNTestModel.compare_params(ref_bias_2, ref_weights_2, self.conv2, self.bn2)
        last_bias = self.last_conv.bias
        assert torch.equal(self.last_conv.weight, torch.Tensor([2, 2]).reshape(1, 2, 1, 1))
        assert torch.equal(last_bias, torch.zeros_like(last_bias))

    def get_minimal_subnet_output(self, input_):
        relu1 = self._get_relu1_output(input_)
        return self._get_model_output(relu1, self.w2_max)

    def _get_model_output(self, relu1, value):
        conv2_output = relu1 * value + value
        bn2_output = (conv2_output - value) / math.sqrt(self.bn2.eps + value) * value + value
        relu2 = nn.ReLU()(bn2_output)
        ref_weights = self.last_conv.weight[:, :1, :, :]
        ref_bias = self.last_conv.bias[:1]
        return do_conv2d(self.last_conv, relu2, weight=ref_weights, bias=ref_bias)

    def _get_relu1_output(self, input_):
        value = self.w1_max
        conv1_output = input_ * value + value
        bn1_output = (conv1_output - value) / math.sqrt(self.bn1.eps + value) * value + value
        relu1 = nn.ReLU()(bn1_output)
        return relu1

    def get_minimal_subnet_output_without_reorg(self, input_):
        relu1 = self._get_relu1_output(input_)
        return self._get_model_output(relu1, self.w2_min)

    def forward(self, x):
        return self.all_layers(x)


class TwoConvAddConvTestModel(nn.Module):
    INPUT_SIZE = [1, 1, 1, 1]
    V13 = 2
    V23 = 4
    V11 = 3
    V21 = 1

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 1)
        self.conv2 = create_conv(1, 3, 1)
        self.last_conv = create_conv(3, 1, 1)
        self._init_params(self.conv1, torch.Tensor([self.V11, 1, self.V13]))
        self._init_params(self.conv2, torch.Tensor([self.V21, 2, self.V23]))

    @staticmethod
    def _init_params(conv, data):
        conv.bias.data = data
        weight = data.reshape(3, 1, 1, 1)
        conv.weight.data = weight

    @staticmethod
    def get_bias_like_params(bn, conv):
        list_params = [conv.bias, bn.weight, bn.bias, bn.running_mean, bn.running_var]
        return list_params

    def check_reorg(self):
        device = get_model_device(self)
        ref_bias_1 = torch.Tensor([self.V13, self.V11, 1])
        ref_bias_2 = torch.Tensor([self.V23, self.V21, 2])
        ref_weights_1 = ref_bias_1.reshape(3, 1, 1, 1)
        ref_weights_2 = ref_bias_2.reshape(3, 1, 1, 1)
        assert torch.equal(self.conv1.weight, ref_weights_1)
        assert torch.equal(self.conv1.bias, ref_bias_1)
        assert torch.equal(self.conv2.weight, ref_weights_2)
        assert torch.equal(self.conv2.bias, ref_bias_2)
        last_bias = self.last_conv.bias
        assert torch.equal(self.last_conv.weight, torch.Tensor([2, 2, 2]).reshape(1, 3, 1, 1))
        assert torch.equal(last_bias, torch.zeros_like(last_bias))

    def get_minimal_subnet_output(self, x):
        o = self.V13 * x + self.V13 + (self.V23 * x + self.V23)
        ref_weights = self.last_conv.weight[:, :1, :, :]
        ref_bias = self.last_conv.bias[:1]
        return do_conv2d(self.last_conv, o, weight=ref_weights, bias=ref_bias)

    def get_minimal_subnet_output_without_reorg(self, x):
        o = self.V11 * x + self.V11 + (self.V21 * x + self.V21)
        return self.last_conv(o)

    def forward(self, x):
        o = self.conv1(x) + self.conv2(x)
        return self.last_conv(o)


class ConvTwoFcTestModel(nn.Module):
    INPUT_SIZE = [1, 1, 1, 1]
    V11 = 2
    V13 = 4

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 3, 1)
        self.fc1 = nn.Linear(3, 2)
        self.fc2 = nn.Linear(2, 3)
        self._init_params_conv(self.conv, torch.Tensor([self.V11, 1, self.V13]))
        self._init_params_fc(self.fc1, torch.Tensor([[3, 1, 2], [4, 6, 5]]))
        self._init_params_fc(self.fc2, torch.Tensor([[2, 1], [3, 4], [6, 5]]))

    @staticmethod
    def _init_params_conv(conv, data):
        conv.bias.data = data
        weight = data.reshape(3, 1, 1, 1)
        conv.weight.data = weight

    @staticmethod
    def _init_params_fc(fc, data):
        fc.weight.data = data
        fc.bias.data = data[:, 0]

    def check_reorg(self):
        device = get_model_device(self)
        ref_bias_1 = torch.Tensor([self.V13, self.V11, 1])
        ref_weights_1 = ref_bias_1.reshape(3, 1, 1, 1)
        fc_weights_1 = torch.Tensor([[5, 4, 6], [2, 3, 1]])
        fc_bias_1 = torch.Tensor([4, 3])
        fc_weights_2 = torch.Tensor([[1, 2], [4, 3], [5, 6]])
        assert torch.equal(self.conv.weight, ref_weights_1)
        assert torch.equal(self.conv.bias, ref_bias_1)
        assert torch.equal(self.fc1.weight, fc_weights_1)
        assert torch.equal(self.fc1.bias, fc_bias_1)
        assert torch.equal(self.fc2.weight, fc_weights_2)

    def get_minimal_subnet_output(self, x):
        device = get_model_device(self)
        fc_weight_1 = torch.Tensor([[5]])
        fc_weight_2 = torch.Tensor([[1], [4], [5]])
        fc_bias_2 = torch.Tensor([2, 3, 6]).reshape(1, 1, 1, 3)
        x = self.V13 * x + self.V13
        x = x * fc_weight_1.t() + 4
        x = x * fc_weight_2.t() + fc_bias_2
        return x

    def forward(self, x):
        x = self.conv(x)
        x = x.view(1, -1)
        x = self.fc1(x)
        x = self.fc2(x)
        return x


class TwoSequentialFcLNTestModel(nn.Module):
    INPUT_SIZE = [1, 1]

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(1, 2)
        self.fc2 = nn.Linear(2, 3)
        self.ln1 = nn.LayerNorm(2)
        self.ln2 = nn.LayerNorm(3)
        ConvTwoFcTestModel._init_params_fc(self.fc1, torch.Tensor([[3], [4]]))
        ConvTwoFcTestModel._init_params_fc(self.fc2, torch.Tensor([[2, 1], [3, 4], [6, 5]]))
        self._init_params_ln(self.ln1)
        self._init_params_ln(self.ln2)

    @staticmethod
    def _init_params_ln(ln):
        ln.weight.data = torch.arange(len(ln.weight.data)).float()
        ln.bias.data = torch.arange(len(ln.bias.data)).float()

    def check_reorg(self):
        device = get_model_device(self)
        ref_fc_weights_1 = torch.Tensor([[4], [3]])
        ref_fc_bias_1 = torch.Tensor([4, 3])
        ref_ln_weights_1 = torch.Tensor([1, 0])
        ref_ln_bias_1 = torch.Tensor([1, 0])
        ref_fc_weights_2 = torch.Tensor([[1, 2], [4, 3], [5, 6]])
        assert torch.equal(self.fc1.weight, ref_fc_weights_1)
        assert torch.equal(self.fc1.bias, ref_fc_bias_1)
        assert torch.equal(self.fc2.weight, ref_fc_weights_2)
        assert torch.equal(self.ln1.weight, ref_ln_weights_1)
        assert torch.equal(self.ln1.bias, ref_ln_bias_1)

    def get_minimal_subnet_output(self, x):
        device = get_model_device(self)
        fc_weight_1 = torch.Tensor([[4]])
        fc_weight_2 = torch.Tensor([[1], [4], [5]])
        fc_bias_2 = torch.Tensor([2, 3, 6]).reshape(1, 1, 3)
        ln_weight_2 = torch.Tensor([0, 1, 2])
        ln_bias_2 = torch.Tensor([0, 1, 2])
        x = x * fc_weight_1.t() + 4
        x = (x - 8) / math.sqrt(self.ln1.eps + 8) * 1 + 1
        x = x * fc_weight_2.t() + fc_bias_2
        x = (x - 7) / math.sqrt(self.ln2.eps + 32 / 3) * ln_weight_2 + ln_bias_2
        return x

    def forward(self, x):
        x = self.fc1(x)
        x = self.ln1(x)
        x = self.fc2(x)
        x = self.ln2(x)
        return x


class Chomp1d(nn.Module):

    def __init__(self, chomp_size):
        super().__init__()
        self.chomp_size = chomp_size

    def forward(self, x):
        return x[:, :, :-self.chomp_size].contiguous()


class TemporalBlock(nn.Module):

    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):
        super().__init__()
        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))
        self.chomp1 = Chomp1d(padding)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(dropout)
        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation))
        self.chomp2 = Chomp1d(padding)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(dropout)
        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1, self.conv2, self.chomp2, self.relu2, self.dropout2)
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
        self.relu = nn.ReLU()
        self.init_weights()

    def init_weights(self):
        self.conv1.weight.data.normal_(0, 0.01)
        self.conv2.weight.data.normal_(0, 0.01)
        if self.downsample is not None:
            self.downsample.weight.data.normal_(0, 0.01)

    def forward(self, x):
        out = self.net(x)
        res = x if self.downsample is None else self.downsample(x)
        return self.relu(out + res)


class TemporalConvNet(nn.Module):

    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):
        super().__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = num_inputs if i == 0 else num_channels[i - 1]
            out_channels = num_channels[i]
            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size, padding=(kernel_size - 1) * dilation_size, dropout=dropout)]
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class TCN(nn.Module):

    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):
        super().__init__()
        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)
        self.linear = nn.Linear(num_channels[-1], output_size)

    def forward(self, inputs):
        """Inputs have to have dimension (N, C_in, L_in)"""
        y1 = self.tcn(inputs)
        o = self.linear(y1[:, :, -1])
        return F.log_softmax(o, dim=1)


class VGG11_K7(nn.Module):

    def __init__(self):
        super().__init__()
        self.features = self._make_layers([64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'])
        self.classifier = nn.Linear(512, 10)

    def forward(self, x):
        out = self.features(x)
        out = out.view(out.size(0), -1)
        out = self.classifier(out)
        return out

    def _make_layers(self, config):
        layers = []
        in_channels = 3
        for x in config:
            if x == 'M':
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                layers += [nn.Conv2d(in_channels, x, kernel_size=7, padding=3, bias=False), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]
                in_channels = x
        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]
        return nn.Sequential(*layers)


class BasicTestSuperNet(nn.Module):
    INPUT_SIZE = [1, 1, 5, 5]

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 5, weight_init=1, bias_init=1)
        self.conv_to_skip = create_conv(3, 3, 1, weight_init=2, bias_init=2)

    def forward(self, x):
        o1 = self.conv1(x)
        o2 = self.conv_to_skip(o1)
        return o1 + o2


class DepthBasicConvTestModel(nn.Module):
    INPUT_SIZE = [1, 1, 15, 15]

    def __init__(self, depth=3):
        super().__init__()
        self._depth = depth
        self._skipped_layers = []
        self.conv1 = create_conv(1, 3, 3, weight_init=1, bias_init=1)
        self.branch_with_blocks = nn.Sequential()
        for idx in range(depth):
            conv = create_conv(3, 3, 5, weight_init=idx + 1, bias_init=idx + 1)
            self.branch_with_blocks.add_module('conv{}'.format(idx), conv)
        self.last_conv = create_conv(3, 1, 1)

    def forward(self, x):
        output = self.conv1(x)
        for name, module in self.branch_with_blocks._modules.items():
            if name not in self._skipped_layers:
                output = module(output)
        output = self.last_conv(output)
        return output

    def set_skipped_layers(self, skipped_layers: Optional[List]=None):
        if skipped_layers is None:
            skipped_layers = []
        self._skipped_layers = skipped_layers


class TwoPermute(nn.Module):
    INPUT_SIZE = [1, 2]

    def __init__(self):
        super().__init__()
        self.dummy = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        num_dims = len(x.size())
        permute_dims = tuple(range(num_dims - 1, -1, -1))
        x = torch.permute(x, dims=permute_dims)
        return torch.permute(x, dims=permute_dims)


class ChunkConcat(nn.Module):
    INPUT_SIZE = [2, 2]

    def __init__(self):
        super().__init__()
        self.dummy = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        chunks = torch.chunk(x, 2, 0)
        return torch.cat(chunks)


class TwoBranchesBeforeInput(nn.Module):
    INPUT_SIZE = [1, 2]

    def __init__(self):
        super().__init__()
        self.dummy = nn.Parameter(torch.ones(1))

    def forward(self, x):
        return x * self.dummy + x


@register_operator()
def custom_identity(x):
    return x


class TwoBranchesAfterInput(nn.Module):
    INPUT_SIZE = [1, 2]

    def __init__(self):
        super().__init__()
        self.dummy = nn.Parameter(torch.ones(1))

    def forward(self, x):
        x = custom_identity(x)
        return x * self.dummy + x


class FilterPruningBlockModel(nn.Module):

    def __init__(self, layer):
        super().__init__()
        self.layer = layer
        pruning_op = FilterPruningMask(layer.weight.size(0), 'test_node')
        self.op_key = self.layer.register_pre_forward_operation(UpdateWeightAndBias(pruning_op))

    @property
    def pruning_op(self):
        return self.layer.get_pre_op(self.op_key).operand

    def forward(self, x):
        return self.layer(x)


class PruningTestModel(nn.Module):
    CONV_1_NODE_NAME = 'PruningTestModel/NNCFConv2d[conv1]/conv2d_0'
    CONV_2_NODE_NAME = 'PruningTestModel/NNCFConv2d[conv2]/conv2d_0'
    CONV_3_NODE_NAME = 'PruningTestModel/NNCFConv2d[conv3]/conv2d_0'

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 2, 9, -2)
        self.relu = nn.ReLU()
        self.conv2 = create_conv(3, 1, 3, -10, 0)
        self.conv3 = create_conv(1, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.conv3(x)
        return x


class DiffConvsModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 32, 2, 9, -2)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 32, 1, groups=32)
        self.conv3 = create_conv(32, 32, 3, -10, 0, stride=2)
        self.conv4 = nn.Conv2d(32, 16, 1, groups=8)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        return x


class BranchingModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 2, 1, -2)
        self.conv2 = create_conv(1, 3, 2, 2, -2)
        self.conv3 = create_conv(1, 3, 2, 3, -2)
        self.relu = nn.ReLU()
        self.conv4 = create_conv(3, 1, 3, 10, 0)
        self.conv5 = create_conv(3, 1, 3, -10, 0)

    def forward(self, x):
        x = self.conv1(x) + self.conv2(x) + self.conv3(x)
        x = self.relu(x)
        x = self.conv4(x) + self.conv5(x)
        return x


class ResidualConnectionModel(nn.Module):

    def __init__(self, last_layer_accept_pruning=True):
        super().__init__()
        self.last_layer_accept_pruning = last_layer_accept_pruning
        self.conv1 = create_conv(1, 8, 3, 1, -2, padding=1)
        self.conv2 = create_conv(8, 8, 3, 2, -2, padding=1)
        self.conv3 = create_conv(8, 8, 3, 3, -2, padding=1)
        self.conv4 = create_conv(8, 1, 3, 10, 0, padding=1)
        self.conv5 = create_conv(8, 1, 3, -10, 0, padding=1)
        self.linear = nn.Linear(64, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv1(x)
        x = x + self.conv2(x)
        x = x + self.conv3(x)
        x = self.relu(x)
        x = self.conv4(x) + self.conv5(x)
        b, *_ = x.size()
        view_const = (b, -1) if self.last_layer_accept_pruning else -1
        x = x.view(view_const)
        x = self.linear(x)
        return x


class EltwiseCombinationModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 8, 3, 1, -2, padding=1)
        self.conv2 = create_conv(8, 8, 3, 2, -2, padding=1)
        self.conv3 = create_conv(8, 8, 3, 3, -2, padding=1)
        self.conv4 = create_conv(8, 8, 3, 10, 0, padding=1)
        self.conv5 = create_conv(8, 1, 3, -10, 0, padding=1)
        self.conv6 = create_conv(8, 1, 3, -10, 0, padding=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.conv1(x)
        x = x + self.conv2(x)
        x_1 = x + self.conv3(x)
        x_2 = x + self.conv4(x)
        x = self.conv5(x_1) + self.conv6(x_2)
        return x


class PruningTestModelConcat(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 2, 1, -2)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(16, 32, 2, 2, -2)
        self.conv3 = create_conv(16, 32, 2, 2, -2)
        for i in range(32):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.relu = nn.ReLU()
        self.conv4 = create_conv(64, 16, 3, 10, 0)
        for i in range(16):
            self.conv4.weight.data[i] += i

    def forward(self, x):
        x = self.conv1(x)
        x = torch.cat([self.conv2(x), self.conv3(x)], dim=1)
        x = self.relu(x)
        x = self.conv4(x)
        return x


class PruningTestModelConcatWithLinear(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 2, 1, -2)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(16, 32, 2, 2, -2)
        for i in range(32):
            self.conv2.weight.data[i] += i
        self.conv3 = create_conv(16, 32, 2, 2, -2)
        for i in range(32):
            self.conv3.weight.data[i] += i
        self.relu = nn.ReLU()
        self.linear = nn.Linear(64 * 6 * 6, 1)
        self.linear.weight.data[0] = 1

    def forward(self, x):
        x = self.conv1(x)
        x = torch.cat([self.conv2(x), self.conv3(x)], dim=1)
        x = self.relu(x.view(1, -1))
        x = self.linear(x)
        return x


class PruningTestModelDiffChInPruningCluster(nn.Module):

    def __init__(self):
        super().__init__()
        self.first_conv = create_conv(1, 16, 2, 1, -2)
        for i in range(16):
            self.first_conv.weight.data[i] += i
        self.conv1 = create_conv(16, 32, 2, 1, -2)
        for i in range(32):
            self.conv1.weight.data[i] += i
        self.linear1 = nn.Linear(16 * 7 * 7, 32 * 6 * 6)
        for i in range(32 * 6 * 6):
            self.linear1.weight.data[i] += i
        self.last_linear = nn.Linear(32 * 6 * 6, 1)
        self.last_linear.weight.data[0] = 1

    def forward(self, x):
        x = self.first_conv(x)
        y = self.conv1(x).view(1, -1)
        z = self.linear1(x.view(1, -1))
        return self.last_linear(y + z)


def fill_linear_weight(linear, value):
    linear.weight.data.fill_(value)
    with torch.no_grad():
        n = min(linear.in_features, linear.out_features)
        linear.weight[:n, :n] += torch.eye(n)


class PruningTestBatchedLinear(nn.Module):

    def __init__(self):
        super().__init__()
        self.first_conv = create_conv(1, 32, 1)
        for i in range(32):
            self.first_conv.weight.data[i] += i
        self.linear1 = nn.Linear(8, 16)
        for i in range(16):
            self.linear1.weight.data[i] = i
        self.last_linear = nn.Linear(32 * 8 * 16, 1)
        fill_linear_weight(self.last_linear, 1)

    def forward(self, x):
        x = self.first_conv(x)
        x = self.linear1(x)
        return self.last_linear(x.view(1, -1))


class PruningTestModelBroadcastedLinear(nn.Module):

    def __init__(self):
        super().__init__()
        self.first_conv = create_conv(1, 32, 1)
        for i in range(32):
            self.first_conv.weight.data[i] += i
        self.conv1 = create_conv(32, 16, 1)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.linear1 = nn.Linear(32 * 8 * 8, 16)
        for i in range(16):
            self.linear1.weight.data[i] = i
        self.last_linear = nn.Linear(16 * 8 * 8, 1)
        fill_linear_weight(self.last_linear, 1)

    def forward(self, x):
        x = self.first_conv(x)
        y = self.conv1(x)
        z = self.linear1(x.view(1, -1))
        x = y + z.view(1, -1, 1, 1)
        return self.last_linear(x.view(1, -1))


class PruningTestModelBroadcastedLinearWithConcat(nn.Module):

    def __init__(self):
        super().__init__()
        self.first_conv = create_conv(1, 32, 1)
        for i in range(32):
            self.first_conv.weight.data[i] += i
        self.conv1 = create_conv(32, 16, 1)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.linear1 = nn.Linear(32 * 8 * 8, 16)
        for i in range(16):
            self.linear1.weight.data[i] = i
        self.conv2 = create_conv(32, 16, 1)
        for i in range(16):
            self.conv2.weight.data[i] += i
        self.last_linear = nn.Linear(2 * 16 * 8 * 8, 1)
        fill_linear_weight(self.last_linear, 1)

    def forward(self, x):
        x = self.first_conv(x)
        y = self.conv1(x)
        z = self.linear1(x.view(1, -1))
        w = y + z.view(1, -1, 1, 1)
        p = self.conv2(x)
        x = torch.cat((w, p), dim=1)
        return self.last_linear(x.view(1, -1))


class PruningTestModelConcatBN(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 1, 1, -2)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(16, 16, 1, 2, -2)
        self.conv3 = create_conv(16, 16, 1, 2, -2)
        for i in range(16):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.relu = nn.ReLU()
        self.conv4 = create_conv(32, 16, 1, 10, 0)
        for i in range(16):
            self.conv4.weight.data[i] += 16 - i
        self.conv5 = create_conv(48, 16, 1, 10, 0)
        self.bn = nn.BatchNorm2d(16)
        self.bn.bias = torch.nn.Parameter(torch.ones(16))
        self.bn1 = nn.BatchNorm2d(32)
        self.bn1.bias = torch.nn.Parameter(torch.ones(32))
        self.bn2 = nn.BatchNorm2d(48)
        self.bn2.bias = torch.nn.Parameter(torch.ones(48))

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn(x)
        x = torch.cat([self.conv2(x), self.conv3(x)], dim=1)
        x1 = self.bn1(x)
        x1 = self.conv4(x1)
        x = torch.cat([x1, x], dim=1)
        x = self.bn2(x)
        x = self.conv5(x)
        return x


class PruningTestModelEltwise(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 2, 1, -2)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(16, 32, 2, 2, -2)
        self.conv3 = create_conv(16, 32, 2, 2, -2)
        for i in range(32):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.relu = nn.ReLU()
        self.conv4 = create_conv(32, 16, 3, 10, 0)
        for i in range(16):
            self.conv4.weight.data[i] += i

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.conv2(x) + self.conv3(x)
        x = self.relu(x)
        x = self.conv4(x)
        return x


def create_bn(num_features, dim=2):
    bn_dim_map = {(1): nn.BatchNorm1d, (2): nn.BatchNorm2d, (3): nn.BatchNorm3d}
    return bn_dim_map[dim](num_features)


def create_depthwise_conv(channels, kernel_size, weight_init, bias_init, padding=0, stride=1, dim=2):
    conv_dim_map = {(1): nn.Conv1d, (2): nn.Conv2d, (3): nn.Conv3d}
    conv = conv_dim_map[dim](channels, channels, kernel_size, padding=padding, stride=stride, groups=channels)
    fill_conv_weight(conv, weight_init, dim)
    fill_bias(conv, bias_init)
    return conv


def create_transpose_conv(in_channels, out_channels, kernel_size, weight_init, bias_init, stride, dim=2):
    conv_dim_map = {(1): nn.ConvTranspose1d, (2): nn.ConvTranspose2d, (3): nn.ConvTranspose3d}
    conv = conv_dim_map[dim](in_channels, out_channels, kernel_size, stride=stride)
    fill_conv_weight(conv, weight_init, dim)
    fill_bias(conv, bias_init)
    return conv


class BigPruningTestModel(nn.Module):

    def __init__(self, dim=2):
        super().__init__()
        self.dim = dim
        self.conv1 = create_conv(1, 16, 2, 0, 1, dim=self.dim)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.bn1 = create_bn(16, dim=self.dim)
        self.relu = nn.ReLU()
        self.conv_depthwise = create_depthwise_conv(16, 3, 0, 1, dim=self.dim)
        for i in range(16):
            self.conv_depthwise.weight.data[i] += i
        self.conv2 = create_conv(16, 32, 3, 20, 0, dim=self.dim)
        for i in range(32):
            self.conv2.weight.data[i] += i
        self.bn2 = create_bn(32, dim=self.dim)
        self.up = create_transpose_conv(32, 64, 3, 3, 1, 2, dim=self.dim)
        for i in range(64):
            self.up.weight.data[0][i] += i
        self.linear = nn.Linear(448 * 7 ** (self.dim - 1), 128)
        self.layernorm = nn.LayerNorm(128)
        for i in range(128):
            self.linear.weight.data[i] = i
            self.layernorm.weight.data[i] = i
        self.linear.bias.data.fill_(1)
        self.layernorm.bias.data.fill_(1)
        self.bn3 = create_bn(128, dim=self.dim)
        self.conv3 = create_conv(128, 1, 1, 5, 1, dim=self.dim)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv_depthwise(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.up(x)
        x = self.relu(x)
        b, *_ = x.size()
        x = self.linear(x.view(b, -1))
        x = self.layernorm(x).view(b, -1, *([1] * self.dim))
        x = self.bn3(x)
        x = self.conv3(x)
        x = x.view(b, -1)
        return x


class TestShuffleUnit(nn.Module):

    def __init__(self, in_channels, out_channels, downsample):
        super().__init__()
        self.downsample = downsample
        mid_channels = out_channels // 2
        self.compress_conv1 = create_conv(in_channels if self.downsample else mid_channels, mid_channels, 1, 1, -2)
        self.dw_conv2 = create_depthwise_conv(mid_channels, 3, 2, -2, padding=1, stride=2 if self.downsample else 1)
        self.expand_conv3 = create_conv(mid_channels, mid_channels, 1, 1, -2)
        if downsample:
            self.dw_conv4 = create_depthwise_conv(in_channels, 3, 2, -2, padding=1, stride=2)
            self.expand_conv5 = create_conv(in_channels, mid_channels, 1, 1, -2)
        self.activ = nn.ReLU(inplace=True)

    def forward(self, x):
        if self.downsample:
            y1 = self.dw_conv4(x)
            y1 = self.expand_conv5(y1)
            y1 = self.activ(y1)
            x2 = x
        else:
            y1, x2 = torch.chunk(x, chunks=2, dim=1)
        y2 = self.compress_conv1(x2)
        y2 = self.activ(y2)
        y2 = self.dw_conv2(y2)
        y2 = self.expand_conv3(y2)
        y2 = self.activ(y2)
        if not self.downsample:
            y2 = y2 + x2
        x = torch.cat((y1, y2), dim=1)
        return x


class ShuffleNetUnitModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 16, 1, 1, -2)
        self.unit1 = TestShuffleUnit(16, 16, False)

    def forward(self, x):
        x = self.conv(x)
        x = self.unit1(x)
        return x


class ShuffleNetUnitModelDW(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 16, 1, 1, -2)
        self.unit1 = TestShuffleUnit(16, 32, True)

    def forward(self, x):
        x = self.conv(x)
        x = self.unit1(x)
        return x


class MultipleForwardModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 1, 1, 1)
        self.conv1 = nn.Conv2d(1, 1, 1, 1)

    def forward(self, x):
        x = self.conv(x)
        x1 = self.conv1(x)
        x2 = self.conv1(x)
        return x1, x2


class GroupNormModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 1, 1, -2)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.gn1 = nn.GroupNorm(16, 16)
        self.conv2 = create_conv(16, 16, 1, 1, -2)
        for i in range(16):
            self.conv2.weight.data[i] += i
        self.gn2 = nn.GroupNorm(2, 16)

    def forward(self, x):
        x = self.conv1(x)
        x = self.gn1(x)
        x = self.conv2(x)
        x = self.gn2(x)
        return x


class PruningTestWideModelConcat(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 512, 1, 1, 1)
        for i in range(512):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(512, 1024, 1, 1, 1)
        self.conv3 = create_conv(512, 1024, 1, 1, 1)
        for i in range(1024):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.conv4 = create_conv(2048, 2048, 1, 1, 1)
        for i in range(2048):
            self.conv4.weight.data[i] += i

    def forward(self, x):
        x = self.conv1(x)
        x = torch.cat([self.conv2(x), self.conv3(x)], dim=1)
        x = self.conv4(x)
        return x


class PruningTestWideModelEltwise(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 512, 1, 1, 1)
        for i in range(512):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(512, 1024, 1, 1, 1)
        self.conv3 = create_conv(512, 1024, 1, 1, 1)
        for i in range(1024):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.conv4 = create_conv(1024, 1024, 1, 1, 1)
        for i in range(1024):
            self.conv4.weight.data[i] += i

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x) + self.conv3(x)
        x = self.conv4(x)
        return x


class PruningTestModelSharedConvs(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 512, 1, 1, 1)
        for i in range(512):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(512, 1024, 3, 1, 1)
        self.conv3 = create_conv(1024, 1024, 1, 1, 1)
        for i in range(1024):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.maxpool = nn.MaxPool2d(2)

    def forward(self, in1):
        in1 = self.conv1(in1)
        in2 = self.maxpool(in1)
        out1 = self.conv2(in1)
        out2 = self.conv2(in2)
        return self.conv3(out1), self.conv3(out2)


class PruningTestModelWrongDims(nn.Module):

    def __init__(self):
        super().__init__()
        self.first_conv = create_conv(1, 8, 1)
        self.last_conv = create_conv(32, 1, 1)

    def forward(self, x):
        x = self.first_conv(x)
        x = x.view(-1, 32, 4, 4)
        return self.last_conv(x)


class PruningTestModelWrongDimsElementwise(nn.Module):

    def __init__(self, use_last_conv=True):
        super().__init__()
        self.use_last_conv = use_last_conv
        self.first_conv = create_conv(1, 8, 1)
        self.branch_conv = create_conv(8, 32, 2, stride=2)
        if use_last_conv:
            self.last_conv = create_conv(32, 1, 1)

    def forward(self, x):
        x = self.first_conv(x)
        y = self.branch_conv(x)
        x = x.view(y.shape)
        x = x + y
        if self.use_last_conv:
            x = self.last_conv(x)
        return x


class PruningTestModelSimplePrunableLinear(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 4, 1)
        self.linear = nn.Linear(256, 32)
        self.last_linear = nn.Linear(32, 1)

    def forward(self, x):
        x = self.conv(x)
        x = self.linear(torch.flatten(x, start_dim=1))
        x = self.last_linear(x)
        return x


class DisconectedGraphModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 1, 1, -2)
        for i in range(16):
            self.conv1.weight.data[i] += i
        self.conv2 = create_conv(16, 16, 1, 1, -2)
        for i in range(16):
            self.conv2.weight.data[i] += i
        self.conv3 = create_conv(16, 1, 1, 1, -2)
        self.relu = nn.ReLU()
        self.fc = nn.Linear(64, 3)

    def forward(self, x):
        x = self.conv1(x)
        with no_nncf_trace():
            x = self.relu(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(-1, 64)
        x = self.fc(x)
        with no_nncf_trace():
            x = self.relu(x)
        x = copy.copy(x)
        return x


class DepthwiseConvolutionModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 512, 1, 1, 1)
        self.conv4 = create_conv(1024, 512, 2, 1, 1)
        for i in range(512):
            self.conv1.weight.data[i] += i
            self.conv4.weight.data[i] += i
        self.conv2 = create_conv(512, 1024, 3, 1, 1)
        self.conv3 = create_conv(512, 1024, 3, 1, 1)
        self.depthwise_conv = create_depthwise_conv(1024, 5, 1, 1)
        for i in range(1024):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
            self.depthwise_conv.weight.data[i] += i

    def forward(self, x):
        x = self.conv1(x)
        x1 = self.conv2(x)
        x2 = self.conv3(x)
        x = x1 + x2
        x = self.depthwise_conv(x)
        return self.conv4(x)


def create_grouped_conv(in_channels, out_channels, kernel_size, groups, weight_init=1, bias_init=0, padding=0, stride=1):
    if in_channels % groups != 0 or out_channels % groups != 0:
        raise RuntimeError('Cannot create grouped convolution. Either `in_channels` or `out_channels` are not divisible by `groups`')
    conv = nn.Conv2d(in_channels, out_channels, kernel_size, groups=groups, padding=padding, stride=stride)
    fill_conv_weight(conv, weight_init)
    fill_bias(conv, bias_init)
    return conv


class MultipleDepthwiseConvolutionModel(nn.Module):
    """
    Model with group conv which has
    out_channels = 2 * in_channels and
    in_channels == groups
    """

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 512, 1, 1, 1)
        self.conv4 = create_conv(2048, 512, 2, 1, 1)
        for i in range(512):
            self.conv1.weight.data[i] += i
            self.conv4.weight.data[i] += i
        self.conv2 = create_conv(512, 1024, 3, 1, 1)
        self.conv3 = create_conv(512, 1024, 3, 1, 1)
        self.depthwise_conv = create_grouped_conv(1024, 2048, 5, 1024, 1, 1)
        for i in range(1024):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
            self.depthwise_conv.weight.data[i] += i

    def forward(self, x):
        x = self.conv1(x)
        x1 = self.conv2(x)
        x2 = self.conv3(x)
        x = x1 + x2
        x = self.depthwise_conv(x)
        return self.conv4(x)


class GroupedConvolutionModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 512, 1, 1, 1)
        for i in range(512):
            self.conv1.weight.data[i] += i
        self.conv2 = create_grouped_conv(512, 128, 3, 4, 1, 1)
        self.conv3 = create_depthwise_conv(128, 3, 1, 1)
        for i in range(128):
            self.conv2.weight.data[i] += i
            self.conv3.weight.data[i] += i
        self.fc = nn.Linear(2048, 128)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = x.view(-1)
        return self.fc(x)


class SplitIdentityModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 4, 1, 1)
        self.conv2 = create_conv(4, 8, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        y1 = torch.chunk(x, chunks=1, dim=1)
        y1 = self.conv2(y1[0])
        return y1


class SplitMaskPropFailModel(nn.Module):
    """
    Weights have shape [N, C, C, W] and split dimension is not 1, but 2.
    Mask propagation should fail because of inconsistency of number of channels (C)
    and length of the resulting mask (C/2).
    """

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 6, 3, 1)
        self.conv2 = create_conv(6, 12, 1, 1)
        self.conv3 = create_conv(6, 12, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        y1, y2 = torch.chunk(x, chunks=2, dim=2)
        y1 = self.conv2(y1)
        y2 = self.conv3(y2)
        return y1, y2


class SplitPruningInvalidModel(nn.Module):
    """
    Weights have shape [N, C, 2C, W] and split dimension is not 1, but 2.
    Mask propagation won't fail with the current code, but pruning will be invalid.
    """

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 3, 3, 1)
        self.conv2 = create_conv(3, 6, 1, 1)
        self.conv3 = create_conv(3, 6, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        y1, y2 = torch.chunk(x, chunks=2, dim=2)
        y1 = self.conv2(y1)
        y2 = self.conv3(y2)
        return y1, y2


class SplitConcatModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 1, 1)
        self.conv2 = create_conv(8, 16, 1, 1)
        self.conv3 = create_conv(8, 16, 1, 1)
        self.conv4 = create_conv(32, 64, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        y1, y2 = torch.chunk(x, chunks=2, dim=1)
        y1 = self.conv2(y1)
        y2 = self.conv3(y2)
        y = torch.cat([y1, y2], dim=1)
        y = self.conv4(y)
        return y


class MultipleSplitConcatModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 1, 1)
        self.conv2 = create_conv(8, 16, 1, 1)
        self.conv3 = create_conv(6, 12, 1, 1)
        self.conv4 = create_conv(4, 8, 1, 1)
        self.conv5 = create_conv(14, 28, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        y1, y2 = torch.chunk(x, chunks=2, dim=1)
        y1 = self.conv2(y1)
        y3, y4, y5 = torch.chunk(x, chunks=3, dim=1)
        y = torch.cat([y2, y3], dim=1)
        y4 = self.conv3(y4)
        y5 = self.conv4(y5)
        y = self.conv5(y)
        return y


class SplitReshapeModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 8, 1, 1)
        self.conv2 = create_conv(4, 8, 1, 1)
        self.conv3 = create_conv(4, 8, 1, 1)

    def forward(self, x):
        x = self.conv1(x)
        y1, y2 = torch.chunk(x, chunks=2, dim=2)
        y1 = torch.reshape(y1, [1, 4, 8, 8])
        y2 = torch.reshape(y2, [1, 4, 8, 8])
        y1 = self.conv2(y1)
        y2 = self.conv3(y2)
        return y1, y2


class HRNetBlock(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 16, 5, 5)
        self.conv2 = create_conv(1, 16, 1, 1)
        self.conv3 = create_conv(8, 16, 5, 5)
        self.conv4 = create_conv(8, 16, 1, 1)
        self.conv5 = create_conv(16, 32, 1, 1)
        self.conv6 = create_conv(48, 48, 1, 1)
        self.conv7 = create_conv(48, 48, 1, 1)
        for conv in [getattr(self, f'conv{i}') for i in range(1, 8)]:
            for i in range(conv.out_channels):
                conv.weight.data[i] = i
        self.avg_pool = nn.AdaptiveAvgPool2d(4)

    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.conv2(x)
        y1, x1 = torch.chunk(x1, chunks=2, dim=1)
        x2, y2 = torch.chunk(x2, chunks=2, dim=1)
        x2 = self.avg_pool(x2)
        y2 = self.conv3(y2)
        y1 = self.conv4(y1)
        y = torch.cat([x1, x2], dim=1)
        y = self.conv5(y)
        out1 = torch.cat([y1, y], dim=1)
        out2 = torch.cat([y, y2], dim=1)
        out1 = self.conv6(out1)
        out2 = self.conv7(out2)
        return out1, out2


def make_divisible(v, divisor, min_value=None):
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class SELayerWithReshape(nn.Module):

    def __init__(self, channel, reduction=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(nn.Conv2d(channel, make_divisible(channel // reduction, 8), 1), nn.BatchNorm2d(make_divisible(channel // reduction, 8)), nn.ReLU(inplace=True), nn.Conv2d(make_divisible(channel // reduction, 8), channel, 1), nn.BatchNorm2d(channel), nn.ReLU(inplace=True))

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x)
        y = y.view(b, c).view(b, c, 1, 1)
        y = self.fc(y)
        return x * y


class SELayerWithReshapeAndLinear(nn.Module):

    def __init__(self, channel, reduction=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(nn.Linear(channel, make_divisible(channel // reduction, 8)), nn.BatchNorm1d(make_divisible(channel // reduction, 8)), nn.ReLU(inplace=True), nn.Linear(make_divisible(channel // reduction, 8), channel), nn.BatchNorm1d(channel), nn.ReLU(inplace=True))

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y


class SELayerWithReshapeAndLinearAndMean(nn.Module):

    def __init__(self, channel, reduction=4):
        super().__init__()
        self.fc = nn.Sequential(nn.Linear(channel, make_divisible(channel // reduction, 8)), nn.ReLU(inplace=True), nn.Linear(make_divisible(channel // reduction, 8), channel), nn.ReLU(inplace=True))

    def forward(self, x):
        b, c, _, _ = x.size()
        y = x.mean((2, 3))
        y = self.fc(y).view(b, c, 1, 1)
        return x * y


class MobilenetV3BlockSEReshape(nn.Module):

    def __init__(self, mode='default'):
        super().__init__()
        se_block_map = {'default': SELayerWithReshape, 'linear': SELayerWithReshapeAndLinear, 'linear_mean': SELayerWithReshapeAndLinearAndMean}
        se_block = se_block_map[mode]
        self.first_conv = nn.Conv2d(1, 6, 2)
        self.inverted_residual = InvertedResidual(6, 6, 6, 5, 1, se_block)
        self.last_conv = nn.Conv2d(6, 1, 1)

    def forward(self, x):
        x = self.first_conv(x)
        x = self.inverted_residual(x)
        return self.last_conv(x)


class SepConv(nn.Module):
    """Separable Convolution."""

    def __init__(self, in_planes, out_planes, kernel_size, stride):
        super().__init__()
        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding=(kernel_size - 1) // 2, bias=False, groups=in_planes)
        self.bn1 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        return self.bn1(self.conv1(x))


class CellB(nn.Module):

    def __init__(self, in_planes, out_planes, stride=1):
        super().__init__()
        self.stride = stride
        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)
        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)
        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)
        if stride == 2:
            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
            self.bn1 = nn.BatchNorm2d(out_planes)
        self.conv2 = nn.Conv2d(2 * out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        y1 = self.sep_conv1(x)
        y2 = self.sep_conv2(x)
        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)
        if self.stride == 2:
            y3 = self.bn1(self.conv1(y3))
        y4 = self.sep_conv3(x)
        b1 = F.relu(y1 + y2)
        b2 = F.relu(y3 + y4)
        y = torch.cat([b1, b2], 1)
        return F.relu(self.bn2(self.conv2(y)))


class NASnetBlock(nn.Module):

    def __init__(self):
        super().__init__()
        self.first_conv = nn.Conv2d(1, 2, (2, 2))
        self.cell = CellB(2, 4, 2)

    def forward(self, x):
        x = self.first_conv(x)
        x = self.cell(x)
        return x


class QuantizeInputsTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv3 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv4 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)
        self.conv5 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1)
        self.conv6 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=2)
        self.linear = nn.Linear(in_features=8, out_features=8)

    def forward(self, input_1, input_2, input_3, input_4, input_5):
        x_1 = self.conv1(input_1)
        x_1 = self.conv4(x_1)
        x_1 = F.adaptive_avg_pool2d(x_1, output_size=1)
        x_1 = x_1.flatten(start_dim=1)
        x_2_br = F.max_pool2d(input_2, kernel_size=2)
        x_2 = self.conv2(x_2_br)
        x_2 = F.adaptive_avg_pool2d(x_2, output_size=1)
        x_2 = x_2.flatten(start_dim=1)
        x_2 = self.linear(x_2)
        x_3 = F.max_pool2d(input_3, kernel_size=2)
        x_3 = x_3 + torch.ones_like(x_3)
        x_3 = self.conv3(x_3)
        x_3 = x_3.flatten(start_dim=1)
        x_2_br = x_2_br.flatten(start_dim=1)
        x_3 = torch.cat([x_2_br, x_3], dim=-1)
        x_3 = self.conv5(x_3.unsqueeze(2).unsqueeze(3).transpose(1, 2))
        x_3 = F.adaptive_avg_pool2d(x_3, output_size=1)
        x_3 = x_3.flatten(start_dim=1)
        x_4 = F.max_pool2d(input_4, kernel_size=2)
        x_5 = F.max_pool2d(input_5, kernel_size=2)
        x_45 = torch.cat([x_4, x_5], dim=1)
        x_45 = self.conv6(x_45)
        x_45 = x_45.flatten(start_dim=1)
        in_5_flat = input_5.flatten(start_dim=1)
        x_45 += F.pad(input_5.flatten(start_dim=1), [0, x_45.shape[1] - in_5_flat.shape[1]])
        return torch.cat([x_1, x_2, x_3, x_45], dim=-1)


class QuantizeOutputsTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv3 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv4 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)
        self.conv5 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3)

    def forward(self, x):
        self.conv5(x)
        return self.conv1(x), self.conv2(x), self.conv3(x), self.conv4(x)


class SharedLayersModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.shared_conv = torch.nn.Conv2d(1, 1, 1)

    def forward(self, x):
        x = self.shared_conv(x)
        x = x + x
        x = self.shared_conv(x)
        x = x * x
        return x


class ConvLinear(nn.Module):
    CONV_FLOPS = 72
    LINEAR_FLOPS = 108

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 1, 2, -1, -2)
        self.fc = nn.Linear(3, 6)

    def forward(self, x):
        return self.fc(self.conv1(x))


class ModelForHWConfigTest(torch.nn.Module):
    CONV2D_OP_NODE_NAME = 'ModelForHWConfigTest/NNCFConv2d[conv2d]/conv2d_0'

    def __init__(self, with_hardswish=False):
        super().__init__()
        self.with_hardswish = with_hardswish
        self.conv2d = torch.nn.Conv2d(2, 1, 1)

    def forward(self, x_):
        if self.with_hardswish:
            x_ = torch.nn.functional.hardswish(x_)
        x_ = self.conv2d(x_)
        x_ = x_.matmul(x_)
        return x_


class TargetCompressionIdxTestModel(torch.nn.Module):
    CONV2D_TARGET_CHANNEL_COUNT = 5
    CONV2D_TRANSPOSE_TARGET_CHANNEL_COUNT = 10

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=self.CONV2D_TARGET_CHANNEL_COUNT, kernel_size=(1, 1))
        self.conv_t = torch.nn.ConvTranspose2d(in_channels=self.CONV2D_TARGET_CHANNEL_COUNT, out_channels=self.CONV2D_TRANSPOSE_TARGET_CHANNEL_COUNT, kernel_size=(1, 1))

    def forward(self, x):
        x = self.conv(x)
        x = self.conv_t(x)
        return x


class ModelWithBranches(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_1 = torch.nn.Conv2d(2, 2, (1, 1))
        self.conv_2 = torch.nn.Conv2d(2, 2, (1, 1), groups=2)
        self.conv_3 = torch.nn.Conv2d(2, 2, (1, 1), groups=2)

    def forward(self, x):
        x1 = self.conv_1(x)
        x2 = self.conv_2(x)
        x3 = self.conv_3(x)
        x4 = x + x
        return x1, x2, x3, x4


class EightConvTestModel(nn.Module):

    def __init__(self, in_out_ch=((1, 3), (3, 5), (5, 7), (7, 10))):
        super().__init__()
        self.features = []
        self.features.append(create_conv(*in_out_ch[0], 2, -1, -2))
        self.features.append(nn.BatchNorm2d(in_out_ch[0][1]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*in_out_ch[1], 5, 1, 1))
        self.features.append(nn.BatchNorm2d(in_out_ch[1][1]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*in_out_ch[2], 1, 2, 2))
        self.features.append(nn.BatchNorm2d(in_out_ch[2][1]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*in_out_ch[3], 9, -1, 0))
        self.features.append(nn.BatchNorm2d(in_out_ch[3][1]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*reversed(in_out_ch[3]), 3, 0, 1))
        self.features.append(nn.BatchNorm2d(in_out_ch[3][0]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*reversed(in_out_ch[2]), 1, -1, 9))
        self.features.append(nn.BatchNorm2d(in_out_ch[2][0]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*reversed(in_out_ch[1]), 2, 10, 1))
        self.features.append(nn.BatchNorm2d(in_out_ch[1][0]))
        self.features.append(nn.ReLU())
        self.features.append(create_conv(*reversed(in_out_ch[0]), 1, 1, 1))
        self.features.append(nn.BatchNorm2d(in_out_ch[0][0]))
        self.features.append(nn.ReLU())
        self.features = nn.Sequential(*self.features)

    def forward(self, x):
        return self.features(x)


class DepthWiseConvTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.features = []
        self.features.append(nn.Conv2d(1, 3, 3, groups=1))
        self.features.append(nn.Conv2d(3, 30, 3, groups=3))
        self.features.append(nn.Conv2d(30, 1, 3))
        self.features = nn.Sequential(*self.features)

    def forward(self, x):
        return self.features(x)


class SingleConv2dIdentityModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv2d = nn.Conv2d(3, 3, 1)
        self.conv2d.weight = torch.nn.Parameter(torch.ones_like(self.conv2d.weight))

    def forward(self, input_):
        return self.conv2d(input_)


class SingleConv2dSyntheticWeightModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv2d = nn.Conv2d(3, 3, 100)
        with torch.no_grad():
            for i in range(0, 100):
                for j in range(0, 100):
                    self.conv2d.weight[0][0][i][j] = i * 100 + j
            for i in range(0, 3):
                for j in range(0, 3):
                    if not (i == 0 and j == 0):
                        self.conv2d.weight[i][j] = self.conv2d.weight[0][0]
                        self.conv2d.weight[i][j] = self.conv2d.weight[0][0]

    def forward(self, input_):
        return self.conv2d(input_)


class EltwiseQuantizerLinkingTestModel(torch.nn.Module):

    def __init__(self):
        super().__init__()


        class Path(torch.nn.Module):

            def forward(self, input_1, input_2):
                retval0 = input_1 + input_2
                retval1 = retval0 * input_2
                retval2 = retval0 + retval1
                return retval0, retval1, retval2
        self.path1 = Path()
        self.path2 = Path()

    def forward(self, input_1, input_2):
        path1_results = self.path1(input_1, input_2)
        path2_results = self.path2(input_1, input_2)
        return tuple(x + y for x, y in zip(path1_results, path2_results))


class SingleCatModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(4, 1, 1)

    def forward(self, x, y):
        x = x * x
        y = y * y
        z = torch.cat([x, y])
        v = self.conv(z)
        return v


class DoubleCatModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(4, 1, 1)

    def forward(self, x, y):
        x = x * x
        y = y * y
        z = torch.cat([x, y])
        v = torch.cat([x, z])
        w = self.conv(v)
        return w


class UNetLikeModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_1 = torch.nn.Conv2d(4, 8, 1)
        self.conv_2 = torch.nn.Conv2d(8, 16, 1)
        self.conv_3 = torch.nn.Conv2d(16, 32, 1)
        self.conv_t_3 = torch.nn.ConvTranspose2d(32, 16, 1)
        self.conv_t_2 = torch.nn.ConvTranspose2d(16, 8, 1)
        self.conv_t_1 = torch.nn.ConvTranspose2d(8, 4, 1)

    def forward(self, x, y):
        y1 = self.conv_1(x)
        y2 = self.conv_2(y1)
        y3 = self.conv_3(y2)
        z3 = self.conv_t_3(y3)
        z3 = torch.cat([z3, y2])
        z2 = self.conv_t_2(z3)
        z2 = torch.cat([z2, y1])
        z1 = self.conv_t_1(z2)
        return z1


class SimplerModelForUnifiedScalesTesting(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv2d_1 = torch.nn.Conv2d(1, 1, 1)
        self.conv2d_2 = torch.nn.Conv2d(1, 1, 1)
        self.conv2d_3 = torch.nn.Conv2d(1, 1, 1)
        self.conv2d_4 = torch.nn.Conv2d(1, 1, 1)
        self.conv2d_5 = torch.nn.Conv2d(1, 1, 1)
        self.conv2d_6 = torch.nn.Conv2d(1, 1, 1)

    def forward(self, x):
        in_1, in_2 = x.chunk(dim=-1, chunks=2)
        in_1 = self.conv2d_1(in_1)
        in_2 = self.conv2d_2(in_2)
        x = in_1 + in_2
        x = torch.stack([x, x], dim=-1)
        x = x.squeeze(dim=0)
        in1, in2 = x.chunk(dim=-1, chunks=2)
        in1 = self.conv2d_3(in1)
        in2 = self.conv2d_3(in2)
        x = torch.cat([in1, in2], dim=-1)
        in_1, in_2 = x.chunk(dim=-1, chunks=2)
        in_1 = self.conv2d_5(in_1)
        in_2 = self.conv2d_6(in_2)
        x = in_1 * in_2
        return x


class TwoEmbeddingAddModel(torch.nn.Module):
    EMBEDDING_IO_SHAPE = [10, 10]

    def __init__(self):
        super().__init__()
        self.embedding1 = torch.nn.Embedding(*self.EMBEDDING_IO_SHAPE)
        self.embedding2 = torch.nn.Embedding(*self.EMBEDDING_IO_SHAPE)

    def forward(self, x):
        y1 = self.embedding1(x)
        y2 = self.embedding2(x)
        return y1 + y2


class SharedEmbeddingAddModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.shared_embedding = torch.nn.Embedding(10, 10)

    def forward(self, x):
        y1 = self.shared_embedding(x)
        y2 = self.shared_embedding(x)
        return y1 + y2


class MagnitudeTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 2, 2, 9, -2)
        self.conv2 = create_conv(2, 1, 3, -10, 0)

    def forward(self, x):
        return self.conv2(self.conv1(x))


class SingleLayerModel(nn.Module):

    def __init__(self, layer, frozen, size=1):
        super().__init__()
        self.size = size
        self.layer = layer
        if frozen is None:
            sparsifier = RBSparsifyingWeight(weight_shape=size)
        else:
            sparsifier = RBSparsifyingWeight(frozen=frozen, weight_shape=size)
        self.op_key = self.layer.register_pre_forward_operation(UpdateWeight(sparsifier))

    @property
    def sparsifier(self):
        return self.layer.get_pre_op(self.op_key).operand

    def forward(self, x):
        return self.layer(x)


class EmbeddingOnlyModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.embedding = torch.nn.Embedding(10, 10)

    def forward(self, x):
        return self.embedding(x)


class BasicLinearTestModel(nn.Module):

    def __init__(self, size=4):
        super().__init__()
        self.fc = nn.Linear(size, size)

    def forward(self, x):
        return self.fc(x)


class BasicTestModelWithTwoInputOutput(nn.Module):

    def __init__(self, size=4):
        super().__init__()
        self.fc0 = nn.Linear(size, size)
        self.fc1 = nn.Linear(size, size)

    def forward(self, x0, x1):
        return self.fc0(x0), self.fc1(x1)


class DeviceCheckingModel(torch.nn.Module):

    def __init__(self, device):
        super().__init__()
        self.model = TwoConvTestModel()
        self.original_device = device
        self.model

    def forward(self, x):
        for param in self.model.parameters():
            assert self.original_device in str(param.device)
        return self.model.forward(x)


class ConvBNLayer(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv2d(3, 9, (3, 3))
        self.bn = torch.nn.BatchNorm2d(9)
        self.conv1 = torch.nn.Conv2d(9, 3, (3, 3))
        self.bn1 = torch.nn.BatchNorm2d(3)

    def forward(self, x):
        x = self.bn(self.conv(x))
        return self.bn1(self.conv1(x))


class OneParameterModel(nn.Module):
    INPUT_SIZE = 0,

    def __init__(self, param):
        super().__init__()
        self.param = param

    def forward(self, _x):
        return self.param.sum()


class CustomConvModule(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones([1, 1, 1, 1]))

    def forward(self, x):
        return torch.nn.functional.conv2d(x, self.weight)


class ModelWithCustomConvModules(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.regular_conv = torch.nn.Conv2d(1, 1, 1)
        self.custom_conv = CustomConvModule()

    def forward(self, x):
        x = self.regular_conv(x)
        x = self.custom_conv(x)
        return x


class ModelWithChangedTrain(nn.Module):

    def __init__(self, in_out_channels: Tuple[Tuple[int, int]]=((1, 3), (3, 5), (5, 7), (7, 10)), freezing_stages: int=-1):
        super().__init__()
        self.freezing_stages = freezing_stages
        self.features = nn.ModuleList()
        for in_out_ch in in_out_channels:
            block = nn.ModuleList()
            block.append(nn.Conv2d(*in_out_ch, 3))
            block.append(nn.BatchNorm2d(in_out_ch[1]))
            block.append(nn.ReLU())
            self.features.append(block)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        for blocks in self.features:
            for module in blocks:
                x = module(x)
        return x

    def train(self: nn.Module, mode: bool=True) ->nn.Module:
        super().train(mode)
        for i in range(self.freezing_stages):
            for module in self.features[i]:
                for p in module.parameters():
                    p.requires_grad = False


class ModelForNameTest(Module):

    def __init__(self, size=1):
        super().__init__()
        self.size = size
        self.conv0 = Conv2d(size, size, size)
        self.conv1 = Conv2d(size, size, size)
        self.bn1 = BatchNorm2d(size)
        self.bn2 = BatchNorm2d(size)
        self.norm10 = BatchNorm2d(size)
        self.norm20 = BatchNorm2d(size)
        self.avgpool = AvgPool2d(size)
        self.layer1 = Sequential(OrderedDict([('conv01', self.conv0), ('norm01', self.norm10), ('relu01', ReLU()), ('pool01', MaxPool2d(size))]))
        self.layer2 = Sequential(OrderedDict([('layer1', self.layer1), ('conv02', self.conv0), ('relu02', ReLU()), ('norm02', self.norm20), ('pool02', MaxPool2d(size))]))

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = ReLU()(x)
        x = F.relu(x)
        x = self.bn2(x)
        x = self.layer2(x)
        x = self.avgpool(x)
        return x


input_shapes = [(1, 3, 224, 224), (2, 3, 224, 224), (1, 3, 500, 500)]


class ModelForTest(torch.nn.Module):
    IN_CHANNELS = 3
    OUT_CHANNELS = 10
    CONV1_OUT_CHANNELS = 15
    CONV2_IN_CHANNELS = CONV1_OUT_CHANNELS + IN_CHANNELS
    MAXPOOL_SIZE = 2

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(self.IN_CHANNELS, self.CONV1_OUT_CHANNELS, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(15)
        self.relu1 = nn.ReLU()
        self.convt1 = nn.ConvTranspose2d(self.CONV1_OUT_CHANNELS, self.IN_CHANNELS, kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(self.CONV2_IN_CHANNELS, self.OUT_CHANNELS, kernel_size=1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x_prev = x
        x = F.max_pool2d(x, self.MAXPOOL_SIZE)
        x = self.convt1(x)
        x = torch.cat([x, x_prev], 1)
        x = self.conv2(x)
        return x

    @staticmethod
    def simple_wrap_fn(args, kwargs):
        arglist = list(args)
        arglist[0] = nncf_model_input(arglist[0])
        args = tuple(arglist)
        return args, kwargs

    @staticmethod
    def simple_user_dummy_forward(model):
        mock_tensor = torch.zeros(input_shapes[0])
        args = mock_tensor,
        kwargs = {}
        args, kwargs = ModelForTest.simple_wrap_fn(args, kwargs)
        return wrap_nncf_model_outputs_with_objwalk(model(*args, **kwargs))


class ModelForTestWithReshapeFlattenAndConcat(ModelForTest):

    def forward(self, x):
        y = super().forward(x)
        size = y.size()
        y = y.view(size + (1, 1))
        y_copy = torch.ones_like(y)
        y = torch.stack([y, y_copy])
        y_copy = torch.ones_like(y)
        y = torch.cat([y, y_copy], -1)
        y = torch.flatten(y)
        _ = y.view(-1)
        y_copy = torch.ones_like(y)
        y = torch.stack([y, y_copy])
        y_copy = torch.ones_like(y)
        y = torch.cat([y, y_copy], -1)
        return y


class ModelForTestWithSplit(ModelForTest):

    def __init__(self):
        super().__init__()
        self.conv3 = nn.Conv2d(5, 10, kernel_size=3, padding=1)

    def forward(self, x):
        y = super().forward(x)
        y1, y2 = torch.chunk(y, chunks=2, dim=1)
        y1 = self.conv3(y1)
        y2 = self.conv3(y2)
        y = torch.cat([y1, y2], axis=1)
        return y


class ModelWithIntegerPaths(torch.nn.Module):
    INPUT_SHAPE = [2, 2, 2, 2]

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(2, 2, 1)
        self.linear = torch.nn.Linear(2, 2)

    def forward(self, x: torch.Tensor):
        x = self.conv1(x)
        sz = torch.tensor(x.shape)
        sz_tensor = torch.cat([sz])
        idx_tensor = sz_tensor // sz_tensor
        single_idx = idx_tensor[0]
        y = x[single_idx][single_idx] * torch.ones([1, 1])
        z = self.linear(y)
        return z


class ModelSpecificException(Exception):
    pass


class ExceptionRaisingModule(torch.nn.Module):


    class Inner(torch.nn.Module):

        def __init__(self):
            super().__init__()
            self.dummy_param = torch.nn.Parameter(torch.ones([1]))

        def forward(self, *args, **kwargs):
            _ = torch.cat((torch.ones([1]), torch.ones([1])))
            raise ModelSpecificException

    def __init__(self):
        super().__init__()
        self.seq = torch.nn.Sequential(self.Inner(), self.Inner())

    def forward(self, *args, **kwargs):
        return self.seq(*args, **kwargs)


class CatModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.dummy_parameter = torch.nn.Parameter(torch.ones([1]))

    def forward(self, x, y):
        return torch.cat([x, y])


class KDOutputModel(torch.nn.Module):

    def __init__(self, target_shapes: List[Tuple[int]]):
        super().__init__()
        self.mock_param = torch.nn.Parameter(torch.ones([1]))
        self.target_shapes = target_shapes

    def forward(self, *args, **kwargs):
        retval = []
        for shape in self.target_shapes:
            retval.append(torch.ones(shape) * self.mock_param)
        return retval


class CustomOutputWeightedModel(torch.nn.Module):

    def __init__(self, input_shape: List[int], outputs_dim_numbers_list: List[int]):
        super().__init__()
        self.outputs_dim_numbers_list = outputs_dim_numbers_list
        self.linear = torch.nn.Linear(in_features=input_shape[3], out_features=input_shape[3])

    def forward(self, x: torch.Tensor):
        x = self.linear(x)
        output = {(4): x, (3): x.view([x.shape[0], x.shape[1], x.shape[2] * x.shape[3]]), (2): x.view([x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]]), (1): x.view([x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3]])}
        return tuple(filter(lambda item: len(item.size()) in self.outputs_dim_numbers_list, output.values()))


class AlexNet(nn.Module):

    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2))
        self.classifier = nn.Sequential(nn.Dropout(), nn.Linear(256 * 2 * 2, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes))

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), 256 * 2 * 2)
        x = self.classifier(x)
        return x


class Transition(nn.Module):

    def __init__(self, in_planes, out_planes):
        super().__init__()
        self.bn = nn.BatchNorm2d(in_planes)
        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)

    def forward(self, x):
        out = self.conv(F.relu(self.bn(x)))
        out = F.avg_pool2d(out, 2)
        return out


class DenseNet(nn.Module):

    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):
        super().__init__()
        self.growth_rate = growth_rate
        num_planes = 2 * growth_rate
        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)
        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])
        num_planes += nblocks[0] * growth_rate
        out_planes = int(math.floor(num_planes * reduction))
        self.trans1 = Transition(num_planes, out_planes)
        num_planes = out_planes
        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])
        num_planes += nblocks[1] * growth_rate
        out_planes = int(math.floor(num_planes * reduction))
        self.trans2 = Transition(num_planes, out_planes)
        num_planes = out_planes
        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])
        num_planes += nblocks[2] * growth_rate
        out_planes = int(math.floor(num_planes * reduction))
        self.trans3 = Transition(num_planes, out_planes)
        num_planes = out_planes
        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])
        num_planes += nblocks[3] * growth_rate
        self.bn = nn.BatchNorm2d(num_planes)
        self.linear = nn.Linear(num_planes, num_classes)

    def _make_dense_layers(self, block, in_planes, nblock):
        layers = []
        for _ in range(nblock):
            layers.append(block(in_planes, self.growth_rate))
            in_planes += self.growth_rate
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.trans1(self.dense1(out))
        out = self.trans2(self.dense2(out))
        out = self.trans3(self.dense3(out))
        out = self.dense4(out)
        out = F.avg_pool2d(F.relu(self.bn(out)), 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class DPN(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']
        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.last_planes = 64
        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)
        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)
        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)
        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)
        self.linear = nn.Linear(out_planes[3] + (num_blocks[3] + 1) * dense_depth[3], 10)

    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for i, block_stride in enumerate(strides):
            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, block_stride, i == 0))
            self.last_planes = out_planes + (i + 2) * dense_depth
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class Inception(nn.Module):

    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):
        super().__init__()
        self.b1 = nn.Sequential(nn.Conv2d(in_planes, n1x1, kernel_size=1), nn.BatchNorm2d(n1x1), nn.ReLU(True))
        self.b2 = nn.Sequential(nn.Conv2d(in_planes, n3x3red, kernel_size=1), nn.BatchNorm2d(n3x3red), nn.ReLU(True), nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1), nn.BatchNorm2d(n3x3), nn.ReLU(True))
        self.b3 = nn.Sequential(nn.Conv2d(in_planes, n5x5red, kernel_size=1), nn.BatchNorm2d(n5x5red), nn.ReLU(True), nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1), nn.BatchNorm2d(n5x5), nn.ReLU(True), nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1), nn.BatchNorm2d(n5x5), nn.ReLU(True))
        self.b4 = nn.Sequential(nn.MaxPool2d(3, stride=1, padding=1), nn.Conv2d(in_planes, pool_planes, kernel_size=1), nn.BatchNorm2d(pool_planes), nn.ReLU(True))

    def forward(self, x):
        y1 = self.b1(x)
        y2 = self.b2(x)
        y3 = self.b3(x)
        y4 = self.b4(x)
        return torch.cat([y1, y2, y3, y4], 1)


class GoogLeNet(nn.Module):

    def __init__(self):
        super().__init__()
        self.pre_layers = nn.Sequential(nn.Conv2d(3, 192, kernel_size=3, padding=1), nn.BatchNorm2d(192), nn.ReLU(True))
        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)
        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)
        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)
        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)
        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)
        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)
        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)
        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)
        self.avgpool = nn.AvgPool2d(8, stride=1)
        self.linear = nn.Linear(1024, 10)

    def forward(self, x):
        out = self.pre_layers(x)
        out = self.a3(out)
        out = self.b3(out)
        out = self.maxpool(out)
        out = self.a4(out)
        out = self.b4(out)
        out = self.c4(out)
        out = self.d4(out)
        out = self.e4(out)
        out = self.maxpool(out)
        out = self.a5(out)
        out = self.b5(out)
        out = self.avgpool(out)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class CellA(nn.Module):

    def __init__(self, in_planes, out_planes, stride=1):
        super().__init__()
        self.stride = stride
        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)
        if stride == 2:
            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
            self.bn1 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        y1 = self.sep_conv1(x)
        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)
        if self.stride == 2:
            y2 = self.bn1(self.conv1(y2))
        return F.relu(y1 + y2)


class PNASNet(nn.Module):

    def __init__(self, cell_type, num_cells, num_planes):
        super().__init__()
        self.in_planes = num_planes
        self.cell_type = cell_type
        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(num_planes)
        self.layer1 = self._make_layer(num_planes, num_cells=6)
        self.layer2 = self._downsample(num_planes * 2)
        self.layer3 = self._make_layer(num_planes * 2, num_cells=6)
        self.layer4 = self._downsample(num_planes * 4)
        self.layer5 = self._make_layer(num_planes * 4, num_cells=6)
        self.linear = nn.Linear(num_planes * 4, 10)

    def _make_layer(self, planes, num_cells):
        layers = []
        for _ in range(num_cells):
            layers.append(self.cell_type(self.in_planes, planes, stride=1))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def _downsample(self, planes):
        layer = self.cell_type(self.in_planes, planes, stride=2)
        self.in_planes = planes
        return layer

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.layer5(out)
        out = F.avg_pool2d(out, 8)
        out = self.linear(out.view(out.size(0), -1))
        return out


class PreActBlock(nn.Module):

    def __init__(self, in_planes, planes, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False))
        self.fc1 = nn.Conv2d(planes, planes // 16, kernel_size=1)
        self.fc2 = nn.Conv2d(planes // 16, planes, kernel_size=1)

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        w = F.avg_pool2d(out, out.size(2))
        w = F.relu(self.fc1(w))
        w = F.sigmoid(self.fc2(w))
        out = out * w
        out += shortcut
        return out


class PreActBottleneck(nn.Module):
    """Pre-activation version of the original Bottleneck module."""
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False))

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out = self.conv3(F.relu(self.bn3(out)))
        out += shortcut
        return out


class PreActResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for block_stride in strides:
            layers.append(block(self.in_planes, planes, block_stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class ElasticResNet(ResNet):

    def __init__(self, block, num_blocks, num_classes, index_skipped_block):
        self._num_blocks = -1
        self._idx_skipped_block = index_skipped_block
        super().__init__(block, num_blocks, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for block_stride in strides:
            self._num_blocks += 1
            if self._num_blocks in self._idx_skipped_block:
                self.in_planes = planes * block.expansion
                continue
            layers.append(block(self.in_planes, planes, block_stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)


class Block(nn.Module):
    """Grouped convolution block."""
    expansion = 2

    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):
        super().__init__()
        group_width = cardinality * bottleneck_width
        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(group_width)
        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)
        self.bn2 = nn.BatchNorm2d(group_width)
        self.conv3 = nn.Conv2d(group_width, self.expansion * group_width, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * group_width)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * group_width:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * group_width, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion * group_width))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNeXt(nn.Module):

    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):
        super().__init__()
        self.cardinality = cardinality
        self.bottleneck_width = bottleneck_width
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(num_blocks[0], 1)
        self.layer2 = self._make_layer(num_blocks[1], 2)
        self.layer3 = self._make_layer(num_blocks[2], 2)
        self.linear = nn.Linear(cardinality * bottleneck_width * 8, num_classes)

    def _make_layer(self, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for block_strde in strides:
            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, block_strde))
            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width
        self.bottleneck_width *= 2
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 8)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class SENet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for block_stride in strides:
            layers.append(block(self.in_planes, planes, block_stride))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class ShuffleNet(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        out_planes = cfg['out_planes']
        num_blocks = cfg['num_blocks']
        groups = cfg['groups']
        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(24)
        self.in_planes = 24
        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)
        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)
        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)
        self.linear = nn.Linear(out_planes[2], 10)

    def _make_layer(self, out_planes, num_blocks, groups):
        layers = []
        for i in range(num_blocks):
            stride = 2 if i == 0 else 1
            cat_planes = self.in_planes if i == 0 else 0
            layers.append(Bottleneck(self.in_planes, out_planes - cat_planes, stride=stride, groups=groups))
            self.in_planes = out_planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class DownBlock(nn.Module):

    def __init__(self, in_channels, out_channels):
        super().__init__()
        mid_channels = out_channels // 2
        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1, groups=in_channels, bias=False)
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.conv2 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_channels)
        self.conv3 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(mid_channels)
        self.conv4 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=2, padding=1, groups=mid_channels, bias=False)
        self.bn4 = nn.BatchNorm2d(mid_channels)
        self.conv5 = nn.Conv2d(mid_channels, mid_channels, kernel_size=1, bias=False)
        self.bn5 = nn.BatchNorm2d(mid_channels)
        self.shuffle = ShuffleBlock()

    def forward(self, x):
        out1 = self.bn1(self.conv1(x))
        out1 = F.relu(self.bn2(self.conv2(out1)))
        out2 = F.relu(self.bn3(self.conv3(x)))
        out2 = self.bn4(self.conv4(out2))
        out2 = F.relu(self.bn5(self.conv5(out2)))
        out = torch.cat([out1, out2], 1)
        out = self.shuffle(out)
        return out


configs = {(0.5): {'out_channels': (48, 96, 192, 1024), 'num_blocks': (3, 7, 3)}, (1): {'out_channels': (116, 232, 464, 1024), 'num_blocks': (3, 7, 3)}, (1.5): {'out_channels': (176, 352, 704, 1024), 'num_blocks': (3, 7, 3)}, (2): {'out_channels': (224, 488, 976, 2048), 'num_blocks': (3, 7, 3)}}


class ShuffleNetV2(nn.Module):

    def __init__(self, net_size=0.5):
        super().__init__()
        out_channels = configs[net_size]['out_channels']
        num_blocks = configs[net_size]['num_blocks']
        self.conv1 = nn.Conv2d(3, 24, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(24)
        self.in_channels = 24
        self.layer1 = self._make_layer(out_channels[0], num_blocks[0])
        self.layer2 = self._make_layer(out_channels[1], num_blocks[1])
        self.layer3 = self._make_layer(out_channels[2], num_blocks[2])
        self.conv2 = nn.Conv2d(out_channels[2], out_channels[3], kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels[3])
        self.linear = nn.Linear(out_channels[3], 10)

    def _make_layer(self, out_channels, num_blocks):
        layers = [DownBlock(self.in_channels, out_channels)]
        for _ in range(num_blocks):
            layers.append(BasicBlock(out_channels))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.relu(self.bn2(self.conv2(out)))
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class Fire(nn.Module):

    def __init__(self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes):
        super().__init__()
        self.inplanes = inplanes
        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)
        self.squeeze_activation = nn.ReLU(inplace=True)
        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)
        self.expand1x1_activation = nn.ReLU(inplace=True)
        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)
        self.expand3x3_activation = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.squeeze_activation(self.squeeze(x))
        return torch.cat([self.expand1x1_activation(self.expand1x1(x)), self.expand3x3_activation(self.expand3x3(x))], 1)


class SqueezeNet(nn.Module):

    def __init__(self, version=1.0, num_classes=1000, dropout=0.5):
        super().__init__()
        if version not in [1.0, 1.1]:
            raise ValueError('Unsupported SqueezeNet version {version}:1.0 or 1.1 expected'.format(version=version))
        self.num_classes = num_classes
        if version == 1.0:
            self.features = nn.Sequential(nn.Conv2d(3, 96, kernel_size=7, stride=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(96, 16, 64, 64), Fire(128, 16, 64, 64), Fire(128, 32, 128, 128), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(256, 32, 128, 128), Fire(256, 48, 192, 192), Fire(384, 48, 192, 192), Fire(384, 64, 256, 256), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(512, 64, 256, 256))
        else:
            self.features = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(64, 16, 64, 64), Fire(128, 16, 64, 64), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(128, 32, 128, 128), Fire(256, 32, 128, 128), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(256, 48, 192, 192), Fire(384, 48, 192, 192), Fire(384, 64, 256, 256), Fire(512, 64, 256, 256))
        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)
        self.classifier = nn.Sequential(nn.Dropout(p=dropout), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)))
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                if m is final_conv:
                    init.normal_(m.weight, mean=0.0, std=0.01)
                else:
                    init.kaiming_uniform_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x.view(x.size(0), self.num_classes)


class SmallBlock(nn.Module):

    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.relu = nn.ReLU(inplace=False)
        self.conv2 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1, bias=False)

    def forward(self, x):
        identity_data = x
        output = self.relu(x)
        output = self.conv1(output)
        output = self.relu(output)
        output = self.conv2(output)
        output = torch.add(output, identity_data)
        return output


class SmallModel(nn.Module):

    def __init__(self, scale=3, num_of_ch_enc=16, num_of_ch_dec=8, num_of_res_blocks=4):
        super().__init__()
        self.conv_input = nn.Conv2d(in_channels=3, out_channels=num_of_ch_enc, kernel_size=3, stride=1, padding=1, bias=True)
        self.relu = nn.ReLU(inplace=False)
        self.sigmoid = nn.Sigmoid()
        self.conv_cubic1 = nn.Conv2d(in_channels=3, out_channels=num_of_ch_dec, kernel_size=3, stride=1, padding=1, bias=True)
        self.conv_cubic2 = nn.Conv2d(in_channels=num_of_ch_dec, out_channels=1, kernel_size=3, stride=1, padding=1, bias=True)
        self.residual1 = SmallBlock(num_of_ch_enc)
        self.residual2 = SmallBlock(num_of_ch_enc)
        self.residual3 = SmallBlock(num_of_ch_enc)
        self.residual4 = SmallBlock(num_of_ch_enc)
        self.conv_mid = nn.Conv2d(in_channels=num_of_ch_enc * (num_of_res_blocks + 1), out_channels=num_of_ch_dec, kernel_size=3, stride=1, padding=1, bias=True)
        if scale == 4:
            factor = 2
            self.upscale = nn.Sequential(nn.Conv2d(in_channels=num_of_ch_dec, out_channels=num_of_ch_dec * factor * factor, kernel_size=3, stride=1, padding=1, bias=True), nn.PixelShuffle(factor), nn.ReLU(inplace=True), nn.Conv2d(num_of_ch_dec, num_of_ch_dec * factor * factor, kernel_size=3, padding=1, stride=1, bias=True), nn.PixelShuffle(factor), nn.ReLU(inplace=True))
        elif scale == 3:
            self.upscale = nn.Sequential(nn.Conv2d(in_channels=num_of_ch_dec, out_channels=num_of_ch_dec * scale * scale, kernel_size=3, stride=1, padding=1, bias=True), nn.PixelShuffle(scale), nn.ReLU(inplace=True))
        else:
            raise NotImplementedError
        self.conv_output = nn.Conv2d(in_channels=num_of_ch_dec, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True)

    def forward(self, x):
        input_ = x[0]
        cubic = x[1]
        c1 = self.conv_cubic1(cubic)
        c1 = self.relu(c1)
        c2 = self.conv_cubic2(c1)
        c2 = self.sigmoid(c2)
        in1 = self.conv_input(input_)
        out = self.relu(in1)
        out1 = self.residual1(out)
        out2 = self.residual2(out1)
        out3 = self.residual3(out2)
        out4 = self.residual4(out3)
        out = torch.cat([out, out1, out2, out3, out4], dim=1)
        out = self.conv_mid(out)
        out = self.relu(out)
        out = self.upscale(out)
        out = self.conv_output(out)
        return [torch.add(out * c2, cubic)]


class ModelWithDummyParameter(nn.Module):

    def __init__(self):
        super().__init__()
        self.dummy_param = Parameter(torch.zeros(1))

    @abstractmethod
    def forward(self, x):
        pass


class ManyNonEvalModules(ModelWithDummyParameter):


    class AuxBranch(nn.Module):

        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(1, 1)
            self.weight = Parameter(torch.ones([1, 1]))

        def forward(self, x):
            x = F.linear(x, self.weight)
            x = self.linear(x)
            x = F.relu(x)
            return x


    class CustomWeightModule(nn.Module):

        def __init__(self):
            super().__init__()
            self.weight = Parameter(torch.ones([1, 1]))

        def forward(self, x):
            x = F.linear(x, self.weight)
            return x


    class ModuleWithMixedModules(nn.Module):

        def __init__(self):
            super().__init__()
            self.custom = ManyNonEvalModules.CustomWeightModule()
            self.not_called_linear = nn.Linear(1, 1)
            self.called_linear = nn.Linear(1, 1)

        def forward(self, x):
            x = Dropout(p=0.2)(x)
            x = self.custom(x)
            x = Dropout(p=0.2)(x)
            x = self.called_linear(x)
            return x

    def __init__(self):
        super().__init__()
        self.aux_branch = self.AuxBranch()
        self.mixed_modules = self.ModuleWithMixedModules()
        self.avg_pool = nn.AvgPool2d(1)

    def forward(self, x):
        x = self.avg_pool(x)
        if self.training:
            aux = self.aux_branch(x)
        x = self.mixed_modules(x)
        return (x, aux) if self.training else x


class PoolUnPool(ModelWithDummyParameter):

    def __init__(self):
        super().__init__()
        self.pool = nn.MaxPool3d(3, stride=2, return_indices=True)
        self.unpool = nn.MaxUnpool3d(3, stride=2)

    def forward(self, x):
        output, indices = self.pool(x)
        return self.unpool(output, indices)


class ArangeModel(ModelWithDummyParameter):

    def forward(self, dummy_x):
        return torch.arange(0, dummy_x.size(0), dtype=torch.int64)


class TransposeModel(ModelWithDummyParameter):

    def forward(self, x):
        o1 = x.transpose(dim0=0, dim1=0)
        o2 = x.permute(dims=[0])
        return o1, o2


class GatherModel(ModelWithDummyParameter):

    def forward(self, x):
        index = torch.zeros(1, dtype=torch.int64)
        o1 = torch.where(self.dummy_param > 0, x, self.dummy_param)
        o2 = torch.index_select(x, dim=0, index=index)
        o3 = x.index_select(dim=0, index=index)
        o4 = x[0]
        return o1, o2, o3, o4


class MaskedFillModel(ModelWithDummyParameter):

    def forward(self, x):
        o1 = x.masked_fill_(self.dummy_param > 0, 1.0)
        o2 = x.masked_fill(self.dummy_param > 0, 1.0)
        return o1, o2


class ReshapeModel(ModelWithDummyParameter):

    def forward(self, x):
        torch.squeeze(x)
        torch.unsqueeze(x, dim=0)
        torch.flatten(x)
        return x.reshape([1]), x.squeeze(), x.flatten(), x.unsqueeze(dim=0), x.view([1])


class MultiBranchesModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv_a = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1, groups=3)
        self.max_pool_b = nn.MaxPool2d(kernel_size=3)
        self.conv_b = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=5, padding=3)
        self.conv_c = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=3)
        self.conv_d = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, padding=0)

    def forward(self, x):
        x = nn.ReLU()(x)
        xa = self.conv_a(x)
        xb = self.conv_b(self.max_pool_b(x))
        xc = self.conv_c(x)
        xd = self.conv_d(x)
        return xa, xb, xc, xd


class PartlyNonDifferentialOutputsModel(nn.Module):

    def __init__(self, input_size=None):
        super().__init__()
        self.input_size = [1, 1, 4, 4] if input_size is None else input_size
        self.conv1 = torch.nn.Conv2d(in_channels=self.input_size[1], out_channels=1, kernel_size=3)
        self.conv2_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
        self.conv2_2 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)

    def forward(self, x):
        xa = self.conv1(x)
        xb = self.conv2_1(xa)
        with torch.no_grad():
            xc = self.conv2_2(xa)
        return xa, xb, xc


class ContainersOutputsModel(nn.Module):

    def __init__(self, input_size=None):
        super().__init__()
        self.input_size = [1, 1, 4, 4] if input_size is None else input_size
        self.conv1 = torch.nn.Conv2d(in_channels=self.input_size[1], out_channels=1, kernel_size=3)
        self.conv2_1 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)
        self.conv2_2 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)

    def forward(self, x):
        xa = self.conv1(x)
        xb = self.conv2_1(xa)
        xc = self.conv2_2(xa)
        return {'xa': xa, 'xb_and_xc': (xb, xc)}


class EmbeddingSumModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(10, 10)
        self.embeddingbag = nn.EmbeddingBag(10, 10)

    def forward(self, x):
        y1 = self.embedding(x)
        y2 = self.embeddingbag(x)
        return y1 + y2


class EmbeddingCatLinearModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.embedding1 = nn.Embedding(10, 10)
        self.embedding2 = nn.Embedding(10, 10)
        self.linear = nn.Linear(10, 1)

    def forward(self, x):
        y1 = self.embedding1(x)
        y2 = self.embedding2(x)
        z = torch.cat([y1, y2])
        return self.linear(z)


class MultiOutputSameTensorModel(torch.nn.Module):

    def forward(self, x):
        return x, x * x, x


class AddTwoConv(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 2, 2, -1, -2)
        self.conv2 = create_conv(1, 2, 2, -1, -2)

    def forward(self, x):
        return self.conv1(x) + self.conv2(x)


class MatMulDivConv(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 2, 2, 2)

    def forward(self, x: torch.Tensor, y: torch.Tensor):
        z = torch.matmul(x, y) / 2
        return self.conv(z)


class MMDivConv(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 1, 1, 2)

    def forward(self, x: torch.Tensor, y: torch.Tensor):
        z = torch.mm(x, y) / 2
        z = z.unsqueeze(0)
        z = z.unsqueeze(0)
        return self.conv(z)


class ConvRelu6HSwishHSigmoid(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = create_conv(1, 2, 2, 2)
        self.conv2 = create_conv(2, 2, 2, 2)
        self.relu6 = torch.nn.ReLU6()

    def _hswish(self, x: torch.Tensor) ->torch.Tensor:
        return x * self.relu6(x + 3) / 6

    def _hsigmoid(self, x: torch.Tensor) ->torch.Tensor:
        return self.relu6(x + 3) / 6

    def forward(self, x: torch.Tensor):
        z = self.conv1(x)
        z = self._hswish(z)
        z = self.conv2(z)
        z = self._hsigmoid(z)
        return z


class ConvGeluGetItem(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(6, 8)
        self.dp = nn.Dropout()
        self.conv1 = nn.Conv1d(8, 8, kernel_size=3, padding=2)

    def forward(self, x):
        x = self.fc1(x)
        x = self.dp(x)
        x1 = x.transpose(2, 1)
        x1 = self.conv1(x1)
        x1 = F.gelu(x1[:, :, :-2])
        return x + x1.transpose(2, 1)


class ConvBNLeakyReLU(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = create_conv(1, 2, 2, 2)
        self.bn = BatchNorm2d(2)

    def forward(self, x: torch.Tensor):
        z = self.conv(x)
        z = self.bn(z)
        z = torch.nn.functional.leaky_relu(z)
        return z


class FC_ConstMul(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(6, 6)
        self.dp = nn.Dropout()

    def forward(self, x):
        x = self.dp(x)
        x1 = self.fc1(x)
        x1 = x1 * 2
        return x + x1


class WeightNormedConvModel(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = weight_norm(torch.nn.Conv1d(1, 1, 1))

    def forward(self, x):
        return self.conv(x)


class ModuleOfUser(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones([1]))
        self.conv = torch.nn.Conv2d(1, 1, 1)

    def forward(self, input_):
        x = input_ * self.weight
        x += torch.rand_like(x)
        x = F.conv2d(x, self.conv.weight)
        x = self.conv(x)
        return x


class RegisteredModuleOfUser(ModuleOfUser):
    pass


class TwoConvTestModelWithUserModule(TwoConvTestModel):

    def __init__(self):
        super().__init__()
        self.user_module = ModuleOfUser()
        self.registered_user_module = RegisteredModuleOfUser()

    def forward(self, x):
        x = super().forward(x)
        x = self.user_module(x)
        x = self.registered_user_module(x)
        return x


class InsertionPointTestModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 1, 1, 1)
        self.linear_wts = nn.Parameter(torch.FloatTensor(size=(100, 100)))
        self.conv2 = nn.Conv2d(1, 1, 1, 1)
        self.relu = nn.ReLU()

    def forward(self, input_):
        x = self.conv1(input_)
        x = x.flatten()
        x = nn.functional.linear(x, self.linear_wts)
        x = x.reshape((1, 1, 10, 10))
        x = self.conv2(x)
        x = self.relu(x)
        return x


class TwoConvTestModelWithUniqueFunction(TwoConvTestModel):

    def __init__(self):
        super().__init__()
        self.unique_attr = 'unique_attr'
        self.non_unique_attr = 'model_non_unique_attr'

    def train_step(self):
        pass

    @staticmethod
    def static_func():
        pass


class ModelForIONamingTest(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv = torch.nn.Conv1d(1, 1, 1)
        self.linear = torch.nn.Linear(1, 1)
        self.embedding = torch.nn.Embedding(1, 1)

    def forward(self, conv_input, linear_input, embedding_input):
        return [self.conv(conv_input), {'linear': self.linear(linear_input), 'embedding': self.embedding(embedding_input)}]


class ReferenceDOREFABinarize(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x):
        norm = x.abs().mean()
        sign = (x > 0).type(x.dtype) * 2 - 1
        output_flat = sign * norm
        return output_flat.view_as(x)

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


class ReferenceXNORBinarize(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x):
        norm = x.abs().mean([1, 2, 3], keepdim=True)
        sign = (x > 0).type(x.dtype) * 2 - 1
        output = sign * norm
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        return grad_outputs[0]


class ReferenceWeightBinarizationModule(nn.Module):

    def __init__(self, mode='xnor'):
        super().__init__()
        self.mode = mode
        if self.mode == 'xnor':
            self.binarize = ReferenceXNORBinarize.apply
        elif self.mode == 'dorefa':
            self.binarize = ReferenceDOREFABinarize.apply

    def forward(self, input_):
        return self.binarize(input_)


class ReferenceActivationBinarize(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_, scale, threshold):
        shape = [(1) for s in input_.shape]
        shape[1] = input_.shape[1]
        t = (threshold * scale).view(shape)
        output = (input_ > t).type(input_.dtype) * scale
        ctx.save_for_backward(input_, scale, output)
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        grad_output = grad_outputs[0]
        input_, scale, output = ctx.saved_variables
        mask_lower = (input_ <= scale).type(input_.dtype)
        grad_input = grad_output * (input_ >= 0).type(input_.dtype) * mask_lower
        err = (output - input_) * scale.reciprocal()
        grad_scale = grad_output * (mask_lower * err + (1 - mask_lower))
        grad_scale = grad_scale.sum().view(1)
        grad_threshold = -grad_output * (input_ > 0).type(input_.dtype) * (input_ < scale).type(input_.dtype)
        for idx, _ in enumerate(input_.shape):
            if idx != 1:
                grad_threshold = grad_threshold.sum(idx, keepdim=True)
        return grad_input, grad_scale, grad_threshold


def get_test_scale(num_channels):
    torch.manual_seed(0)
    retval = torch.Tensor(num_channels)
    retval.random_(0, 1)
    return retval


def get_test_threshold(input_shape):
    torch.manual_seed(0)
    threshold_shape = get_per_channel_scale_shape(input_shape, is_weights=False)
    retval = torch.Tensor(torch.zeros(threshold_shape))
    retval.random_(-10, 10)
    return retval


class ReferenceActivationBinarizationModule(nn.Module):

    def __init__(self, input_shape):
        super().__init__()
        self.input_shape = input_shape
        self.scale = torch.nn.Parameter(get_test_scale(num_channels=1))
        self.threshold = torch.nn.Parameter(get_test_threshold(input_shape))

    def forward(self, input_):
        return ReferenceActivationBinarize.apply(input_, self.scale, self.threshold)


def fp32_accum_wrapper(func):

    def wrapper(tensor_to_sum, ret_tensor):
        half = tensor_to_sum.dtype == np.float16
        if half:
            tensor_to_sum = tensor_to_sum.astype(np.float)
        retval = func(tensor_to_sum, ret_tensor)
        if half:
            retval = retval.astype(np.float16)
        return retval
    return wrapper


@fp32_accum_wrapper
def sum_like(tensor_to_sum, ref_tensor):
    """Warning: may modify tensor_to_sum"""
    if ref_tensor.size == 1:
        return tensor_to_sum.sum()
    for dim, size in enumerate(ref_tensor.shape):
        if size == 1:
            if isinstance(tensor_to_sum, np.ndarray):
                tensor_to_sum = tensor_to_sum.sum(dim, keepdims=True)
            else:
                tensor_to_sum = tensor_to_sum.sum(dim, keepdim=True)
    return tensor_to_sum


class ReferenceQuantizeSymmetric(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input_, scale, bits):
        level_high = scale.new_tensor([2 ** (bits - 1) - 1])
        level_low = scale.new_tensor([-(level_high + 1)])
        s = level_high / scale
        output = input_ * s
        output = output.clamp(min=level_low[0], max=level_high[0])
        output = output.round()
        output = output / s
        ctx.save_for_backward(input_, scale, output)
        ctx.level_high = level_high
        ctx.level_low = level_low
        return output

    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) ->Any:
        grad_output = grad_outputs[0]
        input_, scale, output = ctx.saved_tensors
        level_high = ctx.level_high
        level_low = ctx.level_low
        alpha = float(level_low) / float(level_high)
        mask_hi = (input_ > scale).type(input_.dtype)
        mask_lo = (input_ < scale * alpha).type(input_.dtype)
        mask_in = 1 - mask_hi - mask_lo
        val_grad_out = mask_hi + alpha * mask_lo
        err = (output - input_) * scale.reciprocal()
        grad_scale = grad_output * (err * mask_in + val_grad_out)
        grad_scale = sum_like(grad_scale, scale)
        grad_input = grad_output * mask_in
        return grad_input, grad_scale, None


class ReferenceQuantize(nn.Module):

    def __init__(self, num_bits=8, input_shape=None, is_weights=True, per_channel=False):
        super().__init__()
        self.input_shape = input_shape
        self.is_weights = is_weights
        scale_shape = [1]
        if per_channel:
            scale_shape = get_per_channel_scale_shape(self.input_shape, self.is_weights)
        self.scale = nn.Parameter(torch.ones(scale_shape))
        self.num_bits = num_bits
        self.level_high = 2 ** (self.num_bits - 1) - 1
        self.level_low = -(self.level_high + 1)
        self.quantize = ReferenceQuantizeSymmetric.apply

    def get_scale(self):
        return self.scale

    def forward(self, input_):
        return self.quantize(input_, self.scale, self.num_bits)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ActivationBinarizationScaleThreshold,
     lambda: ([], {'input_shape': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ActivationBinarizer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Actor,
     lambda: ([], {'nb_states': 4, 'nb_actions': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (AddTwoConv,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ArangeModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BaseBinarizer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BaseOp,
     lambda: ([], {'op': _mock_layer()}),
     lambda: ([], {'input': torch.rand([4, 4])}),
     False),
    (BasicConvTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (BasicLinearTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BasicTestModelWithTwoInputOutput,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (BasicTestSuperNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (BranchingModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (CatModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Chomp1d,
     lambda: ([], {'chomp_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ChunkConcat,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Classifier,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ContainersOutputsModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ConvBN,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (ConvBNLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (ConvBNLeakyReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ConvBNReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (ConvRelu6HSwishHSigmoid,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (Critic,
     lambda: ([], {'nb_states': 4, 'nb_actions': 4}),
     lambda: ([(torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]))], {}),
     False),
    (CustomConvModule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (DOREFABinarize,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DepthWiseConvTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (DiffConvsModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (DoubleCatModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (ENet,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (EightConvTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ElasticInputWidthBatchNormOp,
     lambda: ([], {'max_width': 4, 'node_name': 4}),
     lambda: ([], {}),
     False),
    (ElasticInputWidthConvOp,
     lambda: ([], {'max_width': 4, 'node_name': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ElasticInputWidthDWConvOp,
     lambda: ([], {'max_width': 4, 'node_name': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ElasticInputWidthLinearOp,
     lambda: ([], {'max_width': 4, 'node_name': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (EltwiseCombinationModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (EltwiseQuantizerLinkingTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (FilterPruningMask,
     lambda: ([], {'size': 4, 'node_name': 4}),
     lambda: ([], {}),
     False),
    (Fire,
     lambda: ([], {'inplanes': 4, 'squeeze_planes': 4, 'expand1x1_planes': 4, 'expand3x3_planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GatherModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ICNetBackbone,
     lambda: ([], {'in_channels': 4}),
     lambda: ([], {}),
     False),
    (Inception,
     lambda: ([], {'in_planes': 4, 'n1x1': 4, 'n3x3red': 4, 'n3x3': 4, 'n5x5red': 4, 'n5x5': 4, 'pool_planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (L2Norm,
     lambda: ([], {'n_channels': 4, 'scale': 1.0, 'eps': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MMDivConv,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (MagnitudeTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (MaskedFillModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MobilenetV3BlockSEReshape,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (MockModel,
     lambda: ([], {'stub_forward': _mock_layer()}),
     lambda: ([], {'input': torch.rand([4, 4])}),
     False),
    (ModelForHWConfigTest,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 2, 64, 64])], {}),
     True),
    (ModelWithBranches,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 2, 64, 64])], {}),
     True),
    (ModelWithChangedTrain,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ModelWithCustomConvModules,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ModelWithDummyParameter,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MultiBranchesModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (MultiOutputSameTensorModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MultipleForwardModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (MultipleSplitConcatModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (PTCompressionLoss,
     lambda: ([], {}),
     lambda: ([], {}),
     True),
    (PartlyNonDifferentialOutputsModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (PruningTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (PruningTestModelWrongDims,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (PruningTestModelWrongDimsElementwise,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (QuantizeOutputsTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (ReferenceActivationBinarizationModule,
     lambda: ([], {'input_shape': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ReferenceQuantize,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ReferenceWeightBinarizationModule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ResNetBlock,
     lambda: ([], {'in_channels': 4, 'reduce_channels': 4, 'increase_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ReshapeModel,
     lambda: ([], {}),
     lambda: ([torch.rand([1])], {}),
     True),
    (SELayerWithReshape,
     lambda: ([], {'channel': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SELayerWithReshapeAndLinear,
     lambda: ([], {'channel': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SELayerWithReshapeAndLinearAndMean,
     lambda: ([], {'channel': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SepConv,
     lambda: ([], {'in_planes': 4, 'out_planes': 4, 'kernel_size': 4, 'stride': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SharedLayersModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (ShuffleBlock,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ShuffleNetUnitModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     False),
    (ShuffleNetUnitModelDW,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (SingleCatModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (SingleConv2dIdentityModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (SmallBlock,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SplitBlock,
     lambda: ([], {'ratio': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SplitConcatModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (SplitIdentityModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (SplitMaskPropFailModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (SplitPruningInvalidModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (SqueezeNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (TargetCompressionIdxTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (TestShuffleUnit,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'downsample': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ThreeConvModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     False),
    (TwoBranchesAfterInput,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TwoBranchesBeforeInput,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TwoConvAddConvTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (TwoConvModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (TwoConvTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (TwoConvTestModelWithUniqueFunction,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (TwoSequentialConvBNTestModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (UNetConvBlock,
     lambda: ([], {'in_size': 4, 'out_size': 4, 'padding': 4, 'batch_norm': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (UNetLikeModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (WeightBinarizer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (WeightNormedConvModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64])], {}),
     False),
    (XNORBinarize,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
]

class Test_openvinotoolkit_nncf(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

    def test_041(self):
        self._check(*TESTCASES[41])

    def test_042(self):
        self._check(*TESTCASES[42])

    def test_043(self):
        self._check(*TESTCASES[43])

    def test_044(self):
        self._check(*TESTCASES[44])

    def test_045(self):
        self._check(*TESTCASES[45])

    def test_046(self):
        self._check(*TESTCASES[46])

    def test_047(self):
        self._check(*TESTCASES[47])

    def test_048(self):
        self._check(*TESTCASES[48])

    def test_049(self):
        self._check(*TESTCASES[49])

    def test_050(self):
        self._check(*TESTCASES[50])

    def test_051(self):
        self._check(*TESTCASES[51])

    def test_052(self):
        self._check(*TESTCASES[52])

    def test_053(self):
        self._check(*TESTCASES[53])

    def test_054(self):
        self._check(*TESTCASES[54])

    def test_055(self):
        self._check(*TESTCASES[55])

    def test_056(self):
        self._check(*TESTCASES[56])

    def test_057(self):
        self._check(*TESTCASES[57])

    def test_058(self):
        self._check(*TESTCASES[58])

    def test_059(self):
        self._check(*TESTCASES[59])

    def test_060(self):
        self._check(*TESTCASES[60])

    def test_061(self):
        self._check(*TESTCASES[61])

    def test_062(self):
        self._check(*TESTCASES[62])

    def test_063(self):
        self._check(*TESTCASES[63])

    def test_064(self):
        self._check(*TESTCASES[64])

    def test_065(self):
        self._check(*TESTCASES[65])

    def test_066(self):
        self._check(*TESTCASES[66])

    def test_067(self):
        self._check(*TESTCASES[67])

    def test_068(self):
        self._check(*TESTCASES[68])

    def test_069(self):
        self._check(*TESTCASES[69])

    def test_070(self):
        self._check(*TESTCASES[70])

    def test_071(self):
        self._check(*TESTCASES[71])

    def test_072(self):
        self._check(*TESTCASES[72])

    def test_073(self):
        self._check(*TESTCASES[73])

    def test_074(self):
        self._check(*TESTCASES[74])

    def test_075(self):
        self._check(*TESTCASES[75])

    def test_076(self):
        self._check(*TESTCASES[76])

    def test_077(self):
        self._check(*TESTCASES[77])

    def test_078(self):
        self._check(*TESTCASES[78])

    def test_079(self):
        self._check(*TESTCASES[79])

    def test_080(self):
        self._check(*TESTCASES[80])

    def test_081(self):
        self._check(*TESTCASES[81])

    def test_082(self):
        self._check(*TESTCASES[82])

    def test_083(self):
        self._check(*TESTCASES[83])

    def test_084(self):
        self._check(*TESTCASES[84])

    def test_085(self):
        self._check(*TESTCASES[85])

    def test_086(self):
        self._check(*TESTCASES[86])

    def test_087(self):
        self._check(*TESTCASES[87])

    def test_088(self):
        self._check(*TESTCASES[88])

    def test_089(self):
        self._check(*TESTCASES[89])

    def test_090(self):
        self._check(*TESTCASES[90])

    def test_091(self):
        self._check(*TESTCASES[91])

    def test_092(self):
        self._check(*TESTCASES[92])

    def test_093(self):
        self._check(*TESTCASES[93])

    def test_094(self):
        self._check(*TESTCASES[94])

    def test_095(self):
        self._check(*TESTCASES[95])

    def test_096(self):
        self._check(*TESTCASES[96])

    def test_097(self):
        self._check(*TESTCASES[97])

    def test_098(self):
        self._check(*TESTCASES[98])


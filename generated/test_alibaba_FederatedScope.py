import sys
_module = sys.modules[__name__]
del sys
demo = _module
base_fed_surrogate_benchmark = _module
base_fed_tabular_benchmark = _module
femnist_surrogate_benchmark = _module
femnist_tabular_benchmark = _module
example = _module
fedhpobench = _module
benchmarks = _module
base_benchmark = _module
raw_benchmark = _module
surrogate_benchmark = _module
tabular_benchmark = _module
config = _module
optimizers = _module
dehb_optimizer = _module
grid_search = _module
hpbandster_optimizer = _module
optuna_optimizer = _module
smac_optimizer = _module
utils = _module
cost_model = _module
draw = _module
monitor = _module
runner = _module
surrogate_dataloader = _module
tabular_dataloader = _module
util = _module
get_total_run_time = _module
render_paper_res = _module
repeat_best_exp = _module
wandb_to_latex_res = _module
conf = _module
jupyter_server_config = _module
federatedscope = _module
attack = _module
MIA_get_target_data = _module
auxiliary = _module
attack_trainer_builder = _module
backdoor_utils = _module
create_edgeset = _module
poisoning_data = _module
utils = _module
models = _module
gan_based_model = _module
vision = _module
GAN_based_attack = _module
privacy_attacks = _module
passive_PIA = _module
reconstruction_opt = _module
GAN_trainer = _module
MIA_invert_gradient_trainer = _module
PIA_trainer = _module
trainer = _module
backdoor_trainer = _module
benign_trainer = _module
worker_as_attacker = _module
active_client = _module
server_attacker = _module
autotune = _module
algos = _module
choice_types = _module
fedex = _module
client = _module
server = _module
hpbandster = _module
run = _module
smac = _module
cl = _module
Cifar10 = _module
dataloader = _module
client = _module
server = _module
utils = _module
NT_xentloss = _module
loss = _module
SimCLR = _module
model = _module
trainer = _module
contrib = _module
configs = _module
data = _module
mini_graph_dt = _module
example = _module
metrics = _module
poison_acc = _module
example = _module
fedsam_convnet = _module
resnet = _module
optimizer = _module
example = _module
scheduler = _module
example = _module
splitter = _module
fedsam_cifar10_splitter = _module
local_entropy = _module
local_entropy_trainer = _module
sam = _module
sam_trainer = _module
torch_example = _module
worker = _module
core = _module
aggregators = _module
aggregator = _module
asyn_clients_avg_aggregator = _module
clients_avg_aggregator = _module
fedopt_aggregator = _module
server_clients_interpolate_aggregator = _module
ReIterator = _module
auxiliaries = _module
aggregator_builder = _module
criterion_builder = _module
data_builder = _module
dataloader_builder = _module
decorators = _module
feat_engr_builder = _module
logging = _module
metric_builder = _module
model_builder = _module
optimizer_builder = _module
regularizer_builder = _module
runner_builder = _module
sampler_builder = _module
scheduler_builder = _module
splitter_builder = _module
trainer_builder = _module
transform_builder = _module
utils = _module
worker_builder = _module
cmd_args = _module
communication = _module
cfg_asyn = _module
cfg_attack = _module
cfg_data = _module
cfg_differential_privacy = _module
cfg_evaluation = _module
cfg_fl_algo = _module
cfg_fl_setting = _module
cfg_hpo = _module
cfg_model = _module
cfg_training = _module
config = _module
constants = _module
yacs_config = _module
base_data = _module
base_translator = _module
dummy_translator = _module
utils = _module
wrap_dataset = _module
feature = _module
hfl = _module
preprocess = _module
log_transform = _module
min_max_norm = _module
quantile_binning = _module
standard = _module
uniform_binning = _module
selection = _module
correlation_filter = _module
variance_filter = _module
vfl = _module
instance_norm = _module
standardization = _module
iv_filter = _module
fed_runner = _module
gRPC_server = _module
gpu_manager = _module
lr = _module
message = _module
mlp = _module
monitors = _module
early_stopper = _module
metric_calculator = _module
monitor = _module
optimizer = _module
proto = _module
gRPC_comm_manager_pb2 = _module
gRPC_comm_manager_pb2_grpc = _module
regularizer = _module
proximal_regularizer = _module
sampler = _module
secret_sharing = _module
secret_sharing = _module
secure = _module
encrypt = _module
dummy_encrypt = _module
splitters = _module
base_splitter = _module
generic = _module
iid_splitter = _module
lda_splitter = _module
graph = _module
analyzer = _module
louvain_splitter = _module
randchunk_splitter = _module
random_splitter = _module
reltype_splitter = _module
scaffold_lda_splitter = _module
scaffold_splitter = _module
strategy = _module
trainers = _module
base_trainer = _module
context = _module
enums = _module
tf_trainer = _module
torch_trainer = _module
trainer_Ditto = _module
trainer_FedEM = _module
trainer_fedprox = _module
trainer_multi_model = _module
trainer_nbafl = _module
trainer_pFedMe = _module
utils = _module
workers = _module
base_client = _module
base_server = _module
base_worker = _module
client = _module
server = _module
cross_backends = _module
tf_aggregator = _module
tf_lr = _module
cv = _module
dataset = _module
leaf = _module
leaf_cv = _module
celeba_preprocess = _module
cnn = _module
gfl = _module
baseline = _module
dataloader_graph = _module
dataloader_link = _module
dataloader_node = _module
cSBM_dataset = _module
cikm_cup = _module
dblp_new = _module
analyzer_fed_graph = _module
kg = _module
dblp_related = _module
recsys = _module
utils = _module
fedsageplus = _module
trainer = _module
utils = _module
worker = _module
flitplus = _module
trainer = _module
gcflplus = _module
utils = _module
worker = _module
greedy_loss = _module
vat = _module
fedsageplus = _module
gat = _module
gcn = _module
gin = _module
gpr = _module
graph_level = _module
link_level = _module
mpnn = _module
sage = _module
graphtrainer = _module
linktrainer = _module
nodetrainer = _module
hpo = _module
main = _module
mf = _module
movielens = _module
netflix = _module
model = _module
trainer = _module
trainer_sgdmf = _module
nlp = _module
leaf_nlp = _module
leaf_synthetic = _module
leaf_twitter = _module
get_embs = _module
character_loss = _module
rnn = _module
organizer = _module
cfg_client = _module
cfg_server = _module
register = _module
tabular = _module
quadratic = _module
toy = _module
quadratic = _module
Paillier = _module
abstract_paillier = _module
vertical_fl = _module
abalone = _module
adult = _module
blog = _module
credit = _module
vertical_client = _module
vertical_server = _module
xgb_base = _module
Feature_sort_base = _module
Feature_sort_by_bin = _module
Loss_function = _module
Test_base = _module
Tree = _module
XGBClient = _module
XGBServer = _module
gen_data = _module
parse_exp_results_wandb = _module
setup = _module
test_CRA_gan_attack = _module
test_MIA_gradient_ascent = _module
test_PIA_toy = _module
test_asyn_cifar10 = _module
test_backdoor_attack = _module
test_ditto = _module
test_efficient_simulation = _module
test_external_dataset = _module
test_fedem = _module
test_fedopt = _module
test_fedprox = _module
test_fedsageplus = _module
test_femnist = _module
test_finetune_lr = _module
test_global_train_lr = _module
test_graph_node_trainer = _module
test_local_train_lr = _module
test_mf = _module
test_nbafl = _module
test_optimizer = _module
test_pfedme = _module
test_rec_IG_opt_attack = _module
test_rec_opt_attack = _module
test_simclr_cifar10 = _module
test_toy_lr = _module
test_trainer_property = _module
test_unseen_clients_lr = _module
test_vertical_fl = _module
test_xgb = _module
test_yaml = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


import torch.utils.data as data


import torch.nn as nn


import torch.nn.functional as F


import torchvision


import torchvision.transforms as transforms


import random


import numpy as np


import time


import matplotlib


from matplotlib import image as mlt


from torchvision.datasets import MNIST


from torchvision.datasets import EMNIST


from torchvision.datasets import CIFAR10


from torchvision.datasets import DatasetFolder


from torchvision import transforms


import logging


import copy


from torch.utils.data import DataLoader


import matplotlib.pyplot as plt


from copy import deepcopy


from torch.autograd import grad


from torchvision import models


from torchvision import datasets


from typing import Type


from torch.nn.utils import parameters_to_vector


from torch.nn.utils import vector_to_parameters


import math


import torch.backends.cudnn as cudnn


import torchvision.transforms as T


import torchvision.transforms.functional as TF


from torch.utils.data import Dataset


from torchvision.datasets import CIFAR100


from math import pi


from math import cos


from math import e


from collections import OrderedDict


import re


from typing import Callable


from collections import defaultdict


from abc import ABC


from abc import abstractmethod


from scipy.sparse.csc import csc_matrix


import inspect


from random import shuffle


import abc


from collections import deque


from torch.nn import Linear


from torch.nn import ModuleList


from torch.nn import BatchNorm1d


from torch.nn import Identity


from typing import Optional


from typing import Union


from typing import List


from typing import Set


from scipy.special import softmax


from sklearn.metrics import roc_auc_score


from sklearn.metrics import average_precision_score


from sklearn.metrics import f1_score


from typing import Dict


import collections


from torch.nn.functional import softmax as f_softmax


from sklearn.model_selection import train_test_split


from torch.nn import Module


from torch.nn import Sequential


from torch.nn import Conv2d


from torch.nn import BatchNorm2d


from torch.nn import Flatten


from torch.nn import MaxPool2d


from torch.nn import ReLU


from sklearn.feature_extraction.text import CountVectorizer


from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords


import scipy.sparse as sp


from torch.nn import Parameter


from torch.nn import GRU


import numpy


from sklearn.utils import shuffle


class GeneratorFemnist(nn.Module):
    """
    The generator for Femnist dataset
    """

    def __init__(self, noise_dim=100):
        super(GeneratorFemnist, self).__init__()
        module_list = []
        module_list.append(nn.Linear(in_features=noise_dim, out_features=4 * 4 * 256, bias=False))
        module_list.append(nn.BatchNorm1d(num_features=4 * 4 * 256))
        module_list.append(nn.ReLU())
        self.body1 = nn.Sequential(*module_list)
        module_list = []
        module_list.append(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(3, 3), stride=(1, 1), bias=False))
        module_list.append(nn.BatchNorm2d(128))
        module_list.append(nn.ReLU())
        self.body2 = nn.Sequential(*module_list)
        module_list = []
        module_list.append(nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(2, 2), bias=False))
        module_list.append(nn.BatchNorm2d(64))
        module_list.append(nn.ReLU())
        self.body3 = nn.Sequential(*module_list)
        module_list = []
        module_list.append(nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=(4, 4), stride=(2, 2), bias=False))
        module_list.append(nn.BatchNorm2d(1))
        module_list.append(nn.Tanh())
        self.body4 = nn.Sequential(*module_list)

    def forward(self, x):
        tmp1 = self.body1(x).view(-1, 256, 4, 4)
        assert tmp1.size()[1:4] == (256, 4, 4)
        tmp2 = self.body2(tmp1)
        assert tmp2.size()[1:4] == (128, 6, 6)
        tmp3 = self.body3(tmp2)
        assert tmp3.size()[1:4] == (64, 13, 13)
        tmp4 = self.body4(tmp3)
        assert tmp4.size()[1:4] == (1, 28, 28)
        return tmp4


class LeNet(nn.Module):

    def __init__(self):
        super(LeNet, self).__init__()
        act = nn.Sigmoid
        self.body = nn.Sequential(nn.Conv2d(3, 12, kernel_size=5, padding=5 // 2, stride=2), act(), nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2), act(), nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1), act())
        self.fc = nn.Sequential(nn.Linear(768, 100))

    def forward(self, x):
        out = self.body(x)
        out = out.view(out.size(0), -1)
        out = self.fc(out)
        return out


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion * planes))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion * planes)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion * planes))

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class global_NT_xentloss(nn.Module):
    """
    NT_xentloss definition adapted from https://github.com/PatrickHua/SimSiam
    Arguments:
        z1 (torch.tensor): the embedding of model .
        z2 (torch.tensor): the embedding of model using another augmentation.
    returns:
        loss: the NT_xentloss loss for this batch data
    :rtype:
        torch.FloatTensor
    """

    def __init__(self, temperature=0.1, device=torch.device('cpu')):
        super(global_NT_xentloss, self).__init__()
        self.temperature = temperature
        self.device = device

    def forward(self, z1, z2, others_z2=[]):
        N, Z = z1.shape
        z1, z2 = z1, z2
        representations = torch.cat([z1, z2], dim=0)
        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=-1)
        l_pos = torch.diag(similarity_matrix, N)
        r_pos = torch.diag(similarity_matrix, -N)
        positives = torch.cat([l_pos, r_pos]).view(2 * N, 1)
        diag = torch.eye(2 * N, dtype=torch.bool, device=self.device)
        diag[N:, :N] = diag[:N, N:] = diag[:N, :N]
        negatives = similarity_matrix[~diag].view(2 * N, -1)
        if len(others_z2) != 0:
            for z2_ in others_z2:
                z2_ = z2_.detach()
                N2, Z2 = z2_.shape
                representations = torch.cat([z1, z2_], dim=0)
                similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=-1)
                mask = torch.zeros_like(similarity_matrix, dtype=torch.bool, device=self.device)
                mask[N:, :N] = True
                mask[:N, N:] = True
                negatives_other = similarity_matrix[mask].view(2 * N, -1)
                negatives = torch.cat([negatives, negatives_other], dim=1)
        logits = torch.cat([positives, negatives], dim=1) / self.temperature
        labels = torch.zeros(2 * N, dtype=torch.int64, device=self.device)
        loss = F.cross_entropy(logits, labels, reduction='sum') / (2 * N)
        return loss


class NT_xentloss(nn.Module):
    """
    NT_xentloss definition adapted from https://github.com/PatrickHua/SimSiam
    Arguments:
        z1 (torch.tensor): the embedding of model .
        z2 (torch.tensor): the embedding of model using another augmentation.
    returns:
        loss: the NT_xentloss loss for this batch data
    :rtype:
        torch.FloatTensor
    """

    def __init__(self, temperature=0.1):
        super(NT_xentloss, self).__init__()
        self.temperature = temperature

    def forward(self, z1, z2):
        N, Z = z1.shape
        device = z1.device
        representations = torch.cat([z1, z2], dim=0)
        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=-1)
        l_pos = torch.diag(similarity_matrix, N)
        r_pos = torch.diag(similarity_matrix, -N)
        positives = torch.cat([l_pos, r_pos]).view(2 * N, 1)
        diag = torch.eye(2 * N, dtype=torch.bool, device=device)
        diag[N:, :N] = diag[:N, N:] = diag[:N, :N]
        negatives = similarity_matrix[~diag].view(2 * N, -1)
        logits = torch.cat([positives, negatives], dim=1) / self.temperature
        labels = torch.zeros(2 * N, device=device, dtype=torch.int64)
        loss = F.cross_entropy(logits, labels, reduction='sum') / (2 * N)
        return loss


class ResNet_basic(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10, cfg=None):
        super(ResNet_basic, self).__init__()
        self.train_sup = num_classes > 0
        self.in_planes = 16
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16, affine=True)
        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)
        self.output_dim = 512 * block.expansion
        if self.train_sup:
            self.linear = nn.Linear(64 * block.expansion, num_classes, bias=True)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.adaptive_avg_pool2d(out, (1, 1))
        out = out.view(out.size(0), -1)
        if self.train_sup:
            out = self.linear(out)
        return out


class projection_MLP_simclr(nn.Module):

    def __init__(self, in_dim, hidden_dim=512, out_dim=512):
        super(projection_MLP_simclr, self).__init__()
        self.layer1 = nn.Linear(in_dim, hidden_dim, bias=False)
        self.layer1_bn = nn.BatchNorm1d(hidden_dim, affine=True)
        self.layer2 = nn.Linear(hidden_dim, out_dim)
        self.layer2_bn = nn.BatchNorm1d(out_dim, affine=False)

    def forward(self, x):
        x = F.relu(self.layer1_bn(self.layer1(x)))
        x = self.layer2_bn(self.layer2(x))
        return x


def ResNet18():
    return ResNet(BasicBlock, [2, 2, 2, 2])


def ResNet34():
    return ResNet(BasicBlock, [3, 4, 6, 3])


def create_backbone(name, num_classes=10, block='BasicBlock'):
    if name == 'res18':
        net = ResNet18(num_classes=num_classes, block=block)
    elif name == 'res34':
        net = ResNet34(num_classes=num_classes, block=block)
    return net


class simclr(nn.Module):

    def __init__(self, bbone_arch):
        super(simclr, self).__init__()
        self.register_buffer('rounds_done', torch.zeros(1))
        self.backbone = create_backbone(bbone_arch, num_classes=0)
        self.projector = projection_MLP_simclr(self.backbone.output_dim, hidden_dim=512, out_dim=512)

    def forward(self, x1, x2, x3=None, deg_labels=None):
        z1, z2 = self.projector(self.backbone(x1)), self.projector(self.backbone(x2))
        return z1, z2


class simclr_linearprob(nn.Module):

    def __init__(self, bbone_arch, num_classes=10):
        super(simclr_linearprob, self).__init__()
        self.register_buffer('rounds_done', torch.zeros(1))
        self.backbone = create_backbone(bbone_arch, num_classes=0)
        self.linear = nn.Linear(512, num_classes, bias=True)

    def forward(self, x):
        with torch.no_grad():
            out = self.backbone(x)
        out = self.linear(out)
        return out


class simclr_supervised(nn.Module):

    def __init__(self, bbone_arch, num_classes=10):
        super(simclr_supervised, self).__init__()
        self.register_buffer('rounds_done', torch.zeros(1))
        self.backbone = create_backbone(bbone_arch, num_classes=0)
        self.linear = nn.Linear(512, num_classes, bias=True)

    def forward(self, x):
        out = self.backbone(x)
        out = self.linear(out)
        return out


class Conv2Model(nn.Module):

    def __init__(self, num_classes):
        super(Conv2Model, self).__init__()
        self.num_classes = num_classes
        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5), nn.ReLU(), nn.MaxPool2d(kernel_size=2))
        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5), nn.ReLU(), nn.MaxPool2d(kernel_size=2))
        self.classifier = nn.Sequential(nn.Linear(64 * 5 * 5, 384), nn.ReLU(), nn.Linear(384, 192), nn.ReLU(), nn.Linear(192, self.num_classes))
        self.size = self.model_size()

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = torch.reshape(x, (x.shape[0], -1))
        x = self.classifier(x)
        return x

    def model_size(self):
        tot_size = 0
        for param in self.parameters():
            tot_size += param.size()[0]
        return tot_size


class PreActBlock(nn.Module):
    """Pre-activation version of the BasicBlock."""
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBlock, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False))

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out += shortcut
        return out


class PreActBottleneck(nn.Module):
    """Pre-activation version of the original Bottleneck module."""
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(PreActBottleneck, self).__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False))

    def forward(self, x):
        out = F.relu(self.bn1(x))
        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x
        out = self.conv1(out)
        out = self.conv2(F.relu(self.bn2(out)))
        out = self.conv3(F.relu(self.bn3(out)))
        out += shortcut
        return out


class PreActResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super(PreActResNet, self).__init__()
        self.in_planes = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = self.conv1(x)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


class DummyRegularizer(Module):
    """Dummy regularizer that only returns zero.

    """

    def __init__(self):
        super(DummyRegularizer, self).__init__()

    def forward(self, ctx):
        return 0.0


class LogisticRegression(torch.nn.Module):

    def __init__(self, in_channels, class_num, use_bias=True):
        super(LogisticRegression, self).__init__()
        self.fc = torch.nn.Linear(in_channels, class_num, bias=use_bias)

    def forward(self, x):
        return self.fc(x)


class MLP(torch.nn.Module):
    """
    Multilayer Perceptron
    """

    def __init__(self, channel_list, dropout=0.0, batch_norm=True, relu_first=False):
        super().__init__()
        assert len(channel_list) >= 2
        self.channel_list = channel_list
        self.dropout = dropout
        self.relu_first = relu_first
        self.linears = ModuleList()
        self.norms = ModuleList()
        for in_channel, out_channel in zip(channel_list[:-1], channel_list[1:]):
            self.linears.append(Linear(in_channel, out_channel))
            self.norms.append(BatchNorm1d(out_channel) if batch_norm else Identity())

    def forward(self, x):
        x = self.linears[0](x)
        for layer, norm in zip(self.linears[1:], self.norms[:-1]):
            if self.relu_first:
                x = F.relu(x)
            x = norm(x)
            if not self.relu_first:
                x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = layer.forward(x)
        return x


class ProximalRegularizer(Module):
    """Returns the norm of the specific weight update.

        Arguments:
            p (int): The order of norm.
            tensor_before: The original matrix or vector
            tensor_after: The updated matrix or vector

        Returns:
            Tensor: the norm of the given udpate.
    """

    def __init__(self):
        super(ProximalRegularizer, self).__init__()

    def forward(self, ctx, p=2):
        norm = 0.0
        for w_init, w in zip(ctx.weight_init, ctx.model.parameters()):
            norm += torch.pow(torch.norm(w - w_init, p), p)
        return norm * 1.0 / float(p)


class ConvNet2(Module):

    def __init__(self, in_channels, h=32, w=32, hidden=2048, class_num=10, use_bn=True, dropout=0.0):
        super(ConvNet2, self).__init__()
        self.conv1 = Conv2d(in_channels, 32, 5, padding=2)
        self.conv2 = Conv2d(32, 64, 5, padding=2)
        self.use_bn = use_bn
        if use_bn:
            self.bn1 = BatchNorm2d(32)
            self.bn2 = BatchNorm2d(64)
        self.fc1 = Linear(h // 2 // 2 * (w // 2 // 2) * 64, hidden)
        self.fc2 = Linear(hidden, class_num)
        self.relu = ReLU(inplace=True)
        self.maxpool = MaxPool2d(2)
        self.dropout = dropout

    def forward(self, x):
        x = self.bn1(self.conv1(x)) if self.use_bn else self.conv1(x)
        x = self.maxpool(self.relu(x))
        x = self.bn2(self.conv2(x)) if self.use_bn else self.conv2(x)
        x = self.maxpool(self.relu(x))
        x = Flatten()(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc2(x)
        return x


class ConvNet5(Module):

    def __init__(self, in_channels, h=32, w=32, hidden=2048, class_num=10, dropout=0.0):
        super(ConvNet5, self).__init__()
        self.conv1 = Conv2d(in_channels, 32, 5, padding=2)
        self.bn1 = BatchNorm2d(32)
        self.conv2 = Conv2d(32, 64, 5, padding=2)
        self.bn2 = BatchNorm2d(64)
        self.conv3 = Conv2d(64, 64, 5, padding=2)
        self.bn3 = BatchNorm2d(64)
        self.conv4 = Conv2d(64, 128, 5, padding=2)
        self.bn4 = BatchNorm2d(128)
        self.conv5 = Conv2d(128, 128, 5, padding=2)
        self.bn5 = BatchNorm2d(128)
        self.relu = ReLU(inplace=True)
        self.maxpool = MaxPool2d(2)
        self.fc1 = Linear(h // 2 // 2 // 2 // 2 // 2 * (w // 2 // 2 // 2 // 2 // 2) * 128, hidden)
        self.fc2 = Linear(hidden, class_num)
        self.dropout = dropout

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn3(self.conv3(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn4(self.conv4(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn5(self.conv5(x)))
        x = self.maxpool(x)
        x = Flatten()(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc2(x)
        return x


class VGG11(Module):

    def __init__(self, in_channels, h=32, w=32, hidden=128, class_num=10, dropout=0.0):
        super(VGG11, self).__init__()
        self.conv1 = Conv2d(in_channels, 64, 3, padding=1)
        self.bn1 = BatchNorm2d(64)
        self.conv2 = Conv2d(64, 128, 3, padding=1)
        self.bn2 = BatchNorm2d(128)
        self.conv3 = Conv2d(128, 256, 3, padding=1)
        self.bn3 = BatchNorm2d(256)
        self.conv4 = Conv2d(256, 256, 3, padding=1)
        self.bn4 = BatchNorm2d(256)
        self.conv5 = Conv2d(256, 512, 3, padding=1)
        self.bn5 = BatchNorm2d(512)
        self.conv6 = Conv2d(512, 512, 3, padding=1)
        self.bn6 = BatchNorm2d(512)
        self.conv7 = Conv2d(512, 512, 3, padding=1)
        self.bn7 = BatchNorm2d(512)
        self.conv8 = Conv2d(512, 512, 3, padding=1)
        self.bn8 = BatchNorm2d(512)
        self.relu = ReLU(inplace=True)
        self.maxpool = MaxPool2d(2)
        self.fc1 = Linear(h // 2 // 2 // 2 // 2 // 2 * (w // 2 // 2 // 2 // 2 // 2) * 512, hidden)
        self.fc2 = Linear(hidden, hidden)
        self.fc3 = Linear(hidden, class_num)
        self.dropout = dropout

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn2(self.conv2(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn3(self.conv3(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn4(self.conv4(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn5(self.conv5(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn6(self.conv6(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn7(self.conv7(x)))
        x = self.maxpool(x)
        x = self.relu(self.bn8(self.conv8(x)))
        x = self.maxpool(x)
        x = Flatten()(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.relu(self.fc2(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc3(x)
        return x


def _l2_normalize(d):
    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))
    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-08
    return d


class VATLoss(nn.Module):

    def __init__(self, xi=0.001, eps=2.5, ip=1):
        """VAT loss
        Source: https://github.com/lyakaap/VAT-pytorch

        Arguments:
            xi: hyperparameter of VAT in Eq.9, default: 0.0001
            eps: hyperparameter of VAT in Eq.9, default: 2.5
            ip: iteration times of computing adv noise

        Returns:
            loss : the VAT Loss

        """
        super(VATLoss, self).__init__()
        self.xi = xi
        self.eps = eps
        self.ip = ip

    def forward(self, model, graph, criterion):
        pred = model(graph)
        if criterion._get_name() == 'CrossEntropyLoss':
            pred = torch.max(pred, dim=1).indices.long().view(-1)
        nodefea = graph.x
        dn = torch.rand(nodefea.shape).sub(0.5)
        dn = _l2_normalize(dn)
        with _disable_tracking_bn_stats(model):
            with torch.enable_grad():
                for _ in range(self.ip):
                    dn.requires_grad_()
                    x_neighbor = Batch(x=nodefea + self.xi * dn, edge_index=graph.edge_index, y=graph.y, edge_attr=graph.edge_attr, batch=graph.batch)
                    pred_hat = model(x_neighbor)
                    adv_distance = criterion(pred_hat, pred)
                    dn = _l2_normalize(torch.autograd.grad(outputs=adv_distance, inputs=dn, retain_graph=True)[0])
                    model.zero_grad()
                    del x_neighbor, pred_hat, adv_distance
            rn_adv = dn * self.eps
            x_adv = Batch(x=nodefea + rn_adv, edge_index=graph.edge_index, y=graph.y, edge_attr=graph.edge_attr, batch=graph.batch)
            pred_hat = model(x_adv)
            lds = criterion(pred_hat, pred)
        return lds


class Sampling(nn.Module):

    def __init__(self):
        super(Sampling, self).__init__()

    def forward(self, inputs):
        rand = torch.normal(0, 1, size=inputs.shape)
        return inputs + rand


class FeatGenerator(nn.Module):

    def __init__(self, latent_dim, dropout, num_pred, feat_shape):
        super(FeatGenerator, self).__init__()
        self.num_pred = num_pred
        self.feat_shape = feat_shape
        self.dropout = dropout
        self.sample = Sampling()
        self.fc1 = nn.Linear(latent_dim, 256)
        self.fc2 = nn.Linear(256, 2048)
        self.fc_flat = nn.Linear(2048, self.num_pred * self.feat_shape)

    def forward(self, x):
        x = self.sample(x)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.dropout(x, self.dropout, training=self.training)
        x = torch.tanh(self.fc_flat(x))
        return x


class NumPredictor(nn.Module):

    def __init__(self, latent_dim):
        self.latent_dim = latent_dim
        super(NumPredictor, self).__init__()
        self.reg_1 = nn.Linear(self.latent_dim, 1)

    def forward(self, x):
        x = F.relu(self.reg_1(x))
        return x


class MendGraph(nn.Module):

    def __init__(self, num_pred):
        super(MendGraph, self).__init__()
        self.num_pred = num_pred
        for param in self.parameters():
            param.requires_grad = False

    def mend_graph(self, x, edge_index, pred_degree, gen_feats):
        device = gen_feats.device
        num_node, num_feature = x.shape
        new_edges = []
        gen_feats = gen_feats.view(-1, self.num_pred, num_feature)
        if pred_degree.device.type != 'cpu':
            pred_degree = pred_degree.cpu()
        pred_degree = torch._cast_Int(torch.round(pred_degree)).detach()
        x = x.detach()
        fill_feats = torch.vstack((x, gen_feats.view(-1, num_feature)))
        for i in range(num_node):
            for j in range(min(self.num_pred, max(0, pred_degree[i]))):
                new_edges.append(np.asarray([i, num_node + i * self.num_pred + j]))
        new_edges = torch.tensor(np.asarray(new_edges).reshape((-1, 2)), dtype=torch.int64).T
        new_edges = new_edges
        if len(new_edges) > 0:
            fill_edges = torch.hstack((edge_index, new_edges))
        else:
            fill_edges = torch.clone(edge_index)
        return fill_feats, fill_edges

    def forward(self, x, edge_index, pred_missing, gen_feats):
        fill_feats, fill_edges = self.mend_graph(x, edge_index, pred_missing, gen_feats)
        return fill_feats, fill_edges


class SAGE_Net(torch.nn.Module):
    """GraphSAGE model from the "Inductive Representation Learning on
    Large Graphs" paper, in NeurIPS'17

    Source:
    https://github.com/pyg-team/pytorch_geometric/ \\
    blob/master/examples/ogbn_products_sage.py

    Arguments:
        in_channels (int): dimension of input.
        out_channels (int): dimension of output.
        hidden (int): dimension of hidden units, default=64.
        max_depth (int): layers of GNN, default=2.
        dropout (float): dropout ratio, default=.0.

    """

    def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=0.0):
        super(SAGE_Net, self).__init__()
        self.num_layers = max_depth
        self.dropout = dropout
        self.convs = torch.nn.ModuleList()
        self.convs.append(SAGEConv(in_channels, hidden))
        for _ in range(self.num_layers - 2):
            self.convs.append(SAGEConv(hidden, hidden))
        self.convs.append(SAGEConv(hidden, out_channels))

    def reset_parameters(self):
        for conv in self.convs:
            conv.reset_parameters()

    def forward_full(self, data):
        if isinstance(data, Data):
            x, edge_index = data.x, data.edge_index
        elif isinstance(data, tuple):
            x, edge_index = data
        else:
            raise TypeError('Unsupported data type!')
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i + 1 == len(self.convs):
                break
            x = F.relu(F.dropout(x, p=self.dropout, training=self.training))
        return x

    def forward(self, x, edge_index=None, edge_weight=None, adjs=None):
        """
        `train_loader` computes the k-hop neighborhood of a batch of nodes,
        and returns, for each layer, a bipartite graph object, holding the
        bipartite edges `edge_index`, the index `e_id` of the original edges,
        and the size/shape `size` of the bipartite graph.
        Target nodes are also included in the source nodes so that one can
        easily apply skip-connections or add self-loops.

        Arguments:
            x (torch.Tensor or PyG.data or tuple): node features or \\
                full-batch data
            edge_index (torch.Tensor): edge index.
            edge_weight (torch.Tensor): edge weight.
            adjs (List[PyG.loader.neighbor_sampler.EdgeIndex]): \\
                batched edge index
        :returns:
            x: output
        :rtype:
            torch.Tensor
        """
        if isinstance(x, torch.Tensor):
            if edge_index is None:
                for i, (edge_index, _, size) in enumerate(adjs):
                    x_target = x[:size[1]]
                    x = self.convs[i]((x, x_target), edge_index)
                    if i != self.num_layers - 1:
                        x = F.relu(x)
                        x = F.dropout(x, p=self.dropout, training=self.training)
            else:
                for conv in self.convs[:-1]:
                    x = conv(x, edge_index, edge_weight)
                    x = F.relu(x)
                    x = F.dropout(x, p=self.dropout, training=self.training)
                x = self.convs[-1](x, edge_index, edge_weight)
            return x
        elif isinstance(x, Data) or isinstance(x, tuple):
            return self.forward_full(x)
        else:
            raise TypeError

    def inference(self, x_all, subgraph_loader, device):
        """
        Compute representations of nodes layer by layer, using *all*
        available edges. This leads to faster computation in contrast to
        immediately computing the final representations of each batch.

        Arguments:
            x_all (torch.Tensor): all node features
            subgraph_loader (PyG.dataloader): dataloader
            device (str): device
        :returns:
            x_all: output
        """
        total_edges = 0
        for i in range(self.num_layers):
            xs = []
            for batch_size, n_id, adj in subgraph_loader:
                edge_index, _, size = adj
                total_edges += edge_index.size(1)
                x = x_all[n_id]
                x_target = x[:size[1]]
                x = self.convs[i]((x, x_target), edge_index)
                if i != self.num_layers - 1:
                    x = F.relu(x)
                xs.append(x.cpu())
            x_all = torch.cat(xs, dim=0)
        return x_all


class LocalSage_Plus(nn.Module):

    def __init__(self, in_channels, out_channels, hidden, gen_hidden, dropout=0.5, num_pred=5):
        super(LocalSage_Plus, self).__init__()
        self.encoder_model = SAGE_Net(in_channels=in_channels, out_channels=gen_hidden, hidden=hidden, max_depth=2, dropout=dropout)
        self.reg_model = NumPredictor(latent_dim=gen_hidden)
        self.gen = FeatGenerator(latent_dim=gen_hidden, dropout=dropout, num_pred=num_pred, feat_shape=in_channels)
        self.mend_graph = MendGraph(num_pred)
        self.classifier = SAGE_Net(in_channels=in_channels, out_channels=out_channels, hidden=hidden, max_depth=2, dropout=dropout)

    def forward(self, data):
        x = self.encoder_model(data)
        degree = self.reg_model(x)
        gen_feat = self.gen(x)
        mend_feats, mend_edge_index = self.mend_graph(data.x, data.edge_index, degree, gen_feat)
        nc_pred = self.classifier(Data(x=mend_feats, edge_index=mend_edge_index))
        return degree, gen_feat, nc_pred[:data.num_nodes]

    def inference(self, impared_data, raw_data):
        x = self.encoder_model(impared_data)
        degree = self.reg_model(x)
        gen_feat = self.gen(x)
        mend_feats, mend_edge_index = self.mend_graph(raw_data.x, raw_data.edge_index, degree, gen_feat)
        nc_pred = self.classifier(Data(x=mend_feats, edge_index=mend_edge_index))
        return degree, gen_feat, nc_pred[:raw_data.num_nodes]


class FedSage_Plus(nn.Module):

    def __init__(self, local_graph: LocalSage_Plus):
        super(FedSage_Plus, self).__init__()
        self.encoder_model = local_graph.encoder_model
        self.reg_model = local_graph.reg_model
        self.gen = local_graph.gen
        self.mend_graph = local_graph.mend_graph
        self.classifier = local_graph.classifier
        self.encoder_model.requires_grad_(False)
        self.reg_model.requires_grad_(False)
        self.mend_graph.requires_grad_(False)
        self.classifier.requires_grad_(False)

    def forward(self, data):
        x = self.encoder_model(data)
        degree = self.reg_model(x)
        gen_feat = self.gen(x)
        mend_feats, mend_edge_index = self.mend_graph(data.x, data.edge_index, degree, gen_feat)
        nc_pred = self.classifier(Data(x=mend_feats, edge_index=mend_edge_index))
        return degree, gen_feat, nc_pred[:data.num_nodes]


class GAT_Net(torch.nn.Module):
    """GAT model from the "Graph Attention Networks" paper, in ICLR'18

    Arguments:
        in_channels (int): dimension of input.
        out_channels (int): dimension of output.
        hidden (int): dimension of hidden units, default=64.
        max_depth (int): layers of GNN, default=2.
        dropout (float): dropout ratio, default=.0.

    """

    def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=0.0):
        super(GAT_Net, self).__init__()
        self.convs = ModuleList()
        for i in range(max_depth):
            if i == 0:
                self.convs.append(GATConv(in_channels, hidden))
            elif i + 1 == max_depth:
                self.convs.append(GATConv(hidden, out_channels))
            else:
                self.convs.append(GATConv(hidden, hidden))
        self.dropout = dropout

    def reset_parameters(self):
        for m in self.convs:
            m.reset_parameters()

    def forward(self, data):
        if isinstance(data, Data):
            x, edge_index = data.x, data.edge_index
        elif isinstance(data, tuple):
            x, edge_index = data
        else:
            raise TypeError('Unsupported data type!')
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i + 1 == len(self.convs):
                break
            x = F.relu(F.dropout(x, p=self.dropout, training=self.training))
        return x


class GCN_Net(torch.nn.Module):
    """ GCN model from the "Semi-supervised Classification with Graph
    Convolutional Networks" paper, in ICLR'17.

    Arguments:
        in_channels (int): dimension of input.
        out_channels (int): dimension of output.
        hidden (int): dimension of hidden units, default=64.
        max_depth (int): layers of GNN, default=2.
        dropout (float): dropout ratio, default=.0.

    """

    def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=0.0):
        super(GCN_Net, self).__init__()
        self.convs = ModuleList()
        for i in range(max_depth):
            if i == 0:
                self.convs.append(GCNConv(in_channels, hidden))
            elif i + 1 == max_depth:
                self.convs.append(GCNConv(hidden, out_channels))
            else:
                self.convs.append(GCNConv(hidden, hidden))
        self.dropout = dropout

    def reset_parameters(self):
        for m in self.convs:
            m.reset_parameters()

    def forward(self, data):
        if isinstance(data, Data):
            x, edge_index = data.x, data.edge_index
        elif isinstance(data, tuple):
            x, edge_index = data
        else:
            raise TypeError('Unsupported data type!')
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i + 1 == len(self.convs):
                break
            x = F.relu(F.dropout(x, p=self.dropout, training=self.training))
        return x


class GIN_Net(torch.nn.Module):
    """Graph Isomorphism Network model from the "How Powerful are Graph
    Neural Networks?" paper, in ICLR'19

    Arguments:
        in_channels (int): dimension of input.
        out_channels (int): dimension of output.
        hidden (int): dimension of hidden units, default=64.
        max_depth (int): layers of GNN, default=2.
        dropout (float): dropout ratio, default=.0.

    """

    def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=0.0):
        super(GIN_Net, self).__init__()
        self.convs = ModuleList()
        for i in range(max_depth):
            if i == 0:
                self.convs.append(GINConv(MLP([in_channels, hidden, hidden], batch_norm=True)))
            elif i + 1 == max_depth:
                self.convs.append(GINConv(MLP([hidden, hidden, out_channels], batch_norm=True)))
            else:
                self.convs.append(GINConv(MLP([hidden, hidden, hidden], batch_norm=True)))
        self.dropout = dropout

    def forward(self, data):
        if isinstance(data, Data):
            x, edge_index = data.x, data.edge_index
        elif isinstance(data, tuple):
            x, edge_index = data
        else:
            raise TypeError('Unsupported data type!')
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i + 1 == len(self.convs):
                break
            x = F.relu(F.dropout(x, p=self.dropout, training=self.training))
        return x


class GPR_Net(torch.nn.Module):
    """GPR-GNN model from the "Adaptive Universal Generalized PageRank
    Graph Neural Network" paper, in ICLR'21

    Arguments:
        in_channels (int): dimension of input.
        out_channels (int): dimension of output.
        hidden (int): dimension of hidden units, default=64.
        K (int): power of GPR-GNN, default=10.
        dropout (float): dropout ratio, default=.0.
        ppnp (str): propagation method in ['PPNP', 'GPR_prop']
        Init (str): init method in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']

    """

    def __init__(self, in_channels, out_channels, hidden=64, K=10, dropout=0.0, ppnp='GPR_prop', alpha=0.1, Init='PPR', Gamma=None):
        super(GPR_Net, self).__init__()
        self.lin1 = Linear(in_channels, hidden)
        self.lin2 = Linear(hidden, out_channels)
        if ppnp == 'PPNP':
            self.prop1 = APPNP(K, alpha)
        elif ppnp == 'GPR_prop':
            self.prop1 = GPR_prop(K, alpha, Init, Gamma)
        self.Init = Init
        self.dprate = 0.5
        self.dropout = dropout

    def reset_parameters(self):
        self.prop1.reset_parameters()

    def forward(self, data):
        if isinstance(data, Data):
            x, edge_index = data.x, data.edge_index
        elif isinstance(data, tuple):
            x, edge_index = data
        else:
            raise TypeError('Unsupported data type!')
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.lin2(x)
        if self.dprate == 0.0:
            x = self.prop1(x, edge_index)
            return F.log_softmax(x, dim=1)
        else:
            x = F.dropout(x, p=self.dprate, training=self.training)
            x = self.prop1(x, edge_index)
            return F.log_softmax(x, dim=1)


EMD_DIM = 200


class AtomEncoder(torch.nn.Module):

    def __init__(self, in_channels, hidden):
        super(AtomEncoder, self).__init__()
        self.atom_embedding_list = torch.nn.ModuleList()
        for i in range(in_channels):
            emb = torch.nn.Embedding(EMD_DIM, hidden)
            torch.nn.init.xavier_uniform_(emb.weight.data)
            self.atom_embedding_list.append(emb)

    def forward(self, x):
        x_embedding = 0
        for i in range(x.shape[1]):
            x_embedding += self.atom_embedding_list[i](x[:, i])
        return x_embedding


class GNN_Net_Graph(torch.nn.Module):
    """GNN model with pre-linear layer, pooling layer
        and output layer for graph classification tasks.

    Arguments:
        in_channels (int): input channels.
        out_channels (int): output channels.
        hidden (int): hidden dim for all modules.
        max_depth (int): number of layers for gnn.
        dropout (float): dropout probability.
        gnn (str): name of gnn type, use ("gcn" or "gin").
        pooling (str): pooling method, use ("add", "mean" or "max").
    """

    def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=0.0, gnn='gcn', pooling='add'):
        super(GNN_Net_Graph, self).__init__()
        self.dropout = dropout
        self.encoder_atom = AtomEncoder(in_channels, hidden)
        self.encoder = Linear(in_channels, hidden)
        if gnn == 'gcn':
            self.gnn = GCN_Net(in_channels=hidden, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'sage':
            self.gnn = SAGE_Net(in_channels=hidden, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'gat':
            self.gnn = GAT_Net(in_channels=hidden, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'gin':
            self.gnn = GIN_Net(in_channels=hidden, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'gpr':
            self.gnn = GPR_Net(in_channels=hidden, out_channels=hidden, hidden=hidden, K=max_depth, dropout=dropout)
        else:
            raise ValueError(f'Unsupported gnn type: {gnn}.')
        if pooling == 'add':
            self.pooling = global_add_pool
        elif pooling == 'mean':
            self.pooling = global_mean_pool
        elif pooling == 'max':
            self.pooling = global_max_pool
        else:
            raise ValueError(f'Unsupported pooling type: {pooling}.')
        self.linear = Sequential(Linear(hidden, hidden), torch.nn.ReLU())
        self.clf = Linear(hidden, out_channels)

    def forward(self, data):
        if isinstance(data, Batch):
            x, edge_index, batch = data.x, data.edge_index, data.batch
        elif isinstance(data, tuple):
            x, edge_index, batch = data
        else:
            raise TypeError('Unsupported data type!')
        if x.dtype == torch.int64:
            x = self.encoder_atom(x)
        else:
            x = self.encoder(x)
        x = self.gnn((x, edge_index))
        x = self.pooling(x, batch)
        x = self.linear(x)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.clf(x)
        return x


class GNN_Net_Link(torch.nn.Module):

    def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=0.0, gnn='gcn', layers=2):
        """GNN model with LinkPredictor for link prediction tasks.

        Arguments:
            in_channels (int): input channels.
            out_channels (int): output channels.
            hidden (int): hidden dim for all modules.
            max_depth (int): number of layers for gnn.
            dropout (float): dropout probability.
            gnn (str): name of gnn type, use ("gcn" or "gin").
            layers (int): number of layers for LinkPredictor.

        """
        super(GNN_Net_Link, self).__init__()
        self.dropout = dropout
        if gnn == 'gcn':
            self.gnn = GCN_Net(in_channels=in_channels, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'sage':
            self.gnn = SAGE_Net(in_channels=in_channels, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'gat':
            self.gnn = GAT_Net(in_channels=in_channels, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'gin':
            self.gnn = GIN_Net(in_channels=in_channels, out_channels=hidden, hidden=hidden, max_depth=max_depth, dropout=dropout)
        elif gnn == 'gpr':
            self.gnn = GPR_Net(in_channels=in_channels, out_channels=hidden, hidden=hidden, K=max_depth, dropout=dropout)
        else:
            raise ValueError(f'Unsupported gnn type: {gnn}.')
        dim_list = [hidden for _ in range(layers)]
        self.output = MLP([hidden] + dim_list + [out_channels], batch_norm=True)

    def forward(self, data):
        if isinstance(data, Data):
            x, edge_index = data.x, data.edge_index
        elif isinstance(data, tuple):
            x, edge_index = data
        else:
            raise TypeError('Unsupported data type!')
        x = self.gnn((x, edge_index))
        return x

    def link_predictor(self, x, edge_index):
        x = x[edge_index[0]] * x[edge_index[1]]
        x = self.output(x)
        return x


class MPNNs2s(nn.Module):
    """MPNN from "Neural Message Passing for Quantum Chemistry" for
    regression and classification on graphs.
    Source: https://github.com/pyg-team/pytorch_geometric/blob/master
    /examples/qm9_nn_conv.py

        Arguments:
        in_channels (int): Size for the input node features.
        out_channels (int): dimension of output.
        num_nn (int): num_edge_features.
        hidden (int): Size for the output node representations. Default to 64.

    """

    def __init__(self, in_channels, out_channels, num_nn, hidden=64):
        super(MPNNs2s, self).__init__()
        self.lin0 = torch.nn.Linear(in_channels, hidden)
        nn = Sequential(Linear(num_nn, 16), ReLU(), Linear(16, hidden * hidden))
        self.conv = NNConv(hidden, hidden, nn, aggr='add')
        self.gru = GRU(hidden, hidden)
        self.set2set = Set2Set(hidden, processing_steps=3, num_layers=3)
        self.lin1 = torch.nn.Linear(2 * hidden, hidden)
        self.lin2 = torch.nn.Linear(hidden, out_channels)

    def forward(self, data):
        if isinstance(data, Batch):
            x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch
        elif isinstance(data, tuple):
            x, edge_index, edge_attr, batch = data
        else:
            raise TypeError('Unsupported data type!')
        self.gru.flatten_parameters()
        out = F.relu(self.lin0(x.float()))
        h = out.unsqueeze(0)
        for i in range(3):
            m = F.relu(self.conv(out, edge_index, edge_attr.float()))
            out, h = self.gru(m.unsqueeze(0), h)
            out = out.squeeze(0)
        out = self.set2set(out, batch)
        out = F.relu(self.lin1(out))
        out = self.lin2(out)
        return out


class BasicMFNet(Module):
    """Basic model for MF task

    Arguments:
        num_user (int): the number of users
        num_item (int): the number of items
        num_hidden (int): the dimension of embedding vector
    """

    def __init__(self, num_user, num_item, num_hidden):
        super(BasicMFNet, self).__init__()
        self.embed_user = Parameter(torch.normal(mean=0, std=0.1, size=(num_user, num_hidden), requires_grad=True, dtype=torch.float32))
        self.register_parameter('embed_user', self.embed_user)
        self.embed_item = Parameter(torch.normal(mean=0, std=0.1, size=(num_item, num_hidden), requires_grad=True, dtype=torch.float32))
        self.register_parameter('embed_item', self.embed_item)

    def forward(self, indices, ratings):
        pred = torch.matmul(self.embed_user, self.embed_item.T)
        label = torch.sparse_coo_tensor(indices, ratings, size=pred.shape, device=pred.device, dtype=torch.float32).to_dense()
        mask = torch.sparse_coo_tensor(indices, np.ones(len(ratings)), size=pred.shape, device=pred.device, dtype=torch.float32).to_dense()
        return mask * pred, label, float(np.prod(pred.size())) / len(ratings)

    def load_state_dict(self, state_dict, strict: bool=True):
        state_dict[self.name_reserve] = getattr(self, self.name_reserve)
        super().load_state_dict(state_dict, strict)

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        state_dict = super().state_dict(destination, prefix, keep_vars)
        del state_dict[self.name_reserve]
        return state_dict


class VMFNet(BasicMFNet):
    """MF model for vertical federated learning

    """
    name_reserve = 'embed_item'


class HMFNet(BasicMFNet):
    """MF model for horizontal federated learning

    """
    name_reserve = 'embed_user'


class LSTM(nn.Module):

    def __init__(self, in_channels, hidden, out_channels, n_layers=2, embed_size=8, dropout=0.0):
        super(LSTM, self).__init__()
        self.in_channels = in_channels
        self.hidden = hidden
        self.embed_size = embed_size
        self.out_channels = out_channels
        self.n_layers = n_layers
        self.encoder = nn.Embedding(in_channels, embed_size)
        self.rnn = nn.LSTM(input_size=embed_size if embed_size else in_channels, hidden_size=hidden, num_layers=n_layers, batch_first=True, dropout=dropout)
        self.decoder = nn.Linear(hidden, out_channels)

    def forward(self, input_):
        if self.embed_size:
            input_ = self.encoder(input_)
        output, _ = self.rnn(input_)
        output = self.decoder(output)
        output = output.permute(0, 2, 1)
        final_word = output[:, :, -1]
        return final_word


class QuadraticModel(torch.nn.Module):

    def __init__(self, in_channels, class_num):
        super(QuadraticModel, self).__init__()
        x = torch.ones((in_channels, 1))
        self.x = torch.nn.parameter.Parameter(x.uniform_(-10.0, 10.0).float())

    def forward(self, A):
        return torch.sum(self.x * torch.matmul(A, self.x), -1)


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (BasicBlock,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Bottleneck,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DummyRegularizer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FeatGenerator,
     lambda: ([], {'latent_dim': 4, 'dropout': 0.5, 'num_pred': 4, 'feat_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LogisticRegression,
     lambda: ([], {'in_channels': 4, 'class_num': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MLP,
     lambda: ([], {'channel_list': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (NT_xentloss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (NumPredictor,
     lambda: ([], {'latent_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PreActBlock,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PreActBottleneck,
     lambda: ([], {'in_planes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (QuadraticModel,
     lambda: ([], {'in_channels': 4, 'class_num': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Sampling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (VGG11,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 256, 256])], {}),
     False),
    (global_NT_xentloss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (projection_MLP_simclr,
     lambda: ([], {'in_dim': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
]

class Test_alibaba_FederatedScope(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])


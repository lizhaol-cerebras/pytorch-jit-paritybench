import sys
_module = sys.modules[__name__]
del sys
cifar100_bs16 = _module
cifar10_bs16 = _module
cub_bs8_384 = _module
cub_bs8_448 = _module
imagenet21k_bs128 = _module
imagenet_bs128_poolformer_medium_224 = _module
imagenet_bs128_poolformer_small_224 = _module
imagenet_bs256_rsb_a12 = _module
imagenet_bs256_rsb_a3 = _module
imagenet_bs32 = _module
imagenet_bs32_pil_bicubic = _module
imagenet_bs32_pil_resize = _module
imagenet_bs64 = _module
imagenet_bs64_autoaug = _module
imagenet_bs64_convmixer_224 = _module
imagenet_bs64_mixer_224 = _module
imagenet_bs64_pil_resize = _module
imagenet_bs64_pil_resize_autoaug = _module
imagenet_bs64_swin_224 = _module
imagenet_bs64_swin_256 = _module
imagenet_bs64_swin_384 = _module
imagenet_bs64_t2t_224 = _module
auto_aug = _module
rand_aug = _module
stanford_cars_bs8_448 = _module
voc_bs16 = _module
default_runtime = _module
densenet121 = _module
densenet161 = _module
densenet169 = _module
densenet201 = _module
efficientnet_b0 = _module
efficientnet_b1 = _module
efficientnet_b2 = _module
efficientnet_b3 = _module
efficientnet_b4 = _module
efficientnet_b5 = _module
efficientnet_b6 = _module
efficientnet_b7 = _module
efficientnet_b8 = _module
efficientnet_em = _module
efficientnet_es = _module
mlp_mixer_base_patch16 = _module
mlp_mixer_large_patch16 = _module
mobilenet_v2_1x = _module
mobilenet_v3_large_imagenet = _module
mobilenet_v3_small_cifar = _module
mobilenet_v3_small_imagenet = _module
poolformer_m36 = _module
poolformer_m48 = _module
poolformer_s12 = _module
poolformer_s24 = _module
poolformer_s36 = _module
regnetx_12gf = _module
regnetx_400mf = _module
regnetx_800mf = _module
resnest101 = _module
resnest200 = _module
resnest269 = _module
resnest50 = _module
resnet101 = _module
resnet101_cifar = _module
resnet152 = _module
resnet152_cifar = _module
resnet18 = _module
resnet18_cifar = _module
resnet34 = _module
resnet34_cifar = _module
resnet34_gem = _module
resnet50 = _module
resnet50_cifar = _module
resnet50_cifar_cutmix = _module
resnet50_cifar_mixup = _module
resnet50_cutmix = _module
resnet50_label_smooth = _module
resnet50_mixup = _module
resnetv1c50 = _module
resnetv1d101 = _module
resnetv1d152 = _module
resnetv1d50 = _module
resnext101_32x4d = _module
resnext101_32x8d = _module
resnext152_32x4d = _module
resnext50_32x4d = _module
seresnet101 = _module
seresnet50 = _module
seresnext101_32x4d = _module
seresnext50_32x4d = _module
shufflenet_v1_1x = _module
shufflenet_v2_1x = _module
base_224 = _module
base_384 = _module
large_224 = _module
large_384 = _module
small_224 = _module
tiny_224 = _module
base_256 = _module
large_256 = _module
small_256 = _module
tiny_256 = _module
tnt_s_patch16_224 = _module
twins_pcpvt_base = _module
twins_svt_base = _module
van_b0 = _module
van_b1 = _module
van_b2 = _module
van_b3 = _module
van_b4 = _module
van_b5 = _module
van_b6 = _module
van_base = _module
van_large = _module
van_small = _module
van_tiny = _module
vgg11 = _module
vgg11bn = _module
vgg13 = _module
vgg13bn = _module
vgg16 = _module
vgg16bn = _module
vgg19 = _module
vgg19bn = _module
cifar10_bs128 = _module
cub_bs64 = _module
imagenet_bs1024_adamw_conformer = _module
imagenet_bs1024_adamw_swin = _module
imagenet_bs1024_coslr = _module
imagenet_bs1024_linearlr_bn_nowd = _module
imagenet_bs2048 = _module
imagenet_bs2048_AdamW = _module
imagenet_bs2048_coslr = _module
imagenet_bs2048_rsb = _module
imagenet_bs256 = _module
imagenet_bs256_140e = _module
imagenet_bs256_200e_coslr_warmup = _module
imagenet_bs256_coslr = _module
imagenet_bs256_epochstep = _module
imagenet_bs4096_AdamW = _module
stanford_cars_bs8 = _module
cspdarknet50_8xb32_in1k = _module
cspresnet50_8xb32_in1k = _module
cspresnext50_8xb32_in1k = _module
densenet121_4xb256_in1k = _module
densenet161_4xb256_in1k = _module
densenet169_4xb256_in1k = _module
densenet201_4xb256_in1k = _module
resnet50_b32x8_fp16_dynamic_imagenet = _module
resnet50_b32x8_fp16_imagenet = _module
lenet5_mnist = _module
mobilenet_v2_b32x8_imagenet = _module
resnest101_32xb64_in1k = _module
resnest101_b64x32_imagenet = _module
resnest200_64xb32_in1k = _module
resnest200_b32x64_imagenet = _module
resnest269_64xb32_in1k = _module
resnest269_b32x64_imagenet = _module
resnest50_32xb64_in1k = _module
resnest50_b64x32_imagenet = _module
resnet101_8xb16_cifar10 = _module
resnet101_8xb32_in1k = _module
resnet101_b16x8_cifar10 = _module
resnet101_b32x8_imagenet = _module
resnet152_8xb16_cifar10 = _module
resnet152_8xb32_in1k = _module
resnet152_b16x8_cifar10 = _module
resnet152_b32x8_imagenet = _module
resnet18_8xb16_cifar10 = _module
resnet18_8xb32_in1k = _module
resnet18_b16x8_cifar10 = _module
resnet18_b32x8_imagenet = _module
resnet34_8xb16_cifar10 = _module
resnet34_8xb32_in1k = _module
resnet34_b16x8_cifar10 = _module
resnet34_b32x8_imagenet = _module
resnet50_8xb16_cifar10 = _module
resnet50_8xb16_cifar100 = _module
resnet50_8xb32_in1k = _module
resnet50_8xb8_cars = _module
resnet50_8xb8_cub = _module
resnet50_b16x8_cifar10 = _module
resnet50_b16x8_cifar100 = _module
resnet50_b16x8_cifar10_mixup = _module
resnet50_b32x8_coslr_imagenet = _module
resnet50_b32x8_cutmix_imagenet = _module
resnet50_b32x8_imagenet = _module
resnet50_b32x8_label_smooth_imagenet = _module
resnet50_b32x8_mixup_imagenet = _module
resnet50_b64x32_warmup_coslr_imagenet = _module
resnet50_b64x32_warmup_imagenet = _module
resnet50_b64x32_warmup_label_smooth_imagenet = _module
resnetv1c101_8xb32_in1k = _module
resnetv1c152_8xb32_in1k = _module
resnetv1c50_8xb32_in1k = _module
resnetv1d101_8xb32_in1k = _module
resnetv1d101_b32x8_imagenet = _module
resnetv1d152_8xb32_in1k = _module
resnetv1d152_b32x8_imagenet = _module
resnetv1d50_8xb32_in1k = _module
resnetv1d50_b32x8_imagenet = _module
resnext101_32x4d_b32x8_imagenet = _module
resnext101_32x8d_b32x8_imagenet = _module
resnext152_32x4d_b32x8_imagenet = _module
resnext50_32x4d_b32x8_imagenet = _module
seresnet101_8xb32_in1k = _module
seresnet101_b32x8_imagenet = _module
seresnet50_8xb32_in1k = _module
seresnet50_b32x8_imagenet = _module
seresnext101_32x4d_b32x8_imagenet = _module
seresnext50_32x4d_b32x8_imagenet = _module
shufflenet_v1_1x_b64x16_linearlr_bn_nowd_imagenet = _module
shufflenet_v2_1x_b64x16_linearlr_bn_nowd_imagenet = _module
swin_base_224_b16x64_300e_imagenet = _module
swin_base_384_evalonly_imagenet = _module
swin_large_224_evalonly_imagenet = _module
swin_large_384_evalonly_imagenet = _module
swin_small_224_b16x64_300e_imagenet = _module
swin_tiny_224_b16x64_300e_imagenet = _module
tnt_s_patch16_224_evalonly_imagenet = _module
vgg11_8xb32_in1k = _module
vgg11_b32x8_imagenet = _module
vgg11bn_8xb32_in1k = _module
vgg11bn_b32x8_imagenet = _module
vgg13_8xb32_in1k = _module
vgg13_b32x8_imagenet = _module
vgg13bn_8xb32_in1k = _module
vgg13bn_b32x8_imagenet = _module
vgg16_8xb16_voc = _module
vgg16_8xb32_in1k = _module
vgg16_b16x8_voc = _module
vgg16_b32x8_imagenet = _module
vgg16bn_8xb32_in1k = _module
vgg16bn_b32x8_imagenet = _module
vgg19_8xb32_in1k = _module
vgg19_b32x8_imagenet = _module
vgg19bn_8xb32_in1k = _module
vgg19bn_b32x8_imagenet = _module
image_demo = _module
conf = _module
stat = _module
conf = _module
mmcls = _module
apis = _module
inference = _module
test = _module
train = _module
core = _module
evaluation = _module
eval_hooks = _module
eval_metrics = _module
mean_ap = _module
multilabel_eval_metrics = _module
export = _module
test = _module
hook = _module
class_num_check_hook = _module
lr_updater = _module
precise_bn_hook = _module
wandblogger_hook = _module
optimizers = _module
lamb = _module
utils = _module
dist_utils = _module
misc = _module
visualization = _module
image = _module
datasets = _module
base_dataset = _module
builder = _module
cifar = _module
cub = _module
custom = _module
dataset_wrappers = _module
imagenet = _module
imagenet21k = _module
mnist = _module
multi_label = _module
pipelines = _module
auto_augment = _module
compose = _module
formatting = _module
loading = _module
transforms = _module
samplers = _module
distributed_sampler = _module
repeat_aug = _module
stanford_cars = _module
voc = _module
models = _module
backbones = _module
alexnet = _module
base_backbone = _module
conformer = _module
convmixer = _module
convnext = _module
cspnet = _module
deit = _module
densenet = _module
efficientformer = _module
efficientnet = _module
hornet = _module
hrnet = _module
lenet = _module
mlp_mixer = _module
mobilenet_v2 = _module
mobilenet_v3 = _module
mvit = _module
poolformer = _module
regnet = _module
repmlp = _module
repvgg = _module
res2net = _module
resnest = _module
resnet = _module
resnet_cifar = _module
resnext = _module
seresnet = _module
seresnext = _module
shufflenet_v1 = _module
shufflenet_v2 = _module
swin_transformer = _module
swin_transformer_v2 = _module
t2t_vit = _module
timm_backbone = _module
tnt = _module
twins = _module
van = _module
vgg = _module
vision_transformer = _module
classifiers = _module
base = _module
image = _module
heads = _module
base_head = _module
cls_head = _module
conformer_head = _module
deit_head = _module
efficientformer_head = _module
linear_head = _module
multi_label_csra_head = _module
multi_label_head = _module
multi_label_linear_head = _module
stacked_head = _module
vision_transformer_head = _module
losses = _module
accuracy = _module
asymmetric_loss = _module
cross_entropy_loss = _module
focal_loss = _module
label_smooth_loss = _module
seesaw_loss = _module
utils = _module
necks = _module
gap = _module
gem = _module
hr_fuse = _module
attention = _module
augment = _module
augments = _module
cutmix = _module
identity = _module
mixup = _module
resizemix = _module
utils = _module
channel_shuffle = _module
embed = _module
helpers = _module
inverted_residual = _module
layer_scale = _module
make_divisible = _module
position_encoding = _module
se_layer = _module
collect_env = _module
device = _module
distribution = _module
logger = _module
setup_env = _module
version = _module
setup = _module
retinanet = _module
test_builder = _module
test_common = _module
test_dataset_utils = _module
test_dataset_wrapper = _module
test_sampler = _module
test_auto_augment = _module
test_loading = _module
test_transform = _module
test_mmdet_inference = _module
test_losses = _module
test_metrics = _module
test_utils = _module
test_backbones = _module
test_conformer = _module
test_convmixer = _module
test_convnext = _module
test_cspnet = _module
test_deit = _module
test_densenet = _module
test_efficientformer = _module
test_efficientnet = _module
test_hornet = _module
test_hrnet = _module
test_mlp_mixer = _module
test_mobilenet_v2 = _module
test_mobilenet_v3 = _module
test_mvit = _module
test_poolformer = _module
test_regnet = _module
test_repmlp = _module
test_repvgg = _module
test_res2net = _module
test_resnest = _module
test_resnet = _module
test_resnet_cifar = _module
test_resnext = _module
test_seresnet = _module
test_seresnext = _module
test_shufflenet_v1 = _module
test_shufflenet_v2 = _module
test_swin_transformer = _module
test_swin_transformer_v2 = _module
test_t2t_vit = _module
test_timm_backbone = _module
test_tnt = _module
test_twins = _module
test_van = _module
test_vgg = _module
test_vision_transformer = _module
utils = _module
test_classifiers = _module
test_heads = _module
test_neck = _module
test_attention = _module
test_augment = _module
test_embed = _module
test_inverted_residual = _module
test_layer_scale = _module
test_misc = _module
test_position_encoding = _module
test_se = _module
test_eval_hook = _module
test_hooks = _module
test_num_class_hook = _module
test_optimizer = _module
test_preciseBN_hook = _module
test_device = _module
test_logger = _module
test_setup_env = _module
test_version_utils = _module
test_visualization = _module
analyze_logs = _module
analyze_results = _module
eval_metric = _module
get_flops = _module
efficientnet_to_mmcls = _module
hornet2mmcls = _module
mlpmixer_to_mmcls = _module
mobilenetv2_to_mmcls = _module
publish_model = _module
reparameterize_model = _module
reparameterize_repvgg = _module
repvgg_to_mmcls = _module
shufflenetv2_to_mmcls = _module
torchvision_to_mmcls = _module
twins2mmcls = _module
van2mmcls = _module
vgg_to_mmcls = _module
mmcls2torchserve = _module
mmcls_handler = _module
onnx2tensorrt = _module
pytorch2mlmodel = _module
pytorch2onnx = _module
pytorch2torchscript = _module
test_torchserver = _module
print_config = _module
verify_dataset = _module
test = _module
train = _module
vis_cam = _module
vis_lr = _module
vis_pipeline = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import logging


import re


from time import time


from typing import OrderedDict


import numpy as np


import torch


import warnings


import time


import torch.distributed as dist


import random


from torch.nn.modules.batchnorm import _BatchNorm


from numbers import Number


from torch.nn.functional import one_hot


import itertools


from typing import List


from typing import Optional


import torch.nn as nn


from torch.functional import Tensor


from torch.nn import GroupNorm


from torch.nn.modules.instancenorm import _InstanceNorm


from torch.utils.data import DataLoader


import math


from torch.optim import Optimizer


from collections import OrderedDict


from torch._utils import _flatten_dense_tensors


from torch._utils import _take_tensors


from torch._utils import _unflatten_dense_tensors


import copy


from abc import ABCMeta


from abc import abstractmethod


from torch.utils.data import Dataset


from functools import partial


from collections import defaultdict


from torch.utils.data.dataset import ConcatDataset as _ConcatDataset


from typing import Sequence


from typing import Union


from collections.abc import Sequence


from torch.utils.data import DistributedSampler as _DistributedSampler


from torch.utils.data import Sampler


import torch.nn.functional as F


from itertools import chain


import torch.utils.checkpoint as cp


from torch.jit.annotations import List


import torch.utils.checkpoint as checkpoint


from torch import nn


from copy import deepcopy


from typing import Dict


import functools


from torch import Tensor


from torch.nn import functional as F


from torch.nn.parameter import Parameter


import collections.abc


from itertools import repeat


import torch.multiprocessing as mp


import torchvision


from numpy.testing import assert_array_almost_equal


from numpy.testing import assert_array_equal


from torchvision import transforms


from torch.nn.modules import GroupNorm


from torch.nn.modules import AvgPool2d


from typing import Iterable


from torch.autograd import Variable


from torch.optim.optimizer import Optimizer


from tensorflow.python.training import py_checkpoint_reader


from torch.nn import BatchNorm1d


from torch.nn import BatchNorm2d


from torch.nn import LayerNorm


import matplotlib.pyplot as plt


class Residual(nn.Module):

    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    def forward(self, x):
        return self.fn(x) + x


class LayerNorm2d(nn.LayerNorm):
    """LayerNorm on channels for 2d images.

    Args:
        num_channels (int): The number of channels of the input tensor.
        eps (float): a value added to the denominator for numerical stability.
            Defaults to 1e-5.
        elementwise_affine (bool): a boolean value that when set to ``True``,
            this module has learnable per-element affine parameters initialized
            to ones (for weights) and zeros (for biases). Defaults to True.
    """

    def __init__(self, num_channels: int, **kwargs) ->None:
        super().__init__(num_channels, **kwargs)
        self.num_channels = self.normalized_shape[0]

    def forward(self, x):
        assert x.dim() == 4, f'LayerNorm2d only supports inputs with shape (N, C, H, W), but got tensor with shape {x.shape}'
        return F.layer_norm(x.permute(0, 2, 3, 1).contiguous(), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2).contiguous()


class DenseBlock(nn.Module):
    """DenseNet Blocks."""

    def __init__(self, num_layers, in_channels, bn_size, growth_rate, norm_cfg=dict(type='BN'), act_cfg=dict(type='ReLU'), drop_rate=0.0, memory_efficient=False):
        super(DenseBlock, self).__init__()
        self.block = nn.ModuleList([DenseLayer(in_channels + i * growth_rate, growth_rate=growth_rate, bn_size=bn_size, norm_cfg=norm_cfg, act_cfg=act_cfg, drop_rate=drop_rate, memory_efficient=memory_efficient) for i in range(num_layers)])

    def forward(self, init_features):
        features = [init_features]
        for layer in self.block:
            new_features = layer(features)
            features.append(new_features)
        return torch.cat(features, 1)


class DenseTransition(nn.Sequential):
    """DenseNet Transition Layers."""

    def __init__(self, in_channels, out_channels, norm_cfg=dict(type='BN'), act_cfg=dict(type='ReLU')):
        super(DenseTransition, self).__init__()
        self.add_module('norm', build_norm_layer(norm_cfg, in_channels)[1])
        self.add_module('act', build_activation_layer(act_cfg))
        self.add_module('conv', nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))


class Flat(nn.Module):
    """Flat the input from (B, C, H, W) to (B, H*W, C)."""

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor):
        x = x.flatten(2).transpose(1, 2)
        return x


class HorNetLayerNorm(nn.Module):
    """An implementation of LayerNorm of HorNet.

    The differences between HorNetLayerNorm & torch LayerNorm:
        1. Supports two data formats channels_last or channels_first.

    Args:
        normalized_shape (int or list or torch.Size): input shape from an
            expected input of size.
        eps (float): a value added to the denominator for numerical stability.
            Defaults to 1e-5.
        data_format (str): The ordering of the dimensions in the inputs.
            channels_last corresponds to inputs with shape (batch_size, height,
            width, channels) while channels_first corresponds to inputs with
            shape (batch_size, channels, height, width).
            Defaults to 'channels_last'.
    """

    def __init__(self, normalized_shape, eps=1e-06, data_format='channels_last'):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ['channels_last', 'channels_first']:
            raise ValueError('data_format must be channels_last or channels_first')
        self.normalized_shape = normalized_shape,

    def forward(self, x):
        if self.data_format == 'channels_last':
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == 'channels_first':
            u = x.mean(1, keepdim=True)
            s = (x - u).pow(2).mean(1, keepdim=True)
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[:, None, None] * x + self.bias[:, None, None]
            return x


class GlobalLocalFilter(nn.Module):
    """A GlobalLocalFilter of HorNet.

    Args:
        dim (int): Number of input channels.
        h (int): Height of complex_weight.
            Defaults to 14.
        w (int): Width of complex_weight.
            Defaults to 8.
    """

    def __init__(self, dim, h=14, w=8):
        super().__init__()
        self.dw = nn.Conv2d(dim // 2, dim // 2, kernel_size=3, padding=1, bias=False, groups=dim // 2)
        self.complex_weight = nn.Parameter(torch.randn(dim // 2, h, w, 2, dtype=torch.float32) * 0.02)
        self.pre_norm = HorNetLayerNorm(dim, eps=1e-06, data_format='channels_first')
        self.post_norm = HorNetLayerNorm(dim, eps=1e-06, data_format='channels_first')

    def forward(self, x):
        x = self.pre_norm(x)
        x1, x2 = torch.chunk(x, 2, dim=1)
        x1 = self.dw(x1)
        x2 = x2
        B, C, a, b = x2.shape
        x2 = torch.fft.rfft2(x2, dim=(2, 3), norm='ortho')
        weight = self.complex_weight
        if not weight.shape[1:3] == x2.shape[2:4]:
            weight = F.interpolate(weight.permute(3, 0, 1, 2), size=x2.shape[2:4], mode='bilinear', align_corners=True).permute(1, 2, 3, 0)
        weight = torch.view_as_complex(weight.contiguous())
        x2 = x2 * weight
        x2 = torch.fft.irfft2(x2, s=(a, b), dim=(2, 3), norm='ortho')
        x = torch.cat([x1.unsqueeze(2), x2.unsqueeze(2)], dim=2).reshape(B, 2 * C, a, b)
        x = self.post_norm(x)
        return x


def get_dwconv(dim, kernel_size, bias=True):
    """build a pepth-wise convolution."""
    return nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=bias, groups=dim)


class gnConv(nn.Module):
    """A gnConv of HorNet.

    Args:
        dim (int): Number of input channels.
        order (int): Order of gnConv.
            Defaults to 5.
        dw_cfg (dict): The Config for dw conv.
            Defaults to ``dict(type='DW', kernel_size=7)``.
        scale (float): Scaling parameter of gflayer outputs.
            Defaults to 1.0.
    """

    def __init__(self, dim, order=5, dw_cfg=dict(type='DW', kernel_size=7), scale=1.0):
        super().__init__()
        self.order = order
        self.dims = [(dim // 2 ** i) for i in range(order)]
        self.dims.reverse()
        self.proj_in = nn.Conv2d(dim, 2 * dim, 1)
        cfg = copy.deepcopy(dw_cfg)
        dw_type = cfg.pop('type')
        assert dw_type in ['DW', 'GF'], 'dw_type should be `DW` or `GF`'
        if dw_type == 'DW':
            self.dwconv = get_dwconv(sum(self.dims), **cfg)
        elif dw_type == 'GF':
            self.dwconv = GlobalLocalFilter(sum(self.dims), **cfg)
        self.proj_out = nn.Conv2d(dim, dim, 1)
        self.projs = nn.ModuleList([nn.Conv2d(self.dims[i], self.dims[i + 1], 1) for i in range(order - 1)])
        self.scale = scale

    def forward(self, x):
        x = self.proj_in(x)
        y, x = torch.split(x, (self.dims[0], sum(self.dims)), dim=1)
        x = self.dwconv(x) * self.scale
        dw_list = torch.split(x, self.dims, dim=1)
        x = y * dw_list[0]
        for i in range(self.order - 1):
            x = self.projs[i](x) * dw_list[i + 1]
        x = self.proj_out(x)
        return x


class LayerScale(nn.Module):
    """LayerScale layer.

    Args:
        dim (int): Dimension of input features.
        inplace (bool): inplace: can optionally do the
            operation in-place. Default: ``False``
        data_format (str): The input data format, can be 'channels_last'
             and 'channels_first', representing (B, C, H, W) and
             (B, N, C) format data respectively.
    """

    def __init__(self, dim: int, inplace: bool=False, data_format: str='channels_last'):
        super().__init__()
        assert data_format in ('channels_last', 'channels_first'), "'data_format' could only be channels_last or channels_first."
        self.inplace = inplace
        self.data_format = data_format
        self.weight = nn.Parameter(torch.ones(dim) * 1e-05)

    def forward(self, x):
        if self.data_format == 'channels_first':
            if self.inplace:
                return x.mul_(self.weight.view(-1, 1, 1))
            else:
                return x * self.weight.view(-1, 1, 1)
        return x.mul_(self.weight) if self.inplace else x * self.weight


class HorNetBlock(nn.Module):
    """A block of HorNet.

    Args:
        dim (int): Number of input channels.
        order (int): Order of gnConv.
            Defaults to 5.
        dw_cfg (dict): The Config for dw conv.
            Defaults to ``dict(type='DW', kernel_size=7)``.
        scale (float): Scaling parameter of gflayer outputs.
            Defaults to 1.0.
        drop_path_rate (float): Stochastic depth rate. Defaults to 0.
        use_layer_scale (bool): Whether to use use_layer_scale in HorNet
             block. Defaults to True.
    """

    def __init__(self, dim, order=5, dw_cfg=dict(type='DW', kernel_size=7), scale=1.0, drop_path_rate=0.0, use_layer_scale=True):
        super().__init__()
        self.out_channels = dim
        self.norm1 = HorNetLayerNorm(dim, eps=1e-06, data_format='channels_first')
        self.gnconv = gnConv(dim, order, dw_cfg, scale)
        self.norm2 = HorNetLayerNorm(dim, eps=1e-06)
        self.pwconv1 = nn.Linear(dim, 4 * dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        if use_layer_scale:
            self.gamma1 = LayerScale(dim, data_format='channels_first')
            self.gamma2 = LayerScale(dim)
        else:
            self.gamma1, self.gamma2 = nn.Identity(), nn.Identity()
        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()

    def forward(self, x):
        x = x + self.drop_path(self.gamma1(self.gnconv(self.norm1(x))))
        input = x
        x = x.permute(0, 2, 3, 1)
        x = self.norm2(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        x = self.gamma2(x)
        x = x.permute(0, 3, 1, 2)
        x = input + self.drop_path(x)
        return x


def _ntuple(n):
    """A `to_tuple` function generator.

    It returns a function, this function will repeat the input to a tuple of
    length ``n`` if the input is not an Iterable object, otherwise, return the
    input directly.

    Args:
        n (int): The number of the target length.
    """

    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse


to_2tuple = _ntuple(2)


class Pooling(nn.Module):
    """Pooling module.

    Args:
        pool_size (int): Pooling size. Defaults to 3.
    """

    def __init__(self, pool_size=3):
        super().__init__()
        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)

    def forward(self, x):
        return self.pool(x) - x


class Mlp(nn.Module):
    """Mlp implemented by with 1*1 convolutions.

    Input: Tensor with shape [B, C, H, W].
    Output: Tensor with shape [B, C, H, W].
    Args:
        in_features (int): Dimension of input features.
        hidden_features (int): Dimension of hidden features.
        out_features (int): Dimension of output features.
        act_cfg (dict): The config dict for activation between pointwise
            convolution. Defaults to ``dict(type='GELU')``.
        drop (float): Dropout rate. Defaults to 0.0.
    """

    def __init__(self, in_features, hidden_features=None, out_features=None, act_cfg=dict(type='GELU'), drop=0.0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)
        self.act = build_activation_layer(act_cfg)
        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class ConvFFN(nn.Module):
    """ConvFFN implemented by using point-wise convs."""

    def __init__(self, in_channels, hidden_channels=None, out_channels=None, norm_cfg=dict(type='BN', requires_grad=True), act_cfg=dict(type='GELU')):
        super().__init__()
        out_features = out_channels or in_channels
        hidden_features = hidden_channels or in_channels
        self.ffn_fc1 = ConvModule(in_channels=in_channels, out_channels=hidden_features, kernel_size=1, stride=1, padding=0, norm_cfg=norm_cfg, act_cfg=None)
        self.ffn_fc2 = ConvModule(in_channels=hidden_features, out_channels=out_features, kernel_size=1, stride=1, padding=0, norm_cfg=norm_cfg, act_cfg=None)
        self.act = build_activation_layer(act_cfg)

    def forward(self, x):
        x = self.ffn_fc1(x)
        x = self.act(x)
        x = self.ffn_fc2(x)
        return x


class MTSPPF(nn.Module):
    """MTSPPF block for YOLOX-PAI RepVGG backbone.

    Args:
        in_channels (int): The input channels of the block.
        out_channels (int): The output channels of the block.
        norm_cfg (dict): dictionary to construct and config norm layer.
            Default: dict(type='BN').
        act_cfg (dict): Config dict for activation layer.
            Default: dict(type='ReLU').
        kernel_size (int): Kernel size of pooling. Default: 5.
    """

    def __init__(self, in_channels, out_channels, norm_cfg=dict(type='BN'), act_cfg=dict(type='ReLU'), kernel_size=5):
        super().__init__()
        hidden_features = in_channels // 2
        self.conv1 = ConvModule(in_channels, hidden_features, 1, stride=1, norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.conv2 = ConvModule(hidden_features * 4, out_channels, 1, stride=1, norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size // 2)

    def forward(self, x):
        x = self.conv1(x)
        y1 = self.maxpool(x)
        y2 = self.maxpool(y1)
        return self.conv2(torch.cat([x, y1, y2, self.maxpool(y2)], 1))


class RSoftmax(nn.Module):
    """Radix Softmax module in ``SplitAttentionConv2d``.

    Args:
        radix (int): Radix of input.
        groups (int): Groups of input.
    """

    def __init__(self, radix, groups):
        super().__init__()
        self.radix = radix
        self.groups = groups

    def forward(self, x):
        batch = x.size(0)
        if self.radix > 1:
            x = x.view(batch, self.groups, self.radix, -1).transpose(1, 2)
            x = F.softmax(x, dim=1)
            x = x.reshape(batch, -1)
        else:
            x = torch.sigmoid(x)
        return x


class SplitAttentionConv2d(nn.Module):
    """Split-Attention Conv2d.

    Args:
        in_channels (int): Same as nn.Conv2d.
        out_channels (int): Same as nn.Conv2d.
        kernel_size (int | tuple[int]): Same as nn.Conv2d.
        stride (int | tuple[int]): Same as nn.Conv2d.
        padding (int | tuple[int]): Same as nn.Conv2d.
        dilation (int | tuple[int]): Same as nn.Conv2d.
        groups (int): Same as nn.Conv2d.
        radix (int): Radix of SpltAtConv2d. Default: 2
        reduction_factor (int): Reduction factor of SplitAttentionConv2d.
            Default: 4.
        conv_cfg (dict, optional): Config dict for convolution layer.
            Default: None, which means using conv2d.
        norm_cfg (dict, optional): Config dict for normalization layer.
            Default: None.
    """

    def __init__(self, in_channels, channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, radix=2, reduction_factor=4, conv_cfg=None, norm_cfg=dict(type='BN')):
        super(SplitAttentionConv2d, self).__init__()
        inter_channels = max(in_channels * radix // reduction_factor, 32)
        self.radix = radix
        self.groups = groups
        self.channels = channels
        self.conv = build_conv_layer(conv_cfg, in_channels, channels * radix, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups * radix, bias=False)
        self.norm0_name, norm0 = build_norm_layer(norm_cfg, channels * radix, postfix=0)
        self.add_module(self.norm0_name, norm0)
        self.relu = nn.ReLU(inplace=True)
        self.fc1 = build_conv_layer(None, channels, inter_channels, 1, groups=self.groups)
        self.norm1_name, norm1 = build_norm_layer(norm_cfg, inter_channels, postfix=1)
        self.add_module(self.norm1_name, norm1)
        self.fc2 = build_conv_layer(None, inter_channels, channels * radix, 1, groups=self.groups)
        self.rsoftmax = RSoftmax(radix, groups)

    @property
    def norm0(self):
        return getattr(self, self.norm0_name)

    @property
    def norm1(self):
        return getattr(self, self.norm1_name)

    def forward(self, x):
        x = self.conv(x)
        x = self.norm0(x)
        x = self.relu(x)
        batch, rchannel = x.shape[:2]
        if self.radix > 1:
            splits = x.view(batch, self.radix, -1, *x.shape[2:])
            gap = splits.sum(dim=1)
        else:
            gap = x
        gap = F.adaptive_avg_pool2d(gap, 1)
        gap = self.fc1(gap)
        gap = self.norm1(gap)
        gap = self.relu(gap)
        atten = self.fc2(gap)
        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)
        if self.radix > 1:
            attens = atten.view(batch, self.radix, -1, *atten.shape[2:])
            out = torch.sum(attens * splits, dim=1)
        else:
            out = atten * x
        return out.contiguous()


eps = 1e-05


def get_expansion(block, expansion=None):
    """Get the expansion of a residual block.

    The block expansion will be obtained by the following order:

    1. If ``expansion`` is given, just return it.
    2. If ``block`` has the attribute ``expansion``, then return
       ``block.expansion``.
    3. Return the default value according the the block type:
       1 for ``BasicBlock`` and 4 for ``Bottleneck``.

    Args:
        block (class): The block class.
        expansion (int | None): The given expansion ratio.

    Returns:
        int: The expansion of the block.
    """
    if isinstance(expansion, int):
        assert expansion > 0
    elif expansion is None:
        if hasattr(block, 'expansion'):
            expansion = block.expansion
        elif issubclass(block, BasicBlock):
            expansion = 1
        elif issubclass(block, Bottleneck):
            expansion = 4
        else:
            raise TypeError(f'expansion is not specified for {block.__name__}')
    else:
        raise TypeError('expansion must be an integer or None')
    return expansion


class ResLayer(nn.Sequential):
    """ResLayer to build ResNet style backbone.

    Args:
        block (nn.Module): Residual block used to build ResLayer.
        num_blocks (int): Number of blocks.
        in_channels (int): Input channels of this block.
        out_channels (int): Output channels of this block.
        expansion (int, optional): The expansion for BasicBlock/Bottleneck.
            If not specified, it will firstly be obtained via
            ``block.expansion``. If the block has no attribute "expansion",
            the following default values will be used: 1 for BasicBlock and
            4 for Bottleneck. Default: None.
        stride (int): stride of the first block. Default: 1.
        avg_down (bool): Use AvgPool instead of stride conv when
            downsampling in the bottleneck. Default: False
        conv_cfg (dict, optional): dictionary to construct and config conv
            layer. Default: None
        norm_cfg (dict): dictionary to construct and config norm layer.
            Default: dict(type='BN')
    """

    def __init__(self, block, num_blocks, in_channels, out_channels, expansion=None, stride=1, avg_down=False, conv_cfg=None, norm_cfg=dict(type='BN'), **kwargs):
        self.block = block
        self.expansion = get_expansion(block, expansion)
        downsample = None
        if stride != 1 or in_channels != out_channels:
            downsample = []
            conv_stride = stride
            if avg_down and stride != 1:
                conv_stride = 1
                downsample.append(nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False))
            downsample.extend([build_conv_layer(conv_cfg, in_channels, out_channels, kernel_size=1, stride=conv_stride, bias=False), build_norm_layer(norm_cfg, out_channels)[1]])
            downsample = nn.Sequential(*downsample)
        layers = []
        layers.append(block(in_channels=in_channels, out_channels=out_channels, expansion=self.expansion, stride=stride, downsample=downsample, conv_cfg=conv_cfg, norm_cfg=norm_cfg, **kwargs))
        in_channels = out_channels
        for i in range(1, num_blocks):
            layers.append(block(in_channels=in_channels, out_channels=out_channels, expansion=self.expansion, stride=1, conv_cfg=conv_cfg, norm_cfg=norm_cfg, **kwargs))
        super(ResLayer, self).__init__(*layers)


def accuracy_torch(pred, target, topk=(1,), thrs=0.0):
    if isinstance(thrs, Number):
        thrs = thrs,
        res_single = True
    elif isinstance(thrs, tuple):
        res_single = False
    else:
        raise TypeError(f'thrs should be a number or tuple, but got {type(thrs)}.')
    res = []
    maxk = max(topk)
    num = pred.size(0)
    pred = pred.float()
    pred_score, pred_label = pred.topk(maxk, dim=1)
    pred_label = pred_label.t()
    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))
    for k in topk:
        res_thr = []
        for thr in thrs:
            _correct = correct & (pred_score.t() > thr)
            correct_k = _correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res_thr.append(correct_k.mul_(100.0 / num))
        if res_single:
            res.append(res_thr[0])
        else:
            res.append(res_thr)
    return res


def accuracy(pred, target, topk=1, thrs=0.0):
    """Calculate accuracy according to the prediction and target.

    Args:
        pred (torch.Tensor | np.array): The model prediction.
        target (torch.Tensor | np.array): The target of each prediction
        topk (int | tuple[int]): If the predictions in ``topk``
            matches the target, the predictions will be regarded as
            correct ones. Defaults to 1.
        thrs (Number | tuple[Number], optional): Predictions with scores under
            the thresholds are considered negative. Default to 0.

    Returns:
        torch.Tensor | list[torch.Tensor] | list[list[torch.Tensor]]: Accuracy
            - torch.Tensor: If both ``topk`` and ``thrs`` is a single value.
            - list[torch.Tensor]: If one of ``topk`` or ``thrs`` is a tuple.
            - list[list[torch.Tensor]]: If both ``topk`` and ``thrs`` is a               tuple. And the first dim is ``topk``, the second dim is ``thrs``.
    """
    assert isinstance(topk, (int, tuple))
    if isinstance(topk, int):
        topk = topk,
        return_single = True
    else:
        return_single = False
    assert isinstance(pred, (torch.Tensor, np.ndarray)), f'The pred should be torch.Tensor or np.ndarray instead of {type(pred)}.'
    assert isinstance(target, (torch.Tensor, np.ndarray)), f'The target should be torch.Tensor or np.ndarray instead of {type(target)}.'
    to_tensor = lambda x: torch.from_numpy(x) if isinstance(x, np.ndarray) else x
    pred = to_tensor(pred)
    target = to_tensor(target)
    res = accuracy_torch(pred, target, topk, thrs)
    return res[0] if return_single else res


class Accuracy(nn.Module):

    def __init__(self, topk=(1,)):
        """Module to calculate the accuracy.

        Args:
            topk (tuple): The criterion used to calculate the
                accuracy. Defaults to (1,).
        """
        super().__init__()
        self.topk = topk

    def forward(self, pred, target):
        """Forward function to calculate accuracy.

        Args:
            pred (torch.Tensor): Prediction of models.
            target (torch.Tensor): Target for each prediction.

        Returns:
            list[torch.Tensor]: The accuracies under different topk criterions.
        """
        return accuracy(pred, target, self.topk)


def reduce_loss(loss, reduction):
    """Reduce loss as specified.

    Args:
        loss (Tensor): Elementwise loss tensor.
        reduction (str): Options are "none", "mean" and "sum".

    Return:
        Tensor: Reduced loss tensor.
    """
    reduction_enum = F._Reduction.get_enum(reduction)
    if reduction_enum == 0:
        return loss
    elif reduction_enum == 1:
        return loss.mean()
    elif reduction_enum == 2:
        return loss.sum()


def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):
    """Apply element-wise weight and reduce loss.

    Args:
        loss (Tensor): Element-wise loss.
        weight (Tensor): Element-wise weights.
        reduction (str): Same as built-in losses of PyTorch.
        avg_factor (float): Average factor when computing the mean of losses.

    Returns:
        Tensor: Processed loss values.
    """
    if weight is not None:
        loss = loss * weight
    if avg_factor is None:
        loss = reduce_loss(loss, reduction)
    elif reduction == 'mean':
        loss = loss.sum() / avg_factor
    elif reduction != 'none':
        raise ValueError('avg_factor can not be used with reduction="sum"')
    return loss


def asymmetric_loss(pred, target, weight=None, gamma_pos=1.0, gamma_neg=4.0, clip=0.05, reduction='mean', avg_factor=None, use_sigmoid=True, eps=1e-08):
    """asymmetric loss.

    Please refer to the `paper <https://arxiv.org/abs/2009.14119>`__ for
    details.

    Args:
        pred (torch.Tensor): The prediction with shape (N, \\*).
        target (torch.Tensor): The ground truth label of the prediction with
            shape (N, \\*).
        weight (torch.Tensor, optional): Sample-wise loss weight with shape
            (N, ). Defaults to None.
        gamma_pos (float): positive focusing parameter. Defaults to 0.0.
        gamma_neg (float): Negative focusing parameter. We usually set
            gamma_neg > gamma_pos. Defaults to 4.0.
        clip (float, optional): Probability margin. Defaults to 0.05.
        reduction (str): The method used to reduce the loss.
            Options are "none", "mean" and "sum". If reduction is 'none' , loss
            is same shape as pred and label. Defaults to 'mean'.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.
        use_sigmoid (bool): Whether the prediction uses sigmoid instead
            of softmax. Defaults to True.
        eps (float): The minimum value of the argument of logarithm. Defaults
            to 1e-8.

    Returns:
        torch.Tensor: Loss.
    """
    assert pred.shape == target.shape, 'pred and target should be in the same shape.'
    if use_sigmoid:
        pred_sigmoid = pred.sigmoid()
    else:
        pred_sigmoid = nn.functional.softmax(pred, dim=-1)
    target = target.type_as(pred)
    if clip and clip > 0:
        pt = (1 - pred_sigmoid + clip).clamp(max=1) * (1 - target) + pred_sigmoid * target
    else:
        pt = (1 - pred_sigmoid) * (1 - target) + pred_sigmoid * target
    asymmetric_weight = (1 - pt).pow(gamma_pos * target + gamma_neg * (1 - target))
    loss = -torch.log(pt.clamp(min=eps)) * asymmetric_weight
    if weight is not None:
        assert weight.dim() == 1
        weight = weight.float()
        if pred.dim() > 1:
            weight = weight.reshape(-1, 1)
    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)
    return loss


def convert_to_one_hot(targets: torch.Tensor, classes) ->torch.Tensor:
    """This function converts target class indices to one-hot vectors, given
    the number of classes.

    Args:
        targets (Tensor): The ground truth label of the prediction
                with shape (N, 1)
        classes (int): the number of classes.

    Returns:
        Tensor: Processed loss values.
    """
    assert torch.max(targets).item() < classes, 'Class Index must be less than number of classes'
    one_hot_targets = F.one_hot(targets.long().squeeze(-1), num_classes=classes)
    return one_hot_targets


class AsymmetricLoss(nn.Module):
    """asymmetric loss.

    Args:
        gamma_pos (float): positive focusing parameter.
            Defaults to 0.0.
        gamma_neg (float): Negative focusing parameter. We
            usually set gamma_neg > gamma_pos. Defaults to 4.0.
        clip (float, optional): Probability margin. Defaults to 0.05.
        reduction (str): The method used to reduce the loss into
            a scalar.
        loss_weight (float): Weight of loss. Defaults to 1.0.
        use_sigmoid (bool): Whether the prediction uses sigmoid instead
            of softmax. Defaults to True.
        eps (float): The minimum value of the argument of logarithm. Defaults
            to 1e-8.
    """

    def __init__(self, gamma_pos=0.0, gamma_neg=4.0, clip=0.05, reduction='mean', loss_weight=1.0, use_sigmoid=True, eps=1e-08):
        super(AsymmetricLoss, self).__init__()
        self.gamma_pos = gamma_pos
        self.gamma_neg = gamma_neg
        self.clip = clip
        self.reduction = reduction
        self.loss_weight = loss_weight
        self.use_sigmoid = use_sigmoid
        self.eps = eps

    def forward(self, pred, target, weight=None, avg_factor=None, reduction_override=None):
        """asymmetric loss.

        Args:
            pred (torch.Tensor): The prediction with shape (N, \\*).
            target (torch.Tensor): The ground truth label of the prediction
                with shape (N, \\*), N or (N,1).
            weight (torch.Tensor, optional): Sample-wise loss weight with shape
                (N, \\*). Defaults to None.
            avg_factor (int, optional): Average factor that is used to average
                the loss. Defaults to None.
            reduction_override (str, optional): The method used to reduce the
                loss into a scalar. Options are "none", "mean" and "sum".
                Defaults to None.

        Returns:
            torch.Tensor: Loss.
        """
        assert reduction_override in (None, 'none', 'mean', 'sum')
        reduction = reduction_override if reduction_override else self.reduction
        if target.dim() == 1 or target.dim() == 2 and target.shape[1] == 1:
            target = convert_to_one_hot(target.view(-1, 1), pred.shape[-1])
        loss_cls = self.loss_weight * asymmetric_loss(pred, target, weight, gamma_pos=self.gamma_pos, gamma_neg=self.gamma_neg, clip=self.clip, reduction=reduction, avg_factor=avg_factor, use_sigmoid=self.use_sigmoid, eps=self.eps)
        return loss_cls


def binary_cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None, class_weight=None, pos_weight=None):
    """Calculate the binary CrossEntropy loss with logits.

    Args:
        pred (torch.Tensor): The prediction with shape (N, \\*).
        label (torch.Tensor): The gt label with shape (N, \\*).
        weight (torch.Tensor, optional): Element-wise weight of loss with shape
            (N, ). Defaults to None.
        reduction (str): The method used to reduce the loss.
            Options are "none", "mean" and "sum". If reduction is 'none' , loss
            is same shape as pred and label. Defaults to 'mean'.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.
        class_weight (torch.Tensor, optional): The weight for each class with
            shape (C), C is the number of classes. Default None.
        pos_weight (torch.Tensor, optional): The positive weight for each
            class with shape (C), C is the number of classes. Default None.

    Returns:
        torch.Tensor: The calculated loss
    """
    assert pred.dim() == label.dim()
    if class_weight is not None:
        N = pred.size()[0]
        class_weight = class_weight.repeat(N, 1)
    loss = F.binary_cross_entropy_with_logits(pred, label, weight=class_weight, pos_weight=pos_weight, reduction='none')
    if weight is not None:
        assert weight.dim() == 1
        weight = weight.float()
        if pred.dim() > 1:
            weight = weight.reshape(-1, 1)
    loss = weight_reduce_loss(loss, weight=weight, reduction=reduction, avg_factor=avg_factor)
    return loss


def cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None, class_weight=None):
    """Calculate the CrossEntropy loss.

    Args:
        pred (torch.Tensor): The prediction with shape (N, C), C is the number
            of classes.
        label (torch.Tensor): The gt label of the prediction.
        weight (torch.Tensor, optional): Sample-wise loss weight.
        reduction (str): The method used to reduce the loss.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.
        class_weight (torch.Tensor, optional): The weight for each class with
            shape (C), C is the number of classes. Default None.

    Returns:
        torch.Tensor: The calculated loss
    """
    loss = F.cross_entropy(pred, label, weight=class_weight, reduction='none')
    if weight is not None:
        weight = weight.float()
    loss = weight_reduce_loss(loss, weight=weight, reduction=reduction, avg_factor=avg_factor)
    return loss


def soft_cross_entropy(pred, label, weight=None, reduction='mean', class_weight=None, avg_factor=None):
    """Calculate the Soft CrossEntropy loss. The label can be float.

    Args:
        pred (torch.Tensor): The prediction with shape (N, C), C is the number
            of classes.
        label (torch.Tensor): The gt label of the prediction with shape (N, C).
            When using "mixup", the label can be float.
        weight (torch.Tensor, optional): Sample-wise loss weight.
        reduction (str): The method used to reduce the loss.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.
        class_weight (torch.Tensor, optional): The weight for each class with
            shape (C), C is the number of classes. Default None.

    Returns:
        torch.Tensor: The calculated loss
    """
    loss = -label * F.log_softmax(pred, dim=-1)
    if class_weight is not None:
        loss *= class_weight
    loss = loss.sum(dim=-1)
    if weight is not None:
        weight = weight.float()
    loss = weight_reduce_loss(loss, weight=weight, reduction=reduction, avg_factor=avg_factor)
    return loss


class CrossEntropyLoss(nn.Module):
    """Cross entropy loss.

    Args:
        use_sigmoid (bool): Whether the prediction uses sigmoid
            of softmax. Defaults to False.
        use_soft (bool): Whether to use the soft version of CrossEntropyLoss.
            Defaults to False.
        reduction (str): The method used to reduce the loss.
            Options are "none", "mean" and "sum". Defaults to 'mean'.
        loss_weight (float):  Weight of the loss. Defaults to 1.0.
        class_weight (List[float], optional): The weight for each class with
            shape (C), C is the number of classes. Default None.
        pos_weight (List[float], optional): The positive weight for each
            class with shape (C), C is the number of classes. Only enabled in
            BCE loss when ``use_sigmoid`` is True. Default None.
    """

    def __init__(self, use_sigmoid=False, use_soft=False, reduction='mean', loss_weight=1.0, class_weight=None, pos_weight=None):
        super(CrossEntropyLoss, self).__init__()
        self.use_sigmoid = use_sigmoid
        self.use_soft = use_soft
        assert not (self.use_soft and self.use_sigmoid), 'use_sigmoid and use_soft could not be set simultaneously'
        self.reduction = reduction
        self.loss_weight = loss_weight
        self.class_weight = class_weight
        self.pos_weight = pos_weight
        if self.use_sigmoid:
            self.cls_criterion = binary_cross_entropy
        elif self.use_soft:
            self.cls_criterion = soft_cross_entropy
        else:
            self.cls_criterion = cross_entropy

    def forward(self, cls_score, label, weight=None, avg_factor=None, reduction_override=None, **kwargs):
        assert reduction_override in (None, 'none', 'mean', 'sum')
        reduction = reduction_override if reduction_override else self.reduction
        if self.class_weight is not None:
            class_weight = cls_score.new_tensor(self.class_weight)
        else:
            class_weight = None
        if self.pos_weight is not None and self.use_sigmoid:
            pos_weight = cls_score.new_tensor(self.pos_weight)
            kwargs.update({'pos_weight': pos_weight})
        else:
            pos_weight = None
        loss_cls = self.loss_weight * self.cls_criterion(cls_score, label, weight, class_weight=class_weight, reduction=reduction, avg_factor=avg_factor, **kwargs)
        return loss_cls


def sigmoid_focal_loss(pred, target, weight=None, gamma=2.0, alpha=0.25, reduction='mean', avg_factor=None):
    """Sigmoid focal loss.

    Args:
        pred (torch.Tensor): The prediction with shape (N, \\*).
        target (torch.Tensor): The ground truth label of the prediction with
            shape (N, \\*).
        weight (torch.Tensor, optional): Sample-wise loss weight with shape
            (N, ). Defaults to None.
        gamma (float): The gamma for calculating the modulating factor.
            Defaults to 2.0.
        alpha (float): A balanced form for Focal Loss. Defaults to 0.25.
        reduction (str): The method used to reduce the loss.
            Options are "none", "mean" and "sum". If reduction is 'none' ,
            loss is same shape as pred and label. Defaults to 'mean'.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.

    Returns:
        torch.Tensor: Loss.
    """
    assert pred.shape == target.shape, 'pred and target should be in the same shape.'
    pred_sigmoid = pred.sigmoid()
    target = target.type_as(pred)
    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)
    focal_weight = (alpha * target + (1 - alpha) * (1 - target)) * pt.pow(gamma)
    loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none') * focal_weight
    if weight is not None:
        assert weight.dim() == 1
        weight = weight.float()
        if pred.dim() > 1:
            weight = weight.reshape(-1, 1)
    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)
    return loss


class FocalLoss(nn.Module):
    """Focal loss.

    Args:
        gamma (float): Focusing parameter in focal loss.
            Defaults to 2.0.
        alpha (float): The parameter in balanced form of focal
            loss. Defaults to 0.25.
        reduction (str): The method used to reduce the loss into
            a scalar. Options are "none" and "mean". Defaults to 'mean'.
        loss_weight (float): Weight of loss. Defaults to 1.0.
    """

    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean', loss_weight=1.0):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.loss_weight = loss_weight

    def forward(self, pred, target, weight=None, avg_factor=None, reduction_override=None):
        """Sigmoid focal loss.

        Args:
            pred (torch.Tensor): The prediction with shape (N, \\*).
            target (torch.Tensor): The ground truth label of the prediction
                with shape (N, \\*), N or (N,1).
            weight (torch.Tensor, optional): Sample-wise loss weight with shape
                (N, \\*). Defaults to None.
            avg_factor (int, optional): Average factor that is used to average
                the loss. Defaults to None.
            reduction_override (str, optional): The method used to reduce the
                loss into a scalar. Options are "none", "mean" and "sum".
                Defaults to None.

        Returns:
            torch.Tensor: Loss.
        """
        assert reduction_override in (None, 'none', 'mean', 'sum')
        reduction = reduction_override if reduction_override else self.reduction
        if target.dim() == 1 or target.dim() == 2 and target.shape[1] == 1:
            target = convert_to_one_hot(target.view(-1, 1), pred.shape[-1])
        loss_cls = self.loss_weight * sigmoid_focal_loss(pred, target, weight, gamma=self.gamma, alpha=self.alpha, reduction=reduction, avg_factor=avg_factor)
        return loss_cls


class LabelSmoothLoss(nn.Module):
    """Initializer for the label smoothed cross entropy loss.

    Refers to `Rethinking the Inception Architecture for Computer Vision
    <https://arxiv.org/abs/1512.00567>`_

    This decreases gap between output scores and encourages generalization.
    Labels provided to forward can be one-hot like vectors (NxC) or class
    indices (Nx1).
    And this accepts linear combination of one-hot like labels from mixup or
    cutmix except multi-label task.

    Args:
        label_smooth_val (float): The degree of label smoothing.
        num_classes (int, optional): Number of classes. Defaults to None.
        mode (str): Refers to notes, Options are 'original', 'classy_vision',
            'multi_label'. Defaults to 'original'
        reduction (str): The method used to reduce the loss.
            Options are "none", "mean" and "sum". Defaults to 'mean'.
        loss_weight (float):  Weight of the loss. Defaults to 1.0.

    Notes:
        if the mode is "original", this will use the same label smooth method
        as the original paper as:

        .. math::
            (1-\\epsilon)\\delta_{k, y} + \\frac{\\epsilon}{K}

        where epsilon is the `label_smooth_val`, K is the num_classes and
        delta(k,y) is Dirac delta, which equals 1 for k=y and 0 otherwise.

        if the mode is "classy_vision", this will use the same label smooth
        method as the facebookresearch/ClassyVision repo as:

        .. math::
            \\frac{\\delta_{k, y} + \\epsilon/K}{1+\\epsilon}

        if the mode is "multi_label", this will accept labels from multi-label
        task and smoothing them as:

        .. math::
            (1-2\\epsilon)\\delta_{k, y} + \\epsilon
    """

    def __init__(self, label_smooth_val, num_classes=None, mode='original', reduction='mean', loss_weight=1.0):
        super().__init__()
        self.num_classes = num_classes
        self.loss_weight = loss_weight
        assert isinstance(label_smooth_val, float) and 0 <= label_smooth_val < 1, f'LabelSmoothLoss accepts a float label_smooth_val over [0, 1), but gets {label_smooth_val}'
        self.label_smooth_val = label_smooth_val
        accept_reduction = {'none', 'mean', 'sum'}
        assert reduction in accept_reduction, f'LabelSmoothLoss supports reduction {accept_reduction}, but gets {mode}.'
        self.reduction = reduction
        accept_mode = {'original', 'classy_vision', 'multi_label'}
        assert mode in accept_mode, f'LabelSmoothLoss supports mode {accept_mode}, but gets {mode}.'
        self.mode = mode
        self._eps = label_smooth_val
        if mode == 'classy_vision':
            self._eps = label_smooth_val / (1 + label_smooth_val)
        if mode == 'multi_label':
            self.ce = CrossEntropyLoss(use_sigmoid=True)
            self.smooth_label = self.multilabel_smooth_label
        else:
            self.ce = CrossEntropyLoss(use_soft=True)
            self.smooth_label = self.original_smooth_label

    def generate_one_hot_like_label(self, label):
        """This function takes one-hot or index label vectors and computes one-
        hot like label vectors (float)"""
        if label.dim() == 1 or label.dim() == 2 and label.shape[1] == 1:
            label = convert_to_one_hot(label.view(-1, 1), self.num_classes)
        return label.float()

    def original_smooth_label(self, one_hot_like_label):
        assert self.num_classes > 0
        smooth_label = one_hot_like_label * (1 - self._eps)
        smooth_label += self._eps / self.num_classes
        return smooth_label

    def multilabel_smooth_label(self, one_hot_like_label):
        assert self.num_classes > 0
        smooth_label = torch.full_like(one_hot_like_label, self._eps)
        smooth_label.masked_fill_(one_hot_like_label > 0, 1 - self._eps)
        return smooth_label

    def forward(self, cls_score, label, weight=None, avg_factor=None, reduction_override=None, **kwargs):
        """Label smooth loss.

        Args:
            pred (torch.Tensor): The prediction with shape (N, \\*).
            label (torch.Tensor): The ground truth label of the prediction
                with shape (N, \\*).
            weight (torch.Tensor, optional): Sample-wise loss weight with shape
                (N, \\*). Defaults to None.
            avg_factor (int, optional): Average factor that is used to average
                the loss. Defaults to None.
            reduction_override (str, optional): The method used to reduce the
                loss into a scalar. Options are "none", "mean" and "sum".
                Defaults to None.

        Returns:
            torch.Tensor: Loss.
        """
        if self.num_classes is not None:
            assert self.num_classes == cls_score.shape[1], f'num_classes should equal to cls_score.shape[1], but got num_classes: {self.num_classes} and cls_score.shape[1]: {cls_score.shape[1]}'
        else:
            self.num_classes = cls_score.shape[1]
        one_hot_like_label = self.generate_one_hot_like_label(label=label)
        assert one_hot_like_label.shape == cls_score.shape, f'LabelSmoothLoss requires output and target to be same shape, but got output.shape: {cls_score.shape} and target.shape: {one_hot_like_label.shape}'
        smoothed_label = self.smooth_label(one_hot_like_label)
        return self.ce.forward(cls_score, smoothed_label, weight=weight, avg_factor=avg_factor, reduction_override=reduction_override, **kwargs)


def seesaw_ce_loss(cls_score, labels, weight, cum_samples, num_classes, p, q, eps, reduction='mean', avg_factor=None):
    """Calculate the Seesaw CrossEntropy loss.

    Args:
        cls_score (torch.Tensor): The prediction with shape (N, C),
             C is the number of classes.
        labels (torch.Tensor): The learning label of the prediction.
        weight (torch.Tensor): Sample-wise loss weight.
        cum_samples (torch.Tensor): Cumulative samples for each category.
        num_classes (int): The number of classes.
        p (float): The ``p`` in the mitigation factor.
        q (float): The ``q`` in the compenstation factor.
        eps (float): The minimal value of divisor to smooth
             the computation of compensation factor
        reduction (str, optional): The method used to reduce the loss.
        avg_factor (int, optional): Average factor that is used to average
            the loss. Defaults to None.

    Returns:
        torch.Tensor: The calculated loss
    """
    assert cls_score.size(-1) == num_classes
    assert len(cum_samples) == num_classes
    onehot_labels = F.one_hot(labels, num_classes)
    seesaw_weights = cls_score.new_ones(onehot_labels.size())
    if p > 0:
        sample_ratio_matrix = cum_samples[None, :].clamp(min=1) / cum_samples[:, None].clamp(min=1)
        index = (sample_ratio_matrix < 1.0).float()
        sample_weights = sample_ratio_matrix.pow(p) * index + (1 - index)
        mitigation_factor = sample_weights[labels.long(), :]
        seesaw_weights = seesaw_weights * mitigation_factor
    if q > 0:
        scores = F.softmax(cls_score.detach(), dim=1)
        self_scores = scores[torch.arange(0, len(scores)).long(), labels.long()]
        score_matrix = scores / self_scores[:, None].clamp(min=eps)
        index = (score_matrix > 1.0).float()
        compensation_factor = score_matrix.pow(q) * index + (1 - index)
        seesaw_weights = seesaw_weights * compensation_factor
    cls_score = cls_score + seesaw_weights.log() * (1 - onehot_labels)
    loss = F.cross_entropy(cls_score, labels, weight=None, reduction='none')
    if weight is not None:
        weight = weight.float()
    loss = weight_reduce_loss(loss, weight=weight, reduction=reduction, avg_factor=avg_factor)
    return loss


class SeesawLoss(nn.Module):
    """Implementation of seesaw loss.

    Refers to `Seesaw Loss for Long-Tailed Instance Segmentation (CVPR 2021)
    <https://arxiv.org/abs/2008.10032>`_

    Args:
        use_sigmoid (bool): Whether the prediction uses sigmoid of softmax.
             Only False is supported. Defaults to False.
        p (float): The ``p`` in the mitigation factor.
             Defaults to 0.8.
        q (float): The ``q`` in the compenstation factor.
             Defaults to 2.0.
        num_classes (int): The number of classes.
             Default to 1000 for the ImageNet dataset.
        eps (float): The minimal value of divisor to smooth
             the computation of compensation factor, default to 1e-2.
        reduction (str): The method that reduces the loss to a scalar.
             Options are "none", "mean" and "sum". Default to "mean".
        loss_weight (float): The weight of the loss. Defaults to 1.0
    """

    def __init__(self, use_sigmoid=False, p=0.8, q=2.0, num_classes=1000, eps=0.01, reduction='mean', loss_weight=1.0):
        super(SeesawLoss, self).__init__()
        assert not use_sigmoid, '`use_sigmoid` is not supported'
        self.use_sigmoid = False
        self.p = p
        self.q = q
        self.num_classes = num_classes
        self.eps = eps
        self.reduction = reduction
        self.loss_weight = loss_weight
        self.cls_criterion = seesaw_ce_loss
        self.register_buffer('cum_samples', torch.zeros(self.num_classes, dtype=torch.float))

    def forward(self, cls_score, labels, weight=None, avg_factor=None, reduction_override=None):
        """Forward function.

        Args:
            cls_score (torch.Tensor): The prediction with shape (N, C).
            labels (torch.Tensor): The learning label of the prediction.
            weight (torch.Tensor, optional): Sample-wise loss weight.
            avg_factor (int, optional): Average factor that is used to average
                 the loss. Defaults to None.
            reduction (str, optional): The method used to reduce the loss.
                 Options are "none", "mean" and "sum".
        Returns:
            torch.Tensor: The calculated loss
        """
        assert reduction_override in (None, 'none', 'mean', 'sum'), f'The `reduction_override` should be one of (None, "none", "mean", "sum"), but get "{reduction_override}".'
        assert cls_score.size(0) == labels.view(-1).size(0), f'Expected `labels` shape [{cls_score.size(0)}], but got {list(labels.size())}'
        reduction = reduction_override if reduction_override else self.reduction
        assert cls_score.size(-1) == self.num_classes, f'The channel number of output ({cls_score.size(-1)}) does not match the `num_classes` of seesaw loss ({self.num_classes}).'
        unique_labels = labels.unique()
        for u_l in unique_labels:
            inds_ = labels == u_l.item()
            self.cum_samples[u_l] += inds_.sum()
        if weight is not None:
            weight = weight.float()
        else:
            weight = labels.new_ones(labels.size(), dtype=torch.float)
        loss_cls = self.loss_weight * self.cls_criterion(cls_score, labels, weight, self.cum_samples, self.num_classes, self.p, self.q, self.eps, reduction, avg_factor)
        return loss_cls


class GlobalAveragePooling(nn.Module):
    """Global Average Pooling neck.

    Note that we use `view` to remove extra channel after pooling. We do not
    use `squeeze` as it will also remove the batch dimension when the tensor
    has a batch dimension of size 1, which can lead to unexpected errors.

    Args:
        dim (int): Dimensions of each sample channel, can be one of {1, 2, 3}.
            Default: 2
    """

    def __init__(self, dim=2):
        super(GlobalAveragePooling, self).__init__()
        assert dim in [1, 2, 3], f'GlobalAveragePooling dim only support {1, 2, 3}, get {dim} instead.'
        if dim == 1:
            self.gap = nn.AdaptiveAvgPool1d(1)
        elif dim == 2:
            self.gap = nn.AdaptiveAvgPool2d((1, 1))
        else:
            self.gap = nn.AdaptiveAvgPool3d((1, 1, 1))

    def init_weights(self):
        pass

    def forward(self, inputs):
        if isinstance(inputs, tuple):
            outs = tuple([self.gap(x) for x in inputs])
            outs = tuple([out.view(x.size(0), -1) for out, x in zip(outs, inputs)])
        elif isinstance(inputs, torch.Tensor):
            outs = self.gap(inputs)
            outs = outs.view(inputs.size(0), -1)
        else:
            raise TypeError('neck inputs should be tuple or torch.tensor')
        return outs


def gem(x: Tensor, p: Parameter, eps: float=1e-06, clamp=True) ->Tensor:
    if clamp:
        x = x.clamp(min=eps)
    return F.avg_pool2d(x.pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)


class GeneralizedMeanPooling(nn.Module):
    """Generalized Mean Pooling neck.

    Note that we use `view` to remove extra channel after pooling. We do not
    use `squeeze` as it will also remove the batch dimension when the tensor
    has a batch dimension of size 1, which can lead to unexpected errors.

    Args:
        p (float): Parameter value.
            Default: 3.
        eps (float): epsilon.
            Default: 1e-6
        clamp (bool): Use clamp before pooling.
            Default: True
    """

    def __init__(self, p=3.0, eps=1e-06, clamp=True):
        assert p >= 1, "'p' must be a value greater then 1"
        super(GeneralizedMeanPooling, self).__init__()
        self.p = Parameter(torch.ones(1) * p)
        self.eps = eps
        self.clamp = clamp

    def forward(self, inputs):
        if isinstance(inputs, tuple):
            outs = tuple([gem(x, p=self.p, eps=self.eps, clamp=self.clamp) for x in inputs])
            outs = tuple([out.view(x.size(0), -1) for out, x in zip(outs, inputs)])
        elif isinstance(inputs, torch.Tensor):
            outs = gem(inputs, p=self.p, eps=self.eps, clamp=self.clamp)
            outs = outs.view(inputs.size(0), -1)
        else:
            raise TypeError('neck inputs should be tuple or torch.tensor')
        return outs


class BaseFigureContextManager:
    """Context Manager to reuse matplotlib figure.

    It provides a figure for saving and a figure for showing to support
    different settings.

    Args:
        axis (bool): Whether to show the axis lines.
        fig_save_cfg (dict): Keyword parameters of figure for saving.
            Defaults to empty dict.
        fig_show_cfg (dict): Keyword parameters of figure for showing.
            Defaults to empty dict.
    """

    def __init__(self, axis=False, fig_save_cfg={}, fig_show_cfg={}) ->None:
        self.is_inline = 'inline' in plt.get_backend()
        self.fig_save: plt.Figure = None
        self.fig_save_cfg = fig_save_cfg
        self.ax_save: plt.Axes = None
        self.fig_show: plt.Figure = None
        self.fig_show_cfg = fig_show_cfg
        self.ax_show: plt.Axes = None
        self.axis = axis

    def __enter__(self):
        if not self.is_inline:
            self._initialize_fig_save()
            self._initialize_fig_show()
        return self

    def _initialize_fig_save(self):
        fig = plt.figure(**self.fig_save_cfg)
        ax = fig.add_subplot()
        fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
        self.fig_save, self.ax_save = fig, ax

    def _initialize_fig_show(self):
        fig = plt.figure(**self.fig_show_cfg)
        ax = fig.add_subplot()
        fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
        self.fig_show, self.ax_show = fig, ax

    def __exit__(self, exc_type, exc_value, traceback):
        if self.is_inline:
            return
        plt.close(self.fig_save)
        plt.close(self.fig_show)

    def prepare(self):
        if self.is_inline:
            self._initialize_fig_save()
            self.ax_save.cla()
            self.ax_save.axis(self.axis)
            return
        if not plt.fignum_exists(self.fig_show.number):
            self._initialize_fig_show()
        self.ax_save.cla()
        self.ax_save.axis(self.axis)
        self.ax_show.cla()
        self.ax_show.axis(self.axis)

    def wait_continue(self, timeout=0, continue_key=' ') ->int:
        """Show the image and wait for the user's input.

        This implementation refers to
        https://github.com/matplotlib/matplotlib/blob/v3.5.x/lib/matplotlib/_blocking_input.py

        Args:
            timeout (int): If positive, continue after ``timeout`` seconds.
                Defaults to 0.
            continue_key (str): The key for users to continue. Defaults to
                the space key.

        Returns:
            int: If zero, means time out or the user pressed ``continue_key``,
                and if one, means the user closed the show figure.
        """
        if self.is_inline:
            return
        if self.fig_show.canvas.manager:
            self.fig_show.show()
        while True:
            event = None

            def handler(ev):
                nonlocal event
                event = ev if not isinstance(event, CloseEvent) else event
                self.fig_show.canvas.stop_event_loop()
            cids = [self.fig_show.canvas.mpl_connect(name, handler) for name in ('key_press_event', 'close_event')]
            try:
                self.fig_show.canvas.start_event_loop(timeout)
            finally:
                for cid in cids:
                    self.fig_show.canvas.mpl_disconnect(cid)
            if isinstance(event, CloseEvent):
                return 1
            elif event is None or event.key == continue_key:
                return 0


EPS = 0.01


def color_val_matplotlib(color):
    """Convert various input in BGR order to normalized RGB matplotlib color
    tuples,

    Args:
        color (:obj:`mmcv.Color`/str/tuple/int/ndarray): Color inputs

    Returns:
        tuple[float]: A tuple of 3 normalized floats indicating RGB channels.
    """
    color = mmcv.color_val(color)
    color = [(color / 255) for color in color[::-1]]
    return tuple(color)


class ImshowInfosContextManager(BaseFigureContextManager):
    """Context Manager to reuse matplotlib figure and put infos on images.

    Args:
        fig_size (tuple[int]): Size of the figure to show image.

    Examples:
        >>> import mmcv
        >>> from mmcls.core import visualization as vis
        >>> img1 = mmcv.imread("./1.png")
        >>> info1 = {'class': 'cat', 'label': 0}
        >>> img2 = mmcv.imread("./2.png")
        >>> info2 = {'class': 'dog', 'label': 1}
        >>> with vis.ImshowInfosContextManager() as manager:
        ...     # Show img1
        ...     manager.put_img_infos(img1, info1)
        ...     # Show img2 on the same figure and save output image.
        ...     manager.put_img_infos(
        ...         img2, info2, out_file='./2_out.png')
    """

    def __init__(self, fig_size=(15, 10)):
        super().__init__(axis=False, fig_save_cfg=dict(frameon=False, dpi=36), fig_show_cfg=dict(frameon=False, figsize=fig_size))

    def _put_text(self, ax, text, x, y, text_color, font_size):
        ax.text(x, y, f'{text}', bbox={'facecolor': 'black', 'alpha': 0.7, 'pad': 0.2, 'edgecolor': 'none', 'boxstyle': 'round'}, color=text_color, fontsize=font_size, family='monospace', verticalalignment='top', horizontalalignment='left')

    def put_img_infos(self, img, infos, text_color='white', font_size=26, row_width=20, win_name='', show=True, wait_time=0, out_file=None):
        """Show image with extra information.

        Args:
            img (str | ndarray): The image to be displayed.
            infos (dict): Extra infos to display in the image.
            text_color (:obj:`mmcv.Color`/str/tuple/int/ndarray): Extra infos
                display color. Defaults to 'white'.
            font_size (int): Extra infos display font size. Defaults to 26.
            row_width (int): width between each row of results on the image.
            win_name (str): The image title. Defaults to ''
            show (bool): Whether to show the image. Defaults to True.
            wait_time (int): How many seconds to display the image.
                Defaults to 0.
            out_file (Optional[str]): The filename to write the image.
                Defaults to None.

        Returns:
            np.ndarray: The image with extra infomations.
        """
        self.prepare()
        text_color = color_val_matplotlib(text_color)
        img = mmcv.imread(img).astype(np.uint8)
        x, y = 3, row_width // 2
        img = mmcv.bgr2rgb(img)
        width, height = img.shape[1], img.shape[0]
        img = np.ascontiguousarray(img)
        dpi = self.fig_save.get_dpi()
        self.fig_save.set_size_inches((width + EPS) / dpi, (height + EPS) / dpi)
        for k, v in infos.items():
            if isinstance(v, float):
                v = f'{v:.2f}'
            label_text = f'{k}: {v}'
            self._put_text(self.ax_save, label_text, x, y, text_color, font_size)
            if show and not self.is_inline:
                self._put_text(self.ax_show, label_text, x, y, text_color, font_size)
            y += row_width
        self.ax_save.imshow(img)
        stream, _ = self.fig_save.canvas.print_to_buffer()
        buffer = np.frombuffer(stream, dtype='uint8')
        img_rgba = buffer.reshape(height, width, 4)
        rgb, _ = np.split(img_rgba, [3], axis=2)
        img_save = rgb.astype('uint8')
        img_save = mmcv.rgb2bgr(img_save)
        if out_file is not None:
            mmcv.imwrite(img_save, out_file)
        ret = 0
        if show and not self.is_inline:
            self.ax_show.set_title(win_name)
            self.ax_show.set_ylim(height + 20)
            self.ax_show.text(width // 2, height + 18, 'Press SPACE to continue.', ha='center', fontsize=font_size)
            self.ax_show.imshow(img)
            self.fig_show.canvas.draw()
            ret = self.wait_continue(timeout=wait_time)
        elif not show and self.is_inline:
            plt.close(self.fig_save)
        return ret, img_save


def imshow_infos(img, infos, text_color='white', font_size=26, row_width=20, win_name='', show=True, fig_size=(15, 10), wait_time=0, out_file=None):
    """Show image with extra information.

    Args:
        img (str | ndarray): The image to be displayed.
        infos (dict): Extra infos to display in the image.
        text_color (:obj:`mmcv.Color`/str/tuple/int/ndarray): Extra infos
            display color. Defaults to 'white'.
        font_size (int): Extra infos display font size. Defaults to 26.
        row_width (int): width between each row of results on the image.
        win_name (str): The image title. Defaults to ''
        show (bool): Whether to show the image. Defaults to True.
        fig_size (tuple): Image show figure size. Defaults to (15, 10).
        wait_time (int): How many seconds to display the image. Defaults to 0.
        out_file (Optional[str]): The filename to write the image.
            Defaults to None.

    Returns:
        np.ndarray: The image with extra infomations.
    """
    with ImshowInfosContextManager(fig_size=fig_size) as manager:
        _, img = manager.put_img_infos(img, infos, text_color=text_color, font_size=font_size, row_width=row_width, win_name=win_name, show=show, wait_time=wait_time, out_file=out_file)
    return img


class SubModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(2, 2, kernel_size=1, groups=2)
        self.gn = nn.GroupNorm(2, 2)
        self.fc = nn.Linear(2, 2)
        self.param1 = nn.Parameter(torch.ones(1))

    def forward(self, x):
        return x


class SimpleModel(nn.Module):
    """simple model that do nothing in train_step."""

    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv = nn.Conv2d(1, 1, 1)

    def train_step(self, *args, **kwargs):
        pass


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AsymmetricLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (CrossEntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (Flat,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FocalLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (GeneralizedMeanPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalAveragePooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GlobalLocalFilter,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (HorNetLayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LayerNorm2d,
     lambda: ([], {'num_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LayerScale,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Pooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (RSoftmax,
     lambda: ([], {'radix': 4, 'groups': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Residual,
     lambda: ([], {'fn': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SubModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_open_mmlab_mmclassification(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])


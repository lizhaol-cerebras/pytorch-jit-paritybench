import sys
_module = sys.modules[__name__]
del sys
build_count = _module
regenerate = _module
test_list = _module
conf = _module
pulsar_basic = _module
pulsar_basic_unified = _module
pulsar_cam = _module
pulsar_cam_unified = _module
pulsar_multiview = _module
pulsar_optimization = _module
pulsar_optimization_unified = _module
generate_stubs = _module
utils = _module
camera_visualization = _module
generate_cow_renders = _module
plot_image_grid = _module
build_conda = _module
publish = _module
projects = _module
implicitron_trainer = _module
experiment = _module
impl = _module
model_factory = _module
optimizer_factory = _module
training_loop = _module
utils = _module
tests = _module
test_experiment = _module
test_optimizer_factory = _module
test_visualize = _module
visualize_reconstruction = _module
nerf = _module
dataset = _module
eval_video_utils = _module
implicit_function = _module
nerf_renderer = _module
raymarcher = _module
raysampler = _module
stats = _module
utils = _module
test_nerf = _module
test_raymarcher = _module
test_raysampler = _module
train_nerf = _module
pytorch3d = _module
common = _module
compat = _module
datatypes = _module
linear_with_repeat = _module
workaround = _module
symeig3x3 = _module
utils = _module
datasets = _module
r2n2 = _module
r2n2 = _module
utils = _module
shapenet = _module
shapenet_core = _module
shapenet_base = _module
utils = _module
implicitron = _module
blender_dataset_map_provider = _module
data_loader_map_provider = _module
data_source = _module
dataset_base = _module
dataset_map_provider = _module
json_index_dataset = _module
json_index_dataset_map_provider = _module
json_index_dataset_map_provider_v2 = _module
llff_dataset_map_provider = _module
load_blender = _module
load_llff = _module
rendered_mesh_dataset_map_provider = _module
scene_batch_sampler = _module
single_sequence_dataset = _module
types = _module
utils = _module
visualize = _module
eval_demo = _module
evaluation = _module
evaluate_new_view_synthesis = _module
evaluator = _module
models = _module
base_model = _module
feature_extractor = _module
feature_extractor = _module
resnet_feature_extractor = _module
generic_model = _module
global_encoder = _module
autodecoder = _module
global_encoder = _module
base = _module
decoding_functions = _module
idr_feature_field = _module
neural_radiance_field = _module
scene_representation_networks = _module
utils = _module
voxel_grid = _module
voxel_grid_implicit_function = _module
metrics = _module
model_dbir = _module
renderer = _module
base = _module
lstm_renderer = _module
multipass_ea = _module
ray_point_refiner = _module
ray_sampler = _module
ray_tracing = _module
raymarcher = _module
rgb_net = _module
sdf_renderer = _module
view_pooler = _module
feature_aggregator = _module
view_pooler = _module
view_sampler = _module
visualization = _module
render_flyaround = _module
third_party = _module
hyperlayers = _module
pytorch_prototyping = _module
tools = _module
camera_utils = _module
circle_fitting = _module
config = _module
depth_cleanup = _module
eval_video_trajectory = _module
image_utils = _module
metric_utils = _module
model_io = _module
point_cloud_utils = _module
rasterize_mc = _module
utils = _module
video_writer = _module
vis_utils = _module
io = _module
experimental_gltf_io = _module
mtl_io = _module
obj_io = _module
off_io = _module
pluggable = _module
pluggable_formats = _module
ply_io = _module
utils = _module
loss = _module
chamfer = _module
mesh_edge_loss = _module
mesh_laplacian_smoothing = _module
mesh_normal_consistency = _module
point_mesh_distance = _module
ops = _module
ball_query = _module
cameras_alignment = _module
cubify = _module
graph_conv = _module
interp_face_attrs = _module
iou_box3d = _module
knn = _module
laplacian_matrices = _module
marching_cubes = _module
marching_cubes_data = _module
mesh_face_areas_normals = _module
mesh_filtering = _module
packed_to_padded = _module
perspective_n_points = _module
points_alignment = _module
points_normals = _module
points_to_volumes = _module
sample_farthest_points = _module
sample_points_from_meshes = _module
subdivide_meshes = _module
utils = _module
vert_align = _module
blending = _module
camera_conversions = _module
camera_utils = _module
cameras = _module
compositing = _module
fisheyecameras = _module
implicit = _module
harmonic_embedding = _module
raymarching = _module
raysampling = _module
renderer = _module
sample_pdf = _module
utils = _module
lighting = _module
materials = _module
mesh = _module
clip = _module
rasterize_meshes = _module
rasterizer = _module
renderer = _module
shader = _module
shading = _module
textures = _module
utils = _module
opengl = _module
opengl_utils = _module
rasterizer_opengl = _module
points = _module
compositor = _module
pulsar = _module
renderer = _module
unified = _module
rasterize_points = _module
rasterizer = _module
renderer = _module
splatter_blend = _module
utils = _module
structures = _module
meshes = _module
pointclouds = _module
utils = _module
volumes = _module
transforms = _module
math = _module
rotation_conversions = _module
se3 = _module
so3 = _module
transform3d = _module
camera_conversions = _module
checkerboard = _module
ico_sphere = _module
torus = _module
vis = _module
plotly_vis = _module
texture_vis = _module
parse_tutorials = _module
setup = _module
benchmarks = _module
bm_acos_linear_extrapolation = _module
bm_ball_query = _module
bm_barycentric_clipping = _module
bm_blending = _module
bm_cameras = _module
bm_cameras_alignment = _module
bm_chamfer = _module
bm_cubify = _module
bm_face_areas_normals = _module
bm_graph_conv = _module
bm_interpolate_face_attributes = _module
bm_iou_box3d = _module
bm_knn = _module
bm_lighting = _module
bm_main = _module
bm_marching_cubes = _module
bm_mesh_edge_loss = _module
bm_mesh_io = _module
bm_mesh_laplacian_smoothing = _module
bm_mesh_normal_consistency = _module
bm_mesh_rasterizer_transform = _module
bm_meshes = _module
bm_packed_to_padded = _module
bm_perspective_n_points = _module
bm_point_mesh_distance = _module
bm_pointclouds = _module
bm_points_alignment = _module
bm_points_normals = _module
bm_points_to_volumes = _module
bm_pulsar = _module
bm_rasterize_meshes = _module
bm_rasterize_points = _module
bm_raymarching = _module
bm_raysampling = _module
bm_render_implicit = _module
bm_render_volumes = _module
bm_sample_farthest_points = _module
bm_sample_pdf = _module
bm_sample_points_from_meshes = _module
bm_se3 = _module
bm_so3 = _module
bm_subdivide_meshes = _module
bm_symeig3x3 = _module
bm_vert_align = _module
common_testing = _module
common_resources = _module
test_batch_sampler = _module
test_bbox = _module
test_build = _module
test_circle_fitting = _module
test_config = _module
test_config_use = _module
test_data_cow = _module
test_data_json_index = _module
test_data_llff = _module
test_data_source = _module
test_dataset_visualize = _module
test_eval_cameras = _module
test_eval_demo = _module
test_evaluation = _module
test_forward_pass = _module
test_json_index_dataset_provider_v2 = _module
test_model_visualize = _module
test_ray_point_refiner = _module
test_srn = _module
test_types = _module
test_viewsampling = _module
test_voxel_grid_implicit_function = _module
test_voxel_grids = _module
create_multiview = _module
test_channels = _module
test_depth = _module
test_forward = _module
test_hands = _module
test_ortho = _module
test_small_spheres = _module
test_acos_linear_extrapolation = _module
test_ball_query = _module
test_blending = _module
test_camera_conversions = _module
test_camera_pixels = _module
test_camera_utils = _module
test_cameras = _module
test_cameras_alignment = _module
test_chamfer = _module
test_checkerboard = _module
test_common_linear_with_repeat = _module
test_common_testing = _module
test_common_workaround = _module
test_compositing = _module
test_cubify = _module
test_face_areas_normals = _module
test_graph_conv = _module
test_harmonic_embedding = _module
test_interpolate_face_attributes = _module
test_io_gltf = _module
test_io_obj = _module
test_io_off = _module
test_io_ply = _module
test_iou_box3d = _module
test_knn = _module
test_laplacian_matrices = _module
test_lighting = _module
test_marching_cubes = _module
test_materials = _module
test_mesh_edge_loss = _module
test_mesh_filtering = _module
test_mesh_laplacian_smoothing = _module
test_mesh_normal_consistency = _module
test_mesh_rendering_utils = _module
test_meshes = _module
test_opengl_utils = _module
test_ops_utils = _module
test_packed_to_padded = _module
test_perspective_n_points = _module
test_point_mesh_distance = _module
test_pointclouds = _module
test_points_alignment = _module
test_points_normals = _module
test_points_to_volumes = _module
test_r2n2 = _module
test_rasterize_meshes = _module
test_rasterize_points = _module
test_rasterize_rectangle_images = _module
test_rasterizer = _module
test_raymarching = _module
test_raysampling = _module
test_render_implicit = _module
test_render_meshes = _module
test_render_meshes_clipped = _module
test_render_multigpu = _module
test_render_points = _module
test_render_volumes = _module
test_rendering_utils = _module
test_rotation_conversions = _module
test_sample_farthest_points = _module
test_sample_pdf = _module
test_sample_points_from_meshes = _module
test_se3 = _module
test_shader = _module
test_shapenet_core = _module
test_so3 = _module
test_splatter_blend = _module
test_struct_utils = _module
test_subdivide_meshes = _module
test_symeig3x3 = _module
test_texturing = _module
test_transforms = _module
test_vert_align = _module
test_volumes = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import logging


import math


import torch


import numpy as np


from torch import nn


from torch import optim


import warnings


from typing import Optional


import torch.optim


import inspect


from collections import defaultdict


from typing import Any


from typing import Dict


from typing import List


from typing import Tuple


import time


from torch.utils.data import DataLoader


from torch.utils.data import Dataset


import random


from torch.utils.data.dataset import Dataset


import collections


from typing import Sequence


from typing import Union


import torch.nn.functional as F


from torch.nn import init


from torch.nn import Parameter


from enum import Enum


from typing import Iterator


from torch.utils.data import BatchSampler


from torch.utils.data import ChainDataset


from torch.utils.data import RandomSampler


from torch.utils.data import Sampler


from typing import ClassVar


from typing import Iterable


from typing import Mapping


from typing import Type


import copy


import functools


from itertools import islice


from typing import TYPE_CHECKING


from collections import Counter


from torch.utils.data.sampler import Sampler


from typing import cast


from collections import OrderedDict


import torch.nn.functional as Fu


import torchvision


from typing import Callable


from collections.abc import Mapping


from abc import ABC


from abc import abstractmethod


import torch.nn as nn


import torchvision.utils


from torch.nn import functional as F


from math import pi


from collections import deque


from enum import IntEnum


from typing import BinaryIO


from collections import namedtuple


from typing import Deque


import itertools


from typing import ContextManager


from typing import IO


from torch.autograd import Function


from torch.autograd.function import once_differentiable


from typing import NamedTuple


from random import randint


from torch.nn.functional import interpolate


from typing import TypeVar


from itertools import zip_longest


from itertools import tee


from math import cos


from math import sin


from torch.utils.cpp_extension import CppExtension


from torch.utils.cpp_extension import CUDA_HOME


from torch.utils.cpp_extension import CUDAExtension


from itertools import product


from torch.autograd import Variable


from numbers import Real


from typing import Generator


from math import radians


from torch.nn.functional import normalize


import typing


from functools import partial


height = 1000


n_points = 10


width = 1000


class SceneModel(nn.Module):
    """A simple model to demonstrate use in Modules."""

    def __init__(self):
        super(SceneModel, self).__init__()
        self.gamma = 1.0
        torch.manual_seed(1)
        vert_pos = torch.rand((1, n_points, 3), dtype=torch.float32) * 10.0
        vert_pos[:, :, 2] += 25.0
        vert_pos[:, :, :2] -= 5.0
        self.register_parameter('vert_pos', nn.Parameter(vert_pos, requires_grad=False))
        self.register_parameter('vert_col', nn.Parameter(torch.zeros(1, n_points, 3, dtype=torch.float32), requires_grad=True))
        self.register_parameter('vert_rad', nn.Parameter(torch.ones(1, n_points, dtype=torch.float32) * 0.001, requires_grad=False))
        self.register_parameter('vert_opy', nn.Parameter(torch.ones(1, n_points, dtype=torch.float32), requires_grad=False))
        self.register_buffer('cam_params', torch.tensor([[np.sin(angle) * 35.0, 0.0, 30.0 - np.cos(angle) * 35.0, 0.0, -angle, 0.0, 5.0, 2.0] for angle in [-1.5, -0.8, -0.4, -0.1, 0.1, 0.4, 0.8, 1.5]], dtype=torch.float32))
        self.renderer = Renderer(width, height, n_points)

    def forward(self, cam=None):
        if cam is None:
            cam = self.cam_params
            n_views = 8
        else:
            n_views = 1
        return self.renderer.forward(self.vert_pos.expand(n_views, -1, -1), self.vert_col.expand(n_views, -1, -1), self.vert_rad.expand(n_views, -1), cam, self.gamma, 45.0, return_forward_info=True)


class Node(torch.nn.Module):

    def __init__(self, children=(), params=(), param_groups=None):
        super().__init__()
        for i, child in enumerate(children):
            self.add_module('m' + str(i), child)
        for i, param in enumerate(params):
            setattr(self, 'p' + str(i), param)
        if param_groups is not None:
            self.param_groups = param_groups

    def __str__(self):
        return 'modules:\n' + str(self._modules) + '\nparameters\n' + str(self._parameters)


class HarmonicEmbedding(torch.nn.Module):

    def __init__(self, n_harmonic_functions: int=6, omega_0: float=1.0, logspace: bool=True, append_input: bool=True) ->None:
        """
        Given an input tensor `x` of shape [minibatch, ... , dim],
        the harmonic embedding layer converts each feature
        (i.e. vector along the last dimension) in `x`
        into a series of harmonic features `embedding`,
        where for each i in range(dim) the following are present
        in embedding[...]::

            [
                sin(f_1*x[..., i]),
                sin(f_2*x[..., i]),
                ...
                sin(f_N * x[..., i]),
                cos(f_1*x[..., i]),
                cos(f_2*x[..., i]),
                ...
                cos(f_N * x[..., i]),
                x[..., i],              # only present if append_input is True.
            ]

        where N corresponds to `n_harmonic_functions-1`, and f_i is a scalar
        denoting the i-th frequency of the harmonic embedding.

        If `logspace==True`, the frequencies `[f_1, ..., f_N]` are
        powers of 2:
            `f_1, ..., f_N = 2**torch.arange(n_harmonic_functions)`

        If `logspace==False`, frequencies are linearly spaced between
        `1.0` and `2**(n_harmonic_functions-1)`:
            `f_1, ..., f_N = torch.linspace(
                1.0, 2**(n_harmonic_functions-1), n_harmonic_functions
            )`

        Note that `x` is also premultiplied by the base frequency `omega_0`
        before evaluating the harmonic functions.

        Args:
            n_harmonic_functions: int, number of harmonic
                features
            omega_0: float, base frequency
            logspace: bool, Whether to space the frequencies in
                logspace or linear space
            append_input: bool, whether to concat the original
                input to the harmonic embedding. If true the
                output is of the form (x, embed.sin(), embed.cos()

        """
        super().__init__()
        if logspace:
            frequencies = 2.0 ** torch.arange(n_harmonic_functions, dtype=torch.float32)
        else:
            frequencies = torch.linspace(1.0, 2.0 ** (n_harmonic_functions - 1), n_harmonic_functions, dtype=torch.float32)
        self.register_buffer('_frequencies', frequencies * omega_0, persistent=False)
        self.append_input = append_input

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: tensor of shape [..., dim]
        Returns:
            embedding: a harmonic embedding of `x`
                of shape [..., (n_harmonic_functions * 2 + int(append_input)) * dim]
        """
        embed = (x[..., None] * self._frequencies).reshape(*x.shape[:-1], -1)
        embed = torch.cat((embed.sin(), embed.cos(), x) if self.append_input else (embed.sin(), embed.cos()), dim=-1)
        return embed

    @staticmethod
    def get_output_dim_static(input_dims: int, n_harmonic_functions: int, append_input: bool) ->int:
        """
        Utility to help predict the shape of the output of `forward`.

        Args:
            input_dims: length of the last dimension of the input tensor
            n_harmonic_functions: number of embedding frequencies
            append_input: whether or not to concat the original
                input to the harmonic embedding
        Returns:
            int: the length of the last dimension of the output tensor
        """
        return input_dims * (2 * n_harmonic_functions + int(append_input))

    def get_output_dim(self, input_dims: int=3) ->int:
        """
        Same as above. The default for input_dims is 3 for 3D applications
        which use harmonic embedding for positional encoding,
        so the input might be xyz.
        """
        return self.get_output_dim_static(input_dims, len(self._frequencies), self.append_input)


class LinearWithRepeat(torch.nn.Module):
    """
    if x has shape (..., k, n1)
    and y has shape (..., n2)
    then
    LinearWithRepeat(n1 + n2, out_features).forward((x,y))
    is equivalent to
    Linear(n1 + n2, out_features).forward(
        torch.cat([x, y.unsqueeze(-2).expand(..., k, n2)], dim=-1)
    )

    Or visually:
    Given the following, for each ray,

                feature   ->

    ray         xxxxxxxx
    position    xxxxxxxx
      |         xxxxxxxx
      v         xxxxxxxx


    and
                            yyyyyyyy

    where the y's do not depend on the position
    but only on the ray,
    we want to evaluate a Linear layer on both
    types of data at every position.

    It's as if we constructed

                xxxxxxxxyyyyyyyy
                xxxxxxxxyyyyyyyy
                xxxxxxxxyyyyyyyy
                xxxxxxxxyyyyyyyy

    and sent that through the Linear.
    """

    def __init__(self, in_features: int, out_features: int, bias: bool=True, device=None, dtype=None) ->None:
        """
        Copied from torch.nn.Linear.
        """
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
        if bias:
            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self) ->None:
        """
        Copied from torch.nn.Linear.
        """
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            init.uniform_(self.bias, -bound, bound)

    def forward(self, input: Tuple[torch.Tensor, torch.Tensor]) ->torch.Tensor:
        n1 = input[0].shape[-1]
        output1 = F.linear(input[0], self.weight[:, :n1], self.bias)
        output2 = F.linear(input[1], self.weight[:, n1:], None)
        return output1 + output2.unsqueeze(-2)


def _is_actually_dataclass(some_class) ->bool:
    return '__dataclass_fields__' in some_class.__dict__


class ReplaceableBase:
    """
    Base class for a class (a "replaceable") which is a base class for
    dataclass-style implementations. The implementations can be stored
    in the registry. They get expanded into dataclasses with expand_args_fields.
    This expansion is delayed.
    """

    def __new__(cls, *args, **kwargs):
        """
        These classes should be expanded only when needed (because processing
        fixes the list of replaceable subclasses of members of the class). It
        is safer if users expand the classes explicitly. But if the class gets
        instantiated when it hasn't been processed, we expand it here.
        """
        obj = super().__new__(cls)
        if cls is not ReplaceableBase and not _is_actually_dataclass(cls):
            expand_args_fields(cls)
        return obj


class _ProcessType(Enum):
    """
    Type of member which gets rewritten by expand_args_fields.
    """
    CONFIGURABLE = 1
    REPLACEABLE = 2
    OPTIONAL_CONFIGURABLE = 3
    OPTIONAL_REPLACEABLE = 4


def _get_type_to_process(type_) ->Optional[Tuple[Type, _ProcessType]]:
    """
    If a member is annotated as `type_`, and that should expanded in
    expand_args_fields, return how it should be expanded.
    """
    if get_origin(type_) == Union:
        args = get_args(type_)
        if len(args) != 2 or all(a is not type(None) for a in args):
            return
        underlying = args[0] if args[1] is type(None) else args[1]
        if isinstance(underlying, type) and issubclass(underlying, ReplaceableBase) and ReplaceableBase in underlying.__bases__:
            return underlying, _ProcessType.OPTIONAL_REPLACEABLE
        if isinstance(underlying, type) and issubclass(underlying, Configurable):
            return underlying, _ProcessType.OPTIONAL_CONFIGURABLE
    if not isinstance(type_, type):
        return
    if issubclass(type_, ReplaceableBase) and ReplaceableBase in type_.__bases__:
        return type_, _ProcessType.REPLACEABLE
    if issubclass(type_, Configurable):
        return type_, _ProcessType.CONFIGURABLE


def _default_create(name: str, type_: Type, process_type: _ProcessType) ->Callable[[Any], None]:
    """
    Return the default creation function for a member. This is a function which
    could be called in __post_init__ to initialise the member, and will be called
    from run_auto_creation.

    Args:
        name: name of the member
        type_: type of the member (with any Optional removed)
        process_type: Shows whether member's declared type inherits ReplaceableBase,
                    in which case the actual type to be created is decided at
                    runtime.

    Returns:
        Function taking one argument, the object whose member should be
            initialized, i.e. self.
    """
    impl_name = f'{CREATE_PREFIX}{name}{IMPL_SUFFIX}'

    def inner(self):
        expand_args_fields(type_)
        impl = getattr(self, impl_name)
        args = getattr(self, name + ARGS_SUFFIX)
        impl(True, args)

    def inner_optional(self):
        expand_args_fields(type_)
        impl = getattr(self, impl_name)
        enabled = getattr(self, name + ENABLED_SUFFIX)
        args = getattr(self, name + ARGS_SUFFIX)
        impl(enabled, args)

    def inner_pluggable(self):
        type_name = getattr(self, name + TYPE_SUFFIX)
        impl = getattr(self, impl_name)
        if type_name is None:
            args = None
        else:
            args = getattr(self, f'{name}_{type_name}{ARGS_SUFFIX}', None)
        impl(type_name, args)
    if process_type == _ProcessType.OPTIONAL_CONFIGURABLE:
        return inner_optional
    return inner if process_type == _ProcessType.CONFIGURABLE else inner_pluggable


_X = TypeVar('_X')


class _Registry:
    """
    Register from names to classes. In particular, we say that direct subclasses of
    ReplaceableBase are "base classes" and we register subclasses of each base class
    in a separate namespace.
    """

    def __init__(self) ->None:
        self._mapping: Dict[Type[ReplaceableBase], Dict[str, Type[ReplaceableBase]]] = defaultdict(dict)

    def register(self, some_class: Type[_X]) ->Type[_X]:
        """
        A class decorator, to register a class in self.
        """
        name = some_class.__name__
        self._register(some_class, name=name)
        return some_class

    def _register(self, some_class: Type[ReplaceableBase], *, base_class: Optional[Type[ReplaceableBase]]=None, name: str) ->None:
        """
        Register a new member.

        Args:
            cls: the new member
            base_class: (optional) what the new member is a type for
            name: name for the new member
        """
        if base_class is None:
            base_class = self._base_class_from_class(some_class)
            if base_class is None:
                raise ValueError(f'Cannot register {some_class}. Cannot tell what it is.')
        self._mapping[base_class][name] = some_class

    def get(self, base_class_wanted: Type[_X], name: str) ->Type[_X]:
        """
        Retrieve a class from the registry by name

        Args:
            base_class_wanted: parent type of type we are looking for.
                        It determines the namespace.
                        This will typically be a direct subclass of ReplaceableBase.
            name: what to look for

        Returns:
            class type
        """
        if self._is_base_class(base_class_wanted):
            base_class = base_class_wanted
        else:
            base_class = self._base_class_from_class(base_class_wanted)
            if base_class is None:
                raise ValueError(f'Cannot look up {base_class_wanted}. Cannot tell what it is.')
        if not isinstance(name, str):
            raise ValueError(f'Cannot look up a {type(name)} in the registry. Got {name}.')
        result = self._mapping[base_class].get(name)
        if result is None:
            raise ValueError(f'{name} has not been registered.')
        if not issubclass(result, base_class_wanted):
            raise ValueError(f'{name} resolves to {result} which does not subclass {base_class_wanted}')
        return result

    def get_all(self, base_class_wanted: Type[ReplaceableBase]) ->List[Type[ReplaceableBase]]:
        """
        Retrieve all registered implementations from the registry

        Args:
            base_class_wanted: parent type of type we are looking for.
                        It determines the namespace.
                        This will typically be a direct subclass of ReplaceableBase.
        Returns:
            list of class types in alphabetical order of registered name.
        """
        if self._is_base_class(base_class_wanted):
            source = self._mapping[base_class_wanted]
            return [source[key] for key in sorted(source)]
        base_class = self._base_class_from_class(base_class_wanted)
        if base_class is None:
            raise ValueError(f'Cannot look up {base_class_wanted}. Cannot tell what it is.')
        source = self._mapping[base_class]
        return [source[key] for key in sorted(source) if issubclass(source[key], base_class_wanted) and source[key] is not base_class_wanted]

    @staticmethod
    def _is_base_class(some_class: Type[ReplaceableBase]) ->bool:
        """
        Return whether the given type is a direct subclass of ReplaceableBase
        and so gets used as a namespace.
        """
        return ReplaceableBase in some_class.__bases__

    @staticmethod
    def _base_class_from_class(some_class: Type[ReplaceableBase]) ->Optional[Type[ReplaceableBase]]:
        """
        Find the parent class of some_class which inherits ReplaceableBase, or None
        """
        for base in some_class.mro()[-3::-1]:
            if base is not ReplaceableBase and issubclass(base, ReplaceableBase):
                return base
        return None


registry = _Registry()


def _dataclass_name_for_function(C: Any) ->str:
    """
    Returns the name of the dataclass which enable_get_default_args(C)
    creates.
    """
    name = f'_{C.__name__}_default_args_'
    return name


def _is_configurable_class(C) ->bool:
    return isinstance(C, type) and issubclass(C, (Configurable, ReplaceableBase))


def _process_member(*, name: str, type_: Type, process_type: _ProcessType, some_class: Type, creation_functions: List[str], _do_not_process: Tuple[type, ...], known_implementations: Dict[str, Type]) ->None:
    """
    Make the modification (of expand_args_fields) to some_class for a single member.

    Args:
        name: member name
        type_: member type (with Optional removed if needed)
        process_type: whether member has dynamic type
        some_class: (MODIFIED IN PLACE) the class being processed
        creation_functions: (MODIFIED IN PLACE) the names of the create functions
        _do_not_process: as for expand_args_fields.
        known_implementations: (MODIFIED IN PLACE) known types from the registry
    """
    del some_class.__annotations__[name]
    hook = getattr(some_class, name + TWEAK_SUFFIX, None)
    if process_type in (_ProcessType.REPLACEABLE, _ProcessType.OPTIONAL_REPLACEABLE):
        type_name = name + TYPE_SUFFIX
        if type_name not in some_class.__annotations__:
            if process_type == _ProcessType.OPTIONAL_REPLACEABLE:
                some_class.__annotations__[type_name] = Optional[str]
            else:
                some_class.__annotations__[type_name] = str
            setattr(some_class, type_name, 'UNDEFAULTED')
        for derived_type in registry.get_all(type_):
            if derived_type in _do_not_process:
                continue
            if issubclass(derived_type, some_class):
                continue
            known_implementations[derived_type.__name__] = derived_type
            args_name = f'{name}_{derived_type.__name__}{ARGS_SUFFIX}'
            if args_name in some_class.__annotations__:
                raise ValueError(f'Cannot generate {args_name} because it is already present.')
            some_class.__annotations__[args_name] = dict
            if hook is not None:
                hook_closed = partial(hook, derived_type)
            else:
                hook_closed = None
            setattr(some_class, args_name, _get_default_args_field_from_registry(base_class_wanted=type_, name=derived_type.__name__, _do_not_process=_do_not_process + (some_class,), _hook=hook_closed))
    else:
        args_name = name + ARGS_SUFFIX
        if args_name in some_class.__annotations__:
            raise ValueError(f'Cannot generate {args_name} because it is already present.')
        if issubclass(type_, some_class) or type_ in _do_not_process:
            raise ValueError(f'Cannot process {type_} inside {some_class}')
        some_class.__annotations__[args_name] = dict
        if hook is not None:
            hook_closed = partial(hook, type_)
        else:
            hook_closed = None
        setattr(some_class, args_name, get_default_args_field(type_, _do_not_process=_do_not_process + (some_class,), _hook=hook_closed))
        if process_type == _ProcessType.OPTIONAL_CONFIGURABLE:
            enabled_name = name + ENABLED_SUFFIX
            if enabled_name not in some_class.__annotations__:
                raise ValueError(f'{name} is an Optional[{type_.__name__}] member but there is no corresponding member {enabled_name}.')
    creation_function_name = f'{CREATE_PREFIX}{name}'
    if not hasattr(some_class, creation_function_name):
        setattr(some_class, creation_function_name, _default_create(name, type_, process_type))
    creation_functions.append(creation_function_name)
    creation_function_impl_name = f'{CREATE_PREFIX}{name}{IMPL_SUFFIX}'
    if not hasattr(some_class, creation_function_impl_name):
        setattr(some_class, creation_function_impl_name, _default_create_impl(name, type_, process_type))


class Configurable:
    """
    Base class for dataclass-style classes which are not replaceable. These get
    expanded into a dataclass with expand_args_fields.
    This expansion is delayed.
    """

    def __new__(cls, *args, **kwargs):
        """
        These classes should be expanded only when needed (because processing
        fixes the list of replaceable subclasses of members of the class). It
        is safer if users expand the classes explicitly. But if the class gets
        instantiated when it hasn't been processed, we expand it here.
        """
        obj = super().__new__(cls)
        if cls is not Configurable and not _is_actually_dataclass(cls):
            expand_args_fields(cls)
        return obj


class DecoderActivation(Enum):
    RELU = 'relu'
    SOFTPLUS = 'softplus'
    SIGMOID = 'sigmoid'
    IDENTITY = 'identity'


def _xavier_init(linear) ->None:
    """
    Performs the Xavier weight initialization of the linear layer `linear`.
    """
    torch.nn.init.xavier_uniform_(linear.weight.data)


class MLPWithInputSkips(Configurable, torch.nn.Module):
    """
    Implements the multi-layer perceptron architecture of the Neural Radiance Field.

    As such, `MLPWithInputSkips` is a multi layer perceptron consisting
    of a sequence of linear layers with ReLU activations.

    Additionally, for a set of predefined layers `input_skips`, the forward pass
    appends a skip tensor `z` to the output of the preceding layer.

    Note that this follows the architecture described in the Supplementary
    Material (Fig. 7) of [1], for which keep the defaults for:
        - `last_layer_bias_init` to None
        - `last_activation` to "relu"
        - `use_xavier_init` to `true`

    If you want to use this as a part of the color prediction in TensoRF model set:
        - `last_layer_bias_init` to 0
        - `last_activation` to "sigmoid"
        - `use_xavier_init` to `False`

    References:
        [1] Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik
            and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng:
            NeRF: Representing Scenes as Neural Radiance Fields for View
            Synthesis, ECCV2020

    Members:
        n_layers: The number of linear layers of the MLP.
        input_dim: The number of channels of the input tensor.
        output_dim: The number of channels of the output.
        skip_dim: The number of channels of the tensor `z` appended when
            evaluating the skip layers.
        hidden_dim: The number of hidden units of the MLP.
        input_skips: The list of layer indices at which we append the skip
            tensor `z`.
        last_layer_bias_init: If set then all the biases in the last layer
            are initialized to that value.
        last_activation: Which activation to use in the last layer. Options are:
            "relu", "softplus", "sigmoid" and "identity". Default is "relu".
        use_xavier_init: If True uses xavier init for all linear layer weights.
            Otherwise the default PyTorch initialization is used. Default True.
    """
    n_layers: int = 8
    input_dim: int = 39
    output_dim: int = 256
    skip_dim: int = 39
    hidden_dim: int = 256
    input_skips: Tuple[int, ...] = (5,)
    skip_affine_trans: bool = False
    last_layer_bias_init: Optional[float] = None
    last_activation: DecoderActivation = DecoderActivation.RELU
    use_xavier_init: bool = True

    def __post_init__(self):
        super().__init__()
        try:
            last_activation = {DecoderActivation.RELU: torch.nn.ReLU(True), DecoderActivation.SOFTPLUS: torch.nn.Softplus(), DecoderActivation.SIGMOID: torch.nn.Sigmoid(), DecoderActivation.IDENTITY: torch.nn.Identity()}[self.last_activation]
        except KeyError as e:
            raise ValueError('`last_activation` can only be `RELU`, `SOFTPLUS`, `SIGMOID` or `IDENTITY`.') from e
        layers = []
        skip_affine_layers = []
        for layeri in range(self.n_layers):
            dimin = self.hidden_dim if layeri > 0 else self.input_dim
            dimout = self.hidden_dim if layeri + 1 < self.n_layers else self.output_dim
            if layeri > 0 and layeri in self.input_skips:
                if self.skip_affine_trans:
                    skip_affine_layers.append(self._make_affine_layer(self.skip_dim, self.hidden_dim))
                else:
                    dimin = self.hidden_dim + self.skip_dim
            linear = torch.nn.Linear(dimin, dimout)
            if self.use_xavier_init:
                _xavier_init(linear)
            if layeri == self.n_layers - 1 and self.last_layer_bias_init is not None:
                torch.nn.init.constant_(linear.bias, self.last_layer_bias_init)
            layers.append(torch.nn.Sequential(linear, torch.nn.ReLU(True)) if not layeri + 1 < self.n_layers else torch.nn.Sequential(linear, last_activation))
        self.mlp = torch.nn.ModuleList(layers)
        if self.skip_affine_trans:
            self.skip_affines = torch.nn.ModuleList(skip_affine_layers)
        self._input_skips = set(self.input_skips)
        self._skip_affine_trans = self.skip_affine_trans

    def _make_affine_layer(self, input_dim, hidden_dim):
        l1 = torch.nn.Linear(input_dim, hidden_dim * 2)
        l2 = torch.nn.Linear(hidden_dim * 2, hidden_dim * 2)
        if self.use_xavier_init:
            _xavier_init(l1)
            _xavier_init(l2)
        return torch.nn.Sequential(l1, torch.nn.ReLU(True), l2)

    def _apply_affine_layer(self, layer, x, z):
        mu_log_std = layer(z)
        mu, log_std = mu_log_std.split(mu_log_std.shape[-1] // 2, dim=-1)
        std = torch.nn.functional.softplus(log_std)
        return (x - mu) * std

    def forward(self, x: torch.Tensor, z: Optional[torch.Tensor]=None):
        """
        Args:
            x: The input tensor of shape `(..., input_dim)`.
            z: The input skip tensor of shape `(..., skip_dim)` which is appended
                to layers whose indices are specified by `input_skips`.
        Returns:
            y: The output tensor of shape `(..., output_dim)`.
        """
        y = x
        if z is None:
            z = x
        skipi = 0
        for li, layer in enumerate(self.mlp):
            if li in self._input_skips:
                if self._skip_affine_trans:
                    y = self._apply_affine_layer(self.skip_affines[skipi], y, z)
                else:
                    y = torch.cat((y, z), dim=-1)
                skipi += 1
            y = layer(y)
        return y


class RayBundle(NamedTuple):
    """
    Parametrizes points along projection rays by storing:

        origins: A tensor of shape `(..., 3)` denoting the
            origins of the sampling rays in world coords.
        directions: A tensor of shape `(..., 3)` containing the direction
            vectors of sampling rays in world coords. They don't have to be normalized;
            they define unit vectors in the respective 1D coordinate systems; see
            documentation for :func:`ray_bundle_to_ray_points` for the conversion formula.
        lengths: A tensor of shape `(..., num_points_per_ray)`
            containing the lengths at which the rays are sampled.
        xys: A tensor of shape `(..., 2)`, the xy-locations (`xys`) of the ray pixels
    """
    origins: torch.Tensor
    directions: torch.Tensor
    lengths: torch.Tensor
    xys: torch.Tensor


def ray_bundle_variables_to_ray_points(rays_origins: torch.Tensor, rays_directions: torch.Tensor, rays_lengths: torch.Tensor) ->torch.Tensor:
    """
    Converts rays parametrized with origins and directions
    to 3D points by extending each ray according to the corresponding
    ray length:

    E.g. for 2 dimensional input tensors `rays_origins`, `rays_directions`
    and `rays_lengths`, the ray point at position `[i, j]` is::

            rays_points[i, j, :] = (
                rays_origins[i, :]
                + rays_directions[i, :] * rays_lengths[i, j]
            )

    Note that both the directions and magnitudes of the vectors in
    `rays_directions` matter.

    Args:
        rays_origins: A tensor of shape `(..., 3)`
        rays_directions: A tensor of shape `(..., 3)`
        rays_lengths: A tensor of shape `(..., num_points_per_ray)`

    Returns:
        rays_points: A tensor of shape `(..., num_points_per_ray, 3)`
            containing the points sampled along each ray.
    """
    rays_points = rays_origins[..., None, :] + rays_lengths[..., :, None] * rays_directions[..., None, :]
    return rays_points


class NeuralRadianceField(torch.nn.Module):

    def __init__(self, n_harmonic_functions_xyz: int=6, n_harmonic_functions_dir: int=4, n_hidden_neurons_xyz: int=256, n_hidden_neurons_dir: int=128, n_layers_xyz: int=8, append_xyz: Tuple[int, ...]=(5,), use_multiple_streams: bool=True, **kwargs):
        """
        Args:
            n_harmonic_functions_xyz: The number of harmonic functions
                used to form the harmonic embedding of 3D point locations.
            n_harmonic_functions_dir: The number of harmonic functions
                used to form the harmonic embedding of the ray directions.
            n_hidden_neurons_xyz: The number of hidden units in the
                fully connected layers of the MLP that accepts the 3D point
                locations and outputs the occupancy field with the intermediate
                features.
            n_hidden_neurons_dir: The number of hidden units in the
                fully connected layers of the MLP that accepts the intermediate
                features and ray directions and outputs the radiance field
                (per-point colors).
            n_layers_xyz: The number of layers of the MLP that outputs the
                occupancy field.
            append_xyz: The list of indices of the skip layers of the occupancy MLP.
            use_multiple_streams: Whether density and color should be calculated on
                separate CUDA streams.
        """
        super().__init__()
        self.harmonic_embedding_xyz = HarmonicEmbedding(n_harmonic_functions_xyz)
        self.harmonic_embedding_dir = HarmonicEmbedding(n_harmonic_functions_dir)
        embedding_dim_xyz = n_harmonic_functions_xyz * 2 * 3 + 3
        embedding_dim_dir = n_harmonic_functions_dir * 2 * 3 + 3
        self.mlp_xyz = MLPWithInputSkips(n_layers_xyz, embedding_dim_xyz, n_hidden_neurons_xyz, embedding_dim_xyz, n_hidden_neurons_xyz, input_skips=append_xyz)
        self.intermediate_linear = torch.nn.Linear(n_hidden_neurons_xyz, n_hidden_neurons_xyz)
        _xavier_init(self.intermediate_linear)
        self.density_layer = torch.nn.Linear(n_hidden_neurons_xyz, 1)
        _xavier_init(self.density_layer)
        self.density_layer.bias.data[:] = 0.0
        self.color_layer = torch.nn.Sequential(LinearWithRepeat(n_hidden_neurons_xyz + embedding_dim_dir, n_hidden_neurons_dir), torch.nn.ReLU(True), torch.nn.Linear(n_hidden_neurons_dir, 3), torch.nn.Sigmoid())
        self.use_multiple_streams = use_multiple_streams

    def _get_densities(self, features: torch.Tensor, depth_values: torch.Tensor, density_noise_std: float) ->torch.Tensor:
        """
        This function takes `features` predicted by `self.mlp_xyz`
        and converts them to `raw_densities` with `self.density_layer`.
        `raw_densities` are later re-weighted using the depth step sizes
        and mapped to [0-1] range with 1 - inverse exponential of `raw_densities`.
        """
        raw_densities = self.density_layer(features)
        deltas = torch.cat((depth_values[..., 1:] - depth_values[..., :-1], 10000000000.0 * torch.ones_like(depth_values[..., :1])), dim=-1)[..., None]
        if density_noise_std > 0.0:
            raw_densities = raw_densities + torch.randn_like(raw_densities) * density_noise_std
        densities = 1 - (-deltas * torch.relu(raw_densities)).exp()
        return densities

    def _get_colors(self, features: torch.Tensor, rays_directions: torch.Tensor) ->torch.Tensor:
        """
        This function takes per-point `features` predicted by `self.mlp_xyz`
        and evaluates the color model in order to attach to each
        point a 3D vector of its RGB color.
        """
        rays_directions_normed = torch.nn.functional.normalize(rays_directions, dim=-1)
        rays_embedding = self.harmonic_embedding_dir(rays_directions_normed)
        return self.color_layer((self.intermediate_linear(features), rays_embedding))

    def _get_densities_and_colors(self, features: torch.Tensor, ray_bundle: RayBundle, density_noise_std: float) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        The second part of the forward calculation.

        Args:
            features: the output of the common mlp (the prior part of the
                calculation), shape
                (minibatch x ... x self.n_hidden_neurons_xyz).
            ray_bundle: As for forward().
            density_noise_std:  As for forward().

        Returns:
            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`
                denoting the opacity of each ray point.
            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`
                denoting the color of each ray point.
        """
        if self.use_multiple_streams and features.is_cuda:
            current_stream = torch.cuda.current_stream(features.device)
            other_stream = torch.Stream(features.device)
            other_stream.wait_stream(current_stream)
            with torch.cuda.stream(other_stream):
                rays_densities = self._get_densities(features, ray_bundle.lengths, density_noise_std)
            rays_colors = self._get_colors(features, ray_bundle.directions)
            current_stream.wait_stream(other_stream)
        else:
            rays_densities = self._get_densities(features, ray_bundle.lengths, density_noise_std)
            rays_colors = self._get_colors(features, ray_bundle.directions)
        return rays_densities, rays_colors

    def forward(self, ray_bundle: RayBundle, density_noise_std: float=0.0, **kwargs) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        The forward function accepts the parametrizations of
        3D points sampled along projection rays. The forward
        pass is responsible for attaching a 3D vector
        and a 1D scalar representing the point's
        RGB color and opacity respectively.

        Args:
            ray_bundle: A RayBundle object containing the following variables:
                origins: A tensor of shape `(minibatch, ..., 3)` denoting the
                    origins of the sampling rays in world coords.
                directions: A tensor of shape `(minibatch, ..., 3)`
                    containing the direction vectors of sampling rays in world coords.
                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`
                    containing the lengths at which the rays are sampled.
            density_noise_std: A floating point value representing the
                variance of the random normal noise added to the output of
                the opacity function. This can prevent floating artifacts.

        Returns:
            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`
                denoting the opacity of each ray point.
            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`
                denoting the color of each ray point.
        """
        rays_points_world = ray_bundle_to_ray_points(ray_bundle)
        embeds_xyz = self.harmonic_embedding_xyz(rays_points_world)
        features = self.mlp_xyz(embeds_xyz, embeds_xyz)
        rays_densities, rays_colors = self._get_densities_and_colors(features, ray_bundle, density_noise_std)
        return rays_densities, rays_colors


BROADCAST_TYPES = float, int, list, tuple, torch.Tensor, np.ndarray


Device = Union[str, torch.device]


class TensorAccessor(nn.Module):
    """
    A helper class to be used with the __getitem__ method. This can be used for
    getting/setting the values for an attribute of a class at one particular
    index.  This is useful when the attributes of a class are batched tensors
    and one element in the batch needs to be modified.
    """

    def __init__(self, class_object, index: Union[int, slice]) ->None:
        """
        Args:
            class_object: this should be an instance of a class which has
                attributes which are tensors representing a batch of
                values.
            index: int/slice, an index indicating the position in the batch.
                In __setattr__ and __getattr__ only the value of class
                attributes at this index will be accessed.
        """
        self.__dict__['class_object'] = class_object
        self.__dict__['index'] = index

    def __setattr__(self, name: str, value: Any):
        """
        Update the attribute given by `name` to the value given by `value`
        at the index specified by `self.index`.

        Args:
            name: str, name of the attribute.
            value: value to set the attribute to.
        """
        v = getattr(self.class_object, name)
        if not torch.is_tensor(v):
            msg = 'Can only set values on attributes which are tensors; got %r'
            raise AttributeError(msg % type(v))
        if not torch.is_tensor(value):
            value = torch.tensor(value, device=v.device, dtype=v.dtype, requires_grad=v.requires_grad)
        if v.dim() > 1 and value.dim() > 1 and value.shape[1:] != v.shape[1:]:
            msg = 'Expected value to have shape %r; got %r'
            raise ValueError(msg % (v.shape, value.shape))
        if v.dim() == 0 and isinstance(self.index, slice) and len(value) != len(self.index):
            msg = 'Expected value to have len %r; got %r'
            raise ValueError(msg % (len(self.index), len(value)))
        self.class_object.__dict__[name][self.index] = value

    def __getattr__(self, name: str):
        """
        Return the value of the attribute given by "name" on self.class_object
        at the index specified in self.index.

        Args:
            name: string of the attribute name
        """
        if hasattr(self.class_object, name):
            return self.class_object.__dict__[name][self.index]
        else:
            msg = 'Attribute %s not found on %r'
            return AttributeError(msg % (name, self.class_object.__name__))


def make_device(device: Device) ->torch.device:
    """
    Makes an actual torch.device object from the device specified as
    either a string or torch.device object. If the device is `cuda` without
    a specific index, the index of the current device is assigned.

    Args:
        device: Device (as str or torch.device)

    Returns:
        A matching torch.device object
    """
    device = torch.device(device) if isinstance(device, str) else device
    if device.type == 'cuda' and device.index is None:
        device = torch.device(f'cuda:{torch.cuda.current_device()}')
    return device


def format_tensor(input, dtype: torch.dtype=torch.float32, device: Device='cpu') ->torch.Tensor:
    """
    Helper function for converting a scalar value to a tensor.

    Args:
        input: Python scalar, Python list/tuple, torch scalar, 1D torch tensor
        dtype: data type for the input
        device: Device (as str or torch.device) on which the tensor should be placed.

    Returns:
        input_vec: torch tensor with optional added batch dimension.
    """
    device_ = make_device(device)
    if not torch.is_tensor(input):
        input = torch.tensor(input, dtype=dtype, device=device_)
    if input.dim() == 0:
        input = input.view(1)
    if input.device == device_:
        return input
    input = input
    return input


def convert_to_tensors_and_broadcast(*args, dtype: torch.dtype=torch.float32, device: Device='cpu'):
    """
    Helper function to handle parsing an arbitrary number of inputs (*args)
    which all need to have the same batch dimension.
    The output is a list of tensors.

    Args:
        *args: an arbitrary number of inputs
            Each of the values in `args` can be one of the following
                - Python scalar
                - Torch scalar
                - Torch tensor of shape (N, K_i) or (1, K_i) where K_i are
                  an arbitrary number of dimensions which can vary for each
                  value in args. In this case each input is broadcast to a
                  tensor of shape (N, K_i)
        dtype: data type to use when creating new tensors.
        device: torch device on which the tensors should be placed.

    Output:
        args: A list of tensors of shape (N, K_i)
    """
    args_1d = [format_tensor(c, dtype, device) for c in args]
    sizes = [c.shape[0] for c in args_1d]
    N = max(sizes)
    args_Nd = []
    for c in args_1d:
        if c.shape[0] != 1 and c.shape[0] != N:
            msg = 'Got non-broadcastable sizes %r' % sizes
            raise ValueError(msg)
        expand_sizes = (N,) + (-1,) * len(c.shape[1:])
        args_Nd.append(c.expand(*expand_sizes))
    return args_Nd


class TensorProperties(nn.Module):
    """
    A mix-in class for storing tensors as properties with helper methods.
    """

    def __init__(self, dtype: torch.dtype=torch.float32, device: Device='cpu', **kwargs) ->None:
        """
        Args:
            dtype: data type to set for the inputs
            device: Device (as str or torch.device)
            kwargs: any number of keyword arguments. Any arguments which are
                of type (float/int/list/tuple/tensor/array) are broadcasted and
                other keyword arguments are set as attributes.
        """
        super().__init__()
        self.device = make_device(device)
        self._N = 0
        if kwargs is not None:
            args_to_broadcast = {}
            for k, v in kwargs.items():
                if v is None or isinstance(v, (str, bool)):
                    setattr(self, k, v)
                elif isinstance(v, BROADCAST_TYPES):
                    args_to_broadcast[k] = v
                else:
                    msg = 'Arg %s with type %r is not broadcastable'
                    warnings.warn(msg % (k, type(v)))
            names = args_to_broadcast.keys()
            values = tuple(v for v in args_to_broadcast.values())
            if len(values) > 0:
                broadcasted_values = convert_to_tensors_and_broadcast(*values, device=device)
                for i, n in enumerate(names):
                    setattr(self, n, broadcasted_values[i])
                    if self._N == 0:
                        self._N = broadcasted_values[i].shape[0]

    def __len__(self) ->int:
        return self._N

    def isempty(self) ->bool:
        return self._N == 0

    def __getitem__(self, index: Union[int, slice]) ->TensorAccessor:
        """

        Args:
            index: an int or slice used to index all the fields.

        Returns:
            if `index` is an index int/slice return a TensorAccessor class
            with getattribute/setattribute methods which return/update the value
            at the index in the original class.
        """
        if isinstance(index, (int, slice)):
            return TensorAccessor(class_object=self, index=index)
        msg = 'Expected index of type int or slice; got %r'
        raise ValueError(msg % type(index))

    def to(self, device: Device='cpu') ->'TensorProperties':
        """
        In place operation to move class properties which are tensors to a
        specified device. If self has a property "device", update this as well.
        """
        device_ = make_device(device)
        for k in dir(self):
            v = getattr(self, k)
            if k == 'device':
                setattr(self, k, device_)
            if torch.is_tensor(v) and v.device != device_:
                setattr(self, k, v)
        return self

    def cpu(self) ->'TensorProperties':
        return self

    def cuda(self, device: Optional[int]=None) ->'TensorProperties':
        return self

    def clone(self, other) ->'TensorProperties':
        """
        Update the tensor properties of other with the cloned properties of self.
        """
        for k in dir(self):
            v = getattr(self, k)
            if inspect.ismethod(v) or k.startswith('__') or type(v) is TypeVar:
                continue
            if torch.is_tensor(v):
                v_clone = v.clone()
            else:
                v_clone = copy.deepcopy(v)
            setattr(other, k, v_clone)
        return other

    def gather_props(self, batch_idx) ->'TensorProperties':
        """
        This is an in place operation to reformat all tensor class attributes
        based on a set of given indices using torch.gather. This is useful when
        attributes which are batched tensors e.g. shape (N, 3) need to be
        multiplied with another tensor which has a different first dimension
        e.g. packed vertices of shape (V, 3).

        Example

        .. code-block:: python

            self.specular_color = (N, 3) tensor of specular colors for each mesh

        A lighting calculation may use

        .. code-block:: python

            verts_packed = meshes.verts_packed()  # (V, 3)

        To multiply these two tensors the batch dimension needs to be the same.
        To achieve this we can do

        .. code-block:: python

            batch_idx = meshes.verts_packed_to_mesh_idx()  # (V)

        This gives index of the mesh for each vertex in verts_packed.

        .. code-block:: python

            self.gather_props(batch_idx)
            self.specular_color = (V, 3) tensor with the specular color for
                                     each packed vertex.

        torch.gather requires the index tensor to have the same shape as the
        input tensor so this method takes care of the reshaping of the index
        tensor to use with class attributes with arbitrary dimensions.

        Args:
            batch_idx: shape (B, ...) where `...` represents an arbitrary
                number of dimensions

        Returns:
            self with all properties reshaped. e.g. a property with shape (N, 3)
            is transformed to shape (B, 3).
        """
        for k in dir(self):
            v = getattr(self, k)
            if torch.is_tensor(v):
                if v.shape[0] > 1:
                    _batch_idx = batch_idx.clone()
                    idx_dims = _batch_idx.shape
                    tensor_dims = v.shape
                    if len(idx_dims) > len(tensor_dims):
                        msg = 'batch_idx cannot have more dimensions than %s. '
                        msg += 'got shape %r and %s has shape %r'
                        raise ValueError(msg % (k, idx_dims, k, tensor_dims))
                    if idx_dims != tensor_dims:
                        new_dims = len(tensor_dims) - len(idx_dims)
                        new_shape = idx_dims + (1,) * new_dims
                        expand_dims = (-1,) + tensor_dims[1:]
                        _batch_idx = _batch_idx.view(*new_shape)
                        _batch_idx = _batch_idx.expand(*expand_dims)
                    v = v.gather(0, _batch_idx)
                    setattr(self, k, v)
        return self


def _safe_det_3x3(t: torch.Tensor):
    """
    Fast determinant calculation for a batch of 3x3 matrices.

    Note, result of this function might not be the same as `torch.det()`.
    The differences might be in the last significant digit.

    Args:
        t: Tensor of shape (N, 3, 3).

    Returns:
        Tensor of shape (N) with determinants.
    """
    det = t[..., 0, 0] * (t[..., 1, 1] * t[..., 2, 2] - t[..., 1, 2] * t[..., 2, 1]) - t[..., 0, 1] * (t[..., 1, 0] * t[..., 2, 2] - t[..., 2, 0] * t[..., 1, 2]) + t[..., 0, 2] * (t[..., 1, 0] * t[..., 2, 1] - t[..., 2, 0] * t[..., 1, 1])
    return det


@torch.no_grad()
def _check_valid_rotation_matrix(R, tol: float=1e-07) ->None:
    """
    Determine if R is a valid rotation matrix by checking it satisfies the
    following conditions:

    ``RR^T = I and det(R) = 1``

    Args:
        R: an (N, 3, 3) matrix

    Returns:
        None

    Emits a warning if R is an invalid rotation matrix.
    """
    N = R.shape[0]
    eye = torch.eye(3, dtype=R.dtype, device=R.device)
    eye = eye.view(1, 3, 3).expand(N, -1, -1)
    orthogonal = torch.allclose(R.bmm(R.transpose(1, 2)), eye, atol=tol)
    det_R = _safe_det_3x3(R)
    no_distortion = torch.allclose(det_R, torch.ones_like(det_R))
    if not (orthogonal and no_distortion):
        msg = 'R is not a valid rotation matrix'
        warnings.warn(msg)
    return


def get_device(x, device: Optional[Device]=None) ->torch.device:
    """
    Gets the device of the specified variable x if it is a tensor, or
    falls back to a default CPU device otherwise. Allows overriding by
    providing an explicit device.

    Args:
        x: a torch.Tensor to get the device from or another type
        device: Device (as str or torch.device) to fall back to

    Returns:
        A matching torch.device object
    """
    if device is not None:
        return make_device(device)
    if torch.is_tensor(x):
        return x.device
    return torch.device('cpu')


def _axis_angle_rotation(axis: str, angle: torch.Tensor) ->torch.Tensor:
    """
    Return the rotation matrices for one of the rotations about an axis
    of which Euler angles describe, for each value of the angle given.

    Args:
        axis: Axis label "X" or "Y or "Z".
        angle: any shape tensor of Euler angles in radians

    Returns:
        Rotation matrices as tensor of shape (..., 3, 3).
    """
    cos = torch.cos(angle)
    sin = torch.sin(angle)
    one = torch.ones_like(angle)
    zero = torch.zeros_like(angle)
    if axis == 'X':
        R_flat = one, zero, zero, zero, cos, -sin, zero, sin, cos
    elif axis == 'Y':
        R_flat = cos, zero, sin, zero, one, zero, -sin, zero, cos
    elif axis == 'Z':
        R_flat = cos, -sin, zero, sin, cos, zero, zero, zero, one
    else:
        raise ValueError('letter must be either X, Y or Z.')
    return torch.stack(R_flat, -1).reshape(angle.shape + (3, 3))


def _handle_coord(c, dtype: torch.dtype, device: torch.device) ->torch.Tensor:
    """
    Helper function for _handle_input.

    Args:
        c: Python scalar, torch scalar, or 1D torch tensor

    Returns:
        c_vec: 1D torch tensor
    """
    if not torch.is_tensor(c):
        c = torch.tensor(c, dtype=dtype, device=device)
    if c.dim() == 0:
        c = c.view(1)
    if c.device != device or c.dtype != dtype:
        c = c
    return c


def _handle_angle_input(x, dtype: torch.dtype, device: Optional[Device], name: str) ->torch.Tensor:
    """
    Helper function for building a rotation function using angles.
    The output is always of shape (N,).

    The input can be one of:
        - Torch tensor of shape (N,)
        - Python scalar
        - Torch scalar
    """
    device_ = get_device(x, device)
    if torch.is_tensor(x) and x.dim() > 1:
        msg = 'Expected tensor of shape (N,); got %r (in %s)'
        raise ValueError(msg % (x.shape, name))
    else:
        return _handle_coord(x, dtype, device_)


def _handle_input(x, y, z, dtype: torch.dtype, device: Optional[Device], name: str, allow_singleton: bool=False) ->torch.Tensor:
    """
    Helper function to handle parsing logic for building transforms. The output
    is always a tensor of shape (N, 3), but there are several types of allowed
    input.

    Case I: Single Matrix
        In this case x is a tensor of shape (N, 3), and y and z are None. Here just
        return x.

    Case II: Vectors and Scalars
        In this case each of x, y, and z can be one of the following
            - Python scalar
            - Torch scalar
            - Torch tensor of shape (N, 1) or (1, 1)
        In this case x, y and z are broadcast to tensors of shape (N, 1)
        and concatenated to a tensor of shape (N, 3)

    Case III: Singleton (only if allow_singleton=True)
        In this case y and z are None, and x can be one of the following:
            - Python scalar
            - Torch scalar
            - Torch tensor of shape (N, 1) or (1, 1)
        Here x will be duplicated 3 times, and we return a tensor of shape (N, 3)

    Returns:
        xyz: Tensor of shape (N, 3)
    """
    device_ = get_device(x, device)
    if torch.is_tensor(x) and x.dim() == 2:
        if x.shape[1] != 3:
            msg = 'Expected tensor of shape (N, 3); got %r (in %s)'
            raise ValueError(msg % (x.shape, name))
        if y is not None or z is not None:
            msg = 'Expected y and z to be None (in %s)' % name
            raise ValueError(msg)
        return x
    if allow_singleton and y is None and z is None:
        y = x
        z = x
    xyz = [_handle_coord(c, dtype, device_) for c in [x, y, z]]
    sizes = [c.shape[0] for c in xyz]
    N = max(sizes)
    for c in xyz:
        if c.shape[0] != 1 and c.shape[0] != N:
            msg = 'Got non-broadcastable sizes %r (in %s)' % (sizes, name)
            raise ValueError(msg)
    xyz = [c.expand(N) for c in xyz]
    xyz = torch.stack(xyz, dim=1)
    return xyz


def _broadcast_bmm(a, b) ->torch.Tensor:
    """
    Batch multiply two matrices and broadcast if necessary.

    Args:
        a: torch tensor of shape (P, K) or (M, P, K)
        b: torch tensor of shape (N, K, K)

    Returns:
        a and b broadcast multiplied. The output batch dimension is max(N, M).

    To broadcast transforms across a batch dimension if M != N then
    expect that either M = 1 or N = 1. The tensor with batch dimension 1 is
    expanded to have shape N or M.
    """
    if a.dim() == 2:
        a = a[None]
    if len(a) != len(b):
        if not (len(a) == 1 or len(b) == 1):
            msg = 'Expected batch dim for bmm to be equal or 1; got %r, %r'
            raise ValueError(msg % (a.shape, b.shape))
        if len(a) == 1:
            a = a.expand(len(b), -1, -1)
        if len(b) == 1:
            b = b.expand(len(a), -1, -1)
    return a.bmm(b)


def hat(v: torch.Tensor) ->torch.Tensor:
    """
    Compute the Hat operator [1] of a batch of 3D vectors.

    Args:
        v: Batch of vectors of shape `(minibatch , 3)`.

    Returns:
        Batch of skew-symmetric matrices of shape
        `(minibatch, 3 , 3)` where each matrix is of the form:
            `[    0  -v_z   v_y ]
             [  v_z     0  -v_x ]
             [ -v_y   v_x     0 ]`

    Raises:
        ValueError if `v` is of incorrect shape.

    [1] https://en.wikipedia.org/wiki/Hat_operator
    """
    N, dim = v.shape
    if dim != 3:
        raise ValueError('Input vectors have to be 3-dimensional.')
    h = torch.zeros((N, 3, 3), dtype=v.dtype, device=v.device)
    x, y, z = v.unbind(1)
    h[:, 0, 1] = -z
    h[:, 0, 2] = y
    h[:, 1, 0] = z
    h[:, 1, 2] = -x
    h[:, 2, 0] = -y
    h[:, 2, 1] = x
    return h


def _get_se3_V_input(log_rotation: torch.Tensor, eps: float=0.0001):
    """
    A helper function that computes the input variables to the `_se3_V_matrix`
    function.
    """
    nrms = (log_rotation ** 2).sum(-1)
    rotation_angles = torch.clamp(nrms, eps).sqrt()
    log_rotation_hat = hat(log_rotation)
    log_rotation_hat_square = torch.bmm(log_rotation_hat, log_rotation_hat)
    return log_rotation, log_rotation_hat, log_rotation_hat_square, rotation_angles


def _se3_V_matrix(log_rotation: torch.Tensor, log_rotation_hat: torch.Tensor, log_rotation_hat_square: torch.Tensor, rotation_angles: torch.Tensor, eps: float=0.0001) ->torch.Tensor:
    """
    A helper function that computes the "V" matrix from [1], Sec 9.4.2.
    [1] https://jinyongjeong.github.io/Download/SE3/jlblanco2010geometry3d_techrep.pdf
    """
    V = torch.eye(3, dtype=log_rotation.dtype, device=log_rotation.device)[None] + log_rotation_hat * ((1 - torch.cos(rotation_angles)) / rotation_angles ** 2)[:, None, None] + log_rotation_hat_square * ((rotation_angles - torch.sin(rotation_angles)) / rotation_angles ** 3)[:, None, None]
    return V


def hat_inv(h: torch.Tensor) ->torch.Tensor:
    """
    Compute the inverse Hat operator [1] of a batch of 3x3 matrices.

    Args:
        h: Batch of skew-symmetric matrices of shape `(minibatch, 3, 3)`.

    Returns:
        Batch of 3d vectors of shape `(minibatch, 3, 3)`.

    Raises:
        ValueError if `h` is of incorrect shape.
        ValueError if `h` not skew-symmetric.

    [1] https://en.wikipedia.org/wiki/Hat_operator
    """
    N, dim1, dim2 = h.shape
    if dim1 != 3 or dim2 != 3:
        raise ValueError('Input has to be a batch of 3x3 Tensors.')
    ss_diff = torch.abs(h + h.permute(0, 2, 1)).max()
    HAT_INV_SKEW_SYMMETRIC_TOL = 1e-05
    if float(ss_diff) > HAT_INV_SKEW_SYMMETRIC_TOL:
        raise ValueError('One of input matrices is not skew-symmetric.')
    x = h[:, 2, 1]
    y = h[:, 0, 2]
    z = h[:, 1, 0]
    v = torch.stack((x, y, z), dim=1)
    return v


def _dacos_dx(x: float) ->float:
    """
    Calculates the derivative of `arccos(x)` w.r.t. `x`.
    """
    return -1.0 / math.sqrt(1.0 - x * x)


def _acos_linear_approximation(x: torch.Tensor, x0: float) ->torch.Tensor:
    """
    Calculates the 1st order Taylor expansion of `arccos(x)` around `x0`.
    """
    return (x - x0) * _dacos_dx(x0) + math.acos(x0)


def so3_rotation_angle(R: torch.Tensor, eps: float=0.0001, cos_angle: bool=False, cos_bound: float=0.0001) ->torch.Tensor:
    """
    Calculates angles (in radians) of a batch of rotation matrices `R` with
    `angle = acos(0.5 * (Trace(R)-1))`. The trace of the
    input matrices is checked to be in the valid range `[-1-eps,3+eps]`.
    The `eps` argument is a small constant that allows for small errors
    caused by limited machine precision.

    Args:
        R: Batch of rotation matrices of shape `(minibatch, 3, 3)`.
        eps: Tolerance for the valid trace check.
        cos_angle: If==True return cosine of the rotation angles rather than
            the angle itself. This can avoid the unstable
            calculation of `acos`.
        cos_bound: Clamps the cosine of the rotation angle to
            [-1 + cos_bound, 1 - cos_bound] to avoid non-finite outputs/gradients
            of the `acos` call. Note that the non-finite outputs/gradients
            are returned when the angle is requested (i.e. `cos_angle==False`)
            and the rotation angle is close to 0 or π.

    Returns:
        Corresponding rotation angles of shape `(minibatch,)`.
        If `cos_angle==True`, returns the cosine of the angles.

    Raises:
        ValueError if `R` is of incorrect shape.
        ValueError if `R` has an unexpected trace.
    """
    N, dim1, dim2 = R.shape
    if dim1 != 3 or dim2 != 3:
        raise ValueError('Input has to be a batch of 3x3 Tensors.')
    rot_trace = R[:, 0, 0] + R[:, 1, 1] + R[:, 2, 2]
    if ((rot_trace < -1.0 - eps) + (rot_trace > 3.0 + eps)).any():
        raise ValueError('A matrix has trace outside valid range [-1-eps,3+eps].')
    phi_cos = (rot_trace - 1.0) * 0.5
    if cos_angle:
        return phi_cos
    elif cos_bound > 0.0:
        bound = 1.0 - cos_bound
        return acos_linear_extrapolation(phi_cos, (-bound, bound))
    else:
        return torch.acos(phi_cos)


def so3_log_map(R: torch.Tensor, eps: float=0.0001, cos_bound: float=0.0001) ->torch.Tensor:
    """
    Convert a batch of 3x3 rotation matrices `R`
    to a batch of 3-dimensional matrix logarithms of rotation matrices
    The conversion has a singularity around `(R=I)` which is handled
    by clamping controlled with the `eps` and `cos_bound` arguments.

    Args:
        R: batch of rotation matrices of shape `(minibatch, 3, 3)`.
        eps: A float constant handling the conversion singularity.
        cos_bound: Clamps the cosine of the rotation angle to
            [-1 + cos_bound, 1 - cos_bound] to avoid non-finite outputs/gradients
            of the `acos` call when computing `so3_rotation_angle`.
            Note that the non-finite outputs/gradients are returned when
            the rotation angle is close to 0 or π.

    Returns:
        Batch of logarithms of input rotation matrices
        of shape `(minibatch, 3)`.

    Raises:
        ValueError if `R` is of incorrect shape.
        ValueError if `R` has an unexpected trace.
    """
    N, dim1, dim2 = R.shape
    if dim1 != 3 or dim2 != 3:
        raise ValueError('Input has to be a batch of 3x3 Tensors.')
    phi = so3_rotation_angle(R, cos_bound=cos_bound, eps=eps)
    phi_sin = torch.sin(phi)
    phi_factor = torch.empty_like(phi)
    ok_denom = phi_sin.abs() > 0.5 * eps
    phi_factor[~ok_denom] = 0.5 + phi[~ok_denom] ** 2 * (1.0 / 12)
    phi_factor[ok_denom] = phi[ok_denom] / (2.0 * phi_sin[ok_denom])
    log_rot_hat = phi_factor[:, None, None] * (R - R.permute(0, 2, 1))
    log_rot = hat_inv(log_rot_hat)
    return log_rot


def se3_log_map(transform: torch.Tensor, eps: float=0.0001, cos_bound: float=0.0001) ->torch.Tensor:
    """
    Convert a batch of 4x4 transformation matrices `transform`
    to a batch of 6-dimensional SE(3) logarithms of the SE(3) matrices.
    See e.g. [1], Sec 9.4.2. for more detailed description.

    A SE(3) matrix has the following form:
        ```
        [ R 0 ]
        [ T 1 ] ,
        ```
    where `R` is an orthonormal 3x3 rotation matrix and `T` is a 3-D translation vector.
    SE(3) matrices are commonly used to represent rigid motions or camera extrinsics.

    In the SE(3) logarithmic representation SE(3) matrices are
    represented as 6-dimensional vectors `[log_translation | log_rotation]`,
    i.e. a concatenation of two 3D vectors `log_translation` and `log_rotation`.

    The conversion from the 4x4 SE(3) matrix `transform` to the
    6D representation `log_transform = [log_translation | log_rotation]`
    is done as follows:
        ```
        log_transform = log(transform)
        log_translation = log_transform[3, :3]
        log_rotation = inv_hat(log_transform[:3, :3])
        ```
    where `log` is the matrix logarithm
    and `inv_hat` is the inverse of the Hat operator [2].

    Note that for any valid 4x4 `transform` matrix, the following identity holds:
    ```
    se3_exp_map(se3_log_map(transform)) == transform
    ```

    The conversion has a singularity around `(transform=I)` which is handled
    by clamping controlled with the `eps` and `cos_bound` arguments.

    Args:
        transform: batch of SE(3) matrices of shape `(minibatch, 4, 4)`.
        eps: A threshold for clipping the squared norm of the rotation logarithm
            to avoid division by zero in the singular case.
        cos_bound: Clamps the cosine of the rotation angle to
            [-1 + cos_bound, 3 - cos_bound] to avoid non-finite outputs.
            The non-finite outputs can be caused by passing small rotation angles
            to the `acos` function in `so3_rotation_angle` of `so3_log_map`.

    Returns:
        Batch of logarithms of input SE(3) matrices
        of shape `(minibatch, 6)`.

    Raises:
        ValueError if `transform` is of incorrect shape.
        ValueError if `R` has an unexpected trace.

    [1] https://jinyongjeong.github.io/Download/SE3/jlblanco2010geometry3d_techrep.pdf
    [2] https://en.wikipedia.org/wiki/Hat_operator
    """
    if transform.ndim != 3:
        raise ValueError('Input tensor shape has to be (N, 4, 4).')
    N, dim1, dim2 = transform.shape
    if dim1 != 4 or dim2 != 4:
        raise ValueError('Input tensor shape has to be (N, 4, 4).')
    if not torch.allclose(transform[:, :3, 3], torch.zeros_like(transform[:, :3, 3])):
        raise ValueError('All elements of `transform[:, :3, 3]` should be 0.')
    R = transform[:, :3, :3].permute(0, 2, 1)
    log_rotation = so3_log_map(R, eps=eps, cos_bound=cos_bound)
    T = transform[:, 3, :3]
    V = _se3_V_matrix(*_get_se3_V_input(log_rotation), eps=eps)
    log_translation = torch.linalg.solve(V, T[:, :, None])[:, :, 0]
    return torch.cat((log_translation, log_rotation), dim=1)


class Transform3d:
    """
    A Transform3d object encapsulates a batch of N 3D transformations, and knows
    how to transform points and normal vectors. Suppose that t is a Transform3d;
    then we can do the following:

    .. code-block:: python

        N = len(t)
        points = torch.randn(N, P, 3)
        normals = torch.randn(N, P, 3)
        points_transformed = t.transform_points(points)    # => (N, P, 3)
        normals_transformed = t.transform_normals(normals)  # => (N, P, 3)


    BROADCASTING
    Transform3d objects supports broadcasting. Suppose that t1 and tN are
    Transform3d objects with len(t1) == 1 and len(tN) == N respectively. Then we
    can broadcast transforms like this:

    .. code-block:: python

        t1.transform_points(torch.randn(P, 3))     # => (P, 3)
        t1.transform_points(torch.randn(1, P, 3))  # => (1, P, 3)
        t1.transform_points(torch.randn(M, P, 3))  # => (M, P, 3)
        tN.transform_points(torch.randn(P, 3))     # => (N, P, 3)
        tN.transform_points(torch.randn(1, P, 3))  # => (N, P, 3)


    COMBINING TRANSFORMS
    Transform3d objects can be combined in two ways: composing and stacking.
    Composing is function composition. Given Transform3d objects t1, t2, t3,
    the following all compute the same thing:

    .. code-block:: python

        y1 = t3.transform_points(t2.transform_points(t1.transform_points(x)))
        y2 = t1.compose(t2).compose(t3).transform_points(x)
        y3 = t1.compose(t2, t3).transform_points(x)


    Composing transforms should broadcast.

    .. code-block:: python

        if len(t1) == 1 and len(t2) == N, then len(t1.compose(t2)) == N.

    We can also stack a sequence of Transform3d objects, which represents
    composition along the batch dimension; then the following should compute the
    same thing.

    .. code-block:: python

        N, M = len(tN), len(tM)
        xN = torch.randn(N, P, 3)
        xM = torch.randn(M, P, 3)
        y1 = torch.cat([tN.transform_points(xN), tM.transform_points(xM)], dim=0)
        y2 = tN.stack(tM).transform_points(torch.cat([xN, xM], dim=0))

    BUILDING TRANSFORMS
    We provide convenience methods for easily building Transform3d objects
    as compositions of basic transforms.

    .. code-block:: python

        # Scale by 0.5, then translate by (1, 2, 3)
        t1 = Transform3d().scale(0.5).translate(1, 2, 3)

        # Scale each axis by a different amount, then translate, then scale
        t2 = Transform3d().scale(1, 3, 3).translate(2, 3, 1).scale(2.0)

        t3 = t1.compose(t2)
        tN = t1.stack(t3, t3)


    BACKPROP THROUGH TRANSFORMS
    When building transforms, we can also parameterize them by Torch tensors;
    in this case we can backprop through the construction and application of
    Transform objects, so they could be learned via gradient descent or
    predicted by a neural network.

    .. code-block:: python

        s1_params = torch.randn(N, requires_grad=True)
        t_params = torch.randn(N, 3, requires_grad=True)
        s2_params = torch.randn(N, 3, requires_grad=True)

        t = Transform3d().scale(s1_params).translate(t_params).scale(s2_params)
        x = torch.randn(N, 3)
        y = t.transform_points(x)
        loss = compute_loss(y)
        loss.backward()

        with torch.no_grad():
            s1_params -= lr * s1_params.grad
            t_params -= lr * t_params.grad
            s2_params -= lr * s2_params.grad

    CONVENTIONS
    We adopt a right-hand coordinate system, meaning that rotation about an axis
    with a positive angle results in a counter clockwise rotation.

    This class assumes that transformations are applied on inputs which
    are row vectors. The internal representation of the Nx4x4 transformation
    matrix is of the form:

    .. code-block:: python

        M = [
                [Rxx, Ryx, Rzx, 0],
                [Rxy, Ryy, Rzy, 0],
                [Rxz, Ryz, Rzz, 0],
                [Tx,  Ty,  Tz,  1],
            ]

    To apply the transformation to points, which are row vectors, the latter are
    converted to homogeneous (4D) coordinates and right-multiplied by the M matrix:

    .. code-block:: python

        points = [[0, 1, 2]]  # (1 x 3) xyz coordinates of a point
        [transformed_points, 1] ∝ [points, 1] @ M

    """

    def __init__(self, dtype: torch.dtype=torch.float32, device: Device='cpu', matrix: Optional[torch.Tensor]=None) ->None:
        """
        Args:
            dtype: The data type of the transformation matrix.
                to be used if `matrix = None`.
            device: The device for storing the implemented transformation.
                If `matrix != None`, uses the device of input `matrix`.
            matrix: A tensor of shape (4, 4) or of shape (minibatch, 4, 4)
                representing the 4x4 3D transformation matrix.
                If `None`, initializes with identity using
                the specified `device` and `dtype`.
        """
        if matrix is None:
            self._matrix = torch.eye(4, dtype=dtype, device=device).view(1, 4, 4)
        else:
            if matrix.ndim not in (2, 3):
                raise ValueError('"matrix" has to be a 2- or a 3-dimensional tensor.')
            if matrix.shape[-2] != 4 or matrix.shape[-1] != 4:
                raise ValueError('"matrix" has to be a tensor of shape (minibatch, 4, 4) or (4, 4).')
            dtype = matrix.dtype
            device = matrix.device
            self._matrix = matrix.view(-1, 4, 4)
        self._transforms = []
        self._lu = None
        self.device = make_device(device)
        self.dtype = dtype

    def __len__(self) ->int:
        return self.get_matrix().shape[0]

    def __getitem__(self, index: Union[int, List[int], slice, torch.BoolTensor, torch.LongTensor]) ->'Transform3d':
        """
        Args:
            index: Specifying the index of the transform to retrieve.
                Can be an int, slice, list of ints, boolean, long tensor.
                Supports negative indices.

        Returns:
            Transform3d object with selected transforms. The tensors are not cloned.
        """
        if isinstance(index, int):
            index = [index]
        return self.__class__(matrix=self.get_matrix()[index])

    def compose(self, *others: 'Transform3d') ->'Transform3d':
        """
        Return a new Transform3d representing the composition of self with the
        given other transforms, which will be stored as an internal list.

        Args:
            *others: Any number of Transform3d objects

        Returns:
            A new Transform3d with the stored transforms
        """
        out = Transform3d(dtype=self.dtype, device=self.device)
        out._matrix = self._matrix.clone()
        for other in others:
            if not isinstance(other, Transform3d):
                msg = 'Only possible to compose Transform3d objects; got %s'
                raise ValueError(msg % type(other))
        out._transforms = self._transforms + list(others)
        return out

    def get_matrix(self) ->torch.Tensor:
        """
        Returns a 4×4 matrix corresponding to each transform in the batch.

        If the transform was composed from others, the matrix for the composite
        transform will be returned.
        For example, if self.transforms contains transforms t1, t2, and t3, and
        given a set of points x, the following should be true:

        .. code-block:: python

            y1 = t1.compose(t2, t3).transform(x)
            y2 = t3.transform(t2.transform(t1.transform(x)))
            y1.get_matrix() == y2.get_matrix()

        Where necessary, those transforms are broadcast against each other.

        Returns:
            A (N, 4, 4) batch of transformation matrices representing
                the stored transforms. See the class documentation for the conventions.
        """
        composed_matrix = self._matrix.clone()
        if len(self._transforms) > 0:
            for other in self._transforms:
                other_matrix = other.get_matrix()
                composed_matrix = _broadcast_bmm(composed_matrix, other_matrix)
        return composed_matrix

    def get_se3_log(self, eps: float=0.0001, cos_bound: float=0.0001) ->torch.Tensor:
        """
        Returns a 6D SE(3) log vector corresponding to each transform in the batch.

        In the SE(3) logarithmic representation SE(3) matrices are
        represented as 6-dimensional vectors `[log_translation | log_rotation]`,
        i.e. a concatenation of two 3D vectors `log_translation` and `log_rotation`.

        The conversion from the 4x4 SE(3) matrix `transform` to the
        6D representation `log_transform = [log_translation | log_rotation]`
        is done as follows::

            log_transform = log(transform.get_matrix())
            log_translation = log_transform[3, :3]
            log_rotation = inv_hat(log_transform[:3, :3])

        where `log` is the matrix logarithm
        and `inv_hat` is the inverse of the Hat operator [2].

        See the docstring for `se3.se3_log_map` and [1], Sec 9.4.2. for more
        detailed description.

        Args:
            eps: A threshold for clipping the squared norm of the rotation logarithm
                to avoid division by zero in the singular case.
            cos_bound: Clamps the cosine of the rotation angle to
                [-1 + cos_bound, 3 - cos_bound] to avoid non-finite outputs.
                The non-finite outputs can be caused by passing small rotation angles
                to the `acos` function in `so3_rotation_angle` of `so3_log_map`.

        Returns:
            A (N, 6) tensor, rows of which represent the individual transforms
            stored in the object as SE(3) logarithms.

        Raises:
            ValueError if the stored transform is not Euclidean (e.g. R is not a rotation
                matrix or the last column has non-zeros in the first three places).

        [1] https://jinyongjeong.github.io/Download/SE3/jlblanco2010geometry3d_techrep.pdf
        [2] https://en.wikipedia.org/wiki/Hat_operator
        """
        return se3_log_map(self.get_matrix(), eps, cos_bound)

    def _get_matrix_inverse(self) ->torch.Tensor:
        """
        Return the inverse of self._matrix.
        """
        return torch.inverse(self._matrix)

    def inverse(self, invert_composed: bool=False) ->'Transform3d':
        """
        Returns a new Transform3d object that represents an inverse of the
        current transformation.

        Args:
            invert_composed:
                - True: First compose the list of stored transformations
                  and then apply inverse to the result. This is
                  potentially slower for classes of transformations
                  with inverses that can be computed efficiently
                  (e.g. rotations and translations).
                - False: Invert the individual stored transformations
                  independently without composing them.

        Returns:
            A new Transform3d object containing the inverse of the original
            transformation.
        """
        tinv = Transform3d(dtype=self.dtype, device=self.device)
        if invert_composed:
            tinv._matrix = torch.inverse(self.get_matrix())
        else:
            i_matrix = self._get_matrix_inverse()
            if len(self._transforms) > 0:
                tinv._transforms = [t.inverse() for t in reversed(self._transforms)]
                last = Transform3d(dtype=self.dtype, device=self.device)
                last._matrix = i_matrix
                tinv._transforms.append(last)
            else:
                tinv._matrix = i_matrix
        return tinv

    def stack(self, *others: 'Transform3d') ->'Transform3d':
        """
        Return a new batched Transform3d representing the batch elements from
        self and all the given other transforms all batched together.

        Args:
            *others: Any number of Transform3d objects

        Returns:
            A new Transform3d.
        """
        transforms = [self] + list(others)
        matrix = torch.cat([t.get_matrix() for t in transforms], dim=0)
        out = Transform3d(dtype=self.dtype, device=self.device)
        out._matrix = matrix
        return out

    def transform_points(self, points, eps: Optional[float]=None) ->torch.Tensor:
        """
        Use this transform to transform a set of 3D points. Assumes row major
        ordering of the input points.

        Args:
            points: Tensor of shape (P, 3) or (N, P, 3)
            eps: If eps!=None, the argument is used to clamp the
                last coordinate before performing the final division.
                The clamping corresponds to:
                last_coord := (last_coord.sign() + (last_coord==0)) *
                torch.clamp(last_coord.abs(), eps),
                i.e. the last coordinates that are exactly 0 will
                be clamped to +eps.

        Returns:
            points_out: points of shape (N, P, 3) or (P, 3) depending
            on the dimensions of the transform
        """
        points_batch = points.clone()
        if points_batch.dim() == 2:
            points_batch = points_batch[None]
        if points_batch.dim() != 3:
            msg = 'Expected points to have dim = 2 or dim = 3: got shape %r'
            raise ValueError(msg % repr(points.shape))
        N, P, _3 = points_batch.shape
        ones = torch.ones(N, P, 1, dtype=points.dtype, device=points.device)
        points_batch = torch.cat([points_batch, ones], dim=2)
        composed_matrix = self.get_matrix()
        points_out = _broadcast_bmm(points_batch, composed_matrix)
        denom = points_out[..., 3:]
        if eps is not None:
            denom_sign = denom.sign() + (denom == 0.0).type_as(denom)
            denom = denom_sign * torch.clamp(denom.abs(), eps)
        points_out = points_out[..., :3] / denom
        if points_out.shape[0] == 1 and points.dim() == 2:
            points_out = points_out.reshape(points.shape)
        return points_out

    def transform_normals(self, normals) ->torch.Tensor:
        """
        Use this transform to transform a set of normal vectors.

        Args:
            normals: Tensor of shape (P, 3) or (N, P, 3)

        Returns:
            normals_out: Tensor of shape (P, 3) or (N, P, 3) depending
            on the dimensions of the transform
        """
        if normals.dim() not in [2, 3]:
            msg = 'Expected normals to have dim = 2 or dim = 3: got shape %r'
            raise ValueError(msg % (normals.shape,))
        composed_matrix = self.get_matrix()
        mat = composed_matrix[:, :3, :3]
        normals_out = _broadcast_bmm(normals, mat.transpose(1, 2).inverse())
        if normals_out.shape[0] == 1 and normals.dim() == 2:
            normals_out = normals_out.reshape(normals.shape)
        return normals_out

    def translate(self, *args, **kwargs) ->'Transform3d':
        return self.compose(Translate(*args, device=self.device, dtype=self.dtype, **kwargs))

    def scale(self, *args, **kwargs) ->'Transform3d':
        return self.compose(Scale(*args, device=self.device, dtype=self.dtype, **kwargs))

    def rotate(self, *args, **kwargs) ->'Transform3d':
        return self.compose(Rotate(*args, device=self.device, dtype=self.dtype, **kwargs))

    def rotate_axis_angle(self, *args, **kwargs) ->'Transform3d':
        return self.compose(RotateAxisAngle(*args, device=self.device, dtype=self.dtype, **kwargs))

    def clone(self) ->'Transform3d':
        """
        Deep copy of Transforms object. All internal tensors are cloned
        individually.

        Returns:
            new Transforms object.
        """
        other = Transform3d(dtype=self.dtype, device=self.device)
        if self._lu is not None:
            other._lu = [elem.clone() for elem in self._lu]
        other._matrix = self._matrix.clone()
        other._transforms = [t.clone() for t in self._transforms]
        return other

    def to(self, device: Device, copy: bool=False, dtype: Optional[torch.dtype]=None) ->'Transform3d':
        """
        Match functionality of torch.Tensor.to()
        If copy = True or the self Tensor is on a different device, the
        returned tensor is a copy of self with the desired torch.device.
        If copy = False and the self Tensor already has the correct torch.device,
        then self is returned.

        Args:
          device: Device (as str or torch.device) for the new tensor.
          copy: Boolean indicator whether or not to clone self. Default False.
          dtype: If not None, casts the internal tensor variables
              to a given torch.dtype.

        Returns:
          Transform3d object.
        """
        device_ = make_device(device)
        dtype_ = self.dtype if dtype is None else dtype
        skip_to = self.device == device_ and self.dtype == dtype_
        if not copy and skip_to:
            return self
        other = self.clone()
        if skip_to:
            return other
        other.device = device_
        other.dtype = dtype_
        other._matrix = other._matrix
        other._transforms = [t for t in other._transforms]
        return other

    def cpu(self) ->'Transform3d':
        return self

    def cuda(self) ->'Transform3d':
        return self


def get_ndc_to_screen_transform(cameras, with_xyflip: bool=False, image_size: Optional[Union[List, Tuple, torch.Tensor]]=None) ->Transform3d:
    """
    PyTorch3D NDC to screen conversion.
    Conversion from PyTorch3D's NDC space (+X left, +Y up) to screen/image space
    (+X right, +Y down, origin top left).

    Args:
        cameras
        with_xyflip: flips x- and y-axis if set to True.
    Optional kwargs:
        image_size: ((height, width),) specifying the height, width
        of the image. If not provided, it reads it from cameras.

    We represent the NDC to screen conversion as a Transform3d
    with projection matrix

    K = [
            [s,   0,    0,  cx],
            [0,   s,    0,  cy],
            [0,   0,    1,   0],
            [0,   0,    0,   1],
    ]

    """
    if image_size is None:
        msg = 'For NDC to screen conversion, image_size=(height, width) needs to be specified.'
        raise ValueError(msg)
    K = torch.zeros((cameras._N, 4, 4), device=cameras.device, dtype=torch.float32)
    if not torch.is_tensor(image_size):
        image_size = torch.tensor(image_size, device=cameras.device)
    image_size = image_size.view(-1, 2)
    height, width = image_size.unbind(1)
    scale = (image_size.min(dim=1).values - 0.0) / 2.0
    K[:, 0, 0] = scale
    K[:, 1, 1] = scale
    K[:, 0, 3] = -1.0 * (width - 0.0) / 2.0
    K[:, 1, 3] = -1.0 * (height - 0.0) / 2.0
    K[:, 2, 2] = 1.0
    K[:, 3, 3] = 1.0
    transform = Transform3d(matrix=K.transpose(1, 2).contiguous(), device=cameras.device)
    if with_xyflip:
        xyflip = torch.eye(4, device=cameras.device, dtype=torch.float32)
        xyflip[0, 0] = -1.0
        xyflip[1, 1] = -1.0
        xyflip = xyflip.view(1, 4, 4).expand(cameras._N, -1, -1)
        xyflip_transform = Transform3d(matrix=xyflip.transpose(1, 2).contiguous(), device=cameras.device)
        transform = transform.compose(xyflip_transform)
    return transform


def get_screen_to_ndc_transform(cameras, with_xyflip: bool=False, image_size: Optional[Union[List, Tuple, torch.Tensor]]=None) ->Transform3d:
    """
    Screen to PyTorch3D NDC conversion.
    Conversion from screen/image space (+X right, +Y down, origin top left)
    to PyTorch3D's NDC space (+X left, +Y up).

    Args:
        cameras
        with_xyflip: flips x- and y-axis if set to True.
    Optional kwargs:
        image_size: ((height, width),) specifying the height, width
        of the image. If not provided, it reads it from cameras.

    We represent the screen to NDC conversion as a Transform3d
    with projection matrix

    K = [
            [1/s,    0,    0,  cx/s],
            [  0,  1/s,    0,  cy/s],
            [  0,    0,    1,     0],
            [  0,    0,    0,     1],
    ]

    """
    transform = get_ndc_to_screen_transform(cameras, with_xyflip=with_xyflip, image_size=image_size).inverse()
    return transform


class CamerasBase(TensorProperties):
    """
    `CamerasBase` implements a base class for all cameras.

    For cameras, there are four different coordinate systems (or spaces)
    - World coordinate system: This is the system the object lives - the world.
    - Camera view coordinate system: This is the system that has its origin on
        the camera and the Z-axis perpendicular to the image plane.
        In PyTorch3D, we assume that +X points left, and +Y points up and
        +Z points out from the image plane.
        The transformation from world --> view happens after applying a rotation (R)
        and translation (T)
    - NDC coordinate system: This is the normalized coordinate system that confines
        points in a volume the rendered part of the object or scene, also known as
        view volume. For square images, given the PyTorch3D convention, (+1, +1, znear)
        is the top left near corner, and (-1, -1, zfar) is the bottom right far
        corner of the volume.
        The transformation from view --> NDC happens after applying the camera
        projection matrix (P) if defined in NDC space.
        For non square images, we scale the points such that smallest side
        has range [-1, 1] and the largest side has range [-u, u], with u > 1.
    - Screen coordinate system: This is another representation of the view volume with
        the XY coordinates defined in image space instead of a normalized space.

    An illustration of the coordinate systems can be found in pytorch3d/docs/notes/cameras.md.

    CameraBase defines methods that are common to all camera models:
        - `get_camera_center` that returns the optical center of the camera in
            world coordinates
        - `get_world_to_view_transform` which returns a 3D transform from
            world coordinates to the camera view coordinates (R, T)
        - `get_full_projection_transform` which composes the projection
            transform (P) with the world-to-view transform (R, T)
        - `transform_points` which takes a set of input points in world coordinates and
            projects to the space the camera is defined in (NDC or screen)
        - `get_ndc_camera_transform` which defines the transform from screen/NDC to
            PyTorch3D's NDC space
        - `transform_points_ndc` which takes a set of points in world coordinates and
            projects them to PyTorch3D's NDC space
        - `transform_points_screen` which takes a set of points in world coordinates and
            projects them to screen space

    For each new camera, one should implement the `get_projection_transform`
    routine that returns the mapping from camera view coordinates to camera
    coordinates (NDC or screen).

    Another useful function that is specific to each camera model is
    `unproject_points` which sends points from camera coordinates (NDC or screen)
    back to camera view or world coordinates depending on the `world_coordinates`
    boolean argument of the function.
    """
    _FIELDS: Tuple[str, ...] = ()
    _SHARED_FIELDS: Tuple[str, ...] = ()

    def get_projection_transform(self, **kwargs):
        """
        Calculate the projective transformation matrix.

        Args:
            **kwargs: parameters for the projection can be passed in as keyword
                arguments to override the default values set in `__init__`.

        Return:
            a `Transform3d` object which represents a batch of projection
            matrices of shape (N, 3, 3)
        """
        raise NotImplementedError()

    def unproject_points(self, xy_depth: torch.Tensor, **kwargs):
        """
        Transform input points from camera coodinates (NDC or screen)
        to the world / camera coordinates.

        Each of the input points `xy_depth` of shape (..., 3) is
        a concatenation of the x, y location and its depth.

        For instance, for an input 2D tensor of shape `(num_points, 3)`
        `xy_depth` takes the following form:
            `xy_depth[i] = [x[i], y[i], depth[i]]`,
        for a each point at an index `i`.

        The following example demonstrates the relationship between
        `transform_points` and `unproject_points`:

        .. code-block:: python

            cameras = # camera object derived from CamerasBase
            xyz = # 3D points of shape (batch_size, num_points, 3)
            # transform xyz to the camera view coordinates
            xyz_cam = cameras.get_world_to_view_transform().transform_points(xyz)
            # extract the depth of each point as the 3rd coord of xyz_cam
            depth = xyz_cam[:, :, 2:]
            # project the points xyz to the camera
            xy = cameras.transform_points(xyz)[:, :, :2]
            # append depth to xy
            xy_depth = torch.cat((xy, depth), dim=2)
            # unproject to the world coordinates
            xyz_unproj_world = cameras.unproject_points(xy_depth, world_coordinates=True)
            print(torch.allclose(xyz, xyz_unproj_world)) # True
            # unproject to the camera coordinates
            xyz_unproj = cameras.unproject_points(xy_depth, world_coordinates=False)
            print(torch.allclose(xyz_cam, xyz_unproj)) # True

        Args:
            xy_depth: torch tensor of shape (..., 3).
            world_coordinates: If `True`, unprojects the points back to world
                coordinates using the camera extrinsics `R` and `T`.
                `False` ignores `R` and `T` and unprojects to
                the camera view coordinates.
            from_ndc: If `False` (default), assumes xy part of input is in
                NDC space if self.in_ndc(), otherwise in screen space. If
                `True`, assumes xy is in NDC space even if the camera
                is defined in screen space.

        Returns
            new_points: unprojected points with the same shape as `xy_depth`.
        """
        raise NotImplementedError()

    def get_camera_center(self, **kwargs) ->torch.Tensor:
        """
        Return the 3D location of the camera optical center
        in the world coordinates.

        Args:
            **kwargs: parameters for the camera extrinsics can be passed in
                as keyword arguments to override the default values
                set in __init__.

        Setting R or T here will update the values set in init as these
        values may be needed later on in the rendering pipeline e.g. for
        lighting calculations.

        Returns:
            C: a batch of 3D locations of shape (N, 3) denoting
            the locations of the center of each camera in the batch.
        """
        w2v_trans = self.get_world_to_view_transform(**kwargs)
        P = w2v_trans.inverse().get_matrix()
        C = P[:, 3, :3]
        return C

    def get_world_to_view_transform(self, **kwargs) ->Transform3d:
        """
        Return the world-to-view transform.

        Args:
            **kwargs: parameters for the camera extrinsics can be passed in
                as keyword arguments to override the default values
                set in __init__.

        Setting R and T here will update the values set in init as these
        values may be needed later on in the rendering pipeline e.g. for
        lighting calculations.

        Returns:
            A Transform3d object which represents a batch of transforms
            of shape (N, 3, 3)
        """
        R: torch.Tensor = kwargs.get('R', self.R)
        T: torch.Tensor = kwargs.get('T', self.T)
        self.R = R
        self.T = T
        world_to_view_transform = get_world_to_view_transform(R=R, T=T)
        return world_to_view_transform

    def get_full_projection_transform(self, **kwargs) ->Transform3d:
        """
        Return the full world-to-camera transform composing the
        world-to-view and view-to-camera transforms.
        If camera is defined in NDC space, the projected points are in NDC space.
        If camera is defined in screen space, the projected points are in screen space.

        Args:
            **kwargs: parameters for the projection transforms can be passed in
                as keyword arguments to override the default values
                set in __init__.

        Setting R and T here will update the values set in init as these
        values may be needed later on in the rendering pipeline e.g. for
        lighting calculations.

        Returns:
            a Transform3d object which represents a batch of transforms
            of shape (N, 3, 3)
        """
        self.R: torch.Tensor = kwargs.get('R', self.R)
        self.T: torch.Tensor = kwargs.get('T', self.T)
        world_to_view_transform = self.get_world_to_view_transform(R=self.R, T=self.T)
        view_to_proj_transform = self.get_projection_transform(**kwargs)
        return world_to_view_transform.compose(view_to_proj_transform)

    def transform_points(self, points, eps: Optional[float]=None, **kwargs) ->torch.Tensor:
        """
        Transform input points from world to camera space.
        If camera is defined in NDC space, the projected points are in NDC space.
        If camera is defined in screen space, the projected points are in screen space.

        For `CamerasBase.transform_points`, setting `eps > 0`
        stabilizes gradients since it leads to avoiding division
        by excessively low numbers for points close to the camera plane.

        Args:
            points: torch tensor of shape (..., 3).
            eps: If eps!=None, the argument is used to clamp the
                divisor in the homogeneous normalization of the points
                transformed to the ndc space. Please see
                `transforms.Transform3d.transform_points` for details.

                For `CamerasBase.transform_points`, setting `eps > 0`
                stabilizes gradients since it leads to avoiding division
                by excessively low numbers for points close to the
                camera plane.

        Returns
            new_points: transformed points with the same shape as the input.
        """
        world_to_proj_transform = self.get_full_projection_transform(**kwargs)
        return world_to_proj_transform.transform_points(points, eps=eps)

    def get_ndc_camera_transform(self, **kwargs) ->Transform3d:
        """
        Returns the transform from camera projection space (screen or NDC) to NDC space.
        For cameras that can be specified in screen space, this transform
        allows points to be converted from screen to NDC space.
        The default transform scales the points from [0, W]x[0, H]
        to [-1, 1]x[-u, u] or [-u, u]x[-1, 1] where u > 1 is the aspect ratio of the image.
        This function should be modified per camera definitions if need be,
        e.g. for Perspective/Orthographic cameras we provide a custom implementation.
        This transform assumes PyTorch3D coordinate system conventions for
        both the NDC space and the input points.

        This transform interfaces with the PyTorch3D renderer which assumes
        input points to the renderer to be in NDC space.
        """
        if self.in_ndc():
            return Transform3d(device=self.device, dtype=torch.float32)
        else:
            image_size = kwargs.get('image_size', self.get_image_size())
            return get_screen_to_ndc_transform(self, with_xyflip=False, image_size=image_size)

    def transform_points_ndc(self, points, eps: Optional[float]=None, **kwargs) ->torch.Tensor:
        """
        Transforms points from PyTorch3D world/camera space to NDC space.
        Input points follow the PyTorch3D coordinate system conventions: +X left, +Y up.
        Output points are in NDC space: +X left, +Y up, origin at image center.

        Args:
            points: torch tensor of shape (..., 3).
            eps: If eps!=None, the argument is used to clamp the
                divisor in the homogeneous normalization of the points
                transformed to the ndc space. Please see
                `transforms.Transform3d.transform_points` for details.

                For `CamerasBase.transform_points`, setting `eps > 0`
                stabilizes gradients since it leads to avoiding division
                by excessively low numbers for points close to the
                camera plane.

        Returns
            new_points: transformed points with the same shape as the input.
        """
        world_to_ndc_transform = self.get_full_projection_transform(**kwargs)
        if not self.in_ndc():
            to_ndc_transform = self.get_ndc_camera_transform(**kwargs)
            world_to_ndc_transform = world_to_ndc_transform.compose(to_ndc_transform)
        return world_to_ndc_transform.transform_points(points, eps=eps)

    def transform_points_screen(self, points, eps: Optional[float]=None, with_xyflip: bool=True, **kwargs) ->torch.Tensor:
        """
        Transforms points from PyTorch3D world/camera space to screen space.
        Input points follow the PyTorch3D coordinate system conventions: +X left, +Y up.
        Output points are in screen space: +X right, +Y down, origin at top left corner.

        Args:
            points: torch tensor of shape (..., 3).
            eps: If eps!=None, the argument is used to clamp the
                divisor in the homogeneous normalization of the points
                transformed to the ndc space. Please see
                `transforms.Transform3d.transform_points` for details.

                For `CamerasBase.transform_points`, setting `eps > 0`
                stabilizes gradients since it leads to avoiding division
                by excessively low numbers for points close to the
                camera plane.
            with_xyflip: If True, flip x and y directions. In world/camera/ndc coords,
                +x points to the left and +y up. If with_xyflip is true, in screen
                coords +x points right, and +y down, following the usual RGB image
                convention. Warning: do not set to False unless you know what you're
                doing!

        Returns
            new_points: transformed points with the same shape as the input.
        """
        points_ndc = self.transform_points_ndc(points, eps=eps, **kwargs)
        image_size = kwargs.get('image_size', self.get_image_size())
        return get_ndc_to_screen_transform(self, with_xyflip=with_xyflip, image_size=image_size).transform_points(points_ndc, eps=eps)

    def clone(self):
        """
        Returns a copy of `self`.
        """
        cam_type = type(self)
        other = cam_type(device=self.device)
        return super().clone(other)

    def is_perspective(self):
        raise NotImplementedError()

    def in_ndc(self):
        """
        Specifies whether the camera is defined in NDC space
        or in screen (image) space
        """
        raise NotImplementedError()

    def get_znear(self):
        return self.znear if hasattr(self, 'znear') else None

    def get_image_size(self):
        """
        Returns the image size, if provided, expected in the form of (height, width)
        The image size is used for conversion of projected points to screen coordinates.
        """
        return self.image_size if hasattr(self, 'image_size') else None

    def __getitem__(self, index: Union[int, List[int], torch.BoolTensor, torch.LongTensor]) ->'CamerasBase':
        """
        Override for the __getitem__ method in TensorProperties which needs to be
        refactored.

        Args:
            index: an integer index, list/tensor of integer indices, or tensor of boolean
                indicators used to filter all the fields in the cameras given by self._FIELDS.
        Returns:
            an instance of the current cameras class with only the values at the selected index.
        """
        kwargs = {}
        tensor_types = {'bool': (torch.BoolTensor, torch.BoolTensor), 'long': (torch.LongTensor, torch.LongTensor)}
        if not isinstance(index, (int, list, *tensor_types['bool'], *tensor_types['long'])) or isinstance(index, list) and not all(isinstance(i, int) and not isinstance(i, bool) for i in index):
            msg = 'Invalid index type, expected int, List[int] or Bool/LongTensor; got %r'
            raise ValueError(msg % type(index))
        if isinstance(index, int):
            index = [index]
        if isinstance(index, tensor_types['bool']):
            if index.ndim != 1 or index.shape[0] != len(self):
                raise ValueError(f'Boolean index of shape {index.shape} does not match cameras')
        elif max(index) >= len(self):
            raise ValueError(f'Index {max(index)} is out of bounds for select cameras')
        for field in self._FIELDS:
            val = getattr(self, field, None)
            if val is None:
                continue
            if field.startswith('_'):
                field = field[1:]
            if isinstance(val, (str, bool)):
                kwargs[field] = val
            elif isinstance(val, torch.Tensor):
                kwargs[field] = val[index]
            else:
                raise ValueError(f'Field {field} type is not supported for indexing')
        kwargs['device'] = self.device
        return self.__class__(**kwargs)


def _check_density_bounds(rays_densities: torch.Tensor, bounds: Tuple[float, float]=(0.0, 1.0)) ->None:
    """
    Checks whether the elements of `rays_densities` range within `bounds`.
    If not issues a warning.
    """
    with torch.no_grad():
        if rays_densities.max() > bounds[1] or rays_densities.min() < bounds[0]:
            warnings.warn('One or more elements of rays_densities are outside of valid' + f'range {str(bounds)}')


def _check_raymarcher_inputs(rays_densities: torch.Tensor, rays_features: Optional[torch.Tensor], rays_z: Optional[torch.Tensor], features_can_be_none: bool=False, z_can_be_none: bool=False, density_1d: bool=True) ->None:
    """
    Checks the validity of the inputs to raymarching algorithms.
    """
    if not torch.is_tensor(rays_densities):
        raise ValueError('rays_densities has to be an instance of torch.Tensor.')
    if not z_can_be_none and not torch.is_tensor(rays_z):
        raise ValueError('rays_z has to be an instance of torch.Tensor.')
    if not features_can_be_none and not torch.is_tensor(rays_features):
        raise ValueError('rays_features has to be an instance of torch.Tensor.')
    if rays_densities.ndim < 1:
        raise ValueError('rays_densities have to have at least one dimension.')
    if density_1d and rays_densities.shape[-1] != 1:
        raise ValueError('The size of the last dimension of rays_densities has to be one.' + f' Got shape {rays_densities.shape}.')
    rays_shape = rays_densities.shape[:-1]
    if not z_can_be_none and rays_z.shape != rays_shape:
        raise ValueError('rays_z have to be of the same shape as rays_densities.')
    if not features_can_be_none and rays_features.shape[:-1] != rays_shape:
        raise ValueError('The first to previous to last dimensions of rays_features have to be the same as all dimensions of rays_densities.')


def _shifted_cumprod(x, shift: int=1):
    """
    Computes `torch.cumprod(x, dim=-1)` and prepends `shift` number of
    ones and removes `shift` trailing elements to/from the last dimension
    of the result.
    """
    x_cumprod = torch.cumprod(x, dim=-1)
    x_cumprod_shift = torch.cat([torch.ones_like(x_cumprod[..., :shift]), x_cumprod[..., :-shift]], dim=-1)
    return x_cumprod_shift


class EmissionAbsorptionRaymarcher(torch.nn.Module):
    """
    Raymarch using the Emission-Absorption (EA) algorithm.

    The algorithm independently renders each ray by analyzing density and
    feature values sampled at (typically uniformly) spaced 3D locations along
    each ray. The density values `rays_densities` are of shape
    `(..., n_points_per_ray)`, their values should range between [0, 1], and
    represent the opaqueness of each point (the higher the less transparent).
    The feature values `rays_features` of shape
    `(..., n_points_per_ray, feature_dim)` represent the content of the
    point that is supposed to be rendered in case the given point is opaque
    (i.e. its density -> 1.0).

    EA first utilizes `rays_densities` to compute the absorption function
    along each ray as follows::

        absorption = cumprod(1 - rays_densities, dim=-1)

    The value of absorption at position `absorption[..., k]` specifies
    how much light has reached `k`-th point along a ray since starting
    its trajectory at `k=0`-th point.

    Each ray is then rendered into a tensor `features` of shape `(..., feature_dim)`
    by taking a weighed combination of per-ray features `rays_features` as follows::

        weights = absorption * rays_densities
        features = (rays_features * weights).sum(dim=-2)

    Where `weights` denote a function that has a strong peak around the location
    of the first surface point that a given ray passes through.

    Note that for a perfectly bounded volume (with a strictly binary density),
    the `weights = cumprod(1 - rays_densities, dim=-1) * rays_densities`
    function would yield 0 everywhere. In order to prevent this,
    the result of the cumulative product is shifted `self.surface_thickness`
    elements along the ray direction.
    """

    def __init__(self, surface_thickness: int=1) ->None:
        """
        Args:
            surface_thickness: Denotes the overlap between the absorption
                function and the density function.
        """
        super().__init__()
        self.surface_thickness = surface_thickness

    def forward(self, rays_densities: torch.Tensor, rays_features: torch.Tensor, eps: float=1e-10, **kwargs) ->torch.Tensor:
        """
        Args:
            rays_densities: Per-ray density values represented with a tensor
                of shape `(..., n_points_per_ray, 1)` whose values range in [0, 1].
            rays_features: Per-ray feature values represented with a tensor
                of shape `(..., n_points_per_ray, feature_dim)`.
            eps: A lower bound added to `rays_densities` before computing
                the absorption function (cumprod of `1-rays_densities` along
                each ray). This prevents the cumprod to yield exact 0
                which would inhibit any gradient-based learning.

        Returns:
            features_opacities: A tensor of shape `(..., feature_dim+1)`
                that concatenates two tensors along the last dimension:
                    1) features: A tensor of per-ray renders
                        of shape `(..., feature_dim)`.
                    2) opacities: A tensor of per-ray opacity values
                        of shape `(..., 1)`. Its values range between [0, 1] and
                        denote the total amount of light that has been absorbed
                        for each ray. E.g. a value of 0 corresponds to the ray
                        completely passing through a volume. Please refer to the
                        `AbsorptionOnlyRaymarcher` documentation for the
                        explanation of the algorithm that computes `opacities`.
        """
        _check_raymarcher_inputs(rays_densities, rays_features, None, z_can_be_none=True, features_can_be_none=False, density_1d=True)
        _check_density_bounds(rays_densities)
        rays_densities = rays_densities[..., 0]
        absorption = _shifted_cumprod(1.0 + eps - rays_densities, shift=self.surface_thickness)
        weights = rays_densities * absorption
        features = (weights[..., None] * rays_features).sum(dim=-2)
        opacities = 1.0 - torch.prod(1.0 - rays_densities, dim=-1, keepdim=True)
        return torch.cat((features, opacities), dim=-1)


class EmissionAbsorptionNeRFRaymarcher(EmissionAbsorptionRaymarcher):
    """
    This is essentially the `pytorch3d.renderer.EmissionAbsorptionRaymarcher`
    which additionally returns the rendering weights. It also skips returning
    the computation of the alpha-mask which is, in case of NeRF, equal to 1
    everywhere.

    The weights are later used in the NeRF pipeline to carry out the importance
    ray-sampling for the fine rendering pass.

    For more details about the EmissionAbsorptionRaymarcher please refer to
    the documentation of `pytorch3d.renderer.EmissionAbsorptionRaymarcher`.
    """

    def forward(self, rays_densities: torch.Tensor, rays_features: torch.Tensor, eps: float=1e-10, **kwargs) ->torch.Tensor:
        """
        Args:
            rays_densities: Per-ray density values represented with a tensor
                of shape `(..., n_points_per_ray, 1)` whose values range in [0, 1].
            rays_features: Per-ray feature values represented with a tensor
                of shape `(..., n_points_per_ray, feature_dim)`.
            eps: A lower bound added to `rays_densities` before computing
                the absorption function (cumprod of `1-rays_densities` along
                each ray). This prevents the cumprod to yield exact 0
                which would inhibit any gradient-based learning.

        Returns:
            features: A tensor of shape `(..., feature_dim)` containing
                the rendered features for each ray.
            weights: A tensor of shape `(..., n_points_per_ray)` containing
                the ray-specific emission-absorption distribution.
                Each ray distribution `(..., :)` is a valid probability
                distribution, i.e. it contains non-negative values that integrate
                to 1, such that `weights.sum(dim=-1)==1).all()` yields `True`.
        """
        _check_raymarcher_inputs(rays_densities, rays_features, None, z_can_be_none=True, features_can_be_none=False, density_1d=True)
        _check_density_bounds(rays_densities)
        rays_densities = rays_densities[..., 0]
        absorption = _shifted_cumprod(1.0 + eps - rays_densities, shift=self.surface_thickness)
        weights = rays_densities * absorption
        features = (weights[..., None] * rays_features).sum(dim=-2)
        return features, weights


def padded_to_packed(x: torch.Tensor, split_size: Union[list, tuple, None]=None, pad_value: Union[float, int, None]=None):
    """
    Transforms a padded tensor of shape (N, M, K) into a packed tensor
    of shape:
     - (sum(Mi), K) where (Mi, K) are the dimensions of
        each of the tensors in the batch and Mi is specified by split_size(i)
     - (N*M, K) if split_size is None

    Support only for 3-dimensional input tensor and 1-dimensional split size.

    Args:
      x: tensor
      split_size: list, tuple or int defining the number of items for each tensor
        in the output list.
      pad_value: optional value to use to filter the padded values in the input
        tensor.

    Only one of split_size or pad_value should be provided, or both can be None.

    Returns:
      x_packed: a packed tensor.
    """
    if x.ndim != 3:
        raise ValueError('Supports only 3-dimensional input tensors')
    N, M, D = x.shape
    if split_size is not None and pad_value is not None:
        raise ValueError('Only one of split_size or pad_value should be provided.')
    x_packed = x.reshape(-1, D)
    if pad_value is None and split_size is None:
        return x_packed
    if pad_value is not None:
        mask = x_packed.ne(pad_value).any(-1)
        x_packed = x_packed[mask]
        return x_packed
    N = len(split_size)
    if x.shape[0] != N:
        raise ValueError('Split size must be of same length as inputs first dimension')
    if not all(isinstance(i, int) for i in split_size):
        raise ValueError('Support only 1-dimensional unbinded tensor.                 Split size for more dimensions provided')
    padded_to_packed_idx = torch.cat([(torch.arange(v, dtype=torch.int64, device=x.device) + i * M) for i, v in enumerate(split_size)], dim=0)
    return x_packed[padded_to_packed_idx]


def _sample_cameras_and_masks(n_samples: int, cameras: CamerasBase, mask: Optional[torch.Tensor]=None) ->Tuple[CamerasBase, Optional[torch.Tensor], torch.LongTensor, torch.LongTensor, torch.LongTensor]:
    """
    Samples n_rays_total cameras and masks and returns them in a form
    (camera_idx, count), where count represents number of times the same camera
    has been sampled.

    Args:
        n_samples: how many camera and mask pairs to sample
        cameras: A batch of `batch_size` cameras from which the rays are emitted.
        mask: Optional. Should be of size (batch_size, image_height, image_width).
    Returns:
        tuple of a form (sampled_cameras, sampled_masks, unique_sampled_camera_ids,
            number_of_times_each_sampled_camera_has_been_sampled,
            max_number_of_times_camera_has_been_sampled,
            )
    """
    sampled_ids = torch.randint(0, len(cameras), size=(n_samples,), dtype=torch.long)
    unique_ids, counts = torch.unique(sampled_ids, return_counts=True)
    return cameras[unique_ids], mask[unique_ids] if mask is not None else None, unique_ids, counts, torch.max(counts)


def _jiggle_within_stratas(bin_centers: torch.Tensor) ->torch.Tensor:
    """
    Performs sampling of 1 point per bin given the bin centers.

    More specifically, it replaces each point's value `z`
    with a sample from a uniform random distribution on
    `[z - delta_-, z + delta_+]`, where `delta_-` is half of the difference
    between `z` and the previous point, and `delta_+` is half of the difference
    between the next point and `z`. For the first and last items, the
    corresponding boundary deltas are assumed zero.

    Args:
        `bin_centers`: The input points of size (..., N); the result is broadcast
            along all but the last dimension (the rows). Each row should be
            sorted in ascending order.

    Returns:
        a tensor of size (..., N) with the locations jiggled within stratas/bins.
    """
    mids = 0.5 * (bin_centers[..., 1:] + bin_centers[..., :-1])
    upper = torch.cat((mids, bin_centers[..., -1:]), dim=-1)
    lower = torch.cat((bin_centers[..., :1], mids), dim=-1)
    jiggled = lower + (upper - lower) * torch.rand_like(lower)
    return jiggled


def _xy_to_ray_bundle(cameras: CamerasBase, xy_grid: torch.Tensor, min_depth: float, max_depth: float, n_pts_per_ray: int, unit_directions: bool, stratified_sampling: bool=False) ->RayBundle:
    """
    Extends the `xy_grid` input of shape `(batch_size, ..., 2)` to rays.
    This adds to each xy location in the grid a vector of `n_pts_per_ray` depths
    uniformly spaced between `min_depth` and `max_depth`.

    The extended grid is then unprojected with `cameras` to yield
    ray origins, directions and depths.

    Args:
        cameras: cameras object representing a batch of cameras.
        xy_grid: torch.tensor grid of image xy coords.
        min_depth: The minimum depth of each ray-point.
        max_depth: The maximum depth of each ray-point.
        n_pts_per_ray: The number of points sampled along each ray.
        unit_directions: whether to normalize direction vectors in ray bundle.
        stratified_sampling: if True, performs stratified sampling in n_pts_per_ray
            bins for each ray; otherwise takes n_pts_per_ray deterministic points
            on each ray with uniform offsets.
    """
    batch_size = xy_grid.shape[0]
    spatial_size = xy_grid.shape[1:-1]
    n_rays_per_image = spatial_size.numel()
    rays_zs = xy_grid.new_empty((0,))
    if n_pts_per_ray > 0:
        depths = torch.linspace(min_depth, max_depth, n_pts_per_ray, dtype=xy_grid.dtype, device=xy_grid.device)
        rays_zs = depths[None, None].expand(batch_size, n_rays_per_image, n_pts_per_ray)
        if stratified_sampling:
            rays_zs = _jiggle_within_stratas(rays_zs)
    to_unproject = torch.cat((xy_grid.view(batch_size, 1, n_rays_per_image, 2).expand(batch_size, 2, n_rays_per_image, 2).reshape(batch_size, n_rays_per_image * 2, 2), torch.cat((xy_grid.new_ones(batch_size, n_rays_per_image, 1), 2.0 * xy_grid.new_ones(batch_size, n_rays_per_image, 1)), dim=1)), dim=-1)
    unprojected = cameras.unproject_points(to_unproject, from_ndc=True)
    rays_plane_1_world = unprojected[:, :n_rays_per_image]
    rays_plane_2_world = unprojected[:, n_rays_per_image:]
    rays_directions_world = rays_plane_2_world - rays_plane_1_world
    rays_origins_world = rays_plane_1_world - rays_directions_world
    if unit_directions:
        rays_directions_world = F.normalize(rays_directions_world, dim=-1)
    return RayBundle(rays_origins_world.view(batch_size, *spatial_size, 3), rays_directions_world.view(batch_size, *spatial_size, 3), rays_zs.view(batch_size, *spatial_size, n_pts_per_ray), xy_grid)


def _safe_multinomial(input: torch.Tensor, num_samples: int) ->torch.Tensor:
    """
    Wrapper around torch.multinomial that attempts sampling without replacement
    when possible, otherwise resorts to sampling with replacement.

    Args:
        input: tensor of shape [B, n] containing non-negative values;
                rows are interpreted as unnormalized event probabilities
                in categorical distributions.
        num_samples: number of samples to take.

    Returns:
        LongTensor of shape [B, num_samples] containing
        values from {0, ..., n - 1} where the elements [i, :] of row i make
            (1) if there are num_samples or more non-zero values in input[i],
                a random subset of the indices of those values, with
                probabilities proportional to the values in input[i, :].

            (2) if not, a random sample with replacement of the indices of
                those values, with probabilities proportional to them.
                This sample might not contain all the indices of the
                non-zero values.
        Behavior undetermined if there are no non-zero values in a whole row
        or if there are negative values.
    """
    try:
        res = torch.multinomial(input, num_samples, replacement=False)
    except RuntimeError:
        res = torch.multinomial(input, num_samples, replacement=True)
        no_repl = (input > 0.0).sum(dim=-1) >= num_samples
        res[no_repl] = torch.multinomial(input[no_repl], num_samples, replacement=False)
        return res
    repl = (input > 0.0).sum(dim=-1) < num_samples
    if repl.any():
        res[repl] = torch.multinomial(input[repl], num_samples, replacement=True)
    return res


def meshgrid_ij(*A: Union[torch.Tensor, Sequence[torch.Tensor]]) ->Tuple[torch.Tensor, ...]:
    """
    Like torch.meshgrid was before PyTorch 1.10.0, i.e. with indexing set to ij
    """
    if torch.meshgrid.__kwdefaults__ is not None and 'indexing' in torch.meshgrid.__kwdefaults__:
        return torch.meshgrid(*A, indexing='ij')
    return torch.meshgrid(*A)


class NeRFRaysampler(torch.nn.Module):
    """
    Implements the raysampler of NeRF.

    Depending on the `self.training` flag, the raysampler either samples
    a chunk of random rays (`self.training==True`), or returns a subset of rays
    of the full image grid (`self.training==False`).
    The chunking of rays allows for efficient evaluation of the NeRF implicit
    surface function without encountering out-of-GPU-memory errors.

    Additionally, this raysampler supports pre-caching of the ray bundles
    for a set of input cameras (`self.precache_rays`).
    Pre-caching the rays before training greatly speeds-up the ensuing
    raysampling step of the training NeRF iterations.
    """

    def __init__(self, n_pts_per_ray: int, min_depth: float, max_depth: float, n_rays_per_image: int, image_width: int, image_height: int, stratified: bool=False, stratified_test: bool=False):
        """
        Args:
            n_pts_per_ray: The number of points sampled along each ray.
            min_depth: The minimum depth of a ray-point.
            max_depth: The maximum depth of a ray-point.
            n_rays_per_image: Number of Monte Carlo ray samples when training
                (`self.training==True`).
            image_width: The horizontal size of the image grid.
            image_height: The vertical size of the image grid.
            stratified: If `True`, stratifies (=randomly offsets) the depths
                of each ray point during training (`self.training==True`).
            stratified_test: If `True`, stratifies (=randomly offsets) the depths
                of each ray point during evaluation (`self.training==False`).
        """
        super().__init__()
        self._stratified = stratified
        self._stratified_test = stratified_test
        self._grid_raysampler = NDCMultinomialRaysampler(image_width=image_width, image_height=image_height, n_pts_per_ray=n_pts_per_ray, min_depth=min_depth, max_depth=max_depth)
        self._mc_raysampler = MonteCarloRaysampler(min_x=-1.0, max_x=1.0, min_y=-1.0, max_y=1.0, n_rays_per_image=n_rays_per_image, n_pts_per_ray=n_pts_per_ray, min_depth=min_depth, max_depth=max_depth)
        self._ray_cache = {}

    def get_n_chunks(self, chunksize: int, batch_size: int):
        """
        Returns the total number of `chunksize`-sized chunks
        of the raysampler's rays.

        Args:
            chunksize: The number of rays per chunk.
            batch_size: The size of the batch of the raysampler.

        Returns:
            n_chunks: The total number of chunks.
        """
        return int(math.ceil(self._grid_raysampler._xy_grid.numel() * 0.5 * batch_size / chunksize))

    def _print_precaching_progress(self, i, total, bar_len=30):
        """
        Print a progress bar for ray precaching.
        """
        position = round((i + 1) / total * bar_len)
        pbar = '[' + '█' * position + ' ' * (bar_len - position) + ']'
        None

    def precache_rays(self, cameras: List[CamerasBase], camera_hashes: List):
        """
        Precaches the rays emitted from the list of cameras `cameras`,
        where each camera is uniquely identified with the corresponding hash
        from `camera_hashes`.

        The cached rays are moved to cpu and stored in `self._ray_cache`.
        Raises `ValueError` when caching two cameras with the same hash.

        Args:
            cameras: A list of `N` cameras for which the rays are pre-cached.
            camera_hashes: A list of `N` unique identifiers of each
                camera from `cameras`.
        """
        None
        full_chunksize = self._grid_raysampler._xy_grid.numel() // 2 * self._grid_raysampler._n_pts_per_ray
        if self.get_n_chunks(full_chunksize, 1) != 1:
            raise ValueError('There has to be one chunk for precaching rays!')
        for camera_i, (camera, camera_hash) in enumerate(zip(cameras, camera_hashes)):
            ray_bundle = self.forward(camera, caching=True, chunksize=full_chunksize)
            if camera_hash in self._ray_cache:
                raise ValueError('There are redundant cameras!')
            self._ray_cache[camera_hash] = RayBundle(*[v.detach() for v in ray_bundle])
            self._print_precaching_progress(camera_i, len(cameras))
        None

    def _stratify_ray_bundle(self, ray_bundle: RayBundle):
        """
        Stratifies the lengths of the input `ray_bundle`.

        More specifically, the stratification replaces each ray points' depth `z`
        with a sample from a uniform random distribution on
        `[z - delta_depth, z+delta_depth]`, where `delta_depth` is the difference
        of depths of the consecutive ray depth values.

        Args:
            `ray_bundle`: The input `RayBundle`.

        Returns:
            `stratified_ray_bundle`: `ray_bundle` whose `lengths` field is replaced
                with the stratified samples.
        """
        z_vals = ray_bundle.lengths
        mids = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
        upper = torch.cat((mids, z_vals[..., -1:]), dim=-1)
        lower = torch.cat((z_vals[..., :1], mids), dim=-1)
        z_vals = lower + (upper - lower) * torch.rand_like(lower)
        return ray_bundle._replace(lengths=z_vals)

    def _normalize_raybundle(self, ray_bundle: RayBundle):
        """
        Normalizes the ray directions of the input `RayBundle` to unit norm.
        """
        ray_bundle = ray_bundle._replace(directions=torch.nn.functional.normalize(ray_bundle.directions, dim=-1))
        return ray_bundle

    def forward(self, cameras: CamerasBase, chunksize: int=None, chunk_idx: int=0, camera_hash: str=None, caching: bool=False, **kwargs) ->RayBundle:
        """
        Args:
            cameras: A batch of `batch_size` cameras from which the rays are emitted.
            chunksize: The number of rays per chunk.
                Active only when `self.training==False`.
            chunk_idx: The index of the ray chunk. The number has to be in
                `[0, self.get_n_chunks(chunksize, batch_size)-1]`.
                Active only when `self.training==False`.
            camera_hash: A unique identifier of a pre-cached camera. If `None`,
                the cache is not searched and the rays are calculated from scratch.
            caching: If `True`, activates the caching mode that returns the `RayBundle`
                that should be stored into the cache.
        Returns:
            A named tuple `RayBundle` with the following fields:
                origins: A tensor of shape
                    `(batch_size, n_rays_per_image, 3)`
                    denoting the locations of ray origins in the world coordinates.
                directions: A tensor of shape
                    `(batch_size, n_rays_per_image, 3)`
                    denoting the directions of each ray in the world coordinates.
                lengths: A tensor of shape
                    `(batch_size, n_rays_per_image, n_pts_per_ray)`
                    containing the z-coordinate (=depth) of each ray in world units.
                xys: A tensor of shape
                    `(batch_size, n_rays_per_image, 2)`
                    containing the 2D image coordinates of each ray.
        """
        batch_size = cameras.R.shape[0]
        device = cameras.device
        if camera_hash is None and not caching and self.training:
            ray_bundle = self._mc_raysampler(cameras)
            ray_bundle = self._normalize_raybundle(ray_bundle)
        else:
            if camera_hash is not None:
                if batch_size != 1:
                    raise NotImplementedError('Ray caching works only for batches with a single camera!')
                full_ray_bundle = self._ray_cache[camera_hash]
            else:
                full_ray_bundle = self._grid_raysampler(cameras)
                full_ray_bundle = self._normalize_raybundle(full_ray_bundle)
            n_pixels = full_ray_bundle.directions.shape[:-1].numel()
            if self.training:
                sel_rays = torch.randperm(n_pixels, device=device)[:self._mc_raysampler._n_rays_per_image]
            else:
                if chunksize is None:
                    chunksize = n_pixels * batch_size
                start = chunk_idx * chunksize * batch_size
                end = min(start + chunksize, n_pixels)
                sel_rays = torch.arange(start, end, dtype=torch.long, device=full_ray_bundle.lengths.device)
            ray_bundle = RayBundle(*[v.view(n_pixels, -1)[sel_rays].view(batch_size, sel_rays.numel() // batch_size, -1) for v in full_ray_bundle])
        if (self._stratified and self.training or self._stratified_test and not self.training) and not caching:
            ray_bundle = self._stratify_ray_bundle(ray_bundle)
        return ray_bundle


def sample_pdf(bins: torch.Tensor, weights: torch.Tensor, n_samples: int, det: bool=False, eps: float=1e-05) ->torch.Tensor:
    """
    Samples probability density functions defined by bin edges `bins` and
    the non-negative per-bin probabilities `weights`.

    Args:
        bins: Tensor of shape `(..., n_bins+1)` denoting the edges of the sampling bins.
        weights: Tensor of shape `(..., n_bins)` containing non-negative numbers
            representing the probability of sampling the corresponding bin.
        n_samples: The number of samples to draw from each set of bins.
        det: If `False`, the sampling is random. `True` yields deterministic
            uniformly-spaced sampling from the inverse cumulative density function.
        eps: A constant preventing division by zero in case empty bins are present.

    Returns:
        samples: Tensor of shape `(..., n_samples)` containing `n_samples` samples
            drawn from each probability distribution.

    Refs:
        [1] https://github.com/bmild/nerf/blob/55d8b00244d7b5178f4d003526ab6667683c9da9/run_nerf_helpers.py#L183  # noqa E501
    """
    if torch.is_grad_enabled() and (bins.requires_grad or weights.requires_grad):
        raise NotImplementedError('sample_pdf differentiability.')
    if weights.min() <= -eps:
        raise ValueError('Negative weights provided.')
    batch_shape = bins.shape[:-1]
    n_bins = weights.shape[-1]
    if n_bins + 1 != bins.shape[-1] or weights.shape[:-1] != batch_shape:
        shapes = f'{bins.shape}{weights.shape}'
        raise ValueError('Inconsistent shapes of bins and weights: ' + shapes)
    output_shape = batch_shape + (n_samples,)
    if det:
        u = torch.linspace(0.0, 1.0, n_samples, device=bins.device, dtype=torch.float32)
        output = u.expand(output_shape).contiguous()
    else:
        output = torch.rand(output_shape, dtype=torch.float32, device=bins.device)
    _C.sample_pdf(bins.reshape(-1, n_bins + 1), weights.reshape(-1, n_bins), output.reshape(-1, n_samples), eps)
    return output


class ProbabilisticRaysampler(torch.nn.Module):
    """
    Implements the importance sampling of points along rays.
    The input is a `RayBundle` object with a `ray_weights` tensor
    which specifies the probabilities of sampling a point along each ray.

    This raysampler is used for the fine rendering pass of NeRF.
    As such, the forward pass accepts the RayBundle output by the
    raysampling of the coarse rendering pass. Hence, it does not
    take cameras as input.
    """

    def __init__(self, n_pts_per_ray: int, stratified: bool, stratified_test: bool, add_input_samples: bool=True):
        """
        Args:
            n_pts_per_ray: The number of points to sample along each ray.
            stratified: If `True`, the input `ray_weights` are assumed to be
                sampled at equidistant intervals.
            stratified_test: Same as `stratified` with the difference that this
                setting is applied when the module is in the `eval` mode
                (`self.training==False`).
            add_input_samples: Concatenates and returns the sampled values
                together with the input samples.
        """
        super().__init__()
        self._n_pts_per_ray = n_pts_per_ray
        self._stratified = stratified
        self._stratified_test = stratified_test
        self._add_input_samples = add_input_samples

    def forward(self, input_ray_bundle: RayBundle, ray_weights: torch.Tensor, **kwargs) ->RayBundle:
        """
        Args:
            input_ray_bundle: An instance of `RayBundle` specifying the
                source rays for sampling of the probability distribution.
            ray_weights: A tensor of shape
                `(..., input_ray_bundle.legths.shape[-1])` with non-negative
                elements defining the probability distribution to sample
                ray points from.

        Returns:
            ray_bundle: A new `RayBundle` instance containing the input ray
                points together with `n_pts_per_ray` additional sampled
                points per ray.
        """
        z_vals = input_ray_bundle.lengths
        batch_size = z_vals.shape[0]
        with torch.no_grad():
            z_vals_mid = 0.5 * (z_vals[..., 1:] + z_vals[..., :-1])
            z_samples = sample_pdf(z_vals_mid.view(-1, z_vals_mid.shape[-1]), ray_weights.view(-1, ray_weights.shape[-1])[..., 1:-1], self._n_pts_per_ray, det=not (self._stratified and self.training or self._stratified_test and not self.training)).view(batch_size, z_vals.shape[1], self._n_pts_per_ray)
        if self._add_input_samples:
            z_vals = torch.cat((z_vals, z_samples), dim=-1)
        else:
            z_vals = z_samples
        z_vals, _ = torch.sort(z_vals, dim=-1)
        return RayBundle(origins=input_ray_bundle.origins, directions=input_ray_bundle.directions, lengths=z_vals, xys=input_ray_bundle.xys)


def calc_mse(x: torch.Tensor, y: torch.Tensor, mask: Optional[torch.Tensor]=None) ->torch.Tensor:
    """
    Calculates the mean square error between tensors `x` and `y`.
    """
    if mask is None:
        return torch.mean((x - y) ** 2)
    else:
        return ((x - y) ** 2 * mask).sum() / mask.expand_as(x).sum().clamp(1e-05)


def calc_psnr(x: torch.Tensor, y: torch.Tensor, mask: Optional[torch.Tensor]=None) ->torch.Tensor:
    """
    Calculates the Peak-signal-to-noise ratio between tensors `x` and `y`.
    """
    mse = calc_mse(x, y, mask=mask)
    psnr = torch.log10(mse.clamp(1e-10)) * -10.0
    return psnr


def sample_images_at_mc_locs(target_images: torch.Tensor, sampled_rays_xy: torch.Tensor):
    """
    Given a set of pixel locations `sampled_rays_xy` this method samples the tensor
    `target_images` at the respective 2D locations.

    This function is used in order to extract the colors from ground truth images
    that correspond to the colors rendered using a Monte Carlo rendering.

    Args:
        target_images: A tensor of shape `(batch_size, ..., 3)`.
        sampled_rays_xy: A tensor of shape `(batch_size, S_1, ..., S_N, 2)`.

    Returns:
        images_sampled: A tensor of shape `(batch_size, S_1, ..., S_N, 3)`
            containing `target_images` sampled at `sampled_rays_xy`.
    """
    ba = target_images.shape[0]
    dim = target_images.shape[-1]
    spatial_size = sampled_rays_xy.shape[1:-1]
    xy_sample = -sampled_rays_xy.view(ba, -1, 1, 2).clone()
    images_sampled = torch.nn.functional.grid_sample(target_images.permute(0, 3, 1, 2), xy_sample, align_corners=True, mode='bilinear')
    return images_sampled.permute(0, 2, 3, 1).view(ba, *spatial_size, dim)


class RadianceFieldRenderer(torch.nn.Module):
    """
    Implements a renderer of a Neural Radiance Field.

    This class holds pointers to the fine and coarse renderer objects, which are
    instances of `pytorch3d.renderer.ImplicitRenderer`, and pointers to the
    neural networks representing the fine and coarse Neural Radiance Fields,
    which are instances of `NeuralRadianceField`.

    The rendering forward pass proceeds as follows:
        1) For a given input camera, rendering rays are generated with the
            `NeRFRaysampler` object of `self._renderer['coarse']`.
            In the training mode (`self.training==True`), the rays are a set
                of `n_rays_per_image` random 2D locations of the image grid.
            In the evaluation mode (`self.training==False`), the rays correspond
                to the full image grid. The rays are further split to
                `chunk_size_test`-sized chunks to prevent out-of-memory errors.
        2) For each ray point, the coarse `NeuralRadianceField` MLP is evaluated.
            The pointer to this MLP is stored in `self._implicit_function['coarse']`
        3) The coarse radiance field is rendered with the
            `EmissionAbsorptionNeRFRaymarcher` object of `self._renderer['coarse']`.
        4) The coarse raymarcher outputs a probability distribution that guides
            the importance raysampling of the fine rendering pass. The
            `ProbabilisticRaysampler` stored in `self._renderer['fine'].raysampler`
            implements the importance ray-sampling.
        5) Similar to 2) the fine MLP in `self._implicit_function['fine']`
            labels the ray points with occupancies and colors.
        6) self._renderer['fine'].raymarcher` generates the final fine render.
        7) The fine and coarse renders are compared to the ground truth input image
            with PSNR and MSE metrics.
    """

    def __init__(self, image_size: Tuple[int, int], n_pts_per_ray: int, n_pts_per_ray_fine: int, n_rays_per_image: int, min_depth: float, max_depth: float, stratified: bool, stratified_test: bool, chunk_size_test: int, n_harmonic_functions_xyz: int=6, n_harmonic_functions_dir: int=4, n_hidden_neurons_xyz: int=256, n_hidden_neurons_dir: int=128, n_layers_xyz: int=8, append_xyz: Tuple[int, ...]=(5,), density_noise_std: float=0.0, visualization: bool=False):
        """
        Args:
            image_size: The size of the rendered image (`[height, width]`).
            n_pts_per_ray: The number of points sampled along each ray for the
                coarse rendering pass.
            n_pts_per_ray_fine: The number of points sampled along each ray for the
                fine rendering pass.
            n_rays_per_image: Number of Monte Carlo ray samples when training
                (`self.training==True`).
            min_depth: The minimum depth of a sampled ray-point for the coarse rendering.
            max_depth: The maximum depth of a sampled ray-point for the coarse rendering.
            stratified: If `True`, stratifies (=randomly offsets) the depths
                of each ray point during training (`self.training==True`).
            stratified_test: If `True`, stratifies (=randomly offsets) the depths
                of each ray point during evaluation (`self.training==False`).
            chunk_size_test: The number of rays in each chunk of image rays.
                Active only when `self.training==True`.
            n_harmonic_functions_xyz: The number of harmonic functions
                used to form the harmonic embedding of 3D point locations.
            n_harmonic_functions_dir: The number of harmonic functions
                used to form the harmonic embedding of the ray directions.
            n_hidden_neurons_xyz: The number of hidden units in the
                fully connected layers of the MLP that accepts the 3D point
                locations and outputs the occupancy field with the intermediate
                features.
            n_hidden_neurons_dir: The number of hidden units in the
                fully connected layers of the MLP that accepts the intermediate
                features and ray directions and outputs the radiance field
                (per-point colors).
            n_layers_xyz: The number of layers of the MLP that outputs the
                occupancy field.
            append_xyz: The list of indices of the skip layers of the occupancy MLP.
                Prior to evaluating the skip layers, the tensor which was input to MLP
                is appended to the skip layer input.
            density_noise_std: The standard deviation of the random normal noise
                added to the output of the occupancy MLP.
                Active only when `self.training==True`.
            visualization: whether to store extra output for visualization.
        """
        super().__init__()
        self._renderer = torch.nn.ModuleDict()
        self._implicit_function = torch.nn.ModuleDict()
        raymarcher = EmissionAbsorptionNeRFRaymarcher()
        image_height, image_width = image_size
        for render_pass in ('coarse', 'fine'):
            if render_pass == 'coarse':
                raysampler = NeRFRaysampler(n_pts_per_ray=n_pts_per_ray, min_depth=min_depth, max_depth=max_depth, stratified=stratified, stratified_test=stratified_test, n_rays_per_image=n_rays_per_image, image_height=image_height, image_width=image_width)
            elif render_pass == 'fine':
                raysampler = ProbabilisticRaysampler(n_pts_per_ray=n_pts_per_ray_fine, stratified=stratified, stratified_test=stratified_test)
            else:
                raise ValueError(f'No such rendering pass {render_pass}')
            self._renderer[render_pass] = ImplicitRenderer(raysampler=raysampler, raymarcher=raymarcher)
            self._implicit_function[render_pass] = NeuralRadianceField(n_harmonic_functions_xyz=n_harmonic_functions_xyz, n_harmonic_functions_dir=n_harmonic_functions_dir, n_hidden_neurons_xyz=n_hidden_neurons_xyz, n_hidden_neurons_dir=n_hidden_neurons_dir, n_layers_xyz=n_layers_xyz, append_xyz=append_xyz)
        self._density_noise_std = density_noise_std
        self._chunk_size_test = chunk_size_test
        self._image_size = image_size
        self.visualization = visualization

    def precache_rays(self, cache_cameras: List[CamerasBase], cache_camera_hashes: List[str]):
        """
        Precaches the rays emitted from the list of cameras `cache_cameras`,
        where each camera is uniquely identified with the corresponding hash
        from `cache_camera_hashes`.

        The cached rays are moved to cpu and stored in
        `self._renderer['coarse']._ray_cache`.

        Raises `ValueError` when caching two cameras with the same hash.

        Args:
            cache_cameras: A list of `N` cameras for which the rays are pre-cached.
            cache_camera_hashes: A list of `N` unique identifiers for each
                camera from `cameras`.
        """
        self._renderer['coarse'].raysampler.precache_rays(cache_cameras, cache_camera_hashes)

    def _process_ray_chunk(self, camera_hash: Optional[str], camera: CamerasBase, image: torch.Tensor, chunk_idx: int) ->dict:
        """
        Samples and renders a chunk of rays.

        Args:
            camera_hash: A unique identifier of a pre-cached camera.
                If `None`, the cache is not searched and the sampled rays are
                calculated from scratch.
            camera: A batch of cameras from which the scene is rendered.
            image: A batch of corresponding ground truth images of shape
                ('batch_size', ·, ·, 3).
            chunk_idx: The index of the currently rendered ray chunk.
        Returns:
            out: `dict` containing the outputs of the rendering:
                `rgb_coarse`: The result of the coarse rendering pass.
                `rgb_fine`: The result of the fine rendering pass.
                `rgb_gt`: The corresponding ground-truth RGB values.
        """
        coarse_ray_bundle = None
        coarse_weights = None
        for renderer_pass in ('coarse', 'fine'):
            (rgb, weights), ray_bundle_out = self._renderer[renderer_pass](cameras=camera, volumetric_function=self._implicit_function[renderer_pass], chunksize=self._chunk_size_test, chunk_idx=chunk_idx, density_noise_std=self._density_noise_std if self.training else 0.0, input_ray_bundle=coarse_ray_bundle, ray_weights=coarse_weights, camera_hash=camera_hash)
            if renderer_pass == 'coarse':
                rgb_coarse = rgb
                coarse_ray_bundle = ray_bundle_out
                coarse_weights = weights
                if image is not None:
                    rgb_gt = sample_images_at_mc_locs(image[..., :3][None], ray_bundle_out.xys)
                else:
                    rgb_gt = None
            elif renderer_pass == 'fine':
                rgb_fine = rgb
            else:
                raise ValueError(f'No such rendering pass {renderer_pass}')
        out = {'rgb_fine': rgb_fine, 'rgb_coarse': rgb_coarse, 'rgb_gt': rgb_gt}
        if self.visualization:
            out['coarse_ray_bundle'] = type(coarse_ray_bundle)(*[v.detach().cpu() for k, v in coarse_ray_bundle._asdict().items()])
            out['coarse_weights'] = coarse_weights.detach().cpu()
        return out

    def forward(self, camera_hash: Optional[str], camera: CamerasBase, image: torch.Tensor) ->Tuple[dict, dict]:
        """
        Performs the coarse and fine rendering passes of the radiance field
        from the viewpoint of the input `camera`.
        Afterwards, both renders are compared to the input ground truth `image`
        by evaluating the peak signal-to-noise ratio and the mean-squared error.

        The rendering result depends on the `self.training` flag:
            - In the training mode (`self.training==True`), the function renders
              a random subset of image rays (Monte Carlo rendering).
            - In evaluation mode (`self.training==False`), the function renders
              the full image. In order to prevent out-of-memory errors,
              when `self.training==False`, the rays are sampled and rendered
              in batches of size `chunksize`.

        Args:
            camera_hash: A unique identifier of a pre-cached camera.
                If `None`, the cache is not searched and the sampled rays are
                calculated from scratch.
            camera: A batch of cameras from which the scene is rendered.
            image: A batch of corresponding ground truth images of shape
                ('batch_size', ·, ·, 3).
        Returns:
            out: `dict` containing the outputs of the rendering:
                `rgb_coarse`: The result of the coarse rendering pass.
                `rgb_fine`: The result of the fine rendering pass.
                `rgb_gt`: The corresponding ground-truth RGB values.

                The shape of `rgb_coarse`, `rgb_fine`, `rgb_gt` depends on the
                `self.training` flag:
                    If `==True`, all 3 tensors are of shape
                    `(batch_size, n_rays_per_image, 3)` and contain the result
                    of the Monte Carlo training rendering pass.
                    If `==False`, all 3 tensors are of shape
                    `(batch_size, image_size[0], image_size[1], 3)` and contain
                    the result of the full image rendering pass.
            metrics: `dict` containing the error metrics comparing the fine and
                coarse renders to the ground truth:
                `mse_coarse`: Mean-squared error between the coarse render and
                    the input `image`
                `mse_fine`: Mean-squared error between the fine render and
                    the input `image`
                `psnr_coarse`: Peak signal-to-noise ratio between the coarse render and
                    the input `image`
                `psnr_fine`: Peak signal-to-noise ratio between the fine render and
                    the input `image`
        """
        if not self.training:
            n_chunks = self._renderer['coarse'].raysampler.get_n_chunks(self._chunk_size_test, camera.R.shape[0])
        else:
            n_chunks = 1
        chunk_outputs = [self._process_ray_chunk(camera_hash, camera, image, chunk_idx) for chunk_idx in range(n_chunks)]
        if not self.training:
            out = {k: (torch.cat([ch_o[k] for ch_o in chunk_outputs], dim=1).view(-1, *self._image_size, 3) if chunk_outputs[0][k] is not None else None) for k in ('rgb_fine', 'rgb_coarse', 'rgb_gt')}
        else:
            out = chunk_outputs[0]
        metrics = {}
        if image is not None:
            for render_pass in ('coarse', 'fine'):
                for metric_name, metric_fun in zip(('mse', 'psnr'), (calc_mse, calc_psnr)):
                    metrics[f'{metric_name}_{render_pass}'] = metric_fun(out['rgb_' + render_pass][..., :3], out['rgb_gt'][..., :3])
        return out, metrics


class _SymEig3x3(nn.Module):
    """
    Optimized implementation of eigenvalues and eigenvectors computation for symmetric 3x3
     matrices.

    Please see https://en.wikipedia.org/wiki/Eigenvalue_algorithm#3.C3.973_matrices
     and https://www.geometrictools.com/Documentation/RobustEigenSymmetric3x3.pdf
    """

    def __init__(self, eps: Optional[float]=None) ->None:
        """
        Args:
            eps: epsilon to specify, if None then use torch.float eps
        """
        super().__init__()
        self.register_buffer('_identity', torch.eye(3))
        self.register_buffer('_rotation_2d', torch.tensor([[0.0, -1.0], [1.0, 0.0]]))
        self.register_buffer('_rotations_3d', self._create_rotation_matrices(self._rotation_2d))
        self._eps = eps or torch.finfo(torch.float).eps

    @staticmethod
    def _create_rotation_matrices(rotation_2d) ->torch.Tensor:
        """
        Compute rotations for later use in U V computation

        Args:
            rotation_2d: a π/2 rotation matrix.

        Returns:
            a (3, 3, 3) tensor containing 3 rotation matrices around each of the coordinate axes
            by π/2
        """
        rotations_3d = torch.zeros((3, 3, 3))
        rotation_axes = set(range(3))
        for rotation_axis in rotation_axes:
            rest = list(rotation_axes - {rotation_axis})
            rotations_3d[rotation_axis][rest[0], rest] = rotation_2d[0]
            rotations_3d[rotation_axis][rest[1], rest] = rotation_2d[1]
        return rotations_3d

    def forward(self, inputs: torch.Tensor, eigenvectors: bool=True) ->Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Compute eigenvalues and (optionally) eigenvectors

        Args:
            inputs: symmetric matrices with shape of (..., 3, 3)
            eigenvectors: whether should we compute only eigenvalues or eigenvectors as well

        Returns:
            Either a tuple of (eigenvalues, eigenvectors) or eigenvalues only, depending on
             given params. Eigenvalues are of shape (..., 3) and eigenvectors (..., 3, 3)
        """
        if inputs.shape[-2:] != (3, 3):
            raise ValueError('Only inputs of shape (..., 3, 3) are supported.')
        inputs_diag = inputs.diagonal(dim1=-2, dim2=-1)
        inputs_trace = inputs_diag.sum(-1)
        q = inputs_trace / 3.0
        p1 = ((inputs ** 2).sum(dim=(-1, -2)) - (inputs_diag ** 2).sum(-1)) / 2
        p2 = ((inputs_diag - q[..., None]) ** 2).sum(dim=-1) + 2.0 * p1.clamp(self._eps)
        p = torch.sqrt(p2 / 6.0)
        B = (inputs - q[..., None, None] * self._identity) / p[..., None, None]
        r = torch.det(B) / 2.0
        r = r.clamp(-1.0 + self._eps, 1.0 - self._eps)
        phi = torch.acos(r) / 3.0
        eig1 = q + 2 * p * torch.cos(phi)
        eig2 = q + 2 * p * torch.cos(phi + 2 * math.pi / 3)
        eig3 = 3 * q - eig1 - eig2
        eigenvals = torch.stack((eig2, eig3, eig1), dim=-1)
        diag_soft_cond = torch.exp(-(p1 / (6 * self._eps)) ** 2).detach()[..., None]
        diag_eigenvals, _ = torch.sort(inputs_diag, dim=-1)
        eigenvals = diag_soft_cond * diag_eigenvals + (1.0 - diag_soft_cond) * eigenvals
        if eigenvectors:
            eigenvecs = self._construct_eigenvecs_set(inputs, eigenvals)
        else:
            eigenvecs = None
        return eigenvals, eigenvecs

    def _construct_eigenvecs_set(self, inputs: torch.Tensor, eigenvals: torch.Tensor) ->torch.Tensor:
        """
        Construct orthonormal set of eigenvectors by given inputs and pre-computed eigenvalues

        Args:
            inputs: tensor of symmetric matrices of shape (..., 3, 3)
            eigenvals: tensor of pre-computed eigenvalues of of shape (..., 3, 3)

        Returns:
            Tuple of three eigenvector tensors of shape (..., 3, 3), composing an orthonormal
             set
        """
        eigenvecs_tuple_for_01 = self._construct_eigenvecs(inputs, eigenvals[..., 0], eigenvals[..., 1])
        eigenvecs_for_01 = torch.stack(eigenvecs_tuple_for_01, dim=-1)
        eigenvecs_tuple_for_21 = self._construct_eigenvecs(inputs, eigenvals[..., 2], eigenvals[..., 1])
        eigenvecs_for_21 = torch.stack(eigenvecs_tuple_for_21[::-1], dim=-1)
        eigenvecs_cond = (eigenvals[..., 1] - eigenvals[..., 0] > eigenvals[..., 2] - eigenvals[..., 1]).detach()
        eigenvecs = torch.where(eigenvecs_cond[..., None, None], eigenvecs_for_01, eigenvecs_for_21)
        return eigenvecs

    def _construct_eigenvecs(self, inputs: torch.Tensor, alpha0: torch.Tensor, alpha1: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Construct an orthonormal set of eigenvectors by given pair of eigenvalues.

        Args:
            inputs: tensor of symmetric matrices of shape (..., 3, 3)
            alpha0: first eigenvalues of shape (..., 3)
            alpha1: second eigenvalues of shape (..., 3)

        Returns:
            Tuple of three eigenvector tensors of shape (..., 3, 3), composing an orthonormal
             set
        """
        ev0 = self._get_ev0(inputs - alpha0[..., None, None] * self._identity)
        u, v = self._get_uv(ev0)
        ev1 = self._get_ev1(inputs - alpha1[..., None, None] * self._identity, u, v)
        ev2 = torch.cross(ev0, ev1, dim=-1)
        return ev0, ev1, ev2

    def _get_ev0(self, char_poly: torch.Tensor) ->torch.Tensor:
        """
        Construct the first normalized eigenvector given a characteristic polynomial

        Args:
            char_poly: a characteristic polynomials of the input matrices of shape (..., 3, 3)

        Returns:
            Tensor of first eigenvectors of shape (..., 3)
        """
        r01 = torch.cross(char_poly[..., 0, :], char_poly[..., 1, :], dim=-1)
        r12 = torch.cross(char_poly[..., 1, :], char_poly[..., 2, :], dim=-1)
        r02 = torch.cross(char_poly[..., 0, :], char_poly[..., 2, :], dim=-1)
        cross_products = torch.stack((r01, r12, r02), dim=-2)
        cross_products += self._eps * self._sign_without_zero(cross_products[..., :1, :])
        norms_sq = (cross_products ** 2).sum(dim=-1)
        max_norms_index = norms_sq.argmax(dim=-1)
        max_cross_products = self._gather_by_index(cross_products, max_norms_index[..., None, None], -2)
        max_norms_sq = self._gather_by_index(norms_sq, max_norms_index[..., None], -1)
        return max_cross_products / torch.sqrt(max_norms_sq[..., None])

    def _gather_by_index(self, source: torch.Tensor, index: torch.Tensor, dim: int) ->torch.Tensor:
        """
        Selects elements from the given source tensor by provided index tensor.
        Number of dimensions should be the same for source and index tensors.

        Args:
            source: input tensor to gather from
            index: index tensor with indices to gather from source
            dim: dimension to gather across

        Returns:
            Tensor of shape same as the source with exception of specified dimension.
        """
        index_shape = list(source.shape)
        index_shape[dim] = 1
        return source.gather(dim, index.expand(index_shape)).squeeze(dim)

    def _get_uv(self, w: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Computes unit-length vectors U and V such that {U, V, W} is a right-handed
        orthonormal set.

        Args:
            w: eigenvector tensor of shape (..., 3)

        Returns:
            Tuple of U and V unit-length vector tensors of shape (..., 3)
        """
        min_idx = w.abs().argmin(dim=-1)
        rotation_2d = self._rotations_3d[min_idx]
        u = F.normalize((rotation_2d @ w[..., None])[..., 0], dim=-1)
        v = torch.cross(w, u, dim=-1)
        return u, v

    def _get_ev1(self, char_poly: torch.Tensor, u: torch.Tensor, v: torch.Tensor) ->torch.Tensor:
        """
        Computes the second normalized eigenvector given a characteristic polynomial
        and U and V vectors

        Args:
            char_poly: a characteristic polynomials of the input matrices of shape (..., 3, 3)
            u: unit-length vectors from _get_uv method
            v: unit-length vectors from _get_uv method

        Returns:
            desc
        """
        j = torch.stack((u, v), dim=-1)
        m = j.transpose(-1, -2) @ char_poly @ j
        is_acute_sign = self._sign_without_zero((m[..., 0, :] * m[..., 1, :]).sum(dim=-1)).detach()
        rowspace = m[..., 0, :] + is_acute_sign[..., None] * m[..., 1, :]
        rowspace += self._eps * self._sign_without_zero(rowspace[..., :1])
        return (j @ F.normalize(rowspace @ self._rotation_2d, dim=-1)[..., None])[..., 0]

    @staticmethod
    def _sign_without_zero(tensor):
        """
        Args:
            tensor: an arbitrary shaped tensor

        Returns:
            Tensor of the same shape as an input, but with 1.0 if tensor > 0.0 and -1.0
             otherwise
        """
        return 2.0 * (tensor > 0.0) - 1.0


class EvaluationMode(Enum):
    TRAINING = 'training'
    EVALUATION = 'evaluation'


class FeatureExtractorBase(ReplaceableBase, torch.nn.Module):
    """
    Base class for an extractor of a set of features from images.
    """

    def __init__(self):
        super().__init__()

    def get_feat_dims(self) ->int:
        """
        Returns:
            total number of feature dimensions of the output.
            (i.e. sum_i(dim_i))
        """
        raise NotImplementedError

    def forward(self, imgs: Optional[torch.Tensor], masks: Optional[torch.Tensor]=None, **kwargs) ->Dict[Any, torch.Tensor]:
        """
        Args:
            imgs: A batch of input images of shape `(B, 3, H, W)`.
            masks: A batch of input masks of shape `(B, 3, H, W)`.

        Returns:
            out_feats: A dict `{f_i: t_i}` keyed by predicted feature names `f_i`
                and their corresponding tensors `t_i` of shape `(B, dim_i, H_i, W_i)`.
        """
        raise NotImplementedError


IMAGE_FEATURE_NAME = 'image'


MASK_FEATURE_NAME = 'mask'


_FEAT_DIMS = {'resnet18': (64, 128, 256, 512), 'resnet34': (64, 128, 256, 512), 'resnet50': (256, 512, 1024, 2048), 'resnet101': (256, 512, 1024, 2048), 'resnet152': (256, 512, 1024, 2048)}


_RESNET_MEAN = [0.485, 0.456, 0.406]


_RESNET_STD = [0.229, 0.224, 0.225]


logger = logging.getLogger(__name__)


class ResNetFeatureExtractor(FeatureExtractorBase):
    """
    Implements an image feature extractor. Depending on the settings allows
    to extract:
        - deep features: A CNN ResNet backbone from torchvision (with/without
            pretrained weights) which extracts deep features.
        - masks: Segmentation masks.
        - images: Raw input RGB images.

    Settings:
        name: name of the resnet backbone (from torchvision)
        pretrained: If true, will load the pretrained weights
        stages: List of stages from which to extract features.
            Features from each stage are returned as key value
            pairs in the forward function
        normalize_image: If set will normalize the RGB values of
            the image based on the Resnet mean/std
        image_rescale: If not 1.0, this rescale factor will be
            used to resize the image
        first_max_pool: If set, a max pool layer is added after the first
            convolutional layer
        proj_dim: The number of output channels for the convolutional layers
        l2_norm: If set, l2 normalization is applied to the extracted features
        add_masks: If set, the masks will be saved in the output dictionary
        add_images: If set, the images will be saved in the output dictionary
        global_average_pool: If set, global average pooling step is performed
        feature_rescale: If not 1.0, this rescale factor will be used to
            rescale the output features
    """
    name: str = 'resnet34'
    pretrained: bool = True
    stages: Tuple[int, ...] = (1, 2, 3, 4)
    normalize_image: bool = True
    image_rescale: float = 128 / 800.0
    first_max_pool: bool = True
    proj_dim: int = 32
    l2_norm: bool = True
    add_masks: bool = True
    add_images: bool = True
    global_average_pool: bool = False
    feature_rescale: float = 1.0

    def __post_init__(self):
        super().__init__()
        if self.normalize_image:
            for k, v in (('_resnet_mean', _RESNET_MEAN), ('_resnet_std', _RESNET_STD)):
                self.register_buffer(k, torch.FloatTensor(v).view(1, 3, 1, 1), persistent=False)
        self._feat_dim = {}
        if len(self.stages) == 0:
            pass
        else:
            net = getattr(torchvision.models, self.name)(pretrained=self.pretrained)
            if self.first_max_pool:
                self.stem = torch.nn.Sequential(net.conv1, net.bn1, net.relu, net.maxpool)
            else:
                self.stem = torch.nn.Sequential(net.conv1, net.bn1, net.relu)
            self.max_stage = max(self.stages)
            self.layers = torch.nn.ModuleList()
            self.proj_layers = torch.nn.ModuleList()
            for stage in range(self.max_stage):
                stage_name = f'layer{stage + 1}'
                feature_name = self._get_resnet_stage_feature_name(stage)
                if stage + 1 in self.stages:
                    if self.proj_dim > 0 and _FEAT_DIMS[self.name][stage] > self.proj_dim:
                        proj = torch.nn.Conv2d(_FEAT_DIMS[self.name][stage], self.proj_dim, 1, 1, bias=True)
                        self._feat_dim[feature_name] = self.proj_dim
                    else:
                        proj = torch.nn.Identity()
                        self._feat_dim[feature_name] = _FEAT_DIMS[self.name][stage]
                else:
                    proj = torch.nn.Identity()
                self.proj_layers.append(proj)
                self.layers.append(getattr(net, stage_name))
        if self.add_masks:
            self._feat_dim[MASK_FEATURE_NAME] = 1
        if self.add_images:
            self._feat_dim[IMAGE_FEATURE_NAME] = 3
        logger.info(f'Feat extractor total dim = {self.get_feat_dims()}')
        self.stages = set(self.stages)

    def _get_resnet_stage_feature_name(self, stage) ->str:
        return f'res_layer_{stage + 1}'

    def _resnet_normalize_image(self, img: torch.Tensor) ->torch.Tensor:
        return (img - self._resnet_mean) / self._resnet_std

    def get_feat_dims(self) ->int:
        return sum(self._feat_dim.values())

    def forward(self, imgs: Optional[torch.Tensor], masks: Optional[torch.Tensor]=None, **kwargs) ->Dict[Any, torch.Tensor]:
        """
        Args:
            imgs: A batch of input images of shape `(B, 3, H, W)`.
            masks: A batch of input masks of shape `(B, 3, H, W)`.

        Returns:
            out_feats: A dict `{f_i: t_i}` keyed by predicted feature names `f_i`
                and their corresponding tensors `t_i` of shape `(B, dim_i, H_i, W_i)`.
        """
        out_feats = {}
        imgs_input = imgs
        if self.image_rescale != 1.0 and imgs_input is not None:
            imgs_resized = Fu.interpolate(imgs_input, scale_factor=self.image_rescale, mode='bilinear')
        else:
            imgs_resized = imgs_input
        if len(self.stages) > 0:
            assert imgs_resized is not None
            if self.normalize_image:
                imgs_normed = self._resnet_normalize_image(imgs_resized)
            else:
                imgs_normed = imgs_resized
            feats = self.stem(imgs_normed)
            for stage, (layer, proj) in enumerate(zip(self.layers, self.proj_layers)):
                feats = layer(feats)
                assert feats.shape[1] == _FEAT_DIMS[self.name][stage]
                if stage + 1 in self.stages:
                    f = proj(feats)
                    if self.global_average_pool:
                        f = f.mean(dims=(2, 3))
                    if self.l2_norm:
                        normfac = 1.0 / math.sqrt(len(self.stages))
                        f = Fu.normalize(f, dim=1) * normfac
                    feature_name = self._get_resnet_stage_feature_name(stage)
                    out_feats[feature_name] = f
        if self.add_masks:
            assert masks is not None
            out_feats[MASK_FEATURE_NAME] = masks
        if self.add_images:
            assert imgs_resized is not None
            out_feats[IMAGE_FEATURE_NAME] = imgs_resized
        if self.feature_rescale != 1.0:
            out_feats = {k: (self.feature_rescale * f) for k, f in out_feats.items()}
        return out_feats


class ImplicitFunctionWrapper(torch.nn.Module):

    def __init__(self, fn: torch.nn.Module):
        super().__init__()
        self._fn = fn
        self.bound_args = {}

    def bind_args(self, **bound_args):
        self.bound_args = bound_args
        self._fn.on_bind_args()

    def unbind_args(self):
        self.bound_args = {}

    def forward(self, *args, **kwargs):
        return self._fn(*args, **{**kwargs, **self.bound_args})


class _PackedToPadded(Function):
    """
    Torch autograd Function wrapper for packed_to_padded C++/CUDA implementations.
    """

    @staticmethod
    def forward(ctx, inputs, first_idxs, max_size):
        """
        Args:
            ctx: Context object used to calculate gradients.
            inputs: FloatTensor of shape (F, D), representing the packed batch tensor.
                e.g. areas for faces in a batch of meshes.
            first_idxs: LongTensor of shape (N,) where N is the number of
                elements in the batch and `first_idxs[i] = f`
                means that the inputs for batch element i begin at `inputs[f]`.
            max_size: Max length of an element in the batch.

        Returns:
            inputs_padded: FloatTensor of shape (N, max_size, D) where max_size is max
                of `sizes`. The values for batch element i which start at
                `inputs[first_idxs[i]]` will be copied to `inputs_padded[i, :]`,
                with zeros padding out the extra inputs.
        """
        if not inputs.dim() == 2:
            raise ValueError('input can only be 2-dimensional.')
        if not first_idxs.dim() == 1:
            raise ValueError('first_idxs can only be 1-dimensional.')
        if not inputs.dtype == torch.float32:
            raise ValueError('input has to be of type torch.float32.')
        if not first_idxs.dtype == torch.int64:
            raise ValueError('first_idxs has to be of type torch.int64.')
        if not isinstance(max_size, int):
            raise ValueError('max_size has to be int.')
        ctx.save_for_backward(first_idxs)
        ctx.num_inputs = int(inputs.shape[0])
        inputs, first_idxs = inputs.contiguous(), first_idxs.contiguous()
        inputs_padded = _C.packed_to_padded(inputs, first_idxs, max_size)
        return inputs_padded

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        grad_output = grad_output.contiguous()
        first_idxs = ctx.saved_tensors[0]
        num_inputs = ctx.num_inputs
        grad_input = _C.padded_to_packed(grad_output, first_idxs, num_inputs)
        return grad_input, None, None


def packed_to_padded(inputs: torch.Tensor, first_idxs: torch.LongTensor, max_size: int) ->torch.Tensor:
    """
    Torch wrapper that handles allowed input shapes. See description below.

    Args:
        inputs: FloatTensor of shape (F,) or (F, ...), representing the packed
            batch tensor, e.g. areas for faces in a batch of meshes.
        first_idxs: LongTensor of shape (N,) where N is the number of
            elements in the batch and `first_idxs[i] = f`
            means that the inputs for batch element i begin at `inputs[f]`.
        max_size: Max length of an element in the batch.

    Returns:
        inputs_padded: FloatTensor of shape (N, max_size) or (N, max_size, ...)
            where max_size is max of `sizes`. The values for batch element i
            which start at `inputs[first_idxs[i]]` will be copied to
            `inputs_padded[i, :]`, with zeros padding out the extra inputs.

    To handle the allowed input shapes, we convert the inputs tensor of shape
    (F,) to (F, 1). We reshape the output back to (N, max_size) from
    (N, max_size, 1).
    """
    input_shape = inputs.shape
    n_dims = inputs.dim()
    if n_dims == 1:
        inputs = inputs.unsqueeze(1)
    else:
        inputs = inputs.reshape(input_shape[0], -1)
    inputs_padded = _PackedToPadded.apply(inputs, first_idxs, max_size)
    if n_dims == 1:
        return inputs_padded.squeeze(2)
    if n_dims == 2:
        return inputs_padded
    return inputs_padded.view(*inputs_padded.shape[:2], *input_shape[1:])


class GlobalEncoderBase(ReplaceableBase):
    """
    A base class for implementing encoders of global frame-specific quantities.

    The latter includes e.g. the harmonic encoding of a frame timestamp
    (`HarmonicTimeEncoder`), or an autodecoder encoding of the frame's sequence
    (`SequenceAutodecoder`).
    """

    def __init__(self) ->None:
        super().__init__()

    def get_encoding_dim(self):
        """
        Returns the dimensionality of the returned encoding.
        """
        raise NotImplementedError()

    def calculate_squared_encoding_norm(self) ->Optional[torch.Tensor]:
        """
        Calculates the squared norm of the encoding to report as the
        `autodecoder_norm` loss of the model, as a zero dimensional tensor.
        """
        raise NotImplementedError()

    def forward(self, *, frame_timestamp: Optional[torch.Tensor]=None, sequence_name: Optional[Union[torch.LongTensor, List[str]]]=None, **kwargs) ->torch.Tensor:
        """
        Given a set of inputs to encode, generates a tensor containing the encoding.

        Returns:
            encoding: The tensor containing the global encoding.
        """
        raise NotImplementedError()


class RegularizationMetricsBase(ReplaceableBase, torch.nn.Module):
    """
    Replaceable abstract base for regularization metrics.
    `forward()` method produces regularization metrics and (unlike ViewMetrics) can
    depend on the model's parameters.
    """

    def __post_init__(self) ->None:
        super().__init__()

    def forward(self, model: Any, keys_prefix: str='loss_', **kwargs) ->Dict[str, Any]:
        """
        Calculates various regularization terms useful for supervising differentiable
        rendering pipelines.

        Args:
            model: A model instance. Useful, for example, to implement
                weights-based regularization.
            keys_prefix: A common prefix for all keys in the output dictionary
                containing all regularization metrics.

        Returns:
            A dictionary with the resulting regularization metrics. The items
                will have form `{metric_name_i: metric_value_i}` keyed by the
                names of the output metrics `metric_name_i` with their corresponding
                values `metric_value_i` represented as 0-dimensional float tensors.
        """
        raise NotImplementedError


class RenderSamplingMode(Enum):
    MASK_SAMPLE = 'mask_sample'
    FULL_GRID = 'full_grid'


class FeatureAggregatorBase(ABC, ReplaceableBase):
    """
    Base class for aggregating features.

    Typically, the aggregated features and their masks are output by `ViewSampler`
    which samples feature tensors extracted from a set of source images.

    Settings:
        exclude_target_view: If `True`/`False`, enables/disables pooling
            from target view to itself.
        exclude_target_view_mask_features: If `True`,
            mask the features from the target view before aggregation
        concatenate_output: If `True`,
            concatenate the aggregated features into a single tensor,
            otherwise return a dictionary mapping feature names to tensors.
    """
    exclude_target_view: bool = True
    exclude_target_view_mask_features: bool = True
    concatenate_output: bool = True

    @abstractmethod
    def forward(self, feats_sampled: Dict[str, torch.Tensor], masks_sampled: torch.Tensor, camera: Optional[CamerasBase]=None, pts: Optional[torch.Tensor]=None, **kwargs) ->Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            feats_sampled: A `dict` of sampled feature tensors `{f_i: t_i}`,
                where each `t_i` is a tensor of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
            masks_sampled: A binary mask represented as a tensor of shape
                `(minibatch, n_source_views, n_samples, 1)` denoting valid
                sampled features.
            camera: A batch of `n_source_views` `CamerasBase` objects corresponding
                to the source view cameras.
            pts: A tensor of shape `(minibatch, n_samples, 3)` denoting the
                3D points whose 2D projections to source views were sampled in
                order to generate `feats_sampled` and `masks_sampled`.

        Returns:
            feats_aggregated: If `concatenate_output==True`, a tensor
                of shape `(minibatch, reduce_dim, n_samples, sum(dim_1, ... dim_N))`
                containing the concatenation of the aggregated features `feats_sampled`.
                `reduce_dim` depends on the specific feature aggregator
                implementation and typically equals 1 or `n_source_views`.
                If `concatenate_output==False`, the aggregator does not concatenate
                the aggregated features and returns a dictionary of per-feature
                aggregations `{f_i: t_i_aggregated}` instead. Each `t_i_aggregated`
                is of shape `(minibatch, reduce_dim, n_samples, aggr_dim_i)`.
        """
        raise NotImplementedError()

    @abstractmethod
    def get_aggregated_feature_dim(self, feats_or_feats_dim: Union[Dict[str, torch.Tensor], int]):
        """
        Returns the final dimensionality of the output aggregated features.

        Args:
            feats_or_feats_dim: Either a `dict` of sampled features `{f_i: t_i}` corresponding
                to the `feats_sampled` argument of `forward`,
                or an `int` representing the sum of dimensionalities of each `t_i`.

        Returns:
            aggregated_feature_dim: The final dimensionality of the output
                aggregated features.
        """
        raise NotImplementedError()

    def has_aggregation(self) ->bool:
        """
        Specifies whether the aggregator reduces the output `reduce_dim` dimension to 1.

        Returns:
            has_aggregation: `True` if `reduce_dim==1`, else `False`.
        """
        return hasattr(self, 'reduction_functions')


def handle_seq_id(seq_id: Union[torch.LongTensor, List[str], List[int]], device) ->torch.LongTensor:
    """
    Converts the input sequence id to a LongTensor.

    Args:
        seq_id: A sequence of sequence ids.
        device: The target device of the output.
    Returns
        long_seq_id: `seq_id` converted to a `LongTensor` and moved to `device`.
    """
    if not torch.is_tensor(seq_id):
        if isinstance(seq_id[0], str):
            seq_id = [hash(s) for s in seq_id]
        seq_id = torch.tensor(seq_id, dtype=torch.long, device=device)
    return seq_id


def cameras_points_cartesian_product(camera: CamerasBase, pts: torch.Tensor) ->Tuple[CamerasBase, torch.Tensor]:
    """
    Generates all pairs of pairs of elements from 'camera' and 'pts' and returns
    `camera_rep` and `pts_rep` such that::

        camera_rep = [                 pts_rep = [
            camera[0]                      pts[0],
            camera[0]                      pts[1],
            camera[0]                      ...,
            ...                            pts[batch_pts-1],
            camera[1]                      pts[0],
            camera[1]                      pts[1],
            camera[1]                      ...,
            ...                            pts[batch_pts-1],
            ...                            ...,
            camera[n_cameras-1]            pts[0],
            camera[n_cameras-1]            pts[1],
            camera[n_cameras-1]            ...,
            ...                            pts[batch_pts-1],
        ]                              ]

    Args:
        camera: A batch of `n_cameras` cameras.
        pts: A batch of `batch_pts` points of shape `(batch_pts, ..., dim)`

    Returns:
        camera_rep: A batch of batch_pts*n_cameras cameras such that::

            camera_rep = [
                camera[0]
                camera[0]
                camera[0]
                ...
                camera[1]
                camera[1]
                camera[1]
                ...
                ...
                camera[n_cameras-1]
                camera[n_cameras-1]
                camera[n_cameras-1]
            ]


        pts_rep: Repeated `pts` of shape `(batch_pts*n_cameras, ..., dim)`,
            such that::

            pts_rep = [
                pts[0],
                pts[1],
                ...,
                pts[batch_pts-1],
                pts[0],
                pts[1],
                ...,
                pts[batch_pts-1],
                ...,
                pts[0],
                pts[1],
                ...,
                pts[batch_pts-1],
            ]

    """
    n_cameras = camera.R.shape[0]
    batch_pts = pts.shape[0]
    pts_rep = pts.repeat(n_cameras, *[(1) for _ in pts.shape[1:]])
    idx_cams = torch.arange(n_cameras)[:, None].expand(n_cameras, batch_pts).reshape(batch_pts * n_cameras)
    camera_rep = camera[idx_cams]
    return camera_rep, pts_rep


def ndc_to_grid_sample_coords(xy_ndc: torch.Tensor, image_size_hw: Tuple[int, int]) ->torch.Tensor:
    """
    Convert from the PyTorch3D's NDC coordinates to
    `torch.nn.functional.grid_sampler`'s coordinates.

    Args:
        xy_ndc: Tensor of shape `(..., 2)` containing 2D points in the
            PyTorch3D's NDC coordinates.
        image_size_hw: A tuple `(image_height, image_width)` denoting the
            height and width of the image tensor to sample.
    Returns:
        xy_grid_sample: Tensor of shape `(..., 2)` containing 2D points in the
            `torch.nn.functional.grid_sample` coordinates.
    """
    if len(image_size_hw) != 2 or any(s <= 0 for s in image_size_hw):
        raise ValueError("'image_size_hw' has to be a 2-tuple of positive integers")
    aspect = min(image_size_hw) / max(image_size_hw)
    xy_grid_sample = -xy_ndc
    if image_size_hw[0] >= image_size_hw[1]:
        xy_grid_sample[..., 1] *= aspect
    else:
        xy_grid_sample[..., 0] *= aspect
    return xy_grid_sample


def ndc_grid_sample(input: torch.Tensor, grid_ndc: torch.Tensor, *, align_corners: bool=False, **grid_sample_kwargs) ->torch.Tensor:
    """
    Samples a tensor `input` of shape `(B, dim, H, W)` at 2D locations
    specified by a tensor `grid_ndc` of shape `(B, ..., 2)` using
    the `torch.nn.functional.grid_sample` function.
    `grid_ndc` is specified in PyTorch3D NDC coordinate frame.

    Args:
        input: The tensor of shape `(B, dim, H, W)` to be sampled.
        grid_ndc: A tensor of shape `(B, ..., 2)` denoting the set of
            2D locations at which `input` is sampled.
            See [1] for a detailed description of the NDC coordinates.
        align_corners: Forwarded to the `torch.nn.functional.grid_sample`
            call. See its docstring.
        grid_sample_kwargs: Additional arguments forwarded to the
            `torch.nn.functional.grid_sample` call. See the corresponding
            docstring for a listing of the corresponding arguments.

    Returns:
        sampled_input: A tensor of shape `(B, dim, ...)` containing the samples
            of `input` at 2D locations `grid_ndc`.

    References:
        [1] https://pytorch3d.org/docs/cameras
    """
    batch, *spatial_size, pt_dim = grid_ndc.shape
    if batch != input.shape[0]:
        raise ValueError("'input' and 'grid_ndc' have to have the same batch size.")
    if input.ndim != 4:
        raise ValueError("'input' has to be a 4-dimensional Tensor.")
    if pt_dim != 2:
        raise ValueError("The last dimension of 'grid_ndc' has to be == 2.")
    grid_ndc_flat = grid_ndc.reshape(batch, -1, 1, 2)
    grid_flat = ndc_to_grid_sample_coords(grid_ndc_flat, input.shape[2:])
    sampled_input_flat = torch.nn.functional.grid_sample(input, grid_flat, align_corners=align_corners, **grid_sample_kwargs)
    sampled_input = sampled_input_flat.reshape([batch, input.shape[1], *spatial_size])
    return sampled_input


def project_points_and_sample(pts: torch.Tensor, feats: Dict[str, torch.Tensor], camera: CamerasBase, masks: Optional[torch.Tensor], eps: float=0.01, sampling_mode: str='bilinear') ->Tuple[Dict[str, torch.Tensor], torch.Tensor]:
    """
    Project each point cloud from a batch of point clouds to all input cameras
    and sample features at the 2D projection locations.

    Args:
        pts: `(pts_batch, n_pts, 3)` tensor containing a batch of 3D point clouds.
        feats: A dict `{feat_i: feat_T_i}` of features to sample,
            where each `feat_T_i` is a tensor of shape
            `(n_cameras, feat_i_dim, feat_i_H, feat_i_W)`
            of `feat_i_dim`-dimensional features extracted from `n_cameras`
            source views.
        camera: A batch of `n_cameras` cameras corresponding to their feature
            tensors `feat_T_i` from `feats`.
        masks: A tensor of shape `(n_cameras, 1, mask_H, mask_W)` denoting
            valid locations for sampling.
        eps: A small constant controlling the minimum depth of projections
            of `pts` to avoid divisons by zero in the projection operation.
        sampling_mode: Sampling mode of the grid sampler.

    Returns:
        sampled_feats: Dict of sampled features `{feat_i: sampled_T_i}`.
            Each `sampled_T_i` is of shape
            `(pts_batch, n_cameras, n_pts, feat_i_dim)`.
        sampled_masks: A tensor with the mask of the sampled features
            of shape `(pts_batch, n_cameras, n_pts, 1)`.
            If `masks` is `None`, the returned `sampled_masks` will be
            filled with 1s.
    """
    n_cameras = camera.R.shape[0]
    pts_batch = pts.shape[0]
    n_pts = pts.shape[1:-1]
    camera_rep, pts_rep = cameras_points_cartesian_product(camera, pts)
    proj_rep = camera_rep.transform_points(pts_rep.reshape(n_cameras * pts_batch, -1, 3), eps=eps)[..., :2]
    sampling_grid_ndc = proj_rep.view(n_cameras, pts_batch, -1, 2)
    feats_sampled = {k: ndc_grid_sample(f, sampling_grid_ndc, mode=sampling_mode, align_corners=False).permute(2, 0, 3, 1).reshape(pts_batch, n_cameras, *n_pts, -1) for k, f in feats.items()}
    if masks is not None:
        masks_sampled = ndc_grid_sample(masks, sampling_grid_ndc, mode=sampling_mode, align_corners=False).permute(2, 0, 3, 1).reshape(pts_batch, n_cameras, *n_pts, 1)
    else:
        masks_sampled = sampling_grid_ndc.new_ones(pts_batch, n_cameras, *n_pts, 1)
    return feats_sampled, masks_sampled


class ViewSampler(Configurable, torch.nn.Module):
    """
    Implements sampling of image-based features at the 2d projections of a set
    of 3D points.

    Args:
        masked_sampling: If `True`, the `sampled_masks` output of `self.forward`
            contains the input `masks` sampled at the 2d projections. Otherwise,
            all entries of `sampled_masks` are set to 1.
        sampling_mode: Controls the mode of the `torch.nn.functional.grid_sample`
            function used to interpolate the sampled feature tensors at the
            locations of the 2d projections.
    """
    masked_sampling: bool = False
    sampling_mode: str = 'bilinear'

    def __post_init__(self):
        super().__init__()

    def forward(self, *, pts: torch.Tensor, seq_id_pts: Union[List[int], List[str], torch.LongTensor], camera: CamerasBase, seq_id_camera: Union[List[int], List[str], torch.LongTensor], feats: Dict[str, torch.Tensor], masks: Optional[torch.Tensor], **kwargs) ->Tuple[Dict[str, torch.Tensor], torch.Tensor]:
        """
        Project each point cloud from a batch of point clouds to corresponding
        input cameras and sample features at the 2D projection locations.

        Args:
            pts: A tensor of shape `[pts_batch x n_pts x 3]` in world coords.
            seq_id_pts: LongTensor of shape `[pts_batch]` denoting the ids of the scenes
                from which `pts` were extracted, or a list of string names.
            camera: 'n_cameras' cameras, each coresponding to a batch element of `feats`.
            seq_id_camera: LongTensor of shape `[n_cameras]` denoting the ids of the scenes
                corresponding to cameras in `camera`, or a list of string names.
            feats: a dict of tensors of per-image features `{feat_i: T_i}`.
                Each tensor `T_i` is of shape `[n_cameras x dim_i x H_i x W_i]`.
            masks: `[n_cameras x 1 x H x W]`, define valid image regions
                for sampling `feats`.
        Returns:
            sampled_feats: Dict of sampled features `{feat_i: sampled_T_i}`.
                Each `sampled_T_i` of shape `[pts_batch, n_cameras, n_pts, dim_i]`.
            sampled_masks: A tensor with  mask of the sampled features
                of shape `(pts_batch, n_cameras, n_pts, 1)`.
        """
        seq_id_pts, seq_id_camera = [handle_seq_id(seq_id, pts.device) for seq_id in [seq_id_pts, seq_id_camera]]
        if self.masked_sampling and masks is None:
            raise ValueError('Masks have to be provided for `self.masked_sampling==True`')
        sampled_feats_all_cams, sampled_masks_all_cams = project_points_and_sample(pts, feats, camera, masks if self.masked_sampling else None, sampling_mode=self.sampling_mode)
        camera_pts_mask = (seq_id_camera[None] == seq_id_pts[:, None])[..., None, None]
        sampled_feats = {k: (f * camera_pts_mask) for k, f in sampled_feats_all_cams.items()}
        sampled_masks = sampled_masks_all_cams * camera_pts_mask
        return sampled_feats, sampled_masks


def run_auto_creation(self: Any) ->None:
    """
    Run all the functions named in self._creation_functions.
    """
    for create_function in self._creation_functions:
        getattr(self, create_function)()


class ViewPooler(Configurable, torch.nn.Module):
    """
    Implements sampling of image-based features at the 2d projections of a set
    of 3D points, and a subsequent aggregation of the resulting set of features
    per-point.

    Args:
        view_sampler: An instance of ViewSampler which is used for sampling of
            image-based features at the 2D projections of a set
            of 3D points.
        feature_aggregator_class_type: The name of the feature aggregator class which
            is available in the global registry.
        feature_aggregator: A feature aggregator class which inherits from
            FeatureAggregatorBase. Typically, the aggregated features and their
            masks are output by a `ViewSampler` which samples feature tensors extracted
            from a set of source images. FeatureAggregator executes step (4) above.
    """
    view_sampler: ViewSampler
    feature_aggregator_class_type: str = 'AngleWeightedReductionFeatureAggregator'
    feature_aggregator: FeatureAggregatorBase

    def __post_init__(self):
        super().__init__()
        run_auto_creation(self)

    def get_aggregated_feature_dim(self, feats: Union[Dict[str, torch.Tensor], int]):
        """
        Returns the final dimensionality of the output aggregated features.

        Args:
            feats: Either a `dict` of sampled features `{f_i: t_i}` corresponding
                to the `feats_sampled` argument of `feature_aggregator,forward`,
                or an `int` representing the sum of dimensionalities of each `t_i`.

        Returns:
            aggregated_feature_dim: The final dimensionality of the output
                aggregated features.
        """
        return self.feature_aggregator.get_aggregated_feature_dim(feats)

    def has_aggregation(self):
        """
        Specifies whether the `feature_aggregator` reduces the output `reduce_dim`
        dimension to 1.

        Returns:
            has_aggregation: `True` if `reduce_dim==1`, else `False`.
        """
        return self.feature_aggregator.has_aggregation()

    def forward(self, *, pts: torch.Tensor, seq_id_pts: Union[List[int], List[str], torch.LongTensor], camera: CamerasBase, seq_id_camera: Union[List[int], List[str], torch.LongTensor], feats: Dict[str, torch.Tensor], masks: Optional[torch.Tensor], **kwargs) ->Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Project each point cloud from a batch of point clouds to corresponding
        input cameras, sample features at the 2D projection locations in a batch
        of source images, and aggregate the pointwise sampled features.

        Args:
            pts: A tensor of shape `[pts_batch x n_pts x 3]` in world coords.
            seq_id_pts: LongTensor of shape `[pts_batch]` denoting the ids of the scenes
                from which `pts` were extracted, or a list of string names.
            camera: 'n_cameras' cameras, each coresponding to a batch element of `feats`.
            seq_id_camera: LongTensor of shape `[n_cameras]` denoting the ids of the scenes
                corresponding to cameras in `camera`, or a list of string names.
            feats: a dict of tensors of per-image features `{feat_i: T_i}`.
                Each tensor `T_i` is of shape `[n_cameras x dim_i x H_i x W_i]`.
            masks: `[n_cameras x 1 x H x W]`, define valid image regions
                for sampling `feats`.
        Returns:
            feats_aggregated: If `feature_aggregator.concatenate_output==True`, a tensor
                of shape `(pts_batch, reduce_dim, n_pts, sum(dim_1, ... dim_N))`
                containing the aggregated features. `reduce_dim` depends on
                the specific feature aggregator implementation and typically
                equals 1 or `n_cameras`.
                If `feature_aggregator.concatenate_output==False`, the aggregator
                does not concatenate the aggregated features and returns a dictionary
                of per-feature aggregations `{f_i: t_i_aggregated}` instead.
                Each `t_i_aggregated` is of shape
                `(pts_batch, reduce_dim, n_pts, aggr_dim_i)`.
        """
        sampled_feats, sampled_masks = self.view_sampler(pts=pts, seq_id_pts=seq_id_pts, camera=camera, seq_id_camera=seq_id_camera, feats=feats, masks=masks)
        feats_aggregated = self.feature_aggregator(sampled_feats, sampled_masks, pts=pts, camera=camera)
        return feats_aggregated


def cat_dataclass(batch, tensor_collator: Callable):
    """
    Concatenate all fields of a list of dataclasses `batch` to a single
    dataclass object using `tensor_collator`.

    Args:
        batch: Input list of dataclasses.

    Returns:
        concatenated_batch: All elements of `batch` concatenated to a single
            dataclass object.
        tensor_collator: The function used to concatenate tensor fields.
    """
    elem = batch[0]
    collated = {}
    for f in dataclasses.fields(elem):
        elem_f = getattr(elem, f.name)
        if elem_f is None:
            collated[f.name] = None
        elif torch.is_tensor(elem_f):
            collated[f.name] = tensor_collator([getattr(e, f.name) for e in batch])
        elif dataclasses.is_dataclass(elem_f):
            collated[f.name] = cat_dataclass([getattr(e, f.name) for e in batch], tensor_collator)
        elif isinstance(elem_f, collections.abc.Mapping):
            collated[f.name] = {k: (tensor_collator([getattr(e, f.name)[k] for e in batch]) if elem_f[k] is not None else None) for k in elem_f}
        else:
            raise ValueError('Unsupported field type for concatenation')
    return type(elem)(**collated)


def _apply_chunked(func, chunk_generator, tensor_collator):
    """
    Helper function to apply a function on a sequence of
    chunked inputs yielded by a generator and collate
    the result.
    """
    processed_chunks = [func(*chunk_args, **chunk_kwargs) for chunk_args, chunk_kwargs in chunk_generator]
    return cat_dataclass(processed_chunks, tensor_collator)


def prod(iterable, *, start=1):
    """
    Like math.prod in Python 3.8 and later.
    """
    for i in iterable:
        start *= i
    return start


def _tensor_collator(batch, new_dims) ->torch.Tensor:
    """
    Helper function to reshape the batch to the desired shape
    """
    return torch.cat(batch, dim=1).reshape(*new_dims, -1)


_FocalLengthType = Union[float, Sequence[Tuple[float]], Sequence[Tuple[float, float]], torch.Tensor]


_R = torch.eye(3)[None]


_T = torch.zeros(1, 3)


def _get_sfm_calibration_matrix(N: int, device: Device, focal_length, principal_point, orthographic: bool=False) ->torch.Tensor:
    """
    Returns a calibration matrix of a perspective/orthographic camera.

    Args:
        N: Number of cameras.
        focal_length: Focal length of the camera.
        principal_point: xy coordinates of the center of
            the principal point of the camera in pixels.
        orthographic: Boolean specifying if the camera is orthographic or not

        The calibration matrix `K` is set up as follows:

        .. code-block:: python

            fx = focal_length[:,0]
            fy = focal_length[:,1]
            px = principal_point[:,0]
            py = principal_point[:,1]

            for orthographic==True:
                K = [
                        [fx,   0,    0,  px],
                        [0,   fy,    0,  py],
                        [0,    0,    1,   0],
                        [0,    0,    0,   1],
                ]
            else:
                K = [
                        [fx,   0,   px,   0],
                        [0,   fy,   py,   0],
                        [0,    0,    0,   1],
                        [0,    0,    1,   0],
                ]

    Returns:
        A calibration matrix `K` of the SfM-conventioned camera
        of shape (N, 4, 4).
    """
    if not torch.is_tensor(focal_length):
        focal_length = torch.tensor(focal_length, device=device)
    if focal_length.ndim in (0, 1) or focal_length.shape[1] == 1:
        fx = fy = focal_length
    else:
        fx, fy = focal_length.unbind(1)
    if not torch.is_tensor(principal_point):
        principal_point = torch.tensor(principal_point, device=device)
    px, py = principal_point.unbind(1)
    K = fx.new_zeros(N, 4, 4)
    K[:, 0, 0] = fx
    K[:, 1, 1] = fy
    if orthographic:
        K[:, 0, 3] = px
        K[:, 1, 3] = py
        K[:, 2, 2] = 1.0
        K[:, 3, 3] = 1.0
    else:
        K[:, 0, 2] = px
        K[:, 1, 2] = py
        K[:, 3, 2] = 1.0
        K[:, 2, 3] = 1.0
    return K


class PerspectiveCameras(CamerasBase):
    """
    A class which stores a batch of parameters to generate a batch of
    transformation matrices using the multi-view geometry convention for
    perspective camera.

    Parameters for this camera are specified in NDC if `in_ndc` is set to True.
    If parameters are specified in screen space, `in_ndc` must be set to False.
    """
    _FIELDS = 'K', 'R', 'T', 'focal_length', 'principal_point', '_in_ndc', 'image_size'
    _SHARED_FIELDS = '_in_ndc',

    def __init__(self, focal_length: _FocalLengthType=1.0, principal_point=((0.0, 0.0),), R: torch.Tensor=_R, T: torch.Tensor=_T, K: Optional[torch.Tensor]=None, device: Device='cpu', in_ndc: bool=True, image_size: Optional[Union[List, Tuple, torch.Tensor]]=None) ->None:
        """

        Args:
            focal_length: Focal length of the camera in world units.
                A tensor of shape (N, 1) or (N, 2) for
                square and non-square pixels respectively.
            principal_point: xy coordinates of the center of
                the principal point of the camera in pixels.
                A tensor of shape (N, 2).
            in_ndc: True if camera parameters are specified in NDC.
                If camera parameters are in screen space, it must
                be set to False.
            R: Rotation matrix of shape (N, 3, 3)
            T: Translation matrix of shape (N, 3)
            K: (optional) A calibration matrix of shape (N, 4, 4)
                If provided, don't need focal_length, principal_point
            image_size: (height, width) of image size.
                A tensor of shape (N, 2) or a list/tuple. Required for screen cameras.
            device: torch.device or string
        """
        kwargs = {'image_size': image_size} if image_size is not None else {}
        super().__init__(device=device, focal_length=focal_length, principal_point=principal_point, R=R, T=T, K=K, _in_ndc=in_ndc, **kwargs)
        if image_size is not None:
            if (self.image_size < 1).any():
                raise ValueError('Image_size provided has invalid values')
        else:
            self.image_size = None
        if self.focal_length.ndim == 1:
            self.focal_length = self.focal_length[:, None]
        self.focal_length = self.focal_length.expand(-1, 2)

    def get_projection_transform(self, **kwargs) ->Transform3d:
        """
        Calculate the projection matrix using the
        multi-view geometry convention.

        Args:
            **kwargs: parameters for the projection can be passed in as keyword
                arguments to override the default values set in __init__.

        Returns:
            A `Transform3d` object with a batch of `N` projection transforms.

        .. code-block:: python

            fx = focal_length[:, 0]
            fy = focal_length[:, 1]
            px = principal_point[:, 0]
            py = principal_point[:, 1]

            K = [
                    [fx,   0,   px,   0],
                    [0,   fy,   py,   0],
                    [0,    0,    0,   1],
                    [0,    0,    1,   0],
            ]
        """
        K = kwargs.get('K', self.K)
        if K is not None:
            if K.shape != (self._N, 4, 4):
                msg = 'Expected K to have shape of (%r, 4, 4)'
                raise ValueError(msg % self._N)
        else:
            K = _get_sfm_calibration_matrix(self._N, self.device, kwargs.get('focal_length', self.focal_length), kwargs.get('principal_point', self.principal_point), orthographic=False)
        transform = Transform3d(matrix=K.transpose(1, 2).contiguous(), device=self.device)
        return transform

    def unproject_points(self, xy_depth: torch.Tensor, world_coordinates: bool=True, from_ndc: bool=False, **kwargs) ->torch.Tensor:
        """
        Args:
            from_ndc: If `False` (default), assumes xy part of input is in
                NDC space if self.in_ndc(), otherwise in screen space. If
                `True`, assumes xy is in NDC space even if the camera
                is defined in screen space.
        """
        if world_coordinates:
            to_camera_transform = self.get_full_projection_transform(**kwargs)
        else:
            to_camera_transform = self.get_projection_transform(**kwargs)
        if from_ndc:
            to_camera_transform = to_camera_transform.compose(self.get_ndc_camera_transform())
        unprojection_transform = to_camera_transform.inverse()
        xy_inv_depth = torch.cat((xy_depth[..., :2], 1.0 / xy_depth[..., 2:3]), dim=-1)
        return unprojection_transform.transform_points(xy_inv_depth)

    def get_principal_point(self, **kwargs) ->torch.Tensor:
        """
        Return the camera's principal point

        Args:
            **kwargs: parameters for the camera extrinsics can be passed in
                as keyword arguments to override the default values
                set in __init__.
        """
        proj_mat = self.get_projection_transform(**kwargs).get_matrix()
        return proj_mat[:, 2, :2]

    def get_ndc_camera_transform(self, **kwargs) ->Transform3d:
        """
        Returns the transform from camera projection space (screen or NDC) to NDC space.
        If the camera is defined already in NDC space, the transform is identity.
        For cameras defined in screen space, we adjust the principal point computation
        which is defined in the image space (commonly) and scale the points to NDC space.

        This transform leaves the depth unchanged.

        Important: This transforms assumes PyTorch3D conventions for the input points,
        i.e. +X left, +Y up.
        """
        if self.in_ndc():
            ndc_transform = Transform3d(device=self.device, dtype=torch.float32)
        else:
            pr_point_fix = torch.zeros((self._N, 4, 4), device=self.device, dtype=torch.float32)
            pr_point_fix[:, 0, 0] = 1.0
            pr_point_fix[:, 1, 1] = 1.0
            pr_point_fix[:, 2, 2] = 1.0
            pr_point_fix[:, 3, 3] = 1.0
            pr_point_fix[:, :2, 3] = -2.0 * self.get_principal_point(**kwargs)
            pr_point_fix_transform = Transform3d(matrix=pr_point_fix.transpose(1, 2).contiguous(), device=self.device)
            image_size = kwargs.get('image_size', self.get_image_size())
            screen_to_ndc_transform = get_screen_to_ndc_transform(self, with_xyflip=False, image_size=image_size)
            ndc_transform = pr_point_fix_transform.compose(screen_to_ndc_transform)
        return ndc_transform

    def is_perspective(self):
        return True

    def in_ndc(self):
        return self._in_ndc


class Pointclouds:
    """
    This class provides functions for working with batches of 3d point clouds,
    and converting between representations.

    Within Pointclouds, there are three different representations of the data.

    List
       - only used for input as a starting point to convert to other representations.
    Padded
       - has specific batch dimension.
    Packed
       - no batch dimension.
       - has auxiliary variables used to index into the padded representation.

    Example

    Input list of points = [[P_1], [P_2], ... , [P_N]]
    where P_1, ... , P_N are the number of points in each cloud and N is the
    number of clouds.

    # SPHINX IGNORE
     List                      | Padded                  | Packed
    ---------------------------|-------------------------|------------------------
    [[P_1], ... , [P_N]]       | size = (N, max(P_n), 3) |  size = (sum(P_n), 3)
                               |                         |
    Example for locations      |                         |
    or colors:                 |                         |
                               |                         |
    P_1 = 3, P_2 = 4, P_3 = 5  | size = (3, 5, 3)        |  size = (12, 3)
                               |                         |
    List([                     | tensor([                |  tensor([
      [                        |     [                   |    [0.1, 0.3, 0.5],
        [0.1, 0.3, 0.5],       |       [0.1, 0.3, 0.5],  |    [0.5, 0.2, 0.1],
        [0.5, 0.2, 0.1],       |       [0.5, 0.2, 0.1],  |    [0.6, 0.8, 0.7],
        [0.6, 0.8, 0.7]        |       [0.6, 0.8, 0.7],  |    [0.1, 0.3, 0.3],
      ],                       |       [0,    0,    0],  |    [0.6, 0.7, 0.8],
      [                        |       [0,    0,    0]   |    [0.2, 0.3, 0.4],
        [0.1, 0.3, 0.3],       |     ],                  |    [0.1, 0.5, 0.3],
        [0.6, 0.7, 0.8],       |     [                   |    [0.7, 0.3, 0.6],
        [0.2, 0.3, 0.4],       |       [0.1, 0.3, 0.3],  |    [0.2, 0.4, 0.8],
        [0.1, 0.5, 0.3]        |       [0.6, 0.7, 0.8],  |    [0.9, 0.5, 0.2],
      ],                       |       [0.2, 0.3, 0.4],  |    [0.2, 0.3, 0.4],
      [                        |       [0.1, 0.5, 0.3],  |    [0.9, 0.3, 0.8],
        [0.7, 0.3, 0.6],       |       [0,    0,    0]   |  ])
        [0.2, 0.4, 0.8],       |     ],                  |
        [0.9, 0.5, 0.2],       |     [                   |
        [0.2, 0.3, 0.4],       |       [0.7, 0.3, 0.6],  |
        [0.9, 0.3, 0.8],       |       [0.2, 0.4, 0.8],  |
      ]                        |       [0.9, 0.5, 0.2],  |
    ])                         |       [0.2, 0.3, 0.4],  |
                               |       [0.9, 0.3, 0.8]   |
                               |     ]                   |
                               |  ])                     |
    -----------------------------------------------------------------------------

    Auxiliary variables for packed representation

    Name                           |   Size              |  Example from above
    -------------------------------|---------------------|-----------------------
                                   |                     |
    packed_to_cloud_idx            |  size = (sum(P_n))  |   tensor([
                                   |                     |     0, 0, 0, 1, 1, 1,
                                   |                     |     1, 2, 2, 2, 2, 2
                                   |                     |   )]
                                   |                     |   size = (12)
                                   |                     |
    cloud_to_packed_first_idx      |  size = (N)         |   tensor([0, 3, 7])
                                   |                     |   size = (3)
                                   |                     |
    num_points_per_cloud           |  size = (N)         |   tensor([3, 4, 5])
                                   |                     |   size = (3)
                                   |                     |
    padded_to_packed_idx           |  size = (sum(P_n))  |  tensor([
                                   |                     |     0, 1, 2, 5, 6, 7,
                                   |                     |     8, 10, 11, 12, 13,
                                   |                     |     14
                                   |                     |  )]
                                   |                     |  size = (12)
    -----------------------------------------------------------------------------
    # SPHINX IGNORE
    """
    _INTERNAL_TENSORS = ['_points_packed', '_points_padded', '_normals_packed', '_normals_padded', '_features_packed', '_features_padded', '_packed_to_cloud_idx', '_cloud_to_packed_first_idx', '_num_points_per_cloud', '_padded_to_packed_idx', 'valid', 'equisized']

    def __init__(self, points, normals=None, features=None) ->None:
        """
        Args:
            points:
                Can be either

                - List where each element is a tensor of shape (num_points, 3)
                  containing the (x, y, z) coordinates of each point.
                - Padded float tensor with shape (num_clouds, num_points, 3).
            normals:
                Can be either

                - List where each element is a tensor of shape (num_points, 3)
                  containing the normal vector for each point.
                - Padded float tensor of shape (num_clouds, num_points, 3).
            features:
                Can be either

                - List where each element is a tensor of shape (num_points, C)
                  containing the features for the points in the cloud.
                - Padded float tensor of shape (num_clouds, num_points, C).
                where C is the number of channels in the features.
                For example 3 for RGB color.

        Refer to comments above for descriptions of List and Padded
        representations.
        """
        self.device = torch.device('cpu')
        self.equisized = False
        self.valid = None
        self._N = 0
        self._P = 0
        self._C = None
        self._points_list = None
        self._normals_list = None
        self._features_list = None
        self._num_points_per_cloud = None
        self._points_packed = None
        self._normals_packed = None
        self._features_packed = None
        self._packed_to_cloud_idx = None
        self._cloud_to_packed_first_idx = None
        self._points_padded = None
        self._normals_padded = None
        self._features_padded = None
        self._padded_to_packed_idx = None
        if isinstance(points, list):
            self._points_list = points
            self._N = len(self._points_list)
            self.valid = torch.zeros((self._N,), dtype=torch.bool, device=self.device)
            if self._N > 0:
                self.device = self._points_list[0].device
                for p in self._points_list:
                    if len(p) > 0 and (p.dim() != 2 or p.shape[1] != 3):
                        raise ValueError('Clouds in list must be of shape Px3 or empty')
                    if p.device != self.device:
                        raise ValueError('All points must be on the same device')
                num_points_per_cloud = torch.tensor([len(p) for p in self._points_list], device=self.device)
                self._P = int(num_points_per_cloud.max())
                self.valid = torch.tensor([(len(p) > 0) for p in self._points_list], dtype=torch.bool, device=self.device)
                if len(num_points_per_cloud.unique()) == 1:
                    self.equisized = True
                self._num_points_per_cloud = num_points_per_cloud
            else:
                self._num_points_per_cloud = torch.tensor([], dtype=torch.int64)
        elif torch.is_tensor(points):
            if points.dim() != 3 or points.shape[2] != 3:
                raise ValueError('Points tensor has incorrect dimensions.')
            self._points_padded = points
            self._N = self._points_padded.shape[0]
            self._P = self._points_padded.shape[1]
            self.device = self._points_padded.device
            self.valid = torch.ones((self._N,), dtype=torch.bool, device=self.device)
            self._num_points_per_cloud = torch.tensor([self._P] * self._N, device=self.device)
            self.equisized = True
        else:
            raise ValueError('Points must be either a list or a tensor with                     shape (batch_size, P, 3) where P is the maximum number of                     points in a cloud.')
        normals_parsed = self._parse_auxiliary_input(normals)
        self._normals_list, self._normals_padded, normals_C = normals_parsed
        if normals_C is not None and normals_C != 3:
            raise ValueError('Normals are expected to be 3-dimensional')
        features_parsed = self._parse_auxiliary_input(features)
        self._features_list, self._features_padded, features_C = features_parsed
        if features_C is not None:
            self._C = features_C

    def _parse_auxiliary_input(self, aux_input) ->Tuple[Optional[List[torch.Tensor]], Optional[torch.Tensor], Optional[int]]:
        """
        Interpret the auxiliary inputs (normals, features) given to __init__.

        Args:
            aux_input:
              Can be either

                - List where each element is a tensor of shape (num_points, C)
                  containing the features for the points in the cloud.
                - Padded float tensor of shape (num_clouds, num_points, C).
              For normals, C = 3

        Returns:
            3-element tuple of list, padded, num_channels.
            If aux_input is list, then padded is None. If aux_input is a tensor,
            then list is None.
        """
        if aux_input is None or self._N == 0:
            return None, None, None
        aux_input_C = None
        if isinstance(aux_input, list):
            return self._parse_auxiliary_input_list(aux_input)
        if torch.is_tensor(aux_input):
            if aux_input.dim() != 3:
                raise ValueError('Auxiliary input tensor has incorrect dimensions.')
            if self._N != aux_input.shape[0]:
                raise ValueError('Points and inputs must be the same length.')
            if self._P != aux_input.shape[1]:
                raise ValueError('Inputs tensor must have the right maximum                     number of points in each cloud.')
            if aux_input.device != self.device:
                raise ValueError('All auxiliary inputs must be on the same device as the points.')
            aux_input_C = aux_input.shape[2]
            return None, aux_input, aux_input_C
        else:
            raise ValueError('Auxiliary input must be either a list or a tensor with                     shape (batch_size, P, C) where P is the maximum number of                     points in a cloud.')

    def _parse_auxiliary_input_list(self, aux_input: list) ->Tuple[Optional[List[torch.Tensor]], None, Optional[int]]:
        """
        Interpret the auxiliary inputs (normals, features) given to __init__,
        if a list.

        Args:
            aux_input:
                - List where each element is a tensor of shape (num_points, C)
                  containing the features for the points in the cloud.
              For normals, C = 3

        Returns:
            3-element tuple of list, padded=None, num_channels.
            If aux_input is list, then padded is None. If aux_input is a tensor,
            then list is None.
        """
        aux_input_C = None
        good_empty = None
        needs_fixing = False
        if len(aux_input) != self._N:
            raise ValueError('Points and auxiliary input must be the same length.')
        for p, d in zip(self._num_points_per_cloud, aux_input):
            valid_but_empty = p == 0 and d is not None and d.ndim == 2
            if p > 0 or valid_but_empty:
                if p != d.shape[0]:
                    raise ValueError('A cloud has mismatched numbers of points and inputs')
                if d.dim() != 2:
                    raise ValueError('A cloud auxiliary input must be of shape PxC or empty')
                if aux_input_C is None:
                    aux_input_C = d.shape[1]
                elif aux_input_C != d.shape[1]:
                    raise ValueError('The clouds must have the same number of channels')
                if d.device != self.device:
                    raise ValueError('All auxiliary inputs must be on the same device as the points.')
            else:
                needs_fixing = True
        if aux_input_C is None:
            return None, None, None
        if needs_fixing:
            if good_empty is None:
                good_empty = torch.zeros((0, aux_input_C), device=self.device)
            aux_input_out = []
            for p, d in zip(self._num_points_per_cloud, aux_input):
                valid_but_empty = p == 0 and d is not None and d.ndim == 2
                if p > 0 or valid_but_empty:
                    aux_input_out.append(d)
                else:
                    aux_input_out.append(good_empty)
        else:
            aux_input_out = aux_input
        return aux_input_out, None, aux_input_C

    def __len__(self) ->int:
        return self._N

    def __getitem__(self, index: Union[int, List[int], slice, torch.BoolTensor, torch.LongTensor]) ->'Pointclouds':
        """
        Args:
            index: Specifying the index of the cloud to retrieve.
                Can be an int, slice, list of ints or a boolean tensor.

        Returns:
            Pointclouds object with selected clouds. The tensors are not cloned.
        """
        normals, features = None, None
        normals_list = self.normals_list()
        features_list = self.features_list()
        if isinstance(index, int):
            points = [self.points_list()[index]]
            if normals_list is not None:
                normals = [normals_list[index]]
            if features_list is not None:
                features = [features_list[index]]
        elif isinstance(index, slice):
            points = self.points_list()[index]
            if normals_list is not None:
                normals = normals_list[index]
            if features_list is not None:
                features = features_list[index]
        elif isinstance(index, list):
            points = [self.points_list()[i] for i in index]
            if normals_list is not None:
                normals = [normals_list[i] for i in index]
            if features_list is not None:
                features = [features_list[i] for i in index]
        elif isinstance(index, torch.Tensor):
            if index.dim() != 1 or index.dtype.is_floating_point:
                raise IndexError(index)
            if index.dtype == torch.bool:
                index = index.nonzero()
                index = index.squeeze(1) if index.numel() > 0 else index
                index = index.tolist()
            points = [self.points_list()[i] for i in index]
            if normals_list is not None:
                normals = [normals_list[i] for i in index]
            if features_list is not None:
                features = [features_list[i] for i in index]
        else:
            raise IndexError(index)
        return self.__class__(points=points, normals=normals, features=features)

    def isempty(self) ->bool:
        """
        Checks whether any cloud is valid.

        Returns:
            bool indicating whether there is any data.
        """
        return self._N == 0 or self.valid.eq(False).all()

    def points_list(self) ->List[torch.Tensor]:
        """
        Get the list representation of the points.

        Returns:
            list of tensors of points of shape (P_n, 3).
        """
        if self._points_list is None:
            assert self._points_padded is not None, 'points_padded is required to compute points_list.'
            points_list = []
            for i in range(self._N):
                points_list.append(self._points_padded[i, :self.num_points_per_cloud()[i]])
            self._points_list = points_list
        return self._points_list

    def normals_list(self) ->Optional[List[torch.Tensor]]:
        """
        Get the list representation of the normals,
        or None if there are no normals.

        Returns:
            list of tensors of normals of shape (P_n, 3).
        """
        if self._normals_list is None:
            if self._normals_padded is None:
                return None
            self._normals_list = struct_utils.padded_to_list(self._normals_padded, self.num_points_per_cloud().tolist())
        return self._normals_list

    def features_list(self) ->Optional[List[torch.Tensor]]:
        """
        Get the list representation of the features,
        or None if there are no features.

        Returns:
            list of tensors of features of shape (P_n, C).
        """
        if self._features_list is None:
            if self._features_padded is None:
                return None
            self._features_list = struct_utils.padded_to_list(self._features_padded, self.num_points_per_cloud().tolist())
        return self._features_list

    def points_packed(self) ->torch.Tensor:
        """
        Get the packed representation of the points.

        Returns:
            tensor of points of shape (sum(P_n), 3).
        """
        self._compute_packed()
        return self._points_packed

    def normals_packed(self) ->Optional[torch.Tensor]:
        """
        Get the packed representation of the normals.

        Returns:
            tensor of normals of shape (sum(P_n), 3),
            or None if there are no normals.
        """
        self._compute_packed()
        return self._normals_packed

    def features_packed(self) ->Optional[torch.Tensor]:
        """
        Get the packed representation of the features.

        Returns:
            tensor of features of shape (sum(P_n), C),
            or None if there are no features
        """
        self._compute_packed()
        return self._features_packed

    def packed_to_cloud_idx(self):
        """
        Return a 1D tensor x with length equal to the total number of points.
        packed_to_cloud_idx()[i] gives the index of the cloud which contains
        points_packed()[i].

        Returns:
            1D tensor of indices.
        """
        self._compute_packed()
        return self._packed_to_cloud_idx

    def cloud_to_packed_first_idx(self):
        """
        Return a 1D tensor x with length equal to the number of clouds such that
        the first point of the ith cloud is points_packed[x[i]].

        Returns:
            1D tensor of indices of first items.
        """
        self._compute_packed()
        return self._cloud_to_packed_first_idx

    def num_points_per_cloud(self) ->torch.Tensor:
        """
        Return a 1D tensor x with length equal to the number of clouds giving
        the number of points in each cloud.

        Returns:
            1D tensor of sizes.
        """
        return self._num_points_per_cloud

    def points_padded(self) ->torch.Tensor:
        """
        Get the padded representation of the points.

        Returns:
            tensor of points of shape (N, max(P_n), 3).
        """
        self._compute_padded()
        return self._points_padded

    def normals_padded(self) ->Optional[torch.Tensor]:
        """
        Get the padded representation of the normals,
        or None if there are no normals.

        Returns:
            tensor of normals of shape (N, max(P_n), 3).
        """
        self._compute_padded()
        return self._normals_padded

    def features_padded(self) ->Optional[torch.Tensor]:
        """
        Get the padded representation of the features,
        or None if there are no features.

        Returns:
            tensor of features of shape (N, max(P_n), 3).
        """
        self._compute_padded()
        return self._features_padded

    def padded_to_packed_idx(self):
        """
        Return a 1D tensor x with length equal to the total number of points
        such that points_packed()[i] is element x[i] of the flattened padded
        representation.
        The packed representation can be calculated as follows.

        .. code-block:: python

            p = points_padded().reshape(-1, 3)
            points_packed = p[x]

        Returns:
            1D tensor of indices.
        """
        if self._padded_to_packed_idx is not None:
            return self._padded_to_packed_idx
        if self._N == 0:
            self._padded_to_packed_idx = []
        else:
            self._padded_to_packed_idx = torch.cat([(torch.arange(v, dtype=torch.int64, device=self.device) + i * self._P) for i, v in enumerate(self.num_points_per_cloud())], dim=0)
        return self._padded_to_packed_idx

    def _compute_padded(self, refresh: bool=False):
        """
        Computes the padded version from points_list, normals_list and features_list.

        Args:
            refresh: whether to force the recalculation.
        """
        if not (refresh or self._points_padded is None):
            return
        self._normals_padded, self._features_padded = None, None
        if self.isempty():
            self._points_padded = torch.zeros((self._N, 0, 3), device=self.device)
        else:
            self._points_padded = struct_utils.list_to_padded(self.points_list(), (self._P, 3), pad_value=0.0, equisized=self.equisized)
            normals_list = self.normals_list()
            if normals_list is not None:
                self._normals_padded = struct_utils.list_to_padded(normals_list, (self._P, 3), pad_value=0.0, equisized=self.equisized)
            features_list = self.features_list()
            if features_list is not None:
                self._features_padded = struct_utils.list_to_padded(features_list, (self._P, self._C), pad_value=0.0, equisized=self.equisized)

    def _compute_packed(self, refresh: bool=False):
        """
        Computes the packed version from points_list, normals_list and
        features_list and sets the values of auxiliary tensors.

        Args:
            refresh: Set to True to force recomputation of packed
                representations. Default: False.
        """
        if not (refresh or any(v is None for v in [self._points_packed, self._packed_to_cloud_idx, self._cloud_to_packed_first_idx])):
            return
        points_list = self.points_list()
        normals_list = self.normals_list()
        features_list = self.features_list()
        if self.isempty():
            self._points_packed = torch.zeros((0, 3), dtype=torch.float32, device=self.device)
            self._packed_to_cloud_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._cloud_to_packed_first_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._normals_packed = None
            self._features_packed = None
            return
        points_list_to_packed = struct_utils.list_to_packed(points_list)
        self._points_packed = points_list_to_packed[0]
        if not torch.allclose(self._num_points_per_cloud, points_list_to_packed[1]):
            raise ValueError('Inconsistent list to packed conversion')
        self._cloud_to_packed_first_idx = points_list_to_packed[2]
        self._packed_to_cloud_idx = points_list_to_packed[3]
        self._normals_packed, self._features_packed = None, None
        if normals_list is not None:
            normals_list_to_packed = struct_utils.list_to_packed(normals_list)
            self._normals_packed = normals_list_to_packed[0]
        if features_list is not None:
            features_list_to_packed = struct_utils.list_to_packed(features_list)
            self._features_packed = features_list_to_packed[0]

    def clone(self):
        """
        Deep copy of Pointclouds object. All internal tensors are cloned
        individually.

        Returns:
            new Pointclouds object.
        """
        new_points, new_normals, new_features = None, None, None
        if self._points_list is not None:
            new_points = [v.clone() for v in self.points_list()]
            normals_list = self.normals_list()
            features_list = self.features_list()
            if normals_list is not None:
                new_normals = [n.clone() for n in normals_list]
            if features_list is not None:
                new_features = [f.clone() for f in features_list]
        elif self._points_padded is not None:
            new_points = self.points_padded().clone()
            normals_padded = self.normals_padded()
            features_padded = self.features_padded()
            if normals_padded is not None:
                new_normals = self.normals_padded().clone()
            if features_padded is not None:
                new_features = self.features_padded().clone()
        other = self.__class__(points=new_points, normals=new_normals, features=new_features)
        for k in self._INTERNAL_TENSORS:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(other, k, v.clone())
        return other

    def detach(self):
        """
        Detach Pointclouds object. All internal tensors are detached
        individually.

        Returns:
            new Pointclouds object.
        """
        new_points, new_normals, new_features = None, None, None
        if self._points_list is not None:
            new_points = [v.detach() for v in self.points_list()]
            normals_list = self.normals_list()
            features_list = self.features_list()
            if normals_list is not None:
                new_normals = [n.detach() for n in normals_list]
            if features_list is not None:
                new_features = [f.detach() for f in features_list]
        elif self._points_padded is not None:
            new_points = self.points_padded().detach()
            normals_padded = self.normals_padded()
            features_padded = self.features_padded()
            if normals_padded is not None:
                new_normals = self.normals_padded().detach()
            if features_padded is not None:
                new_features = self.features_padded().detach()
        other = self.__class__(points=new_points, normals=new_normals, features=new_features)
        for k in self._INTERNAL_TENSORS:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(other, k, v.detach())
        return other

    def to(self, device: Device, copy: bool=False):
        """
        Match functionality of torch.Tensor.to()
        If copy = True or the self Tensor is on a different device, the
        returned tensor is a copy of self with the desired torch.device.
        If copy = False and the self Tensor already has the correct torch.device,
        then self is returned.

        Args:
          device: Device (as str or torch.device) for the new tensor.
          copy: Boolean indicator whether or not to clone self. Default False.

        Returns:
          Pointclouds object.
        """
        device_ = make_device(device)
        if not copy and self.device == device_:
            return self
        other = self.clone()
        if self.device == device_:
            return other
        other.device = device_
        if other._N > 0:
            other._points_list = [v for v in other.points_list()]
            if other._normals_list is not None:
                other._normals_list = [n for n in other.normals_list()]
            if other._features_list is not None:
                other._features_list = [f for f in other.features_list()]
        for k in self._INTERNAL_TENSORS:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(other, k, v)
        return other

    def cpu(self):
        return self

    def cuda(self):
        return self

    def get_cloud(self, index: int):
        """
        Get tensors for a single cloud from the list representation.

        Args:
            index: Integer in the range [0, N).

        Returns:
            points: Tensor of shape (P, 3).
            normals: Tensor of shape (P, 3)
            features: LongTensor of shape (P, C).
        """
        if not isinstance(index, int):
            raise ValueError('Cloud index must be an integer.')
        if index < 0 or index > self._N:
            raise ValueError('Cloud index must be in the range [0, N) where             N is the number of clouds in the batch.')
        points = self.points_list()[index]
        normals, features = None, None
        normals_list = self.normals_list()
        if normals_list is not None:
            normals = normals_list[index]
        features_list = self.features_list()
        if features_list is not None:
            features = features_list[index]
        return points, normals, features

    def split(self, split_sizes: list):
        """
        Splits Pointclouds object of size N into a list of Pointclouds objects
        of size len(split_sizes), where the i-th Pointclouds object is of size
        split_sizes[i]. Similar to torch.split().

        Args:
            split_sizes: List of integer sizes of Pointclouds objects to be
            returned.

        Returns:
            list[Pointclouds].
        """
        if not all(isinstance(x, int) for x in split_sizes):
            raise ValueError('Value of split_sizes must be a list of integers.')
        cloudlist = []
        curi = 0
        for i in split_sizes:
            cloudlist.append(self[curi:curi + i])
            curi += i
        return cloudlist

    def offset_(self, offsets_packed):
        """
        Translate the point clouds by an offset. In place operation.

        Args:
            offsets_packed: A Tensor of shape (3,) or the same shape
                as self.points_packed giving offsets to be added to
                all points.

        Returns:
            self.
        """
        points_packed = self.points_packed()
        if offsets_packed.shape == (3,):
            offsets_packed = offsets_packed.expand_as(points_packed)
        if offsets_packed.shape != points_packed.shape:
            raise ValueError('Offsets must have dimension (all_p, 3).')
        self._points_packed = points_packed + offsets_packed
        new_points_list = list(self._points_packed.split(self.num_points_per_cloud().tolist(), 0))
        self._points_list = new_points_list
        if self._points_padded is not None:
            for i, points in enumerate(new_points_list):
                if len(points) > 0:
                    self._points_padded[i, :points.shape[0], :] = points
        return self

    def offset(self, offsets_packed):
        """
        Out of place offset.

        Args:
            offsets_packed: A Tensor of the same shape as self.points_packed
                giving offsets to be added to all points.
        Returns:
            new Pointclouds object.
        """
        new_clouds = self.clone()
        return new_clouds.offset_(offsets_packed)

    def subsample(self, max_points: Union[int, Sequence[int]]) ->'Pointclouds':
        """
        Subsample each cloud so that it has at most max_points points.

        Args:
            max_points: maximum number of points in each cloud.

        Returns:
            new Pointclouds object, or self if nothing to be done.
        """
        if isinstance(max_points, int):
            max_points = [max_points] * len(self)
        elif len(max_points) != len(self):
            raise ValueError('wrong number of max_points supplied')
        if all(int(n_points) <= int(max_) for n_points, max_ in zip(self.num_points_per_cloud(), max_points)):
            return self
        points_list = []
        features_list = []
        normals_list = []
        for max_, n_points, points, features, normals in zip_longest(map(int, max_points), map(int, self.num_points_per_cloud()), self.points_list(), self.features_list() or (), self.normals_list() or ()):
            if n_points > max_:
                keep_np = np.random.choice(n_points, max_, replace=False)
                keep = torch.tensor(keep_np, device=points.device, dtype=torch.int64)
                points = points[keep]
                if features is not None:
                    features = features[keep]
                if normals is not None:
                    normals = normals[keep]
            points_list.append(points)
            features_list.append(features)
            normals_list.append(normals)
        return Pointclouds(points=points_list, normals=self.normals_list() and normals_list, features=self.features_list() and features_list)

    def scale_(self, scale):
        """
        Multiply the coordinates of this object by a scalar value.
        - i.e. enlarge/dilate
        In place operation.

        Args:
            scale: A scalar, or a Tensor of shape (N,).

        Returns:
            self.
        """
        if not torch.is_tensor(scale):
            scale = torch.full((len(self),), scale, device=self.device)
        new_points_list = []
        points_list = self.points_list()
        for i, old_points in enumerate(points_list):
            new_points_list.append(scale[i] * old_points)
        self._points_list = new_points_list
        if self._points_packed is not None:
            self._points_packed = torch.cat(new_points_list, dim=0)
        if self._points_padded is not None:
            for i, points in enumerate(new_points_list):
                if len(points) > 0:
                    self._points_padded[i, :points.shape[0], :] = points
        return self

    def scale(self, scale):
        """
        Out of place scale_.

        Args:
            scale: A scalar, or a Tensor of shape (N,).

        Returns:
            new Pointclouds object.
        """
        new_clouds = self.clone()
        return new_clouds.scale_(scale)

    def get_bounding_boxes(self):
        """
        Compute an axis-aligned bounding box for each cloud.

        Returns:
            bboxes: Tensor of shape (N, 3, 2) where bbox[i, j] gives the
            min and max values of cloud i along the jth coordinate axis.
        """
        all_mins, all_maxes = [], []
        for points in self.points_list():
            cur_mins = points.min(dim=0)[0]
            cur_maxes = points.max(dim=0)[0]
            all_mins.append(cur_mins)
            all_maxes.append(cur_maxes)
        all_mins = torch.stack(all_mins, dim=0)
        all_maxes = torch.stack(all_maxes, dim=0)
        bboxes = torch.stack([all_mins, all_maxes], dim=2)
        return bboxes

    def estimate_normals(self, neighborhood_size: int=50, disambiguate_directions: bool=True, assign_to_self: bool=False):
        """
        Estimates the normals of each point in each cloud and assigns
        them to the internal tensors `self._normals_list` and `self._normals_padded`

        The function uses `ops.estimate_pointcloud_local_coord_frames`
        to estimate the normals. Please refer to that function for more
        detailed information about the implemented algorithm.

        Args:
          **neighborhood_size**: The size of the neighborhood used to estimate the
            geometry around each point.
          **disambiguate_directions**: If `True`, uses the algorithm from [1] to
            ensure sign consistency of the normals of neighboring points.
          **normals**: A tensor of normals for each input point
            of shape `(minibatch, num_point, 3)`.
            If `pointclouds` are of `Pointclouds` class, returns a padded tensor.
          **assign_to_self**: If `True`, assigns the computed normals to the
            internal buffers overwriting any previously stored normals.

        References:
          [1] Tombari, Salti, Di Stefano: Unique Signatures of Histograms for
          Local Surface Description, ECCV 2010.
        """
        normals_est = ops.estimate_pointcloud_normals(self, neighborhood_size=neighborhood_size, disambiguate_directions=disambiguate_directions)
        if assign_to_self:
            _, self._normals_padded, _ = self._parse_auxiliary_input(normals_est)
            self._normals_list, self._normals_packed = None, None
            if self._points_list is not None:
                self.normals_list()
            if self._points_packed is not None:
                self._normals_packed = torch.cat(self._normals_list, dim=0)
        return normals_est

    def extend(self, N: int):
        """
        Create new Pointclouds which contains each cloud N times.

        Args:
            N: number of new copies of each cloud.

        Returns:
            new Pointclouds object.
        """
        if not isinstance(N, int):
            raise ValueError('N must be an integer.')
        if N <= 0:
            raise ValueError('N must be > 0.')
        new_points_list, new_normals_list, new_features_list = [], None, None
        for points in self.points_list():
            new_points_list.extend(points.clone() for _ in range(N))
        normals_list = self.normals_list()
        if normals_list is not None:
            new_normals_list = []
            for normals in normals_list:
                new_normals_list.extend(normals.clone() for _ in range(N))
        features_list = self.features_list()
        if features_list is not None:
            new_features_list = []
            for features in features_list:
                new_features_list.extend(features.clone() for _ in range(N))
        return self.__class__(points=new_points_list, normals=new_normals_list, features=new_features_list)

    def update_padded(self, new_points_padded, new_normals_padded=None, new_features_padded=None):
        """
        Returns a Pointcloud structure with updated padded tensors and copies of
        the auxiliary tensors. This function allows for an update of
        points_padded (and normals and features) without having to explicitly
        convert it to the list representation for heterogeneous batches.

        Args:
            new_points_padded: FloatTensor of shape (N, P, 3)
            new_normals_padded: (optional) FloatTensor of shape (N, P, 3)
            new_features_padded: (optional) FloatTensor of shape (N, P, C)

        Returns:
            Pointcloud with updated padded representations
        """

        def check_shapes(x, size):
            if x.shape[0] != size[0]:
                raise ValueError('new values must have the same batch dimension.')
            if x.shape[1] != size[1]:
                raise ValueError('new values must have the same number of points.')
            if size[2] is not None:
                if x.shape[2] != size[2]:
                    raise ValueError('new values must have the same number of channels.')
        check_shapes(new_points_padded, [self._N, self._P, 3])
        if new_normals_padded is not None:
            check_shapes(new_normals_padded, [self._N, self._P, 3])
        if new_features_padded is not None:
            check_shapes(new_features_padded, [self._N, self._P, self._C])
        new = self.__class__(points=new_points_padded, normals=new_normals_padded, features=new_features_padded)
        new.equisized = self.equisized
        if new_normals_padded is None:
            new._normals_list = self._normals_list
            new._normals_padded = self._normals_padded
            new._normals_packed = self._normals_packed
        if new_features_padded is None:
            new._features_list = self._features_list
            new._features_padded = self._features_padded
            new._features_packed = self._features_packed
        copy_tensors = ['_packed_to_cloud_idx', '_cloud_to_packed_first_idx', '_num_points_per_cloud', '_padded_to_packed_idx', 'valid']
        for k in copy_tensors:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(new, k, v)
        new._points_padded = new_points_padded
        assert new._points_list is None
        assert new._points_packed is None
        if new_normals_padded is not None:
            new._normals_padded = new_normals_padded
            new._normals_list = None
            new._normals_packed = None
        if new_features_padded is not None:
            new._features_padded = new_features_padded
            new._features_list = None
            new._features_packed = None
        return new

    def inside_box(self, box):
        """
        Finds the points inside a 3D box.

        Args:
            box: FloatTensor of shape (2, 3) or (N, 2, 3) where N is the number
                of clouds.
                    box[..., 0, :] gives the min x, y & z.
                    box[..., 1, :] gives the max x, y & z.
        Returns:
            idx: BoolTensor of length sum(P_i) indicating whether the packed points are
                within the input box.
        """
        if box.dim() > 3 or box.dim() < 2:
            raise ValueError('Input box must be of shape (2, 3) or (N, 2, 3).')
        if box.dim() == 3 and box.shape[0] != 1 and box.shape[0] != self._N:
            raise ValueError('Input box dimension is incompatible with pointcloud size.')
        if box.dim() == 2:
            box = box[None]
        if (box[..., 0, :] > box[..., 1, :]).any():
            raise ValueError('Input box is invalid: min values larger than max values.')
        points_packed = self.points_packed()
        sumP = points_packed.shape[0]
        if box.shape[0] == 1:
            box = box.expand(sumP, 2, 3)
        elif box.shape[0] == self._N:
            box = box.unbind(0)
            box = [b.expand(p, 2, 3) for b, p in zip(box, self.num_points_per_cloud())]
            box = torch.cat(box, 0)
        coord_inside = (points_packed >= box[:, 0]) * (points_packed <= box[:, 1])
        return coord_inside.all(dim=-1)


def _add_background_color_to_images(pix_idxs, images, background_color):
    """
    Mask pixels in images without corresponding points with a given background_color.

    Args:
        pix_idxs: int32 Tensor of shape (N, points_per_pixel, image_size, image_size)
            giving the indices of the nearest points at each pixel, sorted in z-order.
        images: Tensor of shape (N, 4, image_size, image_size) giving the
            accumulated features at each point, where 4 refers to a rgba feature.
        background_color: Tensor, list, or tuple with 3 or 4 values indicating the rgb/rgba
            value for the new background. Values should be in the interval [0,1].
     Returns:
        images: Tensor of shape (N, 4, image_size, image_size), where pixels with
            no nearest points have features set to the background color, and other
            pixels with accumulated features have unchanged values.
    """
    background_mask = pix_idxs[:, 0] < 0
    if not torch.is_tensor(background_color):
        background_color = images.new_tensor(background_color)
    if background_color.ndim == 0:
        background_color = background_color.expand(images.shape[1])
    if background_color.ndim > 1:
        raise ValueError('Wrong shape of background_color')
    background_color = background_color
    if background_color.shape[0] + 1 == images.shape[1]:
        alpha = images.new_ones(1)
        background_color = torch.cat([background_color, alpha])
    if images.shape[1] != background_color.shape[0]:
        raise ValueError('Background color has %s channels not %s' % (background_color.shape[0], images.shape[1]))
    num_background_pixels = background_mask.sum()
    masked_images = images.permute(0, 2, 3, 1).masked_scatter(background_mask[..., None], background_color[None, :].expand(num_background_pixels, -1))
    return masked_images.permute(0, 3, 1, 2)


class _CompositeAlphaPoints(torch.autograd.Function):
    """
    Composite features within a z-buffer using alpha compositing. Given a z-buffer
    with corresponding features and weights, these values are accumulated according
    to their weights such that features nearer in depth contribute more to the final
    feature than ones further away.

    Concretely this means:
        weighted_fs[b,c,i,j] = sum_k cum_alpha_k * features[c,pointsidx[b,k,i,j]]
        cum_alpha_k = alphas[b,k,i,j] * prod_l=0..k-1 (1 - alphas[b,l,i,j])

    Args:
        features: Packed Tensor of shape (C, P) giving the features of each point.
        alphas: float32 Tensor of shape (N, points_per_pixel, image_size,
            image_size) giving the weight of each point in the z-buffer.
            Values should be in the interval [0, 1].
        pointsidx: int32 Tensor of shape (N, points_per_pixel, image_size, image_size)
            giving the indices of the nearest points at each pixel, sorted in z-order.
            Concretely pointsidx[n, k, y, x] = p means that features[:, p] is the
            feature of the kth closest point (along the z-direction) to pixel (y, x) in
            batch element n. This is weighted by alphas[n, k, y, x].

    Returns:
        weighted_fs: Tensor of shape (N, C, image_size, image_size)
            giving the accumulated features at each point.
    """

    @staticmethod
    def forward(ctx, features, alphas, points_idx):
        pt_cld = _C.accum_alphacomposite(features, alphas, points_idx)
        ctx.save_for_backward(features.clone(), alphas.clone(), points_idx.clone())
        return pt_cld

    @staticmethod
    def backward(ctx, grad_output):
        grad_features = None
        grad_alphas = None
        grad_points_idx = None
        features, alphas, points_idx = ctx.saved_tensors
        grad_features, grad_alphas = _C.accum_alphacomposite_backward(grad_output, features, alphas, points_idx)
        return grad_features, grad_alphas, grad_points_idx, None


def alpha_composite(pointsidx, alphas, pt_clds) ->torch.Tensor:
    """
    Composite features within a z-buffer using alpha compositing. Given a z-buffer
    with corresponding features and weights, these values are accumulated according
    to their weights such that features nearer in depth contribute more to the final
    feature than ones further away.

    Concretely this means:
        weighted_fs[b,c,i,j] = sum_k cum_alpha_k * features[c,pointsidx[b,k,i,j]]
        cum_alpha_k = alphas[b,k,i,j] * prod_l=0..k-1 (1 - alphas[b,l,i,j])


    Args:
        pt_clds: Tensor of shape (N, C, P) giving the features of each point (can use
            RGB for example).
        alphas: float32 Tensor of shape (N, points_per_pixel, image_size,
            image_size) giving the weight of each point in the z-buffer.
            Values should be in the interval [0, 1].
        pointsidx: int32 Tensor of shape (N, points_per_pixel, image_size, image_size)
            giving the indices of the nearest points at each pixel, sorted in z-order.
            Concretely pointsidx[n, k, y, x] = p means that features[n, :, p] is the
            feature of the kth closest point (along the z-direction) to pixel (y, x) in
            batch element n. This is weighted by alphas[n, k, y, x].

    Returns:
        Combined features: Tensor of shape (N, C, image_size, image_size)
            giving the accumulated features at each point.
    """
    return _CompositeAlphaPoints.apply(pt_clds, alphas, pointsidx)


class AlphaCompositor(nn.Module):
    """
    Accumulate points using alpha compositing.
    """

    def __init__(self, background_color: Optional[Union[Tuple, List, torch.Tensor]]=None) ->None:
        super().__init__()
        self.background_color = background_color

    def forward(self, fragments, alphas, ptclds, **kwargs) ->torch.Tensor:
        background_color = kwargs.get('background_color', self.background_color)
        images = alpha_composite(fragments, alphas, ptclds)
        if background_color is not None:
            return _add_background_color_to_images(fragments, images, background_color)
        return images


class PointFragments(NamedTuple):
    idx: torch.Tensor
    zbuf: torch.Tensor
    dists: torch.Tensor


class _RasterizePoints(torch.autograd.Function):

    @staticmethod
    def forward(ctx, points, cloud_to_packed_first_idx, num_points_per_cloud, image_size: Union[List[int], Tuple[int, int]]=(256, 256), radius: Union[float, torch.Tensor]=0.01, points_per_pixel: int=8, bin_size: int=0, max_points_per_bin: int=0):
        args = points, cloud_to_packed_first_idx, num_points_per_cloud, image_size, radius, points_per_pixel, bin_size, max_points_per_bin
        idx, zbuf, dists = _C.rasterize_points(*args)
        ctx.save_for_backward(points, idx)
        ctx.mark_non_differentiable(idx)
        return idx, zbuf, dists

    @staticmethod
    def backward(ctx, grad_idx, grad_zbuf, grad_dists):
        grad_points = None
        grad_cloud_to_packed_first_idx = None
        grad_num_points_per_cloud = None
        grad_image_size = None
        grad_radius = None
        grad_points_per_pixel = None
        grad_bin_size = None
        grad_max_points_per_bin = None
        points, idx = ctx.saved_tensors
        args = points, idx, grad_zbuf, grad_dists
        grad_points = _C.rasterize_points_backward(*args)
        grads = grad_points, grad_cloud_to_packed_first_idx, grad_num_points_per_cloud, grad_image_size, grad_radius, grad_points_per_pixel, grad_bin_size, grad_max_points_per_bin
        return grads


def _format_radius(radius: Union[float, List, Tuple, torch.Tensor], pointclouds) ->torch.Tensor:
    """
    Format the radius as a torch tensor of shape (P_packed,)
    where P_packed is the total number of points in the
    batch (i.e. pointclouds.points_packed().shape[0]).

    This will enable support for a different size radius
    for each point in the batch.

    Args:
        radius: can be a float, List, Tuple or tensor of
            shape (N, P_padded) where P_padded is the
            maximum number of points for each pointcloud
            in the batch.

    Returns:
        radius: torch.Tensor of shape (P_packed)
    """
    N, P_padded = pointclouds._N, pointclouds._P
    points_packed = pointclouds.points_packed()
    P_packed = points_packed.shape[0]
    if isinstance(radius, (list, tuple)):
        radius = torch.tensor(radius).type_as(points_packed)
    if isinstance(radius, torch.Tensor):
        if N == 1 and radius.ndim == 1:
            radius = radius[None, ...]
        if radius.shape != (N, P_padded):
            msg = 'radius must be of shape (N, P): got %s'
            raise ValueError(msg % repr(radius.shape))
        else:
            padded_to_packed_idx = pointclouds.padded_to_packed_idx()
            radius = radius.view(-1)[padded_to_packed_idx]
    elif isinstance(radius, float):
        radius = torch.full((P_packed,), fill_value=radius).type_as(points_packed)
    else:
        msg = 'radius must be a float, list, tuple or tensor; got %s'
        raise ValueError(msg % type(radius))
    return radius


kMaxPointsPerBin = 22


def parse_image_size(image_size: Union[List[int], Tuple[int, int], int]) ->Tuple[int, int]:
    """
    Args:
        image_size: A single int (for square images) or a tuple/list of two ints.

    Returns:
        A tuple of two ints.

    Throws:
        ValueError if got more than two ints, any negative numbers or non-ints.
    """
    if not isinstance(image_size, (tuple, list)):
        return image_size, image_size
    if len(image_size) != 2:
        raise ValueError('Image size can only be a tuple/list of (H, W)')
    if not all(i > 0 for i in image_size):
        raise ValueError('Image sizes must be greater than 0; got %d, %d' % image_size)
    if not all(type(i) == int for i in image_size):
        raise ValueError('Image sizes must be integers; got %f, %f' % image_size)
    return tuple(image_size)


def rasterize_points(pointclouds, image_size: Union[int, List[int], Tuple[int, int]]=256, radius: Union[float, List, Tuple, torch.Tensor]=0.01, points_per_pixel: int=8, bin_size: Optional[int]=None, max_points_per_bin: Optional[int]=None):
    """
    Each pointcloud is rasterized onto a separate image of shape
    (H, W) if `image_size` is a tuple or (image_size, image_size) if it
    is an int.

    If the desired image size is non square (i.e. a tuple of (H, W) where H != W)
    the aspect ratio needs special consideration. There are two aspect ratios
    to be aware of:
        - the aspect ratio of each pixel
        - the aspect ratio of the output image
    The camera can be used to set the pixel aspect ratio. In the rasterizer,
    we assume square pixels, but variable image aspect ratio (i.e rectangle images).

    In most cases you will want to set the camera aspect ratio to
    1.0 (i.e. square pixels) and only vary the
    `image_size` (i.e. the output image dimensions in pix

    Args:
        pointclouds: A Pointclouds object representing a batch of point clouds to be
            rasterized. This is a batch of N pointclouds, where each point cloud
            can have a different number of points; the coordinates of each point
            are (x, y, z). The coordinates are expected to
            be in normalized device coordinates (NDC): [-1, 1]^3 with the camera at
            (0, 0, 0); In the camera coordinate frame the x-axis goes from right-to-left,
            the y-axis goes from bottom-to-top, and the z-axis goes from back-to-front.
        image_size: Size in pixels of the output image to be rasterized.
            Can optionally be a tuple of (H, W) in the case of non square images.
        radius (Optional): The radius (in NDC units) of the disk to
            be rasterized. This can either be a float in which case the same radius is used
            for each point, or a torch.Tensor of shape (N, P) giving a radius per point
            in the batch.
        points_per_pixel (Optional): We will keep track of this many points per
            pixel, returning the nearest points_per_pixel points along the z-axis
        bin_size: Size of bins to use for coarse-to-fine rasterization. Setting
            bin_size=0 uses naive rasterization; setting bin_size=None attempts to
            set it heuristically based on the shape of the input. This should not
            affect the output, but can affect the speed of the forward pass.
        max_points_per_bin: Only applicable when using coarse-to-fine rasterization
            (bin_size > 0); this is the maximum number of points allowed within each
            bin. This should not affect the output values, but can affect
            the memory usage in the forward pass.

    Returns:
        3-element tuple containing

        - **idx**: int32 Tensor of shape (N, image_size, image_size, points_per_pixel)
          giving the indices of the nearest points at each pixel, in ascending
          z-order. Concretely `idx[n, y, x, k] = p` means that `points[p]` is the kth
          closest point (along the z-direction) to pixel (y, x) - note that points
          represents the packed points of shape (P, 3).
          Pixels that are hit by fewer than points_per_pixel are padded with -1.
        - **zbuf**: Tensor of shape (N, image_size, image_size, points_per_pixel)
          giving the z-coordinates of the nearest points at each pixel, sorted in
          z-order. Concretely, if `idx[n, y, x, k] = p` then
          `zbuf[n, y, x, k] = points[n, p, 2]`. Pixels hit by fewer than
          points_per_pixel are padded with -1
        - **dists2**: Tensor of shape (N, image_size, image_size, points_per_pixel)
          giving the squared Euclidean distance (in NDC units) in the x/y plane
          for each point closest to the pixel. Concretely if `idx[n, y, x, k] = p`
          then `dists[n, y, x, k]` is the squared distance between the pixel (y, x)
          and the point `(points[n, p, 0], points[n, p, 1])`. Pixels hit with fewer
          than points_per_pixel are padded with -1.

        In the case that image_size is a tuple of (H, W) then the outputs
        will be of shape `(N, H, W, ...)`.
    """
    points_packed = pointclouds.points_packed()
    cloud_to_packed_first_idx = pointclouds.cloud_to_packed_first_idx()
    num_points_per_cloud = pointclouds.num_points_per_cloud()
    radius = _format_radius(radius, pointclouds)
    im_size = parse_image_size(image_size)
    max_image_size = max(*im_size)
    if bin_size is None:
        if not points_packed.is_cuda:
            bin_size = 0
        else:
            bin_size = int(2 ** max(np.ceil(np.log2(max_image_size)) - 4, 4))
    if bin_size != 0:
        points_per_bin = 1 + (max_image_size - 1) // bin_size
        if points_per_bin >= kMaxPointsPerBin:
            raise ValueError('bin_size too small, number of points per bin must be less than %d; got %d' % (kMaxPointsPerBin, points_per_bin))
    if max_points_per_bin is None:
        max_points_per_bin = int(max(10000, pointclouds._P / 5))
    return _RasterizePoints.apply(points_packed, cloud_to_packed_first_idx, num_points_per_cloud, im_size, radius, points_per_pixel, bin_size, max_points_per_bin)


def try_get_projection_transform(cameras: CamerasBase, cameras_kwargs: Dict[str, Any]) ->Optional[Transform3d]:
    """
    Try block to get projection transform from cameras and cameras_kwargs.

    Args:
        cameras: cameras instance, can be linear cameras or nonliear cameras
        cameras_kwargs: camera parameters to be passed to cameras

    Returns:
        If the camera implemented projection_transform, return the
        projection transform; Otherwise, return None
    """
    transform = None
    try:
        transform = cameras.get_projection_transform(**cameras_kwargs)
    except NotImplementedError:
        pass
    return transform


class PointsRasterizer(nn.Module):
    """
    This class implements methods for rasterizing a batch of pointclouds.
    """

    def __init__(self, cameras=None, raster_settings=None) ->None:
        """
        cameras: A cameras object which has a  `transform_points` method
                which returns the transformed points after applying the
                world-to-view and view-to-ndc transformations.
            raster_settings: the parameters for rasterization. This should be a
                named tuple.

        All these initial settings can be overridden by passing keyword
        arguments to the forward function.
        """
        super().__init__()
        if raster_settings is None:
            raster_settings = PointsRasterizationSettings()
        self.cameras = cameras
        self.raster_settings = raster_settings

    def transform(self, point_clouds, **kwargs) ->Pointclouds:
        """
        Args:
            point_clouds: a set of point clouds

        Returns:
            points_proj: the points with positions projected
            in NDC space

        NOTE: keeping this as a separate function for readability but it could
        be moved into forward.
        """
        cameras = kwargs.get('cameras', self.cameras)
        if cameras is None:
            msg = 'Cameras must be specified either at initialization                 or in the forward pass of PointsRasterizer'
            raise ValueError(msg)
        pts_world = point_clouds.points_padded()
        eps = kwargs.get('eps', None)
        pts_view = cameras.get_world_to_view_transform(**kwargs).transform_points(pts_world, eps=eps)
        to_ndc_transform = cameras.get_ndc_camera_transform(**kwargs)
        projection_transform = try_get_projection_transform(cameras, kwargs)
        if projection_transform is not None:
            projection_transform = projection_transform.compose(to_ndc_transform)
            pts_ndc = projection_transform.transform_points(pts_view, eps=eps)
        else:
            pts_proj = cameras.transform_points(pts_world, eps=eps)
            pts_ndc = to_ndc_transform.transform_points(pts_proj, eps=eps)
        pts_ndc[..., 2] = pts_view[..., 2]
        point_clouds = point_clouds.update_padded(pts_ndc)
        return point_clouds

    def to(self, device):
        if self.cameras is not None:
            self.cameras = self.cameras
        return self

    def forward(self, point_clouds, **kwargs) ->PointFragments:
        """
        Args:
            point_clouds: a set of point clouds with coordinates in world space.
        Returns:
            PointFragments: Rasterization outputs as a named tuple.
        """
        points_proj = self.transform(point_clouds, **kwargs)
        raster_settings = kwargs.get('raster_settings', self.raster_settings)
        idx, zbuf, dists2 = rasterize_points(points_proj, image_size=raster_settings.image_size, radius=raster_settings.radius, points_per_pixel=raster_settings.points_per_pixel, bin_size=raster_settings.bin_size, max_points_per_bin=raster_settings.max_points_per_bin)
        return PointFragments(idx=idx, zbuf=zbuf, dists=dists2)


def _signed_clamp(x, eps):
    sign = x.sign() + (x == 0.0).type_as(x)
    x_clamp = sign * torch.clamp(x.abs(), eps)
    return x_clamp


def _transform_points(cameras, point_clouds, eps, **kwargs):
    pts_world = point_clouds.points_padded()
    pts_view = cameras.get_world_to_view_transform(**kwargs).transform_points(pts_world, eps=eps)
    pts_view = torch.cat((pts_view[..., :-1], _signed_clamp(pts_view[..., -1:], eps)), dim=-1)
    point_clouds = point_clouds.update_padded(pts_view)
    return point_clouds


def render_point_cloud_pytorch3d(camera, point_cloud, render_size: Tuple[int, int], point_radius: float=0.03, topk: int=10, eps: float=0.01, bg_color=None, bin_size: Optional[int]=None, **kwargs):
    featdim = point_cloud.features_packed().shape[-1]
    point_cloud = _transform_points(camera, point_cloud, eps, **kwargs)
    camera_trivial = camera.clone()
    camera_trivial.R[:] = torch.eye(3)
    camera_trivial.T *= 0.0
    bin_size = bin_size if bin_size is not None else 64 if int(max(render_size)) > 1024 else None
    rasterizer = PointsRasterizer(cameras=camera_trivial, raster_settings=PointsRasterizationSettings(image_size=render_size, radius=point_radius, points_per_pixel=topk, bin_size=bin_size))
    fragments = rasterizer(point_cloud, **kwargs)
    r = rasterizer.raster_settings.radius
    dists2 = fragments.dists
    weights = 1 - dists2 / (r * r)
    ok = cast(torch.BoolTensor, fragments.idx >= 0).float()
    weights = weights * ok
    fragments_prm = fragments.idx.long().permute(0, 3, 1, 2)
    weights_prm = weights.permute(0, 3, 1, 2)
    images = AlphaCompositor()(fragments_prm, weights_prm, point_cloud.features_packed().permute(1, 0), background_color=bg_color if bg_color is not None else [0.0] * featdim, **kwargs)
    cumprod = torch.cumprod(1 - weights, dim=-1)
    cumprod = torch.cat((torch.ones_like(cumprod[..., :1]), cumprod[..., :-1]), dim=-1)
    depths = (weights * cumprod * fragments.zbuf).sum(dim=-1)
    render_mask = -torch.prod(1.0 - weights, dim=-1) + 1.0
    rendered_blob = torch.cat((images, depths[:, None], render_mask[:, None]), dim=1)
    rendered_blob = Fu.interpolate(rendered_blob, size=tuple(render_size), mode='bilinear', align_corners=False)
    data_rendered, depth_rendered, render_mask = rendered_blob.split([rendered_blob.shape[1] - 2, 1, 1], dim=1)
    return data_rendered, render_mask, depth_rendered


def rasterize_mc_samples(xys: torch.Tensor, feats: torch.Tensor, image_size_hw: Tuple[int, int], radius: float=0.03, topk: int=5, masks: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Rasterizes Monte-Carlo sampled features back onto the image.

    Specifically, the code uses the PyTorch3D point rasterizer to render
    a z-flat point cloud composed of the xy MC locations and their features.

    Args:
        xys: B x N x 2 2D point locations in PyTorch3D NDC convention
        feats: B x N x dim tensor containing per-point rendered features.
        image_size_hw: Tuple[image_height, image_width] containing
            the size of rasterized image.
        radius: Rasterization point radius.
        topk: The maximum z-buffer size for the PyTorch3D point cloud rasterizer.
        masks: B x N x 1 tensor containing the alpha mask of the
            rendered features.
    """
    if masks is None:
        masks = torch.ones_like(xys[..., :1])
    feats = torch.cat((feats, masks), dim=-1)
    pointclouds = Pointclouds(points=torch.cat([xys, torch.ones_like(xys[..., :1])], dim=-1), features=feats)
    data_rendered, render_mask, _ = render_point_cloud_pytorch3d(PerspectiveCameras(device=feats.device), pointclouds, render_size=image_size_hw, point_radius=radius, topk=topk)
    data_rendered, masks_pt = data_rendered.split([data_rendered.shape[1] - 1, 1], dim=1)
    render_mask = masks_pt * render_mask
    return data_rendered, render_mask


def _save_key_map_hook(self, state_dict, prefix, local_metadata) ->None:
    """
    Args:
        state_dict (dict): a dict containing parameters and
            persistent buffers.
        prefix (str): the prefix for parameters and buffers used in this
            module
        local_metadata (dict): a dict containing the metadata for this module.
    """
    key_map_key = prefix + '_key_map'
    key_map_dict = dict(self._key_map.items())
    state_dict[key_map_key] = key_map_dict


class Autodecoder(Configurable, torch.nn.Module):
    """
    Autodecoder which maps a list of integer or string keys to optimizable embeddings.

    Settings:
        encoding_dim: Embedding dimension for the decoder.
        n_instances: The maximum number of instances stored by the autodecoder.
        init_scale: Scale factor for the initial autodecoder weights.
        ignore_input: If `True`, optimizes a single code for any input.
    """
    encoding_dim: int = 0
    n_instances: int = 1
    init_scale: float = 1.0
    ignore_input: bool = False

    def __post_init__(self):
        super().__init__()
        if self.n_instances <= 0:
            raise ValueError(f'Invalid n_instances {self.n_instances}')
        self._autodecoder_codes = torch.nn.Embedding(self.n_instances, self.encoding_dim, scale_grad_by_freq=True)
        with torch.no_grad():
            self._autodecoder_codes.weight *= self.init_scale
        self._key_map = self._build_key_map()
        self._register_load_state_dict_pre_hook(self._load_key_map_hook)
        self._register_state_dict_hook(_save_key_map_hook)

    def _build_key_map(self, key_map_dict: Optional[Dict[str, int]]=None) ->Dict[str, int]:
        """
        Args:
            key_map_dict: A dictionary used to initialize the key_map.

        Returns:
            key_map: a dictionary of key: id pairs.
        """
        key_map = defaultdict(iter(range(self.n_instances)).__next__)
        if key_map_dict is not None:
            for x, x_id in key_map_dict.items():
                x_id_ = key_map[x]
                assert x_id == x_id_
        return key_map

    def calculate_squared_encoding_norm(self) ->Optional[torch.Tensor]:
        return (self._autodecoder_codes.weight ** 2).mean()

    def get_encoding_dim(self) ->int:
        return self.encoding_dim

    def forward(self, x: Union[torch.LongTensor, List[str]]) ->Optional[torch.Tensor]:
        """
        Args:
            x: A batch of `N` identifiers. Either a long tensor of size
            `(N,)` keys in [0, n_instances), or a list of `N` string keys that
            are hashed to codes (without collisions).

        Returns:
            codes: A tensor of shape `(N, self.encoding_dim)` containing the
                key-specific autodecoder codes.
        """
        if self.ignore_input:
            x = ['singleton']
        if isinstance(x[0], str):
            try:
                x = torch.tensor([self._key_map[elem] for elem in x], dtype=torch.long, device=next(self.parameters()).device)
            except StopIteration:
                raise ValueError('Not enough n_instances in the autodecoder') from None
        return self._autodecoder_codes(x)

    def _load_key_map_hook(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
        """
        Args:
            state_dict (dict): a dict containing parameters and
                persistent buffers.
            prefix (str): the prefix for parameters and buffers used in this
                module
            local_metadata (dict): a dict containing the metadata for this module.
            strict (bool): whether to strictly enforce that the keys in
                :attr:`state_dict` with :attr:`prefix` match the names of
                parameters and buffers in this module
            missing_keys (list of str): if ``strict=True``, add missing keys to
                this list
            unexpected_keys (list of str): if ``strict=True``, add unexpected
                keys to this list
            error_msgs (list of str): error messages should be added to this
                list, and will be reported together in
                :meth:`~torch.nn.Module.load_state_dict`

        Returns:
            Constructed key_map if it exists in the state_dict
            else raises a warning only.
        """
        key_map_key = prefix + '_key_map'
        if key_map_key in state_dict:
            key_map_dict = state_dict.pop(key_map_key)
            self._key_map = self._build_key_map(key_map_dict=key_map_dict)
        else:
            warnings.warn('No key map in Autodecoder state dict!')


class SequenceAutodecoder(GlobalEncoderBase, torch.nn.Module):
    """
    A global encoder implementation which provides an autodecoder encoding
    of the frame's sequence identifier.
    """
    autodecoder: Autodecoder

    def __post_init__(self):
        super().__init__()
        run_auto_creation(self)

    def get_encoding_dim(self):
        return self.autodecoder.get_encoding_dim()

    def forward(self, *, frame_timestamp: Optional[torch.Tensor]=None, sequence_name: Optional[Union[torch.LongTensor, List[str]]]=None, **kwargs) ->torch.Tensor:
        if sequence_name is None:
            raise ValueError('sequence_name must be provided.')
        return self.autodecoder(sequence_name)

    def calculate_squared_encoding_norm(self) ->Optional[torch.Tensor]:
        return self.autodecoder.calculate_squared_encoding_norm()


class HarmonicTimeEncoder(GlobalEncoderBase, torch.nn.Module):
    """
    A global encoder implementation which provides harmonic embeddings
    of each frame's timestamp.
    """
    n_harmonic_functions: int = 10
    append_input: bool = True
    time_divisor: float = 1.0

    def __post_init__(self):
        super().__init__()
        self._harmonic_embedding = HarmonicEmbedding(n_harmonic_functions=self.n_harmonic_functions, append_input=self.append_input)

    def get_encoding_dim(self):
        return self._harmonic_embedding.get_output_dim(1)

    def forward(self, *, frame_timestamp: Optional[torch.Tensor]=None, sequence_name: Optional[Union[torch.LongTensor, List[str]]]=None, **kwargs) ->torch.Tensor:
        if frame_timestamp is None:
            raise ValueError('frame_timestamp must be provided.')
        if frame_timestamp.shape[-1] != 1:
            raise ValueError("Frame timestamp's last dimensions should be one.")
        time = frame_timestamp / self.time_divisor
        return self._harmonic_embedding(time)

    def calculate_squared_encoding_norm(self) ->Optional[torch.Tensor]:
        return None


class DecoderFunctionBase(ReplaceableBase, torch.nn.Module):
    """
    Decoding function is a torch.nn.Module which takes the embedding of a location in
    space and transforms it into the required quantity (for example density and color).
    """

    def __post_init__(self):
        super().__init__()

    def forward(self, features: torch.Tensor, z: Optional[torch.Tensor]=None) ->torch.Tensor:
        """
        Args:
            features (torch.Tensor): tensor of shape (batch, ..., num_in_features)
            z: optional tensor to append to parts of the decoding function
        Returns:
            decoded_features (torch.Tensor) : tensor of
                shape (batch, ..., num_out_features)
        """
        raise NotImplementedError()


class ElementwiseDecoder(DecoderFunctionBase):
    """
    Decoding function which scales the input, adds shift and then applies
    `relu`, `softplus`, `sigmoid` or nothing on its input:
    `result = operation(input * scale + shift)`

    Members:
        scale: a scalar with which input is multiplied before being shifted.
            Defaults to 1.
        shift: a scalar which is added to the scaled input before performing
            the operation. Defaults to 0.
        operation: which operation to perform on the transformed input. Options are:
            `RELU`, `SOFTPLUS`, `SIGMOID` or `IDENTITY`. Defaults to `IDENTITY`.
    """
    scale: float = 1
    shift: float = 0
    operation: DecoderActivation = DecoderActivation.IDENTITY

    def __post_init__(self):
        super().__post_init__()
        if self.operation not in [DecoderActivation.RELU, DecoderActivation.SOFTPLUS, DecoderActivation.SIGMOID, DecoderActivation.IDENTITY]:
            raise ValueError('`operation` can only be `RELU`, `SOFTPLUS`, `SIGMOID` or `IDENTITY`.')

    def forward(self, features: torch.Tensor, z: Optional[torch.Tensor]=None) ->torch.Tensor:
        transfomed_input = features * self.scale + self.shift
        if self.operation == DecoderActivation.SOFTPLUS:
            return torch.nn.functional.softplus(transfomed_input)
        if self.operation == DecoderActivation.RELU:
            return torch.nn.functional.relu(transfomed_input)
        if self.operation == DecoderActivation.SIGMOID:
            return torch.nn.functional.sigmoid(transfomed_input)
        return transfomed_input


class TransformerEncoderLayer(torch.nn.Module):
    """TransformerEncoderLayer is made up of self-attn and feedforward network.
    This standard encoder layer is based on the paper "Attention Is All You Need".
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
    in a different way during application.

    Args:
        d_model: the number of expected features in the input (required).
        nhead: the number of heads in the multiheadattention models (required).
        dim_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of intermediate layer, relu or gelu (default=relu).

    Examples::
        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
        >>> src = torch.rand(10, 32, 512)
        >>> out = encoder_layer(src)
    """

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, d_model_out=-1):
        super(TransformerEncoderLayer, self).__init__()
        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = torch.nn.Linear(d_model, dim_feedforward)
        self.dropout = torch.nn.Dropout(dropout)
        d_model_out = d_model if d_model_out <= 0 else d_model_out
        self.linear2 = torch.nn.Linear(dim_feedforward, d_model_out)
        self.norm1 = torch.nn.LayerNorm(d_model)
        self.norm2 = torch.nn.LayerNorm(d_model_out)
        self.dropout1 = torch.nn.Dropout(dropout)
        self.dropout2 = torch.nn.Dropout(dropout)
        self.activation = torch.nn.functional.relu

    def forward(self, src, src_mask=None, src_key_padding_mask=None):
        """Pass the input through the encoder layer.

        Args:
            src: the sequence to the encoder layer (required).
            src_mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        src2, attn = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        d_out = src2.shape[-1]
        src = src[..., :d_out] + self.dropout2(src2)[..., :d_out]
        src = self.norm2(src)
        return src, attn


class TransformerWithInputSkips(torch.nn.Module):

    def __init__(self, n_layers: int=8, input_dim: int=39, output_dim: int=256, skip_dim: int=39, hidden_dim: int=64, input_skips: Tuple[int, ...]=(5,), dim_down_factor: float=1):
        """
        Args:
            n_layers: The number of linear layers of the MLP.
            input_dim: The number of channels of the input tensor.
            output_dim: The number of channels of the output.
            skip_dim: The number of channels of the tensor `z` appended when
                evaluating the skip layers.
            hidden_dim: The number of hidden units of the MLP.
            input_skips: The list of layer indices at which we append the skip
                tensor `z`.
        """
        super().__init__()
        self.first = torch.nn.Linear(input_dim, hidden_dim)
        _xavier_init(self.first)
        self.skip_linear = torch.nn.ModuleList()
        layers_pool, layers_ray = [], []
        dimout = 0
        for layeri in range(n_layers):
            dimin = int(round(hidden_dim / dim_down_factor ** layeri))
            dimout = int(round(hidden_dim / dim_down_factor ** (layeri + 1)))
            logger.info(f'Tr: {dimin} -> {dimout}')
            for _i, l in enumerate((layers_pool, layers_ray)):
                l.append(TransformerEncoderLayer(d_model=[dimin, dimout][_i], nhead=4, dim_feedforward=hidden_dim, dropout=0.0, d_model_out=dimout))
            if layeri in input_skips:
                self.skip_linear.append(torch.nn.Linear(input_dim, dimin))
        self.last = torch.nn.Linear(dimout, output_dim)
        _xavier_init(self.last)
        self.layers_pool, self.layers_ray = torch.nn.ModuleList(layers_pool), torch.nn.ModuleList(layers_ray)
        self._input_skips = set(input_skips)

    def forward(self, x: torch.Tensor, z: Optional[torch.Tensor]=None):
        """
        Args:
            x: The input tensor of shape
                `(minibatch, n_pooled_feats, ..., n_ray_pts, input_dim)`.
            z: The input skip tensor of shape
                `(minibatch, n_pooled_feats, ..., n_ray_pts, skip_dim)`
                which is appended to layers whose indices are specified by `input_skips`.
        Returns:
            y: The output tensor of shape
                `(minibatch, 1, ..., n_ray_pts, input_dim)`.
        """
        if z is None:
            z = x
        y = self.first(x)
        B, n_pool, n_rays, n_pts, dim = y.shape
        y_p = y.permute(1, 3, 0, 2, 4)
        skipi = 0
        dimh = dim
        for li, (layer_pool, layer_ray) in enumerate(zip(self.layers_pool, self.layers_ray)):
            y_pool_attn = y_p.reshape(n_pool, n_pts * B * n_rays, dimh)
            if li in self._input_skips:
                z_skip = self.skip_linear[skipi](z)
                y_pool_attn = y_pool_attn + z_skip.permute(1, 3, 0, 2, 4).reshape(n_pool, n_pts * B * n_rays, dimh)
                skipi += 1
            y_pool_attn, pool_attn = layer_pool(y_pool_attn, src_key_padding_mask=None)
            dimh = y_pool_attn.shape[-1]
            y_ray_attn = y_pool_attn.view(n_pool, n_pts, B * n_rays, dimh).permute(1, 0, 2, 3).reshape(n_pts, n_pool * B * n_rays, dimh)
            y_ray_attn, ray_attn = layer_ray(y_ray_attn, src_key_padding_mask=None)
            y_p = y_ray_attn.view(n_pts, n_pool, B * n_rays, dimh).permute(1, 0, 2, 3)
        y = y_p.view(n_pool, n_pts, B, n_rays, dimh).permute(2, 0, 3, 1, 4)
        W = torch.softmax(y[..., :1], dim=1)
        y = (y * W).sum(dim=1)
        y = self.last(y)
        return y


def broadcast_global_code(embeds: torch.Tensor, global_code: torch.Tensor):
    """
    Expands the `global_code` of shape (minibatch, dim)
    so that it can be appended to `embeds` of shape (minibatch, ..., dim2),
    and appends to the last dimension of `embeds`.
    """
    bs = embeds.shape[0]
    global_code_broadcast = global_code.view(bs, *([1] * (embeds.ndim - 2)), -1).expand(*embeds.shape[:-1], global_code.shape[-1])
    return torch.cat([embeds, global_code_broadcast], dim=-1)


def create_embeddings_for_implicit_function(xyz_world: torch.Tensor, xyz_in_camera_coords: bool, global_code: Optional[torch.Tensor], camera: Optional[CamerasBase], fun_viewpool: Optional[Callable], xyz_embedding_function: Optional[Callable]) ->torch.Tensor:
    bs, *spatial_size, pts_per_ray, _ = xyz_world.shape
    if xyz_in_camera_coords:
        if camera is None:
            raise ValueError('Camera must be given if xyz_in_camera_coords')
        ray_points_for_embed = camera.get_world_to_view_transform().transform_points(xyz_world.view(bs, -1, 3)).view(xyz_world.shape)
    else:
        ray_points_for_embed = xyz_world
    if xyz_embedding_function is None:
        embeds = torch.empty(bs, 1, prod(spatial_size), pts_per_ray, 0, dtype=xyz_world.dtype, device=xyz_world.device)
    else:
        embeds = xyz_embedding_function(ray_points_for_embed).reshape(bs, 1, prod(spatial_size), pts_per_ray, -1)
    if fun_viewpool is not None:
        embeds_viewpooled = fun_viewpool(xyz_world.reshape(bs, -1, 3))
        embed_shape = bs, embeds_viewpooled.shape[1], prod(spatial_size), pts_per_ray, -1
        embeds_viewpooled = embeds_viewpooled.reshape(*embed_shape)
        if embeds is not None:
            embeds = torch.cat([embeds.expand(*embed_shape), embeds_viewpooled], dim=-1)
        else:
            embeds = embeds_viewpooled
    if global_code is not None:
        embeds = broadcast_global_code(embeds, global_code)
    return embeds


def _kaiming_normal_init(module: torch.nn.Module) ->None:
    if isinstance(module, (torch.nn.Linear, LinearWithRepeat)):
        torch.nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity='relu', mode='fan_in')


_Scalar = Union[int, float]


_Vector = Union[torch.Tensor, Tuple[_Scalar, ...], List[_Scalar]]


_Translation = _Vector


_ScalarOrVector = Union[_Scalar, _Vector]


_VoxelSize = _ScalarOrVector


def interpolate_volume(points: torch.Tensor, source: torch.Tensor, **kwargs) ->torch.Tensor:
    """
    Interpolates values of source grids. The first dimension of points represents
    number of points and the second coordinates, for example
    [[x0, y0, z0], [x1, y1, z1], ...]. The first dimension of a source represents features
    and ones after that the spatial dimension.

    Arguments:
        points: shape (n_grids, n_points, 3),
        source: tensor of shape (n_grids, features, width, height, depth),
    Returns:
        interpolated tensor of shape (n_grids, n_points, features)
    """
    if 'mode' in kwargs and kwargs['mode'] == 'trilinear':
        kwargs = kwargs.copy()
        kwargs['mode'] = 'bilinear'
    source = source.permute(0, 1, 4, 3, 2)
    grid = points[:, :, None, None, :]
    out = F.grid_sample(grid=grid, input=source, **kwargs)
    return out[:, :, :, 0, 0].permute(0, 2, 1)


def interpolate_line(points: torch.Tensor, source: torch.Tensor, **kwargs) ->torch.Tensor:
    """
    Linearly interpolates values of source grids. The first dimension of points represents
    number of points and the second coordinate, for example ([[x0], [x1], ...]). The first
    dimension of argument source represents feature and ones after that the spatial
    dimension.

    Arguments:
        points: shape (n_grids, n_points, 1),
        source: tensor of shape (n_grids, features, width),
    Returns:
        interpolated tensor of shape (n_grids, n_points, features)
    """
    expansion = points.new_zeros(points.shape)
    points = torch.cat((points, expansion), dim=-1)
    source = source[:, :, None, :]
    points = points[:, :, None, :]
    out = F.grid_sample(grid=points, input=source, **kwargs)
    return out[:, :, :, 0].permute(0, 2, 1)


def interpolate_plane(points: torch.Tensor, source: torch.Tensor, **kwargs) ->torch.Tensor:
    """
    Bilinearly interpolates values of source grids. The first dimension of points represents
    number of points and the second coordinates, for example ([[x0, y0], [x1, y1], ...]).
    The first dimension of argument source represents feature and ones after that the
    spatial dimension.

    Arguments:
        points: shape (n_grids, n_points, 2),
        source: tensor of shape (n_grids, features, width, height),
    Returns:
        interpolated tensor of shape (n_grids, n_points, features)
    """
    source = source.permute(0, 1, 3, 2)
    points = points[:, :, None, :]
    out = F.grid_sample(grid=points, input=source, **kwargs)
    return out[:, :, :, 0].permute(0, 2, 1)


class _RegistratedBufferDict(torch.nn.Module, Mapping):
    """
    Mapping class and a torch.nn.Module that registeres its values
    with `self.register_buffer`. Can be indexed like a regular Python
    dictionary, but torch.Tensors it contains are properly registered, and will be visible
    by all Module methods. Supports only `torch.Tensor` as value and str as key.
    """

    def __init__(self, init_dict: Optional[Dict[str, torch.Tensor]]=None) ->None:
        """
        Args:
            init_dict: dictionary which will be used to populate the object
        """
        super().__init__()
        self._keys = set()
        if init_dict is not None:
            for k, v in init_dict.items():
                self[k] = v

    def __iter__(self) ->Iterator[Dict[str, torch.Tensor]]:
        return iter({k: self[k] for k in self._keys})

    def __len__(self) ->int:
        return len(self._keys)

    def __getitem__(self, key: str) ->torch.Tensor:
        return getattr(self, key)

    def __setitem__(self, key, value) ->None:
        self._keys.add(key)
        self.register_buffer(key, value)

    def __hash__(self) ->int:
        return hash(repr(self))


class RegularizationMetrics(RegularizationMetricsBase):

    def forward(self, model: Any, keys_prefix: str='loss_', **kwargs) ->Dict[str, Any]:
        """
        Calculates the AD penalty, or returns an empty dict if the model's autoencoder
        is inactive.

        Args:
            model: A model instance.
            keys_prefix: A common prefix for all keys in the output dictionary
                containing all regularization metrics.

        Returns:
            A dictionary with the resulting regularization metrics. The items
                will have form `{metric_name_i: metric_value_i}` keyed by the
                names of the output metrics `metric_name_i` with their corresponding
                values `metric_value_i` represented as 0-dimensional float tensors.

            The calculated metric is:
                autoencoder_norm: Autoencoder weight norm regularization term.
        """
        metrics = {}
        if getattr(model, 'sequence_autodecoder', None) is not None:
            ad_penalty = model.sequence_autodecoder.calculate_squared_encoding_norm()
            if ad_penalty is not None:
                metrics['autodecoder_norm'] = ad_penalty
        if keys_prefix is not None:
            metrics = {(keys_prefix + k): v for k, v in metrics.items()}
        return metrics


def _get_depth_neg_penalty_loss(depth):
    neg_penalty = depth.clamp(min=None, max=0.0) ** 2
    return torch.mean(neg_penalty)


def _get_eikonal_loss(grad_theta):
    return ((grad_theta.norm(2, dim=1) - 1) ** 2).mean()


def _get_grid_tv_loss(grid, log_domain: bool=True, eps: float=1e-05):
    if log_domain:
        if (grid <= -eps).any():
            warnings.warn('Grid has negative values; this will produce NaN loss')
        grid = torch.log(grid + eps)
    return torch.mean(utils.safe_sqrt((grid[..., :-1, :-1, 1:] - grid[..., :-1, :-1, :-1]) ** 2 + (grid[..., :-1, 1:, :-1] - grid[..., :-1, :-1, :-1]) ** 2 + (grid[..., 1:, :-1, :-1] - grid[..., :-1, :-1, :-1]) ** 2, eps=1e-05))


def _reshape_nongrid_var(x):
    if x is None:
        return None
    ba, *_, dim = x.shape
    return x.reshape(ba, -1, 1, dim).permute(0, 3, 1, 2).contiguous()


def _rgb_metrics(images, images_pred, masks, masks_pred, masks_crop):
    assert masks_crop is not None
    if images.shape[1] != images_pred.shape[1]:
        raise ValueError(f"Network output's RGB images had {images_pred.shape[1]} channels. {images.shape[1]} expected.")
    rgb_squared = ((images_pred - images) ** 2).mean(dim=1, keepdim=True)
    rgb_loss = utils.huber(rgb_squared, scaling=0.03)
    crop_mass = masks_crop.sum().clamp(1.0)
    results = {'rgb_huber': (rgb_loss * masks_crop).sum() / crop_mass, 'rgb_mse': (rgb_squared * masks_crop).sum() / crop_mass, 'rgb_psnr': utils.calc_psnr(images_pred, images, mask=masks_crop)}
    if masks is not None:
        masks = masks_crop * masks
        results['rgb_psnr_fg'] = utils.calc_psnr(images_pred, images, mask=masks)
        results['rgb_mse_fg'] = (rgb_squared * masks).sum() / masks.sum().clamp(1.0)
    return results


def get_rgbd_point_cloud(camera: CamerasBase, image_rgb: torch.Tensor, depth_map: torch.Tensor, mask: Optional[torch.Tensor]=None, mask_thr: float=0.5, mask_points: bool=True) ->Pointclouds:
    """
    Given a batch of images, depths, masks and cameras, generate a colored
    point cloud by unprojecting depth maps to the  and coloring with the source
    pixel colors.
    """
    imh, imw = image_rgb.shape[2:]
    pts_3d = ray_bundle_to_ray_points(NDCMultinomialRaysampler(image_width=imw, image_height=imh, n_pts_per_ray=1, min_depth=1.0, max_depth=1.0)(camera)._replace(lengths=depth_map[:, 0, ..., None]))
    pts_mask = depth_map > 0.0
    if mask is not None:
        pts_mask *= mask > mask_thr
    pts_mask = pts_mask.reshape(-1)
    pts_3d = pts_3d.reshape(-1, 3)[pts_mask]
    pts_colors = torch.nn.functional.interpolate(image_rgb, size=[imh, imw], mode='bilinear', align_corners=False)
    pts_colors = pts_colors.permute(0, 2, 3, 1).reshape(-1, 3)[pts_mask]
    return Pointclouds(points=pts_3d[None], features=pts_colors[None])


DATASET_TYPE_KNOWN = 'known'


def is_known_frame_scalar(frame_type: str) ->bool:
    """
    Given a single frame type corresponding to a single frame, return whether
    the frame is a known frame.
    """
    return frame_type.endswith(DATASET_TYPE_KNOWN)


def is_known_frame(frame_type: List[str], device: Optional[str]=None) ->torch.BoolTensor:
    """
    Given a list `frame_type` of frame types in a batch, return a tensor
    of boolean flags expressing whether the corresponding frame is a known frame.
    """
    return torch.tensor([is_known_frame_scalar(ft) for ft in frame_type], dtype=torch.bool, device=device)


def _init_recurrent_weights(self) ->None:
    for m in self.modules():
        if type(m) in [torch.nn.GRU, torch.nn.LSTM, torch.nn.RNN]:
            for name, param in m.named_parameters():
                if 'weight_ih' in name:
                    torch.nn.init.kaiming_normal_(param.data)
                elif 'weight_hh' in name:
                    torch.nn.init.orthogonal_(param.data)
                elif 'bias' in name:
                    param.data.fill_(0)


def _lstm_forget_gate_init(lstm_layer) ->None:
    for name, parameter in lstm_layer.named_parameters():
        if 'bias' not in name:
            continue
        n = parameter.size(0)
        start, end = n // 4, n // 2
        parameter.data[start:end].fill_(1.0)


def _get_sphere_intersection(cam_loc: torch.Tensor, ray_directions: torch.Tensor, r: float=1.0) ->Tuple[torch.Tensor, torch.Tensor]:
    n_imgs, n_pix, _ = ray_directions.shape
    device = cam_loc.device
    ray_cam_dot = (ray_directions * cam_loc).sum(-1)
    under_sqrt = ray_cam_dot ** 2 - (cam_loc.norm(2, dim=-1) ** 2 - r ** 2)
    under_sqrt = under_sqrt.reshape(-1)
    mask_intersect = under_sqrt > 0
    sphere_intersections = torch.zeros(n_imgs * n_pix, 2, device=device)
    sphere_intersections[mask_intersect] = torch.sqrt(under_sqrt[mask_intersect]).unsqueeze(-1) * torch.tensor([-1.0, 1.0], device=device)
    sphere_intersections[mask_intersect] -= ray_cam_dot.reshape(-1)[mask_intersect].unsqueeze(-1)
    sphere_intersections = sphere_intersections.reshape(n_imgs, n_pix, 2)
    sphere_intersections = sphere_intersections.clamp_min(0.0)
    mask_intersect = mask_intersect.reshape(n_imgs, n_pix)
    return sphere_intersections, mask_intersect


class RayTracing(Configurable, nn.Module):
    """
    Finds the intersection points of rays with the implicit surface defined
    by a signed distance function (SDF). The algorithm follows the pipeline:
    1. Initialise start and end points on rays by the intersections with
        the circumscribing sphere.
    2. Run sphere tracing from both ends.
    3. Divide the untraced segments of non-convergent rays into uniform
        intervals and find the one with the sign transition.
    4. Run the secant method to estimate the point of the sign transition.

    Args:
        object_bounding_sphere: The radius of the initial sphere circumscribing
            the object.
        sdf_threshold: Absolute SDF value small enough for the sphere tracer
            to consider it a surface.
        line_search_step: Length of the backward correction on sphere tracing
            iterations.
        line_step_iters: Number of backward correction iterations.
        sphere_tracing_iters: Maximum number of sphere tracing iterations
            (the actual number of iterations may be smaller if all ray
            intersections are found).
        n_steps: Number of intervals sampled for unconvergent rays.
        n_secant_steps: Number of iterations in the secant algorithm.
    """
    object_bounding_sphere: float = 1.0
    sdf_threshold: float = 5e-05
    line_search_step: float = 0.5
    line_step_iters: int = 1
    sphere_tracing_iters: int = 10
    n_steps: int = 100
    n_secant_steps: int = 8

    def __post_init__(self):
        super().__init__()

    def forward(self, sdf: Callable[[torch.Tensor], torch.Tensor], cam_loc: torch.Tensor, object_mask: torch.BoolTensor, ray_directions: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            sdf: A callable that takes a (N, 3) tensor of points and returns
                a tensor of (N,) SDF values.
            cam_loc: A tensor of (B, N, 3) ray origins.
            object_mask: A (N, 3) tensor of indicators whether a sampled pixel
                corresponds to the rendered object or background.
            ray_directions: A tensor of (B, N, 3) ray directions.

        Returns:
            curr_start_points: A tensor of (B*N, 3) found intersection points
                with the implicit surface.
            network_object_mask: A tensor of (B*N,) indicators denoting whether
                intersections were found.
            acc_start_dis: A tensor of (B*N,) distances from the ray origins
                to intersrection points.
        """
        batch_size, num_pixels, _ = ray_directions.shape
        device = cam_loc.device
        sphere_intersections, mask_intersect = _get_sphere_intersection(cam_loc, ray_directions, r=self.object_bounding_sphere)
        curr_start_points, unfinished_mask_start, acc_start_dis, acc_end_dis, min_dis, max_dis = self.sphere_tracing(batch_size, num_pixels, sdf, cam_loc, ray_directions, mask_intersect, sphere_intersections)
        network_object_mask = acc_start_dis < acc_end_dis
        sampler_mask = unfinished_mask_start
        sampler_net_obj_mask = torch.zeros_like(sampler_mask, dtype=torch.bool, device=device)
        if sampler_mask.sum() > 0:
            sampler_min_max = torch.zeros((batch_size, num_pixels, 2), device=device)
            sampler_min_max.reshape(-1, 2)[sampler_mask, 0] = acc_start_dis[sampler_mask]
            sampler_min_max.reshape(-1, 2)[sampler_mask, 1] = acc_end_dis[sampler_mask]
            sampler_pts, sampler_net_obj_mask, sampler_dists = self.ray_sampler(sdf, cam_loc, object_mask, ray_directions, sampler_min_max, sampler_mask)
            curr_start_points[sampler_mask] = sampler_pts[sampler_mask]
            acc_start_dis[sampler_mask] = sampler_dists[sampler_mask]
            network_object_mask[sampler_mask] = sampler_net_obj_mask[sampler_mask]
        if not self.training:
            return curr_start_points, network_object_mask, acc_start_dis
        ray_directions = ray_directions.reshape(-1, 3)
        mask_intersect = mask_intersect.reshape(-1)
        object_mask = object_mask.reshape(-1)
        in_mask = ~network_object_mask & object_mask & ~sampler_mask
        out_mask = ~object_mask & ~sampler_mask
        mask_left_out = (in_mask | out_mask) & ~mask_intersect
        if mask_left_out.sum() > 0:
            cam_left_out = cam_loc.reshape(-1, 3)[mask_left_out]
            rays_left_out = ray_directions[mask_left_out]
            acc_start_dis[mask_left_out] = -torch.bmm(rays_left_out.view(-1, 1, 3), cam_left_out.view(-1, 3, 1)).squeeze()
            curr_start_points[mask_left_out] = cam_left_out + acc_start_dis[mask_left_out].unsqueeze(1) * rays_left_out
        mask = (in_mask | out_mask) & mask_intersect
        if mask.sum() > 0:
            min_dis[network_object_mask & out_mask] = acc_start_dis[network_object_mask & out_mask]
            min_mask_points, min_mask_dist = self.minimal_sdf_points(sdf, cam_loc, ray_directions, mask, min_dis, max_dis)
            curr_start_points[mask] = min_mask_points
            acc_start_dis[mask] = min_mask_dist
        return curr_start_points, network_object_mask, acc_start_dis

    def sphere_tracing(self, batch_size: int, num_pixels: int, sdf: Callable[[torch.Tensor], torch.Tensor], cam_loc: torch.Tensor, ray_directions: torch.Tensor, mask_intersect: torch.Tensor, sphere_intersections: torch.Tensor) ->Tuple[Any, Any, Any, Any, Any, Any]:
        """
        Run sphere tracing algorithm for max iterations
        from both sides of unit sphere intersection

        Args:
            batch_size:
            num_pixels:
            sdf:
            cam_loc:
            ray_directions:
            mask_intersect:
            sphere_intersections:

        Returns:
            curr_start_points:
            unfinished_mask_start:
            acc_start_dis:
            acc_end_dis:
            min_dis:
            max_dis:
        """
        device = cam_loc.device
        sphere_intersections_points = cam_loc[..., None, :] + sphere_intersections[..., None] * ray_directions[..., None, :]
        unfinished_mask_start = mask_intersect.reshape(-1).clone()
        unfinished_mask_end = mask_intersect.reshape(-1).clone()
        curr_start_points = torch.zeros(batch_size * num_pixels, 3, device=device)
        curr_start_points[unfinished_mask_start] = sphere_intersections_points[:, :, 0, :].reshape(-1, 3)[unfinished_mask_start]
        acc_start_dis = torch.zeros(batch_size * num_pixels, device=device)
        acc_start_dis[unfinished_mask_start] = sphere_intersections.reshape(-1, 2)[unfinished_mask_start, 0]
        curr_end_points = torch.zeros(batch_size * num_pixels, 3, device=device)
        curr_end_points[unfinished_mask_end] = sphere_intersections_points[:, :, 1, :].reshape(-1, 3)[unfinished_mask_end]
        acc_end_dis = torch.zeros(batch_size * num_pixels, device=device)
        acc_end_dis[unfinished_mask_end] = sphere_intersections.reshape(-1, 2)[unfinished_mask_end, 1]
        min_dis = acc_start_dis.clone()
        max_dis = acc_end_dis.clone()
        iters = 0
        next_sdf_start = torch.zeros_like(acc_start_dis)
        next_sdf_start[unfinished_mask_start] = sdf(curr_start_points[unfinished_mask_start])
        next_sdf_end = torch.zeros_like(acc_end_dis)
        next_sdf_end[unfinished_mask_end] = sdf(curr_end_points[unfinished_mask_end])
        while True:
            curr_sdf_start = torch.zeros_like(acc_start_dis)
            curr_sdf_start[unfinished_mask_start] = next_sdf_start[unfinished_mask_start]
            curr_sdf_start[curr_sdf_start <= self.sdf_threshold] = 0
            curr_sdf_end = torch.zeros_like(acc_end_dis)
            curr_sdf_end[unfinished_mask_end] = next_sdf_end[unfinished_mask_end]
            curr_sdf_end[curr_sdf_end <= self.sdf_threshold] = 0
            unfinished_mask_start = unfinished_mask_start & (curr_sdf_start > self.sdf_threshold)
            unfinished_mask_end = unfinished_mask_end & (curr_sdf_end > self.sdf_threshold)
            if unfinished_mask_start.sum() == 0 and unfinished_mask_end.sum() == 0 or iters == self.sphere_tracing_iters:
                break
            iters += 1
            acc_start_dis = acc_start_dis + curr_sdf_start
            acc_end_dis = acc_end_dis - curr_sdf_end
            curr_start_points = (cam_loc + acc_start_dis.reshape(batch_size, num_pixels, 1) * ray_directions).reshape(-1, 3)
            curr_end_points = (cam_loc + acc_end_dis.reshape(batch_size, num_pixels, 1) * ray_directions).reshape(-1, 3)
            next_sdf_start = torch.zeros_like(acc_start_dis)
            next_sdf_start[unfinished_mask_start] = sdf(curr_start_points[unfinished_mask_start])
            next_sdf_end = torch.zeros_like(acc_end_dis)
            next_sdf_end[unfinished_mask_end] = sdf(curr_end_points[unfinished_mask_end])
            not_projected_start = next_sdf_start < 0
            not_projected_end = next_sdf_end < 0
            not_proj_iters = 0
            while (not_projected_start.sum() > 0 or not_projected_end.sum() > 0) and not_proj_iters < self.line_step_iters:
                acc_start_dis[not_projected_start] -= (1 - self.line_search_step) / 2 ** not_proj_iters * curr_sdf_start[not_projected_start]
                curr_start_points[not_projected_start] = (cam_loc + acc_start_dis.reshape(batch_size, num_pixels, 1) * ray_directions).reshape(-1, 3)[not_projected_start]
                acc_end_dis[not_projected_end] += (1 - self.line_search_step) / 2 ** not_proj_iters * curr_sdf_end[not_projected_end]
                curr_end_points[not_projected_end] = (cam_loc + acc_end_dis.reshape(batch_size, num_pixels, 1) * ray_directions).reshape(-1, 3)[not_projected_end]
                next_sdf_start[not_projected_start] = sdf(curr_start_points[not_projected_start])
                next_sdf_end[not_projected_end] = sdf(curr_end_points[not_projected_end])
                not_projected_start = next_sdf_start < 0
                not_projected_end = next_sdf_end < 0
                not_proj_iters += 1
            unfinished_mask_start = unfinished_mask_start & (acc_start_dis < acc_end_dis)
            unfinished_mask_end = unfinished_mask_end & (acc_start_dis < acc_end_dis)
        return curr_start_points, unfinished_mask_start, acc_start_dis, acc_end_dis, min_dis, max_dis

    def ray_sampler(self, sdf: Callable[[torch.Tensor], torch.Tensor], cam_loc: torch.Tensor, object_mask: torch.Tensor, ray_directions: torch.Tensor, sampler_min_max: torch.Tensor, sampler_mask: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Sample the ray in a given range and run secant on rays which have sign transition.

        Args:
            sdf:
            cam_loc:
            object_mask:
            ray_directions:
            sampler_min_max:
            sampler_mask:

        Returns:

        """
        batch_size, num_pixels, _ = ray_directions.shape
        device = cam_loc.device
        n_total_pxl = batch_size * num_pixels
        sampler_pts = torch.zeros(n_total_pxl, 3, device=device)
        sampler_dists = torch.zeros(n_total_pxl, device=device)
        intervals_dist = torch.linspace(0, 1, steps=self.n_steps, device=device).view(1, 1, -1)
        pts_intervals = sampler_min_max[:, :, 0].unsqueeze(-1) + intervals_dist * (sampler_min_max[:, :, 1] - sampler_min_max[:, :, 0]).unsqueeze(-1)
        points = cam_loc[..., None, :] + pts_intervals[..., None] * ray_directions[..., None, :]
        mask_intersect_idx = torch.nonzero(sampler_mask).flatten()
        points = points.reshape((-1, self.n_steps, 3))[sampler_mask, :, :]
        pts_intervals = pts_intervals.reshape((-1, self.n_steps))[sampler_mask]
        sdf_val_all = []
        for pnts in torch.split(points.reshape(-1, 3), 100000, dim=0):
            sdf_val_all.append(sdf(pnts))
        sdf_val = torch.cat(sdf_val_all).reshape(-1, self.n_steps)
        tmp = torch.sign(sdf_val) * torch.arange(self.n_steps, 0, -1, device=device, dtype=torch.float32).reshape(1, self.n_steps)
        sampler_pts_ind = torch.argmin(tmp, -1)
        sampler_pts[mask_intersect_idx] = points[torch.arange(points.shape[0]), sampler_pts_ind, :]
        sampler_dists[mask_intersect_idx] = pts_intervals[torch.arange(pts_intervals.shape[0]), sampler_pts_ind]
        true_surface_pts = object_mask.reshape(-1)[sampler_mask]
        net_surface_pts = sdf_val[torch.arange(sdf_val.shape[0]), sampler_pts_ind] < 0
        p_out_mask = ~(true_surface_pts & net_surface_pts)
        n_p_out = p_out_mask.sum()
        if n_p_out > 0:
            out_pts_idx = torch.argmin(sdf_val[p_out_mask, :], -1)
            sampler_pts[mask_intersect_idx[p_out_mask]] = points[p_out_mask, :, :][torch.arange(n_p_out), out_pts_idx, :]
            sampler_dists[mask_intersect_idx[p_out_mask]] = pts_intervals[p_out_mask, :][torch.arange(n_p_out), out_pts_idx]
        sampler_net_obj_mask = sampler_mask.clone()
        sampler_net_obj_mask[mask_intersect_idx[~net_surface_pts]] = False
        secant_pts = net_surface_pts & true_surface_pts if self.training else net_surface_pts
        n_secant_pts = secant_pts.sum()
        if n_secant_pts > 0:
            z_high = pts_intervals[torch.arange(pts_intervals.shape[0]), sampler_pts_ind][secant_pts]
            sdf_high = sdf_val[torch.arange(sdf_val.shape[0]), sampler_pts_ind][secant_pts]
            z_low = pts_intervals[secant_pts][torch.arange(n_secant_pts), sampler_pts_ind[secant_pts] - 1]
            sdf_low = sdf_val[secant_pts][torch.arange(n_secant_pts), sampler_pts_ind[secant_pts] - 1]
            cam_loc_secant = cam_loc.reshape(-1, 3)[mask_intersect_idx[secant_pts]]
            ray_directions_secant = ray_directions.reshape((-1, 3))[mask_intersect_idx[secant_pts]]
            z_pred_secant = self.secant(sdf_low, sdf_high, z_low, z_high, cam_loc_secant, ray_directions_secant, sdf)
            sampler_pts[mask_intersect_idx[secant_pts]] = cam_loc_secant + z_pred_secant.unsqueeze(-1) * ray_directions_secant
            sampler_dists[mask_intersect_idx[secant_pts]] = z_pred_secant
        return sampler_pts, sampler_net_obj_mask, sampler_dists

    def secant(self, sdf_low: torch.Tensor, sdf_high: torch.Tensor, z_low: torch.Tensor, z_high: torch.Tensor, cam_loc: torch.Tensor, ray_directions: torch.Tensor, sdf: nn.Module) ->torch.Tensor:
        """
        Runs the secant method for interval [z_low, z_high] for n_secant_steps
        """
        z_pred = -sdf_low * (z_high - z_low) / (sdf_high - sdf_low) + z_low
        for _ in range(self.n_secant_steps):
            p_mid = cam_loc + z_pred.unsqueeze(-1) * ray_directions
            sdf_mid = sdf(p_mid)
            ind_low = sdf_mid > 0
            if ind_low.sum() > 0:
                z_low[ind_low] = z_pred[ind_low]
                sdf_low[ind_low] = sdf_mid[ind_low]
            ind_high = sdf_mid < 0
            if ind_high.sum() > 0:
                z_high[ind_high] = z_pred[ind_high]
                sdf_high[ind_high] = sdf_mid[ind_high]
            z_pred = -sdf_low * (z_high - z_low) / (sdf_high - sdf_low) + z_low
        return z_pred

    def minimal_sdf_points(self, sdf: Callable[[torch.Tensor], torch.Tensor], cam_loc: torch.Tensor, ray_directions: torch.Tensor, mask: torch.Tensor, min_dis: torch.Tensor, max_dis: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Find points with minimal SDF value on rays for P_out pixels
        """
        n_mask_points = mask.sum()
        n = self.n_steps
        steps = torch.empty(n, device=cam_loc.device).uniform_(0.0, 1.0)
        mask_max_dis = max_dis[mask].unsqueeze(-1)
        mask_min_dis = min_dis[mask].unsqueeze(-1)
        steps = steps.unsqueeze(0).repeat(n_mask_points, 1) * (mask_max_dis - mask_min_dis) + mask_min_dis
        mask_points = cam_loc.reshape(-1, 3)[mask]
        mask_rays = ray_directions[mask, :]
        mask_points_all = mask_points.unsqueeze(1).repeat(1, n, 1) + steps.unsqueeze(-1) * mask_rays.unsqueeze(1).repeat(1, n, 1)
        points = mask_points_all.reshape(-1, 3)
        mask_sdf_all = []
        for pnts in torch.split(points, 100000, dim=0):
            mask_sdf_all.append(sdf(pnts))
        mask_sdf_all = torch.cat(mask_sdf_all).reshape(-1, n)
        min_vals, min_idx = mask_sdf_all.min(-1)
        min_mask_points = mask_points_all.reshape(-1, n, 3)[torch.arange(0, n_mask_points), min_idx]
        min_mask_dist = steps.reshape(-1, n)[torch.arange(0, n_mask_points), min_idx]
        return min_mask_points, min_mask_dist


_TTensor = torch.Tensor


@torch.enable_grad()
def _gradient(module, rays_points_world):
    rays_points_world.requires_grad_(True)
    y = module.forward(rays_points_world=rays_points_world)[:, :1]
    d_output = torch.ones_like(y, requires_grad=False, device=y.device)
    gradients = torch.autograd.grad(outputs=y, inputs=rays_points_world, grad_outputs=d_output, create_graph=True, retain_graph=True, only_inputs=True)[0]
    return gradients.unsqueeze(1)


def _sample_network(surface_output, surface_sdf_values, surface_points_grad, surface_dists, surface_cam_loc, surface_ray_dirs, eps: float=0.0001):
    surface_ray_dirs_0 = surface_ray_dirs.detach()
    surface_points_dot = torch.bmm(surface_points_grad.view(-1, 1, 3), surface_ray_dirs_0.view(-1, 3, 1)).squeeze(-1)
    dot_sign = (surface_points_dot >= 0) * 2 - 1
    surface_dists_theta = surface_dists - (surface_output - surface_sdf_values) / (surface_points_dot.abs().clip(eps) * dot_sign)
    surface_points_theta_c_v = surface_cam_loc + surface_dists_theta * surface_ray_dirs
    return surface_points_theta_c_v


class ReductionFunction(Enum):
    AVG = 'avg'
    MAX = 'max'
    STD = 'std'
    STD_AVG = 'std_avg'


def _get_reduction_function_output_dim(reduction_function: ReductionFunction, feat_dim: int) ->int:
    if reduction_function == ReductionFunction.STD_AVG:
        return 1
    else:
        return feat_dim


def _get_reduction_aggregator_feature_dim(feats_or_feats_dim: Union[Dict[str, torch.Tensor], int], reduction_functions: Sequence[ReductionFunction]) ->int:
    if isinstance(feats_or_feats_dim, int):
        feat_dim = feats_or_feats_dim
    else:
        feat_dim = int(sum(f.shape[1] for f in feats_or_feats_dim.values()))
    if len(reduction_functions) == 0:
        return feat_dim
    return sum(_get_reduction_function_output_dim(reduction_function, feat_dim) for reduction_function in reduction_functions)


def _get_view_sampling_mask(n_cameras: int, pts_batch: int, device: Union[str, torch.device], exclude_target_view: bool):
    return (-torch.eye(n_cameras, device=device, dtype=torch.float32) * float(exclude_target_view) + 1.0)[:pts_batch]


def _mask_target_view_features(feats_sampled: Dict[str, torch.Tensor]):
    one_feature_sampled = next(iter(feats_sampled.values()))
    pts_batch, n_cameras = one_feature_sampled.shape[:2]
    view_sampling_mask = _get_view_sampling_mask(n_cameras, pts_batch, one_feature_sampled.device, True)
    view_sampling_mask = view_sampling_mask.view(pts_batch, n_cameras, *([1] * (one_feature_sampled.ndim - 2)))
    return {k: (f * view_sampling_mask) for k, f in feats_sampled.items()}


class IdentityFeatureAggregator(torch.nn.Module, FeatureAggregatorBase):
    """
    This aggregator does not perform any feature aggregation. Depending on the
    settings the aggregator allows to mask target view features and concatenate
    the outputs.
    """

    def __post_init__(self):
        super().__init__()

    def get_aggregated_feature_dim(self, feats_or_feats_dim: Union[Dict[str, torch.Tensor], int]):
        return _get_reduction_aggregator_feature_dim(feats_or_feats_dim, [])

    def forward(self, feats_sampled: Dict[str, torch.Tensor], masks_sampled: torch.Tensor, camera: Optional[CamerasBase]=None, pts: Optional[torch.Tensor]=None, **kwargs) ->Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            feats_sampled: A `dict` of sampled feature tensors `{f_i: t_i}`,
                where each `t_i` is a tensor of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
            masks_sampled: A binary mask represented as a tensor of shape
                `(minibatch, n_source_views, n_samples, 1)` denoting valid
                sampled features.
            camera: A batch of `n_source_views` `CamerasBase` objects
                corresponding to the source view cameras.
            pts: A tensor of shape `(minibatch, n_samples, 3)` denoting the
                3D points whose 2D projections to source views were sampled in
                order to generate `feats_sampled` and `masks_sampled`.

        Returns:
            feats_aggregated: If `concatenate_output==True`, a tensor
                of shape `(minibatch, 1, n_samples, sum(dim_1, ... dim_N))`.
                If `concatenate_output==False`, a dictionary `{f_i: t_i_aggregated}`
                with each `t_i_aggregated` of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
        """
        if self.exclude_target_view_mask_features:
            feats_sampled = _mask_target_view_features(feats_sampled)
        feats_aggregated = feats_sampled
        if self.concatenate_output:
            feats_aggregated = torch.cat(tuple(feats_aggregated.values()), dim=-1)
        return feats_aggregated


def wmean(x: torch.Tensor, weight: Optional[torch.Tensor]=None, dim: Union[int, Tuple[int]]=-2, keepdim: bool=True, eps: float=1e-09) ->torch.Tensor:
    """
    Finds the mean of the input tensor across the specified dimension.
    If the `weight` argument is provided, computes weighted mean.
    Args:
        x: tensor of shape `(*, D)`, where D is assumed to be spatial;
        weights: if given, non-negative tensor of shape `(*,)`. It must be
            broadcastable to `x.shape[:-1]`. Note that the weights for
            the last (spatial) dimension are assumed same;
        dim: dimension(s) in `x` to average over;
        keepdim: tells whether to keep the resulting singleton dimension.
        eps: minimum clamping value in the denominator.
    Returns:
        the mean tensor:
        * if `weights` is None => `mean(x, dim)`,
        * otherwise => `sum(x*w, dim) / max{sum(w, dim), eps}`.
    """
    args = {'dim': dim, 'keepdim': keepdim}
    if weight is None:
        return x.mean(**args)
    if any(xd != wd and xd != 1 and wd != 1 for xd, wd in zip(x.shape[-2::-1], weight.shape[::-1])):
        raise ValueError('wmean: weights are not compatible with the tensor')
    return (x * weight[..., None]).sum(**args) / weight[..., None].sum(**args).clamp(eps)


def _avg_reduction_function(x: torch.Tensor, w: torch.Tensor, dim: int=1):
    mu = wmean(x, w, dim=dim, eps=0.01)
    return mu


def _max_reduction_function(x: torch.Tensor, w: torch.Tensor, dim: int=1, big_M_factor: float=10.0):
    big_M = x.max(dim=dim, keepdim=True).values.abs() * big_M_factor
    max_ = (x * w - (1 - w) * big_M).max(dim=dim, keepdim=True).values
    return max_


def _std_reduction_function(x: torch.Tensor, w: torch.Tensor, dim: int=1, mu: Optional[torch.Tensor]=None):
    if mu is None:
        mu = _avg_reduction_function(x, w, dim=dim)
    std = wmean((x - mu) ** 2, w, dim=dim, eps=0.01).clamp(0.0001).sqrt()
    return std


def _std_avg_reduction_function(x: torch.Tensor, w: torch.Tensor, dim: int=1, mu: Optional[torch.Tensor]=None, std: Optional[torch.Tensor]=None):
    if std is None:
        std = _std_reduction_function(x, w, dim=dim, mu=mu)
    stdmean = std.mean(dim=-1, keepdim=True)
    return stdmean


def _avgmaxstd_reduction_function(x: torch.Tensor, w: torch.Tensor, reduction_functions: Sequence[ReductionFunction], dim: int=1):
    """
    Args:
        x: Features to aggreagate. Tensor of shape `(batch, n_views, ..., dim)`.
        w: Aggregation weights. Tensor of shape `(batch, n_views, ...,)`.
        dim: the dimension along which to aggregate.
        reduction_functions: The set of reduction functions.

    Returns:
        x_aggr: Aggregation of `x` to a tensor of shape `(batch, 1, ..., dim_aggregate)`.
    """
    pooled_features = []
    mu = None
    std = None
    if ReductionFunction.AVG in reduction_functions:
        mu = _avg_reduction_function(x, w, dim=dim)
        pooled_features.append(mu)
    if ReductionFunction.STD in reduction_functions:
        std = _std_reduction_function(x, w, dim=dim, mu=mu)
        pooled_features.append(std)
    if ReductionFunction.STD_AVG in reduction_functions:
        stdavg = _std_avg_reduction_function(x, w, dim=dim, mu=mu, std=std)
        pooled_features.append(stdavg)
    if ReductionFunction.MAX in reduction_functions:
        max_ = _max_reduction_function(x, w, dim=dim)
        pooled_features.append(max_)
    x_aggr = torch.cat(pooled_features, dim=-1)
    any_active = (w.max(dim=dim, keepdim=True).values > 0.0001).type_as(x_aggr)
    x_aggr = x_aggr * any_active[..., None]
    assert torch.isfinite(x_aggr).all()
    assert x_aggr.shape[1] == 1
    return x_aggr


class ReductionFeatureAggregator(torch.nn.Module, FeatureAggregatorBase):
    """
    Aggregates using a set of predefined `reduction_functions` and concatenates
    the results of each aggregation function along the
    channel dimension. The reduction functions singularize the second dimension
    of the sampled features which stacks the source views.

    Settings:
        reduction_functions: A list of `ReductionFunction`s` that reduce the
            the stack of source-view-specific features to a single feature.
    """
    reduction_functions: Tuple[ReductionFunction, ...] = (ReductionFunction.AVG, ReductionFunction.STD)

    def __post_init__(self):
        super().__init__()

    def get_aggregated_feature_dim(self, feats_or_feats_dim: Union[Dict[str, torch.Tensor], int]):
        return _get_reduction_aggregator_feature_dim(feats_or_feats_dim, self.reduction_functions)

    def forward(self, feats_sampled: Dict[str, torch.Tensor], masks_sampled: torch.Tensor, camera: Optional[CamerasBase]=None, pts: Optional[torch.Tensor]=None, **kwargs) ->Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            feats_sampled: A `dict` of sampled feature tensors `{f_i: t_i}`,
                where each `t_i` is a tensor of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
            masks_sampled: A binary mask represented as a tensor of shape
                `(minibatch, n_source_views, n_samples, 1)` denoting valid
                sampled features.
            camera: A batch of `n_source_views` `CamerasBase` objects corresponding
                to the source view cameras.
            pts: A tensor of shape `(minibatch, n_samples, 3)` denoting the
                3D points whose 2D projections to source views were sampled in
                order to generate `feats_sampled` and `masks_sampled`.

        Returns:
            feats_aggregated: If `concatenate_output==True`, a tensor
                of shape `(minibatch, 1, n_samples, sum(dim_1, ... dim_N))`.
                If `concatenate_output==False`, a dictionary `{f_i: t_i_aggregated}`
                with each `t_i_aggregated` of shape `(minibatch, 1, n_samples, aggr_dim_i)`.
        """
        pts_batch, n_cameras = masks_sampled.shape[:2]
        if self.exclude_target_view_mask_features:
            feats_sampled = _mask_target_view_features(feats_sampled)
        sampling_mask = _get_view_sampling_mask(n_cameras, pts_batch, masks_sampled.device, self.exclude_target_view)
        aggr_weigths = masks_sampled[..., 0] * sampling_mask[..., None]
        feats_aggregated = {k: _avgmaxstd_reduction_function(f, aggr_weigths, dim=1, reduction_functions=self.reduction_functions) for k, f in feats_sampled.items()}
        if self.concatenate_output:
            feats_aggregated = torch.cat(tuple(feats_aggregated.values()), dim=-1)
        return feats_aggregated


def _get_ray_dir_dot_prods(camera: CamerasBase, pts: torch.Tensor):
    n_cameras = camera.R.shape[0]
    pts_batch = pts.shape[0]
    camera_rep, pts_rep = cameras_points_cartesian_product(camera, pts)
    cam_centers_rep = -torch.bmm(camera_rep.T[:, None], camera_rep.R.permute(0, 2, 1)).reshape(-1, *([1] * (pts.ndim - 2)), 3)
    ray_dirs = F.normalize(pts_rep - cam_centers_rep, dim=-1)
    ray_dirs_reshape = ray_dirs.view(n_cameras, pts_batch, -1, 3)
    ray_dirs_pts = torch.stack([ray_dirs_reshape[i, i] for i in range(pts_batch)])
    ray_dir_dot_prods = (ray_dirs_pts[None] * ray_dirs_reshape).sum(dim=-1)
    return ray_dir_dot_prods.transpose(0, 1)


def _get_ray_angle_weights(camera: CamerasBase, pts: torch.Tensor, min_ray_angle_weight: float, weight_by_ray_angle_gamma: float):
    ray_dir_dot_prods = _get_ray_dir_dot_prods(camera, pts)
    angle_weight_01 = ray_dir_dot_prods * 0.5 + 0.5
    angle_weight = (angle_weight_01 + min_ray_angle_weight) ** weight_by_ray_angle_gamma
    return angle_weight


def _get_angular_reduction_weights(view_sampling_mask: torch.Tensor, masks_sampled: torch.Tensor, camera: CamerasBase, pts: torch.Tensor, min_ray_angle_weight: float, weight_by_ray_angle_gamma: float):
    aggr_weights = masks_sampled.clone()[..., 0]
    assert not any(v is None for v in [camera, pts])
    angle_weight = _get_ray_angle_weights(camera, pts, min_ray_angle_weight, weight_by_ray_angle_gamma)
    assert torch.isfinite(angle_weight).all()
    view_sampling_mask = view_sampling_mask.view(*view_sampling_mask.shape[:2], *([1] * (aggr_weights.ndim - 2)))
    aggr_weights = aggr_weights * angle_weight.reshape_as(aggr_weights) * view_sampling_mask
    return aggr_weights


class AngleWeightedReductionFeatureAggregator(torch.nn.Module, FeatureAggregatorBase):
    """
    Performs a weighted aggregation using a set of predefined `reduction_functions`
    and concatenates the results of each aggregation function along the
    channel dimension. The weights are proportional to the cosine of the
    angle between the target ray and the source ray::

        weight = (
            dot(target_ray, source_ray) * 0.5 + 0.5 + self.min_ray_angle_weight
        )**self.weight_by_ray_angle_gamma

    The reduction functions singularize the second dimension
    of the sampled features which stacks the source views.

    Settings:
        reduction_functions: A list of `ReductionFunction`s that reduce the
            the stack of source-view-specific features to a single feature.
        min_ray_angle_weight: The minimum possible aggregation weight
            before rasising to the power of `self.weight_by_ray_angle_gamma`.
        weight_by_ray_angle_gamma: The exponent of the cosine of the ray angles
            used when calculating the angle-based aggregation weights.
    """
    reduction_functions: Tuple[ReductionFunction, ...] = (ReductionFunction.AVG, ReductionFunction.STD)
    weight_by_ray_angle_gamma: float = 1.0
    min_ray_angle_weight: float = 0.1

    def __post_init__(self):
        super().__init__()

    def get_aggregated_feature_dim(self, feats_or_feats_dim: Union[Dict[str, torch.Tensor], int]):
        return _get_reduction_aggregator_feature_dim(feats_or_feats_dim, self.reduction_functions)

    def forward(self, feats_sampled: Dict[str, torch.Tensor], masks_sampled: torch.Tensor, camera: Optional[CamerasBase]=None, pts: Optional[torch.Tensor]=None, **kwargs) ->Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            feats_sampled: A `dict` of sampled feature tensors `{f_i: t_i}`,
                where each `t_i` is a tensor of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
            masks_sampled: A binary mask represented as a tensor of shape
                `(minibatch, n_source_views, n_samples, 1)` denoting valid
                sampled features.
            camera: A batch of `n_source_views` `CamerasBase` objects
                corresponding to the source view cameras.
            pts: A tensor of shape `(minibatch, n_samples, 3)` denoting the
                3D points whose 2D projections to source views were sampled in
                order to generate `feats_sampled` and `masks_sampled`.

        Returns:
            feats_aggregated: If `concatenate_output==True`, a tensor
                of shape `(minibatch, 1, n_samples, sum(dim_1, ... dim_N))`.
                If `concatenate_output==False`, a dictionary `{f_i: t_i_aggregated}`
                with each `t_i_aggregated` of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
        """
        if camera is None:
            raise ValueError('camera cannot be None for angle weighted aggregation')
        if pts is None:
            raise ValueError('Points cannot be None for angle weighted aggregation')
        pts_batch, n_cameras = masks_sampled.shape[:2]
        if self.exclude_target_view_mask_features:
            feats_sampled = _mask_target_view_features(feats_sampled)
        view_sampling_mask = _get_view_sampling_mask(n_cameras, pts_batch, masks_sampled.device, self.exclude_target_view)
        aggr_weights = _get_angular_reduction_weights(view_sampling_mask, masks_sampled, camera, pts, self.min_ray_angle_weight, self.weight_by_ray_angle_gamma)
        assert torch.isfinite(aggr_weights).all()
        feats_aggregated = {k: _avgmaxstd_reduction_function(f, aggr_weights, dim=1, reduction_functions=self.reduction_functions) for k, f in feats_sampled.items()}
        if self.concatenate_output:
            feats_aggregated = torch.cat(tuple(feats_aggregated.values()), dim=-1)
        return feats_aggregated


class AngleWeightedIdentityFeatureAggregator(torch.nn.Module, FeatureAggregatorBase):
    """
    This aggregator does not perform any feature aggregation. It only weights
    the features by the weights proportional to the cosine of the
    angle between the target ray and the source ray::

        weight = (
            dot(target_ray, source_ray) * 0.5 + 0.5 + self.min_ray_angle_weight
        )**self.weight_by_ray_angle_gamma

    Settings:
        min_ray_angle_weight: The minimum possible aggregation weight
            before rasising to the power of `self.weight_by_ray_angle_gamma`.
        weight_by_ray_angle_gamma: The exponent of the cosine of the ray angles
            used when calculating the angle-based aggregation weights.

    Additionally the aggregator allows to mask target view features and to concatenate
    the outputs.
    """
    weight_by_ray_angle_gamma: float = 1.0
    min_ray_angle_weight: float = 0.1

    def __post_init__(self):
        super().__init__()

    def get_aggregated_feature_dim(self, feats_or_feats_dim: Union[Dict[str, torch.Tensor], int]):
        return _get_reduction_aggregator_feature_dim(feats_or_feats_dim, [])

    def forward(self, feats_sampled: Dict[str, torch.Tensor], masks_sampled: torch.Tensor, camera: Optional[CamerasBase]=None, pts: Optional[torch.Tensor]=None, **kwargs) ->Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        Args:
            feats_sampled: A `dict` of sampled feature tensors `{f_i: t_i}`,
                where each `t_i` is a tensor of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
            masks_sampled: A binary mask represented as a tensor of shape
                `(minibatch, n_source_views, n_samples, 1)` denoting valid
                sampled features.
            camera: A batch of `n_source_views` `CamerasBase` objects corresponding
                to the source view cameras.
            pts: A tensor of shape `(minibatch, n_samples, 3)` denoting the
                3D points whose 2D projections to source views were sampled in
                order to generate `feats_sampled` and `masks_sampled`.

        Returns:
            feats_aggregated: If `concatenate_output==True`, a tensor
                of shape `(minibatch, n_source_views, n_samples, sum(dim_1, ... dim_N))`.
                If `concatenate_output==False`, a dictionary `{f_i: t_i_aggregated}`
                with each `t_i_aggregated` of shape
                `(minibatch, n_source_views, n_samples, dim_i)`.
        """
        if camera is None:
            raise ValueError('camera cannot be None for angle weighted aggregation')
        if pts is None:
            raise ValueError('Points cannot be None for angle weighted aggregation')
        pts_batch, n_cameras = masks_sampled.shape[:2]
        if self.exclude_target_view_mask_features:
            feats_sampled = _mask_target_view_features(feats_sampled)
        view_sampling_mask = _get_view_sampling_mask(n_cameras, pts_batch, masks_sampled.device, self.exclude_target_view)
        aggr_weights = _get_angular_reduction_weights(view_sampling_mask, masks_sampled, camera, pts, self.min_ray_angle_weight, self.weight_by_ray_angle_gamma)
        feats_aggregated = {k: (f * aggr_weights[..., None]) for k, f in feats_sampled.items()}
        if self.concatenate_output:
            feats_aggregated = torch.cat(tuple(feats_aggregated.values()), dim=-1)
        return feats_aggregated


class BatchLinear(nn.Module):

    def __init__(self, weights, biases):
        """Implements a batch linear layer.

        :param weights: Shape: (batch, out_ch, in_ch)
        :param biases: Shape: (batch, 1, out_ch)
        """
        super().__init__()
        self.weights = weights
        self.biases = biases

    def __repr__(self):
        return 'BatchLinear(in_ch=%d, out_ch=%d)' % (self.weights.shape[-1], self.weights.shape[-2])

    def forward(self, input):
        output = input.matmul(self.weights.permute(*[i for i in range(len(self.weights.shape) - 2)], -1, -2))
        output += self.biases
        return output


class LookupLinear(nn.Module):

    def __init__(self, in_ch, out_ch, num_objects):
        super().__init__()
        self.in_ch = in_ch
        self.out_ch = out_ch
        self.hypo_params = nn.Embedding(num_objects, in_ch * out_ch + out_ch)
        for i in range(num_objects):
            nn.init.kaiming_normal_(self.hypo_params.weight.data[i, :self.in_ch * self.out_ch].view(self.out_ch, self.in_ch), a=0.0, nonlinearity='relu', mode='fan_in')
            self.hypo_params.weight.data[i, self.in_ch * self.out_ch:].fill_(0.0)

    def forward(self, obj_idx):
        hypo_params = self.hypo_params(obj_idx)
        weights = hypo_params[..., :self.in_ch * self.out_ch]
        biases = hypo_params[..., self.in_ch * self.out_ch:self.in_ch * self.out_ch + self.out_ch]
        biases = biases.view(*biases.size()[:-1], 1, self.out_ch)
        weights = weights.view(*weights.size()[:-1], self.out_ch, self.in_ch)
        return BatchLinear(weights=weights, biases=biases)


class LookupLayer(nn.Module):

    def __init__(self, in_ch, out_ch, num_objects):
        super().__init__()
        self.out_ch = out_ch
        self.lookup_lin = LookupLinear(in_ch, out_ch, num_objects=num_objects)
        self.norm_nl = nn.Sequential(nn.LayerNorm([self.out_ch], elementwise_affine=False), nn.ReLU(inplace=True))

    def forward(self, obj_idx):
        net = nn.Sequential(self.lookup_lin(obj_idx), self.norm_nl)
        return net


class LookupFC(nn.Module):

    def __init__(self, hidden_ch, num_hidden_layers, num_objects, in_ch, out_ch, outermost_linear=False):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(LookupLayer(in_ch=in_ch, out_ch=hidden_ch, num_objects=num_objects))
        for i in range(num_hidden_layers):
            self.layers.append(LookupLayer(in_ch=hidden_ch, out_ch=hidden_ch, num_objects=num_objects))
        if outermost_linear:
            self.layers.append(LookupLinear(in_ch=hidden_ch, out_ch=out_ch, num_objects=num_objects))
        else:
            self.layers.append(LookupLayer(in_ch=hidden_ch, out_ch=out_ch, num_objects=num_objects))

    def forward(self, obj_idx):
        net = []
        for i in range(len(self.layers)):
            net.append(self.layers[i](obj_idx))
        return nn.Sequential(*net)


def last_hyper_layer_init(m) ->None:
    if type(m) == nn.Linear:
        nn.init.kaiming_normal_(m.weight, a=0.0, nonlinearity='relu', mode='fan_in')
        m.weight.data *= 0.1


class HyperLinear(nn.Module):
    """A hypernetwork that predicts a single linear layer (weights & biases)."""

    def __init__(self, in_ch, out_ch, hyper_in_ch, hyper_num_hidden_layers, hyper_hidden_ch):
        super().__init__()
        self.in_ch = in_ch
        self.out_ch = out_ch
        self.hypo_params = pytorch_prototyping.FCBlock(in_features=hyper_in_ch, hidden_ch=hyper_hidden_ch, num_hidden_layers=hyper_num_hidden_layers, out_features=in_ch * out_ch + out_ch, outermost_linear=True)
        self.hypo_params[-1].apply(last_hyper_layer_init)

    def forward(self, hyper_input):
        hypo_params = self.hypo_params(hyper_input)
        weights = hypo_params[..., :self.in_ch * self.out_ch]
        biases = hypo_params[..., self.in_ch * self.out_ch:self.in_ch * self.out_ch + self.out_ch]
        biases = biases.view(*biases.size()[:-1], 1, self.out_ch)
        weights = weights.view(*weights.size()[:-1], self.out_ch, self.in_ch)
        return BatchLinear(weights=weights, biases=biases)


class HyperLayer(nn.Module):
    """A hypernetwork that predicts a single Dense Layer, including LayerNorm and a ReLU."""

    def __init__(self, in_ch, out_ch, hyper_in_ch, hyper_num_hidden_layers, hyper_hidden_ch):
        super().__init__()
        self.hyper_linear = HyperLinear(in_ch=in_ch, out_ch=out_ch, hyper_in_ch=hyper_in_ch, hyper_num_hidden_layers=hyper_num_hidden_layers, hyper_hidden_ch=hyper_hidden_ch)
        self.norm_nl = nn.Sequential(nn.LayerNorm([out_ch], elementwise_affine=False), nn.ReLU(inplace=True))

    def forward(self, hyper_input):
        """
        :param hyper_input: input to hypernetwork.
        :return: nn.Module; predicted fully connected network.
        """
        return nn.Sequential(self.hyper_linear(hyper_input), self.norm_nl)


def partialclass(cls, *args, **kwds):


    class NewCls(cls):
        __init__ = functools.partialmethod(cls.__init__, *args, **kwds)
    return NewCls


class HyperFC(nn.Module):
    """Builds a hypernetwork that predicts a fully connected neural network."""

    def __init__(self, hyper_in_ch, hyper_num_hidden_layers, hyper_hidden_ch, hidden_ch, num_hidden_layers, in_ch, out_ch, outermost_linear=False):
        super().__init__()
        PreconfHyperLinear = partialclass(HyperLinear, hyper_in_ch=hyper_in_ch, hyper_num_hidden_layers=hyper_num_hidden_layers, hyper_hidden_ch=hyper_hidden_ch)
        PreconfHyperLayer = partialclass(HyperLayer, hyper_in_ch=hyper_in_ch, hyper_num_hidden_layers=hyper_num_hidden_layers, hyper_hidden_ch=hyper_hidden_ch)
        self.layers = nn.ModuleList()
        self.layers.append(PreconfHyperLayer(in_ch=in_ch, out_ch=hidden_ch))
        for i in range(num_hidden_layers):
            self.layers.append(PreconfHyperLayer(in_ch=hidden_ch, out_ch=hidden_ch))
        if outermost_linear:
            self.layers.append(PreconfHyperLinear(in_ch=hidden_ch, out_ch=out_ch))
        else:
            self.layers.append(PreconfHyperLayer(in_ch=hidden_ch, out_ch=out_ch))

    def forward(self, hyper_input):
        """
        :param hyper_input: Input to hypernetwork.
        :return: nn.Module; Predicted fully connected neural network.
        """
        net = []
        for i in range(len(self.layers)):
            net.append(self.layers[i](hyper_input))
        return nn.Sequential(*net)


class FCLayer(nn.Module):

    def __init__(self, in_features, out_features):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_features, out_features), nn.LayerNorm([out_features]), nn.ReLU(inplace=True))

    def forward(self, input):
        return self.net(input)


class LayerNormConv2d(nn.Module):

    def __init__(self, num_features, eps=1e-05, affine=True):
        super().__init__()
        self.num_features = num_features
        self.affine = affine
        self.eps = eps
        if self.affine:
            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())
            self.beta = nn.Parameter(torch.zeros(num_features))

    def forward(self, x):
        shape = [-1] + [1] * (x.dim() - 1)
        mean = x.view(x.size(0), -1).mean(1).view(*shape)
        std = x.view(x.size(0), -1).std(1).view(*shape)
        y = (x - mean) / (std + self.eps)
        if self.affine:
            shape = [1, -1] + [1] * (x.dim() - 2)
            y = self.gamma.view(*shape) * y + self.beta.view(*shape)
        return y


class FCBlock(nn.Module):

    def __init__(self, hidden_ch, num_hidden_layers, in_features, out_features, outermost_linear=False):
        super().__init__()
        self.net = []
        self.net.append(FCLayer(in_features=in_features, out_features=hidden_ch))
        for i in range(num_hidden_layers):
            self.net.append(FCLayer(in_features=hidden_ch, out_features=hidden_ch))
        if outermost_linear:
            self.net.append(nn.Linear(in_features=hidden_ch, out_features=out_features))
        else:
            self.net.append(FCLayer(in_features=hidden_ch, out_features=out_features))
        self.net = nn.Sequential(*self.net)
        self.net.apply(self.init_weights)

    def __getitem__(self, item):
        return self.net[item]

    def init_weights(self, m):
        if type(m) == nn.Linear:
            nn.init.kaiming_normal_(m.weight, a=0.0, nonlinearity='relu', mode='fan_in')

    def forward(self, input):
        return self.net(input)


class DownBlock3D(nn.Module):
    """A 3D convolutional downsampling block."""

    def __init__(self, in_channels, out_channels, norm=nn.BatchNorm3d):
        super().__init__()
        self.net = [nn.ReplicationPad3d(1), nn.Conv3d(in_channels, out_channels, kernel_size=4, padding=0, stride=2, bias=False if norm is not None else True)]
        if norm is not None:
            self.net += [norm(out_channels, affine=True)]
        self.net += [nn.LeakyReLU(0.2, True)]
        self.net = nn.Sequential(*self.net)

    def forward(self, x):
        return self.net(x)


class UpBlock3D(nn.Module):
    """A 3D convolutional upsampling block."""

    def __init__(self, in_channels, out_channels, norm=nn.BatchNorm3d):
        super().__init__()
        self.net = [nn.ConvTranspose3d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False if norm is not None else True)]
        if norm is not None:
            self.net += [norm(out_channels, affine=True)]
        self.net += [nn.ReLU(True)]
        self.net = nn.Sequential(*self.net)

    def forward(self, x, skipped=None):
        if skipped is not None:
            input = torch.cat([skipped, x], dim=1)
        else:
            input = x
        return self.net(input)


class Conv3dSame(torch.nn.Module):
    """3D convolution that pads to keep spatial dimensions equal.
    Cannot deal with stride. Only quadratic kernels (=scalar kernel_size).
    """

    def __init__(self, in_channels, out_channels, kernel_size, bias=True, padding_layer=nn.ReplicationPad3d):
        """
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param kernel_size: Scalar. Spatial dimensions of kernel (only quadratic kernels supported).
        :param bias: Whether or not to use bias.
        :param padding_layer: Which padding to use. Default is reflection padding.
        """
        super().__init__()
        ka = kernel_size // 2
        kb = ka - 1 if kernel_size % 2 == 0 else ka
        self.net = nn.Sequential(padding_layer((ka, kb, ka, kb, ka, kb)), nn.Conv3d(in_channels, out_channels, kernel_size, bias=bias, stride=1))

    def forward(self, x):
        return self.net(x)


class Conv2dSame(torch.nn.Module):
    """2D convolution that pads to keep spatial dimensions equal.
    Cannot deal with stride. Only quadratic kernels (=scalar kernel_size).
    """

    def __init__(self, in_channels, out_channels, kernel_size, bias=True, padding_layer=nn.ReflectionPad2d):
        """
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param kernel_size: Scalar. Spatial dimensions of kernel (only quadratic kernels supported).
        :param bias: Whether or not to use bias.
        :param padding_layer: Which padding to use. Default is reflection padding.
        """
        super().__init__()
        ka = kernel_size // 2
        kb = ka - 1 if kernel_size % 2 == 0 else ka
        self.net = nn.Sequential(padding_layer((ka, kb, ka, kb)), nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias, stride=1))
        self.weight = self.net[1].weight
        self.bias = self.net[1].bias

    def forward(self, x):
        return self.net(x)


class UpBlock(nn.Module):
    """A 2d-conv upsampling block with a variety of options for upsampling, and following best practices / with
    reasonable defaults. (LeakyReLU, kernel size multiple of stride)
    """

    def __init__(self, in_channels, out_channels, post_conv=True, use_dropout=False, dropout_prob=0.1, norm=nn.BatchNorm2d, upsampling_mode='transpose'):
        """
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param post_conv: Whether to have another convolutional layer after the upsampling layer.
        :param use_dropout: bool. Whether to use dropout or not.
        :param dropout_prob: Float. The dropout probability (if use_dropout is True)
        :param norm: Which norm to use. If None, no norm is used. Default is Batchnorm with affinity.
        :param upsampling_mode: Which upsampling mode:
                transpose: Upsampling with stride-2, kernel size 4 transpose convolutions.
                bilinear: Feature map is upsampled with bilinear upsampling, then a conv layer.
                nearest: Feature map is upsampled with nearest neighbor upsampling, then a conv layer.
                shuffle: Feature map is upsampled with pixel shuffling, then a conv layer.
        """
        super().__init__()
        net = list()
        if upsampling_mode == 'transpose':
            net += [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=True if norm is None else False)]
        elif upsampling_mode == 'bilinear':
            net += [nn.UpsamplingBilinear2d(scale_factor=2)]
            net += [Conv2dSame(in_channels, out_channels, kernel_size=3, bias=True if norm is None else False)]
        elif upsampling_mode == 'nearest':
            net += [nn.UpsamplingNearest2d(scale_factor=2)]
            net += [Conv2dSame(in_channels, out_channels, kernel_size=3, bias=True if norm is None else False)]
        elif upsampling_mode == 'shuffle':
            net += [nn.PixelShuffle(upscale_factor=2)]
            net += [Conv2dSame(in_channels // 4, out_channels, kernel_size=3, bias=True if norm is None else False)]
        else:
            raise ValueError('Unknown upsampling mode!')
        if norm is not None:
            net += [norm(out_channels, affine=True)]
        net += [nn.ReLU(True)]
        if use_dropout:
            net += [nn.Dropout2d(dropout_prob, False)]
        if post_conv:
            net += [Conv2dSame(out_channels, out_channels, kernel_size=3, bias=True if norm is None else False)]
            if norm is not None:
                net += [norm(out_channels, affine=True)]
            net += [nn.ReLU(True)]
            if use_dropout:
                net += [nn.Dropout2d(0.1, False)]
        self.net = nn.Sequential(*net)

    def forward(self, x, skipped=None):
        if skipped is not None:
            input = torch.cat([skipped, x], dim=1)
        else:
            input = x
        return self.net(input)


class DownBlock(nn.Module):
    """A 2D-conv downsampling block following best practices / with reasonable defaults
    (LeakyReLU, kernel size multiple of stride)
    """

    def __init__(self, in_channels, out_channels, prep_conv=True, middle_channels=None, use_dropout=False, dropout_prob=0.1, norm=nn.BatchNorm2d):
        """
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param prep_conv: Whether to have another convolutional layer before the downsampling layer.
        :param middle_channels: If prep_conv is true, this sets the number of channels between the prep and downsampling
                                convs.
        :param use_dropout: bool. Whether to use dropout or not.
        :param dropout_prob: Float. The dropout probability (if use_dropout is True)
        :param norm: Which norm to use. If None, no norm is used. Default is Batchnorm with affinity.
        """
        super().__init__()
        if middle_channels is None:
            middle_channels = in_channels
        net = list()
        if prep_conv:
            net += [nn.ReflectionPad2d(1), nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=0, stride=1, bias=True if norm is None else False)]
            if norm is not None:
                net += [norm(middle_channels, affine=True)]
            net += [nn.LeakyReLU(0.2, True)]
            if use_dropout:
                net += [nn.Dropout2d(dropout_prob, False)]
        net += [nn.ReflectionPad2d(1), nn.Conv2d(middle_channels, out_channels, kernel_size=4, padding=0, stride=2, bias=True if norm is None else False)]
        if norm is not None:
            net += [norm(out_channels, affine=True)]
        net += [nn.LeakyReLU(0.2, True)]
        if use_dropout:
            net += [nn.Dropout2d(dropout_prob, False)]
        self.net = nn.Sequential(*net)

    def forward(self, x):
        return self.net(x)


class UnetSkipConnectionBlock3d(nn.Module):
    """Helper class for building a 3D unet."""

    def __init__(self, outer_nc, inner_nc, norm=nn.BatchNorm3d, submodule=None):
        super().__init__()
        if submodule is None:
            model = [DownBlock3D(outer_nc, inner_nc, norm=norm), UpBlock3D(inner_nc, outer_nc, norm=norm)]
        else:
            model = [DownBlock3D(outer_nc, inner_nc, norm=norm), submodule, UpBlock3D(2 * inner_nc, outer_nc, norm=norm)]
        self.model = nn.Sequential(*model)

    def forward(self, x):
        forward_passed = self.model(x)
        return torch.cat([x, forward_passed], 1)


class Unet3d(nn.Module):
    """A 3d-Unet implementation with sane defaults."""

    def __init__(self, in_channels, out_channels, nf0, num_down, max_channels, norm=nn.BatchNorm3d, outermost_linear=False):
        """
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param nf0: Number of features at highest level of U-Net
        :param num_down: Number of downsampling stages.
        :param max_channels: Maximum number of channels (channels multiply by 2 with every downsampling stage)
        :param norm: Which norm to use. If None, no norm is used. Default is Batchnorm with affinity.
        :param outermost_linear: Whether the output layer should be a linear layer or a nonlinear one.
        """
        super().__init__()
        assert num_down > 0, 'Need at least one downsampling layer in UNet3d.'
        self.in_layer = [Conv3dSame(in_channels, nf0, kernel_size=3, bias=False)]
        if norm is not None:
            self.in_layer += [norm(nf0, affine=True)]
        self.in_layer += [nn.LeakyReLU(0.2, True)]
        self.in_layer = nn.Sequential(*self.in_layer)
        self.unet_block = UnetSkipConnectionBlock3d(int(min(2 ** (num_down - 1) * nf0, max_channels)), int(min(2 ** (num_down - 1) * nf0, max_channels)), norm=None)
        for i in list(range(0, num_down - 1))[::-1]:
            self.unet_block = UnetSkipConnectionBlock3d(int(min(2 ** i * nf0, max_channels)), int(min(2 ** (i + 1) * nf0, max_channels)), submodule=self.unet_block, norm=norm)
        self.out_layer = [Conv3dSame(2 * nf0, out_channels, kernel_size=3, bias=outermost_linear)]
        if not outermost_linear:
            if norm is not None:
                self.out_layer += [norm(out_channels, affine=True)]
            self.out_layer += [nn.ReLU(True)]
        self.out_layer = nn.Sequential(*self.out_layer)

    def forward(self, x):
        in_layer = self.in_layer(x)
        unet = self.unet_block(in_layer)
        out_layer = self.out_layer(unet)
        return out_layer


class UnetSkipConnectionBlock(nn.Module):
    """Helper class for building a 2D unet."""

    def __init__(self, outer_nc, inner_nc, upsampling_mode, norm=nn.BatchNorm2d, submodule=None, use_dropout=False, dropout_prob=0.1):
        super().__init__()
        if submodule is None:
            model = [DownBlock(outer_nc, inner_nc, use_dropout=use_dropout, dropout_prob=dropout_prob, norm=norm), UpBlock(inner_nc, outer_nc, use_dropout=use_dropout, dropout_prob=dropout_prob, norm=norm, upsampling_mode=upsampling_mode)]
        else:
            model = [DownBlock(outer_nc, inner_nc, use_dropout=use_dropout, dropout_prob=dropout_prob, norm=norm), submodule, UpBlock(2 * inner_nc, outer_nc, use_dropout=use_dropout, dropout_prob=dropout_prob, norm=norm, upsampling_mode=upsampling_mode)]
        self.model = nn.Sequential(*model)

    def forward(self, x):
        forward_passed = self.model(x)
        return torch.cat([x, forward_passed], 1)


class Unet(nn.Module):
    """A 2d-Unet implementation with sane defaults."""

    def __init__(self, in_channels, out_channels, nf0, num_down, max_channels, use_dropout, upsampling_mode='transpose', dropout_prob=0.1, norm=nn.BatchNorm2d, outermost_linear=False):
        """
        :param in_channels: Number of input channels
        :param out_channels: Number of output channels
        :param nf0: Number of features at highest level of U-Net
        :param num_down: Number of downsampling stages.
        :param max_channels: Maximum number of channels (channels multiply by 2 with every downsampling stage)
        :param use_dropout: Whether to use dropout or no.
        :param dropout_prob: Dropout probability if use_dropout=True.
        :param upsampling_mode: Which type of upsampling should be used. See "UpBlock" for documentation.
        :param norm: Which norm to use. If None, no norm is used. Default is Batchnorm with affinity.
        :param outermost_linear: Whether the output layer should be a linear layer or a nonlinear one.
        """
        super().__init__()
        assert num_down > 0, 'Need at least one downsampling layer in UNet.'
        self.in_layer = [Conv2dSame(in_channels, nf0, kernel_size=3, bias=True if norm is None else False)]
        if norm is not None:
            self.in_layer += [norm(nf0, affine=True)]
        self.in_layer += [nn.LeakyReLU(0.2, True)]
        if use_dropout:
            self.in_layer += [nn.Dropout2d(dropout_prob)]
        self.in_layer = nn.Sequential(*self.in_layer)
        self.unet_block = UnetSkipConnectionBlock(min(2 ** (num_down - 1) * nf0, max_channels), min(2 ** (num_down - 1) * nf0, max_channels), use_dropout=use_dropout, dropout_prob=dropout_prob, norm=None, upsampling_mode=upsampling_mode)
        for i in list(range(0, num_down - 1))[::-1]:
            self.unet_block = UnetSkipConnectionBlock(min(2 ** i * nf0, max_channels), min(2 ** (i + 1) * nf0, max_channels), use_dropout=use_dropout, dropout_prob=dropout_prob, submodule=self.unet_block, norm=norm, upsampling_mode=upsampling_mode)
        self.out_layer = [Conv2dSame(2 * nf0, out_channels, kernel_size=3, bias=outermost_linear or norm is None)]
        if not outermost_linear:
            if norm is not None:
                self.out_layer += [norm(out_channels, affine=True)]
            self.out_layer += [nn.ReLU(True)]
            if use_dropout:
                self.out_layer += [nn.Dropout2d(dropout_prob)]
        self.out_layer = nn.Sequential(*self.out_layer)
        self.out_layer_weight = self.out_layer[0].weight

    def forward(self, x):
        in_layer = self.in_layer(x)
        unet = self.unet_block(in_layer)
        out_layer = self.out_layer(unet)
        return out_layer


class Identity(nn.Module):
    """Helper module to allow Downsampling and Upsampling nets to default to identity if they receive an empty list."""

    def __init__(self):
        super().__init__()

    def forward(self, input):
        return input


class DownsamplingNet(nn.Module):
    """A subnetwork that downsamples a 2D feature map with strided convolutions."""

    def __init__(self, per_layer_out_ch, in_channels, use_dropout, dropout_prob=0.1, last_layer_one=False, norm=nn.BatchNorm2d):
        """
        :param per_layer_out_ch: python list of integers. Defines the number of output channels per layer. Length of
                                list defines number of downsampling steps (each step dowsamples by factor of 2.)
        :param in_channels: Number of input channels.
        :param use_dropout: Whether or not to use dropout.
        :param dropout_prob: Dropout probability.
        :param last_layer_one: Whether the output of the last layer will have a spatial size of 1. In that case,
                               the last layer will not have batchnorm, else, it will.
        :param norm: Which norm to use. Defaults to BatchNorm.
        """
        super().__init__()
        if not len(per_layer_out_ch):
            self.downs = Identity()
        else:
            self.downs = list()
            self.downs.append(DownBlock(in_channels, per_layer_out_ch[0], use_dropout=use_dropout, dropout_prob=dropout_prob, middle_channels=per_layer_out_ch[0], norm=norm))
            for i in range(0, len(per_layer_out_ch) - 1):
                if last_layer_one and i == len(per_layer_out_ch) - 2:
                    norm = None
                self.downs.append(DownBlock(per_layer_out_ch[i], per_layer_out_ch[i + 1], dropout_prob=dropout_prob, use_dropout=use_dropout, norm=norm))
            self.downs = nn.Sequential(*self.downs)

    def forward(self, input):
        return self.downs(input)


class UpsamplingNet(nn.Module):
    """A subnetwork that upsamples a 2D feature map with a variety of upsampling options."""

    def __init__(self, per_layer_out_ch, in_channels, upsampling_mode, use_dropout, dropout_prob=0.1, first_layer_one=False, norm=nn.BatchNorm2d):
        """
        :param per_layer_out_ch: python list of integers. Defines the number of output channels per layer. Length of
                                list defines number of upsampling steps (each step upsamples by factor of 2.)
        :param in_channels: Number of input channels.
        :param upsampling_mode: Mode of upsampling. For documentation, see class "UpBlock"
        :param use_dropout: Whether or not to use dropout.
        :param dropout_prob: Dropout probability.
        :param first_layer_one: Whether the input to the last layer will have a spatial size of 1. In that case,
                               the first layer will not have a norm, else, it will.
        :param norm: Which norm to use. Defaults to BatchNorm.
        """
        super().__init__()
        if not len(per_layer_out_ch):
            self.ups = Identity()
        else:
            self.ups = list()
            self.ups.append(UpBlock(in_channels, per_layer_out_ch[0], use_dropout=use_dropout, dropout_prob=dropout_prob, norm=None if first_layer_one else norm, upsampling_mode=upsampling_mode))
            for i in range(0, len(per_layer_out_ch) - 1):
                self.ups.append(UpBlock(per_layer_out_ch[i], per_layer_out_ch[i + 1], use_dropout=use_dropout, dropout_prob=dropout_prob, norm=norm, upsampling_mode=upsampling_mode))
            self.ups = nn.Sequential(*self.ups)

    def forward(self, input):
        return self.ups(input)


class GatherScatter(Function):
    """
    Torch autograd Function wrapper for gather_scatter C++/CUDA implementations.
    """

    @staticmethod
    def forward(ctx, input, edges, directed=False):
        """
        Args:
            ctx: Context object used to calculate gradients.
            input: Tensor of shape (num_vertices, input_dim)
            edges: Tensor of edge indices of shape (num_edges, 2)
            directed: Bool indicating if edges are directed.

        Returns:
            output: Tensor of same shape as input.
        """
        if not input.dim() == 2:
            raise ValueError('input can only have 2 dimensions.')
        if not edges.dim() == 2:
            raise ValueError('edges can only have 2 dimensions.')
        if not edges.shape[1] == 2:
            raise ValueError('edges must be of shape (num_edges, 2).')
        if not input.dtype == torch.float32:
            raise ValueError('input has to be of type torch.float32.')
        ctx.directed = directed
        input, edges = input.contiguous(), edges.contiguous()
        ctx.save_for_backward(edges)
        backward = False
        output = _C.gather_scatter(input, edges, directed, backward)
        return output

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        grad_output = grad_output.contiguous()
        edges = ctx.saved_tensors[0]
        directed = ctx.directed
        backward = True
        grad_input = _C.gather_scatter(grad_output, edges, directed, backward)
        grad_edges = None
        grad_directed = None
        return grad_input, grad_edges, grad_directed


gather_scatter = GatherScatter.apply


def gather_scatter_python(input, edges, directed: bool=False):
    """
    Python implementation of gather_scatter for aggregating features of
    neighbor nodes in a graph.

    Given a directed graph: v0 -> v1 -> v2 the updated feature for v1 depends
    on v2 in order to be consistent with Morris et al. AAAI 2019
    (https://arxiv.org/abs/1810.02244). This only affects
    directed graphs; for undirected graphs v1 will depend on both v0 and v2,
    no matter which way the edges are physically stored.

    Args:
        input: Tensor of shape (num_vertices, input_dim).
        edges: Tensor of edge indices of shape (num_edges, 2).
        directed: bool indicating if edges are directed.

    Returns:
        output: Tensor of same shape as input.
    """
    if not input.dim() == 2:
        raise ValueError('input can only have 2 dimensions.')
    if not edges.dim() == 2:
        raise ValueError('edges can only have 2 dimensions.')
    if not edges.shape[1] == 2:
        raise ValueError('edges must be of shape (num_edges, 2).')
    num_vertices, input_feature_dim = input.shape
    num_edges = edges.shape[0]
    output = torch.zeros_like(input)
    idx0 = edges[:, 0].view(num_edges, 1).expand(num_edges, input_feature_dim)
    idx1 = edges[:, 1].view(num_edges, 1).expand(num_edges, input_feature_dim)
    output = output.scatter_add(0, idx0, input.gather(0, idx1))
    if not directed:
        output = output.scatter_add(0, idx1, input.gather(0, idx0))
    return output


class GraphConv(nn.Module):
    """A single graph convolution layer."""

    def __init__(self, input_dim: int, output_dim: int, init: str='normal', directed: bool=False) ->None:
        """
        Args:
            input_dim: Number of input features per vertex.
            output_dim: Number of output features per vertex.
            init: Weight initialization method. Can be one of ['zero', 'normal'].
            directed: Bool indicating if edges in the graph are directed.
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.directed = directed
        self.w0 = nn.Linear(input_dim, output_dim)
        self.w1 = nn.Linear(input_dim, output_dim)
        if init == 'normal':
            nn.init.normal_(self.w0.weight, mean=0, std=0.01)
            nn.init.normal_(self.w1.weight, mean=0, std=0.01)
            self.w0.bias.data.zero_()
            self.w1.bias.data.zero_()
        elif init == 'zero':
            self.w0.weight.data.zero_()
            self.w1.weight.data.zero_()
        else:
            raise ValueError('Invalid GraphConv initialization "%s"' % init)

    def forward(self, verts, edges):
        """
        Args:
            verts: FloatTensor of shape (V, input_dim) where V is the number of
                vertices and input_dim is the number of input features
                per vertex. input_dim has to match the input_dim specified
                in __init__.
            edges: LongTensor of shape (E, 2) where E is the number of edges
                where each edge has the indices of the two vertices which
                form the edge.

        Returns:
            out: FloatTensor of shape (V, output_dim) where output_dim is the
            number of output features per vertex.
        """
        if verts.is_cuda != edges.is_cuda:
            raise ValueError('verts and edges tensors must be on the same device.')
        if verts.shape[0] == 0:
            return verts.new_zeros((0, self.output_dim)) * verts.sum()
        verts_w0 = self.w0(verts)
        verts_w1 = self.w1(verts)
        if torch.cuda.is_available() and verts.is_cuda and edges.is_cuda:
            neighbor_sums = gather_scatter(verts_w1, edges, self.directed)
        else:
            neighbor_sums = gather_scatter_python(verts_w1, edges, self.directed)
        out = verts_w0 + neighbor_sums
        return out

    def __repr__(self):
        Din, Dout, directed = self.input_dim, self.output_dim, self.directed
        return 'GraphConv(%d -> %d, directed=%r)' % (Din, Dout, directed)


class Meshes:
    """
    This class provides functions for working with batches of triangulated
    meshes with varying numbers of faces and vertices, and converting between
    representations.

    Within Meshes, there are three different representations of the faces and
    verts data:

    List
      - only used for input as a starting point to convert to other representations.
    Padded
      - has specific batch dimension.
    Packed
      - no batch dimension.
      - has auxiliary variables used to index into the padded representation.

    Example:

    Input list of verts V_n = [[V_1], [V_2], ... , [V_N]]
    where V_1, ... , V_N are the number of verts in each mesh and N is the
    number of meshes.

    Input list of faces F_n = [[F_1], [F_2], ... , [F_N]]
    where F_1, ... , F_N are the number of faces in each mesh.

    # SPHINX IGNORE
     List                      | Padded                  | Packed
    ---------------------------|-------------------------|------------------------
    [[V_1], ... , [V_N]]       | size = (N, max(V_n), 3) |  size = (sum(V_n), 3)
                               |                         |
    Example for verts:         |                         |
                               |                         |
    V_1 = 3, V_2 = 4, V_3 = 5  | size = (3, 5, 3)        |  size = (12, 3)
                               |                         |
    List([                     | tensor([                |  tensor([
      [                        |     [                   |    [0.1, 0.3, 0.5],
        [0.1, 0.3, 0.5],       |       [0.1, 0.3, 0.5],  |    [0.5, 0.2, 0.1],
        [0.5, 0.2, 0.1],       |       [0.5, 0.2, 0.1],  |    [0.6, 0.8, 0.7],
        [0.6, 0.8, 0.7],       |       [0.6, 0.8, 0.7],  |    [0.1, 0.3, 0.3],
      ],                       |       [0,    0,    0],  |    [0.6, 0.7, 0.8],
      [                        |       [0,    0,    0],  |    [0.2, 0.3, 0.4],
        [0.1, 0.3, 0.3],       |     ],                  |    [0.1, 0.5, 0.3],
        [0.6, 0.7, 0.8],       |     [                   |    [0.7, 0.3, 0.6],
        [0.2, 0.3, 0.4],       |       [0.1, 0.3, 0.3],  |    [0.2, 0.4, 0.8],
        [0.1, 0.5, 0.3],       |       [0.6, 0.7, 0.8],  |    [0.9, 0.5, 0.2],
      ],                       |       [0.2, 0.3, 0.4],  |    [0.2, 0.3, 0.4],
      [                        |       [0.1, 0.5, 0.3],  |    [0.9, 0.3, 0.8],
        [0.7, 0.3, 0.6],       |       [0,    0,    0],  |  ])
        [0.2, 0.4, 0.8],       |     ],                  |
        [0.9, 0.5, 0.2],       |     [                   |
        [0.2, 0.3, 0.4],       |       [0.7, 0.3, 0.6],  |
        [0.9, 0.3, 0.8],       |       [0.2, 0.4, 0.8],  |
      ]                        |       [0.9, 0.5, 0.2],  |
    ])                         |       [0.2, 0.3, 0.4],  |
                               |       [0.9, 0.3, 0.8],  |
                               |     ]                   |
                               |  ])                     |
    Example for faces:         |                         |
                               |                         |
    F_1 = 1, F_2 = 2, F_3 = 7  | size = (3, 7, 3)        | size = (10, 3)
                               |                         |
    List([                     | tensor([                | tensor([
      [                        |     [                   |    [ 0,  1,  2],
        [0, 1, 2],             |       [0,   1,  2],     |    [ 3,  4,  5],
      ],                       |       [-1, -1, -1],     |    [ 4,  5,  6],
      [                        |       [-1, -1, -1]      |    [ 8,  9,  7],
        [0, 1, 2],             |       [-1, -1, -1]      |    [ 7,  8, 10],
        [1, 2, 3],             |       [-1, -1, -1]      |    [ 9, 10,  8],
      ],                       |       [-1, -1, -1],     |    [11, 10,  9],
      [                        |       [-1, -1, -1],     |    [11,  7,  8],
        [1, 2, 0],             |     ],                  |    [11, 10,  8],
        [0, 1, 3],             |     [                   |    [11,  9,  8],
        [2, 3, 1],             |       [0,   1,  2],     |  ])
        [4, 3, 2],             |       [1,   2,  3],     |
        [4, 0, 1],             |       [-1, -1, -1],     |
        [4, 3, 1],             |       [-1, -1, -1],     |
        [4, 2, 1],             |       [-1, -1, -1],     |
      ],                       |       [-1, -1, -1],     |
    ])                         |       [-1, -1, -1],     |
                               |     ],                  |
                               |     [                   |
                               |       [1,   2,  0],     |
                               |       [0,   1,  3],     |
                               |       [2,   3,  1],     |
                               |       [4,   3,  2],     |
                               |       [4,   0,  1],     |
                               |       [4,   3,  1],     |
                               |       [4,   2,  1],     |
                               |     ]                   |
                               |   ])                    |
    -----------------------------------------------------------------------------

    Auxiliary variables for packed representation

    Name                           |   Size              |  Example from above
    -------------------------------|---------------------|-----------------------
                                   |                     |
    verts_packed_to_mesh_idx       |  size = (sum(V_n))  |   tensor([
                                   |                     |     0, 0, 0, 1, 1, 1,
                                   |                     |     1, 2, 2, 2, 2, 2
                                   |                     |   )]
                                   |                     |   size = (12)
                                   |                     |
    mesh_to_verts_packed_first_idx |  size = (N)         |   tensor([0, 3, 7])
                                   |                     |   size = (3)
                                   |                     |
    num_verts_per_mesh             |  size = (N)         |   tensor([3, 4, 5])
                                   |                     |   size = (3)
                                   |                     |
    faces_packed_to_mesh_idx       |  size = (sum(F_n))  |   tensor([
                                   |                     |     0, 1, 1, 2, 2, 2,
                                   |                     |     2, 2, 2, 2
                                   |                     |   )]
                                   |                     |   size = (10)
                                   |                     |
    mesh_to_faces_packed_first_idx |  size = (N)         |   tensor([0, 1, 3])
                                   |                     |   size = (3)
                                   |                     |
    num_faces_per_mesh             |  size = (N)         |   tensor([1, 2, 7])
                                   |                     |   size = (3)
                                   |                     |
    verts_padded_to_packed_idx     |  size = (sum(V_n))  |  tensor([
                                   |                     |     0, 1, 2, 5, 6, 7,
                                   |                     |     8, 10, 11, 12, 13,
                                   |                     |     14
                                   |                     |  )]
                                   |                     |  size = (12)
    -----------------------------------------------------------------------------
    # SPHINX IGNORE

    From the faces, edges are computed and have packed and padded
    representations with auxiliary variables.

    E_n = [[E_1], ... , [E_N]]
    where E_1, ... , E_N are the number of unique edges in each mesh.
    Total number of unique edges = sum(E_n)

    # SPHINX IGNORE
    Name                           |   Size                  | Example from above
    -------------------------------|-------------------------|----------------------
                                   |                         |
    edges_packed                   | size = (sum(E_n), 2)    |  tensor([
                                   |                         |     [0, 1],
                                   |                         |     [0, 2],
                                   |                         |     [1, 2],
                                   |                         |       ...
                                   |                         |     [10, 11],
                                   |                         |   )]
                                   |                         |   size = (18, 2)
                                   |                         |
    num_edges_per_mesh             | size = (N)              |  tensor([3, 5, 10])
                                   |                         |  size = (3)
                                   |                         |
    edges_packed_to_mesh_idx       | size = (sum(E_n))       |  tensor([
                                   |                         |    0, 0, 0,
                                   |                         |     . . .
                                   |                         |    2, 2, 2
                                   |                         |   ])
                                   |                         |   size = (18)
                                   |                         |
    faces_packed_to_edges_packed   | size = (sum(F_n), 3)    |  tensor([
                                   |                         |    [2,   1,  0],
                                   |                         |    [5,   4,  3],
                                   |                         |       .  .  .
                                   |                         |    [12, 14, 16],
                                   |                         |   ])
                                   |                         |   size = (10, 3)
                                   |                         |
    mesh_to_edges_packed_first_idx | size = (N)              |  tensor([0, 3, 8])
                                   |                         |  size = (3)
    ----------------------------------------------------------------------------
    # SPHINX IGNORE
    """
    _INTERNAL_TENSORS = ['_verts_packed', '_verts_packed_to_mesh_idx', '_mesh_to_verts_packed_first_idx', '_verts_padded', '_num_verts_per_mesh', '_faces_packed', '_faces_packed_to_mesh_idx', '_mesh_to_faces_packed_first_idx', '_faces_padded', '_faces_areas_packed', '_verts_normals_packed', '_faces_normals_packed', '_num_faces_per_mesh', '_edges_packed', '_edges_packed_to_mesh_idx', '_mesh_to_edges_packed_first_idx', '_faces_packed_to_edges_packed', '_num_edges_per_mesh', '_verts_padded_to_packed_idx', '_laplacian_packed', 'valid', 'equisized']

    def __init__(self, verts, faces, textures=None, *, verts_normals=None) ->None:
        """
        Args:
            verts:
                Can be either

                - List where each element is a tensor of shape (num_verts, 3)
                  containing the (x, y, z) coordinates of each vertex.
                - Padded float tensor with shape (num_meshes, max_num_verts, 3).
                  Meshes should be padded with fill value of 0 so they all have
                  the same number of vertices.
            faces:
                Can be either

                - List where each element is a tensor of shape (num_faces, 3)
                  containing the indices of the 3 vertices in the corresponding
                  mesh in verts which form the triangular face.
                - Padded long tensor of shape (num_meshes, max_num_faces, 3).
                  Meshes should be padded with fill value of -1 so they have
                  the same number of faces.
            textures: Optional instance of the Textures class with mesh
                texture properties.
            verts_normals:
                Optional. Can be either

                - List where each element is a tensor of shape (num_verts, 3)
                  containing the normals of each vertex.
                - Padded float tensor with shape (num_meshes, max_num_verts, 3).
                  They should be padded with fill value of 0 so they all have
                  the same number of vertices.
                Note that modifying the mesh later, e.g. with offset_verts_,
                can cause these normals to be forgotten and normals to be recalculated
                based on the new vertex positions.

        Refer to comments above for descriptions of List and Padded representations.
        """
        self.device = torch.device('cpu')
        if textures is not None and not hasattr(textures, 'sample_textures'):
            msg = 'Expected textures to be an instance of type TexturesBase; got %r'
            raise ValueError(msg % type(textures))
        self.textures = textures
        self.equisized = False
        self.valid = None
        self._N = 0
        self._V = 0
        self._F = 0
        self._verts_list = None
        self._faces_list = None
        self._verts_packed = None
        self._verts_packed_to_mesh_idx = None
        self._verts_padded_to_packed_idx = None
        self._mesh_to_verts_packed_first_idx = None
        self._faces_packed = None
        self._faces_packed_to_mesh_idx = None
        self._mesh_to_faces_packed_first_idx = None
        self._edges_packed = None
        self._edges_packed_to_mesh_idx = None
        self._num_edges_per_mesh = None
        self._mesh_to_edges_packed_first_idx = None
        self._faces_packed_to_edges_packed = None
        self._verts_padded = None
        self._num_verts_per_mesh = None
        self._faces_padded = None
        self._num_faces_per_mesh = None
        self._faces_areas_packed = None
        self._verts_normals_packed = None
        self._faces_normals_packed = None
        self._laplacian_packed = None
        if isinstance(verts, list) and isinstance(faces, list):
            self._verts_list = verts
            self._faces_list = [(f[f.gt(-1).all(1)] if len(f) > 0 else f) for f in faces]
            self._N = len(self._verts_list)
            self.valid = torch.zeros((self._N,), dtype=torch.bool, device=self.device)
            if self._N > 0:
                self.device = self._verts_list[0].device
                if not (all(v.device == self.device for v in verts) and all(f.device == self.device for f in faces)):
                    raise ValueError('All Verts and Faces tensors should be on same device.')
                self._num_verts_per_mesh = torch.tensor([len(v) for v in self._verts_list], device=self.device)
                self._V = int(self._num_verts_per_mesh.max())
                self._num_faces_per_mesh = torch.tensor([len(f) for f in self._faces_list], device=self.device)
                self._F = int(self._num_faces_per_mesh.max())
                self.valid = torch.tensor([(len(v) > 0 and len(f) > 0) for v, f in zip(self._verts_list, self._faces_list)], dtype=torch.bool, device=self.device)
                if len(self._num_verts_per_mesh.unique()) == 1 and len(self._num_faces_per_mesh.unique()) == 1:
                    self.equisized = True
        elif torch.is_tensor(verts) and torch.is_tensor(faces):
            if verts.size(2) != 3 or faces.size(2) != 3:
                raise ValueError('Verts or Faces tensors have incorrect dimensions.')
            self._verts_padded = verts
            self._faces_padded = faces
            self._N = self._verts_padded.shape[0]
            self._V = self._verts_padded.shape[1]
            if verts.device != faces.device:
                msg = 'Verts and Faces tensors should be on same device. \n Got {} and {}.'
                raise ValueError(msg.format(verts.device, faces.device))
            self.device = self._verts_padded.device
            self.valid = torch.zeros((self._N,), dtype=torch.bool, device=self.device)
            if self._N > 0:
                faces_not_padded = self._faces_padded.gt(-1).all(2)
                self._num_faces_per_mesh = faces_not_padded.sum(1)
                if (faces_not_padded[:, :-1] < faces_not_padded[:, 1:]).any():
                    raise ValueError('Padding of faces must be at the end')
                self.valid = self._num_faces_per_mesh > 0
                self._F = int(self._num_faces_per_mesh.max())
                if len(self._num_faces_per_mesh.unique()) == 1:
                    self.equisized = True
                self._num_verts_per_mesh = torch.full(size=(self._N,), fill_value=self._V, dtype=torch.int64, device=self.device)
        else:
            raise ValueError('Verts and Faces must be either a list or a tensor with                     shape (batch_size, N, 3) where N is either the maximum                        number of verts or faces respectively.')
        if self.isempty():
            self._num_verts_per_mesh = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._num_faces_per_mesh = torch.zeros((0,), dtype=torch.int64, device=self.device)
        if textures is not None:
            shape_ok = self.textures.check_shapes(self._N, self._V, self._F)
            if not shape_ok:
                msg = 'Textures do not match the dimensions of Meshes.'
                raise ValueError(msg)
            self.textures._num_faces_per_mesh = self._num_faces_per_mesh.tolist()
            self.textures._num_verts_per_mesh = self._num_verts_per_mesh.tolist()
            self.textures.valid = self.valid
        if verts_normals is not None:
            self._set_verts_normals(verts_normals)

    def _set_verts_normals(self, verts_normals) ->None:
        if isinstance(verts_normals, list):
            if len(verts_normals) != self._N:
                raise ValueError('Invalid verts_normals input')
            for item, n_verts in zip(verts_normals, self._num_verts_per_mesh):
                if not isinstance(item, torch.Tensor) or item.ndim != 2 or item.shape[1] != 3 or item.shape[0] != n_verts:
                    raise ValueError('Invalid verts_normals input')
            self._verts_normals_packed = torch.cat(verts_normals, 0)
        elif torch.is_tensor(verts_normals):
            if verts_normals.ndim != 3 or verts_normals.size(2) != 3 or verts_normals.size(0) != self._N:
                raise ValueError('Vertex normals tensor has incorrect dimensions.')
            self._verts_normals_packed = struct_utils.padded_to_packed(verts_normals, split_size=self._num_verts_per_mesh.tolist())
        else:
            raise ValueError('verts_normals must be a list or tensor')

    def __len__(self) ->int:
        return self._N

    def __getitem__(self, index: Union[int, List[int], slice, torch.BoolTensor, torch.LongTensor]) ->'Meshes':
        """
        Args:
            index: Specifying the index of the mesh to retrieve.
                Can be an int, slice, list of ints or a boolean tensor.

        Returns:
            Meshes object with selected meshes. The mesh tensors are not cloned.
        """
        if isinstance(index, (int, slice)):
            verts = self.verts_list()[index]
            faces = self.faces_list()[index]
        elif isinstance(index, list):
            verts = [self.verts_list()[i] for i in index]
            faces = [self.faces_list()[i] for i in index]
        elif isinstance(index, torch.Tensor):
            if index.dim() != 1 or index.dtype.is_floating_point:
                raise IndexError(index)
            if index.dtype == torch.bool:
                index = index.nonzero()
                index = index.squeeze(1) if index.numel() > 0 else index
                index = index.tolist()
            verts = [self.verts_list()[i] for i in index]
            faces = [self.faces_list()[i] for i in index]
        else:
            raise IndexError(index)
        textures = None if self.textures is None else self.textures[index]
        if torch.is_tensor(verts) and torch.is_tensor(faces):
            return self.__class__(verts=[verts], faces=[faces], textures=textures)
        elif isinstance(verts, list) and isinstance(faces, list):
            return self.__class__(verts=verts, faces=faces, textures=textures)
        else:
            raise ValueError('(verts, faces) not defined correctly')

    def isempty(self) ->bool:
        """
        Checks whether any mesh is valid.

        Returns:
            bool indicating whether there is any data.
        """
        return self._N == 0 or self.valid.eq(False).all()

    def verts_list(self):
        """
        Get the list representation of the vertices.

        Returns:
            list of tensors of vertices of shape (V_n, 3).
        """
        if self._verts_list is None:
            assert self._verts_padded is not None, 'verts_padded is required to compute verts_list.'
            self._verts_list = struct_utils.padded_to_list(self._verts_padded, self.num_verts_per_mesh().tolist())
        return self._verts_list

    def faces_list(self):
        """
        Get the list representation of the faces.

        Returns:
            list of tensors of faces of shape (F_n, 3).
        """
        if self._faces_list is None:
            assert self._faces_padded is not None, 'faces_padded is required to compute faces_list.'
            self._faces_list = struct_utils.padded_to_list(self._faces_padded, self.num_faces_per_mesh().tolist())
        return self._faces_list

    def verts_packed(self):
        """
        Get the packed representation of the vertices.

        Returns:
            tensor of vertices of shape (sum(V_n), 3).
        """
        self._compute_packed()
        return self._verts_packed

    def verts_packed_to_mesh_idx(self):
        """
        Return a 1D tensor with the same first dimension as verts_packed.
        verts_packed_to_mesh_idx[i] gives the index of the mesh which contains
        verts_packed[i].

        Returns:
            1D tensor of indices.
        """
        self._compute_packed()
        return self._verts_packed_to_mesh_idx

    def mesh_to_verts_packed_first_idx(self):
        """
        Return a 1D tensor x with length equal to the number of meshes such that
        the first vertex of the ith mesh is verts_packed[x[i]].

        Returns:
            1D tensor of indices of first items.
        """
        self._compute_packed()
        return self._mesh_to_verts_packed_first_idx

    def num_verts_per_mesh(self):
        """
        Return a 1D tensor x with length equal to the number of meshes giving
        the number of vertices in each mesh.

        Returns:
            1D tensor of sizes.
        """
        return self._num_verts_per_mesh

    def faces_packed(self):
        """
        Get the packed representation of the faces.
        Faces are given by the indices of the three vertices in verts_packed.

        Returns:
            tensor of faces of shape (sum(F_n), 3).
        """
        self._compute_packed()
        return self._faces_packed

    def faces_packed_to_mesh_idx(self):
        """
        Return a 1D tensor with the same first dimension as faces_packed.
        faces_packed_to_mesh_idx[i] gives the index of the mesh which contains
        faces_packed[i].

        Returns:
            1D tensor of indices.
        """
        self._compute_packed()
        return self._faces_packed_to_mesh_idx

    def mesh_to_faces_packed_first_idx(self):
        """
        Return a 1D tensor x with length equal to the number of meshes such that
        the first face of the ith mesh is faces_packed[x[i]].

        Returns:
            1D tensor of indices of first items.
        """
        self._compute_packed()
        return self._mesh_to_faces_packed_first_idx

    def verts_padded(self):
        """
        Get the padded representation of the vertices.

        Returns:
            tensor of vertices of shape (N, max(V_n), 3).
        """
        self._compute_padded()
        return self._verts_padded

    def faces_padded(self):
        """
        Get the padded representation of the faces.

        Returns:
            tensor of faces of shape (N, max(F_n), 3).
        """
        self._compute_padded()
        return self._faces_padded

    def num_faces_per_mesh(self):
        """
        Return a 1D tensor x with length equal to the number of meshes giving
        the number of faces in each mesh.

        Returns:
            1D tensor of sizes.
        """
        return self._num_faces_per_mesh

    def edges_packed(self):
        """
        Get the packed representation of the edges.

        Returns:
            tensor of edges of shape (sum(E_n), 2).
        """
        self._compute_edges_packed()
        return self._edges_packed

    def edges_packed_to_mesh_idx(self):
        """
        Return a 1D tensor with the same first dimension as edges_packed.
        edges_packed_to_mesh_idx[i] gives the index of the mesh which contains
        edges_packed[i].

        Returns:
            1D tensor of indices.
        """
        self._compute_edges_packed()
        return self._edges_packed_to_mesh_idx

    def mesh_to_edges_packed_first_idx(self):
        """
        Return a 1D tensor x with length equal to the number of meshes such that
        the first edge of the ith mesh is edges_packed[x[i]].

        Returns:
            1D tensor of indices of first items.
        """
        self._compute_edges_packed()
        return self._mesh_to_edges_packed_first_idx

    def faces_packed_to_edges_packed(self):
        """
        Get the packed representation of the faces in terms of edges.
        Faces are given by the indices of the three edges in
        the packed representation of the edges.

        Returns:
            tensor of faces of shape (sum(F_n), 3).
        """
        self._compute_edges_packed()
        return self._faces_packed_to_edges_packed

    def num_edges_per_mesh(self):
        """
        Return a 1D tensor x with length equal to the number of meshes giving
        the number of edges in each mesh.

        Returns:
            1D tensor of sizes.
        """
        self._compute_edges_packed()
        return self._num_edges_per_mesh

    def verts_padded_to_packed_idx(self):
        """
        Return a 1D tensor x with length equal to the total number of vertices
        such that verts_packed()[i] is element x[i] of the flattened padded
        representation.
        The packed representation can be calculated as follows.

        .. code-block:: python

            p = verts_padded().reshape(-1, 3)
            verts_packed = p[x]

        Returns:
            1D tensor of indices.
        """
        if self._verts_padded_to_packed_idx is not None:
            return self._verts_padded_to_packed_idx
        self._verts_padded_to_packed_idx = torch.cat([(torch.arange(v, dtype=torch.int64, device=self.device) + i * self._V) for i, v in enumerate(self.num_verts_per_mesh())], dim=0)
        return self._verts_padded_to_packed_idx

    def has_verts_normals(self) ->bool:
        """
        Check whether vertex normals are already present.
        """
        return self._verts_normals_packed is not None

    def verts_normals_packed(self):
        """
        Get the packed representation of the vertex normals.

        Returns:
            tensor of normals of shape (sum(V_n), 3).
        """
        self._compute_vertex_normals()
        return self._verts_normals_packed

    def verts_normals_list(self):
        """
        Get the list representation of the vertex normals.

        Returns:
            list of tensors of normals of shape (V_n, 3).
        """
        if self.isempty():
            return [torch.empty((0, 3), dtype=torch.float32, device=self.device)] * self._N
        verts_normals_packed = self.verts_normals_packed()
        split_size = self.num_verts_per_mesh().tolist()
        return struct_utils.packed_to_list(verts_normals_packed, split_size)

    def verts_normals_padded(self):
        """
        Get the padded representation of the vertex normals.

        Returns:
            tensor of normals of shape (N, max(V_n), 3).
        """
        if self.isempty():
            return torch.zeros((self._N, 0, 3), dtype=torch.float32, device=self.device)
        verts_normals_list = self.verts_normals_list()
        return struct_utils.list_to_padded(verts_normals_list, (self._V, 3), pad_value=0.0, equisized=self.equisized)

    def faces_normals_packed(self):
        """
        Get the packed representation of the face normals.

        Returns:
            tensor of normals of shape (sum(F_n), 3).
        """
        self._compute_face_areas_normals()
        return self._faces_normals_packed

    def faces_normals_list(self):
        """
        Get the list representation of the face normals.

        Returns:
            list of tensors of normals of shape (F_n, 3).
        """
        if self.isempty():
            return [torch.empty((0, 3), dtype=torch.float32, device=self.device)] * self._N
        faces_normals_packed = self.faces_normals_packed()
        split_size = self.num_faces_per_mesh().tolist()
        return struct_utils.packed_to_list(faces_normals_packed, split_size)

    def faces_normals_padded(self):
        """
        Get the padded representation of the face normals.

        Returns:
            tensor of normals of shape (N, max(F_n), 3).
        """
        if self.isempty():
            return torch.zeros((self._N, 0, 3), dtype=torch.float32, device=self.device)
        faces_normals_list = self.faces_normals_list()
        return struct_utils.list_to_padded(faces_normals_list, (self._F, 3), pad_value=0.0, equisized=self.equisized)

    def faces_areas_packed(self):
        """
        Get the packed representation of the face areas.

        Returns:
            tensor of areas of shape (sum(F_n),).
        """
        self._compute_face_areas_normals()
        return self._faces_areas_packed

    def laplacian_packed(self):
        self._compute_laplacian_packed()
        return self._laplacian_packed

    def _compute_face_areas_normals(self, refresh: bool=False):
        """
        Compute the area and normal of each face in faces_packed.
        The convention of a normal for a face consisting of verts [v0, v1, v2]
        is normal = (v1 - v0) x (v2 - v0)

        Args:
            refresh: Set to True to force recomputation of face areas.
                     Default: False.
        """
        if not (refresh or any(v is None for v in [self._faces_areas_packed, self._faces_normals_packed])):
            return
        faces_packed = self.faces_packed()
        verts_packed = self.verts_packed()
        face_areas, face_normals = mesh_face_areas_normals(verts_packed, faces_packed)
        self._faces_areas_packed = face_areas
        self._faces_normals_packed = face_normals

    def _compute_vertex_normals(self, refresh: bool=False):
        """Computes the packed version of vertex normals from the packed verts
        and faces. This assumes verts are shared between faces. The normal for
        a vertex is computed as the sum of the normals of all the faces it is
        part of weighed by the face areas.

        Args:
            refresh: Set to True to force recomputation of vertex normals.
                Default: False.
        """
        if not (refresh or any(v is None for v in [self._verts_normals_packed])):
            return
        if self.isempty():
            self._verts_normals_packed = torch.zeros((self._N, 3), dtype=torch.int64, device=self.device)
        else:
            faces_packed = self.faces_packed()
            verts_packed = self.verts_packed()
            verts_normals = torch.zeros_like(verts_packed)
            vertices_faces = verts_packed[faces_packed]
            faces_normals = torch.cross(vertices_faces[:, 2] - vertices_faces[:, 1], vertices_faces[:, 0] - vertices_faces[:, 1], dim=1)
            verts_normals = verts_normals.index_add(0, faces_packed[:, 0], faces_normals)
            verts_normals = verts_normals.index_add(0, faces_packed[:, 1], faces_normals)
            verts_normals = verts_normals.index_add(0, faces_packed[:, 2], faces_normals)
            self._verts_normals_packed = torch.nn.functional.normalize(verts_normals, eps=1e-06, dim=1)

    def _compute_padded(self, refresh: bool=False):
        """
        Computes the padded version of meshes from verts_list and faces_list.
        """
        if not (refresh or any(v is None for v in [self._verts_padded, self._faces_padded])):
            return
        verts_list = self.verts_list()
        faces_list = self.faces_list()
        assert faces_list is not None and verts_list is not None, 'faces_list and verts_list arguments are required'
        if self.isempty():
            self._faces_padded = torch.zeros((self._N, 0, 3), dtype=torch.int64, device=self.device)
            self._verts_padded = torch.zeros((self._N, 0, 3), dtype=torch.float32, device=self.device)
        else:
            self._faces_padded = struct_utils.list_to_padded(faces_list, (self._F, 3), pad_value=-1.0, equisized=self.equisized)
            self._verts_padded = struct_utils.list_to_padded(verts_list, (self._V, 3), pad_value=0.0, equisized=self.equisized)

    def _compute_packed(self, refresh: bool=False):
        """
        Computes the packed version of the meshes from verts_list and faces_list
        and sets the values of auxiliary tensors.

        Args:
            refresh: Set to True to force recomputation of packed representations.
                Default: False.
        """
        if not (refresh or any(v is None for v in [self._verts_packed, self._verts_packed_to_mesh_idx, self._mesh_to_verts_packed_first_idx, self._faces_packed, self._faces_packed_to_mesh_idx, self._mesh_to_faces_packed_first_idx])):
            return
        verts_list = self.verts_list()
        faces_list = self.faces_list()
        if self.isempty():
            self._verts_packed = torch.zeros((0, 3), dtype=torch.float32, device=self.device)
            self._verts_packed_to_mesh_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._mesh_to_verts_packed_first_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._num_verts_per_mesh = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._faces_packed = -torch.ones((0, 3), dtype=torch.int64, device=self.device)
            self._faces_packed_to_mesh_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._mesh_to_faces_packed_first_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            self._num_faces_per_mesh = torch.zeros((0,), dtype=torch.int64, device=self.device)
            return
        verts_list_to_packed = struct_utils.list_to_packed(verts_list)
        self._verts_packed = verts_list_to_packed[0]
        if not torch.allclose(self.num_verts_per_mesh(), verts_list_to_packed[1]):
            raise ValueError('The number of verts per mesh should be consistent.')
        self._mesh_to_verts_packed_first_idx = verts_list_to_packed[2]
        self._verts_packed_to_mesh_idx = verts_list_to_packed[3]
        faces_list_to_packed = struct_utils.list_to_packed(faces_list)
        faces_packed = faces_list_to_packed[0]
        if not torch.allclose(self.num_faces_per_mesh(), faces_list_to_packed[1]):
            raise ValueError('The number of faces per mesh should be consistent.')
        self._mesh_to_faces_packed_first_idx = faces_list_to_packed[2]
        self._faces_packed_to_mesh_idx = faces_list_to_packed[3]
        faces_packed_offset = self._mesh_to_verts_packed_first_idx[self._faces_packed_to_mesh_idx]
        self._faces_packed = faces_packed + faces_packed_offset.view(-1, 1)

    def _compute_edges_packed(self, refresh: bool=False):
        """
        Computes edges in packed form from the packed version of faces and verts.
        """
        if not (refresh or any(v is None for v in [self._edges_packed, self._faces_packed_to_mesh_idx, self._edges_packed_to_mesh_idx, self._num_edges_per_mesh, self._mesh_to_edges_packed_first_idx])):
            return
        if self.isempty():
            self._edges_packed = torch.full((0, 2), fill_value=-1, dtype=torch.int64, device=self.device)
            self._edges_packed_to_mesh_idx = torch.zeros((0,), dtype=torch.int64, device=self.device)
            return
        faces = self.faces_packed()
        F = faces.shape[0]
        v0, v1, v2 = faces.chunk(3, dim=1)
        e01 = torch.cat([v0, v1], dim=1)
        e12 = torch.cat([v1, v2], dim=1)
        e20 = torch.cat([v2, v0], dim=1)
        edges = torch.cat([e12, e20, e01], dim=0)
        edge_to_mesh = torch.cat([self._faces_packed_to_mesh_idx, self._faces_packed_to_mesh_idx, self._faces_packed_to_mesh_idx], dim=0)
        edges, _ = edges.sort(dim=1)
        V = self._verts_packed.shape[0]
        edges_hash = V * edges[:, 0] + edges[:, 1]
        u, inverse_idxs = torch.unique(edges_hash, return_inverse=True)
        sorted_hash, sort_idx = torch.sort(edges_hash, dim=0)
        unique_mask = torch.ones(edges_hash.shape[0], dtype=torch.bool, device=self.device)
        unique_mask[1:] = sorted_hash[1:] != sorted_hash[:-1]
        unique_idx = sort_idx[unique_mask]
        self._edges_packed = torch.stack([u // V, u % V], dim=1)
        self._edges_packed_to_mesh_idx = edge_to_mesh[unique_idx]
        self._faces_packed_to_edges_packed = inverse_idxs.reshape(3, F).t()
        num_edges_per_mesh = torch.zeros(self._N, dtype=torch.int32, device=self.device)
        ones = torch.ones(1, dtype=torch.int32, device=self.device).expand(self._edges_packed_to_mesh_idx.shape)
        num_edges_per_mesh = num_edges_per_mesh.scatter_add_(0, self._edges_packed_to_mesh_idx, ones)
        self._num_edges_per_mesh = num_edges_per_mesh
        mesh_to_edges_packed_first_idx = torch.zeros(self._N, dtype=torch.int64, device=self.device)
        num_edges_cumsum = num_edges_per_mesh.cumsum(dim=0)
        mesh_to_edges_packed_first_idx[1:] = num_edges_cumsum[:-1].clone()
        self._mesh_to_edges_packed_first_idx = mesh_to_edges_packed_first_idx

    def _compute_laplacian_packed(self, refresh: bool=False):
        """
        Computes the laplacian in packed form.
        The definition of the laplacian is
        L[i, j] =    -1       , if i == j
        L[i, j] = 1 / deg(i)  , if (i, j) is an edge
        L[i, j] =    0        , otherwise
        where deg(i) is the degree of the i-th vertex in the graph

        Returns:
            Sparse FloatTensor of shape (V, V) where V = sum(V_n)

        """
        if not (refresh or self._laplacian_packed is None):
            return
        if self.isempty():
            self._laplacian_packed = torch.zeros((0, 0), dtype=torch.float32, device=self.device).to_sparse()
            return
        verts_packed = self.verts_packed()
        edges_packed = self.edges_packed()
        self._laplacian_packed = laplacian(verts_packed, edges_packed)

    def clone(self):
        """
        Deep copy of Meshes object. All internal tensors are cloned individually.

        Returns:
            new Meshes object.
        """
        verts_list = self.verts_list()
        faces_list = self.faces_list()
        new_verts_list = [v.clone() for v in verts_list]
        new_faces_list = [f.clone() for f in faces_list]
        other = self.__class__(verts=new_verts_list, faces=new_faces_list)
        for k in self._INTERNAL_TENSORS:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(other, k, v.clone())
        if self.textures is not None:
            other.textures = self.textures.clone()
        return other

    def detach(self):
        """
        Detach Meshes object. All internal tensors are detached individually.

        Returns:
            new Meshes object.
        """
        verts_list = self.verts_list()
        faces_list = self.faces_list()
        new_verts_list = [v.detach() for v in verts_list]
        new_faces_list = [f.detach() for f in faces_list]
        other = self.__class__(verts=new_verts_list, faces=new_faces_list)
        for k in self._INTERNAL_TENSORS:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(other, k, v.detach())
        if self.textures is not None:
            other.textures = self.textures.detach()
        return other

    def to(self, device: Device, copy: bool=False):
        """
        Match functionality of torch.Tensor.to()
        If copy = True or the self Tensor is on a different device, the
        returned tensor is a copy of self with the desired torch.device.
        If copy = False and the self Tensor already has the correct torch.device,
        then self is returned.

        Args:
            device: Device (as str or torch.device) for the new tensor.
            copy: Boolean indicator whether or not to clone self. Default False.

        Returns:
            Meshes object.
        """
        device_ = make_device(device)
        if not copy and self.device == device_:
            return self
        other = self.clone()
        if self.device == device_:
            return other
        other.device = device_
        if other._N > 0:
            other._verts_list = [v for v in other._verts_list]
            other._faces_list = [f for f in other._faces_list]
        for k in self._INTERNAL_TENSORS:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(other, k, v)
        if self.textures is not None:
            other.textures = other.textures
        return other

    def cpu(self):
        return self

    def cuda(self):
        return self

    def get_mesh_verts_faces(self, index: int):
        """
        Get tensors for a single mesh from the list representation.

        Args:
            index: Integer in the range [0, N).

        Returns:
            verts: Tensor of shape (V, 3).
            faces: LongTensor of shape (F, 3).
        """
        if not isinstance(index, int):
            raise ValueError('Mesh index must be an integer.')
        if index < 0 or index > self._N:
            raise ValueError('Mesh index must be in the range [0, N) where             N is the number of meshes in the batch.')
        verts = self.verts_list()
        faces = self.faces_list()
        return verts[index], faces[index]

    def split(self, split_sizes: list):
        """
        Splits Meshes object of size N into a list of Meshes objects of
        size len(split_sizes), where the i-th Meshes object is of size split_sizes[i].
        Similar to torch.split().

        Args:
            split_sizes: List of integer sizes of Meshes objects to be returned.

        Returns:
            list[Meshes].
        """
        if not all(isinstance(x, int) for x in split_sizes):
            raise ValueError('Value of split_sizes must be a list of integers.')
        meshlist = []
        curi = 0
        for i in split_sizes:
            meshlist.append(self[curi:curi + i])
            curi += i
        return meshlist

    def offset_verts_(self, vert_offsets_packed):
        """
        Add an offset to the vertices of this Meshes. In place operation.
        If normals are present they may be recalculated.

        Args:
            vert_offsets_packed: A Tensor of shape (3,) or the same shape as
                                self.verts_packed, giving offsets to be added
                                to all vertices.
        Returns:
            self.
        """
        verts_packed = self.verts_packed()
        if vert_offsets_packed.shape == (3,):
            update_normals = False
            vert_offsets_packed = vert_offsets_packed.expand_as(verts_packed)
        else:
            update_normals = True
        if vert_offsets_packed.shape != verts_packed.shape:
            raise ValueError('Verts offsets must have dimension (all_v, 3).')
        self._verts_packed = verts_packed + vert_offsets_packed
        new_verts_list = list(self._verts_packed.split(self.num_verts_per_mesh().tolist(), 0))
        self._verts_list = new_verts_list
        if self._verts_padded is not None:
            for i, verts in enumerate(new_verts_list):
                if len(verts) > 0:
                    self._verts_padded[i, :verts.shape[0], :] = verts
        if update_normals and any(v is not None for v in [self._faces_areas_packed, self._faces_normals_packed]):
            self._compute_face_areas_normals(refresh=True)
        if update_normals and self._verts_normals_packed is not None:
            self._compute_vertex_normals(refresh=True)
        return self

    def offset_verts(self, vert_offsets_packed):
        """
        Out of place offset_verts.

        Args:
            vert_offsets_packed: A Tensor of the same shape as self.verts_packed
                giving offsets to be added to all vertices.
        Returns:
            new Meshes object.
        """
        new_mesh = self.clone()
        return new_mesh.offset_verts_(vert_offsets_packed)

    def scale_verts_(self, scale):
        """
        Multiply the vertices of this Meshes object by a scalar value.
        In place operation.

        Args:
            scale: A scalar, or a Tensor of shape (N,).

        Returns:
            self.
        """
        if not torch.is_tensor(scale):
            scale = torch.full((len(self),), scale, device=self.device)
        new_verts_list = []
        verts_list = self.verts_list()
        for i, old_verts in enumerate(verts_list):
            new_verts_list.append(scale[i] * old_verts)
        self._verts_list = new_verts_list
        if self._verts_packed is not None:
            self._verts_packed = torch.cat(new_verts_list, dim=0)
        if self._verts_padded is not None:
            for i, verts in enumerate(self._verts_list):
                if len(verts) > 0:
                    self._verts_padded[i, :verts.shape[0], :] = verts
        if any(v is not None for v in [self._faces_areas_packed, self._faces_normals_packed]):
            self._compute_face_areas_normals(refresh=True)
        return self

    def scale_verts(self, scale):
        """
        Out of place scale_verts.

        Args:
            scale: A scalar, or a Tensor of shape (N,).

        Returns:
            new Meshes object.
        """
        new_mesh = self.clone()
        return new_mesh.scale_verts_(scale)

    def update_padded(self, new_verts_padded):
        """
        This function allows for an update of verts_padded without having to
        explicitly convert it to the list representation for heterogeneous batches.
        Returns a Meshes structure with updated padded tensors and copies of the
        auxiliary tensors at construction time.
        It updates self._verts_padded with new_verts_padded, and does a
        shallow copy of (faces_padded, faces_list, num_verts_per_mesh, num_faces_per_mesh).
        If packed representations are computed in self, they are updated as well.

        Args:
            new_points_padded: FloatTensor of shape (N, V, 3)

        Returns:
            Meshes with updated padded representations
        """

        def check_shapes(x, size):
            if x.shape[0] != size[0]:
                raise ValueError('new values must have the same batch dimension.')
            if x.shape[1] != size[1]:
                raise ValueError('new values must have the same number of points.')
            if x.shape[2] != size[2]:
                raise ValueError('new values must have the same dimension.')
        check_shapes(new_verts_padded, [self._N, self._V, 3])
        new = self.__class__(verts=new_verts_padded, faces=self.faces_padded())
        if new._N != self._N or new._V != self._V or new._F != self._F:
            raise ValueError('Inconsistent sizes after construction.')
        new.equisized = self.equisized
        new.textures = self.textures
        copy_tensors = ['_num_verts_per_mesh', '_num_faces_per_mesh', 'valid']
        for k in copy_tensors:
            v = getattr(self, k)
            if torch.is_tensor(v):
                setattr(new, k, v)
        new._faces_list = self._faces_list
        if self._verts_packed is not None:
            copy_tensors = ['_faces_packed', '_verts_packed_to_mesh_idx', '_faces_packed_to_mesh_idx', '_mesh_to_verts_packed_first_idx', '_mesh_to_faces_packed_first_idx']
            for k in copy_tensors:
                v = getattr(self, k)
                assert torch.is_tensor(v)
                setattr(new, k, v)
            pad_to_packed = self.verts_padded_to_packed_idx()
            new_verts_packed = new_verts_padded.reshape(-1, 3)[pad_to_packed, :]
            new._verts_packed = new_verts_packed
            new._verts_padded_to_packed_idx = pad_to_packed
        if self._edges_packed is not None:
            copy_tensors = ['_edges_packed', '_edges_packed_to_mesh_idx', '_mesh_to_edges_packed_first_idx', '_faces_packed_to_edges_packed', '_num_edges_per_mesh']
            for k in copy_tensors:
                v = getattr(self, k)
                assert torch.is_tensor(v)
                setattr(new, k, v)
        if self._laplacian_packed is not None:
            new._laplacian_packed = self._laplacian_packed
        assert new._verts_list is None
        assert new._verts_normals_packed is None
        assert new._faces_normals_packed is None
        assert new._faces_areas_packed is None
        return new

    def get_bounding_boxes(self):
        """
        Compute an axis-aligned bounding box for each mesh in this Meshes object.

        Returns:
            bboxes: Tensor of shape (N, 3, 2) where bbox[i, j] gives the
            min and max values of mesh i along the jth coordinate axis.
        """
        all_mins, all_maxes = [], []
        for verts in self.verts_list():
            cur_mins = verts.min(dim=0)[0]
            cur_maxes = verts.max(dim=0)[0]
            all_mins.append(cur_mins)
            all_maxes.append(cur_maxes)
        all_mins = torch.stack(all_mins, dim=0)
        all_maxes = torch.stack(all_maxes, dim=0)
        bboxes = torch.stack([all_mins, all_maxes], dim=2)
        return bboxes

    def extend(self, N: int):
        """
        Create new Meshes class which contains each input mesh N times

        Args:
            N: number of new copies of each mesh.

        Returns:
            new Meshes object.
        """
        if not isinstance(N, int):
            raise ValueError('N must be an integer.')
        if N <= 0:
            raise ValueError('N must be > 0.')
        new_verts_list, new_faces_list = [], []
        for verts, faces in zip(self.verts_list(), self.faces_list()):
            new_verts_list.extend(verts.clone() for _ in range(N))
            new_faces_list.extend(faces.clone() for _ in range(N))
        tex = None
        if self.textures is not None:
            tex = self.textures.extend(N)
        return self.__class__(verts=new_verts_list, faces=new_faces_list, textures=tex)

    def sample_textures(self, fragments):
        if self.textures is not None:
            shape_ok = self.textures.check_shapes(self._N, self._V, self._F)
            if not shape_ok:
                msg = 'Textures do not match the dimensions of Meshes.'
                raise ValueError(msg)
            return self.textures.sample_textures(fragments, faces_packed=self.faces_packed())
        else:
            raise ValueError('Meshes does not have textures')

    def submeshes(self, face_indices: Union[List[List[torch.LongTensor]], List[torch.LongTensor], torch.LongTensor]) ->'Meshes':
        """
        Split a batch of meshes into a batch of submeshes.

        The return value is a Meshes object representing
            [mesh restricted to only faces indexed by selected_faces
            for mesh, selected_faces_list in zip(self, face_indices)
            for faces in selected_faces_list]

        Args:
          face_indices:
            Let the original mesh have verts_list() of length N.
            Can be either
              - List of lists of LongTensors. The n-th element is a list of length
              num_submeshes_n (empty lists are allowed). The k-th element of the n-th
              sublist is a LongTensor of length num_faces_submesh_n_k.
              - List of LongTensors. The n-th element is a (possibly empty) LongTensor
                of shape (num_submeshes_n, num_faces_n).
              - A LongTensor of shape (N, num_submeshes_per_mesh, num_faces_per_submesh)
                where all meshes in the batch will have the same number of submeshes.
                This will result in an output Meshes object with batch size equal to
                N * num_submeshes_per_mesh.

        Returns:
          Meshes object of length `sum(len(ids) for ids in face_indices)`.

        Submeshing only works with no textures or with the TexturesVertex texture.

        Example 1:

        If `meshes` has batch size 1, and `face_indices` is a 1D LongTensor,
        then `meshes.submeshes([[face_indices]]) and
        `meshes.submeshes(face_indices[None, None])` both produce a Meshes of length 1,
        containing a single submesh with a subset of `meshes`' faces, whose indices are
        specified by `face_indices`.

        Example 2:

        Take a Meshes object `cubes` with 4 meshes, each a translated cube. Then:
            * len(cubes) is 4, len(cubes.verts_list()) is 4, len(cubes.faces_list()) 4,
            * [cube_verts.size for cube_verts in cubes.verts_list()] is [8, 8, 8, 8],
            * [cube_faces.size for cube_faces in cubes.faces_list()] if [6, 6, 6, 6],

        Now let front_facet, top_and_bottom, all_facets be LongTensors of
        sizes (2), (4), and (12), each picking up a number of facets of a cube by
        specifying the appropriate triangular faces.

        Then let `subcubes = cubes.submeshes([[front_facet, top_and_bottom], [],
                                              [all_facets], []])`.
            * len(subcubes) is 3.
            * subcubes[0] is the front facet of the cube contained in cubes[0].
            * subcubes[1] is a mesh containing the (disconnected) top and bottom facets
              of cubes[0].
            * subcubes[2] is cubes[2].
            * There are no submeshes of cubes[1] and cubes[3] in subcubes.
            * subcubes[0] and subcubes[1] are not watertight. subcubes[2] is.
        """
        if len(face_indices) != len(self):
            raise ValueError('You must specify exactly one set of submeshes for each mesh in this Meshes object.')
        sub_verts = []
        sub_verts_ids = []
        sub_faces = []
        sub_face_ids = []
        for face_ids_per_mesh, faces, verts in zip(face_indices, self.faces_list(), self.verts_list()):
            sub_verts_ids.append([])
            sub_face_ids.append([])
            for submesh_face_ids in face_ids_per_mesh:
                faces_to_keep = faces[submesh_face_ids]
                sub_face_ids[-1].append(faces_to_keep)
                vertex_ids_to_keep = torch.unique(faces_to_keep, sorted=True)
                sub_verts.append(verts[vertex_ids_to_keep])
                sub_verts_ids[-1].append(vertex_ids_to_keep)
                _, ids_of_unique_ids_in_sorted = torch.unique(faces_to_keep, return_inverse=True)
                sub_faces.append(ids_of_unique_ids_in_sorted)
        return self.__class__(verts=sub_verts, faces=sub_faces, textures=self.textures.submeshes(sub_verts_ids, sub_face_ids) if self.textures else None)


def _create_faces_index(faces_per_mesh: torch.Tensor, device=None):
    """
    Helper function to group the faces indices for each mesh. New faces are
    stacked at the end of the original faces tensor, so in order to have
    sequential packing, the faces tensor needs to be reordered to that faces
    corresponding to each mesh are grouped together.

    Args:
        faces_per_mesh: Tensor of shape (N,) giving the number of faces
            in each mesh in the batch where N is the batch size.

    Returns:
        faces_idx: A tensor with face indices for each mesh ordered sequentially
            by mesh index.
    """
    F = faces_per_mesh.sum()
    faces_per_mesh_cumsum = faces_per_mesh.cumsum(dim=0)
    switch1_idx = faces_per_mesh_cumsum.clone()
    switch1_idx[1:] += 3 * faces_per_mesh_cumsum[:-1]
    switch2_idx = 2 * faces_per_mesh_cumsum
    switch2_idx[1:] += 2 * faces_per_mesh_cumsum[:-1]
    switch3_idx = 3 * faces_per_mesh_cumsum
    switch3_idx[1:] += faces_per_mesh_cumsum[:-1]
    switch4_idx = 4 * faces_per_mesh_cumsum[:-1]
    switch123_offset = F - faces_per_mesh
    idx_diffs = torch.ones(4 * F, device=device, dtype=torch.int64)
    idx_diffs[switch1_idx] += switch123_offset
    idx_diffs[switch2_idx] += switch123_offset
    idx_diffs[switch3_idx] += switch123_offset
    idx_diffs[switch4_idx] -= 3 * F
    faces_idx = idx_diffs.cumsum(dim=0) - 1
    return faces_idx


def _create_verts_index(verts_per_mesh, edges_per_mesh, device=None):
    """
    Helper function to group the vertex indices for each mesh. New vertices are
    stacked at the end of the original verts tensor, so in order to have
    sequential packing, the verts tensor needs to be reordered so that the
    vertices corresponding to each mesh are grouped together.

    Args:
        verts_per_mesh: Tensor of shape (N,) giving the number of vertices
            in each mesh in the batch where N is the batch size.
        edges_per_mesh: Tensor of shape (N,) giving the number of edges
            in each mesh in the batch

    Returns:
        verts_idx: A tensor with vert indices for each mesh ordered sequentially
            by mesh index.
    """
    V = verts_per_mesh.sum()
    E = edges_per_mesh.sum()
    verts_per_mesh_cumsum = verts_per_mesh.cumsum(dim=0)
    edges_per_mesh_cumsum = edges_per_mesh.cumsum(dim=0)
    v_to_e_idx = verts_per_mesh_cumsum.clone()
    v_to_e_idx[1:] += edges_per_mesh_cumsum[:-1]
    v_to_e_offset = V - verts_per_mesh_cumsum
    v_to_e_offset[1:] += edges_per_mesh_cumsum[:-1]
    e_to_v_idx = verts_per_mesh_cumsum[:-1] + edges_per_mesh_cumsum[:-1]
    e_to_v_offset = verts_per_mesh_cumsum[:-1] - edges_per_mesh_cumsum[:-1] - V
    idx_diffs = torch.ones(V + E, device=device, dtype=torch.int64)
    idx_diffs[v_to_e_idx] += v_to_e_offset
    idx_diffs[e_to_v_idx] += e_to_v_offset
    verts_idx = idx_diffs.cumsum(dim=0) - 1
    return verts_idx


class SubdivideMeshes(nn.Module):
    """
    Subdivide a triangle mesh by adding a new vertex at the center of each edge
    and dividing each face into four new faces. Vectors of vertex
    attributes can also be subdivided by averaging the values of the attributes
    at the two vertices which form each edge. This implementation
    preserves face orientation - if the vertices of a face are all ordered
    counter-clockwise, then the faces in the subdivided meshes will also have
    their vertices ordered counter-clockwise.

    If meshes is provided as an input, the initializer performs the relatively
    expensive computation of determining the new face indices. This one-time
    computation can be reused for all meshes with the same face topology
    but different vertex positions.
    """

    def __init__(self, meshes=None) ->None:
        """
        Args:
            meshes: Meshes object or None. If a meshes object is provided,
                the first mesh is used to compute the new faces of the
                subdivided topology which can be reused for meshes with
                the same input topology.
        """
        super(SubdivideMeshes, self).__init__()
        self.precomputed = False
        self._N = -1
        if meshes is not None:
            mesh = meshes[0]
            with torch.no_grad():
                subdivided_faces = self.subdivide_faces(mesh)
                if subdivided_faces.shape[1] != 3:
                    raise ValueError('faces can only have three vertices')
                self.register_buffer('_subdivided_faces', subdivided_faces)
                self.precomputed = True

    def subdivide_faces(self, meshes):
        """
        Args:
            meshes: a Meshes object.

        Returns:
            subdivided_faces_packed: (4*sum(F_n), 3) shape LongTensor of
            original and new faces.

        Refer to pytorch3d.structures.meshes.py for more details on packed
        representations of faces.

        Each face is split into 4 faces e.g. Input face
        ::
                   v0
                   /\\
                  /  \\
                 /    \\
             e1 /      \\ e0
               /        \\
              /          \\
             /            \\
            /______________\\
          v2       e2       v1

          faces_packed = [[0, 1, 2]]
          faces_packed_to_edges_packed = [[2, 1, 0]]

        `faces_packed_to_edges_packed` is used to represent all the new
        vertex indices corresponding to the mid-points of edges in the mesh.
        The actual vertex coordinates will be computed in the forward function.
        To get the indices of the new vertices, offset
        `faces_packed_to_edges_packed` by the total number of vertices.
        ::
            faces_packed_to_edges_packed = [[2, 1, 0]] + 3 = [[5, 4, 3]]

        e.g. subdivided face
        ::
                   v0
                   /\\
                  /  \\
                 / f0 \\
             v4 /______\\ v3
               /\\      /\\
              /  \\ f3 /  \\
             / f2 \\  / f1 \\
            /______\\/______\\
           v2       v5       v1

           f0 = [0, 3, 4]
           f1 = [1, 5, 3]
           f2 = [2, 4, 5]
           f3 = [5, 4, 3]

        """
        verts_packed = meshes.verts_packed()
        with torch.no_grad():
            faces_packed = meshes.faces_packed()
            faces_packed_to_edges_packed = meshes.faces_packed_to_edges_packed() + verts_packed.shape[0]
            f0 = torch.stack([faces_packed[:, 0], faces_packed_to_edges_packed[:, 2], faces_packed_to_edges_packed[:, 1]], dim=1)
            f1 = torch.stack([faces_packed[:, 1], faces_packed_to_edges_packed[:, 0], faces_packed_to_edges_packed[:, 2]], dim=1)
            f2 = torch.stack([faces_packed[:, 2], faces_packed_to_edges_packed[:, 1], faces_packed_to_edges_packed[:, 0]], dim=1)
            f3 = faces_packed_to_edges_packed
            subdivided_faces_packed = torch.cat([f0, f1, f2, f3], dim=0)
            return subdivided_faces_packed

    def forward(self, meshes, feats=None):
        """
        Subdivide a batch of meshes by adding a new vertex on each edge, and
        dividing each face into four new faces. New meshes contains two types
        of vertices:
        1) Vertices that appear in the input meshes.
           Data for these vertices are copied from the input meshes.
        2) New vertices at the midpoint of each edge.
           Data for these vertices is the average of the data for the two
           vertices that make up the edge.

        Args:
            meshes: Meshes object representing a batch of meshes.
            feats: Per-vertex features to be subdivided along with the verts.
                Should be parallel to the packed vert representation of the
                input meshes; so it should have shape (V, D) where V is the
                total number of verts in the input meshes. Default: None.

        Returns:
            2-element tuple containing

            - **new_meshes**: Meshes object of a batch of subdivided meshes.
            - **new_feats**: (optional) Tensor of subdivided feats, parallel to the
              (packed) vertices of the subdivided meshes. Only returned
              if feats is not None.

        """
        self._N = len(meshes)
        if self.precomputed:
            return self.subdivide_homogeneous(meshes, feats)
        else:
            return self.subdivide_heterogenerous(meshes, feats)

    def subdivide_homogeneous(self, meshes, feats=None):
        """
        Subdivide verts (and optionally features) of a batch of meshes
        where each mesh has the same topology of faces. The subdivided faces
        are precomputed in the initializer.

        Args:
            meshes: Meshes object representing a batch of meshes.
            feats: Per-vertex features to be subdivided along with the verts.

        Returns:
            2-element tuple containing

            - **new_meshes**: Meshes object of a batch of subdivided meshes.
            - **new_feats**: (optional) Tensor of subdivided feats, parallel to the
              (packed) vertices of the subdivided meshes. Only returned
              if feats is not None.
        """
        verts = meshes.verts_padded()
        edges = meshes[0].edges_packed()
        new_faces = self._subdivided_faces.view(1, -1, 3).expand(self._N, -1, -1)
        new_verts = verts[:, edges].mean(dim=2)
        new_verts = torch.cat([verts, new_verts], dim=1)
        new_feats = None
        if feats is not None:
            if feats.dim() == 2:
                feats = feats.view(verts.size(0), verts.size(1), feats.size(1))
            if feats.dim() != 3:
                raise ValueError('features need to be of shape (N, V, D) or (N*V, D)')
            new_feats = feats[:, edges].mean(dim=2)
            new_feats = torch.cat([feats, new_feats], dim=1)
        new_meshes = Meshes(verts=new_verts, faces=new_faces)
        if feats is None:
            return new_meshes
        else:
            return new_meshes, new_feats

    def subdivide_heterogenerous(self, meshes, feats=None):
        """
        Subdivide faces, verts (and optionally features) of a batch of meshes
        where each mesh can have different face topologies.

        Args:
            meshes: Meshes object representing a batch of meshes.
            feats: Per-vertex features to be subdivided along with the verts.

        Returns:
            2-element tuple containing

            - **new_meshes**: Meshes object of a batch of subdivided meshes.
            - **new_feats**: (optional) Tensor of subdivided feats, parallel to the
              (packed) vertices of the subdivided meshes. Only returned
              if feats is not None.
        """
        verts = meshes.verts_packed()
        with torch.no_grad():
            new_faces = self.subdivide_faces(meshes)
            edges = meshes.edges_packed()
            face_to_mesh_idx = meshes.faces_packed_to_mesh_idx()
            edge_to_mesh_idx = meshes.edges_packed_to_mesh_idx()
            num_edges_per_mesh = edge_to_mesh_idx.bincount(minlength=self._N)
            num_verts_per_mesh = meshes.num_verts_per_mesh()
            num_faces_per_mesh = meshes.num_faces_per_mesh()
            new_verts_per_mesh = num_verts_per_mesh + num_edges_per_mesh
            new_face_to_mesh_idx = torch.cat([face_to_mesh_idx] * 4, dim=0)
            verts_sort_idx = _create_verts_index(num_verts_per_mesh, num_edges_per_mesh, meshes.device)
            verts_ordered_idx_init = torch.zeros(new_verts_per_mesh.sum(), dtype=torch.int64, device=meshes.device)
            verts_ordered_idx = verts_ordered_idx_init.scatter_add(0, verts_sort_idx, torch.arange(new_verts_per_mesh.sum(), device=meshes.device))
            new_faces = verts_ordered_idx[new_faces]
            face_sort_idx = _create_faces_index(num_faces_per_mesh, device=meshes.device)
            new_faces = new_faces[face_sort_idx]
            new_face_to_mesh_idx = new_face_to_mesh_idx[face_sort_idx]
            new_faces_per_mesh = new_face_to_mesh_idx.bincount(minlength=self._N)
        new_verts = verts[edges].mean(dim=1)
        new_verts = torch.cat([verts, new_verts], dim=0)
        new_verts = new_verts[verts_sort_idx]
        if feats is not None:
            new_feats = feats[edges].mean(dim=1)
            new_feats = torch.cat([feats, new_feats], dim=0)
            new_feats = new_feats[verts_sort_idx]
        verts_list = list(new_verts.split(new_verts_per_mesh.tolist(), 0))
        faces_list = list(new_faces.split(new_faces_per_mesh.tolist(), 0))
        new_verts_per_mesh_cumsum = torch.cat([new_verts_per_mesh.new_full(size=(1,), fill_value=0.0), new_verts_per_mesh.cumsum(0)[:-1]], dim=0)
        faces_list = [(faces_list[n] - new_verts_per_mesh_cumsum[n]) for n in range(self._N)]
        if feats is not None:
            feats_list = new_feats.split(new_verts_per_mesh.tolist(), 0)
        new_meshes = Meshes(verts=verts_list, faces=faces_list)
        if feats is None:
            return new_meshes
        else:
            new_feats = torch.cat(feats_list, dim=0)
            return new_meshes, new_feats


class AbsorptionOnlyRaymarcher(torch.nn.Module):
    """
    Raymarch using the Absorption-Only (AO) algorithm.

    The algorithm independently renders each ray by analyzing density and
    feature values sampled at (typically uniformly) spaced 3D locations along
    each ray. The density values `rays_densities` are of shape
    `(..., n_points_per_ray, 1)`, their values should range between [0, 1], and
    represent the opaqueness of each point (the higher the less transparent).
    The algorithm only measures the total amount of light absorbed along each ray
    and, besides outputting per-ray `opacity` values of shape `(...,)`,
    does not produce any feature renderings.

    The algorithm simply computes `total_transmission = prod(1 - rays_densities)`
    of shape `(..., 1)` which, for each ray, measures the total amount of light
    that passed through the volume.
    It then returns `opacities = 1 - total_transmission`.
    """

    def __init__(self) ->None:
        super().__init__()

    def forward(self, rays_densities: torch.Tensor, **kwargs) ->Union[None, torch.Tensor]:
        """
        Args:
            rays_densities: Per-ray density values represented with a tensor
                of shape `(..., n_points_per_ray)` whose values range in [0, 1].

        Returns:
            opacities: A tensor of per-ray opacity values of shape `(..., 1)`.
                Its values range between [0, 1] and denote the total amount
                of light that has been absorbed for each ray. E.g. a value
                of 0 corresponds to the ray completely passing through a volume.
        """
        _check_raymarcher_inputs(rays_densities, None, None, features_can_be_none=True, z_can_be_none=True, density_1d=True)
        rays_densities = rays_densities[..., 0]
        _check_density_bounds(rays_densities)
        total_transmission = torch.prod(1 - rays_densities, dim=-1, keepdim=True)
        opacities = 1.0 - total_transmission
        return opacities


_TensorBatch = Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]


class Volumes:
    """
    This class provides functions for working with batches of volumetric grids
    of possibly varying spatial sizes.

    VOLUME DENSITIES

    The Volumes class can be either constructed from a 5D tensor of
    `densities` of size `batch x density_dim x depth x height x width` or
    from a list of differently-sized 4D tensors `[D_1, ..., D_batch]`,
    where each `D_i` is of size `[density_dim x depth_i x height_i x width_i]`.

    In case the `Volumes` object is initialized from the list of `densities`,
    the list of tensors is internally converted to a single 5D tensor by
    zero-padding the relevant dimensions. Both list and padded representations can be
    accessed with the `Volumes.densities()` or `Volumes.densities_list()` getters.
    The sizes of the individual volumes in the structure can be retrieved
    with the `Volumes.get_grid_sizes()` getter.

    The `Volumes` class is immutable. I.e. after generating a `Volumes` object,
    one cannot change its properties, such as `self._densities` or `self._features`
    anymore.


    VOLUME FEATURES

    While the `densities` field is intended to represent various measures of the
    "density" of the volume cells (opacity, signed/unsigned distances
    from the nearest surface, ...), one can additionally initialize the
    object with the `features` argument. `features` are either a 5D tensor
    of shape `batch x feature_dim x depth x height x width` or a list of
    of differently-sized 4D tensors `[F_1, ..., F_batch]`,
    where each `F_i` is of size `[feature_dim x depth_i x height_i x width_i]`.
    `features` are intended to describe other properties of volume cells,
    such as per-voxel 3D vectors of RGB colors that can be later used
    for rendering the volume.


    VOLUME COORDINATES

    Additionally, using the `VolumeLocator` class the `Volumes` class keeps track
    of the locations of the centers of the volume cells in the local volume
    coordinates as well as in the world coordinates.

        Local coordinates:
            - Represent the locations of the volume cells in the local coordinate
              frame of the volume.
            - The center of the voxel indexed with `[·, ·, 0, 0, 0]` in the volume
              has its 3D local coordinate set to `[-1, -1, -1]`, while the voxel
              at index `[·, ·, depth_i-1, height_i-1, width_i-1]` has its
              3D local coordinate set to `[1, 1, 1]`.
            - The first/second/third coordinate of each of the 3D per-voxel
              XYZ vector denotes the horizontal/vertical/depth-wise position
              respectively. I.e the order of the coordinate dimensions in the
              volume is reversed w.r.t. the order of the 3D coordinate vectors.
            - The intermediate coordinates between `[-1, -1, -1]` and `[1, 1, 1]`.
              are linearly interpolated over the spatial dimensions of the volume.
            - Note that the convention is the same as for the 5D version of the
              `torch.nn.functional.grid_sample` function called with
              `align_corners==True`.
            - Note that the local coordinate convention of `Volumes`
              (+X = left to right, +Y = top to bottom, +Z = away from the user)
              is *different* from the world coordinate convention of the
              renderer for `Meshes` or `Pointclouds`
              (+X = right to left, +Y = bottom to top, +Z = away from the user).

        World coordinates:
            - These define the locations of the centers of the volume cells
              in the world coordinates.
            - They are specified with the following mapping that converts
              points `x_local` in the local coordinates to points `x_world`
              in the world coordinates::

                    x_world = (
                        x_local * (volume_size - 1) * 0.5 * voxel_size
                    ) - volume_translation,

              here `voxel_size` specifies the size of each voxel of the volume,
              and `volume_translation` is the 3D offset of the central voxel of
              the volume w.r.t. the origin of the world coordinate frame.
              Both `voxel_size` and `volume_translation` are specified in
              the world coordinate units. `volume_size` is the spatial size of
              the volume in form of a 3D vector `[width, height, depth]`.
            - Given the above definition of `x_world`, one can derive the
              inverse mapping from `x_world` to `x_local` as follows::

                    x_local = (
                        (x_world + volume_translation) / (0.5 * voxel_size)
                    ) / (volume_size - 1)

            - For a trivial volume with `volume_translation==[0, 0, 0]`
              with `voxel_size=-1`, `x_world` would range
              from -(volume_size-1)/2` to `+(volume_size-1)/2`.

    Coordinate tensors that denote the locations of each of the volume cells in
    local / world coordinates (with shape `(depth x height x width x 3)`)
    can be retrieved by calling the `Volumes.get_coord_grid()` getter with the
    appropriate `world_coordinates` argument.

    Internally, the mapping between `x_local` and `x_world` is represented
    as a `Transform3d` object `Volumes.VolumeLocator._local_to_world_transform`.
    Users can access the relevant transformations with the
    `Volumes.get_world_to_local_coords_transform()` and
    `Volumes.get_local_to_world_coords_transform()`
    functions.

    Example coordinate conversion:
        - For a "trivial" volume with `voxel_size = 1.`,
          `volume_translation=[0., 0., 0.]`, and the spatial size of
          `DxHxW = 5x5x5`, the point `x_world = (-2, 0, 2)` gets mapped
          to `x_local=(-1, 0, 1)`.
        - For a "trivial" volume `v` with `voxel_size = 1.`,
          `volume_translation=[0., 0., 0.]`, the following holds:

                torch.nn.functional.grid_sample(
                    v.densities(),
                    v.get_coord_grid(world_coordinates=False),
                    align_corners=True,
                ) == v.densities(),

            i.e. sampling the volume at trivial local coordinates
            (no scaling with `voxel_size`` or shift with `volume_translation`)
            results in the same volume.
    """

    def __init__(self, densities: _TensorBatch, features: Optional[_TensorBatch]=None, voxel_size: _VoxelSize=1.0, volume_translation: _Translation=(0.0, 0.0, 0.0)) ->None:
        """
        Args:
            **densities**: Batch of input feature volume occupancies of shape
                `(minibatch, density_dim, depth, height, width)`, or a list
                of 4D tensors `[D_1, ..., D_minibatch]` where each `D_i` has
                shape `(density_dim, depth_i, height_i, width_i)`.
                Typically, each voxel contains a non-negative number
                corresponding to its opaqueness.
            **features**: Batch of input feature volumes of shape:
                `(minibatch, feature_dim, depth, height, width)` or a list
                of 4D tensors `[F_1, ..., F_minibatch]` where each `F_i` has
                shape `(feature_dim, depth_i, height_i, width_i)`.
                The field is optional and can be set to `None` in case features are
                not required.
            **voxel_size**: Denotes the size of each volume voxel in world units.
                Has to be one of:
                a) A scalar (square voxels)
                b) 3-tuple or a 3-list of scalars
                c) a Tensor of shape (3,)
                d) a Tensor of shape (minibatch, 3)
                e) a Tensor of shape (minibatch, 1)
                f) a Tensor of shape (1,) (square voxels)
            **volume_translation**: Denotes the 3D translation of the center
                of the volume in world units. Has to be one of:
                a) 3-tuple or a 3-list of scalars
                b) a Tensor of shape (3,)
                c) a Tensor of shape (minibatch, 3)
                d) a Tensor of shape (1,) (square voxels)
        """
        densities_, grid_sizes = self._convert_densities_features_to_tensor(densities, 'densities')
        self.device = densities_.device
        self._densities = densities_
        self.locator = VolumeLocator(batch_size=len(self), grid_sizes=grid_sizes, voxel_size=voxel_size, volume_translation=volume_translation, device=self.device)
        self._features = None
        if features is not None:
            self._set_features(features)

    def _convert_densities_features_to_tensor(self, x: _TensorBatch, var_name: str) ->Tuple[torch.Tensor, torch.LongTensor]:
        """
        Handle the `densities` or `features` arguments to the constructor.
        """
        if isinstance(x, (list, tuple)):
            x_tensor = struct_utils.list_to_padded(x)
            if any(x_.ndim != 4 for x_ in x):
                raise ValueError(f'`{var_name}` has to be a list of 4-dim tensors of shape: ({var_name}_dim, height, width, depth)')
            if any(x_.shape[0] != x[0].shape[0] for x_ in x):
                raise ValueError(f'Each entry in the list of `{var_name}` has to have the same number of channels (first dimension in the tensor).')
            x_shapes = torch.stack([torch.tensor(list(x_.shape[1:]), dtype=torch.long, device=x_tensor.device) for x_ in x], dim=0)
        elif torch.is_tensor(x):
            if x.ndim != 5:
                raise ValueError(f'`{var_name}` has to be a 5-dim tensor of shape: (minibatch, {var_name}_dim, height, width, depth)')
            x_tensor = x
            x_shapes = torch.tensor(list(x.shape[2:]), dtype=torch.long, device=x.device)[None].repeat(x.shape[0], 1)
        else:
            raise ValueError(f'{var_name} must be either a list or a tensor with shape (batch_size, {var_name}_dim, H, W, D).')
        return x_tensor, x_shapes

    def __len__(self) ->int:
        return self._densities.shape[0]

    def __getitem__(self, index: Union[int, List[int], Tuple[int], slice, torch.BoolTensor, torch.LongTensor]) ->'Volumes':
        """
        Args:
            index: Specifying the index of the volume to retrieve.
                Can be an int, slice, list of ints or a boolean or a long tensor.

        Returns:
            Volumes object with selected volumes. The tensors are not cloned.
        """
        if isinstance(index, int):
            index = torch.LongTensor([index])
        elif isinstance(index, (slice, list, tuple)):
            pass
        elif torch.is_tensor(index):
            if index.dim() != 1 or index.dtype.is_floating_point:
                raise IndexError(index)
        else:
            raise IndexError(index)
        new = self.__class__(features=self.features()[index] if self._features is not None else None, densities=self.densities()[index])
        self.locator._copy_transform_and_sizes(new.locator, index=index)
        return new

    def features(self) ->Optional[torch.Tensor]:
        """
        Returns the features of the volume.

        Returns:
            **features**: The tensor of volume features.
        """
        return self._features

    def densities(self) ->torch.Tensor:
        """
        Returns the densities of the volume.

        Returns:
            **densities**: The tensor of volume densities.
        """
        return self._densities

    def densities_list(self) ->List[torch.Tensor]:
        """
        Get the list representation of the densities.

        Returns:
            list of tensors of densities of shape (dim_i, D_i, H_i, W_i).
        """
        return self._features_densities_list(self.densities())

    def features_list(self) ->List[torch.Tensor]:
        """
        Get the list representation of the features.

        Returns:
            list of tensors of features of shape (dim_i, D_i, H_i, W_i)
            or `None` for feature-less volumes.
        """
        features_ = self.features()
        if features_ is None:
            return None
        return self._features_densities_list(features_)

    def _features_densities_list(self, x: torch.Tensor) ->List[torch.Tensor]:
        """
        Retrieve the list representation of features/densities.

        Args:
            x: self.features() or self.densities()

        Returns:
            list of tensors of features/densities of shape (dim_i, D_i, H_i, W_i).
        """
        x_dim = x.shape[1]
        pad_sizes = torch.nn.functional.pad(self.get_grid_sizes(), [1, 0], mode='constant', value=x_dim)
        x_list = struct_utils.padded_to_list(x, pad_sizes.tolist())
        return x_list

    def update_padded(self, new_densities: torch.Tensor, new_features: Optional[torch.Tensor]=None) ->'Volumes':
        """
        Returns a Volumes structure with updated padded tensors and copies of
        the auxiliary tensors `self._local_to_world_transform`,
        `device` and `self._grid_sizes`. This function allows for an update of
        densities (and features) without having to explicitly
        convert it to the list representation for heterogeneous batches.

        Args:
            new_densities: FloatTensor of shape (N, dim_density, D, H, W)
            new_features: (optional) FloatTensor of shape (N, dim_feature, D, H, W)

        Returns:
            Volumes with updated features and densities
        """
        new = copy.copy(self)
        new._set_densities(new_densities)
        if new_features is None:
            new._features = None
        else:
            new._set_features(new_features)
        return new

    def _set_features(self, features: _TensorBatch) ->None:
        self._set_densities_features('features', features)

    def _set_densities(self, densities: _TensorBatch) ->None:
        self._set_densities_features('densities', densities)

    def _set_densities_features(self, var_name: str, x: _TensorBatch) ->None:
        x_tensor, grid_sizes = self._convert_densities_features_to_tensor(x, var_name)
        if x_tensor.device != self.device:
            raise ValueError(f'`{var_name}` have to be on the same device as `self.densities`.')
        if len(x_tensor.shape) != 5:
            raise ValueError(f'{var_name} has to be a 5-dim tensor of shape: (minibatch, {var_name}_dim, height, width, depth)')
        if not (self.get_grid_sizes().shape == grid_sizes.shape and torch.allclose(self.get_grid_sizes(), grid_sizes)):
            raise ValueError(f'The size of every grid in `{var_name}` has to match the size ofthe corresponding `densities` grid.')
        setattr(self, '_' + var_name, x_tensor)

    def clone(self) ->'Volumes':
        """
        Deep copy of Volumes object. All internal tensors are cloned
        individually.

        Returns:
            new Volumes object.
        """
        return copy.deepcopy(self)

    def to(self, device: Device, copy: bool=False) ->'Volumes':
        """
        Match the functionality of torch.Tensor.to()
        If copy = True or the self Tensor is on a different device, the
        returned tensor is a copy of self with the desired torch.device.
        If copy = False and the self Tensor already has the correct torch.device,
        then self is returned.

        Args:
            device: Device (as str or torch.device) for the new tensor.
            copy: Boolean indicator whether or not to clone self. Default False.

        Returns:
            Volumes object.
        """
        device_ = make_device(device)
        if not copy and self.device == device_:
            return self
        other = self.clone()
        if self.device == device_:
            return other
        other.device = device_
        other._densities = self._densities
        if self._features is not None:
            other._features = self.features()
        self.locator._copy_transform_and_sizes(other.locator, device=device_)
        other.locator = other.locator
        return other

    def cpu(self) ->'Volumes':
        return self

    def cuda(self) ->'Volumes':
        return self

    def get_grid_sizes(self) ->torch.LongTensor:
        """
        Returns the sizes of individual volumetric grids in the structure.

        Returns:
            **grid_sizes**: Tensor of spatial sizes of each of the volumes
                of size (batchsize, 3), where i-th row holds (D_i, H_i, W_i).
        """
        return self.locator.get_grid_sizes()

    def get_local_to_world_coords_transform(self) ->Transform3d:
        """
        Return a Transform3d object that converts points in the
        the local coordinate frame of the volume to world coordinates.
        Local volume coordinates are scaled s.t. the coordinates along one
        side of the volume are in range [-1, 1].

        Returns:
            **local_to_world_transform**: A Transform3d object converting
                points from local coordinates to the world coordinates.
        """
        return self.locator.get_local_to_world_coords_transform()

    def get_world_to_local_coords_transform(self) ->Transform3d:
        """
        Return a Transform3d object that converts points in the
        world coordinates to the local coordinate frame of the volume.
        Local volume coordinates are scaled s.t. the coordinates along one
        side of the volume are in range [-1, 1].

        Returns:
            **world_to_local_transform**: A Transform3d object converting
                points from world coordinates to local coordinates.
        """
        return self.get_local_to_world_coords_transform().inverse()

    def world_to_local_coords(self, points_3d_world: torch.Tensor) ->torch.Tensor:
        """
        Convert a batch of 3D point coordinates `points_3d_world` of shape
        (minibatch, ..., dim) in the world coordinates to
        the local coordinate frame of the volume. Local volume
        coordinates are scaled s.t. the coordinates along one side of the volume
        are in range [-1, 1].

        Args:
            **points_3d_world**: A tensor of shape `(minibatch, ..., 3)`
                containing the 3D coordinates of a set of points that will
                be converted from the local volume coordinates (ranging
                within [-1, 1]) to the world coordinates
                defined by the `self.center` and `self.voxel_size` parameters.

        Returns:
            **points_3d_local**: `points_3d_world` converted to the local
                volume coordinates of shape `(minibatch, ..., 3)`.
        """
        return self.locator.world_to_local_coords(points_3d_world)

    def local_to_world_coords(self, points_3d_local: torch.Tensor) ->torch.Tensor:
        """
        Convert a batch of 3D point coordinates `points_3d_local` of shape
        (minibatch, ..., dim) in the local coordinate frame of the volume
        to the world coordinates.

        Args:
            **points_3d_local**: A tensor of shape `(minibatch, ..., 3)`
                containing the 3D coordinates of a set of points that will
                be converted from the local volume coordinates (ranging
                within [-1, 1]) to the world coordinates
                defined by the `self.center` and `self.voxel_size` parameters.

        Returns:
            **points_3d_world**: `points_3d_local` converted to the world
                coordinates of the volume of shape `(minibatch, ..., 3)`.
        """
        return self.locator.local_to_world_coords(points_3d_local)

    def get_coord_grid(self, world_coordinates: bool=True) ->torch.Tensor:
        """
        Return the 3D coordinate grid of the volumetric grid
        in local (`world_coordinates=False`) or world coordinates
        (`world_coordinates=True`).

        The grid records location of each center of the corresponding volume voxel.

        Local coordinates are scaled s.t. the values along one side of the
        volume are in range [-1, 1].

        Args:
            **world_coordinates**: if `True`, the method
                returns the grid in the world coordinates,
                otherwise, in local coordinates.

        Returns:
            **coordinate_grid**: The grid of coordinates of shape
                `(minibatch, depth, height, width, 3)`, where `minibatch`,
                `height`, `width` and `depth` are the batch size, height, width
                and depth of the volume `features` or `densities`.
        """
        return self.locator.get_coord_grid(world_coordinates)


def _validate_ray_bundle_variables(rays_origins: torch.Tensor, rays_directions: torch.Tensor, rays_lengths: torch.Tensor) ->None:
    """
    Validate the shapes of RayBundle variables
    `rays_origins`, `rays_directions`, and `rays_lengths`.
    """
    ndim = rays_origins.ndim
    if any(r.ndim != ndim for r in (rays_directions, rays_lengths)):
        raise ValueError('rays_origins, rays_directions and rays_lengths' + ' have to have the same number of dimensions.')
    if ndim <= 2:
        raise ValueError('rays_origins, rays_directions and rays_lengths' + ' have to have at least 3 dimensions.')
    spatial_size = rays_origins.shape[:-1]
    if any(spatial_size != r.shape[:-1] for r in (rays_directions, rays_lengths)):
        raise ValueError('The shapes of rays_origins, rays_directions and rays_lengths' + ' may differ only in the last dimension.')
    if any(r.shape[-1] != 3 for r in (rays_origins, rays_directions)):
        raise ValueError('The size of the last dimension of rays_origins/rays_directions' + 'has to be 3.')


def eyes(dim: int, N: int, device: Optional[torch.device]=None, dtype: torch.dtype=torch.float32) ->torch.Tensor:
    """
    Generates a batch of `N` identity matrices of shape `(N, dim, dim)`.

    Args:
        **dim**: The dimensionality of the identity matrices.
        **N**: The number of identity matrices.
        **device**: The device to be used for allocating the matrices.
        **dtype**: The datatype of the matrices.

    Returns:
        **identities**: A batch of identity matrices of shape `(N, dim, dim)`.
    """
    identities = torch.eye(dim, device=device, dtype=dtype)
    return identities[None].repeat(N, 1, 1)


class ClipFrustum:
    """
    Helper class to store the information needed to represent a view frustum
    (left, right, top, bottom, znear, zfar), which is used to clip or cull triangles.
    Values left as None mean that culling should not be performed for that axis.
    The parameters perspective_correct, cull, and z_clip_value are used to define
    behavior for clipping triangles to the frustum.

    Args:
        left: NDC coordinate of the left clipping plane (along x axis)
        right: NDC coordinate of the right clipping plane (along x axis)
        top: NDC coordinate of the top clipping plane (along y axis)
        bottom: NDC coordinate of the bottom clipping plane (along y axis)
        znear: world space z coordinate of the near clipping plane
        zfar: world space z coordinate of the far clipping plane
        perspective_correct: should be set to True for a perspective camera
        cull: if True, triangles outside the frustum should be culled
        z_clip_value: if not None, then triangles should be clipped (possibly into
            smaller triangles) such that z >= z_clip_value.  This avoids projections
            that go to infinity as z->0
    """
    __slots__ = ['left', 'right', 'top', 'bottom', 'znear', 'zfar', 'perspective_correct', 'cull', 'z_clip_value']

    def __init__(self, left: Optional[float]=None, right: Optional[float]=None, top: Optional[float]=None, bottom: Optional[float]=None, znear: Optional[float]=None, zfar: Optional[float]=None, perspective_correct: bool=False, cull: bool=True, z_clip_value: Optional[float]=None) ->None:
        self.left = left
        self.right = right
        self.top = top
        self.bottom = bottom
        self.znear = znear
        self.zfar = zfar
        self.perspective_correct = perspective_correct
        self.cull = cull
        self.z_clip_value = z_clip_value


class _RasterizeFaceVerts(torch.autograd.Function):
    """
    Torch autograd wrapper for forward and backward pass of rasterize_meshes
    implemented in C++/CUDA.

    Args:
        face_verts: Tensor of shape (F, 3, 3) giving (packed) vertex positions
            for faces in all the meshes in the batch. Concretely,
            face_verts[f, i] = [x, y, z] gives the coordinates for the
            ith vertex of the fth face. These vertices are expected to
            be in NDC coordinates in the range [-1, 1].
        mesh_to_face_first_idx: LongTensor of shape (N) giving the index in
            faces_verts of the first face in each mesh in
            the batch.
        num_faces_per_mesh: LongTensor of shape (N) giving the number of faces
            for each mesh in the batch.
        image_size, blur_radius, faces_per_pixel: same as rasterize_meshes.
        perspective_correct: same as rasterize_meshes.
        cull_backfaces: same as rasterize_meshes.

    Returns:
        same as rasterize_meshes function.
    """

    @staticmethod
    def forward(ctx, face_verts: torch.Tensor, mesh_to_face_first_idx: torch.Tensor, num_faces_per_mesh: torch.Tensor, clipped_faces_neighbor_idx: torch.Tensor, image_size: Union[List[int], Tuple[int, int]]=(256, 256), blur_radius: float=0.01, faces_per_pixel: int=0, bin_size: int=0, max_faces_per_bin: int=0, perspective_correct: bool=False, clip_barycentric_coords: bool=False, cull_backfaces: bool=False, z_clip_value: Optional[float]=None, cull_to_frustum: bool=True):
        pix_to_face, zbuf, barycentric_coords, dists = _C.rasterize_meshes(face_verts, mesh_to_face_first_idx, num_faces_per_mesh, clipped_faces_neighbor_idx, image_size, blur_radius, faces_per_pixel, bin_size, max_faces_per_bin, perspective_correct, clip_barycentric_coords, cull_backfaces)
        ctx.save_for_backward(face_verts, pix_to_face)
        ctx.mark_non_differentiable(pix_to_face)
        ctx.perspective_correct = perspective_correct
        ctx.clip_barycentric_coords = clip_barycentric_coords
        return pix_to_face, zbuf, barycentric_coords, dists

    @staticmethod
    def backward(ctx, grad_pix_to_face, grad_zbuf, grad_barycentric_coords, grad_dists):
        grad_face_verts = None
        grad_mesh_to_face_first_idx = None
        grad_num_faces_per_mesh = None
        grad_clipped_faces_neighbor_idx = None
        grad_image_size = None
        grad_radius = None
        grad_faces_per_pixel = None
        grad_bin_size = None
        grad_max_faces_per_bin = None
        grad_perspective_correct = None
        grad_clip_barycentric_coords = None
        grad_cull_backfaces = None
        face_verts, pix_to_face = ctx.saved_tensors
        grad_face_verts = _C.rasterize_meshes_backward(face_verts, pix_to_face, grad_zbuf, grad_barycentric_coords, grad_dists, ctx.perspective_correct, ctx.clip_barycentric_coords)
        grads = grad_face_verts, grad_mesh_to_face_first_idx, grad_num_faces_per_mesh, grad_clipped_faces_neighbor_idx, grad_image_size, grad_radius, grad_faces_per_pixel, grad_bin_size, grad_max_faces_per_bin, grad_perspective_correct, grad_clip_barycentric_coords, grad_cull_backfaces
        return grads


class ClippedFaces:
    """
    Helper class to store the data for the clipped version of a Meshes object
    (face_verts, mesh_to_face_first_idx, num_faces_per_mesh) along with
    conversion information (faces_clipped_to_unclipped_idx, barycentric_conversion,
    faces_clipped_to_conversion_idx, clipped_faces_neighbor_idx) required to convert
    barycentric coordinates from rasterization of the clipped Meshes to barycentric
    coordinates in terms of the unclipped Meshes.

    Args:
        face_verts: FloatTensor of shape (F_clipped, 3, 3) giving the verts of
            each of the clipped faces
        mesh_to_face_first_idx: an tensor of shape (N,), where N is the number of meshes
            in the batch.  The ith element stores the index into face_verts
            of the first face of the ith mesh.
        num_faces_per_mesh: a tensor of shape (N,) storing the number of faces in each mesh.
        faces_clipped_to_unclipped_idx: (F_clipped,) shaped LongTensor mapping each clipped
            face back to the face in faces_unclipped (i.e. the faces in the original meshes
            obtained using meshes.faces_packed())
        barycentric_conversion: (T, 3, 3) FloatTensor, where barycentric_conversion[i, :, k]
            stores the barycentric weights in terms of the world coordinates of the original
            (big) unclipped triangle for the kth vertex in the clipped (small) triangle.
            If the rasterizer then expresses some NDC coordinate in terms of barycentric
            world coordinates for the clipped (small) triangle as alpha_clipped[i,:],
            alpha_unclipped[i, :] = barycentric_conversion[i, :, :]*alpha_clipped[i, :]
        faces_clipped_to_conversion_idx: (F_clipped,) shaped LongTensor mapping each clipped
            face to the applicable row of barycentric_conversion (or set to -1 if conversion is
            not needed).
        clipped_faces_neighbor_idx: LongTensor of shape (F_clipped,) giving the index of the
            neighboring face for each case 4 triangle. e.g. for a case 4 face with f split
            into two triangles (t1, t2): clipped_faces_neighbor_idx[t1_idx] = t2_idx.
            Faces which are not clipped and subdivided are set to -1 (i.e cases 1/2/3).
    """
    __slots__ = ['face_verts', 'mesh_to_face_first_idx', 'num_faces_per_mesh', 'faces_clipped_to_unclipped_idx', 'barycentric_conversion', 'faces_clipped_to_conversion_idx', 'clipped_faces_neighbor_idx']

    def __init__(self, face_verts: torch.Tensor, mesh_to_face_first_idx: torch.Tensor, num_faces_per_mesh: torch.Tensor, faces_clipped_to_unclipped_idx: Optional[torch.Tensor]=None, barycentric_conversion: Optional[torch.Tensor]=None, faces_clipped_to_conversion_idx: Optional[torch.Tensor]=None, clipped_faces_neighbor_idx: Optional[torch.Tensor]=None) ->None:
        self.face_verts = face_verts
        self.mesh_to_face_first_idx = mesh_to_face_first_idx
        self.num_faces_per_mesh = num_faces_per_mesh
        self.faces_clipped_to_unclipped_idx = faces_clipped_to_unclipped_idx
        self.barycentric_conversion = barycentric_conversion
        self.faces_clipped_to_conversion_idx = faces_clipped_to_conversion_idx
        self.clipped_faces_neighbor_idx = clipped_faces_neighbor_idx


def _find_verts_intersecting_clipping_plane(face_verts: torch.Tensor, p1_face_ind: torch.Tensor, clip_value: float, perspective_correct: bool) ->Tuple[Tuple[Any, Any, Any, Any, Any], List[Any]]:
    """
    Helper function to find the vertices used to form a new triangle for case 3/case 4 faces.

    Given a list of triangles that are already known to intersect the clipping plane,
    solve for the two vertices p4 and p5 where the edges of the triangle intersects the
    clipping plane.

                       p1
                       /\\
                      /  \\
                     /  t \\
     _____________p4/______\\p5__________ clip_value
                   /        \\
                  /____      \\
                p2     ---____\\p3

    Args:
        face_verts: An (F,3,3) tensor, where F is the number of faces in
            the packed representation of the Meshes, the 2nd dimension represents
            the 3 vertices of the face, and the 3rd dimension stores the xyz locations of each
            vertex.  The z-coordinates must be represented in world coordinates, while
            the xy-coordinates may be in NDC/screen coordinates (i.e. after projection).
        p1_face_ind: A tensor of shape (N,) with values in the range of 0 to 2.  In each
            case 3/case 4 triangle, two vertices are on the same side of the
            clipping plane and the 3rd is on the other side.  p1_face_ind stores the index of
            the vertex that is not on the same side as any other vertex in the triangle.
        clip_value: Float, the z-value defining where to clip the triangle.
        perspective_correct: Bool, Should be set to true if a perspective camera was
            used and xy-coordinates of face_verts_unclipped are in NDC/screen coordinates.

    Returns:
        A 2-tuple
            p: (p1, p2, p3, p4, p5))
            p_barycentric (p1_bary, p2_bary, p3_bary, p4_bary, p5_bary)

        Each of p1...p5 is an (F,3) tensor of the xyz locations of the 5 points in the
        diagram above for case 3/case 4 faces. Each p1_bary...p5_bary is an (F, 3) tensor
        storing the barycentric weights used to encode p1...p5 in terms of the the original
        unclipped triangle.
    """
    T = face_verts.shape[0]
    p2_face_ind = torch.remainder(p1_face_ind + 1, 3)
    p3_face_ind = torch.remainder(p1_face_ind + 2, 3)
    p1 = face_verts.gather(1, p1_face_ind[:, None, None].expand(-1, -1, 3)).squeeze(1)
    p2 = face_verts.gather(1, p2_face_ind[:, None, None].expand(-1, -1, 3)).squeeze(1)
    p3 = face_verts.gather(1, p3_face_ind[:, None, None].expand(-1, -1, 3)).squeeze(1)
    w2 = (p1[:, 2] - clip_value) / (p1[:, 2] - p2[:, 2])
    p4 = p1 * (1 - w2[:, None]) + p2 * w2[:, None]
    if perspective_correct:
        p1_world = p1[:, :2] * p1[:, 2:3]
        p2_world = p2[:, :2] * p2[:, 2:3]
        p4[:, :2] = (p1_world * (1 - w2[:, None]) + p2_world * w2[:, None]) / clip_value
    w3 = (p1[:, 2] - clip_value) / (p1[:, 2] - p3[:, 2])
    w3 = w3.detach()
    p5 = p1 * (1 - w3[:, None]) + p3 * w3[:, None]
    if perspective_correct:
        p1_world = p1[:, :2] * p1[:, 2:3]
        p3_world = p3[:, :2] * p3[:, 2:3]
        p5[:, :2] = (p1_world * (1 - w3[:, None]) + p3_world * w3[:, None]) / clip_value
    T_idx = torch.arange(T, device=face_verts.device)
    p_barycentric = [torch.zeros((T, 3), device=face_verts.device) for i in range(5)]
    p_barycentric[0][T_idx, p1_face_ind] = 1
    p_barycentric[1][T_idx, p2_face_ind] = 1
    p_barycentric[2][T_idx, p3_face_ind] = 1
    p_barycentric[3][T_idx, p1_face_ind] = 1 - w2
    p_barycentric[3][T_idx, p2_face_ind] = w2
    p_barycentric[4][T_idx, p1_face_ind] = 1 - w3
    p_barycentric[4][T_idx, p3_face_ind] = w3
    p = p1, p2, p3, p4, p5
    return p, p_barycentric


def _get_culled_faces(face_verts: torch.Tensor, frustum: ClipFrustum) ->torch.Tensor:
    """
    Helper function used to find all the faces in Meshes which are
    fully outside the view frustum. A face is culled if all 3 vertices are outside
    the same axis of the view frustum.

    Args:
        face_verts: An (F,3,3) tensor, where F is the number of faces in
            the packed representation of Meshes. The 2nd dimension represents the 3 vertices
            of a triangle, and the 3rd dimension stores the xyz locations of each
            vertex.
        frustum: An instance of the ClipFrustum class with the information on the
            position of the clipping planes.

    Returns:
        faces_culled: An boolean tensor of size F specifying whether or not each face should be
            culled.
    """
    clipping_planes = (frustum.left, 0, '<'), (frustum.right, 0, '>'), (frustum.top, 1, '<'), (frustum.bottom, 1, '>'), (frustum.znear, 2, '<'), (frustum.zfar, 2, '>')
    faces_culled = torch.zeros([face_verts.shape[0]], dtype=torch.bool, device=face_verts.device)
    for plane in clipping_planes:
        clip_value, axis, op = plane
        if frustum.cull and clip_value is not None:
            if op == '<':
                verts_clipped = face_verts[:, axis] < clip_value
            else:
                verts_clipped = face_verts[:, axis] > clip_value
            faces_culled |= verts_clipped.sum(1) == 3
    return faces_culled


def clip_faces(face_verts_unclipped: torch.Tensor, mesh_to_face_first_idx: torch.Tensor, num_faces_per_mesh: torch.Tensor, frustum: ClipFrustum) ->ClippedFaces:
    """
    Clip a mesh to the portion contained within a view frustum and with z > z_clip_value.

    There are two types of clipping:
      1) Cull triangles that are completely outside the view frustum.  This is purely
         to save computation by reducing the number of triangles that need to be
         rasterized.
      2) Clip triangles into the portion of the triangle where z > z_clip_value. The
         clipped region may be a quadrilateral, which results in splitting a triangle
         into two triangles. This does not save computation, but is necessary to
         correctly rasterize using perspective cameras for triangles that pass through
         z <= 0, because NDC/screen coordinates go to infinity at z=0.

    Args:
        face_verts_unclipped: An (F, 3, 3) tensor, where F is the number of faces in
            the packed representation of Meshes, the 2nd dimension represents the 3 vertices
            of the triangle, and the 3rd dimension stores the xyz locations of each
            vertex.  The z-coordinates must be represented in world coordinates, while
            the xy-coordinates may be in NDC/screen coordinates
        mesh_to_face_first_idx: an tensor of shape (N,), where N is the number of meshes
            in the batch.  The ith element stores the index into face_verts_unclipped
            of the first face of the ith mesh.
        num_faces_per_mesh: a tensor of shape (N,) storing the number of faces in each mesh.
        frustum: a ClipFrustum object defining the frustum used to cull faces.

    Returns:
        clipped_faces: ClippedFaces object storing a clipped version of the Meshes
            along with tensors that can be used to convert barycentric coordinates
            returned by rasterization of the clipped meshes into a barycentric
            coordinates for the unclipped meshes.
    """
    F = face_verts_unclipped.shape[0]
    device = face_verts_unclipped.device
    faces_culled = _get_culled_faces(face_verts_unclipped, frustum)
    z_clip_value = frustum.z_clip_value
    perspective_correct = frustum.perspective_correct
    if z_clip_value is not None:
        faces_clipped_verts = face_verts_unclipped[:, :, 2] < z_clip_value
        faces_num_clipped_verts = faces_clipped_verts.sum(1)
    else:
        faces_num_clipped_verts = torch.zeros([F], device=device)
    if faces_num_clipped_verts.sum().item() == 0 and faces_culled.sum().item() == 0:
        return ClippedFaces(face_verts=face_verts_unclipped, mesh_to_face_first_idx=mesh_to_face_first_idx, num_faces_per_mesh=num_faces_per_mesh)
    faces_unculled = ~faces_culled
    cases1_unclipped = (faces_num_clipped_verts == 0) & faces_unculled
    case1_unclipped_idx = cases1_unclipped.nonzero(as_tuple=True)[0]
    case2_unclipped = (faces_num_clipped_verts == 3) | faces_culled
    case3_unclipped = (faces_num_clipped_verts == 2) & faces_unculled
    case3_unclipped_idx = case3_unclipped.nonzero(as_tuple=True)[0]
    case4_unclipped = (faces_num_clipped_verts == 1) & faces_unculled
    case4_unclipped_idx = case4_unclipped.nonzero(as_tuple=True)[0]
    faces_delta = case4_unclipped.int() - case2_unclipped.int()
    faces_delta_cum = faces_delta.cumsum(0) - faces_delta
    delta = 1 + case4_unclipped.int() - case2_unclipped.int()
    faces_unclipped_to_clipped_idx = delta.cumsum(0) - delta
    F_clipped = F + faces_delta_cum[-1].item() + faces_delta[-1].item()
    face_verts_clipped = torch.zeros((F_clipped, 3, 3), dtype=face_verts_unclipped.dtype, device=device)
    faces_clipped_to_unclipped_idx = torch.zeros([F_clipped], dtype=torch.int64, device=device)
    mesh_to_face_first_idx_clipped = faces_unclipped_to_clipped_idx[mesh_to_face_first_idx]
    F_clipped_t = torch.full([1], F_clipped, dtype=torch.int64, device=device)
    num_faces_next = torch.cat((mesh_to_face_first_idx_clipped[1:], F_clipped_t))
    num_faces_per_mesh_clipped = num_faces_next - mesh_to_face_first_idx_clipped
    case1_clipped_idx = faces_unclipped_to_clipped_idx[case1_unclipped_idx]
    face_verts_clipped[case1_clipped_idx] = face_verts_unclipped[case1_unclipped_idx]
    faces_clipped_to_unclipped_idx[case1_clipped_idx] = case1_unclipped_idx
    if case3_unclipped_idx.shape[0] + case4_unclipped_idx.shape[0] == 0:
        return ClippedFaces(face_verts=face_verts_clipped, mesh_to_face_first_idx=mesh_to_face_first_idx_clipped, num_faces_per_mesh=num_faces_per_mesh_clipped, faces_clipped_to_unclipped_idx=faces_clipped_to_unclipped_idx)
    faces_case3 = face_verts_unclipped[case3_unclipped_idx]
    p1_face_ind = torch.where(~faces_clipped_verts[case3_unclipped_idx])[1]
    p, p_barycentric = _find_verts_intersecting_clipping_plane(faces_case3, p1_face_ind, z_clip_value, perspective_correct)
    p1, _, _, p4, p5 = p
    p1_barycentric, _, _, p4_barycentric, p5_barycentric = p_barycentric
    case3_clipped_idx = faces_unclipped_to_clipped_idx[case3_unclipped_idx]
    t_barycentric = torch.stack((p4_barycentric, p5_barycentric, p1_barycentric), 2)
    face_verts_clipped[case3_clipped_idx] = torch.stack((p4, p5, p1), 1)
    faces_clipped_to_unclipped_idx[case3_clipped_idx] = case3_unclipped_idx
    faces_case4 = face_verts_unclipped[case4_unclipped_idx]
    p1_face_ind = torch.where(faces_clipped_verts[case4_unclipped_idx])[1]
    p, p_barycentric = _find_verts_intersecting_clipping_plane(faces_case4, p1_face_ind, z_clip_value, perspective_correct)
    _, p2, p3, p4, p5 = p
    _, p2_barycentric, p3_barycentric, p4_barycentric, p5_barycentric = p_barycentric
    case4_clipped_idx = faces_unclipped_to_clipped_idx[case4_unclipped_idx]
    face_verts_clipped[case4_clipped_idx] = torch.stack((p4, p2, p5), 1)
    face_verts_clipped[case4_clipped_idx + 1] = torch.stack((p5, p2, p3), 1)
    t1_barycentric = torch.stack((p4_barycentric, p2_barycentric, p5_barycentric), 2)
    t2_barycentric = torch.stack((p5_barycentric, p2_barycentric, p3_barycentric), 2)
    faces_clipped_to_unclipped_idx[case4_clipped_idx] = case4_unclipped_idx
    faces_clipped_to_unclipped_idx[case4_clipped_idx + 1] = case4_unclipped_idx
    barycentric_conversion = torch.cat((t_barycentric, t1_barycentric, t2_barycentric))
    faces_to_convert_idx = torch.cat((case3_clipped_idx, case4_clipped_idx, case4_clipped_idx + 1), 0)
    barycentric_idx = torch.arange(barycentric_conversion.shape[0], dtype=torch.int64, device=device)
    faces_clipped_to_conversion_idx = torch.full([F_clipped], -1, dtype=torch.int64, device=device)
    faces_clipped_to_conversion_idx[faces_to_convert_idx] = barycentric_idx
    clipped_faces_neighbor_idx = torch.full([F_clipped], -1, dtype=torch.int64, device=device)
    clipped_faces_neighbor_idx[case4_clipped_idx] = case4_clipped_idx + 1
    clipped_faces_neighbor_idx[case4_clipped_idx + 1] = case4_clipped_idx
    clipped_faces = ClippedFaces(face_verts=face_verts_clipped, mesh_to_face_first_idx=mesh_to_face_first_idx_clipped, num_faces_per_mesh=num_faces_per_mesh_clipped, faces_clipped_to_unclipped_idx=faces_clipped_to_unclipped_idx, barycentric_conversion=barycentric_conversion, faces_clipped_to_conversion_idx=faces_clipped_to_conversion_idx, clipped_faces_neighbor_idx=clipped_faces_neighbor_idx)
    return clipped_faces


def convert_clipped_rasterization_to_original_faces(pix_to_face_clipped, bary_coords_clipped, clipped_faces: ClippedFaces) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Convert rasterization Fragments (expressed as pix_to_face_clipped,
    bary_coords_clipped, dists_clipped) of clipped Meshes computed using clip_faces()
    to the corresponding rasterization Fragments where barycentric coordinates and
    face indices are in terms of the original unclipped Meshes. The distances are
    handled in the rasterizer C++/CUDA kernels (i.e. for Cases 1/3 the distance
    can be used directly and for Case 4 triangles the distance of the pixel to
    the closest of the two subdivided triangles is used).

    Args:
        pix_to_face_clipped: LongTensor of shape (N, image_size, image_size,
            faces_per_pixel) giving the indices of the nearest faces at each pixel,
            sorted in ascending z-order. Concretely
            ``pix_to_face_clipped[n, y, x, k] = f`` means that ``faces_verts_clipped[f]``
            is the kth closest face (in the z-direction) to pixel (y, x). Pixels that
            are hit by fewer than faces_per_pixel are padded with -1.
        bary_coords_clipped: FloatTensor of shape
            (N, image_size, image_size, faces_per_pixel, 3) giving the barycentric
            coordinates in world coordinates of the nearest faces at each pixel, sorted
            in ascending z-order.  Concretely, if ``pix_to_face_clipped[n, y, x, k] = f``
            then ``[w0, w1, w2] = bary_coords_clipped[n, y, x, k]`` gives the
            barycentric coords for pixel (y, x) relative to the face defined by
            ``unproject(face_verts_clipped[f])``. Pixels hit by fewer than
            faces_per_pixel are padded with -1.
        clipped_faces: an instance of ClippedFaces class giving the auxillary variables
            for converting rasterization outputs from clipped to unclipped Meshes.

    Returns:
        3-tuple: (pix_to_face_unclipped, bary_coords_unclipped, dists_unclipped) that
        have the same definition as (pix_to_face_clipped, bary_coords_clipped,
        dists_clipped) except that they pertain to faces_verts_unclipped instead of
        faces_verts_clipped (i.e the original meshes as opposed to the modified meshes)
    """
    faces_clipped_to_unclipped_idx = clipped_faces.faces_clipped_to_unclipped_idx
    if faces_clipped_to_unclipped_idx is None or faces_clipped_to_unclipped_idx.numel() == 0:
        return pix_to_face_clipped, bary_coords_clipped
    device = pix_to_face_clipped.device
    empty = torch.full(pix_to_face_clipped.shape, -1, device=device, dtype=torch.int64)
    pix_to_face_unclipped = torch.where(pix_to_face_clipped != -1, faces_clipped_to_unclipped_idx[pix_to_face_clipped], empty)
    barycentric_conversion = clipped_faces.barycentric_conversion
    faces_clipped_to_conversion_idx = clipped_faces.faces_clipped_to_conversion_idx
    if barycentric_conversion is not None:
        bary_coords_unclipped = bary_coords_clipped.clone()
        pix_to_conversion_idx = torch.where(pix_to_face_clipped != -1, faces_clipped_to_conversion_idx[pix_to_face_clipped], empty)
        faces_to_convert_mask = pix_to_conversion_idx != -1
        N = faces_to_convert_mask.sum().item()
        faces_to_convert_mask_expanded = faces_to_convert_mask[:, :, :, :, None].expand(-1, -1, -1, -1, 3)
        conversion_idx_subset = pix_to_conversion_idx[faces_to_convert_mask]
        bary_coords_clipped_subset = bary_coords_clipped[faces_to_convert_mask_expanded]
        bary_coords_clipped_subset = bary_coords_clipped_subset.reshape((N, 3, 1))
        bary_conversion_subset = barycentric_conversion[conversion_idx_subset]
        bary_coords_unclipped_subset = bary_conversion_subset.bmm(bary_coords_clipped_subset)
        bary_coords_unclipped_subset = bary_coords_unclipped_subset.reshape([N * 3])
        bary_coords_unclipped[faces_to_convert_mask_expanded] = bary_coords_unclipped_subset
    else:
        bary_coords_unclipped = bary_coords_clipped
    return pix_to_face_unclipped, bary_coords_unclipped


kMaxFacesPerBin = 22


def rasterize_meshes(meshes, image_size: Union[int, List[int], Tuple[int, int]]=256, blur_radius: float=0.0, faces_per_pixel: int=8, bin_size: Optional[int]=None, max_faces_per_bin: Optional[int]=None, perspective_correct: bool=False, clip_barycentric_coords: bool=False, cull_backfaces: bool=False, z_clip_value: Optional[float]=None, cull_to_frustum: bool=False):
    """
    Rasterize a batch of meshes given the shape of the desired output image.
    Each mesh is rasterized onto a separate image of shape
    (H, W) if `image_size` is a tuple or (image_size, image_size) if it
    is an int.

    If the desired image size is non square (i.e. a tuple of (H, W) where H != W)
    the aspect ratio needs special consideration. There are two aspect ratios
    to be aware of:
        - the aspect ratio of each pixel
        - the aspect ratio of the output image
    The camera can be used to set the pixel aspect ratio. In the rasterizer,
    we assume square pixels, but variable image aspect ratio (i.e rectangle images).

    In most cases you will want to set the camera aspect ratio to
    1.0 (i.e. square pixels) and only vary the
    `image_size` (i.e. the output image dimensions in pixels).

    Args:
        meshes: A Meshes object representing a batch of meshes, batch size N.
        image_size: Size in pixels of the output image to be rasterized.
            Can optionally be a tuple of (H, W) in the case of non square images.
        blur_radius: Float distance in the range [0, 2] used to expand the face
            bounding boxes for rasterization. Setting blur radius
            results in blurred edges around the shape instead of a
            hard boundary. Set to 0 for no blur.
        faces_per_pixel (Optional): Number of faces to save per pixel, returning
            the nearest faces_per_pixel points along the z-axis.
        bin_size: Size of bins to use for coarse-to-fine rasterization. Setting
            bin_size=0 uses naive rasterization; setting bin_size=None attempts to
            set it heuristically based on the shape of the input. This should not
            affect the output, but can affect the speed of the forward pass.
        max_faces_per_bin: Only applicable when using coarse-to-fine rasterization
            (bin_size > 0); this is the maximum number of faces allowed within each
            bin. This should not affect the output values, but can affect
            the memory usage in the forward pass.
        perspective_correct: Bool, Whether to apply perspective correction when computing
            barycentric coordinates for pixels. This should be set to True if a perspective
            camera is used.
        clip_barycentric_coords: Whether, after any perspective correction is applied
            but before the depth is calculated (e.g. for z clipping),
            to "correct" a location outside the face (i.e. with a negative
            barycentric coordinate) to a position on the edge of the face.
        cull_backfaces: Bool, Whether to only rasterize mesh faces which are
            visible to the camera.  This assumes that vertices of
            front-facing triangles are ordered in an anti-clockwise
            fashion, and triangles that face away from the camera are
            in a clockwise order relative to the current view
            direction. NOTE: This will only work if the mesh faces are
            consistently defined with counter-clockwise ordering when
            viewed from the outside.
        z_clip_value: if not None, then triangles will be clipped (and possibly
            subdivided into smaller triangles) such that z >= z_clip_value.
            This avoids camera projections that go to infinity as z->0.
            Default is None as clipping affects rasterization speed and
            should only be turned on if explicitly needed.
            See clip.py for all the extra computation that is required.
        cull_to_frustum: if True, triangles outside the view frustum will be culled.
            Culling involves removing all faces which fall outside view frustum.
            Default is False so that it is turned on only when needed.

    Returns:
        4-element tuple containing

        - **pix_to_face**: LongTensor of shape
          (N, image_size, image_size, faces_per_pixel)
          giving the indices of the nearest faces at each pixel,
          sorted in ascending z-order.
          Concretely ``pix_to_face[n, y, x, k] = f`` means that
          ``faces_verts[f]`` is the kth closest face (in the z-direction)
          to pixel (y, x). Pixels that are hit by fewer than
          faces_per_pixel are padded with -1.
        - **zbuf**: FloatTensor of shape (N, image_size, image_size, faces_per_pixel)
          giving the NDC z-coordinates of the nearest faces at each pixel,
          sorted in ascending z-order.
          Concretely, if ``pix_to_face[n, y, x, k] = f`` then
          ``zbuf[n, y, x, k] = face_verts[f, 2]``. Pixels hit by fewer than
          faces_per_pixel are padded with -1.
        - **barycentric**: FloatTensor of shape
          (N, image_size, image_size, faces_per_pixel, 3)
          giving the barycentric coordinates in NDC units of the
          nearest faces at each pixel, sorted in ascending z-order.
          Concretely, if ``pix_to_face[n, y, x, k] = f`` then
          ``[w0, w1, w2] = barycentric[n, y, x, k]`` gives
          the barycentric coords for pixel (y, x) relative to the face
          defined by ``face_verts[f]``. Pixels hit by fewer than
          faces_per_pixel are padded with -1.
        - **pix_dists**: FloatTensor of shape
          (N, image_size, image_size, faces_per_pixel)
          giving the signed Euclidean distance (in NDC units) in the
          x/y plane of each point closest to the pixel. Concretely if
          ``pix_to_face[n, y, x, k] = f`` then ``pix_dists[n, y, x, k]`` is the
          squared distance between the pixel (y, x) and the face given
          by vertices ``face_verts[f]``. Pixels hit with fewer than
          ``faces_per_pixel`` are padded with -1.

        In the case that image_size is a tuple of (H, W) then the outputs
        will be of shape `(N, H, W, ...)`.
    """
    verts_packed = meshes.verts_packed()
    faces_packed = meshes.faces_packed()
    face_verts = verts_packed[faces_packed]
    mesh_to_face_first_idx = meshes.mesh_to_faces_packed_first_idx()
    num_faces_per_mesh = meshes.num_faces_per_mesh()
    im_size = parse_image_size(image_size)
    max_image_size = max(*im_size)
    clipped_faces_neighbor_idx = None
    if z_clip_value is not None or cull_to_frustum:
        frustum = ClipFrustum(left=-1, right=1, top=-1, bottom=1, perspective_correct=perspective_correct, z_clip_value=z_clip_value, cull=cull_to_frustum)
        clipped_faces = clip_faces(face_verts, mesh_to_face_first_idx, num_faces_per_mesh, frustum=frustum)
        face_verts = clipped_faces.face_verts
        mesh_to_face_first_idx = clipped_faces.mesh_to_face_first_idx
        num_faces_per_mesh = clipped_faces.num_faces_per_mesh
        clipped_faces_neighbor_idx = clipped_faces.clipped_faces_neighbor_idx
    if clipped_faces_neighbor_idx is None:
        clipped_faces_neighbor_idx = torch.full(size=(face_verts.shape[0],), fill_value=-1, device=meshes.device, dtype=torch.int64)
    if bin_size is None:
        if not verts_packed.is_cuda:
            bin_size = 0
        elif max_image_size <= 64:
            bin_size = 8
        else:
            bin_size = int(2 ** max(np.ceil(np.log2(max_image_size)) - 4, 4))
    if bin_size != 0:
        faces_per_bin = 1 + (max_image_size - 1) // bin_size
        if faces_per_bin >= kMaxFacesPerBin:
            raise ValueError('bin_size too small, number of faces per bin must be less than %d; got %d' % (kMaxFacesPerBin, faces_per_bin))
    if max_faces_per_bin is None:
        max_faces_per_bin = int(max(10000, meshes._F / 5))
    pix_to_face, zbuf, barycentric_coords, dists = _RasterizeFaceVerts.apply(face_verts, mesh_to_face_first_idx, num_faces_per_mesh, clipped_faces_neighbor_idx, im_size, blur_radius, faces_per_pixel, bin_size, max_faces_per_bin, perspective_correct, clip_barycentric_coords, cull_backfaces)
    if z_clip_value is not None or cull_to_frustum:
        outputs = convert_clipped_rasterization_to_original_faces(pix_to_face, barycentric_coords, clipped_faces)
        pix_to_face, barycentric_coords = outputs
    return pix_to_face, zbuf, barycentric_coords, dists


class MeshRenderer(nn.Module):
    """
    A class for rendering a batch of heterogeneous meshes. The class should
    be initialized with a rasterizer (a MeshRasterizer or a MeshRasterizerOpenGL)
    and shader class which each have a forward function.
    """

    def __init__(self, rasterizer, shader) ->None:
        super().__init__()
        self.rasterizer = rasterizer
        self.shader = shader

    def to(self, device):
        self.rasterizer
        self.shader
        return self

    def forward(self, meshes_world: Meshes, **kwargs) ->torch.Tensor:
        """
        Render a batch of images from a batch of meshes by rasterizing and then
        shading.

        NOTE: If the blur radius for rasterization is > 0.0, some pixels can
        have one or more barycentric coordinates lying outside the range [0, 1].
        For a pixel with out of bounds barycentric coordinates with respect to a
        face f, clipping is required before interpolating the texture uv
        coordinates and z buffer so that the colors and depths are limited to
        the range for the corresponding face.
        For this set rasterizer.raster_settings.clip_barycentric_coords=True
        """
        fragments = self.rasterizer(meshes_world, **kwargs)
        images = self.shader(fragments, meshes_world, **kwargs)
        return images


class MeshRendererWithFragments(nn.Module):
    """
    A class for rendering a batch of heterogeneous meshes. The class should
    be initialized with a rasterizer (a MeshRasterizer or a MeshRasterizerOpenGL)
    and shader class which each have a forward function.

    In the forward pass this class returns the `fragments` from which intermediate
    values such as the depth map can be easily extracted e.g.

    .. code-block:: python
        images, fragments = renderer(meshes)
        depth = fragments.zbuf
    """

    def __init__(self, rasterizer, shader) ->None:
        super().__init__()
        self.rasterizer = rasterizer
        self.shader = shader

    def to(self, device):
        self.rasterizer
        self.shader
        return self

    def forward(self, meshes_world: Meshes, **kwargs) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Render a batch of images from a batch of meshes by rasterizing and then
        shading.

        NOTE: If the blur radius for rasterization is > 0.0, some pixels can
        have one or more barycentric coordinates lying outside the range [0, 1].
        For a pixel with out of bounds barycentric coordinates with respect to a
        face f, clipping is required before interpolating the texture uv
        coordinates and z buffer so that the colors and depths are limited to
        the range for the corresponding face.
        For this set rasterizer.raster_settings.clip_barycentric_coords=True
        """
        fragments = self.rasterizer(meshes_world, **kwargs)
        images = self.shader(fragments, meshes_world, **kwargs)
        return images, fragments


class BlendParams(NamedTuple):
    """
    Data class to store blending params with defaults

    Members:
        sigma (float): For SoftmaxPhong, controls the width of the sigmoid
            function used to calculate the 2D distance based probability. Determines
            the sharpness of the edges of the shape. Higher => faces have less defined
            edges. For SplatterPhong, this is the standard deviation of the Gaussian
            kernel. Higher => splats have a stronger effect and the rendered image is
            more blurry.
        gamma (float): Controls the scaling of the exponential function used
            to set the opacity of the color.
            Higher => faces are more transparent.
        background_color: RGB values for the background color as a tuple or
            as a tensor of three floats.
    """
    sigma: float = 0.0001
    gamma: float = 0.0001
    background_color: Union[torch.Tensor, Sequence[float]] = (1.0, 1.0, 1.0)


class Materials(TensorProperties):
    """
    A class for storing a batch of material properties. Currently only one
    material per batch element is supported.
    """

    def __init__(self, ambient_color=((1, 1, 1),), diffuse_color=((1, 1, 1),), specular_color=((1, 1, 1),), shininess=64, device: Device='cpu') ->None:
        """
        Args:
            ambient_color: ambient reflectivity of the material
            diffuse_color: diffuse reflectivity of the material
            specular_color: specular reflectivity of the material
            shininess: The specular exponent for the material. This defines
                the focus of the specular highlight with a high value
                resulting in a concentrated highlight. Shininess values
                can range from 0-1000.
            device: Device (as str or torch.device) on which the tensors should be located

        ambient_color, diffuse_color and specular_color can be of shape
        (1, C) or (N, C) where C is typically 3 (for RGB). shininess can be of shape (1,)
        or (N,).

        The colors and shininess are broadcast against each other so need to
        have either the same batch dimension or batch dimension = 1.
        """
        super().__init__(device=device, diffuse_color=diffuse_color, ambient_color=ambient_color, specular_color=specular_color, shininess=shininess)
        C = self.ambient_color.shape[-1]
        for n in ['ambient_color', 'diffuse_color', 'specular_color']:
            t = getattr(self, n)
            if t.shape[-1] != C:
                msg = 'Expected %s to have shape (N, %d); got %r'
                raise ValueError(msg % (n, C, t.shape))
        if self.shininess.shape != torch.Size([self._N]):
            msg = 'shininess should have shape (N); got %r'
            raise ValueError(msg % repr(self.shininess.shape))

    def clone(self):
        other = Materials(device=self.device)
        return super().clone(other)


def _validate_light_properties(obj) ->None:
    props = 'ambient_color', 'diffuse_color', 'specular_color'
    for n in props:
        t = getattr(obj, n)
        if t.shape[-1] != 3:
            msg = 'Expected %s to have shape (N, 3); got %r'
            raise ValueError(msg % (n, t.shape))


class PointLights(TensorProperties):

    def __init__(self, ambient_color=((0.5, 0.5, 0.5),), diffuse_color=((0.3, 0.3, 0.3),), specular_color=((0.2, 0.2, 0.2),), location=((0, 1, 0),), device: Device='cpu') ->None:
        """
        Args:
            ambient_color: RGB color of the ambient component
            diffuse_color: RGB color of the diffuse component
            specular_color: RGB color of the specular component
            location: xyz position of the light.
            device: Device (as str or torch.device) on which the tensors should be located

        The inputs can each be
            - 3 element tuple/list or list of lists
            - torch tensor of shape (1, 3)
            - torch tensor of shape (N, 3)
        The inputs are broadcast against each other so they all have batch
        dimension N.
        """
        super().__init__(device=device, ambient_color=ambient_color, diffuse_color=diffuse_color, specular_color=specular_color, location=location)
        _validate_light_properties(self)
        if self.location.shape[-1] != 3:
            msg = 'Expected location to have shape (N, 3); got %r'
            raise ValueError(msg % repr(self.location.shape))

    def clone(self):
        other = self.__class__(device=self.device)
        return super().clone(other)

    def reshape_location(self, points) ->torch.Tensor:
        """
        Reshape the location tensor to have dimensions
        compatible with the points which can either be of
        shape (P, 3) or (N, H, W, K, 3).
        """
        if self.location.ndim == points.ndim:
            return self.location
        return self.location[:, None, None, None, :]

    def diffuse(self, normals, points) ->torch.Tensor:
        location = self.reshape_location(points)
        direction = location - points
        return diffuse(normals=normals, color=self.diffuse_color, direction=direction)

    def specular(self, normals, points, camera_position, shininess) ->torch.Tensor:
        location = self.reshape_location(points)
        direction = location - points
        return specular(points=points, normals=normals, color=self.specular_color, direction=direction, camera_position=camera_position, shininess=shininess)


class ShaderBase(nn.Module):

    def __init__(self, device: Device='cpu', cameras: Optional[TensorProperties]=None, lights: Optional[TensorProperties]=None, materials: Optional[Materials]=None, blend_params: Optional[BlendParams]=None) ->None:
        super().__init__()
        self.lights = lights if lights is not None else PointLights(device=device)
        self.materials = materials if materials is not None else Materials(device=device)
        self.cameras = cameras
        self.blend_params = blend_params if blend_params is not None else BlendParams()

    def _get_cameras(self, **kwargs):
        cameras = kwargs.get('cameras', self.cameras)
        if cameras is None:
            msg = 'Cameras must be specified either at initialization                 or in the forward pass of the shader.'
            raise ValueError(msg)
        return cameras

    def to(self, device: Device):
        cameras = self.cameras
        if cameras is not None:
            self.cameras = cameras
        self.materials = self.materials
        self.lights = self.lights
        return self


def _get_background_color(blend_params: BlendParams, device: Device, dtype=torch.float32) ->torch.Tensor:
    background_color_ = blend_params.background_color
    if isinstance(background_color_, torch.Tensor):
        background_color = background_color_
    else:
        background_color = torch.tensor(background_color_, dtype=dtype, device=device)
    return background_color


def hard_rgb_blend(colors: torch.Tensor, fragments, blend_params: BlendParams) ->torch.Tensor:
    """
    Naive blending of top K faces to return an RGBA image
      - **RGB** - choose color of the closest point i.e. K=0
      - **A** - 1.0

    Args:
        colors: (N, H, W, K, 3) RGB color for each of the top K faces per pixel.
        fragments: the outputs of rasterization. From this we use
            - pix_to_face: LongTensor of shape (N, H, W, K) specifying the indices
              of the faces (in the packed representation) which
              overlap each pixel in the image. This is used to
              determine the output shape.
        blend_params: BlendParams instance that contains a background_color
        field specifying the color for the background
    Returns:
        RGBA pixel_colors: (N, H, W, 4)
    """
    background_color = _get_background_color(blend_params, fragments.pix_to_face.device)
    is_background = fragments.pix_to_face[..., 0] < 0
    num_background_pixels = is_background.sum()
    pixel_colors = colors[..., 0, :].masked_scatter(is_background[..., None], background_color[None, :].expand(num_background_pixels, -1))
    alpha = (~is_background).type_as(pixel_colors)[..., None]
    return torch.cat([pixel_colors, alpha], dim=-1)


def _apply_lighting(points, normals, lights, cameras, materials) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Args:
        points: torch tensor of shape (N, ..., 3) or (P, 3).
        normals: torch tensor of shape (N, ..., 3) or (P, 3)
        lights: instance of the Lights class.
        cameras: instance of the Cameras class.
        materials: instance of the Materials class.

    Returns:
        ambient_color: same shape as materials.ambient_color
        diffuse_color: same shape as the input points
        specular_color: same shape as the input points
    """
    light_diffuse = lights.diffuse(normals=normals, points=points)
    light_specular = lights.specular(normals=normals, points=points, camera_position=cameras.get_camera_center(), shininess=materials.shininess)
    ambient_color = materials.ambient_color * lights.ambient_color
    diffuse_color = materials.diffuse_color * light_diffuse
    specular_color = materials.specular_color * light_specular
    if normals.dim() == 2 and points.dim() == 2:
        return ambient_color.squeeze(), diffuse_color.squeeze(), specular_color.squeeze()
    if ambient_color.ndim != diffuse_color.ndim:
        ambient_color = ambient_color[:, None, None, None, :]
    return ambient_color, diffuse_color, specular_color


class _InterpFaceAttrs(Function):

    @staticmethod
    def forward(ctx, pix_to_face, barycentric_coords, face_attrs):
        args = pix_to_face, barycentric_coords, face_attrs
        ctx.save_for_backward(*args)
        return _C.interp_face_attrs_forward(*args)

    @staticmethod
    @once_differentiable
    def backward(ctx, grad_pix_attrs):
        args = ctx.saved_tensors
        args = args + (grad_pix_attrs,)
        grads = _C.interp_face_attrs_backward(*args)
        grad_pix_to_face = None
        grad_barycentric_coords = grads[0]
        grad_face_attrs = grads[1]
        return grad_pix_to_face, grad_barycentric_coords, grad_face_attrs


def interpolate_face_attributes_python(pix_to_face: torch.Tensor, barycentric_coords: torch.Tensor, face_attributes: torch.Tensor) ->torch.Tensor:
    F, FV, D = face_attributes.shape
    N, H, W, K, _ = barycentric_coords.shape
    mask = pix_to_face < 0
    pix_to_face = pix_to_face.clone()
    pix_to_face[mask] = 0
    idx = pix_to_face.view(N * H * W * K, 1, 1).expand(N * H * W * K, 3, D)
    pixel_face_vals = face_attributes.gather(0, idx).view(N, H, W, K, 3, D)
    pixel_vals = (barycentric_coords[..., None] * pixel_face_vals).sum(dim=-2)
    pixel_vals[mask] = 0
    return pixel_vals


def interpolate_face_attributes(pix_to_face: torch.Tensor, barycentric_coords: torch.Tensor, face_attributes: torch.Tensor) ->torch.Tensor:
    """
    Interpolate arbitrary face attributes using the barycentric coordinates
    for each pixel in the rasterized output.

    Args:
        pix_to_face: LongTensor of shape (...) specifying the indices
            of the faces (in the packed representation) which overlap each
            pixel in the image. A value < 0 indicates that the pixel does not
            overlap any face and should be skipped.
        barycentric_coords: FloatTensor of shape (N, H, W, K, 3) specifying
            the barycentric coordinates of each pixel
            relative to the faces (in the packed
            representation) which overlap the pixel.
        face_attributes: packed attributes of shape (total_faces, 3, D),
            specifying the value of the attribute for each
            vertex in the face.

    Returns:
        pixel_vals: tensor of shape (N, H, W, K, D) giving the interpolated
        value of the face attribute for each pixel.
    """
    F, FV, D = face_attributes.shape
    if FV != 3:
        raise ValueError('Faces can only have three vertices; got %r' % FV)
    N, H, W, K, _ = barycentric_coords.shape
    if pix_to_face.shape != (N, H, W, K):
        msg = 'pix_to_face must have shape (batch_size, H, W, K); got %r'
        raise ValueError(msg % (pix_to_face.shape,))
    if not pix_to_face.is_cuda:
        args = pix_to_face, barycentric_coords, face_attributes
        return interpolate_face_attributes_python(*args)
    N, H, W, K = pix_to_face.shape
    pix_to_face = pix_to_face.view(-1)
    barycentric_coords = barycentric_coords.view(N * H * W * K, 3)
    args = pix_to_face, barycentric_coords, face_attributes
    out = _InterpFaceAttrs.apply(*args)
    out = out.view(N, H, W, K, -1)
    return out


def _phong_shading_with_pixels(meshes, fragments, lights, cameras, materials, texels) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Apply per pixel shading. First interpolate the vertex normals and
    vertex coordinates using the barycentric coordinates to get the position
    and normal at each pixel. Then compute the illumination for each pixel.
    The pixel color is obtained by multiplying the pixel textures by the ambient
    and diffuse illumination and adding the specular component.

    Args:
        meshes: Batch of meshes
        fragments: Fragments named tuple with the outputs of rasterization
        lights: Lights class containing a batch of lights
        cameras: Cameras class containing a batch of cameras
        materials: Materials class containing a batch of material properties
        texels: texture per pixel of shape (N, H, W, K, 3)

    Returns:
        colors: (N, H, W, K, 3)
        pixel_coords: (N, H, W, K, 3), camera coordinates of each intersection.
    """
    verts = meshes.verts_packed()
    faces = meshes.faces_packed()
    vertex_normals = meshes.verts_normals_packed()
    faces_verts = verts[faces]
    faces_normals = vertex_normals[faces]
    pixel_coords_in_camera = interpolate_face_attributes(fragments.pix_to_face, fragments.bary_coords, faces_verts)
    pixel_normals = interpolate_face_attributes(fragments.pix_to_face, fragments.bary_coords, faces_normals)
    ambient, diffuse, specular = _apply_lighting(pixel_coords_in_camera, pixel_normals, lights, cameras, materials)
    colors = (ambient + diffuse) * texels + specular
    return colors, pixel_coords_in_camera


def phong_shading(meshes, fragments, lights, cameras, materials, texels) ->torch.Tensor:
    """
    Apply per pixel shading. First interpolate the vertex normals and
    vertex coordinates using the barycentric coordinates to get the position
    and normal at each pixel. Then compute the illumination for each pixel.
    The pixel color is obtained by multiplying the pixel textures by the ambient
    and diffuse illumination and adding the specular component.

    Args:
        meshes: Batch of meshes
        fragments: Fragments named tuple with the outputs of rasterization
        lights: Lights class containing a batch of lights
        cameras: Cameras class containing a batch of cameras
        materials: Materials class containing a batch of material properties
        texels: texture per pixel of shape (N, H, W, K, 3)

    Returns:
        colors: (N, H, W, K, 3)
    """
    colors, _ = _phong_shading_with_pixels(meshes, fragments, lights, cameras, materials, texels)
    return colors


def softmax_rgb_blend(colors: torch.Tensor, fragments, blend_params: BlendParams, znear: Union[float, torch.Tensor]=1.0, zfar: Union[float, torch.Tensor]=100) ->torch.Tensor:
    """
    RGB and alpha channel blending to return an RGBA image based on the method
    proposed in [1]
      - **RGB** - blend the colors based on the 2D distance based probability map and
        relative z distances.
      - **A** - blend based on the 2D distance based probability map.

    Args:
        colors: (N, H, W, K, 3) RGB color for each of the top K faces per pixel.
        fragments: namedtuple with outputs of rasterization. We use properties
            - pix_to_face: LongTensor of shape (N, H, W, K) specifying the indices
              of the faces (in the packed representation) which
              overlap each pixel in the image.
            - dists: FloatTensor of shape (N, H, W, K) specifying
              the 2D euclidean distance from the center of each pixel
              to each of the top K overlapping faces.
            - zbuf: FloatTensor of shape (N, H, W, K) specifying
              the interpolated depth from each pixel to to each of the
              top K overlapping faces.
        blend_params: instance of BlendParams dataclass containing properties
            - sigma: float, parameter which controls the width of the sigmoid
              function used to calculate the 2D distance based probability.
              Sigma controls the sharpness of the edges of the shape.
            - gamma: float, parameter which controls the scaling of the
              exponential function used to control the opacity of the color.
            - background_color: (3) element list/tuple/torch.Tensor specifying
              the RGB values for the background color.
        znear: float, near clipping plane in the z direction
        zfar: float, far clipping plane in the z direction

    Returns:
        RGBA pixel_colors: (N, H, W, 4)

    [0] Shichen Liu et al, 'Soft Rasterizer: A Differentiable Renderer for
    Image-based 3D Reasoning'
    """
    N, H, W, K = fragments.pix_to_face.shape
    pixel_colors = torch.ones((N, H, W, 4), dtype=colors.dtype, device=colors.device)
    background_color = _get_background_color(blend_params, fragments.pix_to_face.device)
    eps = 1e-10
    mask = fragments.pix_to_face >= 0
    prob_map = torch.sigmoid(-fragments.dists / blend_params.sigma) * mask
    alpha = torch.prod(1.0 - prob_map, dim=-1)
    if torch.is_tensor(zfar):
        zfar = zfar[:, None, None, None]
    if torch.is_tensor(znear):
        znear = znear[:, None, None, None]
    z_inv = (zfar - fragments.zbuf) / (zfar - znear) * mask
    z_inv_max = torch.max(z_inv, dim=-1).values[..., None].clamp(min=eps)
    weights_num = prob_map * torch.exp((z_inv - z_inv_max) / blend_params.gamma)
    delta = torch.exp((eps - z_inv_max) / blend_params.gamma).clamp(min=eps)
    denom = weights_num.sum(dim=-1)[..., None] + delta
    weighted_colors = (weights_num[..., None] * colors).sum(dim=-2)
    weighted_background = delta * background_color
    pixel_colors[..., :3] = (weighted_colors + weighted_background) / denom
    pixel_colors[..., 3] = 1.0 - alpha
    return pixel_colors


class TexturesBase:

    def isempty(self):
        if self._N is not None and self.valid is not None:
            return self._N == 0 or self.valid.eq(False).all()
        return False

    def to(self, device):
        for k in dir(self):
            v = getattr(self, k)
            if isinstance(v, (list, tuple)) and all(torch.is_tensor(elem) for elem in v):
                v = [elem for elem in v]
                setattr(self, k, v)
            if torch.is_tensor(v) and v.device != device:
                setattr(self, k, v)
        self.device = device
        return self

    def _extend(self, N: int, props: List[str]) ->Dict[str, Union[torch.Tensor, List]]:
        """
        Create a dict with the specified properties
        repeated N times per batch element.

        Args:
            N: number of new copies of each texture
                in the batch.
            props: a List of strings which refer to either
                class attributes or class methods which
                return tensors or lists.

        Returns:
            Dict with the same keys as props. The values are the
            extended properties.
        """
        if not isinstance(N, int):
            raise ValueError('N must be an integer.')
        if N <= 0:
            raise ValueError('N must be > 0.')
        new_props = {}
        for p in props:
            t = getattr(self, p)
            if callable(t):
                t = t()
            if isinstance(t, list):
                if not all(isinstance(elem, (int, float)) for elem in t):
                    raise ValueError('Extend only supports lists of scalars')
                t = [([ti] * N) for ti in t]
                new_props[p] = list(itertools.chain(*t))
            elif torch.is_tensor(t):
                new_props[p] = t.repeat_interleave(N, dim=0)
        return new_props

    def _getitem(self, index: Union[int, slice], props: List[str]):
        """
        Helper function for __getitem__
        """
        new_props = {}
        if isinstance(index, (int, slice)):
            for p in props:
                t = getattr(self, p)
                if callable(t):
                    t = t()
                new_props[p] = t[index]
        elif isinstance(index, list):
            index = torch.tensor(index)
        if isinstance(index, torch.Tensor):
            if index.dtype == torch.bool:
                index = index.nonzero()
                index = index.squeeze(1) if index.numel() > 0 else index
                index = index.tolist()
            for p in props:
                t = getattr(self, p)
                if callable(t):
                    t = t()
                new_props[p] = [t[i] for i in index]
        return new_props

    def sample_textures(self) ->torch.Tensor:
        """
        Different texture classes sample textures in different ways
        e.g. for vertex textures, the values at each vertex
        are interpolated across the face using the barycentric
        coordinates.
        Each texture class should implement a sample_textures
        method to take the `fragments` from rasterization.
        Using `fragments.pix_to_face` and `fragments.bary_coords`
        this function should return the sampled texture values for
        each pixel in the output image.
        """
        raise NotImplementedError()

    def submeshes(self, vertex_ids_list: List[List[torch.LongTensor]], faces_ids_list: List[List[torch.LongTensor]]) ->'TexturesBase':
        """
        Extract sub-textures used for submeshing.
        """
        raise NotImplementedError(f'{self.__class__} does not support submeshes')

    def faces_verts_textures_packed(self) ->torch.Tensor:
        """
        Returns the texture for each vertex for each face in the mesh.
        For N meshes, this function returns sum(Fi)x3xC where Fi is the
        number of faces in the i-th mesh and C is the dimensional of
        the feature (C = 3 for RGB textures).
        You can use the utils function in structures.utils to convert the
        packed representation to a list or padded.
        """
        raise NotImplementedError()

    def clone(self) ->'TexturesBase':
        """
        Each texture class should implement a method
        to clone all necessary internal tensors.
        """
        raise NotImplementedError()

    def detach(self) ->'TexturesBase':
        """
        Each texture class should implement a method
        to detach all necessary internal tensors.
        """
        raise NotImplementedError()

    def __getitem__(self, index) ->'TexturesBase':
        """
        Each texture class should implement a method
        to get the texture properties for the
        specified elements in the batch.
        The TexturesBase._getitem(i) method
        can be used as a helper function to retrieve the
        class attributes for item i. Then, a new
        instance of the child class can be created with
        the attributes.
        """
        raise NotImplementedError()


def list_to_packed(x: List[torch.Tensor]):
    """
    Transforms a list of N tensors each of shape (Mi, K, ...) into a single
    tensor of shape (sum(Mi), K, ...).

    Args:
      x: list of tensors.

    Returns:
        4-element tuple containing

        - **x_packed**: tensor consisting of packed input tensors along the
          1st dimension.
        - **num_items**: tensor of shape N containing Mi for each element in x.
        - **item_packed_first_idx**: tensor of shape N indicating the index of
          the first item belonging to the same element in the original list.
        - **item_packed_to_list_idx**: tensor of shape sum(Mi) containing the
          index of the element in the list the item belongs to.
    """
    N = len(x)
    num_items = torch.zeros(N, dtype=torch.int64, device=x[0].device)
    item_packed_first_idx = torch.zeros(N, dtype=torch.int64, device=x[0].device)
    item_packed_to_list_idx = []
    cur = 0
    for i, y in enumerate(x):
        num = len(y)
        num_items[i] = num
        item_packed_first_idx[i] = cur
        item_packed_to_list_idx.append(torch.full((num,), i, dtype=torch.int64, device=y.device))
        cur += num
    x_packed = torch.cat(x, dim=0)
    item_packed_to_list_idx = torch.cat(item_packed_to_list_idx, dim=0)
    return x_packed, num_items, item_packed_first_idx, item_packed_to_list_idx


def list_to_padded(x: Union[List[torch.Tensor], Tuple[torch.Tensor]], pad_size: Union[Sequence[int], None]=None, pad_value: float=0.0, equisized: bool=False) ->torch.Tensor:
    """
    Transforms a list of N tensors each of shape (Si_0, Si_1, ... Si_D)
    into:
    - a single tensor of shape (N, pad_size(0), pad_size(1), ..., pad_size(D))
      if pad_size is provided
    - or a tensor of shape (N, max(Si_0), max(Si_1), ..., max(Si_D)) if pad_size is None.

    Args:
      x: list of Tensors
      pad_size: list(int) specifying the size of the padded tensor.
        If `None` (default), the largest size of each dimension
        is set as the `pad_size`.
      pad_value: float value to be used to fill the padded tensor
      equisized: bool indicating whether the items in x are of equal size
        (sometimes this is known and if provided saves computation)

    Returns:
      x_padded: tensor consisting of padded input tensors stored
        over the newly allocated memory.
    """
    if equisized:
        return torch.stack(x, 0)
    if not all(torch.is_tensor(y) for y in x):
        raise ValueError('All items have to be instances of a torch.Tensor.')
    element_ndim = max(y.ndim for y in x)
    x = [(y.new_zeros([0] * element_ndim) if y.ndim == 1 and y.nelement() == 0 else y) for y in x]
    if any(y.ndim != x[0].ndim for y in x):
        raise ValueError('All items have to have the same number of dimensions!')
    if pad_size is None:
        pad_dims = [max(y.shape[dim] for y in x if len(y) > 0) for dim in range(x[0].ndim)]
    else:
        if any(len(pad_size) != y.ndim for y in x):
            raise ValueError('Pad size must contain target size for all dimensions.')
        pad_dims = pad_size
    N = len(x)
    x_padded = x[0].new_full((N, *pad_dims), pad_value)
    for i, y in enumerate(x):
        if len(y) > 0:
            slices = i, *(slice(0, y.shape[dim]) for dim in range(y.ndim))
            x_padded[slices] = y
    return x_padded


def padded_to_list(x: torch.Tensor, split_size: Union[Sequence[int], Sequence[Sequence[int]], None]=None):
    """
    Transforms a padded tensor of shape (N, S_1, S_2, ..., S_D) into a list
    of N tensors of shape:
    - (Si_1, Si_2, ..., Si_D) where (Si_1, Si_2, ..., Si_D) is specified in split_size(i)
    - or (S_1, S_2, ..., S_D) if split_size is None
    - or (Si_1, S_2, ..., S_D) if split_size(i) is an integer.

    Args:
      x: tensor
      split_size: optional 1D or 2D list/tuple of ints defining the number of
        items for each tensor.

    Returns:
      x_list: a list of tensors sharing the memory with the input.
    """
    x_list = list(x.unbind(0))
    if split_size is None:
        return x_list
    N = len(split_size)
    if x.shape[0] != N:
        raise ValueError('Split size must be of same length as inputs first dimension')
    for i in range(N):
        if isinstance(split_size[i], int):
            x_list[i] = x_list[i][:split_size[i]]
        else:
            slices = tuple(slice(0, s) for s in split_size[i])
            x_list[i] = x_list[i][slices]
    return x_list


class TexturesVertex(TexturesBase):

    def __init__(self, verts_features: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]) ->None:
        """
        Batched texture representation where each vertex in a mesh
        has a C dimensional feature vector.

        Args:
            verts_features: list of (Vi, C) or (N, V, C) tensor giving a feature
                vector with arbitrary dimensions for each vertex.
        """
        if isinstance(verts_features, (tuple, list)):
            correct_shape = all(torch.is_tensor(v) and v.ndim == 2 for v in verts_features)
            if not correct_shape:
                raise ValueError('Expected verts_features to be a list of tensors of shape (V, C).')
            self._verts_features_list = verts_features
            self._verts_features_padded = None
            self.device = torch.device('cpu')
            self._N = len(verts_features)
            self._num_verts_per_mesh = [len(fv) for fv in verts_features]
            if self._N > 0:
                self.device = verts_features[0].device
        elif torch.is_tensor(verts_features):
            if verts_features.ndim != 3:
                msg = 'Expected verts_features to be of shape (N, V, C); got %r'
                raise ValueError(msg % repr(verts_features.shape))
            self._verts_features_padded = verts_features
            self._verts_features_list = None
            self.device = verts_features.device
            self._N = len(verts_features)
            max_F = verts_features.shape[1]
            self._num_verts_per_mesh = [max_F] * self._N
        else:
            raise ValueError('verts_features must be a tensor or list of tensors')
        self.valid = torch.ones((self._N,), dtype=torch.bool, device=self.device)

    def clone(self) ->'TexturesVertex':
        tex = self.__class__(self.verts_features_padded().clone())
        if self._verts_features_list is not None:
            tex._verts_features_list = [f.clone() for f in self._verts_features_list]
        tex._num_verts_per_mesh = self._num_verts_per_mesh.copy()
        tex.valid = self.valid.clone()
        return tex

    def detach(self) ->'TexturesVertex':
        tex = self.__class__(self.verts_features_padded().detach())
        if self._verts_features_list is not None:
            tex._verts_features_list = [f.detach() for f in self._verts_features_list]
        tex._num_verts_per_mesh = self._num_verts_per_mesh.copy()
        tex.valid = self.valid.detach()
        return tex

    def __getitem__(self, index) ->'TexturesVertex':
        props = ['verts_features_list', '_num_verts_per_mesh']
        new_props = self._getitem(index, props)
        verts_features = new_props['verts_features_list']
        if isinstance(verts_features, list):
            if len(verts_features) == 0:
                verts_features = torch.empty(size=(0, 0, 3), dtype=torch.float32, device=self.verts_features_padded().device)
            new_tex = self.__class__(verts_features=verts_features)
        elif torch.is_tensor(verts_features):
            new_tex = self.__class__(verts_features=[verts_features])
        else:
            raise ValueError('Not all values are provided in the correct format')
        new_tex._num_verts_per_mesh = new_props['_num_verts_per_mesh']
        return new_tex

    def verts_features_padded(self) ->torch.Tensor:
        if self._verts_features_padded is None:
            if self.isempty():
                self._verts_features_padded = torch.zeros((self._N, 0, 3, 0), dtype=torch.float32, device=self.device)
            else:
                self._verts_features_padded = list_to_padded(self._verts_features_list, pad_value=0.0)
        return self._verts_features_padded

    def verts_features_list(self) ->List[torch.Tensor]:
        if self._verts_features_list is None:
            if self.isempty():
                self._verts_features_list = [torch.empty((0, 3), dtype=torch.float32, device=self.device)] * self._N
            else:
                self._verts_features_list = padded_to_list(self._verts_features_padded, split_size=self._num_verts_per_mesh)
        return self._verts_features_list

    def verts_features_packed(self) ->torch.Tensor:
        if self.isempty():
            return torch.zeros((self._N, 3, 0), dtype=torch.float32, device=self.device)
        verts_features_list = self.verts_features_list()
        return list_to_packed(verts_features_list)[0]

    def extend(self, N: int) ->'TexturesVertex':
        new_props = self._extend(N, ['verts_features_padded', '_num_verts_per_mesh'])
        new_tex = self.__class__(verts_features=new_props['verts_features_padded'])
        new_tex._num_verts_per_mesh = new_props['_num_verts_per_mesh']
        return new_tex

    def sample_textures(self, fragments, faces_packed=None) ->torch.Tensor:
        """
        Determine the color for each rasterized face. Interpolate the colors for
        vertices which form the face using the barycentric coordinates.
        Args:
            fragments:
                The outputs of rasterization. From this we use

                - pix_to_face: LongTensor of shape (N, H, W, K) specifying the indices
                of the faces (in the packed representation) which
                overlap each pixel in the image.
                - barycentric_coords: FloatTensor of shape (N, H, W, K, 3) specifying
                the barycentric coordinates of each pixel
                relative to the faces (in the packed
                representation) which overlap the pixel.

        Returns:
            texels: An texture per pixel of shape (N, H, W, K, C).
            There will be one C dimensional value for each element in
            fragments.pix_to_face.
        """
        verts_features_packed = self.verts_features_packed()
        faces_verts_features = verts_features_packed[faces_packed]
        texels = interpolate_face_attributes(fragments.pix_to_face, fragments.bary_coords, faces_verts_features)
        return texels

    def submeshes(self, vertex_ids_list: List[List[torch.LongTensor]], faces_ids_list: List[List[torch.LongTensor]]) ->'TexturesVertex':
        """
        Extract a sub-texture for use in a submesh.

        If the meshes batch corresponding to this TexturesVertex contains
        `n = len(vertex_ids_list)` meshes, then self.verts_features_list()
        will be of length n. After submeshing, we obtain a batch of
        `k = sum(len(v) for v in vertex_ids_list` submeshes (see Meshes.submeshes). This
        function creates a corresponding TexturesVertex object with `verts_features_list`
        of length `k`.

        Args:
            vertex_ids_list: A list of length equal to self.verts_features_list. Each
                element is a LongTensor listing the vertices that the submesh keeps in
                each respective mesh.

            face_ids_list: Not used when submeshing TexturesVertex.

        Returns:
            A TexturesVertex in which verts_features_list has length
            sum(len(vertices) for vertices in vertex_ids_list). Each element contains
            vertex features corresponding to the subset of vertices in that submesh.
        """
        if len(vertex_ids_list) != len(self.verts_features_list()):
            raise IndexError('verts_features_list must be of the same length as vertex_ids_list.')
        sub_features = []
        for vertex_ids, features in zip(vertex_ids_list, self.verts_features_list()):
            for vertex_ids_submesh in vertex_ids:
                sub_features.append(features[vertex_ids_submesh])
        return self.__class__(sub_features)

    def faces_verts_textures_packed(self, faces_packed=None) ->torch.Tensor:
        """
        Samples texture from each vertex and for each face in the mesh.
        For N meshes with {Fi} number of faces, it returns a
        tensor of shape sum(Fi)x3xC (C = 3 for RGB).
        You can use the utils function in structures.utils to convert the
        packed representation to a list or padded.
        """
        verts_features_packed = self.verts_features_packed()
        faces_verts_features = verts_features_packed[faces_packed]
        return faces_verts_features

    def join_batch(self, textures: List['TexturesVertex']) ->'TexturesVertex':
        """
        Join the list of textures given by `textures` to
        self to create a batch of textures. Return a new
        TexturesVertex object with the combined textures.

        Args:
            textures: List of TexturesVertex objects

        Returns:
            new_tex: TexturesVertex object with the combined
            textures from self and the list `textures`.
        """
        tex_types_same = all(isinstance(tex, TexturesVertex) for tex in textures)
        if not tex_types_same:
            raise ValueError('All textures must be of type TexturesVertex.')
        verts_features_list = []
        verts_features_list += self.verts_features_list()
        num_verts_per_mesh = self._num_verts_per_mesh.copy()
        for tex in textures:
            verts_features_list += tex.verts_features_list()
            num_verts_per_mesh += tex._num_verts_per_mesh
        new_tex = self.__class__(verts_features=verts_features_list)
        new_tex._num_verts_per_mesh = num_verts_per_mesh
        return new_tex

    def join_scene(self) ->'TexturesVertex':
        """
        Return a new TexturesVertex amalgamating the batch.
        """
        return self.__class__(verts_features=[torch.cat(self.verts_features_list())])

    def check_shapes(self, batch_size: int, max_num_verts: int, max_num_faces: int) ->bool:
        """
        Check if the dimensions of the verts features match that of the mesh verts
        """
        return self.verts_features_padded().shape[:-1] == (batch_size, max_num_verts)


def gouraud_shading(meshes, fragments, lights, cameras, materials) ->torch.Tensor:
    """
    Apply per vertex shading. First compute the vertex illumination by applying
    ambient, diffuse and specular lighting. If vertex color is available,
    combine the ambient and diffuse vertex illumination with the vertex color
    and add the specular component to determine the vertex shaded color.
    Then interpolate the vertex shaded colors using the barycentric coordinates
    to get a color per pixel.

    Gouraud shading is only supported for meshes with texture type `TexturesVertex`.
    This is because the illumination is applied to the vertex colors.

    Args:
        meshes: Batch of meshes
        fragments: Fragments named tuple with the outputs of rasterization
        lights: Lights class containing a batch of lights parameters
        cameras: Cameras class containing a batch of cameras parameters
        materials: Materials class containing a batch of material properties

    Returns:
        colors: (N, H, W, K, 3)
    """
    if not isinstance(meshes.textures, TexturesVertex):
        raise ValueError('Mesh textures must be an instance of TexturesVertex')
    faces = meshes.faces_packed()
    verts = meshes.verts_packed()
    verts_normals = meshes.verts_normals_packed()
    verts_colors = meshes.textures.verts_features_packed()
    vert_to_mesh_idx = meshes.verts_packed_to_mesh_idx()
    if len(meshes) > 1:
        lights = lights.clone().gather_props(vert_to_mesh_idx)
        cameras = cameras.clone().gather_props(vert_to_mesh_idx)
        materials = materials.clone().gather_props(vert_to_mesh_idx)
    ambient, diffuse, specular = _apply_lighting(verts, verts_normals, lights, cameras, materials)
    verts_colors_shaded = verts_colors * (ambient + diffuse) + specular
    face_colors = verts_colors_shaded[faces]
    colors = interpolate_face_attributes(fragments.pix_to_face, fragments.bary_coords, face_colors)
    return colors


def flat_shading(meshes, fragments, lights, cameras, materials, texels) ->torch.Tensor:
    """
    Apply per face shading. Use the average face position and the face normals
    to compute the ambient, diffuse and specular lighting. Apply the ambient
    and diffuse color to the pixel color and add the specular component to
    determine the final pixel color.

    Args:
        meshes: Batch of meshes
        fragments: Fragments named tuple with the outputs of rasterization
        lights: Lights class containing a batch of lights parameters
        cameras: Cameras class containing a batch of cameras parameters
        materials: Materials class containing a batch of material properties
        texels: texture per pixel of shape (N, H, W, K, 3)

    Returns:
        colors: (N, H, W, K, 3)
    """
    verts = meshes.verts_packed()
    faces = meshes.faces_packed()
    face_normals = meshes.faces_normals_packed()
    faces_verts = verts[faces]
    face_coords = faces_verts.mean(dim=-2)
    mask = fragments.pix_to_face == -1
    pix_to_face = fragments.pix_to_face.clone()
    pix_to_face[mask] = 0
    N, H, W, K = pix_to_face.shape
    idx = pix_to_face.view(N * H * W * K, 1).expand(N * H * W * K, 3)
    pixel_coords = face_coords.gather(0, idx).view(N, H, W, K, 3)
    pixel_coords[mask] = 0.0
    pixel_normals = face_normals.gather(0, idx).view(N, H, W, K, 3)
    pixel_normals[mask] = 0.0
    ambient, diffuse, specular = _apply_lighting(pixel_coords, pixel_normals, lights, cameras, materials)
    colors = (ambient + diffuse) * texels + specular
    return colors


class _SigmoidAlphaBlend(torch.autograd.Function):

    @staticmethod
    def forward(ctx, dists, pix_to_face, sigma):
        alphas = _C.sigmoid_alpha_blend(dists, pix_to_face, sigma)
        ctx.save_for_backward(dists, pix_to_face, alphas)
        ctx.sigma = sigma
        return alphas

    @staticmethod
    def backward(ctx, grad_alphas):
        dists, pix_to_face, alphas = ctx.saved_tensors
        sigma = ctx.sigma
        grad_dists = _C.sigmoid_alpha_blend_backward(grad_alphas, alphas, dists, pix_to_face, sigma)
        return grad_dists, None, None


_sigmoid_alpha = _SigmoidAlphaBlend.apply


def sigmoid_alpha_blend(colors, fragments, blend_params: BlendParams) ->torch.Tensor:
    """
    Silhouette blending to return an RGBA image
      - **RGB** - choose color of the closest point.
      - **A** - blend based on the 2D distance based probability map [1].

    Args:
        colors: (N, H, W, K, 3) RGB color for each of the top K faces per pixel.
        fragments: the outputs of rasterization. From this we use
            - pix_to_face: LongTensor of shape (N, H, W, K) specifying the indices
              of the faces (in the packed representation) which
              overlap each pixel in the image.
            - dists: FloatTensor of shape (N, H, W, K) specifying
              the 2D euclidean distance from the center of each pixel
              to each of the top K overlapping faces.

    Returns:
        RGBA pixel_colors: (N, H, W, 4)

    [1] Liu et al, 'Soft Rasterizer: A Differentiable Renderer for Image-based
        3D Reasoning', ICCV 2019
    """
    N, H, W, K = fragments.pix_to_face.shape
    pixel_colors = torch.ones((N, H, W, 4), dtype=colors.dtype, device=colors.device)
    pixel_colors[..., :3] = colors[..., 0, :]
    alpha = _sigmoid_alpha(fragments.dists, fragments.pix_to_face, blend_params.sigma)
    pixel_colors[..., 3] = alpha
    return pixel_colors


_BatchFloatType = Union[float, Sequence[float], torch.Tensor]


class FoVPerspectiveCameras(CamerasBase):
    """
    A class which stores a batch of parameters to generate a batch of
    projection matrices by specifying the field of view.
    The definitions of the parameters follow the OpenGL perspective camera.

    The extrinsics of the camera (R and T matrices) can also be set in the
    initializer or passed in to `get_full_projection_transform` to get
    the full transformation from world -> ndc.

    The `transform_points` method calculates the full world -> ndc transform
    and then applies it to the input points.

    The transforms can also be returned separately as Transform3d objects.

    * Setting the Aspect Ratio for Non Square Images *

    If the desired output image size is non square (i.e. a tuple of (H, W) where H != W)
    the aspect ratio needs special consideration: There are two aspect ratios
    to be aware of:
        - the aspect ratio of each pixel
        - the aspect ratio of the output image
    The `aspect_ratio` setting in the FoVPerspectiveCameras sets the
    pixel aspect ratio. When using this camera with the differentiable rasterizer
    be aware that in the rasterizer we assume square pixels, but allow
    variable image aspect ratio (i.e rectangle images).

    In most cases you will want to set the camera `aspect_ratio=1.0`
    (i.e. square pixels) and only vary the output image dimensions in pixels
    for rasterization.
    """
    _FIELDS = 'K', 'znear', 'zfar', 'aspect_ratio', 'fov', 'R', 'T', 'degrees'
    _SHARED_FIELDS = 'degrees',

    def __init__(self, znear: _BatchFloatType=1.0, zfar: _BatchFloatType=100.0, aspect_ratio: _BatchFloatType=1.0, fov: _BatchFloatType=60.0, degrees: bool=True, R: torch.Tensor=_R, T: torch.Tensor=_T, K: Optional[torch.Tensor]=None, device: Device='cpu') ->None:
        """

        Args:
            znear: near clipping plane of the view frustrum.
            zfar: far clipping plane of the view frustrum.
            aspect_ratio: aspect ratio of the image pixels.
                1.0 indicates square pixels.
            fov: field of view angle of the camera.
            degrees: bool, set to True if fov is specified in degrees.
            R: Rotation matrix of shape (N, 3, 3)
            T: Translation matrix of shape (N, 3)
            K: (optional) A calibration matrix of shape (N, 4, 4)
                If provided, don't need znear, zfar, fov, aspect_ratio, degrees
            device: Device (as str or torch.device)
        """
        super().__init__(device=device, znear=znear, zfar=zfar, aspect_ratio=aspect_ratio, fov=fov, R=R, T=T, K=K)
        self.degrees = degrees

    def compute_projection_matrix(self, znear, zfar, fov, aspect_ratio, degrees: bool) ->torch.Tensor:
        """
        Compute the calibration matrix K of shape (N, 4, 4)

        Args:
            znear: near clipping plane of the view frustrum.
            zfar: far clipping plane of the view frustrum.
            fov: field of view angle of the camera.
            aspect_ratio: aspect ratio of the image pixels.
                1.0 indicates square pixels.
            degrees: bool, set to True if fov is specified in degrees.

        Returns:
            torch.FloatTensor of the calibration matrix with shape (N, 4, 4)
        """
        K = torch.zeros((self._N, 4, 4), device=self.device, dtype=torch.float32)
        ones = torch.ones(self._N, dtype=torch.float32, device=self.device)
        if degrees:
            fov = np.pi / 180 * fov
        if not torch.is_tensor(fov):
            fov = torch.tensor(fov, device=self.device)
        tanHalfFov = torch.tan(fov / 2)
        max_y = tanHalfFov * znear
        min_y = -max_y
        max_x = max_y * aspect_ratio
        min_x = -max_x
        z_sign = 1.0
        K[:, 0, 0] = 2.0 * znear / (max_x - min_x)
        K[:, 1, 1] = 2.0 * znear / (max_y - min_y)
        K[:, 0, 2] = (max_x + min_x) / (max_x - min_x)
        K[:, 1, 2] = (max_y + min_y) / (max_y - min_y)
        K[:, 3, 2] = z_sign * ones
        K[:, 2, 2] = z_sign * zfar / (zfar - znear)
        K[:, 2, 3] = -(zfar * znear) / (zfar - znear)
        return K

    def get_projection_transform(self, **kwargs) ->Transform3d:
        """
        Calculate the perspective projection matrix with a symmetric
        viewing frustrum. Use column major order.
        The viewing frustrum will be projected into ndc, s.t.
        (max_x, max_y) -> (+1, +1)
        (min_x, min_y) -> (-1, -1)

        Args:
            **kwargs: parameters for the projection can be passed in as keyword
                arguments to override the default values set in `__init__`.

        Return:
            a Transform3d object which represents a batch of projection
            matrices of shape (N, 4, 4)

        .. code-block:: python

            h1 = (max_y + min_y)/(max_y - min_y)
            w1 = (max_x + min_x)/(max_x - min_x)
            tanhalffov = tan((fov/2))
            s1 = 1/tanhalffov
            s2 = 1/(tanhalffov * (aspect_ratio))

            # To map z to the range [0, 1] use:
            f1 =  far / (far - near)
            f2 = -(far * near) / (far - near)

            # Projection matrix
            K = [
                    [s1,   0,   w1,   0],
                    [0,   s2,   h1,   0],
                    [0,    0,   f1,  f2],
                    [0,    0,    1,   0],
            ]
        """
        K = kwargs.get('K', self.K)
        if K is not None:
            if K.shape != (self._N, 4, 4):
                msg = 'Expected K to have shape of (%r, 4, 4)'
                raise ValueError(msg % self._N)
        else:
            K = self.compute_projection_matrix(kwargs.get('znear', self.znear), kwargs.get('zfar', self.zfar), kwargs.get('fov', self.fov), kwargs.get('aspect_ratio', self.aspect_ratio), kwargs.get('degrees', self.degrees))
        transform = Transform3d(matrix=K.transpose(1, 2).contiguous(), device=self.device)
        return transform

    def unproject_points(self, xy_depth: torch.Tensor, world_coordinates: bool=True, scaled_depth_input: bool=False, **kwargs) ->torch.Tensor:
        """>!
        FoV cameras further allow for passing depth in world units
        (`scaled_depth_input=False`) or in the [0, 1]-normalized units
        (`scaled_depth_input=True`)

        Args:
            scaled_depth_input: If `True`, assumes the input depth is in
                the [0, 1]-normalized units. If `False` the input depth is in
                the world units.
        """
        if world_coordinates:
            to_ndc_transform = self.get_full_projection_transform()
        else:
            to_ndc_transform = self.get_projection_transform()
        if scaled_depth_input:
            xy_sdepth = xy_depth
        else:
            K_matrix = self.get_projection_transform(**kwargs.copy()).get_matrix()
            unsqueeze_shape = [1] * xy_depth.dim()
            unsqueeze_shape[0] = K_matrix.shape[0]
            f1 = K_matrix[:, 2, 2].reshape(unsqueeze_shape)
            f2 = K_matrix[:, 3, 2].reshape(unsqueeze_shape)
            sdepth = (f1 * xy_depth[..., 2:3] + f2) / xy_depth[..., 2:3]
            xy_sdepth = torch.cat((xy_depth[..., 0:2], sdepth), dim=-1)
        unprojection_transform = to_ndc_transform.inverse()
        return unprojection_transform.transform_points(xy_sdepth)

    def is_perspective(self):
        return True

    def in_ndc(self):
        return True


def _compute_occlusion_layers(q_depth: torch.Tensor) ->torch.Tensor:
    """
    For each splatting pixel, decide whether it splats from a background, surface, or
    foreground depth relative to the splatted pixel. See unit tests in
    test_splatter_blend for some enlightening examples.

    Args:
        q_depth: (N, H, W, K) tensor of z-values of the splatted pixels.

    Returns:
        occlusion_layers: (N, H, W, 9) long tensor. Each of the 9 values corresponds to
            one of the nine splatting directions ([-1, -1], [-1, 0], ..., [1,
            1]). The value at nhwd (where d is the splatting direction) is 0 if
            the splat in direction d is on the same surface level as the pixel at
            hw. The value is negative if the splat is in the background (occluded
            by another splat above it that is at the same surface level as the
            pixel splatted on), and the value is positive if the splat is in the
            foreground.
    """
    N, H, W, K = q_depth.shape
    q_depth = q_depth.permute(0, 3, 1, 2)
    p_depth = F.unfold(q_depth, kernel_size=3, padding=1)
    q_depth = q_depth.view(N, K, 1, H, W)
    p_depth = p_depth.view(N, K, 9, H, W)
    qtop_to_p_zdist = torch.abs(p_depth - q_depth[:, 0:1])
    qtop_to_p_closest_zdist, qtop_to_p_closest_id = qtop_to_p_zdist.min(dim=1)
    ptop_to_q_zdist = torch.abs(p_depth[:, 0:1] - q_depth)
    ptop_to_q_closest_zdist, ptop_to_q_closest_id = ptop_to_q_zdist.min(dim=1)
    occlusion_offsets = torch.where(ptop_to_q_closest_zdist < qtop_to_p_closest_zdist, -ptop_to_q_closest_id, qtop_to_p_closest_id)
    occlusion_layers = occlusion_offsets.permute((0, 2, 3, 1))
    return occlusion_layers


def _compute_splatted_colors_and_weights(occlusion_layers: torch.Tensor, splat_colors_and_weights: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Accumulate splatted colors in background, surface and foreground occlusion buffers.

    Args:
        occlusion_layers: (N, H, W, 9) tensor. See _compute_occlusion_layers.
        splat_colors_and_weights: (N, H, W, K, 9, 5) tensor. See _offset_splats.

    Returns:
        splatted_colors: (N, H, W, 4, 3) tensor. Last dimension corresponds to back-
            ground, surface, and foreground splat colors.
        splatted_weights: (N, H, W, 1, 3) tensor. Last dimension corresponds to back-
            ground, surface, and foreground splat weights and is used for normalization.

    """
    N, H, W, K, _, _ = splat_colors_and_weights.shape
    layer_ids = torch.arange(K, device=splat_colors_and_weights.device).view(1, 1, 1, K, 1)
    occlusion_layers = occlusion_layers.view(N, H, W, 1, 9)
    occlusion_layer_mask = torch.stack([occlusion_layers > layer_ids, occlusion_layers == layer_ids, occlusion_layers < layer_ids], dim=5).float()
    splatted_colors_and_weights = torch.bmm(splat_colors_and_weights.permute(0, 1, 2, 5, 3, 4).reshape((N * H * W, 5, K * 9)), occlusion_layer_mask.reshape((N * H * W, K * 9, 3))).reshape((N, H, W, 5, 3))
    return splatted_colors_and_weights[..., :4, :], splatted_colors_and_weights[..., 4:5, :]


def _get_splat_kernel_normalization(offsets: torch.Tensor, sigma: float=0.5):
    if sigma <= 0.0:
        raise ValueError('Only positive standard deviations make sense.')
    epsilon = 0.05
    normalization_constant = torch.exp(-(offsets ** 2).sum(dim=1) / (2 * sigma ** 2)).sum()
    return (1 + epsilon) / normalization_constant


def _compute_splatting_colors_and_weights(pixel_coords_screen: torch.Tensor, colors: torch.Tensor, sigma: float, offsets: torch.Tensor) ->torch.Tensor:
    """
    For each center pixel q, compute the splatting weights of its surrounding nine spla-
    tting pixels p, as well as their splatting colors (which are just their colors re-
    weighted by the splatting weights).

    Args:
        pixel_coords_screen: (N, H, W, K, 2) tensor of pixel screen coords.
        colors: (N, H, W, K, 4) RGBA tensor of pixel colors.
        sigma: splatting kernel variance.
        offsets: (9, 2) tensor computed by _precompute, indicating the nine
            splatting directions ([-1, -1], ..., [1, 1]).

    Returns:
        splat_colors_and_weights: (N, H, W, K, 9, 5) tensor.
            splat_colors_and_weights[..., :4] corresponds to the splatting colors, and
            splat_colors_and_weights[..., 4:5] to the splatting weights. The "9" di-
            mension corresponds to the nine splatting directions.
    """
    N, H, W, K, C = colors.shape
    splat_kernel_normalization = _get_splat_kernel_normalization(offsets, sigma)
    q_to_px_center = (torch.floor(pixel_coords_screen[..., :2]) - pixel_coords_screen[..., :2] + 0.5).view((N, H, W, K, 1, 2))
    dist2_p_q = torch.sum((q_to_px_center + offsets) ** 2, dim=5)
    splat_weights = torch.exp(-dist2_p_q / (2 * sigma ** 2))
    alpha = colors[..., 3:4]
    splat_weights = (alpha * splat_kernel_normalization * splat_weights).unsqueeze(5)
    splat_colors = splat_weights * colors.unsqueeze(4)
    return torch.cat([splat_colors, splat_weights], dim=5)


def _normalize_and_compose_all_layers(background_color: torch.Tensor, splatted_colors_per_occlusion_layer: torch.Tensor, splatted_weights_per_occlusion_layer: torch.Tensor) ->torch.Tensor:
    """
    Normalize each bg/surface/fg buffer by its weight, and compose.

    Args:
        background_color: (3) RGB tensor.
        splatter_colors_per_occlusion_layer: (N, H, W, 4, 3) RGBA tensor, last dimension
            corresponds to foreground, surface, and background splatting.
        splatted_weights_per_occlusion_layer: (N, H, W, 1, 3) weight tensor.

    Returns:
        output_colors: (N, H, W, 4) RGBA tensor.
    """
    device = splatted_colors_per_occlusion_layer.device
    normalization_scales = 1.0 / torch.maximum(splatted_weights_per_occlusion_layer, torch.tensor([1.0], device=device))
    normalized_splatted_colors = splatted_colors_per_occlusion_layer * normalization_scales
    output_colors = torch.cat([background_color, torch.tensor([0.0], device=device)])
    for occlusion_layer_id in (-1, -2, -3):
        alpha = normalized_splatted_colors[..., 3:4, occlusion_layer_id]
        output_colors = normalized_splatted_colors[..., occlusion_layer_id] + (1.0 - alpha) * output_colors
    return output_colors


def _offset_splats(splat_colors_and_weights: torch.Tensor, crop_ids_h: torch.Tensor, crop_ids_w: torch.Tensor) ->torch.Tensor:
    """
    Pad splatting colors and weights so that tensor locations/coordinates are aligned
    with the splatting directions. For example, say we have an example input Red channel
    splat_colors_and_weights[n, :, :, k, direction=0, channel=0] equal to
       .1  .2  .3
       .4  .5  .6
       .7  .8  .9
    the (h, w) entry indicates that pixel n, h, w, k splats the given color in direction
    equal to 0, which corresponds to offsets[0] = (-1, -1). Note that this is the x-y
    direction, not h-w. This function pads and crops this array to
        0   0   0
       .2  .3   0
       .5  .6   0
    which indicates, for example, that:
        * There is no pixel splatting in direction (-1, -1) whose splat lands on pixel
          h=w=0.
        * There is a pixel splatting in direction (-1, -1) whose splat lands on the pi-
          xel h=1, w=0, and that pixel's splatting color is .2.
        * There is a pixel splatting in direction (-1, -1) whose splat lands on the pi-
          xel h=2, w=1, and that pixel's splatting color is .6.

    Args:
        *splat_colors_and_weights*: (N, H, W, K, 9, 5) tensor of colors and weights,
        where dim=-2 corresponds to the splatting directions/offsets.
        *crop_ids_h*: (N, H, W+2, K, 9, 5) precomputed tensor used for padding within
            torch.gather. See _precompute for more info.
        *crop_ids_w*: (N, H, W, K, 9, 5) precomputed tensor used for padding within
            torch.gather. See _precompute for more info.


    Returns:
        *splat_colors_and_weights*: (N, H, W, K, 9, 5) tensor.
    """
    N, H, W, K, _, _ = splat_colors_and_weights.shape
    splat_colors_and_weights = F.pad(splat_colors_and_weights, [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0])
    splat_colors_and_weights = torch.gather(splat_colors_and_weights, dim=1, index=crop_ids_h)
    splat_colors_and_weights = torch.gather(splat_colors_and_weights, dim=2, index=crop_ids_w)
    return splat_colors_and_weights


def _precompute(input_shape: Tuple[int, int, int, int], device: Device) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Precompute padding and offset constants that won't change for a given NHWK shape.

    Args:
        input_shape: Tuple indicating N (batch size), H, W (image size) and K (number of
            intersections) output by the rasterizer.
        device: Device to store the tensors on.

    returns:
        crop_ids_h: An (N, H, W+2, K, 9, 5) tensor, used during splatting to offset the
            p-pixels (splatting pixels) in one of the 9 splatting directions within a
            call to torch.gather. See comments and offset_splats for details.
        crop_ids_w: An (N, H, W, K, 9, 5) tensor, used similarly to crop_ids_h.
        offsets: A (1, 1, 1, 1, 9, 2) tensor (shaped so for broadcasting) containing va-
            lues [-1, -1], [-1, 0], [-1, 1], [0, -1], ..., [1, 1] which correspond to
            the nine splatting directions.
    """
    N, H, W, K = input_shape
    crop_ids_h = (torch.arange(0, H, device=device).view(1, H, 1, 1, 1, 1) + torch.tensor([0, 1, 2, 0, 1, 2, 0, 1, 2], device=device).view(1, 1, 1, 1, 9, 1)).expand(N, H, W + 2, K, 9, 5)
    crop_ids_w = (torch.arange(0, W, device=device).view(1, 1, W, 1, 1, 1) + torch.tensor([0, 0, 0, 1, 1, 1, 2, 2, 2], device=device).view(1, 1, 1, 1, 9, 1)).expand(N, H, W, K, 9, 5)
    offsets = torch.tensor(list(itertools.product((-1, 0, 1), repeat=2)), dtype=torch.long, device=device)
    return crop_ids_h, crop_ids_w, offsets


def _prepare_pixels_and_colors(pixel_coords_cameras: torch.Tensor, colors: torch.Tensor, cameras: FoVPerspectiveCameras, background_mask: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Project pixel coords into the un-inverted screen frame of reference, and set
    background pixel z-values to 1.0 and alphas to 0.0.

    Args:
        pixel_coords_cameras: (N, H, W, K, 3) float tensor.
        colors: (N, H, W, K, 3) float tensor.
        cameras: PyTorch3D cameras, for now we assume FoVPerspectiveCameras.
        background_mask: (N, H, W, K) boolean tensor.

    Returns:
        pixel_coords_screen: (N, H, W, K, 3) float tensor. Background pixels have
            x=y=z=1.0.
        colors: (N, H, W, K, 4). Alpha is set to 1 for foreground pixels and 0 for back-
            ground pixels.
    """
    N, H, W, K, C = colors.shape
    pixel_coords_screen = cameras.transform_points_screen(pixel_coords_cameras.view([N, -1, 3]), image_size=(H, W), with_xyflip=False).reshape(pixel_coords_cameras.shape)
    colors = torch.cat([colors, torch.ones_like(colors[..., :1])], dim=-1)
    pixel_coords_screen[background_mask] = 1.0
    colors[background_mask] = 0.0
    return pixel_coords_screen, colors


class SplatterBlender(torch.nn.Module):

    def __init__(self, input_shape: Tuple[int, int, int, int], device):
        """
        A splatting blender. See `forward` docs for details of the splatting mechanism.

        Args:
            input_shape: Tuple (N, H, W, K) indicating the batch size, image height,
                image width, and number of rasterized layers. Used to precompute
                constant tensors that do not change as long as this tuple is unchanged.
        """
        super().__init__()
        self.crop_ids_h, self.crop_ids_w, self.offsets = _precompute(input_shape, device)

    def to(self, device):
        self.offsets = self.offsets
        self.crop_ids_h = self.crop_ids_h
        self.crop_ids_w = self.crop_ids_w
        super()

    def forward(self, colors: torch.Tensor, pixel_coords_cameras: torch.Tensor, cameras: FoVPerspectiveCameras, background_mask: torch.Tensor, blend_params: BlendParams) ->torch.Tensor:
        """
        RGB blending using splatting, as proposed in [0].

        Args:
            colors: (N, H, W, K, 3) tensor of RGB colors at each h, w pixel location for
                K intersection layers.
            pixel_coords_cameras: (N, H, W, K, 3) tensor of pixel coordinates in the
                camera frame of reference. It is *crucial* that these are computed by
                interpolating triangle vertex positions using barycentric coordinates --
                this allows gradients to travel through pixel_coords_camera back to the
                vertex positions.
            cameras: Cameras object used to project pixel_coords_cameras screen coords.
            background_mask: (N, H, W, K, 3) boolean tensor, True for bg pixels. A pixel
                is considered "background" if no mesh triangle projects to it. This is
                typically computed by the rasterizer.
            blend_params: BlendParams, from which we use sigma (splatting kernel
                variance) and background_color.

        Returns:
            output_colors: (N, H, W, 4) tensor of RGBA values. The alpha layer is set to
                fully transparent in the background.

        [0] Cole, F. et al., "Differentiable Surface Rendering via Non-differentiable
            Sampling".
        """
        pixel_coords_screen, colors = _prepare_pixels_and_colors(pixel_coords_cameras, colors, cameras, background_mask)
        occlusion_layers = _compute_occlusion_layers(pixel_coords_screen[..., 2:3].squeeze(dim=-1))
        splat_colors_and_weights = _compute_splatting_colors_and_weights(pixel_coords_screen, colors, blend_params.sigma, self.offsets)
        splat_colors_and_weights = _offset_splats(splat_colors_and_weights, self.crop_ids_h, self.crop_ids_w)
        splatted_colors_per_occlusion_layer, splatted_weights_per_occlusion_layer = _compute_splatted_colors_and_weights(occlusion_layers, splat_colors_and_weights)
        output_colors = _normalize_and_compose_all_layers(_get_background_color(blend_params, colors.device), splatted_colors_per_occlusion_layer, splatted_weights_per_occlusion_layer)
        return output_colors


class FoVOrthographicCameras(CamerasBase):
    """
    A class which stores a batch of parameters to generate a batch of
    projection matrices by specifying the field of view.
    The definitions of the parameters follow the OpenGL orthographic camera.
    """
    _FIELDS = 'K', 'znear', 'zfar', 'R', 'T', 'max_y', 'min_y', 'max_x', 'min_x', 'scale_xyz'

    def __init__(self, znear: _BatchFloatType=1.0, zfar: _BatchFloatType=100.0, max_y: _BatchFloatType=1.0, min_y: _BatchFloatType=-1.0, max_x: _BatchFloatType=1.0, min_x: _BatchFloatType=-1.0, scale_xyz=((1.0, 1.0, 1.0),), R: torch.Tensor=_R, T: torch.Tensor=_T, K: Optional[torch.Tensor]=None, device: Device='cpu'):
        """

        Args:
            znear: near clipping plane of the view frustrum.
            zfar: far clipping plane of the view frustrum.
            max_y: maximum y coordinate of the frustrum.
            min_y: minimum y coordinate of the frustrum.
            max_x: maximum x coordinate of the frustrum.
            min_x: minimum x coordinate of the frustrum
            scale_xyz: scale factors for each axis of shape (N, 3).
            R: Rotation matrix of shape (N, 3, 3).
            T: Translation of shape (N, 3).
            K: (optional) A calibration matrix of shape (N, 4, 4)
                If provided, don't need znear, zfar, max_y, min_y, max_x, min_x, scale_xyz
            device: torch.device or string.

        Only need to set min_x, max_x, min_y, max_y for viewing frustrums
        which are non symmetric about the origin.
        """
        super().__init__(device=device, znear=znear, zfar=zfar, max_y=max_y, min_y=min_y, max_x=max_x, min_x=min_x, scale_xyz=scale_xyz, R=R, T=T, K=K)

    def compute_projection_matrix(self, znear, zfar, max_x, min_x, max_y, min_y, scale_xyz) ->torch.Tensor:
        """
        Compute the calibration matrix K of shape (N, 4, 4)

        Args:
            znear: near clipping plane of the view frustrum.
            zfar: far clipping plane of the view frustrum.
            max_x: maximum x coordinate of the frustrum.
            min_x: minimum x coordinate of the frustrum
            max_y: maximum y coordinate of the frustrum.
            min_y: minimum y coordinate of the frustrum.
            scale_xyz: scale factors for each axis of shape (N, 3).
        """
        K = torch.zeros((self._N, 4, 4), dtype=torch.float32, device=self.device)
        ones = torch.ones(self._N, dtype=torch.float32, device=self.device)
        z_sign = +1.0
        K[:, 0, 0] = 2.0 / (max_x - min_x) * scale_xyz[:, 0]
        K[:, 1, 1] = 2.0 / (max_y - min_y) * scale_xyz[:, 1]
        K[:, 0, 3] = -(max_x + min_x) / (max_x - min_x)
        K[:, 1, 3] = -(max_y + min_y) / (max_y - min_y)
        K[:, 3, 3] = ones
        K[:, 2, 2] = z_sign * (1.0 / (zfar - znear)) * scale_xyz[:, 2]
        K[:, 2, 3] = -znear / (zfar - znear)
        return K

    def get_projection_transform(self, **kwargs) ->Transform3d:
        """
        Calculate the orthographic projection matrix.
        Use column major order.

        Args:
            **kwargs: parameters for the projection can be passed in to
                      override the default values set in __init__.
        Return:
            a Transform3d object which represents a batch of projection
               matrices of shape (N, 4, 4)

        .. code-block:: python

            scale_x = 2 / (max_x - min_x)
            scale_y = 2 / (max_y - min_y)
            scale_z = 2 / (far-near)
            mid_x = (max_x + min_x) / (max_x - min_x)
            mix_y = (max_y + min_y) / (max_y - min_y)
            mid_z = (far + near) / (far - near)

            K = [
                    [scale_x,        0,         0,  -mid_x],
                    [0,        scale_y,         0,  -mix_y],
                    [0,              0,  -scale_z,  -mid_z],
                    [0,              0,         0,       1],
            ]
        """
        K = kwargs.get('K', self.K)
        if K is not None:
            if K.shape != (self._N, 4, 4):
                msg = 'Expected K to have shape of (%r, 4, 4)'
                raise ValueError(msg % self._N)
        else:
            K = self.compute_projection_matrix(kwargs.get('znear', self.znear), kwargs.get('zfar', self.zfar), kwargs.get('max_x', self.max_x), kwargs.get('min_x', self.min_x), kwargs.get('max_y', self.max_y), kwargs.get('min_y', self.min_y), kwargs.get('scale_xyz', self.scale_xyz))
        transform = Transform3d(matrix=K.transpose(1, 2).contiguous(), device=self.device)
        return transform

    def unproject_points(self, xy_depth: torch.Tensor, world_coordinates: bool=True, scaled_depth_input: bool=False, **kwargs) ->torch.Tensor:
        """>!
        FoV cameras further allow for passing depth in world units
        (`scaled_depth_input=False`) or in the [0, 1]-normalized units
        (`scaled_depth_input=True`)

        Args:
            scaled_depth_input: If `True`, assumes the input depth is in
                the [0, 1]-normalized units. If `False` the input depth is in
                the world units.
        """
        if world_coordinates:
            to_ndc_transform = self.get_full_projection_transform(**kwargs.copy())
        else:
            to_ndc_transform = self.get_projection_transform(**kwargs.copy())
        if scaled_depth_input:
            xy_sdepth = xy_depth
        else:
            K = self.get_projection_transform(**kwargs).get_matrix()
            unsqueeze_shape = [1] * K.dim()
            unsqueeze_shape[0] = K.shape[0]
            mid_z = K[:, 3, 2].reshape(unsqueeze_shape)
            scale_z = K[:, 2, 2].reshape(unsqueeze_shape)
            scaled_depth = scale_z * xy_depth[..., 2:3] + mid_z
            xy_sdepth = torch.cat((xy_depth[..., :2], scaled_depth), dim=-1)
        unprojection_transform = to_ndc_transform.inverse()
        return unprojection_transform.transform_points(xy_sdepth)

    def is_perspective(self):
        return False

    def in_ndc(self):
        return True


def _torch_to_opengl(torch_tensor, cuda_context, cuda_buffer):
    cuda_context.push()
    mapping_obj = cuda_buffer.map()
    data_ptr, sz = mapping_obj.device_ptr_and_size()
    cuda_copy = cuda.Memcpy2D()
    cuda_copy.set_src_device(torch_tensor.data_ptr())
    cuda_copy.set_dst_device(data_ptr)
    cuda_copy.width_in_bytes = cuda_copy.src_pitch = cuda_copy.dst_ptch = torch_tensor.shape[1] * 4
    cuda_copy.height = torch_tensor.shape[0]
    cuda_copy(False)
    mapping_obj.unmap()
    cuda_context.pop()


fragment_shader = """
#version 430

in layout(location = 0) vec2 bary_coords;
in layout(location = 1) float depth;
in layout(location = 2) float p2f;


out vec4 bary_depth_p2f;

void main() {
    bary_depth_p2f = vec4(bary_coords, depth, round(p2f));
}
"""


geometry_shader = """
#version 430

layout (points) in;
layout (triangle_strip, max_vertices = 3) out;

out layout (location = 0) vec2 bary_coords;
out layout (location = 1) float depth;
out layout (location = 2) float p2f;

layout(binding=0) buffer triangular_mesh { float mesh_buffer[]; };

uniform mat4 perspective_projection;

vec3 get_vertex_position(int vertex_index) {
    int offset = gl_PrimitiveIDIn * 9 + vertex_index * 3;
    return vec3(
        mesh_buffer[offset],
        mesh_buffer[offset + 1],
        mesh_buffer[offset + 2]
    );
}

void main() {
    vec3 positions[3] = {
        get_vertex_position(0),
        get_vertex_position(1),
        get_vertex_position(2)
    };
    vec4 projected_vertices[3] = {
        perspective_projection * vec4(positions[0], 1.0),
        perspective_projection * vec4(positions[1], 1.0),
        perspective_projection * vec4(positions[2], 1.0)
    };

    for (int i = 0; i < 3; ++i) {
        gl_Position = projected_vertices[i];
        bary_coords = vec2(i==0 ? 1.0 : 0.0, i==1 ? 1.0 : 0.0);
        // At the moment, we output depth as the distance from the image plane in
        // view coordinates -- NOT distance along the camera ray.
        depth = positions[i][2];
        p2f = gl_PrimitiveIDIn;
        EmitVertex();
    }
    EndPrimitive();
}
"""


EGL_PLATFORM_DEVICE_EXT = 12607


def _egl_convert_to_int_array(egl_attributes):
    """
    Convert a Python dict of EGL attributes into an array of ints (some of which are
    special EGL ints.

    Args:
        egl_attributes: A dict where keys are EGL attributes, and values are their vals.

    Returns:
        A c-list of length 2 * len(egl_attributes) + 1, of the form [key1, val1, ...,
        keyN, valN, EGL_NONE]
    """
    attributes_list = sum(([k, v] for k, v in egl_attributes.items()), []) + [egl.EGL_NONE]
    return (egl.EGLint * len(attributes_list))(*attributes_list)


EGL_CUDA_DEVICE_NV = 12858


def _get_cuda_device(requested_device_id: int):
    """
    Find an EGL device with a given CUDA device ID.

    Args:
        requested_device_id: The desired CUDA device ID, e.g. "1" for "cuda:1".

    Returns:
        EGL device with the desired CUDA ID.
    """
    num_devices = egl.EGLint()
    if not egl.eglQueryDevicesEXT(0, None, ctypes.pointer(num_devices)) or num_devices.value < 1:
        raise RuntimeError('EGL requires a system that supports at least one device.')
    devices = (egl.EGLDeviceEXT * num_devices.value)()
    if not egl.eglQueryDevicesEXT(num_devices.value, devices, ctypes.pointer(num_devices)) or num_devices.value < 1:
        raise RuntimeError('EGL sees no available devices.')
    if len(devices) < requested_device_id + 1:
        raise ValueError(f'Device {requested_device_id} not available. Found only {len(devices)} devices.')
    for device in devices:
        available_device_id = egl.EGLAttrib(ctypes.c_int(-1))
        egl.eglQueryDeviceAttribEXT(device, EGL_CUDA_DEVICE_NV, available_device_id)
        if available_device_id.contents.value == requested_device_id:
            return device
    raise ValueError(f'Found {len(devices)} CUDA devices, but none with CUDA id {requested_device_id}.')


def _get_egl_config(egl_dpy, surface_type):
    """
    Get an EGL config with reasonable settings (for use with MeshRasterizerOpenGL).

    Args:
        egl_dpy: An EGL display constant (int).
        surface_type: An EGL surface_type int.

    Returns:
        An EGL config object.

    Throws:
        ValueError if the desired config is not available or invalid.
    """
    egl_config_dict = {egl.EGL_RED_SIZE: 8, egl.EGL_GREEN_SIZE: 8, egl.EGL_BLUE_SIZE: 8, egl.EGL_ALPHA_SIZE: 8, egl.EGL_DEPTH_SIZE: 24, egl.EGL_STENCIL_SIZE: egl.EGL_DONT_CARE, egl.EGL_RENDERABLE_TYPE: egl.EGL_OPENGL_BIT, egl.EGL_SURFACE_TYPE: surface_type}
    egl_config_array = _egl_convert_to_int_array(egl_config_dict)
    egl_config = egl.EGLConfig()
    num_configs = egl.EGLint()
    if not egl.eglChooseConfig(egl_dpy, egl_config_array, ctypes.pointer(egl_config), 1, ctypes.pointer(num_configs)) or num_configs.value == 0:
        raise ValueError('Invalid EGL config.')
    return egl_config


def _init_cuda_context(device_id: int=0):
    """
    Initialize a pycuda context on a chosen device.

    Args:
        device_id: int, specifies which GPU to use.

    Returns:
        A pycuda Context.
    """
    device = cuda.Device(device_id)
    cuda_context = device.make_context()
    return cuda_context


class _DeviceContextStore:
    """
    DeviceContextStore provides thread-safe storage for EGL and pycuda contexts. It
    should not be used directly. opengl_utils instantiates a module-global variable
    called opengl_utils.global_device_context_store. MeshRasterizerOpenGL uses this
    store to avoid unnecessary context creation and destruction.

    The EGL/CUDA contexts are not meant to be created and destroyed all the time,
    and having multiple on a single device can be troublesome. Intended use is entirely
    transparent to the user::

        rasterizer1 = MeshRasterizerOpenGL(...some args...)
        mesh1 = load_mesh_on_cuda_0()

        # Now rasterizer1 will request EGL/CUDA contexts from
        # global_device_context_store on cuda:0, and since there aren't any, the
        # store will create new ones.
        rasterizer1.rasterize(mesh1)

        # rasterizer2 also needs EGL & CUDA contexts. But global_context_store
        # already has them for cuda:0. Instead of creating new contexts, the store
        # will tell rasterizer2 to use them.
        rasterizer2 = MeshRasterizerOpenGL(dcs)
        rasterize2.rasterize(mesh1)

        # When rasterizer1 needs to render on cuda:1, the store will create new contexts.
        mesh2 = load_mesh_on_cuda_1()
        rasterizer1.rasterize(mesh2)

    """

    def __init__(self):
        cuda.init()
        self._cuda_contexts = {}
        self._egl_contexts = {}
        self._context_data = {}
        self._lock = threading.Lock()
        self.max_egl_width = 2048
        self.max_egl_height = 2048

    def get_cuda_context(self, device):
        """
        Return a pycuda's CUDA context on a given CUDA device. If we have not created
        such a context yet, create a new one and store it in a dict. The context is
        popped (you need to call context.push() to start using it). This function
        is thread-safe.

        Args:
            device: A torch.device.

        Returns: A pycuda context corresponding to the given device.
        """
        cuda_device_id = device.index
        with self._lock:
            if cuda_device_id not in self._cuda_contexts:
                self._cuda_contexts[cuda_device_id] = _init_cuda_context(cuda_device_id)
                self._cuda_contexts[cuda_device_id].pop()
            return self._cuda_contexts[cuda_device_id]

    def get_egl_context(self, device):
        """
        Return an EGL context on a given CUDA device. If we have not created such a
        context yet, create a new one and store it in a dict. The context if not current
        (you should use the `with egl_context.active_and_locked:` context manager when
        you need it to be current). This function is thread-safe.

        Args:
            device: A torch.device.

        Returns: An EGLContext on the requested device. The context will have size
            self.max_egl_width and self.max_egl_height.
        """
        cuda_device_id = device.index
        with self._lock:
            egl_context = self._egl_contexts.get(cuda_device_id, None)
            if egl_context is None:
                self._egl_contexts[cuda_device_id] = EGLContext(self.max_egl_width, self.max_egl_height, cuda_device_id)
            return self._egl_contexts[cuda_device_id]

    def set_context_data(self, device, value):
        """
        Set arbitrary data in a per-device dict.

        This function is intended for storing precompiled OpenGL objects separately for
        EGL contexts on different devices. Each such context needs a separate compiled
        OpenGL program, but (in case e.g. of MeshRasterizerOpenGL) there's no need to
        re-compile it each time we move the rasterizer to the same device repeatedly,
        as it happens when using DataParallel.

        Args:
            device: A torch.device
            value: An arbitrary Python object.
        """
        cuda_device_id = device.index
        self._context_data[cuda_device_id] = value

    def get_context_data(self, device):
        """
        Get arbitrary data in a per-device dict. See set_context_data for more detail.

        Args:
            device: A torch.device

        Returns:
            The most recent object stored using set_context_data.
        """
        cuda_device_id = device.index
        return self._context_data.get(cuda_device_id, None)

    def release(self):
        """
        Release all CUDA and EGL contexts.
        """
        for context in self._cuda_contexts.values():
            context.detach()
        for context in self._egl_contexts.values():
            context.release()


vertex_shader = """
// The vertex shader does nothing.
#version 430

void main() { }
"""


class _OpenGLMachinery:
    """
    A class holding OpenGL machinery used by MeshRasterizerOpenGL.
    """

    def __init__(self, max_faces: int=10000000) ->None:
        self.max_faces = max_faces
        self.program = None
        self.egl_context = None
        self.cuda_context = None
        self.perspective_projection_uniform = None
        self.mesh_buffer_object = None
        self.vao = None
        self.fbo = None
        self.cuda_buffer = None

    def __call__(self, meshes_gl_ndc: Meshes, projection_matrix: torch.Tensor, image_size: Tuple[int, int]) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Rasterize a batch of meshes, using a given batch of projection matrices and
        image size.

        Args:
            meshes_gl_ndc: A Meshes object, with vertices in the OpenGL NDC convention.
            projection_matrix: A 3x3 camera projection matrix, or a tensor of projection
                matrices equal in length to the number of meshes in meshes_gl_ndc.
            image_size: Image size to rasterize. Must be smaller than the max height and
                width stored in global_device_context_store.

        Returns:
            pix_to_faces: A BHW1 tensor of ints, filled with -1 where no face projects
                to a given pixel.
            bary_coords: A BHW3 float tensor, filled with -1 where no face projects to
                a given pixel.
            zbuf: A BHW1 float tensor, filled with 1 where no face projects to a given
                pixel. NOTE: this zbuf uses the opengl zbuf convention, where the z-vals
                are between 0 (at projection plane) and 1 (at clipping distance), and
                are a non-linear function of the depth values of the camera ray inter-
                sections.
        """
        self.initialize_device_data(meshes_gl_ndc.device)
        with self.egl_context.active_and_locked():
            if projection_matrix.shape[0] == 1:
                self._projection_matrix_to_opengl(projection_matrix)
            pix_to_faces = []
            bary_coords = []
            zbufs = []
            for mesh_id, mesh in enumerate(meshes_gl_ndc):
                pix_to_face, bary_coord, zbuf = self._rasterize_mesh(mesh, image_size, projection_matrix=projection_matrix[mesh_id] if projection_matrix.shape[0] > 1 else None)
                pix_to_faces.append(pix_to_face)
                bary_coords.append(bary_coord)
                zbufs.append(zbuf)
        return torch.cat(pix_to_faces, dim=0), torch.cat(bary_coords, dim=0), torch.cat(zbufs, dim=0)

    def initialize_device_data(self, device) ->None:
        """
        Initialize data specific to a GPU device: the EGL and CUDA contexts, the OpenGL
        program, as well as various buffer and array objects used to communicate with
        OpenGL.

        Args:
            device: A torch.device.
        """
        self.egl_context = global_device_context_store.get_egl_context(device)
        self.cuda_context = global_device_context_store.get_cuda_context(device)
        if global_device_context_store.get_context_data(device) is None:
            with self.egl_context.active_and_locked():
                self.program = self._compile_and_link_gl_program()
                self._set_up_gl_program_properties(self.program)
                self.perspective_projection_uniform, self.mesh_buffer_object, self.vao, self.fbo = self._prepare_persistent_opengl_objects(self.program, self.max_faces)
                self.cuda_context.push()
                self.cuda_buffer = pycuda.gl.RegisteredBuffer(int(self.mesh_buffer_object), pycuda.gl.graphics_map_flags.WRITE_DISCARD)
                self.cuda_context.pop()
            global_device_context_store.set_context_data(device, (self.program, self.perspective_projection_uniform, self.mesh_buffer_object, self.vao, self.fbo, self.cuda_buffer))
        self.program, self.perspective_projection_uniform, self.mesh_buffer_object, self.vao, self.fbo, self.cuda_buffer = global_device_context_store.get_context_data(device)

    def release(self) ->None:
        """
        Release CUDA and OpenGL resources.
        """
        torch.cuda.synchronize()
        self.cuda_context.synchronize()
        self.cuda_context.push()
        self.cuda_buffer.unregister()
        self.cuda_context.pop()
        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)
        gl.glDeleteFramebuffers(1, [self.fbo])
        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)
        del self.fbo
        gl.glBindBufferBase(gl.GL_SHADER_STORAGE_BUFFER, 0, self.mesh_buffer_object)
        gl.glDeleteBuffers(1, [self.mesh_buffer_object])
        gl.glBindBufferBase(gl.GL_SHADER_STORAGE_BUFFER, 0, 0)
        del self.mesh_buffer_object
        gl.glDeleteProgram(self.program)
        self.egl_context.release()

    def _projection_matrix_to_opengl(self, projection_matrix: torch.Tensor) ->None:
        """
        Transfer a torch projection matrix to OpenGL.

        Args:
            projection matrix: A 3x3 float tensor.
        """
        gl.glUseProgram(self.program)
        gl.glUniformMatrix4fv(self.perspective_projection_uniform, 1, gl.GL_FALSE, projection_matrix.detach().flatten().cpu().numpy().astype(np.float32))
        gl.glUseProgram(0)

    def _rasterize_mesh(self, mesh: Meshes, image_size: Tuple[int, int], projection_matrix: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Rasterize a single mesh using OpenGL.

        Args:
            mesh: A Meshes object, containing a single mesh only.
            projection_matrix: A 3x3 camera projection matrix, or a tensor of projection
                matrices equal in length to the number of meshes in meshes_gl_ndc.
            image_size: Image size to rasterize. Must be smaller than the max height and
                width stored in global_device_context_store.

        Returns:
            pix_to_faces: A 1HW1 tensor of ints, filled with -1 where no face projects
                to a given pixel.
            bary_coords: A 1HW3 float tensor, filled with -1 where no face projects to
                a given pixel.
            zbuf: A 1HW1 float tensor, filled with 1 where no face projects to a given
                pixel. NOTE: this zbuf uses the opengl zbuf convention, where the z-vals
                are between 0 (at projection plane) and 1 (at clipping distance), and
                are a non-linear function of the depth values of the camera ray inter-
                sections.
        """
        height, width = image_size
        verts_packed = mesh.verts_packed().detach()
        faces_packed = mesh.faces_packed().detach()
        face_verts = verts_packed[faces_packed].reshape(-1, 9)
        _torch_to_opengl(face_verts, self.cuda_context, self.cuda_buffer)
        if projection_matrix is not None:
            self._projection_matrix_to_opengl(projection_matrix)
        gl.glUseProgram(self.program)
        gl.glViewport(0, 0, width, height)
        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)
        gl.glClearColor(-1.0, -1.0, -1.0, -1.0)
        gl.glClearDepth(1.0)
        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)
        gl.glBindVertexArray(self.vao)
        gl.glDrawArrays(gl.GL_POINTS, 0, len(face_verts))
        gl.glBindVertexArray(0)
        bary_depth_p2f_gl = gl.glReadPixels(0, 0, width, height, gl.GL_RGBA, gl.GL_FLOAT)
        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)
        gl.glUseProgram(0)
        bary_depth_p2f = torch.frombuffer(bary_depth_p2f_gl, dtype=torch.float).reshape(1, height, width, 1, -1)
        barycentric_coords = torch.cat([bary_depth_p2f[..., :2], 1.0 - bary_depth_p2f[..., 0:1] - bary_depth_p2f[..., 1:2]], dim=-1)
        barycentric_coords = torch.where(barycentric_coords == 3, -1, barycentric_coords)
        depth = bary_depth_p2f[..., 2:3].squeeze(-1)
        pix_to_face = bary_depth_p2f[..., -1].long()
        return pix_to_face, barycentric_coords, depth

    @staticmethod
    def _compile_and_link_gl_program():
        """
        Compile the vertex, geometry, and fragment shaders and link them into an OpenGL
        program. The shader sources are strongly inspired by https://github.com/tensorflow/
        graphics/blob/master/tensorflow_graphics/rendering/opengl/rasterization_backend.py.

        Returns:
            An OpenGL program for mesh rasterization.
        """
        program = gl.glCreateProgram()
        shader_objects = []
        for shader_string, shader_type in zip([vertex_shader, geometry_shader, fragment_shader], [gl.GL_VERTEX_SHADER, gl.GL_GEOMETRY_SHADER, gl.GL_FRAGMENT_SHADER]):
            shader_objects.append(gl.glCreateShader(shader_type))
            gl.glShaderSource(shader_objects[-1], shader_string)
            gl.glCompileShader(shader_objects[-1])
            status = gl.glGetShaderiv(shader_objects[-1], gl.GL_COMPILE_STATUS)
            if status == gl.GL_FALSE:
                gl.glDeleteShader(shader_objects[-1])
                gl.glDeleteProgram(program)
                error_msg = gl.glGetShaderInfoLog(shader_objects[-1]).decode('utf-8')
                raise RuntimeError(f'Compilation failure:\n {error_msg}')
            gl.glAttachShader(program, shader_objects[-1])
            gl.glDeleteShader(shader_objects[-1])
        gl.glLinkProgram(program)
        status = gl.glGetProgramiv(program, gl.GL_LINK_STATUS)
        if status == gl.GL_FALSE:
            gl.glDeleteProgram(program)
            error_msg = gl.glGetProgramInfoLog(program)
            raise RuntimeError(f'Link failure:\n {error_msg}')
        return program

    @staticmethod
    def _set_up_gl_program_properties(program) ->None:
        """
        Set basic OpenGL program properties: disable blending, enable depth testing,
        and disable face culling.
        """
        gl.glUseProgram(program)
        gl.glDisable(gl.GL_BLEND)
        gl.glEnable(gl.GL_DEPTH_TEST)
        gl.glDisable(gl.GL_CULL_FACE)
        gl.glUseProgram(0)

    @staticmethod
    def _prepare_persistent_opengl_objects(program, max_faces: int):
        """
        Prepare OpenGL objects that we want to persist between rasterizations.

        Args:
            program: The OpenGL program the resources will be tied to.
            max_faces: Max number of faces of any mesh we will rasterize.

        Returns:
            perspective_projection_uniform: An OpenGL object pointing to a location of
                the perspective projection matrix in OpenGL memory.
            mesh_buffer_object: An OpenGL object pointing to the location of the mesh
                buffer object in OpenGL memory.
            vao: The OpenGL input array object.
            fbo: The OpenGL output framebuffer.

        """
        gl.glUseProgram(program)
        perspective_projection_uniform = gl.glGetUniformLocation(program, 'perspective_projection')
        mesh_buffer_object = gl.glGenBuffers(1)
        gl.glBindBufferBase(gl.GL_SHADER_STORAGE_BUFFER, 0, mesh_buffer_object)
        gl.glBufferData(gl.GL_SHADER_STORAGE_BUFFER, max_faces * 9 * 4, np.zeros((max_faces, 9), dtype=np.float32), gl.GL_DYNAMIC_COPY)
        vao = gl.glGenVertexArrays(1)
        MAX_EGL_WIDTH = global_device_context_store.max_egl_width
        MAX_EGL_HEIGHT = global_device_context_store.max_egl_height
        color_buffer = gl.glGenRenderbuffers(1)
        gl.glBindRenderbuffer(gl.GL_RENDERBUFFER, color_buffer)
        gl.glRenderbufferStorage(gl.GL_RENDERBUFFER, gl.GL_RGBA32F, MAX_EGL_WIDTH, MAX_EGL_HEIGHT)
        gl.glBindRenderbuffer(gl.GL_RENDERBUFFER, 0)
        depth_buffer = gl.glGenRenderbuffers(1)
        gl.glBindRenderbuffer(gl.GL_RENDERBUFFER, depth_buffer)
        gl.glRenderbufferStorage(gl.GL_RENDERBUFFER, gl.GL_DEPTH_COMPONENT, MAX_EGL_WIDTH, MAX_EGL_HEIGHT)
        gl.glBindRenderbuffer(gl.GL_RENDERBUFFER, 0)
        fbo = gl.glGenFramebuffers(1)
        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, fbo)
        gl.glFramebufferRenderbuffer(gl.GL_FRAMEBUFFER, gl.GL_COLOR_ATTACHMENT0, gl.GL_RENDERBUFFER, color_buffer)
        gl.glFramebufferRenderbuffer(gl.GL_FRAMEBUFFER, gl.GL_DEPTH_ATTACHMENT, gl.GL_RENDERBUFFER, depth_buffer)
        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)
        gl.glUseProgram(0)
        return perspective_projection_uniform, mesh_buffer_object, vao, fbo


def _check_cameras(cameras) ->None:
    if cameras is None:
        msg = 'Cameras must be specified either at initialization             or in the forward pass of MeshRasterizer'
        raise ValueError(msg)
    if type(cameras).__name__ in {'PerspectiveCameras', 'OrthographicCameras'}:
        raise ValueError('MeshRasterizerOpenGL only works with FoVPerspectiveCameras and FoVOrthographicCameras, which are OpenGL compatible.')


def _check_raster_settings(raster_settings) ->None:
    if raster_settings.faces_per_pixel > 1:
        warnings.warn('MeshRasterizerOpenGL currently works only with one face per pixel.')
    if raster_settings.cull_backfaces:
        warnings.warn('MeshRasterizerOpenGL cannot cull backfaces yet, rasterizing without culling.')
    if raster_settings.cull_to_frustum:
        warnings.warn('MeshRasterizerOpenGL cannot cull to frustum yet, rasterizing without culling.')
    if raster_settings.z_clip_value is not None:
        raise NotImplementedError('MeshRasterizerOpenGL cannot do z-clipping yet.')
    if raster_settings.perspective_correct is False:
        raise ValueError('MeshRasterizerOpenGL always uses perspective-correct interpolation.')


def _convert_meshes_to_gl_ndc(meshes_world: Meshes, image_size: Tuple[int, int], camera, **kwargs) ->Meshes:
    """
    Convert a batch of world-coordinate meshes to GL NDC coordinates.

    Args:
        meshes_world: Meshes in the world coordinate system.
        image_size: Image height and width, used to modify mesh coords for rendering in
            non-rectangular images. OpenGL will expand anything within the [-1, 1] NDC
            range to fit the width and height of the screen, so we will squeeze the NDCs
            appropriately if rendering a rectangular image.
        camera: FoV cameras.
        kwargs['R'], kwargs['T']: If present, used to define the world-view transform.
    """
    height, width = image_size
    verts_ndc = camera.get_world_to_view_transform(**kwargs).compose(camera.get_ndc_camera_transform(**kwargs)).transform_points(meshes_world.verts_padded(), eps=None)
    verts_ndc[..., 0] = -verts_ndc[..., 0]
    verts_ndc[..., 1] = -verts_ndc[..., 1]
    dtype, device = verts_ndc.dtype, verts_ndc.device
    if height > width:
        verts_ndc = verts_ndc * torch.tensor([1, width / height, 1], dtype=dtype, device=device)
    elif width > height:
        verts_ndc = verts_ndc * torch.tensor([height / width, 1, 1], dtype=dtype, device=device)
    meshes_gl_ndc = meshes_world.update_padded(new_verts_padded=verts_ndc)
    return meshes_gl_ndc


def _parse_and_verify_image_size(image_size: Union[Tuple[int, int], int]) ->Tuple[int, int]:
    """
    Parse image_size as a tuple of ints. Throw ValueError if the size is incompatible
    with the maximum renderable size as set in global_device_context_store.
    """
    height, width = parse_image_size(image_size)
    max_h = global_device_context_store.max_egl_height
    max_w = global_device_context_store.max_egl_width
    if height > max_h or width > max_w:
        raise ValueError(f'Max rasterization size is height={max_h}, width={max_w}. Cannot raster an image of size {height}, {width}. You can change max allowed rasterization size by modifying the MAX_EGL_HEIGHT and MAX_EGL_WIDTH environment variables.')
    return height, width


class _CompositeNormWeightedSumPoints(torch.autograd.Function):
    """
    Composite features within a z-buffer using normalized weighted sum. Given a z-buffer
    with corresponding features and weights, these values are accumulated
    according to their weights such that depth is ignored; the weights are used to
    perform a weighted sum.

    Concretely this means:
        weighted_fs[b,c,i,j] =
         sum_k alphas[b,k,i,j] * features[c,pointsidx[b,k,i,j]] / sum_k alphas[b,k,i,j]

    Args:
        features: Packed Tensor of shape (C, P) giving the features of each point.
        alphas: float32 Tensor of shape (N, points_per_pixel, image_size,
            image_size) giving the weight of each point in the z-buffer.
            Values should be in the interval [0, 1].
        pointsidx: int32 Tensor of shape (N, points_per_pixel, image_size, image_size)
            giving the indices of the nearest points at each pixel, sorted in z-order.
            Concretely pointsidx[n, k, y, x] = p means that features[:, p] is the
            feature of the kth closest point (along the z-direction) to pixel (y, x) in
            batch element n. This is weighted by alphas[n, k, y, x].

    Returns:
        weighted_fs: Tensor of shape (N, C, image_size, image_size)
            giving the accumulated features at each point.
    """

    @staticmethod
    def forward(ctx, features, alphas, points_idx):
        pt_cld = _C.accum_weightedsumnorm(features, alphas, points_idx)
        ctx.save_for_backward(features.clone(), alphas.clone(), points_idx.clone())
        return pt_cld

    @staticmethod
    def backward(ctx, grad_output):
        grad_features = None
        grad_alphas = None
        grad_points_idx = None
        features, alphas, points_idx = ctx.saved_tensors
        grad_features, grad_alphas = _C.accum_weightedsumnorm_backward(grad_output, features, alphas, points_idx)
        return grad_features, grad_alphas, grad_points_idx, None


def norm_weighted_sum(pointsidx, alphas, pt_clds) ->torch.Tensor:
    """
    Composite features within a z-buffer using normalized weighted sum. Given a z-buffer
    with corresponding features and weights, these values are accumulated
    according to their weights such that depth is ignored; the weights are used to
    perform a weighted sum.

    Concretely this means:
        weighted_fs[b,c,i,j] =
         sum_k alphas[b,k,i,j] * features[c,pointsidx[b,k,i,j]] / sum_k alphas[b,k,i,j]

    Args:
        pt_clds: Packed feature tensor of shape (C, P) giving the features of each point
            (can use RGB for example).
        alphas: float32 Tensor of shape (N, points_per_pixel, image_size,
            image_size) giving the weight of each point in the z-buffer.
            Values should be in the interval [0, 1].
        pointsidx: int32 Tensor of shape (N, points_per_pixel, image_size, image_size)
            giving the indices of the nearest points at each pixel, sorted in z-order.
            Concretely pointsidx[n, k, y, x] = p means that features[:, p] is the
            feature of the kth closest point (along the z-direction) to pixel (y, x) in
            batch element n. This is weighted by alphas[n, k, y, x].

    Returns:
        Combined features: Tensor of shape (N, C, image_size, image_size)
            giving the accumulated features at each point.
    """
    return _CompositeNormWeightedSumPoints.apply(pt_clds, alphas, pointsidx)


class NormWeightedCompositor(nn.Module):
    """
    Accumulate points using a normalized weighted sum.
    """

    def __init__(self, background_color: Optional[Union[Tuple, List, torch.Tensor]]=None) ->None:
        super().__init__()
        self.background_color = background_color

    def forward(self, fragments, alphas, ptclds, **kwargs) ->torch.Tensor:
        background_color = kwargs.get('background_color', self.background_color)
        images = norm_weighted_sum(fragments, alphas, ptclds)
        if background_color is not None:
            return _add_background_color_to_images(fragments, images, background_color)
        return images


def axis_angle_to_quaternion(axis_angle: torch.Tensor) ->torch.Tensor:
    """
    Convert rotations given as axis/angle to quaternions.

    Args:
        axis_angle: Rotations given as a vector in axis angle form,
            as a tensor of shape (..., 3), where the magnitude is
            the angle turned anticlockwise in radians around the
            vector's direction.

    Returns:
        quaternions with real part first, as tensor of shape (..., 4).
    """
    angles = torch.norm(axis_angle, p=2, dim=-1, keepdim=True)
    half_angles = angles * 0.5
    eps = 1e-06
    small_angles = angles.abs() < eps
    sin_half_angles_over_angles = torch.empty_like(angles)
    sin_half_angles_over_angles[~small_angles] = torch.sin(half_angles[~small_angles]) / angles[~small_angles]
    sin_half_angles_over_angles[small_angles] = 0.5 - angles[small_angles] * angles[small_angles] / 48
    quaternions = torch.cat([torch.cos(half_angles), axis_angle * sin_half_angles_over_angles], dim=-1)
    return quaternions


def quaternion_to_matrix(quaternions: torch.Tensor) ->torch.Tensor:
    """
    Convert rotations given as quaternions to rotation matrices.

    Args:
        quaternions: quaternions with real part first,
            as tensor of shape (..., 4).

    Returns:
        Rotation matrices as tensor of shape (..., 3, 3).
    """
    r, i, j, k = torch.unbind(quaternions, -1)
    two_s = 2.0 / (quaternions * quaternions).sum(-1)
    o = torch.stack((1 - two_s * (j * j + k * k), two_s * (i * j - k * r), two_s * (i * k + j * r), two_s * (i * j + k * r), 1 - two_s * (i * i + k * k), two_s * (j * k - i * r), two_s * (i * k - j * r), two_s * (j * k + i * r), 1 - two_s * (i * i + j * j)), -1)
    return o.reshape(quaternions.shape[:-1] + (3, 3))


def axis_angle_to_matrix(axis_angle: torch.Tensor) ->torch.Tensor:
    """
    Convert rotations given as axis/angle to rotation matrices.

    Args:
        axis_angle: Rotations given as a vector in axis angle form,
            as a tensor of shape (..., 3), where the magnitude is
            the angle turned anticlockwise in radians around the
            vector's direction.

    Returns:
        Rotation matrices as tensor of shape (..., 3, 3).
    """
    return quaternion_to_matrix(axis_angle_to_quaternion(axis_angle))


def rotation_6d_to_matrix(d6: torch.Tensor) ->torch.Tensor:
    """
    Converts 6D rotation representation by Zhou et al. [1] to rotation matrix
    using Gram--Schmidt orthogonalization per Section B of [1].
    Args:
        d6: 6D rotation representation, of size (*, 6)

    Returns:
        batch of rotation matrices of size (*, 3, 3)

    [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.
    On the Continuity of Rotation Representations in Neural Networks.
    IEEE Conference on Computer Vision and Pattern Recognition, 2019.
    Retrieved from http://arxiv.org/abs/1812.07035
    """
    a1, a2 = d6[..., :3], d6[..., 3:]
    b1 = F.normalize(a1, dim=-1)
    b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1
    b2 = F.normalize(b2, dim=-1)
    b3 = torch.cross(b1, b2, dim=-1)
    return torch.stack((b1, b2, b3), dim=-2)


class OrthographicCameras(CamerasBase):
    """
    A class which stores a batch of parameters to generate a batch of
    transformation matrices using the multi-view geometry convention for
    orthographic camera.

    Parameters for this camera are specified in NDC if `in_ndc` is set to True.
    If parameters are specified in screen space, `in_ndc` must be set to False.
    """
    _FIELDS = 'K', 'R', 'T', 'focal_length', 'principal_point', '_in_ndc', 'image_size'
    _SHARED_FIELDS = '_in_ndc',

    def __init__(self, focal_length: _FocalLengthType=1.0, principal_point=((0.0, 0.0),), R: torch.Tensor=_R, T: torch.Tensor=_T, K: Optional[torch.Tensor]=None, device: Device='cpu', in_ndc: bool=True, image_size: Optional[Union[List, Tuple, torch.Tensor]]=None) ->None:
        """

        Args:
            focal_length: Focal length of the camera in world units.
                A tensor of shape (N, 1) or (N, 2) for
                square and non-square pixels respectively.
            principal_point: xy coordinates of the center of
                the principal point of the camera in pixels.
                A tensor of shape (N, 2).
            in_ndc: True if camera parameters are specified in NDC.
                If False, then camera parameters are in screen space.
            R: Rotation matrix of shape (N, 3, 3)
            T: Translation matrix of shape (N, 3)
            K: (optional) A calibration matrix of shape (N, 4, 4)
                If provided, don't need focal_length, principal_point, image_size
            image_size: (height, width) of image size.
                A tensor of shape (N, 2) or list/tuple. Required for screen cameras.
            device: torch.device or string
        """
        kwargs = {'image_size': image_size} if image_size is not None else {}
        super().__init__(device=device, focal_length=focal_length, principal_point=principal_point, R=R, T=T, K=K, _in_ndc=in_ndc, **kwargs)
        if image_size is not None:
            if (self.image_size < 1).any():
                raise ValueError('Image_size provided has invalid values')
        else:
            self.image_size = None
        if self.focal_length.ndim == 1:
            self.focal_length = self.focal_length[:, None]
        self.focal_length = self.focal_length.expand(-1, 2)

    def get_projection_transform(self, **kwargs) ->Transform3d:
        """
        Calculate the projection matrix using
        the multi-view geometry convention.

        Args:
            **kwargs: parameters for the projection can be passed in as keyword
                arguments to override the default values set in __init__.

        Returns:
            A `Transform3d` object with a batch of `N` projection transforms.

        .. code-block:: python

            fx = focal_length[:,0]
            fy = focal_length[:,1]
            px = principal_point[:,0]
            py = principal_point[:,1]

            K = [
                    [fx,   0,    0,  px],
                    [0,   fy,    0,  py],
                    [0,    0,    1,   0],
                    [0,    0,    0,   1],
            ]
        """
        K = kwargs.get('K', self.K)
        if K is not None:
            if K.shape != (self._N, 4, 4):
                msg = 'Expected K to have shape of (%r, 4, 4)'
                raise ValueError(msg % self._N)
        else:
            K = _get_sfm_calibration_matrix(self._N, self.device, kwargs.get('focal_length', self.focal_length), kwargs.get('principal_point', self.principal_point), orthographic=True)
        transform = Transform3d(matrix=K.transpose(1, 2).contiguous(), device=self.device)
        return transform

    def unproject_points(self, xy_depth: torch.Tensor, world_coordinates: bool=True, from_ndc: bool=False, **kwargs) ->torch.Tensor:
        """
        Args:
            from_ndc: If `False` (default), assumes xy part of input is in
                NDC space if self.in_ndc(), otherwise in screen space. If
                `True`, assumes xy is in NDC space even if the camera
                is defined in screen space.
        """
        if world_coordinates:
            to_camera_transform = self.get_full_projection_transform(**kwargs)
        else:
            to_camera_transform = self.get_projection_transform(**kwargs)
        if from_ndc:
            to_camera_transform = to_camera_transform.compose(self.get_ndc_camera_transform())
        unprojection_transform = to_camera_transform.inverse()
        return unprojection_transform.transform_points(xy_depth)

    def get_principal_point(self, **kwargs) ->torch.Tensor:
        """
        Return the camera's principal point

        Args:
            **kwargs: parameters for the camera extrinsics can be passed in
                as keyword arguments to override the default values
                set in __init__.
        """
        proj_mat = self.get_projection_transform(**kwargs).get_matrix()
        return proj_mat[:, 3, :2]

    def get_ndc_camera_transform(self, **kwargs) ->Transform3d:
        """
        Returns the transform from camera projection space (screen or NDC) to NDC space.
        If the camera is defined already in NDC space, the transform is identity.
        For cameras defined in screen space, we adjust the principal point computation
        which is defined in the image space (commonly) and scale the points to NDC space.

        Important: This transforms assumes PyTorch3D conventions for the input points,
        i.e. +X left, +Y up.
        """
        if self.in_ndc():
            ndc_transform = Transform3d(device=self.device, dtype=torch.float32)
        else:
            pr_point_fix = torch.zeros((self._N, 4, 4), device=self.device, dtype=torch.float32)
            pr_point_fix[:, 0, 0] = 1.0
            pr_point_fix[:, 1, 1] = 1.0
            pr_point_fix[:, 2, 2] = 1.0
            pr_point_fix[:, 3, 3] = 1.0
            pr_point_fix[:, :2, 3] = -2.0 * self.get_principal_point(**kwargs)
            pr_point_fix_transform = Transform3d(matrix=pr_point_fix.transpose(1, 2).contiguous(), device=self.device)
            image_size = kwargs.get('image_size', self.get_image_size())
            screen_to_ndc_transform = get_screen_to_ndc_transform(self, with_xyflip=False, image_size=image_size)
            ndc_transform = pr_point_fix_transform.compose(screen_to_ndc_transform)
        return ndc_transform

    def is_perspective(self):
        return False

    def in_ndc(self):
        return self._in_ndc


def _ensure_float_tensor(val_in, device):
    """Make sure that the value provided is wrapped a PyTorch float tensor."""
    if not isinstance(val_in, torch.Tensor):
        val_out = torch.tensor(val_in, dtype=torch.float32, device=device).reshape((1,))
    else:
        val_out = val_in.to(torch.float32).reshape((1,))
    return val_out


def _opencv_from_cameras_projection(cameras: PerspectiveCameras, image_size: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    R_pytorch3d = cameras.R.clone()
    T_pytorch3d = cameras.T.clone()
    focal_pytorch3d = cameras.focal_length
    p0_pytorch3d = cameras.principal_point
    T_pytorch3d[:, :2] *= -1
    R_pytorch3d[:, :, :2] *= -1
    tvec = T_pytorch3d
    R = R_pytorch3d.permute(0, 2, 1)
    image_size_wh = image_size.flip(dims=(1,))
    scale = image_size_wh.min(dim=1, keepdim=True)[0] / 2.0
    scale = scale.expand(-1, 2)
    c0 = image_size_wh / 2.0
    principal_point = -p0_pytorch3d * scale + c0
    focal_length = focal_pytorch3d * scale
    camera_matrix = torch.zeros_like(R)
    camera_matrix[:, :2, 2] = principal_point
    camera_matrix[:, 2, 2] = 1.0
    camera_matrix[:, 0, 0] = focal_length[:, 0]
    camera_matrix[:, 1, 1] = focal_length[:, 1]
    return R, tvec, camera_matrix


LOGGER = logging.getLogger(__name__)


def matrix_to_rotation_6d(matrix: torch.Tensor) ->torch.Tensor:
    """
    Converts rotation matrices to 6D rotation representation by Zhou et al. [1]
    by dropping the last row. Note that 6D representation is not unique.
    Args:
        matrix: batch of rotation matrices of size (*, 3, 3)

    Returns:
        6D rotation representation, of size (*, 6)

    [1] Zhou, Y., Barnes, C., Lu, J., Yang, J., & Li, H.
    On the Continuity of Rotation Representations in Neural Networks.
    IEEE Conference on Computer Vision and Pattern Recognition, 2019.
    Retrieved from http://arxiv.org/abs/1812.07035
    """
    batch_dim = matrix.size()[:-2]
    return matrix[..., :2, :].clone().reshape(batch_dim + (6,))


def _pulsar_from_opencv_projection(R: torch.Tensor, tvec: torch.Tensor, camera_matrix: torch.Tensor, image_size: torch.Tensor, znear: float=0.1) ->torch.Tensor:
    assert len(camera_matrix.size()) == 3, 'This function requires batched inputs!'
    assert len(R.size()) == 3, 'This function requires batched inputs!'
    assert len(tvec.size()) in (2, 3), 'This function reuqires batched inputs!'
    image_size_wh = image_size.flip(dims=(1,))
    assert torch.all(image_size_wh > 0), 'height and width must be positive but min is: %s' % str(image_size_wh.min().item())
    assert camera_matrix.size(1) == 3 and camera_matrix.size(2) == 3, 'Incorrect camera matrix shape: expected 3x3 but got %dx%d' % (camera_matrix.size(1), camera_matrix.size(2))
    assert R.size(1) == 3 and R.size(2) == 3, 'Incorrect R shape: expected 3x3 but got %dx%d' % (R.size(1), R.size(2))
    if len(tvec.size()) == 2:
        tvec = tvec.unsqueeze(2)
    assert tvec.size(1) == 3 and tvec.size(2) == 1, 'Incorrect tvec shape: expected 3x1 but got %dx%d' % (tvec.size(1), tvec.size(2))
    batch_size = camera_matrix.size(0)
    assert R.size(0) == batch_size, 'Expected R to have batch size %d. Has size %d.' % (batch_size, R.size(0))
    assert tvec.size(0) == batch_size, 'Expected tvec to have batch size %d. Has size %d.' % (batch_size, tvec.size(0))
    image_w = image_size_wh[0, 0]
    image_h = image_size_wh[0, 1]
    assert torch.all(image_size_wh[:, 0] == image_w), 'All images in a batch must have the same width!'
    assert torch.all(image_size_wh[:, 1] == image_h), 'All images in a batch must have the same height!'
    fx = camera_matrix[:, 0, 0].unsqueeze(1)
    fy = camera_matrix[:, 1, 1].unsqueeze(1)
    fx_y = fx / fy
    if torch.any(fx_y > 1.01) or torch.any(fx_y < 0.99):
        LOGGER.warning('Pulsar only supports a single focal lengths. For converting OpenCV focal lengths, we average them for x and y directions. The focal lengths for x and y you provided differ by more than 1%, which means this could introduce a noticeable error.')
    f = (fx + fy) / 2
    focal_length_px = f / image_w
    focal_length = torch.tensor([znear - 1e-05], dtype=torch.float32, device=R.device)
    focal_length = focal_length[None, :].repeat(batch_size, 1)
    sensor_width = focal_length / focal_length_px
    cx = camera_matrix[:, 0, 2].unsqueeze(1)
    cy = camera_matrix[:, 1, 2].unsqueeze(1)
    cx = -(cx - image_w / 2)
    cy = cy - image_h / 2
    param = torch.cat([focal_length, sensor_width, cx, cy], dim=1)
    R_trans = R.permute(0, 2, 1)
    cam_pos = -torch.bmm(R_trans, tvec).squeeze(2)
    cam_rot = matrix_to_rotation_6d(R_trans)
    cam_params = torch.cat([cam_pos, cam_rot, param], dim=1)
    return cam_params


def _pulsar_from_cameras_projection(cameras: PerspectiveCameras, image_size: torch.Tensor) ->torch.Tensor:
    opencv_R, opencv_T, opencv_K = _opencv_from_cameras_projection(cameras, image_size)
    return _pulsar_from_opencv_projection(opencv_R, opencv_T, opencv_K, image_size)


class PulsarPointsRenderer(nn.Module):
    """
    This renderer is a PyTorch3D interface wrapper around the pulsar renderer.

    It provides an interface consistent with PyTorch3D Pointcloud rendering.
    It will extract all necessary information from the rasterizer and compositor
    objects and convert them to the pulsar required format, then invoke rendering
    in the pulsar renderer. All gradients are handled appropriately through the
    wrapper and the wrapper should provide equivalent results to using the pulsar
    renderer directly.
    """

    def __init__(self, rasterizer: PointsRasterizer, compositor: Optional[Union[NormWeightedCompositor, AlphaCompositor]]=None, n_channels: int=3, max_num_spheres: int=int(1000000.0), **kwargs) ->None:
        """
        rasterizer (PointsRasterizer): An object encapsulating rasterization parameters.
        compositor (ignored): Only keeping this for interface consistency. Default: None.
        n_channels (int): The number of channels of the resulting image. Default: 3.
        max_num_spheres (int): The maximum number of spheres intended to render with
            this renderer. Default: 1e6.
        kwargs (Any): kwargs to pass on to the pulsar renderer.
            See `pytorch3d.renderer.points.pulsar.renderer.Renderer` for all options.
        """
        super().__init__()
        self.rasterizer = rasterizer
        if compositor is not None:
            warnings.warn('Creating a `PulsarPointsRenderer` with a compositor object! This object is ignored and just allowed as an argument for interface compatibility.')
        if not isinstance(rasterizer.cameras, (FoVOrthographicCameras, FoVPerspectiveCameras, PerspectiveCameras, OrthographicCameras)):
            raise ValueError('Only FoVPerspectiveCameras, PerspectiveCameras, FoVOrthographicCameras and OrthographicCameras are supported by the pulsar backend.')
        if isinstance(rasterizer.raster_settings.image_size, tuple):
            height, width = rasterizer.raster_settings.image_size
        else:
            width = rasterizer.raster_settings.image_size
            height = rasterizer.raster_settings.image_size
        width = int(width)
        height = int(height)
        max_num_spheres = int(max_num_spheres)
        orthogonal_projection = isinstance(rasterizer.cameras, (FoVOrthographicCameras, OrthographicCameras))
        n_channels = int(n_channels)
        self.renderer = PulsarRenderer(width=width, height=height, max_num_balls=max_num_spheres, orthogonal_projection=orthogonal_projection, right_handed_system=False, n_channels=n_channels, **kwargs)

    def _conf_check(self, point_clouds, kwargs: Dict[str, Any]) ->bool:
        """
        Verify internal configuration state with kwargs and pointclouds.

        This method will raise ValueError's for any inconsistencies found. It
        returns whether an orthogonal projection will be used.
        """
        if 'gamma' not in kwargs.keys():
            raise ValueError('gamma is a required keyword argument for the PulsarPointsRenderer!')
        if len(point_clouds) != len(self.rasterizer.cameras) and len(self.rasterizer.cameras) != 1:
            raise ValueError('The len(point_clouds) must either be equal to len(rasterizer.cameras) or only one camera must be used. len(point_clouds): %d, len(rasterizer.cameras): %d.' % (len(point_clouds), len(self.rasterizer.cameras)))
        orthogonal_projection = isinstance(self.rasterizer.cameras, (FoVOrthographicCameras, OrthographicCameras))
        if orthogonal_projection != self.renderer._renderer.orthogonal:
            raise (ValueError('The camera type can not be changed after renderer initialization! Current camera orthogonal: %r. Original orthogonal: %r.') % (orthogonal_projection, self.renderer._renderer.orthogonal))
        image_size = self.rasterizer.raster_settings.image_size
        if isinstance(image_size, tuple):
            expected_height, expected_width = image_size
        else:
            expected_height = expected_width = image_size
        if expected_width != self.renderer._renderer.width:
            raise ValueError('The rasterizer width can not be changed after renderer initialization! Current width: %s. Original width: %d.' % (expected_width, self.renderer._renderer.width))
        if expected_height != self.renderer._renderer.height:
            raise ValueError('The rasterizer height can not be changed after renderer initialization! Current height: %s. Original height: %d.' % (expected_height, self.renderer._renderer.height))
        return orthogonal_projection

    def _extract_intrinsics(self, orthogonal_projection, kwargs, cloud_idx, device) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, float, float]:
        """
        Translate the camera intrinsics from PyTorch3D format to pulsar format.
        """
        cameras = self.rasterizer.cameras
        if orthogonal_projection:
            focal_length = torch.zeros((1,), dtype=torch.float32)
            if isinstance(cameras, FoVOrthographicCameras):
                znear = kwargs.get('znear', cameras.znear)[cloud_idx]
                zfar = kwargs.get('zfar', cameras.zfar)[cloud_idx]
                max_y = kwargs.get('max_y', cameras.max_y)[cloud_idx]
                min_y = kwargs.get('min_y', cameras.min_y)[cloud_idx]
                max_x = kwargs.get('max_x', cameras.max_x)[cloud_idx]
                min_x = kwargs.get('min_x', cameras.min_x)[cloud_idx]
                if max_y != -min_y:
                    raise ValueError(f'The orthographic camera must be centered around 0. Max is {max_y} and min is {min_y}.')
                if max_x != -min_x:
                    raise ValueError(f'The orthographic camera must be centered around 0. Max is {max_x} and min is {min_x}.')
                if not torch.all(kwargs.get('scale_xyz', cameras.scale_xyz)[cloud_idx] == 1.0):
                    raise ValueError(f"The orthographic camera scale must be ((1.0, 1.0, 1.0),). {kwargs.get('scale_xyz', cameras.scale_xyz)[cloud_idx]}.")
                sensor_width = max_x - min_x
                if not sensor_width > 0.0:
                    raise ValueError(f'The orthographic camera must have positive size! Is: {sensor_width}.')
                principal_point_x, principal_point_y = torch.zeros((1,), dtype=torch.float32), torch.zeros((1,), dtype=torch.float32)
            else:
                focal_length_conf = kwargs.get('focal_length', cameras.focal_length)[cloud_idx]
                if focal_length_conf.numel() == 2 and focal_length_conf[0] * self.renderer._renderer.width - focal_length_conf[1] * self.renderer._renderer.height > 1e-05:
                    raise ValueError('Pulsar only supports a single focal length! Provided: %s.' % str(focal_length_conf))
                if focal_length_conf.numel() == 2:
                    sensor_width = 2.0 / focal_length_conf[0]
                else:
                    if focal_length_conf.numel() != 1:
                        raise ValueError('Focal length not parsable: %s.' % str(focal_length_conf))
                    sensor_width = 2.0 / focal_length_conf
                if 'znear' not in kwargs.keys() or 'zfar' not in kwargs.keys():
                    raise ValueError('pulsar needs znear and zfar values for the OrthographicCameras. Please provide them as keyword argument to the forward method.')
                znear = kwargs['znear'][cloud_idx]
                zfar = kwargs['zfar'][cloud_idx]
                principal_point_x = kwargs.get('principal_point', cameras.principal_point)[cloud_idx][0] * 0.5 * self.renderer._renderer.width
                principal_point_y = kwargs.get('principal_point', cameras.principal_point)[cloud_idx][1] * 0.5 * self.renderer._renderer.height
        elif not isinstance(cameras, PerspectiveCameras):
            znear = kwargs.get('znear', cameras.znear)[cloud_idx]
            zfar = kwargs.get('zfar', cameras.zfar)[cloud_idx]
            focal_length = znear - 1e-06
            afov = kwargs.get('fov', cameras.fov)[cloud_idx]
            if kwargs.get('degrees', cameras.degrees):
                afov *= math.pi / 180.0
            sensor_width = math.tan(afov / 2.0) * 2.0 * focal_length
            if not kwargs.get('aspect_ratio', cameras.aspect_ratio)[cloud_idx] - self.renderer._renderer.width / self.renderer._renderer.height < 1e-06:
                raise ValueError(f"The aspect ratio ({kwargs.get('aspect_ratio', cameras.aspect_ratio)[cloud_idx]}) must agree with the resolution width / height ({self.renderer._renderer.width / self.renderer._renderer.height}).")
            principal_point_x, principal_point_y = torch.zeros((1,), dtype=torch.float32), torch.zeros((1,), dtype=torch.float32)
        else:
            focal_length_conf = kwargs.get('focal_length', cameras.focal_length)[cloud_idx]
            if focal_length_conf.numel() == 2 and focal_length_conf[0] * self.renderer._renderer.width - focal_length_conf[1] * self.renderer._renderer.height > 1e-05:
                raise ValueError('Pulsar only supports a single focal length! Provided: %s.' % str(focal_length_conf))
            if 'znear' not in kwargs.keys() or 'zfar' not in kwargs.keys():
                raise ValueError('pulsar needs znear and zfar values for the PerspectiveCameras. Please provide them as keyword argument to the forward method.')
            znear = kwargs['znear'][cloud_idx]
            zfar = kwargs['zfar'][cloud_idx]
            if focal_length_conf.numel() == 2:
                focal_length_px = focal_length_conf[0]
            else:
                if focal_length_conf.numel() != 1:
                    raise ValueError('Focal length not parsable: %s.' % str(focal_length_conf))
                focal_length_px = focal_length_conf
            focal_length = torch.tensor([znear - 1e-06], dtype=torch.float32, device=focal_length_px.device)
            sensor_width = focal_length / focal_length_px * 2.0
            principal_point_x = kwargs.get('principal_point', cameras.principal_point)[cloud_idx][0] * 0.5 * self.renderer._renderer.width
            principal_point_y = kwargs.get('principal_point', cameras.principal_point)[cloud_idx][1] * 0.5 * self.renderer._renderer.height
        focal_length = _ensure_float_tensor(focal_length, device)
        sensor_width = _ensure_float_tensor(sensor_width, device)
        principal_point_x = _ensure_float_tensor(principal_point_x, device)
        principal_point_y = _ensure_float_tensor(principal_point_y, device)
        znear = _ensure_float_tensor(znear, device)
        zfar = _ensure_float_tensor(zfar, device)
        return focal_length, sensor_width, principal_point_x, principal_point_y, znear, zfar

    def _extract_extrinsics(self, kwargs, cloud_idx) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Extract the extrinsic information from the kwargs for a specific point cloud.

        Instead of implementing a direct translation from the PyTorch3D to the Pulsar
        camera model, we chain the two conversions of PyTorch3D->OpenCV and
        OpenCV->Pulsar for better maintainability (PyTorch3D->OpenCV is maintained and
        tested by the core PyTorch3D team, whereas OpenCV->Pulsar is maintained and
        tested by the Pulsar team).
        """
        cameras = self.rasterizer.cameras
        R = kwargs.get('R', cameras.R)[cloud_idx]
        T = kwargs.get('T', cameras.T)[cloud_idx]
        tmp_cams = PerspectiveCameras(R=R.unsqueeze(0), T=T.unsqueeze(0), device=R.device)
        size_tensor = torch.tensor([[self.renderer._renderer.height, self.renderer._renderer.width]])
        pulsar_cam = _pulsar_from_cameras_projection(tmp_cams, size_tensor)
        cam_pos = pulsar_cam[0, :3]
        cam_rot = pulsar_cam[0, 3:9]
        return cam_pos, cam_rot

    def _get_vert_rad(self, vert_pos, cam_pos, orthogonal_projection, focal_length, kwargs, cloud_idx) ->torch.Tensor:
        """
        Get point radiuses.

        These can be depending on the camera position in case of a perspective
        transform.
        """
        raster_rad = self.rasterizer.raster_settings.radius
        if kwargs.get('radius_world', False):
            return raster_rad
        if isinstance(raster_rad, torch.Tensor) and raster_rad.numel() > 1 and raster_rad.ndim > 1:
            raster_rad = raster_rad[cloud_idx]
        if orthogonal_projection:
            vert_rad = torch.ones((vert_pos.shape[0],), dtype=torch.float32, device=vert_pos.device) * raster_rad
        else:
            point_dists = torch.norm(vert_pos - cam_pos, p=2, dim=1, keepdim=False)
            vert_rad = raster_rad / focal_length * point_dists
            if isinstance(self.rasterizer.cameras, PerspectiveCameras):
                pass
            else:
                vert_rad = vert_rad / 2.0
        return vert_rad

    def forward(self, point_clouds, **kwargs) ->torch.Tensor:
        """
        Get the rendering of the provided `Pointclouds`.

        The number of point clouds in the `Pointclouds` object determines the
        number of resulting images. The provided cameras can be either 1 or equal
        to the number of pointclouds (in the first case, the same camera will be
        used for all clouds, in the latter case each point cloud will be rendered
        with the corresponding camera).

        The following kwargs are support from PyTorch3D (depending on the selected
        camera model potentially overriding camera parameters):
            radius_world (bool): use the provided radiuses from the raster_settings
              plain as radiuses in world space. Default: False.
            znear (Iterable[float]): near geometry cutoff. Is required for
              OrthographicCameras and PerspectiveCameras.
            zfar (Iterable[float]): far geometry cutoff. Is required for
              OrthographicCameras and PerspectiveCameras.
            R (torch.Tensor): [Bx3x3] camera rotation matrices.
            T (torch.Tensor): [Bx3] camera translation vectors.
            principal_point (torch.Tensor): [Bx2] camera intrinsic principal
              point offset vectors.
            focal_length (torch.Tensor): [Bx1] camera intrinsic focal lengths.
            aspect_ratio (Iterable[float]): camera aspect ratios.
            fov (Iterable[float]): camera FOVs.
            degrees (bool): whether FOVs are specified in degrees or
              radians.
            min_x (Iterable[float]): minimum x for the FoVOrthographicCameras.
            max_x (Iterable[float]): maximum x for the FoVOrthographicCameras.
            min_y (Iterable[float]): minimum y for the FoVOrthographicCameras.
            max_y (Iterable[float]): maximum y for the FoVOrthographicCameras.

        The following kwargs are supported from pulsar:
            gamma (float): The gamma value to use. This defines the transparency for
                differentiability (see pulsar paper for details). Must be in [1., 1e-5]
                with 1.0 being mostly transparent. This keyword argument is *required*!
            bg_col (torch.Tensor): The background color. Must be a tensor on the same
                device as the point clouds, with as many channels as features (no batch
                dimension - it is the same for all images in the batch).
                Default: 0.0 for all channels.
            percent_allowed_difference (float): a value in [0., 1.[ with the maximum
                allowed difference in channel space. This is used to speed up the
                computation. Default: 0.01.
            max_n_hits (int): a hard limit on the number of sphere hits per ray.
                Default: max int.
            mode (int): render mode in {0, 1}. 0: render image; 1: render hit map.
        """
        orthogonal_projection: bool = self._conf_check(point_clouds, kwargs)
        position_list = point_clouds.points_list()
        features_list = point_clouds.features_list()
        images = []
        for cloud_idx, (vert_pos, vert_col) in enumerate(zip(position_list, features_list)):
            cam_pos, cam_rot = self._extract_extrinsics(kwargs, cloud_idx)
            focal_length, sensor_width, principal_point_x, principal_point_y, znear, zfar = self._extract_intrinsics(orthogonal_projection, kwargs, cloud_idx, cam_pos.device)
            cam_params = torch.cat((cam_pos, cam_rot, torch.cat([focal_length, sensor_width, principal_point_x, principal_point_y])))
            vert_rad = self._get_vert_rad(vert_pos, cam_pos, orthogonal_projection, focal_length, kwargs, cloud_idx)
            gamma = kwargs['gamma'][cloud_idx]
            if 'first_R_then_T' in kwargs.keys():
                raise ValueError('`first_R_then_T` is not supported in this interface.')
            otherargs = {argn: argv for argn, argv in kwargs.items() if argn not in ['radius_world', 'gamma', 'znear', 'zfar', 'R', 'T', 'principal_point', 'focal_length', 'aspect_ratio', 'fov', 'degrees', 'min_x', 'max_x', 'min_y', 'max_y']}
            if 'bg_col' not in otherargs:
                bg_col = torch.zeros(vert_col.shape[1], device=cam_params.device, dtype=torch.float32)
                otherargs['bg_col'] = bg_col
            images.append(self.renderer(vert_pos=vert_pos, vert_col=vert_col, vert_rad=vert_rad, cam_params=cam_params, gamma=gamma, max_depth=zfar, min_depth=znear, **otherargs).flip(dims=[0]))
        return torch.stack(images, dim=0)


class PointsRenderer(nn.Module):
    """
    A class for rendering a batch of points. The class should
    be initialized with a rasterizer and compositor class which each have a forward
    function.
    """

    def __init__(self, rasterizer, compositor) ->None:
        super().__init__()
        self.rasterizer = rasterizer
        self.compositor = compositor

    def to(self, device):
        self.rasterizer = self.rasterizer
        self.compositor = self.compositor
        return self

    def forward(self, point_clouds, **kwargs) ->torch.Tensor:
        fragments = self.rasterizer(point_clouds, **kwargs)
        r = self.rasterizer.raster_settings.radius
        dists2 = fragments.dists.permute(0, 3, 1, 2)
        weights = 1 - dists2 / (r * r)
        images = self.compositor(fragments.idx.long().permute(0, 3, 1, 2), weights, point_clouds.features_packed().permute(1, 0), **kwargs)
        images = images.permute(0, 2, 3, 1)
        return images


def join_cameras_as_batch(cameras_list: Sequence[CamerasBase]) ->CamerasBase:
    """
    Create a batched cameras object by concatenating a list of input
    cameras objects. All the tensor attributes will be joined along
    the batch dimension.

    Args:
        cameras_list: List of camera classes all of the same type and
            on the same device. Each represents one or more cameras.
    Returns:
        cameras: single batched cameras object of the same
            type as all the objects in the input list.
    """
    c0 = cameras_list[0]
    fields = c0._FIELDS
    shared_fields = c0._SHARED_FIELDS
    if not all(isinstance(c, CamerasBase) for c in cameras_list):
        raise ValueError('cameras in cameras_list must inherit from CamerasBase')
    if not all(type(c) is type(c0) for c in cameras_list[1:]):
        raise ValueError('All cameras must be of the same type')
    if not all(c.device == c0.device for c in cameras_list[1:]):
        raise ValueError('All cameras in the batch must be on the same device')
    kwargs = {}
    kwargs['device'] = c0.device
    for field in fields:
        field_not_none = [(getattr(c, field) is not None) for c in cameras_list]
        if not any(field_not_none):
            continue
        if not all(field_not_none):
            raise ValueError(f'Attribute {field} is inconsistently present')
        attrs_list = [getattr(c, field) for c in cameras_list]
        if field in shared_fields:
            if not all(a == attrs_list[0] for a in attrs_list):
                raise ValueError(f'Attribute {field} is not constant across inputs')
            if field.startswith('_'):
                field = field[1:]
            kwargs[field] = attrs_list[0]
        elif isinstance(attrs_list[0], torch.Tensor):
            kwargs[field] = torch.cat(attrs_list, dim=0)
        else:
            raise ValueError(f'Field {field} type is not supported for batching')
    return c0.__class__(**kwargs)


def join_pointclouds_as_batch(pointclouds: Sequence[Pointclouds]) ->Pointclouds:
    """
    Merge a list of Pointclouds objects into a single batched Pointclouds
    object. All pointclouds must be on the same device.

    Args:
        batch: List of Pointclouds objects each with batch dim [b1, b2, ..., bN]
    Returns:
        pointcloud: Poinclouds object with all input pointclouds collated into
            a single object with batch dim = sum(b1, b2, ..., bN)
    """
    if isinstance(pointclouds, Pointclouds) or not isinstance(pointclouds, Sequence):
        raise ValueError('Wrong first argument to join_points_as_batch.')
    device = pointclouds[0].device
    if not all(p.device == device for p in pointclouds):
        raise ValueError('Pointclouds must all be on the same device')
    kwargs = {}
    for field in ('points', 'normals', 'features'):
        field_list = [getattr(p, field + '_list')() for p in pointclouds]
        if None in field_list:
            if field == 'points':
                raise ValueError('Pointclouds cannot have their points set to None!')
            if not all(f is None for f in field_list):
                raise ValueError(f"Pointclouds in the batch have some fields '{field}'" + ' defined and some set to None.')
            field_list = None
        else:
            field_list = [p for points in field_list for p in points]
        kwargs[field] = field_list
    return Pointclouds(**kwargs)


class _GenericWorkaround:
    """
    OmegaConf.structured has a weirdness when you try to apply
    it to a dataclass whose first base class is a Generic which is not
    Dict. The issue is with a function called get_dict_key_value_types
    in omegaconf/_utils.py.
    For example this fails:

        @dataclass(eq=False)
        class D(torch.utils.data.Dataset[int]):
            a: int = 3

        OmegaConf.structured(D)

    We avoid the problem by adding this class as an extra base class.
    """
    pass


def _bbox_xyxy_to_xywh(xyxy: torch.Tensor) ->torch.Tensor:
    wh = xyxy[2:] - xyxy[:2]
    xywh = torch.cat([xyxy[:2], wh])
    return xywh


def _clamp_box_to_image_bounds_and_round(bbox_xyxy: torch.Tensor, image_size_hw: Tuple[int, int]) ->torch.LongTensor:
    bbox_xyxy = bbox_xyxy.clone()
    bbox_xyxy[[0, 2]] = torch.clamp(bbox_xyxy[[0, 2]], 0, image_size_hw[-1])
    bbox_xyxy[[1, 3]] = torch.clamp(bbox_xyxy[[1, 3]], 0, image_size_hw[-2])
    if not isinstance(bbox_xyxy, torch.LongTensor):
        bbox_xyxy = bbox_xyxy.round().long()
    return bbox_xyxy


def _crop_around_box(tensor, bbox, impath: str=''):
    bbox = _clamp_box_to_image_bounds_and_round(bbox, image_size_hw=tensor.shape[-2:])
    tensor = tensor[..., bbox[1]:bbox[3], bbox[0]:bbox[2]]
    assert all(c > 0 for c in tensor.shape), f'squashed image {impath}'
    return tensor


def _get_1d_bounds(arr) ->Tuple[int, int]:
    nz = np.flatnonzero(arr)
    return nz[0], nz[-1] + 1


def _get_bbox_from_mask(mask, thr, decrease_quant: float=0.05) ->Tuple[int, int, int, int]:
    masks_for_box = np.zeros_like(mask)
    while masks_for_box.sum() <= 1.0:
        masks_for_box = (mask > thr).astype(np.float32)
        thr -= decrease_quant
    if thr <= 0.0:
        warnings.warn(f'Empty masks_for_bbox (thr={thr}) => using full image.')
    x0, x1 = _get_1d_bounds(masks_for_box.sum(axis=-2))
    y0, y1 = _get_1d_bounds(masks_for_box.sum(axis=-1))
    return x0, y0, x1 - x0, y1 - y0


def _bbox_xywh_to_xyxy(xywh: torch.Tensor, clamp_size: Optional[int]=None) ->torch.Tensor:
    xyxy = xywh.clone()
    if clamp_size is not None:
        xyxy[2:] = torch.clamp(xyxy[2:], clamp_size)
    xyxy[2:] += xyxy[:2]
    return xyxy


def _get_clamp_bbox(bbox: torch.Tensor, box_crop_context: float=0.0, image_path: str='') ->torch.Tensor:
    bbox = bbox.clone()
    if box_crop_context > 0.0:
        c = box_crop_context
        bbox = bbox.float()
        bbox[0] -= bbox[2] * c / 2
        bbox[1] -= bbox[3] * c / 2
        bbox[2] += bbox[2] * c
        bbox[3] += bbox[3] * c
    if (bbox[2:] <= 1.0).any():
        raise ValueError(f'squashed image {image_path}!! The bounding box contains no pixels.')
    bbox[2:] = torch.clamp(bbox[2:], 2)
    bbox_xyxy = _bbox_xywh_to_xyxy(bbox, clamp_size=2)
    return bbox_xyxy


def _load_16big_png_depth(depth_png) ->np.ndarray:
    with Image.open(depth_png) as depth_pil:
        depth = np.frombuffer(np.array(depth_pil, dtype=np.uint16), dtype=np.float16).astype(np.float32).reshape((depth_pil.size[1], depth_pil.size[0]))
    return depth


def _load_depth(path, scale_adjustment) ->np.ndarray:
    if not path.lower().endswith('.png'):
        raise ValueError('unsupported depth file name "%s"' % path)
    d = _load_16big_png_depth(path) * scale_adjustment
    d[~np.isfinite(d)] = 0.0
    return d[None]


def _load_1bit_png_mask(file: str) ->np.ndarray:
    with Image.open(file) as pil_im:
        mask = (np.array(pil_im.convert('L')) > 0.0).astype(np.float32)
    return mask


def _load_depth_mask(path: str) ->np.ndarray:
    if not path.lower().endswith('.png'):
        raise ValueError('unsupported depth mask file name "%s"' % path)
    m = _load_1bit_png_mask(path)
    return m[None]


def _load_image(path) ->np.ndarray:
    with Image.open(path) as pil_im:
        im = np.array(pil_im.convert('RGB'))
    im = im.transpose((2, 0, 1))
    im = im.astype(np.float32) / 255.0
    return im


def _load_mask(path) ->np.ndarray:
    with Image.open(path) as pil_im:
        mask = np.array(pil_im)
    mask = mask.astype(np.float32) / 255.0
    return mask[None]


def _rescale_bbox(bbox: torch.Tensor, orig_res, new_res) ->torch.Tensor:
    assert bbox is not None
    assert np.prod(orig_res) > 1e-08
    rel_size = (new_res[0] / orig_res[0] + new_res[1] / orig_res[1]) / 2.0
    return bbox * rel_size


def _safe_as_tensor(data, dtype):
    if data is None:
        return None
    return torch.tensor(data, dtype=dtype)


def _seq_name_to_seed(seq_name) ->int:
    return int(hashlib.sha1(seq_name.encode('utf-8')).hexdigest(), 16)


class TensorPropertiesTestClass(TensorProperties):

    def __init__(self, x=None, y=None, device='cpu'):
        super().__init__(device=device, x=x, y=y)

    def clone(self):
        other = TensorPropertiesTestClass()
        return super().clone(other)


class NearestNeighborPointsRenderer(PointsRenderer):
    """
    A class for rendering a batch of points by a trivial nearest
    neighbor assignment.
    """

    def forward(self, point_clouds, **kwargs) ->torch.Tensor:
        fragments = self.rasterizer(point_clouds, **kwargs)
        dists2 = fragments.dists.permute(0, 3, 1, 2)
        weights = torch.ones_like(dists2)
        images = self.compositor(fragments.idx.long().permute(0, 3, 1, 2), weights, point_clouds.features_packed().permute(1, 0), **kwargs)
        return images


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (Conv2dSame,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Conv3dSame,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DownBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DownBlock3D,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     True),
    (DownsamplingNet,
     lambda: ([], {'per_layer_out_ch': [4, 4], 'in_channels': 4, 'use_dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FCBlock,
     lambda: ([], {'hidden_ch': 4, 'num_hidden_layers': 1, 'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FCLayer,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (HarmonicEmbedding,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (HyperFC,
     lambda: ([], {'hyper_in_ch': 4, 'hyper_num_hidden_layers': 1, 'hyper_hidden_ch': 4, 'hidden_ch': 4, 'num_hidden_layers': 1, 'in_ch': 4, 'out_ch': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (HyperLayer,
     lambda: ([], {'in_ch': 4, 'out_ch': 4, 'hyper_in_ch': 4, 'hyper_num_hidden_layers': 1, 'hyper_hidden_ch': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (HyperLinear,
     lambda: ([], {'in_ch': 4, 'out_ch': 4, 'hyper_in_ch': 4, 'hyper_num_hidden_layers': 1, 'hyper_hidden_ch': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Identity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ImplicitFunctionWrapper,
     lambda: ([], {'fn': _mock_layer()}),
     lambda: ([], {'input': torch.rand([4, 4])}),
     False),
    (LayerNormConv2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TransformerEncoderLayer,
     lambda: ([], {'d_model': 4, 'nhead': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (Unet,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'nf0': 4, 'num_down': 4, 'max_channels': 4, 'use_dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 64, 64])], {}),
     False),
    (UnetSkipConnectionBlock3d,
     lambda: ([], {'outer_nc': 4, 'inner_nc': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     False),
    (UpBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (UpBlock3D,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     True),
]

class Test_facebookresearch_pytorch3d(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])


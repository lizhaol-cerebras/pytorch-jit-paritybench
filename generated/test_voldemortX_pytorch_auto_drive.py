import sys
_module = sys.modules[__name__]
del sys
enet_culane = _module
enet_tusimple = _module
erfnet_culane = _module
erfnet_llamas = _module
erfnet_tusimple = _module
erfnet_tusimple_aug = _module
mobilenetv2_culane = _module
mobilenetv2_tusimple = _module
mobilenetv3_large_culane = _module
mobilenetv3_large_tusimple = _module
repvgg_a0_culane = _module
repvgg_a1_culane = _module
repvgg_b0_culane = _module
repvgg_b1g2_culane = _module
repvgg_b2_culane = _module
resnet101_culane = _module
resnet101_tusimple = _module
resnet18_culane = _module
resnet18_tusimple = _module
resnet34_culane = _module
resnet34_llamas = _module
resnet34_tusimple = _module
resnet34_tusimple_aug = _module
resnet50_culane = _module
resnet50_tusimple = _module
swin_tiny_culane = _module
vgg16_culane = _module
vgg16_llamas = _module
vgg16_tusimple = _module
resnet18_culane_aug1b = _module
resnet18_llamas_aug1b = _module
resnet18_tusimple_aug1b = _module
resnet34_culane_aug1b = _module
resnet34_llamas_aug1b = _module
resnet34_tusimple_aug1b = _module
vis_resnet34_culane_aug1b = _module
vis_resnet34_tusimple_aug1b = _module
_utils = _module
culane = _module
culane_bezier = _module
culane_seg = _module
llamas = _module
llamas_bezier = _module
llamas_seg = _module
test_288 = _module
test_360 = _module
train_level0_288 = _module
train_level0_360 = _module
train_level1a_288 = _module
train_level1a_360 = _module
train_level1a_360_717 = _module
train_level1b_288 = _module
train_level1b_360 = _module
tusimple = _module
tusimple_bezier = _module
tusimple_seg = _module
adam000025 = _module
adam00006_dcn = _module
adamw00006_swin = _module
adamw0001_swin = _module
ep10_poly_warmup200 = _module
ep12_poly_warmup200 = _module
ep12_poly_warmup500 = _module
ep12_poly_warmup600 = _module
ep150_step = _module
ep18_poly_warmup200 = _module
ep2000_step = _module
ep20_cosine = _module
ep36_cosine = _module
ep400_cosine = _module
ep50_poly_warmup200 = _module
ep50_poly_warmup250 = _module
ep50_poly_warmup500 = _module
matchingloss_bezier = _module
matchingloss_polynomial = _module
segloss_5class = _module
segloss_7class = _module
sgd0048 = _module
sgd006 = _module
sgd008 = _module
sgd009 = _module
sgd01 = _module
sgd013 = _module
sgd02 = _module
sgd025 = _module
sgd03 = _module
sgd035 = _module
sgd04 = _module
sgd05 = _module
sgd06 = _module
sgd08 = _module
resnet18s_culane = _module
resnet18s_culane_aug = _module
resnet18s_tusimple = _module
resnet18s_tusimple_aug = _module
resnet34_culane_aug = _module
mobilenetv3_large_culane = _module
mobilenetv3_large_tusimple = _module
city_test_half = _module
city_test_half_wo_norm = _module
city_test_hd = _module
city_train_half_256 = _module
city_train_half_512_wo_norm = _module
city_train_hd_512 = _module
cityscapes = _module
gtav = _module
gtav_train_hd_512 = _module
pascal_voc = _module
synthia = _module
synthia_train_hd_512 = _module
voc_test_505 = _module
voc_train_321 = _module
adam00007 = _module
adam00008_wd00002 = _module
celoss = _module
celoss_cityscapes_balanced = _module
ep10 = _module
ep150_epoch = _module
ep20 = _module
ep30 = _module
ep300 = _module
ep60 = _module
sgd0002 = _module
sgd0004 = _module
resnet101_cityscapes_256x512 = _module
resnet101_cityscapes_512x1024 = _module
resnet101_gtav_512x1024 = _module
resnet101_pascalvoc_321x321 = _module
resnet101_synthia_512x1024 = _module
cityscapes_512x1024 = _module
cityscapes_512x1024_encoder = _module
statics = _module
main_landet = _module
main_semseg = _module
tools = _module
cityscapes_data_list = _module
cal_total = _module
culane_metric = _module
evaluate = _module
culane_list_convertor = _module
gen_bezier_annotations = _module
loader = _module
lpd_metric = _module
upperbound = _module
dcn_flops = _module
generate_cityscapes_demo = _module
gtav_data_list = _module
llamas_evaluation = _module
llamas_official_scripts = _module
llamas_list_convertor = _module
profiling = _module
synthia_data_list = _module
synthia_label_convertor = _module
to_onnx = _module
to_tensorrt = _module
lane = _module
tusimple_list_convertor = _module
tusimple_to_video = _module
lane_img_dir = _module
lane_video = _module
seg_img_dir = _module
seg_video = _module
utils = _module
args = _module
common = _module
csrc = _module
apis = _module
curve_utils = _module
custom_op_flop_counters = _module
datasets = _module
bdd100k = _module
builder = _module
culane_vis = _module
image_folder = _module
image_folder_lane_base = _module
lane_as_bezier = _module
lane_as_segmentation = _module
llamas_vis = _module
segmentation = _module
tusimple_vis = _module
utils = _module
video = _module
ddp_utils = _module
frames_to_video = _module
lane_det_utils = _module
losses = _module
_utils = _module
focal_loss = _module
hungarian_bezier_loss = _module
hungarian_loss = _module
lane_seg_loss = _module
laneatt_loss = _module
torch_loss = _module
weighted_ce_loss = _module
lr_schedulers = _module
cosine_scheduler_wrapper = _module
poly_scheduler = _module
step_scheduler = _module
torch_scheduler = _module
models = _module
_utils = _module
backbone_wrappers = _module
common_models = _module
blocks = _module
dilated_bottleneck = _module
inverted_residual = _module
non_bottleneck_1d = _module
heads = _module
busd = _module
encoder_decoder_lane_exist = _module
plain_decoder = _module
resa_lane_exist = _module
scnn_decoder = _module
simple_lane_exist = _module
uper_head = _module
plugins = _module
conv_projection_1d = _module
dilated_blocks = _module
feature_flip_fusion = _module
ppm = _module
resa_layer = _module
resa_reducer = _module
se = _module
spatial_conv = _module
erfnet_encoder = _module
lane_detection = _module
bezier_base = _module
bezier_lane_net = _module
laneatt = _module
lstr = _module
resa = _module
mlp = _module
mobilenet_v2 = _module
mobilenet_v3 = _module
rep_vgg = _module
resnet = _module
_utils = _module
deeplab = _module
deeplab_vgg = _module
enet = _module
erfnet = _module
fcn = _module
simple_seg_head = _module
swin = _module
transformer = _module
position_encoding = _module
transformer = _module
utils = _module
vgg_encoder = _module
onnx_utils = _module
optimizers = _module
torch_optimizer = _module
profiling_utils = _module
registry = _module
runners = _module
base = _module
lane_det_tester = _module
lane_det_trainer = _module
lane_det_visualizer = _module
seg_tester = _module
seg_trainer = _module
seg_visualizer = _module
seg_utils = _module
tensorrt_utils = _module
torch_amp_dummy = _module
transforms = _module
functional = _module
functional_keypoints = _module
functional_pil = _module
functional_tensor = _module
imgaug_affine = _module
transforms = _module
vis_utils = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


import numpy as np


from collections import OrderedDict


from torch.utils.cpp_extension import load


from scipy.interpolate import splprep


from scipy.interpolate import splev


from scipy.special import comb as n_over_k


import torchvision


from torchvision.datasets import VisionDataset


import collections.abc


from torch.utils.data._utils.collate import default_collate_err_msg_format


from torch.utils.data._utils.collate import np_str_obj_array_pattern


import torch.distributed as dist


from torch import Tensor


from typing import Optional


from torch.nn import _reduction as _Reduction


import torch.nn as nn


import torch.nn.functional as F


from torch.nn import functional as F


from scipy.optimize import linear_sum_assignment


from torch import nn


from torch.optim import lr_scheduler


import math


from torch import nn as nn


from typing import List


from torch import load


from torch.nn.parameter import Parameter


from torch.nn import BatchNorm2d


import torch.utils.checkpoint as checkpoint


import copy


from torchvision.models import vgg16_bn


from torch import optim


import time


from torch.utils.tensorboard import SummaryWriter


from abc import ABC


from abc import abstractmethod


import functools


import numbers


import warnings


from typing import Any


from torch.jit.annotations import List


from torch.jit.annotations import Tuple


from typing import Sequence


from typing import Dict


from typing import Tuple


from torch.nn.functional import grid_sample


from torch.nn.functional import conv2d


from torch.nn.functional import interpolate


from torch.nn.functional import pad as torch_pad


from torch.jit.annotations import BroadcastingList2


import random


from collections.abc import Sequence


from scipy.interpolate import InterpolatedUnivariateSpline


def upcast(t):
    if t.is_floating_point():
        return t if t.dtype in (torch.float32, torch.float64) else t.float()
    else:
        return t if t.dtype in (torch.int32, torch.int64) else t.int()


class BezierSampler(torch.nn.Module):

    def __init__(self, order, num_sample_points, proj_coefficient=0):
        super().__init__()
        self.proj_coefficient = proj_coefficient
        self.num_control_points = order + 1
        self.num_sample_points = num_sample_points
        self.control_points = []
        self.bezier_coeff = self.get_bezier_coefficient()
        self.bernstein_matrix = self.get_bernstein_matrix()

    def get_bezier_coefficient(self):
        Mtk = lambda n, t, k: t ** k * (1 - t) ** (n - k) * n_over_k(n, k)
        BezierCoeff = lambda ts: [[Mtk(self.num_control_points - 1, t, k) for k in range(self.num_control_points)] for t in ts]
        return BezierCoeff

    def get_bernstein_matrix(self):
        t = torch.linspace(0, 1, self.num_sample_points)
        if self.proj_coefficient != 0:
            t[t > 0.5] = t[t > 0.5] + (1 - t[t > 0.5]) * t[t > 0.5] ** self.proj_coefficient
            t[t < 0.5] = 1 - (1 - t[t < 0.5] + t[t < 0.5] * (1 - t[t < 0.5]) ** self.proj_coefficient)
        c_matrix = torch.tensor(self.bezier_coeff(t))
        return c_matrix

    def get_sample_points(self, control_points_matrix):
        if control_points_matrix.numel() == 0:
            return control_points_matrix
        if self.bernstein_matrix.device != control_points_matrix.device:
            self.bernstein_matrix = self.bernstein_matrix
        return upcast(self.bernstein_matrix).matmul(upcast(control_points_matrix))


class _Loss(torch.nn.Module):
    reduction: str

    def __init__(self, size_average=None, reduce=None, reduction: str='mean') ->None:
        super(_Loss, self).__init__()
        if size_average is not None or reduce is not None:
            self.reduction = _Reduction.legacy_get_string(size_average, reduce)
        else:
            self.reduction = reduction


class WeightedLoss(_Loss):

    def __init__(self, weight: Optional[Tensor]=None, size_average=None, reduce=None, reduction: str='mean') ->None:
        super(WeightedLoss, self).__init__(size_average, reduce, reduction)
        if weight is not None and not isinstance(weight, Tensor):
            weight = torch.tensor(weight)
        self.register_buffer('weight', weight)


def one_hot(labels: torch.Tensor, num_classes: int, device: Optional[torch.device]=None, dtype: Optional[torch.dtype]=None, eps: Optional[float]=1e-06) ->torch.Tensor:
    """Converts an integer label x-D tensor to a one-hot (x+1)-D tensor.

    Args:
        labels (torch.Tensor) : tensor with labels of shape :math:`(N, *)`,
                                where N is batch size. Each value is an integer
                                representing correct classification.
        num_classes (int): number of classes in labels.
        device (Optional[torch.device]): the desired device of returned tensor.
         Default: if None, uses the current device for the default tensor type
         (see torch.set_default_tensor_type()). device will be the CPU for CPU
         tensor types and the current CUDA device for CUDA tensor types.
        dtype (Optional[torch.dtype]): the desired data type of returned
         tensor. Default: if None, infers data type from values.

    Returns:
        torch.Tensor: the labels in one hot tensor of shape :math:`(N, C, *)`,

    Examples::
        >>> labels = torch.LongTensor([[[0, 1], [2, 0]]])
        >>> kornia.losses.one_hot(labels, num_classes=3)
        tensor([[[[1., 0.],
                  [0., 1.]],
                 [[0., 1.],
                  [0., 0.]],
                 [[0., 0.],
                  [1., 0.]]]]
    """
    if not torch.is_tensor(labels):
        raise TypeError('Input labels type is not a torch.Tensor. Got {}'.format(type(labels)))
    if not labels.dtype == torch.int64:
        raise ValueError('labels must be of the same dtype torch.int64. Got: {}'.format(labels.dtype))
    if num_classes < 1:
        raise ValueError('The number of classes must be bigger than one. Got: {}'.format(num_classes))
    shape = labels.shape
    one_hot = torch.zeros(shape[0], num_classes, *shape[1:], device=device, dtype=dtype)
    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps


def _focal_loss(input: torch.Tensor, target: torch.Tensor, alpha: float, gamma: float=2.0, reduction: str='none', eps: float=1e-08) ->torch.Tensor:
    """Function that computes Focal loss.

    See :class:`~kornia.losses.FocalLoss` for details.
    """
    if not torch.is_tensor(input):
        raise TypeError('Input type is not a torch.Tensor. Got {}'.format(type(input)))
    if not len(input.shape) >= 2:
        raise ValueError('Invalid input shape, we expect BxCx*. Got: {}'.format(input.shape))
    if input.size(0) != target.size(0):
        raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'.format(input.size(0), target.size(0)))
    n = input.size(0)
    out_size = (n,) + input.size()[2:]
    if target.size()[1:] != input.size()[2:]:
        raise ValueError('Expected target size {}, got {}'.format(out_size, target.size()))
    if not input.device == target.device:
        raise ValueError('input and target must be in the same device. Got: {} and {}'.format(input.device, target.device))
    input_soft: torch.Tensor = F.softmax(input, dim=1) + eps
    target_one_hot: torch.Tensor = one_hot(target, num_classes=input.shape[1], device=input.device, dtype=input.dtype)
    weight = torch.pow(-input_soft + 1.0, gamma)
    focal = -alpha * weight * torch.log(input_soft)
    loss_tmp = torch.sum(target_one_hot * focal, dim=1)
    if reduction == 'none':
        loss = loss_tmp
    elif reduction == 'mean':
        loss = torch.mean(loss_tmp)
    elif reduction == 'sum':
        loss = torch.sum(loss_tmp)
    else:
        raise NotImplementedError('Invalid reduction mode: {}'.format(reduction))
    return loss


class FocalLoss(nn.Module):
    """Criterion that computes Focal loss.

    According to [1], the Focal loss is computed as follows:

    .. math::

        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)

    where:
       - :math:`p_t` is the model's estimated probability for each class.


    Arguments:
        alpha (float): Weighting factor :math:`\\alpha \\in [0, 1]`.
        gamma (float): Focusing parameter :math:`\\gamma >= 0`.
        reduction (str, optional): Specifies the reduction to apply to the
         output: ‘none’ | ‘mean’ | ‘sum’. ‘none’: no reduction will be applied,
         ‘mean’: the sum of the output will be divided by the number of elements
         in the output, ‘sum’: the output will be summed. Default: ‘none’.

    Shape:
        - Input: :math:`(N, C, *)` where C = number of classes.
        - Target: :math:`(N, *)` where each value is
          :math:`0 ≤ targets[i] ≤ C−1`.

    Examples:
        >>> N = 5  # num_classes
        >>> kwargs = {"alpha": 0.5, "gamma": 2.0, "reduction": 'mean'}
        >>> loss = kornia.losses.FocalLoss(**kwargs)
        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)
        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)
        >>> output = loss(input, target)
        >>> output.backward()

    References:
        [1] https://arxiv.org/abs/1708.02002
    """

    def __init__(self, alpha: float, gamma: float=2.0, reduction: str='none') ->None:
        super(FocalLoss, self).__init__()
        self.alpha: float = alpha
        self.gamma: float = gamma
        self.reduction: str = reduction
        self.eps: float = 1e-06

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        return _focal_loss(input, target, self.alpha, self.gamma, self.reduction, self.eps)


@torch.no_grad()
def get_valid_points(points):
    if points.numel() == 0:
        return torch.tensor([1], dtype=torch.bool, device=points.device)
    return (points[..., 0] > 0) * (points[..., 0] < 1) * (points[..., 1] > 0) * (points[..., 1] < 1)


@torch.no_grad()
def cubic_bezier_curve_segment(control_points, sample_points):
    if control_points.numel() == 0 or sample_points.numel() == 0:
        return control_points
    B, N = sample_points.shape[:-1]
    valid_points = get_valid_points(sample_points)
    t = torch.linspace(0.0, 1.0, steps=N, dtype=sample_points.dtype, device=sample_points.device)
    t0 = t[(valid_points + torch.arange(N, device=valid_points.device).flip([0]) * valid_points).max(dim=-1).indices]
    t1 = t[(valid_points + torch.arange(N, device=valid_points.device) * valid_points).max(dim=-1).indices]
    u0 = 1 - t0
    u1 = 1 - t1
    transform_matrix_c = [torch.stack([(u0 ** (3 - i) * u1 ** i) for i in range(4)], dim=-1), torch.stack([3 * t0 * u0 ** 2, 2 * t0 * u0 * u1 + u0 ** 2 * t1, t0 * u1 ** 2 + 2 * u0 * u1 * t1, 3 * t1 * u1 ** 2], dim=-1), torch.stack([3 * t0 ** 2 * u0, t0 ** 2 * u1 + 2 * t0 * t1 * u0, 2 * t0 * t1 * u1 + t1 ** 2 * u0, 3 * t1 ** 2 * u1], dim=-1), torch.stack([(t0 ** (3 - i) * t1 ** i) for i in range(4)], dim=-1)]
    transform_matrix = torch.stack(transform_matrix_c, dim=-2).transpose(-2, -1)
    transform_matrix = transform_matrix.unsqueeze(1).expand(B, 2, 4, 4)
    res = transform_matrix.matmul(control_points.permute(0, 2, 1).unsqueeze(-1))
    return res.squeeze(-1).permute(0, 2, 1)


class _HungarianMatcher(torch.nn.Module):
    """This class computes an assignment between the targets and the predictions of the network

    For efficiency reasons, the targets don't include the no_object. Because of this, in general,
    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,
    while the others are un-matched (and thus treated as non-objects).
    POTO matching, which maximizes the cost matrix.
    """

    def __init__(self, alpha=0.8, bezier_order=3, num_sample_points=100, k=7):
        super().__init__()
        self.k = k
        self.alpha = alpha
        self.num_sample_points = num_sample_points
        self.bezier_sampler = BezierSampler(num_sample_points=num_sample_points, order=bezier_order)

    @torch.no_grad()
    def forward(self, outputs, targets):
        B, Q = outputs['logits'].shape
        target_keypoints = torch.cat([i['keypoints'] for i in targets], dim=0)
        target_sample_points = torch.cat([i['sample_points'] for i in targets], dim=0)
        target_keypoints = cubic_bezier_curve_segment(target_keypoints, target_sample_points)
        target_sample_points = self.bezier_sampler.get_sample_points(target_keypoints)
        G, N = target_keypoints.shape[:2]
        out_prob = outputs['logits'].sigmoid()
        out_lane = outputs['curves']
        sizes = [target['keypoints'].shape[0] for target in targets]
        _, max_indices = torch.nn.functional.max_pool1d(out_prob.unsqueeze(1), kernel_size=self.k, stride=1, padding=(self.k - 1) // 2, return_indices=True)
        max_indices = max_indices.squeeze(1)
        indices = torch.arange(0, Q, dtype=out_prob.dtype, device=out_prob.device).unsqueeze(0).expand_as(max_indices)
        local_maxima = (max_indices == indices).flatten().unsqueeze(-1).expand(-1, G)
        out_prob = out_prob.flatten()
        out_lane = out_lane.flatten(end_dim=1)
        cost_label = out_prob.unsqueeze(-1).expand(-1, G)
        cost_curve = 1 - torch.cdist(self.bezier_sampler.get_sample_points(out_lane).flatten(start_dim=-2), target_sample_points.flatten(start_dim=-2), p=1) / self.num_sample_points
        cost_curve = cost_curve.clamp(min=0, max=1)
        C = local_maxima * cost_label ** (1 - self.alpha) * cost_curve ** self.alpha
        C = -C.view(B, Q, -1).cpu()
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]


def cubic_curve_with_projection(coefficients, y):
    y = y.permute(-1, *[i for i in range(len(y.shape) - 1)])
    x = coefficients[..., 0] / (y - coefficients[..., 1]) ** 2 + coefficients[..., 2] / (y - coefficients[..., 1]) + coefficients[..., 3] + coefficients[..., 4] * y - coefficients[..., 5]
    return x.permute(*[(i + 1) for i in range(len(x.shape) - 1)], 0)


@torch.no_grad()
def lane_normalize_in_batch(keypoints):
    valid_points = keypoints[..., 0] > 0
    norm_weights = (valid_points.sum().float() / valid_points.sum(dim=-1).float()) ** 0.5
    norm_weights /= norm_weights.max()
    return norm_weights, valid_points


class HungarianMatcher(torch.nn.Module):
    """This class computes an assignment between the targets and the predictions of the network

    For efficiency reasons, the targets don't include the no_object. Because of this, in general,
    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,
    while the others are un-matched (and thus treated as non-objects).
    """

    def __init__(self, upper_weight=2, lower_weight=2, curve_weight=5, label_weight=3):
        super().__init__()
        self.lower_weight = lower_weight
        self.upper_weight = upper_weight
        self.curve_weight = curve_weight
        self.label_weight = label_weight

    @torch.no_grad()
    def forward(self, outputs, targets):
        bs, num_queries = outputs['logits'].shape[:2]
        out_prob = outputs['logits'].softmax(dim=-1)
        out_lane = outputs['curves'].flatten(end_dim=-2)
        target_uppers = torch.cat([i['uppers'] for i in targets])
        target_lowers = torch.cat([i['lowers'] for i in targets])
        sizes = [target['labels'].shape[0] for target in targets]
        num_gt = sum(sizes)
        cost_label = -out_prob[..., 1].unsqueeze(-1).flatten(end_dim=-2).repeat(1, num_gt)
        cost_upper = torch.cdist(out_lane[:, 0:1], target_uppers.unsqueeze(-1), p=1)
        cost_lower = torch.cdist(out_lane[:, 1:2], target_lowers.unsqueeze(-1), p=1)
        target_keypoints = torch.cat([i['keypoints'] for i in targets], dim=0)
        norm_weights, valid_points = lane_normalize_in_batch(target_keypoints)
        expand_shape = [bs * num_queries, num_gt, target_keypoints.shape[-2]]
        coefficients = out_lane[:, 2:].unsqueeze(1).expand(*expand_shape[:-1], -1)
        out_x = cubic_curve_with_projection(y=target_keypoints[:, :, 1].unsqueeze(0).expand(expand_shape), coefficients=coefficients)
        cost_curve = ((out_x - target_keypoints[:, :, 0].unsqueeze(0).expand(expand_shape)).abs() * valid_points.unsqueeze(0).expand(expand_shape)).sum(-1)
        cost_curve *= norm_weights
        C = self.label_weight * cost_label + self.curve_weight * cost_curve + self.lower_weight * cost_lower + self.upper_weight * cost_upper
        C = C.view(bs, num_queries, -1).cpu()
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]


class SimpleRegistry(object):

    def __init__(self) ->None:
        self._map = {}

    def register(self):

        def decorator(function_or_class):
            name = function_or_class.__name__
            if name in self._map.keys():
                raise ValueError('Conflicting name for registered Function or Class {}'.format(name))
            self._map[name] = function_or_class
            return function_or_class
        return decorator

    def get(self, name):
        res = self._map.get(name)
        if res is None:
            raise KeyError('Class or Function {} not found in registry!'.format(name))
        return res

    def from_dict(self, dict_params, **kwargs):
        if dict_params is None:
            return None
        dict_params_ = dict_params.copy()
        dict_params_.update(kwargs)
        name = dict_params_.pop('name')
        function_or_class = self.get(name)
        try:
            return function_or_class(**dict_params_)
        except Exception as e:
            None
            raise e


LOSSES = SimpleRegistry()


def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True


def get_world_size():
    if not is_dist_avail_and_initialized():
        return 1
    return dist.get_world_size()


class HungarianLoss(WeightedLoss):
    __constants__ = ['reduction']

    def __init__(self, upper_weight=2, lower_weight=2, curve_weight=5, label_weight=3, weight=None, size_average=None, reduce=None, reduction='mean'):
        super(HungarianLoss, self).__init__(weight, size_average, reduce, reduction)
        self.lower_weight = lower_weight
        self.upper_weight = upper_weight
        self.curve_weight = curve_weight
        self.label_weight = label_weight
        self.matcher = HungarianMatcher(upper_weight, lower_weight, curve_weight, label_weight)

    @staticmethod
    def get_src_permutation_idx(indices):
        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])
        image_idx = torch.cat([src for src, _ in indices])
        return batch_idx, image_idx

    def forward(self, inputs: Tensor, targets: Tensor, net):
        if 'padding_mask' in targets[0].keys():
            padding_masks = torch.stack([i['padding_mask'] for i in targets])
            outputs = net(inputs, padding_masks)
        else:
            outputs = net(inputs)
        loss, log_dict = self.calc_full_loss(outputs=outputs, targets=targets)
        if 'aux' in outputs:
            for i in range(len(outputs['aux'])):
                aux_loss, aux_log_dict = self.calc_full_loss(outputs=outputs['aux'][i], targets=targets)
                loss += aux_loss
                for k in list(log_dict):
                    log_dict[k + ' aux' + str(i)] = aux_log_dict[k]
        return loss, log_dict

    def calc_full_loss(self, outputs, targets):
        indices = self.matcher(outputs=outputs, targets=targets)
        idx = self.get_src_permutation_idx(indices)
        target_lowers = torch.cat([t['lowers'][i] for t, (_, i) in zip(targets, indices)], dim=0)
        target_uppers = torch.cat([t['uppers'][i] for t, (_, i) in zip(targets, indices)], dim=0)
        target_keypoints = torch.cat([t['keypoints'][i] for t, (_, i) in zip(targets, indices)], dim=0)
        target_labels = torch.zeros(outputs['logits'].shape[:-1], dtype=torch.int64, device=outputs['logits'].device)
        target_labels[idx] = 1
        loss_label = self.classification_loss(inputs=outputs['logits'].permute(0, 2, 1), targets=target_labels)
        output_curves = outputs['curves'][idx]
        norm_weights, valid_points = lane_normalize_in_batch(target_keypoints)
        out_x = cubic_curve_with_projection(coefficients=output_curves[:, 2:], y=target_keypoints[:, :, 1].clone().detach())
        loss_curve = self.point_loss(inputs=out_x, targets=target_keypoints[:, :, 0], norm_weights=norm_weights, valid_points=valid_points)
        loss_upper = self.point_loss(inputs=output_curves[:, 0], targets=target_uppers)
        loss_lower = self.point_loss(inputs=output_curves[:, 1], targets=target_lowers)
        loss = self.label_weight * loss_label + self.curve_weight * loss_curve + self.lower_weight * loss_lower + self.upper_weight * loss_upper
        return loss, {'training loss': loss, 'loss label': loss_label, 'loss curve': loss_curve, 'loss upper': loss_upper, 'loss lower': loss_lower}

    def point_loss(self, inputs: Tensor, targets: Tensor, norm_weights=None, valid_points=None) ->Tensor:
        loss = F.l1_loss(inputs, targets, reduction='none')
        if norm_weights is not None:
            loss *= norm_weights.unsqueeze(-1).expand_as(loss)
        if valid_points is not None:
            loss = loss[valid_points]
        if self.reduction == 'mean':
            normalizer = torch.as_tensor([targets.shape[0]], dtype=inputs.dtype, device=inputs.device)
            if is_dist_avail_and_initialized():
                torch.distributed.all_reduce(normalizer)
            normalizer = torch.clamp(normalizer / get_world_size(), min=1).item()
            loss = loss.sum() / normalizer
        elif self.reduction == 'sum':
            loss = loss.sum()
        return loss

    def classification_loss(self, inputs: Tensor, targets: Tensor) ->Tensor:
        return F.cross_entropy(inputs, targets, reduction=self.reduction)


class HungarianBezierLoss(WeightedLoss):

    def __init__(self, curve_weight=1, label_weight=0.1, seg_weight=0.75, alpha=0.8, num_sample_points=100, bezier_order=3, weight=None, size_average=None, reduce=None, reduction='mean', ignore_index=-100, weight_seg=None, k=9):
        super().__init__(weight, size_average, reduce, reduction)
        self.curve_weight = curve_weight
        self.label_weight = label_weight
        self.seg_weight = seg_weight
        self.weight_seg = weight_seg
        self.ignore_index = ignore_index
        self.bezier_sampler = BezierSampler(num_sample_points=num_sample_points, order=bezier_order)
        self.matcher = _HungarianMatcher(alpha=alpha, num_sample_points=num_sample_points, bezier_order=bezier_order, k=k)
        if self.weight is not None and not isinstance(self.weight, torch.Tensor):
            self.weight = torch.tensor(self.weight)
        if self.weight_seg is not None and not isinstance(self.weight_seg, torch.Tensor):
            self.weight_seg = torch.tensor(self.weight_seg)
        self.register_buffer('pos_weight', self.weight[1] / self.weight[0])
        self.register_buffer('pos_weight_seg', self.weight_seg[1] / self.weight_seg[0])

    def forward(self, inputs, targets, net):
        outputs = net(inputs)
        output_curves = outputs['curves']
        target_labels = torch.zeros_like(outputs['logits'])
        target_segmentations = torch.stack([target['segmentation_mask'] for target in targets])
        total_targets = 0
        for i in targets:
            total_targets += i['keypoints'].numel()
        if total_targets > 0:
            indices = self.matcher(outputs=outputs, targets=targets)
            idx = HungarianLoss.get_src_permutation_idx(indices)
            output_curves = output_curves[idx]
            target_keypoints = torch.cat([t['keypoints'][i] for t, (_, i) in zip(targets, indices)], dim=0)
            target_sample_points = torch.cat([t['sample_points'][i] for t, (_, i) in zip(targets, indices)], dim=0)
            target_keypoints = cubic_bezier_curve_segment(target_keypoints, target_sample_points)
            target_sample_points = self.bezier_sampler.get_sample_points(target_keypoints)
            target_labels[idx] = 1
        else:
            target_sample_points = torch.tensor([], dtype=torch.float32, device=output_curves.device)
        target_valid_points = get_valid_points(target_sample_points)
        loss_curve = self.point_loss(self.bezier_sampler.get_sample_points(output_curves), target_sample_points)
        loss_label = self.classification_loss(inputs=outputs['logits'], targets=target_labels)
        loss_seg = self.binary_seg_loss(inputs=outputs['segmentations'], targets=target_segmentations)
        loss = self.label_weight * loss_label + self.curve_weight * loss_curve + self.seg_weight * loss_seg
        return loss, {'training loss': loss, 'loss label': loss_label, 'loss curve': loss_curve, 'loss seg': loss_seg, 'valid portion': target_valid_points.float().mean()}

    def point_loss(self, inputs, targets, valid_points=None):
        if targets.numel() == 0:
            targets = inputs.clone().detach()
        loss = F.l1_loss(inputs, targets, reduction='none')
        if valid_points is not None:
            loss *= valid_points.unsqueeze(-1)
            normalizer = valid_points.sum()
        else:
            normalizer = targets.shape[0] * targets.shape[1]
            normalizer = torch.as_tensor([normalizer], dtype=inputs.dtype, device=inputs.device)
        if self.reduction == 'mean':
            if is_dist_avail_and_initialized():
                torch.distributed.all_reduce(normalizer)
            normalizer = torch.clamp(normalizer / get_world_size(), min=1).item()
            loss = loss.sum() / normalizer
        elif self.reduction == 'sum':
            loss = loss.sum()
        return loss

    def classification_loss(self, inputs, targets):
        return F.binary_cross_entropy_with_logits(inputs.unsqueeze(1), targets.unsqueeze(1), pos_weight=self.pos_weight, reduction=self.reduction) / self.pos_weight

    def binary_seg_loss(self, inputs, targets):
        inputs = torch.nn.functional.interpolate(inputs, size=targets.shape[-2:], mode='bilinear', align_corners=True)
        inputs = inputs.squeeze(1)
        valid_map = targets != self.ignore_index
        targets[~valid_map] = 0
        targets = targets.float()
        loss = F.binary_cross_entropy_with_logits(inputs, targets, pos_weight=self.pos_weight_seg, reduction='none') / self.pos_weight_seg
        loss *= valid_map
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


class LaneLoss(WeightedLoss):
    __constants__ = ['ignore_index', 'reduction']
    ignore_index: int

    def __init__(self, existence_weight: float=0.1, weight: Optional[Tensor]=None, size_average=None, ignore_index: int=-100, reduce=None, reduction: str='mean'):
        super(LaneLoss, self).__init__(weight, size_average, reduce, reduction)
        self.ignore_index = ignore_index
        self.existence_weight = existence_weight

    def forward(self, inputs: Tensor, targets: Tensor, lane_existence: Tensor, net, interp_size):
        outputs = net(inputs)
        prob_maps = torch.nn.functional.interpolate(outputs['out'], size=interp_size, mode='bilinear', align_corners=True)
        targets[targets > lane_existence.shape[-1]] = 255
        segmentation_loss = F.cross_entropy(prob_maps, targets, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
        existence_loss = F.binary_cross_entropy_with_logits(outputs['lane'], lane_existence, weight=None, pos_weight=None, reduction=self.reduction)
        total_loss = segmentation_loss + self.existence_weight * existence_loss
        return total_loss, {'training loss': total_loss, 'loss seg': segmentation_loss, 'loss exist': existence_loss}


class SADLoss(WeightedLoss):
    __constants__ = ['ignore_index', 'reduction']
    ignore_index: int

    def __init__(self, existence_weight: float=0.1, weight: Optional[Tensor]=None, size_average=None, ignore_index: int=-100, reduce=None, reduction: str='mean'):
        super(SADLoss, self).__init__(weight, size_average, reduce, reduction)
        self.ignore_index = ignore_index
        self.existence_weight = existence_weight

    def forward(self, inputs: Tensor, targets: Tensor):
        pass


INFINITY = 987654.0


class LaneAttLoss(WeightedLoss):

    def __init__(self, cls_weight: float=10.0, reg_weight: float=1.0, alpha: float=0.25, gamma: float=2.0, num_strips: int=72 - 1, num_offsets: int=72, t_pos: float=15.0, t_neg: float=20.0, weight: Optional[Tensor]=None, size_average=None, reduce=None, reduction: str='mean'):
        super(LaneAttLoss, self).__init__(weight, size_average, reduce, reduction)
        self.cls_weight = cls_weight
        self.reg_weight = reg_weight
        self.num_strips = num_strips
        self.num_offsets = num_offsets
        self.t_pos = t_pos
        self.t_neg = t_neg
        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma)
        self.smooth_l1_loss = torch.nn.SmoothL1Loss()

    def forward(self, inputs, targets, net):
        labels = {'offsets': torch.stack([i['offsets'] for i in targets], dim=0), 'starts': torch.stack([i['starts'] for i in targets], dim=0), 'lengths': torch.stack([i['lengths'] for i in targets], dim=0), 'flags': torch.stack([i['flags'] for i in targets], dim=0)}
        batch_size = inputs.shape[0]
        outputs = net(inputs)
        cls_loss = torch.tensor(0, dtype=torch.float32, device=inputs.device)
        reg_loss = torch.tensor(0, dtype=torch.float32, device=inputs.device)
        total_positives = 0
        for i in range(batch_size):
            target = {k: v[i][labels['flags'][i]] for k, v in labels.items()}
            if len(target['offsets']) == 0:
                cls_target = torch.zeros(outputs['logits'][i].shape[0], dtype=torch.long, device=outputs['logits'].device)
                cls_loss = cls_loss + self.focal_loss(outputs['logits'][i], cls_target).sum()
                continue
            positives_mask, invalid_offsets_mask, negatives_mask, target_positives_indices = self.match_proposals_with_targets(net.module.anchors.clone() if get_world_size() >= 2 else net.anchors.clone(), target)
            positives = {k: v[i][positives_mask] for k, v in outputs.items()}
            num_positives = positives['logits'].shape[0]
            total_positives += num_positives
            negatives = {k: v[i][negatives_mask] for k, v in outputs.items()}
            num_negatives = negatives['logits'].shape[0]
            if num_positives == 0:
                cls_target = torch.zeros(outputs['logits'][i].shape[0], dtype=torch.long, device=outputs['logits'].device)
                cls_loss = cls_loss + self.focal_loss(outputs['logits'][i], cls_target).sum()
                continue
            cls_target = torch.zeros(num_positives + num_negatives, dtype=torch.long, device=outputs['logits'].device)
            cls_target[:num_positives] = 1
            cls_pred = torch.cat([positives['logits'], negatives['logits']], dim=0)
            reg_pred = torch.cat([positives['lengths'][..., None], positives['offsets']], dim=1)
            with torch.no_grad():
                target = {k: v[target_positives_indices] for k, v in target.items()}
                positive_starts = (positives['starts'] * self.num_strips).round().long()
                targets_starts = (target['starts'] * self.num_strips).round().long()
                target['lengths'] -= positive_starts - targets_starts
                all_indices = torch.arange(num_positives, dtype=torch.long)
                ends = (positive_starts + target['lengths'] - 1).round().long()
                invalid_offsets_mask = torch.zeros((num_positives, 1 + self.num_offsets + 1), dtype=torch.int)
                invalid_offsets_mask[all_indices, 1 + positive_starts] = 1
                invalid_offsets_mask[all_indices, 1 + ends + 1] -= 1
                invalid_offsets_mask = invalid_offsets_mask.cumsum(dim=1) == 0
                invalid_offsets_mask = invalid_offsets_mask[:, :-1]
                invalid_offsets_mask[:, 0] = False
                reg_target = torch.cat([target['lengths'][..., None], target['offsets']], dim=1)
                reg_target[invalid_offsets_mask] = reg_pred[invalid_offsets_mask]
            reg_loss += self.smooth_l1_loss(reg_pred, reg_target)
            cls_loss += self.focal_loss(cls_pred, cls_target).sum() / num_positives
        if self.reduction == 'mean':
            reg_loss /= batch_size
            cls_loss /= batch_size
        elif self.reduction != 'sum':
            raise NotImplementedError
        total_loss = self.cls_weight * cls_loss + self.reg_weight * reg_loss
        return total_loss, {'total loss': total_loss, 'cls loss': cls_loss, 'reg loss': reg_loss, 'all positives': torch.tensor(total_positives)}

    @torch.no_grad()
    def match_proposals_with_targets(self, proposals, labels):
        num_proposals = proposals.shape[0]
        targets = torch.cat([labels['starts'][..., None], labels['lengths'][..., None], labels['offsets']], dim=1)
        num_targets = targets.shape[0]
        proposals_pad = proposals.new_zeros(proposals.shape[0], proposals.shape[1] + 1)
        proposals_pad[:, :-1] = proposals
        proposals = proposals_pad
        targets_pad = targets.new_zeros(targets.shape[0], targets.shape[1] + 1)
        targets_pad[:, :-1] = targets
        targets = targets_pad
        proposals = torch.repeat_interleave(proposals, num_targets, dim=0)
        targets = torch.cat(num_proposals * [targets])
        targets_starts = targets[:, 0] * self.num_strips
        proposals_starts = proposals[:, 0] * self.num_strips
        starts = torch.max(targets_starts.float(), proposals_starts).round().long()
        ends = (targets_starts + targets[:, 1].float() - 1.0).round().long()
        lengths = ends - starts + 1
        ends[lengths < 0] = starts[lengths < 0] - 1
        lengths[lengths < 0] = 0
        valid_offsets_mask = targets.new_zeros(targets.shape)
        all_indices = torch.arange(valid_offsets_mask.shape[0], dtype=torch.long, device=targets.device)
        valid_offsets_mask[all_indices, 2 + starts] = 1
        valid_offsets_mask[all_indices, 2 + ends + 1] -= 1
        valid_offsets_mask = valid_offsets_mask.cumsum(dim=1) != 0
        invalid_offsets_mask = ~valid_offsets_mask
        distances = torch.abs((targets - proposals) * valid_offsets_mask.float()).sum(dim=1) / (lengths.float() + 1e-09)
        distances[lengths == 0] = INFINITY
        invalid_offsets_mask = invalid_offsets_mask.view(num_proposals, num_targets, invalid_offsets_mask.shape[1])
        distances = distances.view(num_proposals, num_targets)
        positives = distances.min(dim=1)[0] < self.t_pos
        negatives = distances.min(dim=1)[0] > self.t_neg
        if positives.sum() == 0:
            target_positives_indices = torch.tensor([], device=positives.device, dtype=torch.long)
        else:
            target_positives_indices = distances[positives].argmin(dim=1)
        invalid_offsets_mask = invalid_offsets_mask[positives, target_positives_indices]
        return positives, invalid_offsets_mask[:, :-1], negatives, target_positives_indices


class WeightedCrossEntropyLoss(WeightedLoss):
    __constants__ = ['ignore_index', 'reduction']

    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean'):
        super().__init__(weight, size_average, reduce, reduction)
        self.ignore_index = ignore_index

    def forward(self, inputs, targets):
        return F.cross_entropy(inputs, targets, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)


class IntermediateLayerGetter(nn.ModuleDict):
    """
    Module wrapper that returns intermediate layers from a model

    It has a strong assumption that the modules have been registered
    into the model in the same order as they are used.
    This means that one should **not** reuse the same nn.Module
    twice in the forward if you want this to work.

    Additionally, it is only able to query submodules that are directly
    assigned to the model. So if `model` is passed, `model.feature1` can
    be returned, but not `model.feature1.layer2`.

    Arguments:
        model (nn.Module): model on which we will extract the features
        return_layers (Dict[name, new_name]): a dict containing the names
            of the modules for which the activations will be returned as
            the key of the dict, and the value of the dict is the name
            of the returned activation (which the user can specify).

    Examples::

        >>> m = torchvision.models.resnet18(pretrained=True)
        >>> # extract layer1 and layer3, giving as names `feat1` and feat2`
        >>> new_m = torchvision.models._utils.IntermediateLayerGetter(m,
        >>>     {'layer1': 'feat1', 'layer3': 'feat2'})
        >>> out = new_m(torch.rand(1, 3, 224, 224))
        >>> print([(k, v.shape) for k, v in out.items()])
        >>>     [('feat1', torch.Size([1, 64, 56, 56])),
        >>>      ('feat2', torch.Size([1, 256, 14, 14]))]
    """

    def __init__(self, model, return_layers):
        if not set(return_layers).issubset([name for name, _ in model.named_children()]):
            raise ValueError('return_layers are not present in model')
        orig_return_layers = return_layers
        return_layers = {k: v for k, v in return_layers.items()}
        layers = OrderedDict()
        for name, module in model.named_children():
            layers[name] = module
            if name in return_layers:
                del return_layers[name]
            if not return_layers:
                break
        super(IntermediateLayerGetter, self).__init__(layers)
        self.return_layers = orig_return_layers

    def forward(self, x):
        out = OrderedDict()
        for name, module in self.named_children():
            x = module(x)
            if name in self.return_layers:
                out_name = self.return_layers[name]
                out[out_name] = x
        return out


MODELS = SimpleRegistry()


class DilatedBottleneck(nn.Module):

    def __init__(self, in_channels=512, mid_channels=128, dilation=1):
        super().__init__()
        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, mid_channels, kernel_size=1, padding=0), nn.BatchNorm2d(mid_channels))
        self.conv2 = nn.Sequential(nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=dilation, dilation=dilation), nn.BatchNorm2d(mid_channels))
        self.conv3 = nn.Sequential(nn.Conv2d(mid_channels, in_channels, kernel_size=1, padding=0), nn.BatchNorm2d(in_channels))
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        identity = x
        out = self.conv1(x)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.relu(out)
        out = out + identity
        return out


class InvertedResidual(nn.Module):
    """InvertedResidual block for MobileNetV2.
    Args:
        in_channels (int): The input channels of the InvertedResidual block.
        out_channels (int): The output channels of the InvertedResidual block.
        stride (int): Stride of the middle (first) 3x3 convolution.
        expand_ratio (int): Adjusts number of channels of the hidden layer
            in InvertedResidual by this amount.
        dilation (int): Dilation rate of depthwise conv. Default: 1
    Returns:
        Tensor: The output tensor.
    """

    def __init__(self, in_channels, out_channels, stride, expand_ratio, dilation=1, bias=False):
        super(InvertedResidual, self).__init__()
        self.stride = stride
        assert stride in [1, 2], f'stride must in [1, 2]. But received {stride}.'
        self.use_res_connect = self.stride == 1 and in_channels == out_channels
        hidden_dim = int(round(in_channels * expand_ratio))
        layers = []
        if expand_ratio != 1:
            layers.extend([nn.Conv2d(in_channels=in_channels, out_channels=hidden_dim, kernel_size=1, bias=bias), nn.BatchNorm2d(hidden_dim), nn.ReLU6()])
        layers.extend([nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, groups=hidden_dim, bias=bias), nn.BatchNorm2d(hidden_dim), nn.ReLU6(), nn.Conv2d(in_channels=hidden_dim, out_channels=out_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(out_channels)])
        self.conv = nn.Sequential(*layers)

    def forward(self, x):

        def _inner_forward(x):
            if self.use_res_connect:
                return x + self.conv(x)
            else:
                return self.conv(x)
        out = _inner_forward(x)
        return out


def make_divisible(value, divisor, min_value=None, min_ratio=0.9):
    """Make divisible function.
    This function rounds the channel number to the nearest value that can be
    divisible by the divisor. It is taken from the original tf repo. It ensures
    that all layers have a channel number that is divisible by divisor. It can
    be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py  # noqa
    Args:
        value (int): The original channel number.
        divisor (int): The divisor to fully divide the channel number.
        min_value (int): The minimum value of the output channel.
            Default: None, means that the minimum value equal to the divisor.
        min_ratio (float): The minimum ratio of the rounded channel number to
            the original channel number. Default: 0.9.
    Returns:
        int: The modified output channel number.
    """
    if min_value is None:
        min_value = divisor
    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)
    if new_value < min_ratio * value:
        new_value += divisor
    return new_value


class SELayer(nn.Module):
    """Squeeze-and-Excitation Module.
    Args:
        channels (int): The input (and output) channels of the SE layer.
        ratio (int): Squeeze ratio in SELayer, the intermediate channel will be
            ``int(channels/ratio)``. Default: 16.
    """

    def __init__(self, channels, ratio=16, act=nn.ReLU, scale_act=nn.Sigmoid):
        super(SELayer, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Conv2d(in_channels=channels, out_channels=make_divisible(channels // ratio, 8), kernel_size=1, stride=1)
        self.fc2 = nn.Conv2d(in_channels=make_divisible(channels // ratio, 8), out_channels=channels, kernel_size=1, stride=1)
        self.activation = act()
        self.scale_activation = scale_act()

    def forward(self, x):
        out = self.avgpool(x)
        out = self.fc1(out)
        out = self.activation(out)
        out = self.fc2(out)
        out = self.scale_activation(out)
        return x * out


class InvertedResidualV3(nn.Module):
    """Inverted Residual Block for MobileNetV3.
    Args:
        in_channels (int): The input channels of this Module.
        out_channels (int): The output channels of this Module.
        mid_channels (int): The input channels of the depthwise convolution.
        kernel_size (int): The kernel size of the depthwise convolution. Default: 3.
        stride (int): The stride of the depthwise convolution. Default: 1.
        with_se (dict): with or without se layer. Default: False, which means no se layer.
        with_expand_conv (bool): Use expand conv or not. If set False,
            mid_channels must be the same with in_channels. Default: True.
        act_cfg (dict): Config dict for activation layer.
            Default: dict(type='ReLU').
    Returns:
        Tensor: The output tensor.
    """

    def __init__(self, in_channels, out_channels, mid_channels, kernel_size=3, stride=1, with_se=False, with_expand_conv=True, act='HSwish', bias=False, dilation=1):
        super(InvertedResidualV3, self).__init__()
        self.with_res_shortcut = stride == 1 and in_channels == out_channels
        assert stride in [1, 2]
        activation_layer = nn.Hardswish if act == 'HSwish' else nn.ReLU6
        self.with_se = with_se
        self.with_expand_conv = with_expand_conv
        if not self.with_expand_conv:
            assert mid_channels == in_channels
        if self.with_expand_conv:
            self.expand_conv = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=1, stride=1, padding=0, bias=bias), nn.BatchNorm2d(mid_channels), activation_layer())
        if stride > 1 and dilation > 1:
            raise ValueError("Can't have stride and dilation both > 1 in MobileNetV3")
        self.depthwise_conv = nn.Sequential(nn.Conv2d(in_channels=mid_channels, out_channels=mid_channels, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2 * dilation, dilation=dilation, groups=mid_channels, bias=bias), nn.BatchNorm2d(mid_channels), activation_layer())
        if self.with_se:
            self.se = SELayer(channels=mid_channels, ratio=4)
        self.linear_conv = nn.Sequential(nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False), nn.BatchNorm2d(out_channels))

    def forward(self, x):

        def _inner_forward(x):
            out = x
            if self.with_expand_conv:
                out = self.expand_conv(out)
            out = self.depthwise_conv(out)
            if self.with_se:
                out = self.se(out)
            out = self.linear_conv(out)
            if self.with_res_shortcut:
                return x + out
            else:
                return out
        out = _inner_forward(x)
        return out


class non_bottleneck_1d(nn.Module):

    def __init__(self, chann, dropprob, dilated):
        super().__init__()
        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1, 0), bias=True)
        self.conv1x3_1 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1), bias=True)
        self.bn1 = nn.BatchNorm2d(chann, eps=0.001)
        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1 * dilated, 0), bias=True, dilation=(dilated, 1))
        self.conv1x3_2 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1 * dilated), bias=True, dilation=(1, dilated))
        self.bn2 = nn.BatchNorm2d(chann, eps=0.001)
        self.dropout = nn.Dropout2d(dropprob)

    def forward(self, input):
        output = self.conv3x1_1(input)
        output = F.relu(output)
        output = self.conv1x3_1(output)
        output = self.bn1(output)
        output = F.relu(output)
        output = self.conv3x1_2(output)
        output = F.relu(output)
        output = self.conv1x3_2(output)
        output = self.bn2(output)
        if self.dropout.p != 0:
            output = self.dropout(output)
        return F.relu(output + input)


class BilateralUpsamplerBlock(nn.Module):

    def __init__(self, ninput, noutput):
        super(BilateralUpsamplerBlock, self).__init__()
        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)
        self.bn = nn.BatchNorm2d(noutput, eps=0.001, track_running_stats=True)
        self.follows = nn.ModuleList(non_bottleneck_1d(noutput, 0, 1) for _ in range(2))
        self.interpolate_conv = nn.Conv2d(ninput, noutput, kernel_size=1, bias=False)
        self.interpolate_bn = nn.BatchNorm2d(noutput, eps=0.001)

    def forward(self, input):
        output = self.conv(input)
        output = self.bn(output)
        out = F.relu(output)
        for follow in self.follows:
            out = follow(out)
        interpolate_output = self.interpolate_conv(input)
        interpolate_output = self.interpolate_bn(interpolate_output)
        interpolate_output = F.relu(interpolate_output)
        interpolated = F.interpolate(interpolate_output, size=out.shape[-2:], mode='bilinear', align_corners=True)
        return out + interpolated


class BUSD(nn.Module):

    def __init__(self, in_channels=128, num_classes=5):
        super(BUSD, self).__init__()
        base = in_channels // 8
        self.layers = nn.ModuleList(BilateralUpsamplerBlock(ninput=base * 2 ** (3 - i), noutput=base * 2 ** (2 - i)) for i in range(3))
        self.output_proj = nn.Conv2d(base, num_classes, kernel_size=1, bias=True)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return self.output_proj(x)


class EDLaneExist(nn.Module):

    def __init__(self, num_output, flattened_size=3965, dropout=0.1, pool='avg'):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(nn.Conv2d(128, 32, (3, 3), stride=1, padding=(4, 4), bias=False, dilation=(4, 4)))
        self.layers.append(nn.BatchNorm2d(32, eps=0.001))
        self.layers_final = nn.ModuleList()
        self.layers_final.append(nn.Dropout2d(dropout))
        self.layers_final.append(nn.Conv2d(32, 5, (1, 1), stride=1, padding=(0, 0), bias=True))
        if pool == 'max':
            self.pool = nn.MaxPool2d(2, stride=2)
        elif pool == 'avg':
            self.pool = nn.AvgPool2d(2, stride=2)
        else:
            raise RuntimeError('This type of pool has not been defined yet!')
        self.linear1 = nn.Linear(flattened_size, 128)
        self.linear2 = nn.Linear(128, num_output)

    def forward(self, input):
        output = input
        for layer in self.layers:
            output = layer(output)
        output = F.relu(output)
        for layer in self.layers_final:
            output = layer(output)
        output = F.softmax(output, dim=1)
        output = self.pool(output)
        output = output.flatten(start_dim=1)
        output = self.linear1(output)
        output = F.relu(output)
        output = self.linear2(output)
        return output


class PlainDecoder(nn.Module):

    def __init__(self, in_channels=128, num_classes=5):
        super(PlainDecoder, self).__init__()
        self.dropout1 = nn.Dropout2d(0.1)
        self.conv1 = nn.Conv2d(in_channels, num_classes, 1, bias=True)

    def forward(self, x):
        x = self.dropout1(x)
        x = self.conv1(x)
        return x


class RESALaneExist(nn.Module):

    def __init__(self, num_output, flattened_size=3965, dropout=0.1, in_channels=128):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(nn.Dropout2d(dropout))
        self.layers.append(nn.Conv2d(in_channels, num_output + 1, (1, 1), stride=1, padding=(0, 0), bias=True))
        self.pool = nn.AvgPool2d(2, stride=2)
        self.linear1 = nn.Linear(flattened_size, 128)
        self.linear2 = nn.Linear(128, num_output)

    def forward(self, input):
        output = input
        for layer in self.layers:
            output = layer(output)
        output = F.softmax(output, dim=1)
        output = self.pool(output)
        output = output.flatten(start_dim=1)
        output = self.linear1(output)
        output = F.relu(output)
        output = self.linear2(output)
        return output


class SCNNDecoder(nn.Module):

    def __init__(self, in_channels=2048, num_classes=5):
        super(SCNNDecoder, self).__init__()
        out_channels = in_channels // 4
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.dropout1 = nn.Dropout2d(0.1)
        self.conv2 = nn.Conv2d(out_channels, num_classes, 1, bias=False)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout1(x)
        x = self.conv2(x)
        return x


class SimpleLaneExist(nn.Module):

    def __init__(self, num_output, flattened_size=4500):
        super().__init__()
        self.avgpool = nn.AvgPool2d(2, 2)
        self.linear1 = nn.Linear(flattened_size, 128)
        self.linear2 = nn.Linear(128, num_output)

    def forward(self, input, predict=False):
        output = self.avgpool(input)
        output = output.flatten(start_dim=1)
        output = self.linear1(output)
        output = F.relu(output)
        output = self.linear2(output)
        if predict:
            output = torch.sigmoid(output)
        return output


class PPM(nn.ModuleList):
    """
    Pooling pyramid module used in PSPNet
    Args:
        pool_scales(tuple(int)): Pooling scales used in pooling Pyramid Module
        applied on the last feature. default: (1, 2, 3, 6)
    """

    def __init__(self, pool_scales, in_channels, channels, align_corners=False):
        super(PPM, self).__init__()
        self.pool_scales = pool_scales
        self.align_corners = align_corners
        self.in_channels = in_channels
        self.channels = channels
        for pool_scale in pool_scales:
            self.append(nn.Sequential(nn.AdaptiveAvgPool2d(pool_scale), nn.Conv2d(self.in_channels, self.channels, 1), nn.BatchNorm2d(self.channels), nn.ReLU()))

    def forward(self, x):
        ppm_outs = []
        for ppm in self:
            ppm_out = ppm(x)
            upsampled_ppm_out = F.interpolate(ppm_out, size=x.size()[2:], mode='bilinear', align_corners=self.align_corners)
            ppm_outs.append(upsampled_ppm_out)
        return ppm_outs


class UperHead(nn.Module):

    def __init__(self, in_channels, channels, pool_scales=(1, 2, 3, 6), align_corners=False):
        super(UperHead, self).__init__()
        self.in_channels = in_channels
        self.channels = channels
        self.align_corners = align_corners
        self.psp_modules = PPM(pool_scales=pool_scales, in_channels=self.in_channels[-1], channels=self.channels, align_corners=align_corners)
        self.psp_bottleneck = nn.Sequential(nn.Conv2d(in_channels=self.in_channels[-1] + len(pool_scales) * self.channels, out_channels=self.channels, kernel_size=3, padding=1), nn.BatchNorm2d(self.channels), nn.ReLU())
        self.lateral_convs = nn.ModuleList()
        self.fpn_convs = nn.ModuleList()
        for in_channel in self.in_channels[:-1]:
            lateral_conv = nn.Sequential(nn.Conv2d(in_channel, self.channels, 1), nn.BatchNorm2d(self.channels), nn.ReLU())
            fpn_conv = nn.Sequential(nn.Conv2d(self.channels, self.channels, 3, padding=1), nn.BatchNorm2d(self.channels), nn.ReLU())
            self.lateral_convs.append(lateral_conv)
            self.fpn_convs.append(fpn_conv)
        self.fpn_bottleneck = nn.Sequential(nn.Conv2d(len(self.in_channels) * self.channels, self.channels, 3, padding=1), nn.BatchNorm2d(self.channels), nn.ReLU())

    def psp_forward(self, inputs):
        x = inputs[-1]
        psp_outs = [x]
        psp_outs.extend(self.psp_modules(x))
        psp_outs = torch.cat(psp_outs, dim=1)
        outputs = self.psp_bottleneck(psp_outs)
        return outputs

    def forward(self, inputs):
        assert isinstance(inputs, tuple), 'inputs must be a tuple'
        inputs = list(inputs)
        laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]
        laterals.append(self.psp_forward(inputs))
        used_backbone_levels = len(laterals)
        for i in range(used_backbone_levels - 1, 0, -1):
            prev_shape = laterals[i - 1].shape[2:]
            laterals[i - 1] = laterals[i - 1] + F.interpolate(laterals[i], size=prev_shape, mode='bilinear', align_corners=self.align_corners)
        fpn_outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels - 1)]
        fpn_outs.append(laterals[-1])
        for i in range(used_backbone_levels - 1, 0, -1):
            fpn_outs[i] = F.interpolate(fpn_outs[i], fpn_outs[0].shape[2:], mode='bilinear', align_corners=self.align_corners)
        fpn_outs = torch.cat(fpn_outs, dim=1)
        output = self.fpn_bottleneck(fpn_outs)
        return output


class ConvProjection_1D(torch.nn.Module):

    def __init__(self, num_layers, in_channels, bias=True, k=3):
        super().__init__()
        self.num_layers = num_layers
        self.hidden_layers = nn.ModuleList(nn.Conv1d(in_channels if i > 0 else in_channels, in_channels, kernel_size=k, bias=bias, padding=(k - 1) // 2) for i in range(num_layers))
        self.hidden_norms = nn.ModuleList(nn.BatchNorm1d(in_channels) for _ in range(num_layers))

    def forward(self, x):
        for conv, norm in zip(self.hidden_layers, self.hidden_norms):
            x = F.relu(norm(conv(x)))
        return x


class FeatureFlipFusion(nn.Module):

    def __init__(self, channels):
        super().__init__()
        self.proj1 = nn.Sequential(nn.Conv2d(channels, channels, kernel_size=1, padding=0), nn.BatchNorm2d(channels))
        self.proj2_conv = DCN_v2_Ref(channels, channels, kernel_size=(3, 3), padding=1)
        self.proj2_norm = nn.BatchNorm2d(channels)

    def forward(self, feature):
        flipped = feature.flip(-1)
        feature = self.proj1(feature)
        flipped = self.proj2_conv(flipped, feature)
        flipped = self.proj2_norm(flipped)
        return F.relu(feature + flipped)


def is_tracing() ->bool:
    if torch.__version__ >= '1.7.0':
        return torch.jit.is_tracing()
    elif torch.__version__ >= '1.6.0':
        return torch._C._is_tracing()
    else:
        return False


class RESA(nn.Module):

    def __init__(self, num_channels=128, iteration=5, alpha=2.0, trace_arg=None, os=8):
        super(RESA, self).__init__()
        self.iteration = iteration
        self.alpha = alpha
        self.conv_d = nn.ModuleList(nn.Conv2d(num_channels, num_channels, (1, 9), padding=(0, 4), bias=False) for _ in range(iteration))
        self.conv_u = nn.ModuleList(nn.Conv2d(num_channels, num_channels, (1, 9), padding=(0, 4), bias=False) for _ in range(iteration))
        self.conv_r = nn.ModuleList(nn.Conv2d(num_channels, num_channels, (9, 1), padding=(4, 0), bias=False) for _ in range(iteration))
        self.conv_l = nn.ModuleList(nn.Conv2d(num_channels, num_channels, (9, 1), padding=(4, 0), bias=False) for _ in range(iteration))
        self._adjust_initializations(num_channels=num_channels)
        if trace_arg is not None:
            h = (trace_arg['h'] - 1) // os + 1
            w = (trace_arg['w'] - 1) // os + 1
            self.offset_h = []
            self.offset_w = []
            for i in range(self.iteration):
                self.offset_h.append(h // 2 ** (self.iteration - i))
                self.offset_w.append(w // 2 ** (self.iteration - i))

    def _adjust_initializations(self, num_channels=128):
        bound = math.sqrt(2.0 / (num_channels * 9 * 5))
        for i in self.conv_d:
            nn.init.uniform_(i.weight, -bound, bound)
        for i in self.conv_u:
            nn.init.uniform_(i.weight, -bound, bound)
        for i in self.conv_r:
            nn.init.uniform_(i.weight, -bound, bound)
        for i in self.conv_l:
            nn.init.uniform_(i.weight, -bound, bound)

    def forward(self, x):
        y = x
        h, w = y.shape[-2:]
        if 2 ** self.iteration > min(h, w):
            None
        for i in range(self.iteration):
            if is_tracing():
                temp = torch.cat([y[:, :, self.offset_h[i]:, :], y[:, :, :self.offset_h[i], :]], dim=-2)
                y = y.add(self.alpha * F.relu(self.conv_d[i](temp)))
            else:
                idx = (torch.arange(h) + h // 2 ** (self.iteration - i)) % h
                y.add_(self.alpha * F.relu(self.conv_d[i](y[:, :, idx, :])))
        for i in range(self.iteration):
            if is_tracing():
                temp = torch.cat([y[:, :, h - self.offset_h[i]:, :], y[:, :, :h - self.offset_h[i], :]], dim=-2)
                y = y.add(self.alpha * F.relu(self.conv_u[i](temp)))
            else:
                idx = (torch.arange(h) - h // 2 ** (self.iteration - i)) % h
                y.add_(self.alpha * F.relu(self.conv_u[i](y[:, :, idx, :])))
        for i in range(self.iteration):
            if is_tracing():
                temp = torch.cat([y[:, :, :, self.offset_w[i]:], y[:, :, :, :self.offset_w[i]]], dim=-1)
                y = y.add(self.alpha * F.relu(self.conv_r[i](temp)))
            else:
                idx = (torch.arange(w) + w // 2 ** (self.iteration - i)) % w
                y.add_(self.alpha * F.relu(self.conv_r[i](y[:, :, :, idx])))
        for i in range(self.iteration):
            if is_tracing():
                temp = torch.cat([y[:, :, :, w - self.offset_w[i]:], y[:, :, :, :w - self.offset_w[i]]], dim=-1)
                y = y.add(self.alpha * F.relu(self.conv_l[i](temp)))
            else:
                idx = (torch.arange(w) - w // 2 ** (self.iteration - i)) % w
                y.add_(self.alpha * F.relu(self.conv_l[i](y[:, :, :, idx])))
        return y


class RESAReducer(nn.Module):

    def __init__(self, in_channels=512, reduce=128, bn_relu=True):
        super(RESAReducer, self).__init__()
        self.bn_relu = bn_relu
        self.conv1 = nn.Conv2d(in_channels, reduce, 1, bias=False)
        if self.bn_relu:
            self.bn1 = nn.BatchNorm2d(reduce)

    def forward(self, x):
        x = self.conv1(x)
        if self.bn_relu:
            x = self.bn1(x)
            x = F.relu(x)
        return x


class SpatialConv(nn.Module):

    def __init__(self, num_channels=128):
        super().__init__()
        self.conv_d = nn.Conv2d(num_channels, num_channels, (1, 9), padding=(0, 4))
        self.conv_u = nn.Conv2d(num_channels, num_channels, (1, 9), padding=(0, 4))
        self.conv_r = nn.Conv2d(num_channels, num_channels, (9, 1), padding=(4, 0))
        self.conv_l = nn.Conv2d(num_channels, num_channels, (9, 1), padding=(4, 0))
        self._adjust_initializations(num_channels=num_channels)

    def _adjust_initializations(self, num_channels=128):
        bound = math.sqrt(2.0 / (num_channels * 9 * 5))
        nn.init.uniform_(self.conv_d.weight, -bound, bound)
        nn.init.uniform_(self.conv_u.weight, -bound, bound)
        nn.init.uniform_(self.conv_r.weight, -bound, bound)
        nn.init.uniform_(self.conv_l.weight, -bound, bound)

    def forward(self, input):
        output = input
        if is_tracing():
            for i in range(1, output.shape[2]):
                output[:, :, i:i + 1, :] = output[:, :, i:i + 1, :].add(F.relu(self.conv_d(output[:, :, i - 1:i, :])))
            for i in range(output.shape[2] - 2, 0, -1):
                output[:, :, i:i + 1, :] = output[:, :, i:i + 1, :].add(F.relu(self.conv_u(output[:, :, i + 1:i + 2, :])))
            for i in range(1, output.shape[3]):
                output[:, :, :, i:i + 1] = output[:, :, :, i:i + 1].add(F.relu(self.conv_r(output[:, :, :, i - 1:i])))
            for i in range(output.shape[3] - 2, 0, -1):
                output[:, :, :, i:i + 1] = output[:, :, :, i:i + 1].add(F.relu(self.conv_l(output[:, :, :, i + 1:i + 2])))
        else:
            for i in range(1, output.shape[2]):
                output[:, :, i:i + 1, :].add_(F.relu(self.conv_d(output[:, :, i - 1:i, :])))
            for i in range(output.shape[2] - 2, 0, -1):
                output[:, :, i:i + 1, :].add_(F.relu(self.conv_u(output[:, :, i + 1:i + 2, :])))
            for i in range(1, output.shape[3]):
                output[:, :, :, i:i + 1].add_(F.relu(self.conv_r(output[:, :, :, i - 1:i])))
            for i in range(output.shape[3] - 2, 0, -1):
                output[:, :, :, i:i + 1].add_(F.relu(self.conv_l(output[:, :, :, i + 1:i + 2])))
        return output


class DownsamplerBlock(nn.Module):

    def __init__(self, ninput, noutput):
        super().__init__()
        self.conv = nn.Conv2d(ninput, noutput - ninput, (3, 3), stride=2, padding=1, bias=True)
        self.pool = nn.MaxPool2d(2, stride=2)
        self.bn = nn.BatchNorm2d(noutput, eps=0.001)

    def forward(self, input):
        output = torch.cat([self.conv(input), self.pool(input)], 1)
        output = self.bn(output)
        return F.relu(output)


class ERFNetEncoder(nn.Module):

    def __init__(self, num_classes, dropout_1=0.03, dropout_2=0.3, pretrained_weights=None):
        super().__init__()
        self.initial_block = DownsamplerBlock(3, 16)
        self.layers = nn.ModuleList()
        self.layers.append(DownsamplerBlock(16, 64))
        for x in range(0, 5):
            self.layers.append(non_bottleneck_1d(64, dropout_1, 1))
        self.layers.append(DownsamplerBlock(64, 128))
        for x in range(0, 2):
            self.layers.append(non_bottleneck_1d(128, dropout_2, 2))
            self.layers.append(non_bottleneck_1d(128, dropout_2, 4))
            self.layers.append(non_bottleneck_1d(128, dropout_2, 8))
            self.layers.append(non_bottleneck_1d(128, dropout_2, 16))
        if pretrained_weights is not None:
            self._load_encoder_weights(pretrained_weights)
        else:
            self._init_weights()

    def _init_weights(self):
        pass

    def _load_encoder_weights(self, pretrained_weights):
        try:
            saved_weights = torch.load(pretrained_weights)['state_dict']
        except FileNotFoundError:
            raise FileNotFoundError('pretrained_weights is not there! Please set pretrained_weights=None if you are only testing.')
        original_weights = self.state_dict()
        for key in saved_weights.keys():
            my_key = key.replace('module.features.', '')
            if my_key in original_weights.keys():
                original_weights[my_key] = saved_weights[key]
        self.load_state_dict(original_weights)

    def forward(self, input):
        output = self.initial_block(input)
        for layer in self.layers:
            output = layer(output)
        return output


class BezierCurve(object):

    def __init__(self, order, num_sample_points=50):
        self.num_point = order + 1
        self.control_points = []
        self.bezier_coeff = self.get_bezier_coefficient()
        self.num_sample_points = num_sample_points
        self.c_matrix = self.get_bernstein_matrix()

    def get_bezier_coefficient(self):
        Mtk = lambda n, t, k: t ** k * (1 - t) ** (n - k) * n_over_k(n, k)
        BezierCoeff = lambda ts: [[Mtk(self.num_point - 1, t, k) for k in range(self.num_point)] for t in ts]
        return BezierCoeff

    def interpolate_lane(self, x, y, n=50):
        assert len(x) == len(y)
        tck, _ = splprep([x, y], s=0, t=n, k=min(3, len(x) - 1))
        u = np.linspace(0.0, 1.0, n)
        return np.array(splev(u, tck)).T

    def get_control_points(self, x, y, interpolate=False):
        if interpolate:
            points = self.interpolate_lane(x, y)
            x = np.array([x for x, _ in points])
            y = np.array([y for _, y in points])
        middle_points = self.get_middle_control_points(x, y)
        for idx in range(0, len(middle_points) - 1, 2):
            self.control_points.append([middle_points[idx], middle_points[idx + 1]])

    def get_bernstein_matrix(self):
        tokens = np.linspace(0, 1, self.num_sample_points)
        c_matrix = self.bezier_coeff(tokens)
        return np.array(c_matrix)

    def save_control_points(self):
        return self.control_points

    def assign_control_points(self, control_points):
        self.control_points = control_points

    def quick_sample_point(self, image_size=None):
        control_points_matrix = np.array(self.control_points)
        sample_points = self.c_matrix.dot(control_points_matrix)
        if image_size is not None:
            sample_points[:, 0] = sample_points[:, 0] * image_size[-1]
            sample_points[:, -1] = sample_points[:, -1] * image_size[0]
        return sample_points

    def get_sample_point(self, n=50, image_size=None):
        """
            :param n: the number of sampled points
            :return: a list of sampled points
        """
        t = np.linspace(0, 1, n)
        coeff_matrix = np.array(self.bezier_coeff(t))
        control_points_matrix = np.array(self.control_points)
        sample_points = coeff_matrix.dot(control_points_matrix)
        if image_size is not None:
            sample_points[:, 0] = sample_points[:, 0] * image_size[-1]
            sample_points[:, -1] = sample_points[:, -1] * image_size[0]
        return sample_points

    def get_middle_control_points(self, x, y):
        dy = y[1:] - y[:-1]
        dx = x[1:] - x[:-1]
        dt = (dx ** 2 + dy ** 2) ** 0.5
        t = dt / dt.sum()
        t = np.hstack(([0], t))
        t = t.cumsum()
        data = np.column_stack((x, y))
        Pseudoinverse = np.linalg.pinv(self.bezier_coeff(t))
        control_points = Pseudoinverse.dot(data)
        medi_ctp = control_points[:, :].flatten().tolist()
        return medi_ctp


def lane_pruning(existence, existence_conf, max_lane):
    while (existence.sum(dim=1) > max_lane).sum() > 0:
        indices = (existence.sum(dim=1, keepdim=True) > max_lane).expand_as(existence) * (existence_conf == existence_conf.min(dim=1, keepdim=True).values)
        existence[indices] = 0
        existence_conf[indices] = 1.1
    return existence, existence_conf


class BezierBaseNet(torch.nn.Module):

    def __init__(self, thresh=0.5, local_maximum_window_size=9):
        super().__init__()
        self.thresh = thresh
        self.local_maximum_window_size = local_maximum_window_size

    def forward(self, *args, **kwargs):
        raise NotImplementedError

    @staticmethod
    def bezier_to_coordinates(control_points, existence, resize_shape, dataset, bezier_curve, ppl=56, gap=10):
        H, W = resize_shape
        cps_of_lanes = []
        for flag, cp in zip(existence, control_points):
            if flag:
                cps_of_lanes.append(cp.tolist())
        coordinates = []
        for cps_of_lane in cps_of_lanes:
            bezier_curve.assign_control_points(cps_of_lane)
            if dataset == 'tusimple':
                bezier_threshold = 5.0 / H
                h_samples = np.array([(1.0 - (ppl - i) * gap / H) for i in range(ppl)], dtype=np.float32)
                sampled_points = bezier_curve.quick_sample_point(image_size=None)
                temp = []
                dis = np.abs(np.expand_dims(h_samples, -1) - sampled_points[:, 1])
                idx = np.argmin(dis, axis=-1)
                for i in range(ppl):
                    h = H - (ppl - i) * gap
                    if dis[i][idx[i]] > bezier_threshold or sampled_points[idx[i]][0] > 1 or sampled_points[idx[i]][0] < 0:
                        temp.append([-2, h])
                    else:
                        temp.append([sampled_points[idx[i]][0] * W, h])
                coordinates.append(temp)
            elif dataset in ['culane', 'llamas']:
                temp = bezier_curve.quick_sample_point(image_size=None)
                temp[:, 0] = temp[:, 0] * W
                temp[:, 1] = temp[:, 1] * H
                coordinates.append(temp.tolist())
            else:
                raise ValueError
        return coordinates

    @torch.no_grad()
    def inference(self, inputs, input_sizes, gap, ppl, dataset, max_lane=0, forward=True, return_cps=False, n=50):
        outputs = self.forward(inputs) if forward else inputs
        existence_conf = outputs['logits'].sigmoid()
        existence = existence_conf > self.thresh
        if self.local_maximum_window_size > 0:
            _, max_indices = torch.nn.functional.max_pool1d(existence_conf.unsqueeze(1), kernel_size=self.local_maximum_window_size, stride=1, padding=(self.local_maximum_window_size - 1) // 2, return_indices=True)
            max_indices = max_indices.squeeze(1)
            indices = torch.arange(0, existence_conf.shape[1], dtype=existence_conf.dtype, device=existence_conf.device).unsqueeze(0).expand_as(max_indices)
            local_maxima = max_indices == indices
            existence *= local_maxima
        control_points = outputs['curves']
        if max_lane != 0:
            existence, _ = lane_pruning(existence, existence_conf, max_lane=max_lane)
        if return_cps:
            image_size = torch.tensor([input_sizes[1][1], input_sizes[1][0]], dtype=torch.float32, device=control_points.device)
            cps = control_points * image_size
            cps = [cps[i][existence[i]].cpu().numpy() for i in range(existence.shape[0])]
        existence = existence.cpu().numpy()
        control_points = control_points.cpu().numpy()
        H, _ = input_sizes[1]
        b = BezierCurve(order=3, num_sample_points=H if dataset == 'tusimple' else n)
        lane_coordinates = []
        for j in range(existence.shape[0]):
            lane_coordinates.append(self.bezier_to_coordinates(control_points=control_points[j], existence=existence[j], resize_shape=input_sizes[1], dataset=dataset, bezier_curve=b, gap=gap, ppl=ppl))
        if return_cps:
            return cps, lane_coordinates
        else:
            return lane_coordinates


class autocast(object):

    def __init__(self, enabled=True):
        pass

    def __enter__(self):
        pass

    def __exit__(self, *args):
        pass

    def __call__(self, func):

        @functools.wraps(func)
        def decorate_autocast(*args, **kwargs):
            with self:
                return func(*args, **kwargs)
        return decorate_autocast


class BezierLaneNet(BezierBaseNet):

    def __init__(self, backbone_cfg, reducer_cfg, dilated_blocks_cfg, feature_fusion_cfg, head_cfg, aux_seg_head_cfg, image_height=360, num_regression_parameters=8, thresh=0.5, local_maximum_window_size=9):
        super(BezierLaneNet, self).__init__(thresh, local_maximum_window_size)
        global_stride = 16
        branch_channels = 256
        self.backbone = MODELS.from_dict(backbone_cfg)
        self.reducer = MODELS.from_dict(reducer_cfg)
        self.dilated_blocks = MODELS.from_dict(dilated_blocks_cfg)
        self.simple_flip_2d = MODELS.from_dict(feature_fusion_cfg)
        self.aggregator = nn.AvgPool2d(kernel_size=((image_height - 1) // global_stride + 1, 1), stride=1, padding=0)
        self.regression_head = MODELS.from_dict(head_cfg)
        self.proj_classification = nn.Conv1d(branch_channels, 1, kernel_size=1, bias=True, padding=0)
        self.proj_regression = nn.Conv1d(branch_channels, num_regression_parameters, kernel_size=1, bias=True, padding=0)
        self.segmentation_head = MODELS.from_dict(aux_seg_head_cfg)

    def forward(self, x):
        x = self.backbone(x)
        if isinstance(x, dict):
            x = x['out']
        if self.reducer is not None:
            x = self.reducer(x)
        if self.segmentation_head is not None:
            segmentations = self.segmentation_head(x)
        else:
            segmentations = None
        if self.dilated_blocks is not None:
            x = self.dilated_blocks(x)
        with autocast(False):
            x = self.simple_flip_2d(x.float())
        x = self.aggregator(x)[:, :, 0, :]
        x = self.regression_head(x)
        logits = self.proj_classification(x).squeeze(1)
        curves = self.proj_regression(x)
        return {'logits': logits, 'curves': curves.permute(0, 2, 1).reshape(curves.shape[0], -1, curves.shape[-2] // 2, 2).contiguous(), 'segmentations': segmentations}

    def eval(self, profiling=False):
        super().eval()
        if profiling:
            self.segmentation_head = None


csrc_path = 'utils/csrc'


def line_nms(boxes, scores, overlap, top_k):
    return line_nms_.forward(boxes.contiguous(), scores.contiguous(), overlap, top_k)


class LaneAtt(nn.Module):
    left_angles = [72.0, 60.0, 49.0, 39.0, 30.0, 22.0]
    right_angles = [108.0, 120.0, 131.0, 141.0, 150.0, 158.0]
    bottom_angles = [165.0, 150.0, 141.0, 131.0, 120.0, 108.0, 100.0, 90.0, 80.0, 72.0, 60.0, 49.0, 39.0, 30.0, 15.0]

    def __init__(self, backbone_cfg, backbone_channels, backbone_os, num_points=72, img_w=640, img_h=360, topk_anchors=None, anchor_freq_path=None, anchor_feat_channels=None, conf_thres=None, nms_thres=0, nms_topk=3000, trace_arg=None):
        super().__init__()
        self.backbone = MODELS.from_dict(backbone_cfg)
        self.backbone_channels = backbone_channels
        self.stride = backbone_os
        self.num_strips = num_points - 1
        self.num_offsets = num_points
        self.img_h = img_h
        self.img_w = img_w
        self.featmap_h = img_h // self.stride
        self.featmap_w = img_w // self.stride
        self.anchor_ys = torch.linspace(1, 0, steps=self.num_offsets, dtype=torch.float32)
        self.anchor_cut_ys = torch.linspace(1, 0, steps=self.featmap_h, dtype=torch.float32)
        self.anchor_feat_channels = anchor_feat_channels
        self.conf_thres = conf_thres
        self.nms_thres = nms_thres
        self.nms_topk = nms_topk
        if trace_arg is not None:
            attention_matrix = torch.eye(topk_anchors).repeat(trace_arg['bs'], 1, 1)
            self.pre_non_diag_inds = torch.nonzero(attention_matrix == 0.0, as_tuple=False)
        self.anchors, self.anchors_cut = self.generate_anchors(lateral_n=72, bottom_n=128)
        if anchor_freq_path is not None:
            anchors_mask = torch.load(anchor_freq_path).cpu()
            assert topk_anchors is not None, 'topk_anchors cannot be None'
            idx = torch.argsort(anchors_mask, descending=True)[:topk_anchors]
            self.anchors = self.anchors[idx]
            self.anchors_cut = self.anchors_cut[idx]
        self.cut_zs, self.cut_ys, self.cut_xs, self.invalid_mask = self.compute_anchor_cut_indices(self.anchor_feat_channels, self.featmap_w, self.featmap_h)
        self.conv1 = nn.Conv2d(self.backbone_channels, self.anchor_feat_channels, kernel_size=1)
        self.cls_layer = nn.Linear(2 * self.anchor_feat_channels * self.featmap_h, 2)
        self.reg_layer = nn.Linear(2 * self.anchor_feat_channels * self.featmap_h, self.num_offsets + 1)
        self.attention_layer = nn.Linear(self.anchor_feat_channels * self.featmap_h, len(self.anchors) - 1)
        self.initialize_layer(self.attention_layer)
        self.initialize_layer(self.conv1)
        self.initialize_layer(self.cls_layer)
        self.initialize_layer(self.reg_layer)

    def generate_anchors(self, lateral_n, bottom_n):
        left_anchors, left_cut = self.generate_side_anchors(self.left_angles, x=0.0, nb_origins=lateral_n)
        right_anchors, right_cut = self.generate_side_anchors(self.right_angles, x=1.0, nb_origins=lateral_n)
        bottom_anchors, bottom_cut = self.generate_side_anchors(self.bottom_angles, y=1.0, nb_origins=bottom_n)
        return torch.cat([left_anchors, bottom_anchors, right_anchors]), torch.cat([left_cut, bottom_cut, right_cut])

    def generate_side_anchors(self, angles, nb_origins, x=None, y=None):
        if x is None and y is not None:
            starts = [(x, y) for x in np.linspace(1.0, 0.0, num=nb_origins)]
        elif x is not None and y is None:
            starts = [(x, y) for y in np.linspace(1.0, 0.0, num=nb_origins)]
        else:
            raise Exception('Please define exactly one of `x` or `y` (not neither nor both)')
        n_anchors = nb_origins * len(angles)
        anchors = torch.zeros((n_anchors, 2 + self.num_offsets))
        anchors_cut = torch.zeros((n_anchors, 2 + self.featmap_h))
        for i, start in enumerate(starts):
            for j, angle in enumerate(angles):
                k = i * len(angles) + j
                anchors[k] = self.generate_anchor(start, angle)
                anchors_cut[k] = self.generate_anchor(start, angle, cut=True)
        return anchors, anchors_cut

    def generate_anchor(self, start, angle, cut=False):
        if cut:
            anchor_ys = self.anchor_cut_ys
            anchor = torch.zeros(2 + self.featmap_h)
        else:
            anchor_ys = self.anchor_ys
            anchor = torch.zeros(2 + self.num_offsets)
        angle = angle * math.pi / 180.0
        start_x, start_y = start
        anchor[0] = 1 - start_y
        anchor[1] = start_x
        anchor[2:] = (start_x + (1 - anchor_ys - 1 + start_y) / math.tan(angle)) * self.img_w
        return anchor

    def compute_anchor_cut_indices(self, num_channels, feat_w, feat_h):
        num_proposal = len(self.anchors_cut)
        unclamped_xs = torch.flip((self.anchors_cut[:, 2:] / self.stride).round().long(), dims=(1,))
        unclamped_xs = unclamped_xs[..., None]
        unclamped_xs = torch.repeat_interleave(unclamped_xs, num_channels, dim=0).reshape(-1, 1)
        cut_xs = torch.clamp(unclamped_xs, 0, feat_w - 1)
        unclamped_xs = unclamped_xs.reshape(num_proposal, num_channels, feat_h, 1)
        invalid_mask = (unclamped_xs < 0) | (unclamped_xs > feat_w)
        cut_ys = torch.arange(0, feat_h)
        cut_ys = cut_ys.repeat(num_channels * num_proposal)[:, None].reshape(num_proposal, num_channels, feat_h)
        cut_ys = cut_ys.reshape(-1, 1)
        cut_zs = torch.arange(num_channels).repeat_interleave(feat_h).repeat(num_proposal)[:, None]
        return cut_zs, cut_ys, cut_xs, invalid_mask

    def cut_anchor_features(self, features):
        batch_size = features.shape[0]
        n_proposals = len(self.anchors)
        n_fmaps = features.shape[1]
        batch_anchor_features = torch.zeros((batch_size, n_proposals, n_fmaps, self.featmap_h, 1), device=features.device)
        for batch_idx, img_features in enumerate(features):
            rois = img_features[self.cut_zs, self.cut_ys, self.cut_xs].view(n_proposals, n_fmaps, self.featmap_h, 1)
            rois[self.invalid_mask] = 0
            batch_anchor_features[batch_idx] = rois
        return batch_anchor_features

    @staticmethod
    def initialize_layer(layer):
        if isinstance(layer, (nn.Conv2d, nn.Linear)):
            torch.nn.init.normal_(layer.weight, mean=0.0, std=0.001)
            if layer.bias is not None:
                torch.nn.init.constant_(layer.bias, 0)

    def cuda(self, device=None):
        cuda_self = super()
        cuda_self.anchors = cuda_self.anchors
        cuda_self.anchor_ys = cuda_self.anchor_ys
        cuda_self.cut_zs = cuda_self.cut_zs
        cuda_self.cut_ys = cuda_self.cut_ys
        cuda_self.cut_xs = cuda_self.cut_xs
        cuda_self.invalid_mask = cuda_self.invalid_mask
        return cuda_self

    def to(self, *args, **kwargs):
        device_self = super()
        device_self.anchors = device_self.anchors
        device_self.anchor_ys = device_self.anchor_ys
        device_self.cut_zs = device_self.cut_zs
        device_self.cut_ys = device_self.cut_ys
        device_self.cut_xs = device_self.cut_xs
        device_self.invalid_mask = device_self.invalid_mask
        return device_self

    def forward(self, x):
        batch_features = self.backbone(x)['out']
        batch_features = self.conv1(batch_features)
        batch_anchor_features = self.cut_anchor_features(batch_features)
        batch_anchor_features = batch_anchor_features.view(-1, self.anchor_feat_channels * self.featmap_h)
        softmax = nn.Softmax(dim=1)
        scores = self.attention_layer(batch_anchor_features)
        attention = softmax(scores).reshape(x.shape[0], len(self.anchors), -1)
        attention_matrix = torch.eye(attention.shape[1], device=x.device).repeat(x.shape[0], 1, 1)
        if is_tracing():
            non_diag_inds = self.pre_non_diag_inds
        else:
            non_diag_inds = torch.nonzero(attention_matrix == 0.0, as_tuple=False)
        attention_matrix[:] = 0
        attention_matrix[non_diag_inds[:, 0], non_diag_inds[:, 1], non_diag_inds[:, 2]] = attention.flatten()
        batch_anchor_features = batch_anchor_features.reshape(x.shape[0], len(self.anchors), -1)
        attention_features = torch.bmm(torch.transpose(batch_anchor_features, 1, 2), torch.transpose(attention_matrix, 1, 2)).transpose(1, 2)
        attention_features = attention_features.reshape(-1, self.anchor_feat_channels * self.featmap_h)
        batch_anchor_features = batch_anchor_features.reshape(-1, self.anchor_feat_channels * self.featmap_h)
        batch_anchor_features = torch.cat((attention_features, batch_anchor_features), dim=1)
        cls_logits = self.cls_layer(batch_anchor_features)
        reg = self.reg_layer(batch_anchor_features)
        cls_logits = cls_logits.reshape(x.shape[0], -1, cls_logits.shape[1])
        reg = reg.reshape(x.shape[0], -1, reg.shape[1])
        reg_proposals = torch.zeros((*cls_logits.shape[:2], self.num_offsets + 2), device=x.device)
        reg_proposals += self.anchors
        reg_proposals[:, :, 1:] += reg
        return {'offsets': reg_proposals[..., 2:], 'starts': reg_proposals[..., 0], 'lengths': reg_proposals[..., 1], 'logits': cls_logits}

    @torch.no_grad()
    def inference(self, inputs, input_sizes, forward=True, *args, **kwargs):
        outputs = self.forward(inputs) if forward else inputs
        to_tusimple = True if args[1] == 'tusimple' else False
        batch_regs = self.nms(outputs)
        decoded = []
        for regs in batch_regs:
            regs[:, 1] = torch.round(regs[:, 1])
            if regs.shape[0] == 0:
                decoded.append([])
                continue
            pred = self.proposals_to_pred(regs, input_sizes[1], to_tusimple)
            decoded.append(pred)
        return decoded

    @torch.no_grad()
    def nms(self, batch_proposals):
        proposals_list = []
        for i in range(len(batch_proposals['logits'])):
            scores = F.softmax(batch_proposals['logits'][i], dim=1)[:, 1]
            regs = torch.cat([batch_proposals['starts'][i][..., None], batch_proposals['lengths'][i][..., None], batch_proposals['offsets'][i]], dim=1)
            if self.conf_thres is not None:
                above_threshold = scores > self.conf_thres
                regs = regs[above_threshold]
                scores = scores[above_threshold]
            if regs.shape[0] == 0:
                proposals_list.append(regs[[]])
                continue
            keep, num_to_keep, _ = line_nms(regs, scores, self.nms_thres, self.nms_topk)
            keep = keep[:num_to_keep]
            regs = regs[keep]
            proposals_list.append(regs)
        return proposals_list

    def proposals_to_pred(self, proposals, image_size, to_tusimple=False):
        self.anchor_ys = self.anchor_ys
        lanes = []
        for lane in proposals:
            lane_xs = lane[2:] / self.img_w
            start = int(round(lane[0].item() * self.num_strips))
            length = int(round(lane[1].item()))
            end = start + length - 1
            end = min(end, len(self.anchor_ys) - 1)
            mask = ~((lane_xs[:start] >= 0.0) & (lane_xs[:start] <= 1.0)).cpu().numpy()[::-1].cumprod()[::-1].astype(np.bool)
            lane_xs[end + 1:] = -2
            lane_xs[:start][mask] = -2
            lane_ys = self.anchor_ys[lane_xs >= 0]
            lane_xs = lane_xs[lane_xs >= 0]
            lane_xs = lane_xs.flip(0)
            lane_ys = lane_ys.flip(0)
            if len(lane_xs) <= 1:
                continue
            points = torch.stack((lane_xs.reshape(-1, 1), lane_ys.reshape(-1, 1)), dim=1).squeeze(2)
            points = points.cpu().numpy()
            lane_coords = []
            for i in range(points.shape[0]):
                lane_coords.append([points[i, 0] * float(image_size[1]), points[i, 1] * float(image_size[0])])
            if to_tusimple:
                lanes.append(self.convert_to_tusimple(lane_coords))
            else:
                lanes.append(lane_coords)
        return lanes

    def convert_to_tusimple(self, points, n=200, bezier_threshold=5):
        """Spline interpolation of a lane. Used on the predictions"""
        x = [x for x, _ in points]
        y = [y for _, y in points]
        tck, _ = splprep([x, y], s=0, t=n, k=min(3, len(points) - 1))
        u = np.linspace(0.0, 1.0, n)
        rep_points = np.array(splev(u, tck)).T
        h_samples = [(160 + y * 10) for y in range(56)]
        temp = []
        for h_sample in h_samples:
            dis = np.abs(h_sample - rep_points[:, 1])
            idx = np.argmin(dis)
            if dis[idx] > bezier_threshold:
                temp.append([-2, h_sample])
            else:
                temp.append([round(rep_points[:, 0][idx], 3), h_sample])
        return temp


class MLP(nn.Module):

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x


class PositionEmbeddingLearned(nn.Module):
    """
    Absolute pos embedding, learned.
    """

    def __init__(self, num_pos_feats=256):
        super().__init__()
        self.row_embed = nn.Embedding(50, num_pos_feats)
        self.col_embed = nn.Embedding(50, num_pos_feats)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.uniform_(self.row_embed.weight)
        nn.init.uniform_(self.col_embed.weight)

    def forward(self, x):
        h, w = x.shape[-2:]
        i = torch.arange(w, device=x.device)
        j = torch.arange(h, device=x.device)
        x_emb = self.col_embed(i)
        y_emb = self.row_embed(j)
        pos = torch.cat([x_emb.unsqueeze(0).repeat(h, 1, 1), y_emb.unsqueeze(1).repeat(1, w, 1)], dim=-1).permute(2, 0, 1).unsqueeze(0).repeat(x.shape[0], 1, 1, 1)
        return pos


class PositionEmbeddingSine(nn.Module):
    """
    This is a more standard version of the position embedding, very similar to the one
    used by the Attention is all you need paper, generalized to work on images.
    """

    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):
        super().__init__()
        self.num_pos_feats = num_pos_feats
        self.temperature = temperature
        self.normalize = normalize
        if scale is not None and normalize is False:
            raise ValueError('normalize should be True if scale is passed')
        if scale is None:
            scale = 2 * math.pi
        self.scale = scale

    def forward(self, x, mask):
        assert mask is not None
        not_mask = ~mask
        y_embed = not_mask.cumsum(1, dtype=torch.float32)
        x_embed = not_mask.cumsum(2, dtype=torch.float32)
        if self.normalize:
            eps = 1e-06
            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale
        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)
        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
        pos_x = x_embed[:, :, :, None] / dim_t
        pos_y = y_embed[:, :, :, None] / dim_t
        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
        return pos


def build_position_encoding(hidden_dim, position_embedding):
    N_steps = hidden_dim // 2
    if position_embedding in ('v2', 'sine'):
        position_embedding = PositionEmbeddingSine(N_steps, normalize=True)
    elif position_embedding in ('v3', 'learned'):
        position_embedding = PositionEmbeddingLearned(N_steps)
    else:
        raise ValueError(f'not supported {position_embedding}')
    return position_embedding


def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class TransformerDecoder(nn.Module):

    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.return_intermediate = return_intermediate

    def forward(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        output = tgt
        intermediate = []
        for layer in self.layers:
            output = layer(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, pos=pos, query_pos=query_pos)
            if self.return_intermediate:
                intermediate.append(self.norm(output))
        if self.norm is not None:
            output = self.norm(output)
            if self.return_intermediate:
                intermediate.pop()
                intermediate.append(output)
        if self.return_intermediate:
            return torch.stack(intermediate)
        return output.unsqueeze(0)


def _get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == 'relu':
        return F.relu
    elif activation == 'gelu':
        return F.gelu
    elif activation == 'glu':
        return F.glu
    else:
        raise RuntimeError(f'activation should be relu/gelu, not {activation}.')


def _with_pos_embed(tensor, pos: Optional[Tensor]):
    return tensor if pos is None else tensor + pos


class TransformerDecoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu', normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def forward_post(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        q = k = _with_pos_embed(tgt, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        tgt2 = self.multihead_attn(query=_with_pos_embed(tgt, query_pos), key=_with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt

    def forward_pre(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        tgt2 = self.norm1(tgt)
        q = k = _with_pos_embed(tgt2, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt2 = self.norm2(tgt)
        tgt2 = self.multihead_attn(query=_with_pos_embed(tgt2, query_pos), key=_with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt2 = self.norm3(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))
        tgt = tgt + self.dropout3(tgt2)
        return tgt

    def forward(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        if self.normalize_before:
            return self.forward_pre(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)
        else:
            return self.forward_post(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)


class TransformerEncoder(nn.Module):

    def __init__(self, encoder_layer, num_layers, norm=None):
        super().__init__()
        self.layers = _get_clones(encoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm

    def forward(self, src, mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        output = src
        for layer in self.layers:
            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask, pos=pos)
        if self.norm is not None:
            output = self.norm(output)
        return output


class TransformerEncoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu', normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def forward_post(self, src, src_mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        q = k = _with_pos_embed(src, pos)
        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

    def forward_pre(self, src, src_mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        src2 = self.norm1(src)
        q = k = _with_pos_embed(src2, pos)
        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        src = src + self.dropout2(src2)
        return src

    def forward(self, src, src_mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        if self.normalize_before:
            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)
        else:
            return self.forward_post(src, src_mask, src_key_padding_mask, pos)


class Transformer(nn.Module):

    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu', normalize_before=False, return_intermediate_dec=False):
        super().__init__()
        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation, normalize_before)
        encoder_norm = nn.LayerNorm(d_model) if normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)
        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation, normalize_before)
        decoder_norm = nn.LayerNorm(d_model)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm, return_intermediate=return_intermediate_dec)
        self._reset_parameters()
        self.d_model = d_model
        self.nhead = nhead

    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, mask, query_embed, pos_embed):
        bs, c, h, w = src.shape
        src = src.flatten(2).permute(2, 0, 1)
        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)
        query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)
        if mask is not None:
            mask = mask.flatten(1)
        tgt = torch.zeros_like(query_embed)
        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)
        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask, pos=pos_embed, query_pos=query_embed)
        return hs.transpose(1, 2), memory.permute(1, 2, 0).view(bs, c, h, w)


def build_transformer(hidden_dim, dropout, nheads, dim_feedforward, enc_layers, dec_layers, pre_norm=False, return_intermediate_dec=False):
    return Transformer(d_model=hidden_dim, dropout=dropout, nhead=nheads, dim_feedforward=dim_feedforward, num_encoder_layers=enc_layers, num_decoder_layers=dec_layers, normalize_before=pre_norm, return_intermediate_dec=return_intermediate_dec)


class LSTR(nn.Module):

    def __init__(self, expansion=1, num_queries=7, aux_loss=True, pos_type='sine', drop_out=0.1, num_heads=2, enc_layers=2, dec_layers=2, pre_norm=False, return_intermediate=True, lsp_dim=8, mlp_layers=3, backbone_cfg=None, thresh=0.95, trace_arg=None):
        super().__init__()
        self.thresh = thresh
        self.backbone = MODELS.from_dict(backbone_cfg)
        hidden_dim = 32 * expansion
        self.aux_loss = aux_loss
        self.position_embedding = build_position_encoding(hidden_dim=hidden_dim, position_embedding=pos_type)
        if trace_arg is not None:
            trace_arg['h'] = (trace_arg['h'] - 1) // 32 + 1
            trace_arg['w'] = (trace_arg['w'] - 1) // 32 + 1
            x = torch.zeros((trace_arg['bs'], trace_arg['h'], trace_arg['w']), dtype=torch.bool)
            y = torch.zeros((trace_arg['bs'], 128 * expansion, trace_arg['h'], trace_arg['w']), dtype=torch.float32)
            self.pos = torch.nn.Parameter(data=self.position_embedding(y, x), requires_grad=False)
        self.query_embed = nn.Embedding(num_queries, hidden_dim)
        self.input_proj = nn.Conv2d(128 * expansion, hidden_dim, kernel_size=1)
        self.transformer = build_transformer(hidden_dim=hidden_dim, dropout=drop_out, nheads=num_heads, dim_feedforward=128 * expansion, enc_layers=enc_layers, dec_layers=dec_layers, pre_norm=pre_norm, return_intermediate_dec=return_intermediate)
        self.class_embed = nn.Linear(hidden_dim, 2)
        self.specific_embed = MLP(hidden_dim, hidden_dim, lsp_dim - 4, mlp_layers)
        self.shared_embed = MLP(hidden_dim, hidden_dim, 4, mlp_layers)

    def forward(self, images, padding_masks=None):
        p = self.backbone(images)['out']
        if is_tracing():
            pos = self.pos
        else:
            if padding_masks is None:
                padding_masks = torch.zeros((p.shape[0], p.shape[2], p.shape[3]), dtype=torch.bool, device=p.device)
            else:
                padding_masks = F.interpolate(padding_masks[None].float(), size=p.shape[-2:])[0]
            pos = self.position_embedding(p, padding_masks)
        hs, _ = self.transformer(self.input_proj(p), padding_masks, self.query_embed.weight, pos)
        output_class = self.class_embed(hs)
        output_specific = self.specific_embed(hs)
        output_shared = self.shared_embed(hs)
        output_shared = torch.mean(output_shared, dim=-2, keepdim=True)
        output_shared = output_shared.repeat(1, 1, output_specific.shape[2], 1)
        output_curve = torch.cat([output_specific[:, :, :, :2], output_shared, output_specific[:, :, :, 2:]], dim=-1)
        out = {'logits': output_class[-1], 'curves': output_curve[-1]}
        if self.aux_loss:
            out['aux'] = self._set_aux_loss(output_class, output_curve)
        return out

    def eval(self, profiling=False):
        super().eval()
        if profiling:
            self.aux_loss = False
            self.transformer.decoder.return_intermediate = False

    @torch.no_grad()
    def inference(self, inputs, input_sizes, gap, ppl, dataset, max_lane=0, forward=True, **kwargs):
        outputs = self.forward(inputs) if forward else inputs
        existence_conf = outputs['logits'].softmax(dim=-1)[..., 1]
        existence = existence_conf > self.thresh
        if max_lane != 0:
            existence, _ = lane_pruning(existence, existence_conf, max_lane=max_lane)
        existence = existence.cpu().numpy()
        lane_coordinates = []
        for j in range(existence.shape[0]):
            lane_coordinates.append(self.coefficients_to_coordinates(outputs['curves'][j, :, 2:], existence[j], resize_shape=input_sizes[1], dataset=dataset, ppl=ppl, gap=gap, curve_function=cubic_curve_with_projection, upper_bound=outputs['curves'][j, :, 0], lower_bound=outputs['curves'][j, :, 1]))
        return lane_coordinates

    @staticmethod
    def coefficients_to_coordinates(coefficients, existence, resize_shape, dataset, ppl, gap, curve_function, upper_bound, lower_bound):
        H, W = resize_shape
        if dataset == 'tusimple':
            y = torch.tensor([(1.0 - (ppl - i) * gap / H) for i in range(ppl)], dtype=coefficients.dtype, device=coefficients.device)
        elif dataset in ['culane', 'llamas']:
            y = torch.tensor([(1.0 - i * gap / H) for i in range(ppl)], dtype=coefficients.dtype, device=coefficients.device)
        else:
            raise ValueError
        coords = curve_function(coefficients=coefficients, y=y.unsqueeze(0).expand(coefficients.shape[0], -1))
        coordinates = []
        for i in range(existence.shape[0]):
            if existence[i]:
                valid_points = (coords[i] >= 0) * (coords[i] <= 1) * (y < lower_bound[i]) * (y > upper_bound[i])
                if valid_points.sum() < 2:
                    continue
                if dataset == 'tusimple':
                    coordinates.append([([(coords[i][j] * W).item(), H - (ppl - j) * gap] if valid_points[j] else [-2, H - (ppl - j) * gap]) for j in range(ppl)])
                elif dataset in ['culane', 'llamas']:
                    coordinates.append([[(coords[i][j] * W).item(), H - j * gap] for j in range(ppl) if valid_points[j]])
                else:
                    raise ValueError
        return coordinates

    @torch.jit.unused
    def _set_aux_loss(self, output_class, output_curve):
        return [{'logits': a, 'curves': b} for a, b in zip(output_class[:-1], output_curve[:-1])]


class RESA_Net(nn.Module):

    def __init__(self, backbone_cfg, reducer_cfg, spatial_conv_cfg, classifier_cfg, lane_classifier_cfg, trace_arg=None):
        super().__init__()
        self.backbone = MODELS.from_dict(backbone_cfg)
        self.channel_reducer = MODELS.from_dict(reducer_cfg)
        self.spatial_conv = MODELS.from_dict(spatial_conv_cfg, trace_arg=trace_arg)
        self.decoder = MODELS.from_dict(classifier_cfg)
        self.lane_classifier = MODELS.from_dict(lane_classifier_cfg)

    def forward(self, x):
        x = self.backbone(x)
        if isinstance(x, dict):
            x = x['out']
        x = self.channel_reducer(x)
        x = self.spatial_conv(x)
        res = {'out': self.decoder(x), 'lane': self.lane_classifier(x)}
        return res


class MobileNetV2Encoder(nn.Module):
    """MobileNetV2 backbone (up to second-to-last feature map).
    This backbone is the implementation of
    `MobileNetV2: Inverted Residuals and Linear Bottlenecks
    <https://arxiv.org/abs/1801.04381>`_.
    Args:
        widen_factor (float): Width multiplier, multiply number of
            channels in each layer by this amount. Default: 1.0.
        strides (Sequence[int], optional): Strides of the first block of each
            layer. If not specified, default config in ``arch_setting`` will
            be used.
        dilations (Sequence[int]): Dilation of each layer.
        out_indices (None or Sequence[int]): Output from which stages.
            Default: (7, ).
        frozen_stages (int): Stages to be frozen (all param fixed).
            Default: -1, which means not freezing any parameters.
        norm_eval (bool): Whether to set norm layers to eval mode, namely,
            freeze running stats (mean and var). Note: Effect on Batch Norm
            and its variants only. Default: False.
        pretrained (str, optional): model pretrained path. Default: None
        out_stride (int): the output stride of the output feature map
    """
    arch_settings = [[1, 16, 1], [6, 24, 2], [6, 32, 3], [6, 64, 4], [6, 96, 3], [6, 160, 3], [6, 320, 1]]

    def __init__(self, widen_factor=1.0, strides=(1, 2, 2, 2, 1, 2, 1), dilations=(1, 1, 1, 1, 1, 1, 1), out_indices=(1, 2, 4, 6), frozen_stages=-1, norm_eval=False, pretrained=None, progress=True, out_stride=0):
        super(MobileNetV2Encoder, self).__init__()
        self.pretrained = pretrained
        self.widen_factor = widen_factor
        self.strides = strides
        self.dilations = dilations
        assert len(strides) == len(dilations) == len(self.arch_settings)
        self.out_indices = out_indices
        for index in out_indices:
            if index not in range(0, 7):
                raise ValueError('the item in out_indices must in range(0, 7). But received {index}')
        if frozen_stages not in range(-1, 7):
            raise ValueError('frozen_stages must be in range(-1, 7). But received {frozen_stages}')
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        self.norm_eval = norm_eval
        self.out_stride = out_stride
        self.in_channels = make_divisible(32 * widen_factor, 8)
        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=self.in_channels, kernel_size=3, stride=2, padding=1, bias=False), nn.BatchNorm2d(self.in_channels), nn.ReLU6())
        self.layers = []
        for i, layer_cfg in enumerate(self.arch_settings):
            expand_ratio, channel, num_blocks = layer_cfg
            stride = self.strides[i]
            dilation = self.dilations[i]
            out_channels = make_divisible(channel * widen_factor, 8)
            inverted_res_layer = self.make_layer(out_channels=out_channels, num_blocks=num_blocks, stride=stride, dilation=dilation, expand_ratio=expand_ratio)
            layer_name = f'layer{i + 1}'
            self.add_module(layer_name, inverted_res_layer)
            self.layers.append(layer_name)
        if self.pretrained is None:
            self.weight_initialization()
        else:
            self.load_pretrained(progress=progress)

    def load_pretrained(self, progress):
        state_dict = load_state_dict_from_url(self.pretrained, progress=progress)
        self_state_dict = self.state_dict()
        self_keys = list(self_state_dict.keys())
        for i, (_, v) in enumerate(state_dict.items()):
            if i > len(self_keys) - 1:
                break
            self_state_dict[self_keys[i]] = v
        self.load_state_dict(self_state_dict)

    def weight_initialization(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)

    def make_layer(self, out_channels, num_blocks, stride, dilation, expand_ratio):
        """Stack InvertedResidual blocks to build a layer for MobileNetV2.
        Args:
            out_channels (int): out_channels of block.
            num_blocks (int): Number of blocks.
            stride (int): Stride of the first block.
            dilation (int): Dilation of the first block.
            expand_ratio (int): Expand the number of channels of the
                hidden layer in InvertedResidual by this ratio.
        """
        layers = []
        for i in range(num_blocks):
            layers.append(InvertedResidual(self.in_channels, out_channels, stride if i == 0 else 1, expand_ratio=expand_ratio, dilation=dilation if i == 0 else 1))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        outs = []
        for i, layer_name in enumerate(self.layers):
            layer = getattr(self, layer_name)
            x = layer(x)
            if i in self.out_indices:
                outs.append(x)
        if len(outs) == 1:
            return outs[0]
        else:
            return tuple(outs)

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            for param in self.conv1.parameters():
                param.requires_grad = False
        for i in range(1, self.frozen_stages + 1):
            layer = getattr(self, f'layer{i}')
            layer.eval()
            for param in layer.parameters():
                param.requires_grad = False


class MobileNetV3Encoder(nn.Module):
    """MobileNetV3 backbone (keep the last 1x1)).
    This backbone is the improved implementation of `Searching for MobileNetV3
    <https://ieeexplore.ieee.org/document/9008835>`_.
    Args:
        arch (str): Architecture of mobilnetv3, from {'small', 'large'}.
            Default: 'small'.
        out_indices (tuple[int]): Output from which layer.
            Default: (0, 1, 12).
        frozen_stages (int): Stages to be frozen (all param fixed).
            Default: -1, which means not freezing any parameters.
        norm_eval (bool): Whether to set norm layers to eval mode, namely,
            freeze running stats (mean and var). Note: Effect on Batch Norm
            and its variants only. Default: False.
        pretrained (str, optional): model pretrained path. Default: None
    """
    arch_settings = {'small': [[3, 16, 16, True, 'ReLU'], [3, 72, 24, False, 'ReLU'], [3, 88, 24, False, 'ReLU'], [5, 96, 40, True, 'HSwish'], [5, 240, 40, True, 'HSwish'], [5, 240, 40, True, 'HSwish'], [5, 120, 48, True, 'HSwish'], [5, 144, 48, True, 'HSwish'], [5, 288, 96, True, 'HSwish'], [5, 576, 96, True, 'HSwish'], [5, 576, 96, True, 'HSwish']], 'large': [[3, 16, 16, False, 'ReLU'], [3, 64, 24, False, 'ReLU'], [3, 72, 24, False, 'ReLU'], [5, 72, 40, True, 'ReLU'], [5, 120, 40, True, 'ReLU'], [5, 120, 40, True, 'ReLU'], [3, 240, 80, False, 'HSwish'], [3, 200, 80, False, 'HSwish'], [3, 184, 80, False, 'HSwish'], [3, 184, 80, False, 'HSwish'], [3, 480, 112, True, 'HSwish'], [3, 672, 112, True, 'HSwish'], [5, 672, 160, True, 'HSwish'], [5, 960, 160, True, 'HSwish'], [5, 960, 160, True, 'HSwish']]}

    def __init__(self, arch='small', out_indices=(12,), frozen_stages=-1, reduction_factor=1, norm_eval=False, pretrained=None, strides=(2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1), dilations=(1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2)):
        super(MobileNetV3Encoder, self).__init__()
        self.pretrained = pretrained
        assert arch in self.arch_settings
        assert isinstance(reduction_factor, int) and reduction_factor > 0
        self.strides = strides
        self.dilations = dilations
        for index in out_indices:
            if index not in range(0, len(self.arch_settings[arch]) + 2):
                raise ValueError(f'the item in out_indices must in range(0, {len(self.arch_settings[arch]) + 2}). But received {index}')
        if frozen_stages not in range(-1, len(self.arch_settings[arch]) + 2):
            raise ValueError(f'frozen_stages must be in range(-1, {len(self.arch_settings[arch]) + 2}). But received {frozen_stages}')
        self.arch = arch
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        self.reduction_factor = reduction_factor
        self.norm_eval = norm_eval
        self.layers = self._make_layer()
        if self.pretrained is None:
            self.weight_initialization()
        else:
            self.load_pretrained()

    def _make_layer(self):
        layers = []
        in_channels = 16
        layer = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=in_channels, kernel_size=3, stride=2, padding=1, bias=False), nn.BatchNorm2d(in_channels), nn.Hardswish())
        self.add_module('layer0', layer)
        layers.append('layer0')
        layer_setting = self.arch_settings[self.arch]
        for i, params in enumerate(layer_setting):
            kernel_size, mid_channels, out_channels, with_se, act = params
            stride = self.strides[i]
            dilation = self.dilations[i]
            if self.arch == 'large' and i >= 12 or self.arch == 'small' and i >= 8:
                mid_channels = mid_channels // self.reduction_factor
                out_channels = out_channels // self.reduction_factor
            layer = InvertedResidualV3(in_channels=in_channels, out_channels=out_channels, mid_channels=mid_channels, kernel_size=kernel_size, stride=stride, with_se=with_se, act=act, with_expand_conv=in_channels != mid_channels, dilation=dilation)
            in_channels = out_channels
            layer_name = 'layer{}'.format(i + 1)
            self.add_module(layer_name, layer)
            layers.append(layer_name)
        out_channels = 576 if self.arch == 'small' else 960
        layer = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False), nn.BatchNorm2d(out_channels), nn.Hardswish())
        layer_name = 'layer{}'.format(len(layer_setting) + 1)
        self.add_module(layer_name, layer)
        layers.append(layer_name)
        return layers

    def forward(self, x):
        outs = []
        for i, layer_name in enumerate(self.layers):
            layer = getattr(self, layer_name)
            x = layer(x)
            if i in self.out_indices:
                outs.append(x)
        return outs[-1]

    def _freeze_stages(self):
        for i in range(self.frozen_stages + 1):
            layer = getattr(self, f'layer{i}')
            layer.eval()
            for param in layer.parameters():
                param.requires_grad = False

    def load_pretrained(self):
        state_dict = torch.load(self.pretrained)
        self_state_dict = self.state_dict()
        self_keys = list(self_state_dict.keys())
        for i, (_, v) in enumerate(state_dict.items()):
            if i > len(self_keys) - 1:
                break
            self_state_dict[self_keys[i]] = v
        self.load_state_dict(self_state_dict)

    def weight_initialization(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.zeros_(m.bias)


class SEBlock(nn.Module):

    def __init__(self, input_channels, internal_neurons):
        super(SEBlock, self).__init__()
        self.down = nn.Conv2d(in_channels=input_channels, out_channels=internal_neurons, kernel_size=1, stride=1, bias=True)
        self.up = nn.Conv2d(in_channels=internal_neurons, out_channels=input_channels, kernel_size=1, stride=1, bias=True)
        self.input_channels = input_channels

    def forward(self, inputs):
        x = F.avg_pool2d(inputs, kernel_size=inputs.size(3))
        x = self.down(x)
        x = F.relu(x)
        x = self.up(x)
        x = torch.sigmoid(x)
        x = x.view(-1, self.input_channels, 1, 1)
        return inputs * x


def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):
    result = nn.Sequential()
    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))
    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))
    return result


class RepVGGBlock(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):
        super(RepVGGBlock, self).__init__()
        self.deploy = deploy
        self.groups = groups
        self.in_channels = in_channels
        assert kernel_size == 3
        assert padding == 1
        padding_11 = padding - kernel_size // 2
        self.nonlinearity = nn.ReLU()
        if use_se:
            self.se = SEBlock(out_channels, internal_neurons=out_channels // 16)
        else:
            self.se = nn.Identity()
        if deploy:
            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)
        else:
            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None
            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)
            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)
            None

    def forward(self, inputs):
        if hasattr(self, 'rbr_reparam'):
            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))
        if self.rbr_identity is None:
            id_out = 0
        else:
            id_out = self.rbr_identity(inputs)
        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))

    def get_custom_L2(self):
        K3 = self.rbr_dense.conv.weight
        K1 = self.rbr_1x1.conv.weight
        t3 = (self.rbr_dense.bn.weight / (self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt()).reshape(-1, 1, 1, 1).detach()
        t1 = (self.rbr_1x1.bn.weight / (self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt()).reshape(-1, 1, 1, 1).detach()
        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()
        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1
        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()
        return l2_loss_eq_kernel + l2_loss_circle

    def get_equivalent_kernel_bias(self):
        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)
        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)
        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)
        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid

    def _pad_1x1_to_3x3_tensor(self, kernel1x1):
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])

    def _fuse_bn_tensor(self, branch):
        if branch is None:
            return 0, 0
        if isinstance(branch, nn.Sequential):
            kernel = branch.conv.weight
            running_mean = branch.bn.running_mean
            running_var = branch.bn.running_var
            gamma = branch.bn.weight
            beta = branch.bn.bias
            eps = branch.bn.eps
        else:
            assert isinstance(branch, nn.BatchNorm2d)
            if not hasattr(self, 'id_tensor'):
                input_dim = self.in_channels // self.groups
                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)
                for i in range(self.in_channels):
                    kernel_value[i, i % input_dim, 1, 1] = 1
                self.id_tensor = torch.from_numpy(kernel_value)
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std

    def switch_to_deploy(self):
        if hasattr(self, 'rbr_reparam'):
            return
        kernel, bias = self.get_equivalent_kernel_bias()
        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels, kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride, padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)
        self.rbr_reparam.weight.data = kernel
        self.rbr_reparam.bias.data = bias
        for para in self.parameters():
            para.detach_()
        self.__delattr__('rbr_dense')
        self.__delattr__('rbr_1x1')
        if hasattr(self, 'rbr_identity'):
            self.__delattr__('rbr_identity')
        if hasattr(self, 'id_tensor'):
            self.__delattr__('id_tensor')
        self.deploy = True


class RepVGG(nn.Module):

    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False, use_se=False):
        super(RepVGG, self).__init__()
        assert len(width_multiplier) == 4
        self.deploy = deploy
        self.override_groups_map = override_groups_map or dict()
        self.use_se = use_se
        assert 0 not in self.override_groups_map
        self.in_planes = min(64, int(64 * width_multiplier[0]))
        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy, use_se=self.use_se)
        self.cur_layer_idx = 1
        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)
        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)
        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)
        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)
        self.gap = nn.AdaptiveAvgPool2d(output_size=1)
        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)

    def _make_stage(self, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        blocks = []
        for stride in strides:
            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)
            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3, stride=stride, padding=1, groups=cur_groups, deploy=self.deploy, use_se=self.use_se))
            self.in_planes = planes
            self.cur_layer_idx += 1
        return nn.Sequential(*blocks)

    def forward(self, x):
        out = self.stage0(x)
        out = self.stage1(out)
        out = self.stage2(out)
        out = self.stage3(out)
        out = self.stage4(out)
        out = self.gap(out)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def create_RepVGG_A0(deploy=False):
    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000, width_multiplier=[0.75, 0.75, 0.75, 2.5], override_groups_map=None, deploy=deploy)


def create_RepVGG_A1(deploy=False):
    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000, width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)


def create_RepVGG_A2(deploy=False):
    return RepVGG(num_blocks=[2, 4, 14, 1], num_classes=1000, width_multiplier=[1.5, 1.5, 1.5, 2.75], override_groups_map=None, deploy=deploy)


def create_RepVGG_B0(deploy=False):
    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000, width_multiplier=[1, 1, 1, 2.5], override_groups_map=None, deploy=deploy)


def create_RepVGG_B1(deploy=False):
    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000, width_multiplier=[2, 2, 2, 4], override_groups_map=None, deploy=deploy)


optional_groupwise_layers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26]


g2_map = {l: (2) for l in optional_groupwise_layers}


def create_RepVGG_B1g2(deploy=False):
    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000, width_multiplier=[2, 2, 2, 4], override_groups_map=g2_map, deploy=deploy)


def create_RepVGG_B2(deploy=False):
    return RepVGG(num_blocks=[4, 6, 16, 1], num_classes=1000, width_multiplier=[2.5, 2.5, 2.5, 5], override_groups_map=None, deploy=deploy)


func_dict = {'RepVGG-A0': create_RepVGG_A0, 'RepVGG-A1': create_RepVGG_A1, 'RepVGG-A2': create_RepVGG_A2, 'RepVGG-B0': create_RepVGG_B0, 'RepVGG-B1': create_RepVGG_B1, 'RepVGG-B1g2': create_RepVGG_B1g2, 'RepVGG-B2': create_RepVGG_B2}


def get_RepVGG_func_by_name(name):
    return func_dict[name]


pretrained_model_dict = {'RepVGG-A0': 'RepVGG-A0-train.pth', 'RepVGG-A1': 'RepVGG-A1-train.pth', 'RepVGG-A2': 'RepVGG-A2-train.pth', 'RepVGG-B0': 'RepVGG-B0-train.pth', 'RepVGG-B1': 'RepVGG-B1-train.pth', 'RepVGG-B1g2': 'RepVGG-B1g2-train.pth', 'RepVGG-B2': 'RepVGG-B2-train.pth'}


class RepVggEncoder(nn.Module):

    def __init__(self, backbone_name, pretrained=False, deploy=False):
        super(RepVggEncoder, self).__init__()
        repvgg_fn = get_RepVGG_func_by_name(backbone_name)
        self.encoder = repvgg_fn(deploy)
        if pretrained:
            checkpoint = torch.load(pretrained_model_dict[backbone_name])
            if 'state_dict' in checkpoint:
                checkpoint = checkpoint['state_dict']
            ckpt = {k.replace('module.', ''): v for k, v in checkpoint.items()}
            self.encoder.load_state_dict(ckpt)
        self.layer0 = self.encoder.stage0
        self.layer1 = self.encoder.stage1
        self.layer2 = self.encoder.stage2
        self.layer3 = self.encoder.stage3
        self.layer4 = self.encoder.stage4
        secondlast_channel = 0
        for n, m in self.layer3.named_modules():
            if ('rbr_dense' in n or 'rbr_reparam' in n) and isinstance(m, nn.Conv2d):
                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)
                None
                secondlast_channel = m.out_channels
            elif 'rbr_1x1' in n and isinstance(m, nn.Conv2d):
                m.stride = 1, 1
                None
        last_channel = 0
        for n, m in self.layer4.named_modules():
            if ('rbr_dense' in n or 'rbr_reparam' in n) and isinstance(m, nn.Conv2d):
                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)
                None
                last_channel = m.out_channels
            elif 'rbr_1x1' in n and isinstance(m, nn.Conv2d):
                m.stride = 1, 1
                None
        self.fea_dim = last_channel

    def forward(self, x):
        x0 = self.layer0(x)
        x1 = self.layer1(x0)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)
        return x4


class FrozenBatchNorm2d(torch.nn.Module):
    """
    BatchNorm2d where the batch statistics and the affine parameters
    are fixed
    """

    def __init__(self, num_features: int, eps: float=1e-05, n: Optional[int]=None):
        if n is not None:
            warnings.warn('`n` argument is deprecated and has been renamed `num_features`', DeprecationWarning)
            num_features = n
        super(FrozenBatchNorm2d, self).__init__()
        self.eps = eps
        self.register_buffer('weight', torch.ones(num_features))
        self.register_buffer('bias', torch.zeros(num_features))
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

    def _load_from_state_dict(self, state_dict: dict, prefix: str, local_metadata: dict, strict: bool, missing_keys: List[str], unexpected_keys: List[str], error_msgs: List[str]):
        num_batches_tracked_key = prefix + 'num_batches_tracked'
        if num_batches_tracked_key in state_dict:
            del state_dict[num_batches_tracked_key]
        super(FrozenBatchNorm2d, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)

    def forward(self, x: Tensor) ->Tensor:
        w = self.weight.reshape(1, -1, 1, 1)
        b = self.bias.reshape(1, -1, 1, 1)
        rv = self.running_var.reshape(1, -1, 1, 1)
        rm = self.running_mean.reshape(1, -1, 1, 1)
        scale = w * (rv + self.eps).rsqrt()
        bias = b - rm * scale
        return x * scale + bias

    def __repr__(self) ->str:
        return f'{self.__class__.__name__}({self.weight.shape[0]}, eps={self.eps})'


def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        self.conv1 = conv3x3(inplanes, planes, stride, 1, dilation)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out


def conv1x1(in_planes, out_planes, stride=1):
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.0)) * groups
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out


class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, channels=None):
        super(ResNet, self).__init__()
        if channels is None:
            channels = [64, 128, 256, 512]
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer
        self.inplanes = channels[0]
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError('replace_stride_with_dilation should be None or a 3-element tuple, got {}'.format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, channels[0], layers[0])
        self.layer2 = self._make_layer(block, channels[1], layers[1], stride=2, dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2, dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2, dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(channels[3] * block.expansion, num_classes)
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)

    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion))
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


class _EncoderDecoderModel(nn.Module):

    def __init__(self):
        super().__init__()

    def _load_encoder(self, pretrained_weights):
        if pretrained_weights is not None:
            try:
                saved_weights = load(pretrained_weights)['model']
            except FileNotFoundError:
                raise FileNotFoundError('pretrained_weights is not there! Please set pretrained_weights=None if you are only testing.')
            original_weights = self.state_dict()
            for key in saved_weights.keys():
                if key in original_weights.keys():
                    original_weights[key] = saved_weights[key]
            self.load_state_dict(original_weights)
        else:
            None

    def forward(self, x):
        pass


class _SimpleSegmentationModel(nn.Module):

    def __init__(self, backbone, classifier, aux_classifier=None, recon_head=None, lane_classifier=None, channel_reducer=None, scnn_layer=None):
        super(_SimpleSegmentationModel, self).__init__()
        self.backbone = backbone
        self.classifier = classifier
        self.aux_classifier = aux_classifier
        self.lane_classifier = lane_classifier
        self.channel_reducer = channel_reducer
        self.scnn_layer = scnn_layer
        self.recon_head = recon_head

    def forward(self, x):
        features = self.backbone(x)
        result = OrderedDict()
        x = features['out']
        if self.channel_reducer is not None:
            x = self.channel_reducer(x)
        if self.scnn_layer is not None:
            x = self.scnn_layer(x)
        x = self.classifier(x)
        result['out'] = x
        if self.lane_classifier is not None:
            result['lane'] = self.lane_classifier(x.softmax(dim=1))
        if self.aux_classifier is not None:
            x = features['aux']
            x = self.aux_classifier(x)
            result['aux'] = x
        return result


class ASPPConv(nn.Sequential):

    def __init__(self, in_channels, out_channels, dilation):
        modules = [nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU()]
        super(ASPPConv, self).__init__(*modules)


class ASPPPooling(nn.Sequential):

    def __init__(self, in_channels, out_channels):
        super(ASPPPooling, self).__init__(nn.AdaptiveAvgPool2d(1), nn.Conv2d(in_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU())

    def forward(self, x):
        size = x.shape[-2:]
        x = super(ASPPPooling, self).forward(x)
        return F.interpolate(x, size=size, mode='bilinear', align_corners=True)


class ASPP(nn.Module):

    def __init__(self, in_channels, atrous_rates):
        super(ASPP, self).__init__()
        out_channels = 256
        modules = []
        modules.append(nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU()))
        rate1, rate2, rate3 = tuple(atrous_rates)
        modules.append(ASPPConv(in_channels, out_channels, rate1))
        modules.append(ASPPConv(in_channels, out_channels, rate2))
        modules.append(ASPPConv(in_channels, out_channels, rate3))
        modules.append(ASPPPooling(in_channels, out_channels))
        self.convs = nn.ModuleList(modules)
        self.project = nn.Sequential(nn.Conv2d(5 * out_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(), nn.Dropout(0.5))

    def forward(self, x):
        res = []
        for conv in self.convs:
            res.append(conv(x))
        res = torch.cat(res, dim=1)
        return self.project(res)


class DeepLabV3Head(nn.Sequential):

    def __init__(self, in_channels, num_classes):
        super(DeepLabV3Head, self).__init__(ASPP(in_channels, [12, 24, 36]), nn.Conv2d(256, 256, 3, padding=1, bias=False), nn.BatchNorm2d(256), nn.ReLU(), nn.Conv2d(256, num_classes, 1))


class ASPP_V2(nn.Module):

    def __init__(self, in_channels, num_classes, atrous_rates):
        super(ASPP_V2, self).__init__()
        self.convs = nn.ModuleList()
        for rates in atrous_rates:
            self.convs.append(nn.Conv2d(in_channels, num_classes, kernel_size=3, stride=1, padding=rates, dilation=rates, bias=True))

    def forward(self, x):
        res = self.convs[0](x)
        for i in range(len(self.convs) - 1):
            res += self.convs[i + 1](x)
            return res


class DeepLabV2Head(nn.Sequential):

    def __init__(self, in_channels, num_classes):
        super(DeepLabV2Head, self).__init__(ASPP_V2(in_channels, num_classes, [6, 12, 18, 24]))


class LargeFOV(nn.Module):

    def __init__(self, in_channels, num_classes, dilation=12):
        super(LargeFOV, self).__init__()
        self.conv = nn.Conv2d(in_channels, num_classes, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=True)

    def forward(self, x):
        return self.conv(x)


class DeepLabV1Head(nn.Sequential):

    def __init__(self, in_channels, num_classes, dilation=12):
        super(DeepLabV1Head, self).__init__(LargeFOV(in_channels, num_classes, dilation))


class DeepLabV1(nn.Module):

    def __init__(self, backbone_cfg, num_classes, spatial_conv_cfg=None, lane_classifier_cfg=None, dropout_1=0.1):
        super(DeepLabV1, self).__init__()
        self.encoder = MODELS.from_dict(backbone_cfg)
        self.fc67 = nn.Sequential(nn.Conv2d(512, 1024, 3, padding=4, dilation=4, bias=False), nn.BatchNorm2d(1024), nn.ReLU(), nn.Conv2d(1024, 128, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU())
        self.scnn = MODELS.from_dict(spatial_conv_cfg)
        self.fc8 = nn.Sequential(nn.Dropout2d(dropout_1), nn.Conv2d(128, num_classes, 1))
        self.softmax = nn.Softmax(dim=1)
        self.lane_classifier = MODELS.from_dict(lane_classifier_cfg)

    def forward(self, x):
        out = OrderedDict()
        output = self.encoder(x)
        output = self.fc67(output)
        if self.scnn is not None:
            output = self.scnn(output)
        output = self.fc8(output)
        out['out'] = output
        if self.lane_classifier is not None:
            output = self.softmax(output)
            out['lane'] = self.lane_classifier(output)
        return out


class DeepLabV1Lane(nn.Module):

    def __init__(self, backbone_cfg=None, spatial_conv_cfg=None, lane_classifier_cfg=None, reducer_cfg=None, classifier_cfg=None, uper_cfg=None):
        super().__init__()
        self.encoder = MODELS.from_dict(backbone_cfg)
        self.reducer = MODELS.from_dict(reducer_cfg)
        self.scnn = MODELS.from_dict(spatial_conv_cfg)
        self.classifier = MODELS.from_dict(classifier_cfg)
        self.softmax = nn.Softmax(dim=1)
        self.lane_classifier = MODELS.from_dict(lane_classifier_cfg)
        self.uper_decoder = MODELS.from_dict(uper_cfg)

    def forward(self, input):
        out = OrderedDict()
        output = self.encoder(input)
        if self.uper_decoder is not None:
            output = self.uper_decoder(output)
        if self.reducer is not None:
            output = self.reducer(output)
        if self.scnn is not None:
            output = self.scnn(output)
        output = self.classifier(output)
        out['out'] = output
        if self.lane_classifier is not None:
            output = self.softmax(output)
            out['lane'] = self.lane_classifier(output)
        return out


class SegRepVGG(DeepLabV1Lane):

    def eval(self, profiling=False):
        """A secure copy of eval()
        """
        if profiling:
            for module in self.encoder.modules():
                if hasattr(module, 'switch_to_deploy'):
                    module.switch_to_deploy()
            None
            return self.train(False)
        else:
            return self.train(False)


class InitialBlock(nn.Module):
    """The initial block is composed of two branches:
    1. a main branch which performs a regular convolution with stride 2;
    2. an extension branch which performs max-pooling.
    Doing both operations in parallel and concatenating their results
    allows for efficient downsampling and expansion. The main branch
    outputs 13 feature maps while the extension branch outputs 3, for a
    total of 16 feature maps after concatenation.
    Keyword arguments:
    - in_channels (int): the number of input channels.
    - out_channels (int): the number output channels.
    - kernel_size (int, optional): the kernel size of the filters used in
    the convolution layer. Default: 3.
    - padding (int, optional): zero-padding added to both sides of the
    input. Default: 0.
    - bias (bool, optional): Adds a learnable bias to the output if
    ``True``. Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.
    """

    def __init__(self, in_channels, out_channels, bias=False, relu=True):
        super().__init__()
        if relu:
            activation = nn.ReLU
        else:
            activation = nn.PReLU
        self.main_branch = nn.Conv2d(in_channels, out_channels - 3, kernel_size=3, stride=2, padding=1, bias=bias)
        self.ext_branch = nn.MaxPool2d(3, stride=2, padding=1)
        self.batch_norm = nn.BatchNorm2d(out_channels)
        self.out_activation = activation()

    def forward(self, x):
        main = self.main_branch(x)
        ext = self.ext_branch(x)
        out = torch.cat((main, ext), 1)
        out = self.batch_norm(out)
        return self.out_activation(out)


class RegularBottleneck(nn.Module):
    """Regular bottlenecks are the main building block of ENet.
    Main branch:
    1. Shortcut connection.
    Extension branch:
    1. 1x1 convolution which decreases the number of channels by
    ``internal_ratio``, also called a projection;
    2. regular, dilated or asymmetric convolution;
    3. 1x1 convolution which increases the number of channels back to
    ``channels``, also called an expansion;
    4. dropout as a regularizer.
    Keyword arguments:
    - channels (int): the number of input and output channels.
    - internal_ratio (int, optional): a scale factor applied to
    ``channels`` used to compute the number of
    channels after the projection. eg. given ``channels`` equal to 128 and
    internal_ratio equal to 2 the number of channels after the projection
    is 64. Default: 4.
    - kernel_size (int, optional): the kernel size of the filters used in
    the convolution layer described above in item 2 of the extension
    branch. Default: 3.
    - padding (int, optional): zero-padding added to both sides of the
    input. Default: 0.
    - dilation (int, optional): spacing between kernel elements for the
    convolution described in item 2 of the extension branch. Default: 1.
    asymmetric (bool, optional): flags if the convolution described in
    item 2 of the extension branch is asymmetric or not. Default: False.
    - dropout_prob (float, optional): probability of an element to be
    zeroed. Default: 0 (no dropout).
    - bias (bool, optional): Adds a learnable bias to the output if
    ``True``. Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.
    """

    def __init__(self, channels, internal_ratio=4, kernel_size=3, padding=0, dilation=1, asymmetric=False, dropout_prob=0.0, is_dropout=True, bias=False, relu=True):
        super().__init__()
        if internal_ratio <= 1 or internal_ratio > channels:
            raise RuntimeError('Value out of range. Expected value in the interval [1, {0}], got internal_scale={1}.'.format(channels, internal_ratio))
        internal_channels = channels // internal_ratio
        self.is_dropout = is_dropout
        if relu:
            activation = nn.ReLU
        else:
            activation = nn.PReLU
        self.ext_conv1 = nn.Sequential(nn.Conv2d(channels, internal_channels, kernel_size=1, stride=1, bias=bias), nn.BatchNorm2d(internal_channels), activation())
        if asymmetric:
            self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, internal_channels, kernel_size=(kernel_size, 1), stride=1, padding=(padding, 0), dilation=dilation, bias=bias), nn.BatchNorm2d(internal_channels), activation(), nn.Conv2d(internal_channels, internal_channels, kernel_size=(1, kernel_size), stride=1, padding=(0, padding), dilation=dilation, bias=bias), nn.BatchNorm2d(internal_channels), activation())
        else:
            self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, internal_channels, kernel_size=kernel_size, stride=1, padding=padding, dilation=dilation, bias=bias), nn.BatchNorm2d(internal_channels), activation())
        self.ext_conv3 = nn.Sequential(nn.Conv2d(internal_channels, channels, kernel_size=1, stride=1, bias=bias), nn.BatchNorm2d(channels))
        self.ext_regul = nn.Dropout2d(p=dropout_prob)
        self.out_activation = activation()

    def forward(self, x):
        main = x
        ext = self.ext_conv1(x)
        ext = self.ext_conv2(ext)
        ext = self.ext_conv3(ext)
        if self.is_dropout:
            ext = self.ext_regul(ext)
        out = main + ext
        return self.out_activation(out)


class DownsamplingBottleneck(nn.Module):
    """Downsampling bottlenecks further downsample the feature map size.
    Main branch:
    1. max pooling with stride 2; indices are saved to be used for
    unpooling later.
    Extension branch:
    1. 2x2 convolution with stride 2 that decreases the number of channels
    by ``internal_ratio``, also called a projection;
    2. regular convolution (by default, 3x3);
    3. 1x1 convolution which increases the number of channels to
    ``out_channels``, also called an expansion;
    4. dropout as a regularizer.
    Keyword arguments:
    - in_channels (int): the number of input channels.
    - out_channels (int): the number of output channels.
    - internal_ratio (int, optional): a scale factor applied to ``channels``
    used to compute the number of channels after the projection. eg. given
    ``channels`` equal to 128 and internal_ratio equal to 2 the number of
    channels after the projection is 64. Default: 4.
    - return_indices (bool, optional):  if ``True``, will return the max
    indices along with the outputs. Useful when unpooling later.
    - dropout_prob (float, optional): probability of an element to be
    zeroed. Default: 0 (no dropout).
    - bias (bool, optional): Adds a learnable bias to the output if
    ``True``. Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.
    """

    def __init__(self, in_channels, out_channels, internal_ratio=4, return_indices=False, dropout_prob=0.0, is_dropout=True, bias=False, relu=True):
        super().__init__()
        self.return_indices = return_indices
        self.is_dropout = is_dropout
        if internal_ratio <= 1 or internal_ratio > in_channels:
            raise RuntimeError('Value out of range. Expected value in the interval [1, {0}], got internal_scale={1}. '.format(in_channels, internal_ratio))
        internal_channels = in_channels // internal_ratio
        if relu:
            activation = nn.ReLU
        else:
            activation = nn.PReLU
        self.main_max1 = nn.MaxPool2d(2, stride=2, return_indices=return_indices)
        self.ext_conv1 = nn.Sequential(nn.Conv2d(in_channels, internal_channels, kernel_size=2, stride=2, bias=bias), nn.BatchNorm2d(internal_channels), activation())
        self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, internal_channels, kernel_size=3, stride=1, padding=1, bias=bias), nn.BatchNorm2d(internal_channels), activation())
        self.ext_conv3 = nn.Sequential(nn.Conv2d(internal_channels, out_channels, kernel_size=1, stride=1, bias=bias), nn.BatchNorm2d(out_channels))
        self.ext_regul = nn.Dropout2d(p=dropout_prob)
        self.out_activation = activation()

    def forward(self, x):
        if self.return_indices:
            main, max_indices = self.main_max1(x)
        else:
            main = self.main_max1(x)
        ext = self.ext_conv1(x)
        ext = self.ext_conv2(ext)
        ext = self.ext_conv3(ext)
        if self.is_dropout:
            ext = self.ext_regul(ext)
        n, ch_ext, h, w = ext.size()
        ch_main = main.size()[1]
        padding = torch.zeros(n, ch_ext - ch_main, h, w)
        if main.is_cuda:
            padding = padding
        main = torch.cat((main, padding), 1)
        out = main + ext
        return self.out_activation(out), max_indices


class UpsamplingBottleneck(nn.Module):
    """The upsampling bottlenecks upsample the feature map resolution using max
    pooling indices stored from the corresponding downsampling bottleneck.
    Main branch:
    1. 1x1 convolution with stride 1 that decreases the number of channels by
    ``internal_ratio``, also called a projection;
    2. max unpool layer using the max pool indices from the corresponding
    downsampling max pool layer.
    Extension branch:
    1. 1x1 convolution with stride 1 that decreases the number of channels by
    ``internal_ratio``, also called a projection;
    2. transposed convolution (by default, 3x3);
    3. 1x1 convolution which increases the number of channels to
    ``out_channels``, also called an expansion;
    4. dropout as a regularizer.
    Keyword arguments:
    - in_channels (int): the number of input channels.
    - out_channels (int): the number of output channels.
    - internal_ratio (int, optional): a scale factor applied to ``in_channels``
     used to compute the number of channels after the projection. eg. given
     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number
     of channels after the projection is 64. Default: 4.
    - dropout_prob (float, optional): probability of an element to be zeroed.
    Default: 0 (no dropout).
    - bias (bool, optional): Adds a learnable bias to the output if ``True``.
    Default: False.
    - relu (bool, optional): When ``True`` ReLU is used as the activation
    function; otherwise, PReLU is used. Default: True.
    """

    def __init__(self, in_channels, out_channels, internal_ratio=4, dropout_prob=0.0, is_dropout=True, bias=False, relu=True):
        super().__init__()
        if internal_ratio <= 1 or internal_ratio > in_channels:
            raise RuntimeError('Value out of range. Expected value in the interval [1, {0}], got internal_scale={1}. '.format(in_channels, internal_ratio))
        internal_channels = in_channels // internal_ratio
        self.is_dropout = is_dropout
        if relu:
            activation = nn.ReLU
        else:
            activation = nn.PReLU
        self.main_conv1 = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(out_channels))
        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)
        self.ext_conv1 = nn.Sequential(nn.Conv2d(in_channels, internal_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(internal_channels), activation())
        self.ext_tconv1 = nn.ConvTranspose2d(internal_channels, internal_channels, kernel_size=2, stride=2, bias=bias)
        self.ext_tconv1_bnorm = nn.BatchNorm2d(internal_channels)
        self.ext_tconv1_activation = activation()
        self.ext_conv2 = nn.Sequential(nn.Conv2d(internal_channels, out_channels, kernel_size=1, bias=bias), nn.BatchNorm2d(out_channels))
        self.ext_regul = nn.Dropout2d(p=dropout_prob)
        self.out_activation = activation()

    def forward(self, x, max_indices, output_size):
        main = self.main_conv1(x)
        main = self.main_unpool1(main, max_indices, output_size=output_size)
        ext = self.ext_conv1(x)
        ext = self.ext_tconv1(ext, output_size=output_size)
        ext = self.ext_tconv1_bnorm(ext)
        ext = self.ext_tconv1_activation(ext)
        ext = self.ext_conv2(ext)
        if self.is_dropout:
            ext = self.ext_regul(ext)
        out = main + ext
        return self.out_activation(out)


class SpatialSoftmax(nn.Module):

    def __init__(self, temperature=1, device='cpu'):
        super(SpatialSoftmax, self).__init__()
        if temperature:
            self.temperature = Parameter(torch.ones(1) * temperature)
        else:
            self.temperature = 1.0

    def forward(self, feature):
        feature = feature.view(feature.shape[0], -1, feature.shape[1] * feature.shape[2])
        softmax_attention = F.softmax(feature / self.temperature, dim=-1)
        return softmax_attention


class Encoder(nn.Module):

    def __init__(self, num_classes, dropout_1=0.03, dropout_2=0.3):
        super().__init__()
        self.initial_block = DownsamplerBlock(3, 16)
        self.layers = nn.ModuleList()
        self.layers.append(DownsamplerBlock(16, 64))
        for x in range(0, 5):
            self.layers.append(non_bottleneck_1d(64, dropout_1, 1))
        self.layers.append(DownsamplerBlock(64, 128))
        for x in range(0, 2):
            self.layers.append(non_bottleneck_1d(128, dropout_2, 2))
            self.layers.append(non_bottleneck_1d(128, dropout_2, 4))
            self.layers.append(non_bottleneck_1d(128, dropout_2, 8))
            self.layers.append(non_bottleneck_1d(128, dropout_2, 16))
        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)

    def forward(self, input, predict=False):
        output = self.initial_block(input)
        for layer in self.layers:
            output = layer(output)
        if predict:
            output = self.output_conv(output)
        return output


class UpsamplerBlock(nn.Module):

    def __init__(self, ninput, noutput):
        super().__init__()
        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)
        self.bn = nn.BatchNorm2d(noutput, eps=0.001)

    def forward(self, input):
        output = self.conv(input)
        output = self.bn(output)
        return F.relu(output)


class Decoder(nn.Module):

    def __init__(self, num_classes):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(UpsamplerBlock(128, 64))
        self.layers.append(non_bottleneck_1d(64, 0, 1))
        self.layers.append(non_bottleneck_1d(64, 0, 1))
        self.layers.append(UpsamplerBlock(64, 16))
        self.layers.append(non_bottleneck_1d(16, 0, 1))
        self.layers.append(non_bottleneck_1d(16, 0, 1))
        self.output_conv = nn.ConvTranspose2d(16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)

    def forward(self, input):
        output = input
        for layer in self.layers:
            output = layer(output)
        output = self.output_conv(output)
        return output


class ENet(_EncoderDecoderModel):

    def __init__(self, num_classes, lane_classifier_cfg=None, encoder_relu=False, decoder_relu=True, dropout_1=0.01, dropout_2=0.1, encoder_only=False, pretrained_weights=None):
        super().__init__()
        self.encoder_conv = None
        self.encoder = Encoder(encoder_relu=encoder_relu, dropout_1=dropout_1, dropout_2=dropout_2)
        if encoder_only:
            self.decoder = None
            self.encoder_conv = nn.Conv2d(128, num_classes, kernel_size=1, stride=1)
        else:
            self.decoder = Decoder(num_classes=num_classes, decoder_relu=decoder_relu, dropout_2=dropout_2)
        self.lane_classifier = MODELS.from_dict(lane_classifier_cfg)
        self._load_encoder(pretrained_weights)

    def forward(self, x):
        out = OrderedDict()
        x, max_indices1_0, stage1_input_size, max_indices2_0, stage2_input_size, input_size = self.encoder(x)
        if self.encoder_conv is not None:
            x = self.encoder_conv(x)
        if self.lane_classifier is not None:
            out['lane'] = self.lane_classifier(x)
        if self.decoder is not None:
            x = self.decoder.forward(x, max_indices1_0, stage1_input_size, max_indices2_0, stage2_input_size, input_size)
        out['out'] = x
        return out


class ERFNet(_EncoderDecoderModel):

    def __init__(self, num_classes, lane_classifier_cfg=None, spatial_conv_cfg=None, dropout_1=0.03, dropout_2=0.3, pretrained_weights=None):
        super().__init__()
        self.encoder = Encoder(num_classes=num_classes, dropout_1=dropout_1, dropout_2=dropout_2)
        self.decoder = Decoder(num_classes)
        self.spatial_conv = MODELS.from_dict(spatial_conv_cfg)
        self.lane_classifier = MODELS.from_dict(lane_classifier_cfg)
        self._load_encoder(pretrained_weights)

    def _load_encoder(self, pretrained_weights):
        if pretrained_weights is not None:
            try:
                saved_weights = torch.load(pretrained_weights)['state_dict']
            except FileNotFoundError:
                raise FileNotFoundError('pretrained_weights is not there! Please set pretrained_weights=None if you are only testing.')
            original_weights = self.state_dict()
            for key in saved_weights.keys():
                my_key = key.replace('module.features.', '')
                if my_key in original_weights.keys():
                    original_weights[my_key] = saved_weights[key]
            self.load_state_dict(original_weights)
        else:
            None

    def forward(self, x, only_encode=False):
        out = OrderedDict()
        if only_encode:
            return self.encoder.forward(x, predict=True)
        else:
            output = self.encoder(x)
            if self.spatial_conv is not None:
                output = self.spatial_conv(output)
            out['out'] = self.decoder.forward(output)
            if self.lane_classifier is not None:
                out['lane'] = self.lane_classifier(output)
            return out


class FCNHead(nn.Sequential):

    def __init__(self, in_channels, num_classes):
        inter_channels = in_channels // 4
        layers = [nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False), nn.BatchNorm2d(inter_channels), nn.ReLU(), nn.Dropout(0.1), nn.Conv2d(inter_channels, num_classes, 1)]
        super(FCNHead, self).__init__(*layers)


class ConvBNReLU(nn.Module):

    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1):
        super(ConvBNReLU, self).__init__()
        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size=ks, stride=stride, padding=padding, bias=False)
        self.bn = BatchNorm2d(out_chan)
        self.relu = nn.ReLU()
        self.init_weight()

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        x = self.relu(x)
        return x

    def init_weight(self):
        for ly in self.children():
            if isinstance(ly, nn.Conv2d):
                nn.init.kaiming_normal_(ly.weight, a=1)
                if not ly.bias is None:
                    nn.init.constant_(ly.bias, 0)


class SimpleSegHead(nn.Module):

    def __init__(self, in_channels, mid_channels, num_classes):
        super().__init__()
        self.conv = ConvBNReLU(in_channels, mid_channels, ks=3, stride=1, padding=1)
        self.conv_out = nn.Conv2d(mid_channels, num_classes, kernel_size=1, bias=False)
        self.init_weight()

    def forward(self, x):
        x = self.conv(x)
        x = self.conv_out(x)
        return x

    def init_weight(self):
        for ly in self.children():
            if isinstance(ly, nn.Conv2d):
                nn.init.kaiming_normal_(ly.weight, a=1)
                if not ly.bias is None:
                    nn.init.constant_(ly.bias, 0)

    def get_params(self):
        wd_params, nowd_params = [], []
        for _, module in self.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                wd_params.append(module.weight)
                if not module.bias is None:
                    nowd_params.append(module.bias)
            elif isinstance(module, BatchNorm2d):
                nowd_params += list(module.parameters())
        return wd_params, nowd_params


class Mlp(nn.Module):
    """ Multilayer perceptron."""

    def __init__(self, in_features, hidden_features=None, out_features=None, act_fn=F.gelu, drop=0.0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_fn
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class WindowAttention(nn.Module):
    """ Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0.0, proj_drop=0.0):
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer('relative_position_index', relative_position_index)
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=0.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """ Forward function.

        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / float(window_size) / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class SwinTransformerBlock(nn.Module):
    """ Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_fn (nn.Module, optional): Activation layer. Default: F.gelu
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_fn=F.gelu, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        assert 0 <= self.shift_size < self.window_size, 'shift_size must in 0-window_size'
        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(dim, window_size=to_2tuple(self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_fn=act_fn, drop=drop)
        self.H = None
        self.W = None

    def forward(self, x, mask_matrix):
        """ Forward function.

        Args:
            x: Input feature, tensor size (B, H*W, C).
            H, W: Spatial resolution of the input feature.
            mask_matrix: Attention mask for cyclic shift.
        """
        B, L, C = x.shape
        H, W = self.H, self.W
        assert L == H * W, 'input feature has wrong size'
        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)
        pad_l = pad_t = 0
        pad_r = (self.window_size - W % self.window_size) % self.window_size
        pad_b = (self.window_size - H % self.window_size) % self.window_size
        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
        _, Hp, Wp, _ = x.shape
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
            attn_mask = mask_matrix
        else:
            shifted_x = x
            attn_mask = None
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)
        attn_windows = self.attn(x_windows, mask=attn_mask)
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        if pad_r > 0 or pad_b > 0:
            x = x[:, :H, :W, :].contiguous()
        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class PatchMerging(nn.Module):
    """ Patch Merging Layer

    Args:
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x, H, W):
        """ Forward function.

        Args:
            x: Input feature, tensor size (B, H*W, C).
            H, W: Spatial resolution of the input feature.
        """
        B, L, C = x.shape
        assert L == H * W, 'input feature has wrong size'
        x = x.view(B, H, W, C)
        pad_input = H % 2 == 1 or W % 2 == 1
        if pad_input:
            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))
        x0 = x[:, 0::2, 0::2, :]
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        x = torch.cat([x0, x1, x2, x3], -1)
        x = x.view(B, -1, 4 * C)
        x = self.norm(x)
        x = self.reduction(x)
        return x


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of feature channels
        depth (int): Depths of this stage.
        num_heads (int): Number of attention head.
        window_size (int): Local window size. Default: 7.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, dim, depth, num_heads, window_size=7, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):
        super().__init__()
        self.window_size = window_size
        self.shift_size = window_size // 2
        self.depth = depth
        self.use_checkpoint = use_checkpoint
        self.blocks = nn.ModuleList([SwinTransformerBlock(dim=dim, num_heads=num_heads, window_size=window_size, shift_size=0 if i % 2 == 0 else window_size // 2, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop, drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer) for i in range(depth)])
        if downsample is not None:
            self.downsample = downsample(dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x, H, W):
        """ Forward function.

        Args:
            x: Input feature, tensor size (B, H*W, C).
            H, W: Spatial resolution of the input feature.
        """
        Hp = int(np.ceil(float(H) / float(self.window_size))) * self.window_size
        Wp = int(np.ceil(float(W) / float(self.window_size))) * self.window_size
        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)
        h_slices = slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)
        w_slices = slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)
        cnt = 0
        for h in h_slices:
            for w in w_slices:
                img_mask[:, h, w, :] = cnt
                cnt += 1
        mask_windows = window_partition(img_mask, self.window_size)
        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        for blk in self.blocks:
            blk.H, blk.W = H, W
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x, attn_mask)
            else:
                x = blk(x, attn_mask)
        if self.downsample is not None:
            x_down = self.downsample(x, H, W)
            Wh, Ww = (H + 1) // 2, (W + 1) // 2
            return x, H, W, x_down, Wh, Ww
        else:
            return x, H, W, x, H, W


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding

    Args:
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        patch_size = to_2tuple(patch_size)
        self.patch_size = patch_size
        self.in_chans = in_chans
        self.embed_dim = embed_dim
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        """Forward function."""
        _, _, H, W = x.size()
        if W % self.patch_size[1] != 0:
            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))
        if H % self.patch_size[0] != 0:
            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))
        x = self.proj(x)
        if self.norm is not None:
            Wh, Ww = x.size(2), x.size(3)
            x = x.flatten(2).transpose(1, 2)
            x = self.norm(x)
            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)
        return x


class SwinTransformer(nn.Module):
    """ Swin Transformer backbone.
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        pretrain_img_size (int): Input image size for training the pretrained model,
            used in absolute postion embedding. Default 224.
        patch_size (int | tuple(int)): Patch size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        depths (tuple[int]): Depths of each Swin Transformer stage.
        num_heads (tuple[int]): Number of attention head of each stage.
        window_size (int): Window size. Default: 7.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.
        drop_rate (float): Dropout rate.
        attn_drop_rate (float): Attention dropout rate. Default: 0.
        drop_path_rate (float): Stochastic depth rate. Default: 0.2.
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.
        patch_norm (bool): If True, add normalization after patch embedding. Default: True.
        out_indices (Sequence[int]): Output from which stages.
        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).
            -1 means not freezing any parameters.
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    """

    def __init__(self, pretrain_img_size=224, patch_size=4, in_chans=3, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.2, norm_layer=nn.LayerNorm, ape=False, patch_norm=True, out_indices=(0, 1, 2, 3), frozen_stages=-1, use_checkpoint=False, pretrained=None, chosen_stages=-1):
        super().__init__()
        self.pretrain_img_size = pretrain_img_size
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.out_indices = out_indices
        self.frozen_stages = frozen_stages
        self.chosen_stages = chosen_stages
        self.patch_embed = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None)
        if self.ape:
            pretrain_img_size = to_2tuple(pretrain_img_size)
            patch_size = to_2tuple(patch_size)
            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]]
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1]))
            trunc_normal_(self.absolute_pos_embed, std=0.02)
        self.pos_drop = nn.Dropout(p=drop_rate)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer), depth=depths[i_layer], num_heads=num_heads[i_layer], window_size=window_size, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], norm_layer=norm_layer, downsample=PatchMerging if i_layer < self.num_layers - 1 else None, use_checkpoint=use_checkpoint)
            self.layers.append(layer)
        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]
        self.num_features = num_features
        for i_layer in out_indices:
            layer = norm_layer(num_features[i_layer])
            layer_name = f'norm{i_layer}'
            self.add_module(layer_name, layer)
        if pretrained is None:
            self.weight_initialization()
        else:
            assert isinstance(pretrained, str), 'pretrained must be a str'
            self.load_checkpoint(pretrained=pretrained)
        self._freeze_stages()

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for param in self.patch_embed.parameters():
                param.requires_grad = False
        if self.frozen_stages >= 1 and self.ape:
            self.absolute_pos_embed.requires_grad = False
        if self.frozen_stages >= 2:
            self.pos_drop.eval()
            for i in range(0, self.frozen_stages - 1):
                m = self.layers[i]
                m.eval()
                for param in m.parameters():
                    param.requires_grad = False

    def weight_initialization(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=0.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

    def load_checkpoint(self, pretrained, strict=False):
        state_dict = torch.load(pretrained)['model']
        self.load_state_dict(state_dict, strict=strict)

    def forward(self, x):
        """Forward function."""
        x = self.patch_embed(x)
        Wh, Ww = x.size(2), x.size(3)
        if self.ape:
            absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')
            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)
        else:
            x = x.flatten(2).transpose(1, 2)
        x = self.pos_drop(x)
        outs = []
        for i in range(self.num_layers):
            layer = self.layers[i]
            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)
            if i in self.out_indices:
                norm_layer = getattr(self, f'norm{i}')
                x_out = norm_layer(x_out)
                out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()
                outs.append(out)
        if self.chosen_stages == -1:
            return tuple(outs)
        else:
            return tuple(outs)[self.chosen_stages]

    def train(self, mode=True):
        """Convert the model into training mode while keep layers freezed."""
        super(SwinTransformer, self).train(mode)
        self._freeze_stages()


class VGG16(nn.Module):

    def __init__(self, pretrained=True):
        super().__init__()
        self.pretrained = pretrained
        self.net = vgg16_bn(pretrained=self.pretrained).features
        for i in [34, 37, 40]:
            conv = self.net._modules[str(i)]
            dilated_conv = nn.Conv2d(conv.in_channels, conv.out_channels, conv.kernel_size, stride=conv.stride, padding=tuple(p * 2 for p in conv.padding), dilation=2, bias=conv.bias is not None)
            dilated_conv.load_state_dict(conv.state_dict())
            self.net._modules[str(i)] = dilated_conv
        self.net._modules.pop('33')
        self.net._modules.pop('43')

    def forward(self, x):
        x = self.net(x)
        return x


TRANSFORMS = SimpleRegistry()


def _check_sequence_input(x, name, req_sizes):
    msg = req_sizes[0] if len(req_sizes) < 2 else ' or '.join([str(s) for s in req_sizes])
    if not isinstance(x, Sequence):
        raise TypeError('{} should be a sequence of length {}.'.format(name, msg))
    if len(x) not in req_sizes:
        raise ValueError('{} should be sequence of length {}.'.format(name, msg))


def _setup_angle(x, name, req_sizes=(2,)):
    if isinstance(x, numbers.Number):
        if x < 0:
            raise ValueError('If {} is a single number, it must be positive.'.format(name))
        x = [-x, x]
    else:
        _check_sequence_input(x, name, req_sizes)
    return [float(d) for d in x]


class RandomAffine(torch.nn.Module):
    """Before BC-Break of resample.
    Random affine transformation of the image keeping center invariant.
    The image can be a PIL Image or a Tensor, in which case it is expected
    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.
    Args:
        degrees (sequence or float or int): Range of degrees to select from.
            If degrees is a number instead of sequence like (min, max), the range of degrees
            will be (-degrees, +degrees). Set to 0 to deactivate rotations.
        translate (tuple, optional): tuple of maximum absolute pixels for horizontal
            and vertical translations. For example translate=(a, b), then horizontal shift
            is randomly sampled in the range a < dx < a and vertical shift is
            randomly sampled in the range b < dy < b. Will not translate by default.
        scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is
            randomly sampled from the range a <= scale <= b. Will keep original scale by default.
        shear (sequence or float or int, optional): Range of degrees to select from.
            If shear is a number, a shear parallel to the x axis in the range (-shear, +shear)
            will be applied. Else if shear is a tuple or list of 2 values a shear parallel to the x axis in the
            range (shear[0], shear[1]) will be applied. Else if shear is a tuple or list of 4 values,
            a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied.
            Will not apply shear by default.
        resample (int, optional): An optional resampling filter. See `filters`_ for more information.
            If omitted, or if the image has mode "1" or "P", it is set to ``PIL.Image.NEAREST``.
            If input is Tensor, only ``PIL.Image.NEAREST`` and ``PIL.Image.BILINEAR`` are supported.
        fillcolor (tuple or int): Optional fill color (Tuple for RGB Image and int for grayscale) for the area
            outside the transform in the output image (Pillow>=5.0.0). This option is not supported for Tensor
            input. Fill value for the area outside the transform in the output image is always 0.
    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
    """

    def __init__(self, degrees, translate=None, scale=None, shear=None, ignore_x=-2):
        super().__init__()
        self.ignore_x = ignore_x
        self.degrees = _setup_angle(degrees, name='degrees', req_sizes=(2,))
        if translate is not None:
            _check_sequence_input(translate, 'translate', req_sizes=(2,))
        self.translate = translate
        if scale is not None:
            _check_sequence_input(scale, 'scale', req_sizes=(2,))
            for s in scale:
                if s <= 0:
                    raise ValueError('scale values should be positive')
        self.scale = scale
        if shear is not None:
            self.shear = _setup_angle(shear, name='shear', req_sizes=(2, 4))
        else:
            self.shear = shear

    @staticmethod
    def get_params(degrees, translate, scale_ranges, shears):
        """Get parameters for affine transformation
        Returns:
            params to be passed to the affine transformation
        """
        angle = float(torch.empty(1).uniform_(float(degrees[0]), float(degrees[1])).item())
        if translate is not None:
            max_dx = float(translate[0])
            max_dy = float(translate[1])
            tx = int(round(torch.empty(1).uniform_(-max_dx, max_dx).item()))
            ty = int(round(torch.empty(1).uniform_(-max_dy, max_dy).item()))
            translations = tx, ty
        else:
            translations = 0, 0
        if scale_ranges is not None:
            scale = float(torch.empty(1).uniform_(scale_ranges[0], scale_ranges[1]).item())
        else:
            scale = 1.0
        shear_x = shear_y = 0.0
        if shears is not None:
            shear_x = float(torch.empty(1).uniform_(shears[0], shears[1]).item())
            if len(shears) == 4:
                shear_y = float(torch.empty(1).uniform_(shears[2], shears[3]).item())
        shear = shear_x, shear_y
        return angle, translations, scale, shear

    def forward(self, image, target):
        img_size = F._get_image_size(image)
        ret = self.get_params(self.degrees, self.translate, self.scale, self.shear)
        image = F.affine(image, *ret, resample=Image.LINEAR, fillcolor=0)
        if target is None or isinstance(target, str):
            return image, target
        elif isinstance(target, dict):
            if 'keypoints' in target.keys():
                target['keypoints'] = F_kp.affine(target['keypoints'], *ret, height=img_size[1], width=img_size[0], ignore_x=self.ignore_x)
            if 'segmentation_mask' in target.keys():
                target['segmentation_mask'] = F.affine(target['segmentation_mask'], *ret, resample=Image.NEAREST, fillcolor=255)
        else:
            target = F.affine(target, *ret, resample=Image.NEAREST, fillcolor=255)
        return image, target


class LaneATTLabelFormat(torch.nn.Module):

    def __init__(self, num_points, image_size, max_lanes, ignore_x=-2):
        super().__init__()
        assert isinstance(image_size, (tuple, list))
        self.img_h, self.img_w = image_size
        self.num_points = num_points
        self.num_offsets = num_points
        self.num_strips = num_points - 1
        self.strip_size = self.img_h / self.num_strips
        self.max_lanes = max_lanes
        self.ignore_x = ignore_x
        self.offsets_ys = np.arange(self.img_h, -1, -self.strip_size)

    def clip_out_of_image(self, x):
        num_lanes = x.shape[0]
        lanes_ = []
        for i in range(num_lanes):
            lane = x[i]
            lane_temp = [[point[0], point[1]] for point in lane if point[0] != self.ignore_x]
            lanes_.append(lane_temp)
        return lanes_

    def filter_lane(self, lane):
        assert lane[-1][1] <= lane[0][1]
        filtered_lane = []
        used = set()
        for p in lane:
            if p[1] not in used:
                filtered_lane.append(p)
                used.add(p[1])
        return filtered_lane

    def transform_annotation(self, offsets):
        lanes_ = offsets
        lanes_ = filter(lambda x: len(x) > 1, lanes_)
        lanes_ = [sorted(lane, key=lambda x: -x[1]) for lane in lanes_]
        lanes_ = [self.filter_lane(lane) for lane in lanes_]
        offsets = np.ones((self.max_lanes, self.num_offsets), dtype=np.float32) * -100000.0
        starts = np.ones(self.max_lanes, dtype=np.float32) * -100000.0
        lengths = np.ones(self.max_lanes, dtype=np.float32) * -100000.0
        flags = np.zeros(self.max_lanes, dtype=np.bool)
        for i, lane in enumerate(lanes_):
            try:
                xs_outside_image, xs_inside_image = self.sample_lane(lane, self.offsets_ys)
            except AssertionError:
                continue
            if len(xs_inside_image) == 0:
                continue
            all_xs = np.hstack((xs_outside_image, xs_inside_image))
            flags[i] = True
            starts[i] = len(xs_outside_image) / self.num_strips
            lengths[i] = len(xs_inside_image)
            offsets[i, :len(all_xs)] = all_xs
        return {'offsets': offsets, 'lengths': lengths, 'starts': starts, 'flags': flags}

    def sample_lane(self, points, sample_ys):
        points = np.array(points)
        if not np.all(points[1:, 1] < points[:-1, 1]):
            raise Exception('Annotaion points have to be sorted')
        x, y = points[:, 0], points[:, 1]
        assert len(points) > 1
        interp = InterpolatedUnivariateSpline(y[::-1], x[::-1], k=min(3, len(points) - 1))
        domain_min_y = y.min()
        domain_max_y = y.max()
        sample_ys_inside_domain = sample_ys[(sample_ys >= domain_min_y) & (sample_ys <= domain_max_y)]
        assert len(sample_ys_inside_domain) > 0
        interp_xs = interp(sample_ys_inside_domain)
        two_closest_points = points[:2]
        extrap = np.polyfit(two_closest_points[:, 1], two_closest_points[:, 0], deg=1)
        extrap_ys = sample_ys[sample_ys > domain_max_y]
        extrap_xs = np.polyval(extrap, extrap_ys)
        all_xs = np.hstack((extrap_xs, interp_xs))
        inside_mask = (all_xs >= 0) & (all_xs < self.img_w)
        xs_inside_image = all_xs[inside_mask]
        xs_outside_image = all_xs[~inside_mask]
        return xs_outside_image, xs_inside_image

    def forward(self, image, target):
        keypoints = target['keypoints']
        cilp_lanes = self.clip_out_of_image(keypoints)
        labels = self.transform_annotation(cilp_lanes)
        return image, labels


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ASPP,
     lambda: ([], {'in_channels': 4, 'atrous_rates': [4, 4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ASPPConv,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'dilation': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ASPPPooling,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ASPP_V2,
     lambda: ([], {'in_channels': 4, 'num_classes': 4, 'atrous_rates': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BUSD,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 128, 4, 4])], {}),
     False),
    (BasicBlock,
     lambda: ([], {'inplanes': 4, 'planes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BilateralUpsamplerBlock,
     lambda: ([], {'ninput': 4, 'noutput': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ConvBNReLU,
     lambda: ([], {'in_chan': 4, 'out_chan': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ConvProjection_1D,
     lambda: ([], {'num_layers': 1, 'in_channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (Decoder,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 128, 4, 4])], {}),
     False),
    (DeepLabV1Head,
     lambda: ([], {'in_channels': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DeepLabV2Head,
     lambda: ([], {'in_channels': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DeepLabV3Head,
     lambda: ([], {'in_channels': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DilatedBottleneck,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 512, 64, 64])], {}),
     True),
    (ERFNet,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (ERFNetEncoder,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (Encoder,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (FCNHead,
     lambda: ([], {'in_channels': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FrozenBatchNorm2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (InvertedResidual,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'stride': 1, 'expand_ratio': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (InvertedResidualV3,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'mid_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LargeFOV,
     lambda: ([], {'in_channels': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MLP,
     lambda: ([], {'input_dim': 4, 'hidden_dim': 4, 'output_dim': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Mlp,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MobileNetV2Encoder,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
    (PlainDecoder,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 128, 64, 64])], {}),
     True),
    (PositionEmbeddingLearned,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (RESA,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 128, 64, 64])], {}),
     False),
    (RESAReducer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 512, 64, 64])], {}),
     True),
    (SADLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (SCNNDecoder,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 2048, 64, 64])], {}),
     True),
    (SEBlock,
     lambda: ([], {'input_channels': 4, 'internal_neurons': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SELayer,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SimpleSegHead,
     lambda: ([], {'in_channels': 4, 'mid_channels': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SpatialConv,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 128, 4, 4])], {}),
     False),
    (SpatialSoftmax,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TransformerDecoderLayer,
     lambda: ([], {'d_model': 4, 'nhead': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (TransformerEncoderLayer,
     lambda: ([], {'d_model': 4, 'nhead': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (UperHead,
     lambda: ([], {'in_channels': [4, 4], 'channels': 4}),
     lambda: ([(torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]))], {}),
     True),
    (UpsamplerBlock,
     lambda: ([], {'ninput': 4, 'noutput': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (VGG16,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (WeightedCrossEntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (_EncoderDecoderModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (non_bottleneck_1d,
     lambda: ([], {'chann': 4, 'dropprob': 0.5, 'dilated': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_voldemortX_pytorch_auto_drive(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

    def test_041(self):
        self._check(*TESTCASES[41])

    def test_042(self):
        self._check(*TESTCASES[42])

    def test_043(self):
        self._check(*TESTCASES[43])


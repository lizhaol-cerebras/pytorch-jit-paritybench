import sys
_module = sys.modules[__name__]
del sys
conf = _module
locallaunch = _module
mmpt = _module
datasets = _module
fairseqmmdataset = _module
mmdataset = _module
evaluators = _module
evaluator = _module
metric = _module
predictor = _module
losses = _module
fairseqmmloss = _module
loss = _module
nce = _module
models = _module
fairseqmmmodel = _module
mmfusion = _module
mmfusionnlg = _module
transformermodel = _module
modules = _module
mm = _module
retri = _module
vectorpool = _module
processors = _module
dedupprocessor = _module
dsprocessor = _module
how2processor = _module
how2retriprocessor = _module
s3dg = _module
processor = _module
tasks = _module
fairseqmmtask = _module
milncetask = _module
retritask = _module
task = _module
vlmtask = _module
utils = _module
load_config = _module
shardedtensor = _module
localjob = _module
predict = _module
pretokenization = _module
extract = _module
model = _module
pathbuilder = _module
preprocessing = _module
random_sequence_shuffler = _module
shard_feature = _module
videoreader = _module
setup = _module
examples = _module
adaptive_span = _module
adagrad_with_grad_clip = _module
adaptive_span_attention = _module
adaptive_span_loss = _module
adaptive_span_model = _module
adaptive_span_model_wrapper = _module
src = _module
data = _module
speech_to_text_dataset_with_domain = _module
attention_head_selection = _module
head_selection_s2t_transformer = _module
head_selection_transformer = _module
attn_head_selector = _module
head_selection_transformer_layer = _module
multihead_attention_selection = _module
multihead_functional = _module
speech_to_text_head_selection = _module
generate_manifests = _module
deduplicate_lines = _module
extract_bt_data = _module
summarize = _module
get_bitext = _module
gru_transformer = _module
normalize = _module
tok = _module
mine = _module
save_encoder = _module
encoder_analysis = _module
data2vec = _module
add_class_target_dataset = _module
image_dataset = _module
mae_finetuning_image_dataset = _module
mae_image_dataset = _module
modality = _module
path_dataset = _module
fb_convert_beit_cp = _module
audio_classification = _module
data2vec2 = _module
data2vec_audio = _module
data2vec_image_classification = _module
data2vec_text = _module
data2vec_text_classification = _module
data2vec_vision = _module
mae = _module
mae_image_classification = _module
modalities = _module
audio = _module
base = _module
images = _module
modules = _module
text = _module
utils = _module
convert_audioset_labels = _module
glue = _module
glue_lr = _module
unprocess_data = _module
valids = _module
audio_classification = _module
image_classification = _module
image_pretraining = _module
mae_image_classification = _module
mae_image_pretraining = _module
multimodal = _module
discriminative_reranking_nmt = _module
criterions = _module
discriminative_reranking_criterion = _module
drnmt_rerank = _module
discriminative_reranking_model = _module
prep_data = _module
discriminative_reranking_task = _module
emotion_models = _module
duration_predictor = _module
pitch_predictor = _module
utils = _module
fairseq_models = _module
preprocess = _module
build_hifigan_manifest = _module
build_translation_manifests = _module
create_core_manifest = _module
extract_f0 = _module
process_km = _module
split_emov_km_tsv_by_uttid = _module
split_km = _module
split_km_tsv = _module
synthesize = _module
fast_noisy_channel = _module
noisy_channel_beam_search = _module
noisy_channel_sequence_generator = _module
noisy_channel_translation = _module
measure_teacher_quality = _module
dump_hubert_feature = _module
dump_hubert_feature_s2t = _module
dump_km_label = _module
dump_mfcc_feature = _module
dump_w2v2_feature = _module
feature_utils = _module
learn_kmeans = _module
update_ckpt = _module
laser_src = _module
laser_lstm = _module
laser_task = _module
laser_transformer = _module
multitask_data_utils = _module
latent_depth_src = _module
latent_depth = _module
latent_multilingual_transformer = _module
latent_transformer = _module
latent_layers = _module
multilingual_translation_latent_depth = _module
linformer_src = _module
linformer_roberta = _module
linformer_sentence_encoder = _module
linformer_sentence_encoder_layer = _module
multihead_linear_attention = _module
clean_histogram = _module
dedup_data = _module
remove_too_much_punc = _module
tokenize_indic = _module
tokenize_thai = _module
tokenize_zh = _module
detok = _module
mms_infer = _module
align_and_segment = _module
align_utils = _module
norm_config = _module
text_normalization = _module
infer = _module
infer = _module
falign = _module
lib = _module
uromanize = _module
make_parallel_single_runs = _module
merge_by_lang = _module
prep_wav_list = _module
run_single_lang = _module
split_by_lang = _module
rerank = _module
tune_coefficients = _module
infer_asr = _module
infer_lid = _module
get_sample_size = _module
infer = _module
binarize = _module
check_iswlt_test_data = _module
check_self_overlaps = _module
check_valid_test_overlaps = _module
dedup_all = _module
download_ted_and_extract = _module
download_wmt19_and_before = _module
remove_valid_test_in_train = _module
dedup = _module
fasttext_multi_filter = _module
noisychannel = _module
rerank_generate = _module
rerank_options = _module
rerank_score_bw = _module
rerank_score_lm = _module
rerank_tune = _module
rerank_utils = _module
paraphrase = _module
pointer_generator_src = _module
transformer_pg = _module
postprocess = _module
commonsense_qa = _module
commonsense_qa_task = _module
multiprocessing_bpe_encoder = _module
preprocess_RACE = _module
wsc = _module
wsc_criterion = _module
wsc_task = _module
wsc_utils = _module
rxf = _module
rxf_src = _module
label_smoothed_cross_entropy_r3f = _module
sentence_prediction_r3f = _module
simultaneous_translation = _module
simul_t2t_enja = _module
convtransformer_simul_trans = _module
transformer_monotonic_attention = _module
fixed_pre_decision = _module
monotonic_multihead_attention = _module
monotonic_transformer_layer = _module
test_alignment_train = _module
test_text_models = _module
functions = _module
monotonic_attention = _module
p_choose_strategy = _module
speech_recognition = _module
ASG_loss = _module
cross_entropy_acc = _module
asr_dataset = _module
collaters = _module
data_utils = _module
replabels = _module
asr_prep_json = _module
infer = _module
kaldi = _module
kaldi_decoder = _module
kaldi_initializer = _module
vggtransformer = _module
w2l_conv_glu_enc = _module
new = _module
decoders = _module
base_decoder = _module
decoder = _module
decoder_config = _module
flashlight_decoder = _module
viterbi_decoder = _module
infer = _module
speech_recognition = _module
wer_utils = _module
w2l_decoder = _module
speech_synthesis = _module
data_utils = _module
evaluation = _module
eval_asr = _module
eval_f0 = _module
eval_sp = _module
get_eval_manifest = _module
generate_waveform = _module
denoise_and_vad_audio = _module
denoiser = _module
demucs = _module
pretrained = _module
resample = _module
utils = _module
get_common_voice_audio_manifest = _module
get_feature_manifest = _module
get_ljspeech_audio_manifest = _module
get_speaker_embedding = _module
get_vctk_audio_manifest = _module
speaker_embedder = _module
vad = _module
utils = _module
speech_text_joint_to_text = _module
multi_modality_compound = _module
multi_modality_cross_entropy = _module
text_guide_cross_entropy_acc = _module
pair_denoising_dataset = _module
joint_speech_text_pretrain_transformer = _module
s2t_dualinputtransformer = _module
s2t_dualinputwavtransformer = _module
s2t_dualinputxmtransformer = _module
convert_model = _module
g2p_encode = _module
pair_denoising = _module
speech_text_denoise_pretrain = _module
speech_text_joint = _module
speech_to_speech = _module
asr_bleu = _module
compute_asr_bleu = _module
utils = _module
core = _module
data_utils = _module
get_metrics = _module
generate_waveform_from_code = _module
prep_s2spect_data = _module
prep_s2ut_data = _module
prep_sn_data = _module
prep_sn_output_data = _module
unity = _module
sequence_generator = _module
sequence_generator_multi_decoder = _module
data_utils = _module
prep_covost_data = _module
prep_librispeech_data = _module
prep_mtedx_data = _module
prep_mustc_data = _module
seg_mustc_data = _module
fairseq_simul_st_agent = _module
create_code_file = _module
dgslm_utils = _module
sample_speech_dlm = _module
generate_stereo_waveform = _module
dump_abx_feats = _module
continuation_eval = _module
bleu_utils = _module
cut_as = _module
ppx = _module
self_auto_bleu = _module
speech2unit = _module
clustering = _module
cluster_kmeans = _module
dump_feats = _module
quantize_with_kmeans = _module
cpc_feature_reader = _module
hubert_feature_reader = _module
logmel_feature_reader = _module
utils = _module
w2v2_feature_reader = _module
resynthesize_speech = _module
sample = _module
convert_to_16k = _module
glow = _module
multiproc = _module
synthesize_audio_from_units = _module
tacotron2 = _module
audio_processing = _module
cleaners = _module
cmudict = _module
layers = _module
model = _module
numbers = _module
stft = _module
symbols = _module
utils = _module
waveglow_denoiser = _module
tts_data = _module
utils = _module
data_utils = _module
eval = _module
cont_metrics = _module
generate_waveform = _module
inference_dataset = _module
naive_decoder = _module
prepare_dataset = _module
preprocess_f0 = _module
quantize_f0 = _module
sample = _module
join_units_manifest = _module
truncated_laplace = _module
score = _module
translation_moe_src = _module
logsumexp_moe = _module
mean_pool_gating_network = _module
translation_moe = _module
truncated_bptt = _module
transformer_xl_model = _module
truncated_bptt_lm_task = _module
aggregate_scores = _module
meteor = _module
repeat_lines = _module
wav2vec = _module
libri_labels = _module
unsupervised = _module
extracted_features_dataset = _module
random_input_dataset = _module
copy_aligned_text = _module
prepare_data_from_w2v = _module
unsup_select = _module
wav2vec_u = _module
apply_pca = _module
copy_labels = _module
filter_lexicon = _module
filter_tsv = _module
g2p_wrd_to_phn = _module
ltr_to_wrd = _module
mean_pool = _module
merge_clusters = _module
normalize_and_filter_text = _module
normalize_text = _module
pca = _module
phonemize_with_sil = _module
remove_silence = _module
vads = _module
wav2vec_apply_cluster_faiss = _module
wav2vec_cluster_faiss = _module
wav2vec_extract_features = _module
wer = _module
wrd_to_ltr = _module
unpaired_audio_text = _module
w2vu_generate = _module
wav2vec_featurize = _module
wav2vec_manifest = _module
eval_speaker_clf_task = _module
gen_audio_embedding = _module
query_occupations_from_wikidata = _module
preprocess_nli = _module
fairseq = _module
benchmark = _module
benchmark_multihead_attention = _module
dummy_dataset = _module
dummy_lm = _module
dummy_masked_lm = _module
dummy_model = _module
dummy_mt = _module
binarizer = _module
checkpoint_utils = _module
config = _module
adaptive_loss = _module
composite_loss = _module
cross_entropy = _module
ctc = _module
fairseq_criterion = _module
fastspeech2_loss = _module
hubert_criterion = _module
label_smoothed_cross_entropy = _module
label_smoothed_cross_entropy_latency_augmented = _module
label_smoothed_cross_entropy_with_alignment = _module
label_smoothed_cross_entropy_with_ctc = _module
label_smoothed_cross_entropy_with_rdrop = _module
legacy_masked_lm = _module
masked_lm = _module
model_criterion = _module
nat_loss = _module
sentence_prediction = _module
sentence_prediction_adapters = _module
sentence_ranking = _module
speech_dlm_criterion = _module
speech_to_speech_criterion = _module
speech_ulm_criterion = _module
tacotron2_loss = _module
wav2vec_criterion = _module
add_class_target_dataset = _module
add_target_dataset = _module
append_token_dataset = _module
audio_utils = _module
data_cfg = _module
dataset_transforms = _module
concataugment = _module
noisyoverlapaugment = _module
feature_transforms = _module
delta_deltas = _module
global_cmvn = _module
specaugment = _module
utterance_cmvn = _module
frm_text_to_speech_dataset = _module
hubert_dataset = _module
multi_modality_dataset = _module
raw_audio_dataset = _module
speech_to_speech_dataset = _module
speech_to_text_dataset = _module
speech_to_text_joint_dataset = _module
text_to_speech_dataset = _module
waveform_transforms = _module
noiseaugment = _module
backtranslation_dataset = _module
base_wrapper_dataset = _module
bucket_pad_length_dataset = _module
codedataset = _module
colorize_dataset = _module
concat_dataset = _module
concat_sentences_dataset = _module
data_utils = _module
denoising_dataset = _module
dictionary = _module
encoders = _module
byte_bpe = _module
byte_utils = _module
bytes = _module
characters = _module
fastbpe = _module
gpt2_bpe = _module
gpt2_bpe_utils = _module
hf_bert_bpe = _module
hf_byte_bpe = _module
moses_tokenizer = _module
nltk_tokenizer = _module
sentencepiece_bpe = _module
space_tokenizer = _module
subword_nmt_bpe = _module
utils = _module
fairseq_dataset = _module
fasta_dataset = _module
huffman = _module
huffman_coder = _module
huffman_mmap_indexed_dataset = _module
id_dataset = _module
indexed_dataset = _module
iterators = _module
language_pair_dataset = _module
legacy = _module
block_pair_dataset = _module
masked_lm_dataset = _module
masked_lm_dictionary = _module
list_dataset = _module
lm_context_window_dataset = _module
lru_cache_dataset = _module
mask_tokens_dataset = _module
monolingual_dataset = _module
multi_corpus_dataset = _module
multi_corpus_sampled_dataset = _module
multilingual = _module
multilingual_data_manager = _module
multilingual_utils = _module
sampled_multi_dataset = _module
sampled_multi_epoch_dataset = _module
sampling_method = _module
nested_dictionary_dataset = _module
noising = _module
num_samples_dataset = _module
numel_dataset = _module
offset_tokens_dataset = _module
pad_dataset = _module
padding_mask_dataset = _module
plasma_utils = _module
prepend_dataset = _module
prepend_token_dataset = _module
raw_label_dataset = _module
replace_dataset = _module
resampling_dataset = _module
roll_dataset = _module
round_robin_zip_datasets = _module
shorten_dataset = _module
sort_dataset = _module
span_mask_tokens_dataset = _module
speech_dlm_dataset = _module
strip_token_dataset = _module
subsample_dataset = _module
text_compressor = _module
token_block_dataset = _module
transform_eos_concat_langpair_dataset = _module
transform_eos_dataset = _module
transform_eos_lang_pair_dataset = _module
dataclass = _module
configs = _module
constants = _module
initialize = _module
distributed = _module
distributed_timeout_wrapper = _module
fully_sharded_data_parallel = _module
legacy_distributed_data_parallel = _module
module_proxy_wrapper = _module
tpu_distributed_data_parallel = _module
utils = _module
file_chunker_utils = _module
file_io = _module
file_utils = _module
hub_utils = _module
incremental_decoding_utils = _module
iterative_refinement_generator = _module
logging = _module
meters = _module
metrics = _module
progress_bar = _module
model_parallel = _module
vocab_parallel_cross_entropy = _module
megatron_trainer = _module
pipeline_parallel_transformer = _module
layers = _module
model = _module
roberta = _module
model = _module
transformer = _module
transformer_lm = _module
multihead_attention = _module
transformer_layer = _module
bart = _module
hub_interface = _module
model = _module
composite_encoder = _module
distributed_fairseq_model = _module
ema = _module
ema = _module
fairseq_decoder = _module
fairseq_encoder = _module
fairseq_incremental_decoder = _module
fairseq_model = _module
fconv = _module
fconv_lm = _module
fconv_self_att = _module
hubert = _module
hubert = _module
hubert_asr = _module
huggingface = _module
hf_gpt2 = _module
lightconv = _module
lightconv_lm = _module
lstm = _module
lstm_lm = _module
masked_lm = _module
model_utils = _module
multilingual_transformer = _module
multires_hubert = _module
multires_hubert = _module
multires_hubert_asr = _module
nat = _module
cmlm_transformer = _module
fairseq_nat_model = _module
insertion_transformer = _module
iterative_nonautoregressive_transformer = _module
levenshtein_transformer = _module
levenshtein_utils = _module
nat_crf_transformer = _module
nonautoregressive_ensembles = _module
nonautoregressive_transformer = _module
alignment_utils = _module
enc_dec = _module
hub_interface = _module
model = _module
model_camembert = _module
model_gottbert = _module
model_xlmr = _module
speech_dlm = _module
hub_interface = _module
speech_dlm_decoder = _module
speech_dlm_decoder_layer = _module
multichannel_search = _module
multichannel_sequence_generator = _module
ctc_decoder = _module
stacked_embedding = _module
transformer_decoder_aug = _module
transformer_encoder = _module
s2s_conformer = _module
s2s_conformer_translatotron2 = _module
s2s_conformer_unity = _module
s2s_transformer = _module
speech_to_text = _module
berard = _module
convtransformer = _module
hub_interface = _module
augmented_memory_attention = _module
convolution = _module
emformer = _module
multi_modality_model = _module
s2t_conformer = _module
s2t_transformer = _module
s2t_wav_transformer = _module
utils = _module
xm_transformer = _module
xm_transformer_unity = _module
text_to_speech = _module
codehifigan = _module
fastspeech2 = _module
hifigan = _module
hub_interface = _module
tacotron2 = _module
tts_transformer = _module
vocoder = _module
transformer_base = _module
transformer_config = _module
transformer_decoder = _module
transformer_decoder_aug = _module
transformer_encoder = _module
transformer_legacy = _module
transformer_align = _module
transformer_from_pretrained_xlm = _module
transformer_lm = _module
transformer_ulm = _module
utils = _module
wav2vec = _module
wav2vec2 = _module
wav2vec2_asr = _module
wav2vec2_classification = _module
wav2vec2_laser = _module
xmod = _module
hub_interface = _module
model = _module
transformer_layer_xmod = _module
adaptive_input = _module
adaptive_softmax = _module
base_layer = _module
beamable_mm = _module
character_token_embedder = _module
checkpoint_activations = _module
conformer_layer = _module
conv_tbc = _module
cross_entropy = _module
downsampled_multihead_attention = _module
dynamic_convolution = _module
dynamic_crf_layer = _module
dynamicconv_layer = _module
cuda_function_gen = _module
dynamicconv_layer = _module
setup = _module
ema_module = _module
espnet_multihead_attention = _module
fairseq_dropout = _module
fp32_batch_norm = _module
fp32_group_norm = _module
fp32_instance_norm = _module
gelu = _module
grad_multiply = _module
gumbel_vector_quantizer = _module
kmeans_attention = _module
kmeans_vector_quantizer = _module
layer_drop = _module
layer_norm = _module
learned_positional_embedding = _module
lightconv_layer = _module
lightconv_layer = _module
setup = _module
lightweight_convolution = _module
linearized_convolution = _module
location_attention = _module
lstm_cell_with_zoneout = _module
multihead_attention = _module
positional_embedding = _module
positional_encoding = _module
quant_noise = _module
quantization = _module
pq = _module
em = _module
qconv = _module
qemb = _module
qlinear = _module
utils = _module
quantization_options = _module
scalar = _module
qact = _module
qconv = _module
qemb = _module
qlinear = _module
ops = _module
utils = _module
rotary_positional_embedding = _module
same_pad = _module
scalar_bias = _module
sinusoidal_positional_embedding = _module
sparse_multihead_attention = _module
sparse_transformer_sentence_encoder = _module
sparse_transformer_sentence_encoder_layer = _module
transformer_layer = _module
transformer_layer_aug = _module
transformer_sentence_encoder = _module
transformer_sentence_encoder_layer = _module
transpose_last = _module
unfold = _module
vggblock = _module
nan_detector = _module
ngram_repeat_block = _module
optim = _module
adadelta = _module
adafactor = _module
adagrad = _module
adam = _module
adamax = _module
amp_optimizer = _module
bmuf = _module
composite = _module
cpu_adam = _module
dynamic_loss_scaler = _module
fairseq_optimizer = _module
fp16_optimizer = _module
fused_adam = _module
fused_lamb = _module
lr_scheduler = _module
cosine_lr_scheduler = _module
fairseq_lr_scheduler = _module
fixed_schedule = _module
inverse_square_root_schedule = _module
manual_lr_scheduler = _module
pass_through = _module
polynomial_decay_schedule = _module
reduce_lr_on_plateau = _module
step_lr_scheduler = _module
tri_stage_lr_scheduler = _module
triangular_lr_scheduler = _module
nag = _module
sgd = _module
shard = _module
options = _module
pdb = _module
quantization_utils = _module
registry = _module
scoring = _module
bertscore = _module
bleu = _module
chrf = _module
tokenizer = _module
search = _module
sequence_generator = _module
sequence_scorer = _module
speech_generator = _module
audio_classification = _module
audio_finetuning = _module
audio_pretraining = _module
cross_lingual_lm = _module
denoising = _module
fairseq_task = _module
frm_text_to_speech = _module
hubert_pretraining = _module
language_modeling = _module
multilingual_denoising = _module
multilingual_language_modeling = _module
multilingual_masked_lm = _module
multilingual_translation = _module
multires_hubert_pretraining = _module
nlu_finetuning = _module
online_backtranslation = _module
semisupervised_translation = _module
span_masked_lm = _module
speech_dlm_task = _module
speech_to_speech = _module
speech_ulm_task = _module
text_to_speech = _module
translation = _module
translation_from_pretrained_bart = _module
translation_from_pretrained_xlm = _module
translation_lev = _module
translation_multi_simple_epoch = _module
token_generation_constraints = _module
trainer = _module
utils = _module
fairseq_cli = _module
eval_lm = _module
generate = _module
hydra_train = _module
hydra_validate = _module
interactive = _module
train = _module
validate = _module
hubconf = _module
dependency_submitit_launcher = _module
launcher = _module
release_utils = _module
scripts = _module
average_checkpoints = _module
build_sym_alignment = _module
check_installation = _module
compare_namespaces = _module
count_docs = _module
read_binarized = _module
rm_pt = _module
shard_docs = _module
split_train_valid_docs = _module
spm_decode = _module
spm_encode = _module
spm_train = _module
setup = _module
tests = _module
test_bmuf = _module
test_distributed_timeout_wrapper = _module
test_module_proxy_wrapper = _module
test_utils = _module
utils = _module
gpu = _module
test_binaries_gpu = _module
test_ema_gpu = _module
speech = _module
test_convtransformer_simul_trans = _module
test_dual_input_wav_transformer = _module
test_dualinput_s2t_transformer = _module
test_fastspeech2 = _module
test_s2s_transformer = _module
test_s2t_conformer = _module
test_s2t_transformer = _module
test_tts_transformer = _module
test_wav2vec2 = _module
test_xm_transformer = _module
asr_test_base = _module
test_collaters = _module
test_cross_entropy = _module
test_data_utils = _module
test_vggtransformer = _module
test_denoising = _module
test_masked_lm = _module
test_multilingual_denoising = _module
test_span_masked_lm = _module
test_activation_checkpointing = _module
test_amp_optimizer = _module
test_average_checkpoints = _module
test_backtranslation_dataset = _module
test_binaries = _module
test_binarizer = _module
test_character_token_embedder = _module
test_checkpoint_utils = _module
test_checkpoint_utils_for_task_level_attributes = _module
test_concat_dataset = _module
test_constraints = _module
test_convtbc = _module
test_dataclass_utils = _module
test_dataset = _module
test_dictionary = _module
test_ema = _module
test_espnet_multihead_attention = _module
test_export = _module
test_file_chunker_utils = _module
test_file_io = _module
test_fp16_optimizer = _module
test_hf_hub = _module
test_huffman = _module
test_inference_dropout = _module
test_iopath = _module
test_iterators = _module
test_label_smoothing = _module
test_lm_context_window = _module
test_lstm_jitable = _module
test_memory_efficient_fp16 = _module
test_metrics = _module
test_multi_corpus_dataset = _module
test_multi_corpus_sampled_dataset = _module
test_multihead_attention = _module
test_noising = _module
test_online_backtranslation = _module
test_plasma_utils = _module
test_positional_encoding = _module
test_reproducibility = _module
test_resampling_dataset = _module
test_roberta = _module
test_rotary_positional_embedding = _module
test_sequence_generator = _module
test_sequence_scorer = _module
test_sparse_multihead_attention = _module
test_token_block_dataset = _module
test_train = _module
test_transformer = _module
test_utils = _module
test_valid_subset_checks = _module
utils = _module

from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from collections import OrderedDict


from torch.utils.data import Dataset


from torch.utils.data.dataloader import default_collate


import torch


import random


import numpy as np


import math


from torch import nn


from torch.nn import functional as F


from typing import Optional


from typing import Iterable


from collections import defaultdict


from collections import deque


from typing import Tuple


from typing import List


import torch as th


import torch.nn.functional as F


import torch.nn as nn


import re


from torch.utils.data import DataLoader


from torch.utils.data.distributed import DistributedSampler


from torch.utils.data.sampler import Sampler


import pandas as pd


from torch.optim import Adagrad


import logging


from typing import Dict


from torch.nn.modules.loss import _Loss


from torch import Tensor


from typing import Any


from torch.nn import Parameter


from torch.nn.functional import linear


from torch.nn.functional import softmax


from torch.nn.functional import dropout


from torch.nn.functional import pad


from torch.nn.functional import has_torch_function


from torch.nn.functional import handle_torch_function


from torch.nn.functional import _in_projection_packed


import warnings


from typing import Callable


from typing import Set


from torchvision.datasets.vision import VisionDataset


from torchvision.transforms import ToTensor


from torchvision import datasets


from torchvision import transforms


from functools import partial


import time


import torch.distributed as dist


from enum import Enum


from enum import auto


from collections import namedtuple


from sklearn import metrics as sklearn_metrics


import itertools


from scipy.io.wavfile import read


from scipy.ndimage import gaussian_filter1d


import torchaudio


import torchaudio.functional as F


from torchaudio.models import wav2vec2_model


import copy


import torch.utils.cpp_extension


from scipy.io.wavfile import write


from typing import NamedTuple


from torch.testing._internal.common_utils import TestCase


from collections.abc import Iterable


import itertools as it


from typing import Union


from itertools import groupby


import matplotlib.pyplot as plt


import torch.hub


import functools


import inspect


import torch.utils.data


from scipy.interpolate import interp1d


from functools import reduce


import torchaudio.compliance.kaldi as kaldi


from torch.autograd import Variable


from scipy.signal import get_window


from math import sqrt


import torch.distributions as distr


import collections


import scipy


import torch.multiprocessing as mp


from torch.utils.data import DistributedSampler


from types import SimpleNamespace


from itertools import starmap


from torch.distributions.categorical import Categorical


from torch import autograd


import sklearn


from torch.utils import benchmark


import typing as tp


from abc import ABC


from abc import abstractmethod


from collections import Counter


from itertools import chain


from sklearn.metrics import f1_score


from sklearn.metrics import matthews_corrcoef as _matthews_corrcoef


from scipy.stats import pearsonr


from scipy.stats import spearmanr


from functools import lru_cache


from typing import BinaryIO


import queue


from typing import Iterator


from typing import Sequence


from typing import Mapping


from functools import wraps


import uuid


from numbers import Number


from torch.nn.parallel import DistributedDataParallel


from torch import device as Device


from itertools import repeat


from torch.nn import Conv1d


from torch.nn import ConvTranspose1d


from torch.nn.utils import remove_weight_norm


from torch.nn.utils import weight_norm


import torch.utils.checkpoint as checkpoint


from torch.nn.modules.utils import _single


from torch.autograd import Function


from torch.utils.cpp_extension import BuildExtension


from torch.utils.cpp_extension import CUDAExtension


from inspect import isfunction


from torch.nn.modules.utils import _pair


from torch.nn.modules.conv import _ConvNd


import torch.onnx.operators


from numpy.random import uniform


import torch.optim


from collections.abc import Collection


import types


import torch.optim.lr_scheduler


from torch.optim.optimizer import Optimizer


from torch.optim.optimizer import required


from typing import OrderedDict


from itertools import accumulate


from typing import TYPE_CHECKING


import numpy


from torch.utils import cpp_extension


from copy import deepcopy


from inspect import currentframe


from inspect import getframeinfo


from torch.utils.checkpoint import checkpoint


from torch.cuda.amp import GradScaler


from torch.cuda.amp import autocast


import string


class Aligner(object):
    """
    An alignprocessor align video and text and output a dict of tensors (for a model).
    """

    def __init__(self, config):
        """__init__ needs to be light weight for more workers/threads."""
        self.split = config.split
        self.max_video_len = config.max_video_len
        self.max_len = config.max_len
        tokenizer = AutoTokenizer.from_pretrained(str(config.bert_name), use_fast=config.use_fast)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id
        self.mask_token_id = tokenizer.mask_token_id

    def __call__(self, video_id, video_feature, text_feature):
        raise NotImplementedError

    def _build_video_seq(self, video_feature, video_clips=None):
        """
        `video_feature`: available video tokens.
        `video_clips`: video clip sequence to build.
        """
        if not isinstance(video_feature, np.ndarray):
            raise ValueError('unsupported type of video_feature', type(video_feature))
        if video_clips is None:
            video_start = 0
            video_end = min(len(video_feature), self.max_video_len)
            video_clips = {'start': [video_start], 'end': [video_end]}
        vfeats = np.zeros((self.max_video_len, video_feature.shape[1]), dtype=np.float32)
        vmasks = torch.zeros((self.max_video_len,), dtype=torch.bool)
        video_len = 0
        for start, end in zip(video_clips['start'], video_clips['end']):
            clip_len = min(self.max_video_len - video_len, end - start)
            if clip_len > 0:
                vfeats[video_len:video_len + clip_len] = video_feature[start:start + clip_len]
                vmasks[video_len:video_len + clip_len] = 1
                video_len += clip_len
        vfeats = torch.from_numpy(vfeats)
        return vfeats, vmasks

    def _build_text_seq(self, text_feature, text_clip_indexs=None):
        """
        `text_feature`: all available clips.
        `text_clip_indexes`: clip sequence to build.
        """
        if text_clip_indexs is None:
            text_clip_indexs = [0]
        full_caps = []
        if isinstance(text_feature, dict):
            for clip_idx in text_clip_indexs:
                full_caps.extend(text_feature['cap'][clip_idx])
        else:
            full_caps = text_feature
        max_text_len = self.max_len - self.max_video_len - 3
        full_caps = full_caps[:max_text_len]
        full_caps = [self.cls_token_id, self.sep_token_id] + full_caps + [self.sep_token_id]
        text_pad_len = self.max_len - len(full_caps) - self.max_video_len
        padded_full_caps = full_caps + [self.pad_token_id] * text_pad_len
        caps = torch.LongTensor(padded_full_caps)
        cmasks = torch.zeros((len(padded_full_caps),), dtype=torch.bool)
        cmasks[:len(full_caps)] = 1
        return caps, cmasks

    def batch_post_processing(self, batch, video_feature):
        return batch


class STConv3D(nn.Module):

    def __init__(self, input_dim, output_dim, kernel_size, stride=1, padding=0, separable=False):
        super(STConv3D, self).__init__()
        self.separable = separable
        self.relu = nn.ReLU(inplace=True)
        assert len(kernel_size) == 3
        if separable and kernel_size[0] != 1:
            spatial_kernel_size = [1, kernel_size[1], kernel_size[2]]
            temporal_kernel_size = [kernel_size[0], 1, 1]
            if isinstance(stride, list) and len(stride) == 3:
                spatial_stride = [1, stride[1], stride[2]]
                temporal_stride = [stride[0], 1, 1]
            else:
                spatial_stride = [1, stride, stride]
                temporal_stride = [stride, 1, 1]
            if isinstance(padding, list) and len(padding) == 3:
                spatial_padding = [0, padding[1], padding[2]]
                temporal_padding = [padding[0], 0, 0]
            else:
                spatial_padding = [0, padding, padding]
                temporal_padding = [padding, 0, 0]
        if separable:
            self.conv1 = nn.Conv3d(input_dim, output_dim, kernel_size=spatial_kernel_size, stride=spatial_stride, padding=spatial_padding, bias=False)
            self.bn1 = nn.BatchNorm3d(output_dim)
            self.conv2 = nn.Conv3d(output_dim, output_dim, kernel_size=temporal_kernel_size, stride=temporal_stride, padding=temporal_padding, bias=False)
            self.bn2 = nn.BatchNorm3d(output_dim)
        else:
            self.conv1 = nn.Conv3d(input_dim, output_dim, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
            self.bn1 = nn.BatchNorm3d(output_dim)

    def forward(self, input):
        out = self.relu(self.bn1(self.conv1(input)))
        if self.separable:
            out = self.relu(self.bn2(self.conv2(out)))
        return out


class SelfGating(nn.Module):

    def __init__(self, input_dim):
        super(SelfGating, self).__init__()
        self.fc = nn.Linear(input_dim, input_dim)

    def forward(self, input_tensor):
        """Feature gating as used in S3D-G.
      """
        spatiotemporal_average = th.mean(input_tensor, dim=[2, 3, 4])
        weights = self.fc(spatiotemporal_average)
        weights = th.sigmoid(weights)
        return weights[:, :, None, None, None] * input_tensor


class InceptionBlock(nn.Module):

    def __init__(self, input_dim, num_outputs_0_0a, num_outputs_1_0a, num_outputs_1_0b, num_outputs_2_0a, num_outputs_2_0b, num_outputs_3_0b, gating=True):
        super(InceptionBlock, self).__init__()
        self.conv_b0 = STConv3D(input_dim, num_outputs_0_0a, [1, 1, 1])
        self.conv_b1_a = STConv3D(input_dim, num_outputs_1_0a, [1, 1, 1])
        self.conv_b1_b = STConv3D(num_outputs_1_0a, num_outputs_1_0b, [3, 3, 3], padding=1, separable=True)
        self.conv_b2_a = STConv3D(input_dim, num_outputs_2_0a, [1, 1, 1])
        self.conv_b2_b = STConv3D(num_outputs_2_0a, num_outputs_2_0b, [3, 3, 3], padding=1, separable=True)
        self.maxpool_b3 = th.nn.MaxPool3d((3, 3, 3), stride=1, padding=1)
        self.conv_b3_b = STConv3D(input_dim, num_outputs_3_0b, [1, 1, 1])
        self.gating = gating
        self.output_dim = num_outputs_0_0a + num_outputs_1_0b + num_outputs_2_0b + num_outputs_3_0b
        if gating:
            self.gating_b0 = SelfGating(num_outputs_0_0a)
            self.gating_b1 = SelfGating(num_outputs_1_0b)
            self.gating_b2 = SelfGating(num_outputs_2_0b)
            self.gating_b3 = SelfGating(num_outputs_3_0b)

    def forward(self, input):
        """Inception block
      """
        b0 = self.conv_b0(input)
        b1 = self.conv_b1_a(input)
        b1 = self.conv_b1_b(b1)
        b2 = self.conv_b2_a(input)
        b2 = self.conv_b2_b(b2)
        b3 = self.maxpool_b3(input)
        b3 = self.conv_b3_b(b3)
        if self.gating:
            b0 = self.gating_b0(b0)
            b1 = self.gating_b1(b1)
            b2 = self.gating_b2(b2)
            b3 = self.gating_b3(b3)
        return th.cat((b0, b1, b2, b3), dim=1)


class MaxPool3dTFPadding(th.nn.Module):

    def __init__(self, kernel_size, stride=None, padding='SAME'):
        super(MaxPool3dTFPadding, self).__init__()
        if padding == 'SAME':
            padding_shape = self._get_padding_shape(kernel_size, stride)
            self.padding_shape = padding_shape
            self.pad = th.nn.ConstantPad3d(padding_shape, 0)
        self.pool = th.nn.MaxPool3d(kernel_size, stride, ceil_mode=True)

    def _get_padding_shape(self, filter_shape, stride):

        def _pad_top_bottom(filter_dim, stride_val):
            pad_along = max(filter_dim - stride_val, 0)
            pad_top = pad_along // 2
            pad_bottom = pad_along - pad_top
            return pad_top, pad_bottom
        padding_shape = []
        for filter_dim, stride_val in zip(filter_shape, stride):
            pad_top, pad_bottom = _pad_top_bottom(filter_dim, stride_val)
            padding_shape.append(pad_top)
            padding_shape.append(pad_bottom)
        depth_top = padding_shape.pop(0)
        depth_bottom = padding_shape.pop(0)
        padding_shape.append(depth_top)
        padding_shape.append(depth_bottom)
        return tuple(padding_shape)

    def forward(self, inp):
        inp = self.pad(inp)
        out = self.pool(inp)
        return out


class Sentence_Embedding(nn.Module):

    def __init__(self, embd_dim, num_embeddings=66250, word_embedding_dim=300, token_to_word_path='dict.npy', max_words=16, output_dim=2048):
        super(Sentence_Embedding, self).__init__()
        self.word_embd = nn.Embedding(num_embeddings, word_embedding_dim)
        self.fc1 = nn.Linear(word_embedding_dim, output_dim)
        self.fc2 = nn.Linear(output_dim, embd_dim)
        self.word_to_token = {}
        self.max_words = max_words
        token_to_word = np.load(token_to_word_path)
        for i, t in enumerate(token_to_word):
            self.word_to_token[t] = i + 1

    def _zero_pad_tensor_token(self, tensor, size):
        if len(tensor) >= size:
            return tensor[:size]
        else:
            zero = th.zeros(size - len(tensor)).long()
            return th.cat((tensor, zero), dim=0)

    def _split_text(self, sentence):
        w = re.findall("[\\w']+", str(sentence))
        return w

    def _words_to_token(self, words):
        words = [self.word_to_token[word] for word in words if word in self.word_to_token]
        if words:
            we = self._zero_pad_tensor_token(th.LongTensor(words), self.max_words)
            return we
        else:
            return th.zeros(self.max_words).long()

    def _words_to_ids(self, x):
        split_x = [self._words_to_token(self._split_text(sent.lower())) for sent in x]
        return th.stack(split_x, dim=0)

    def forward(self, x):
        x = self._words_to_ids(x)
        x = self.word_embd(x)
        x = F.relu(self.fc1(x))
        x = th.max(x, dim=1)[0]
        x = self.fc2(x)
        return {'text_embedding': x}


class S3D(nn.Module):

    def __init__(self, dict_path, num_classes=512, gating=True, space_to_depth=True):
        super(S3D, self).__init__()
        self.num_classes = num_classes
        self.gating = gating
        self.space_to_depth = space_to_depth
        if space_to_depth:
            self.conv1 = STConv3D(24, 64, [2, 4, 4], stride=1, padding=(1, 2, 2), separable=False)
        else:
            self.conv1 = STConv3D(3, 64, [3, 7, 7], stride=2, padding=(1, 3, 3), separable=False)
        self.conv_2b = STConv3D(64, 64, [1, 1, 1], separable=False)
        self.conv_2c = STConv3D(64, 192, [3, 3, 3], padding=1, separable=True)
        self.gating = SelfGating(192)
        self.maxpool_2a = MaxPool3dTFPadding(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding='SAME')
        self.maxpool_3a = MaxPool3dTFPadding(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding='SAME')
        self.mixed_3b = InceptionBlock(192, 64, 96, 128, 16, 32, 32)
        self.mixed_3c = InceptionBlock(self.mixed_3b.output_dim, 128, 128, 192, 32, 96, 64)
        self.maxpool_4a = MaxPool3dTFPadding(kernel_size=(3, 3, 3), stride=(2, 2, 2), padding='SAME')
        self.mixed_4b = InceptionBlock(self.mixed_3c.output_dim, 192, 96, 208, 16, 48, 64)
        self.mixed_4c = InceptionBlock(self.mixed_4b.output_dim, 160, 112, 224, 24, 64, 64)
        self.mixed_4d = InceptionBlock(self.mixed_4c.output_dim, 128, 128, 256, 24, 64, 64)
        self.mixed_4e = InceptionBlock(self.mixed_4d.output_dim, 112, 144, 288, 32, 64, 64)
        self.mixed_4f = InceptionBlock(self.mixed_4e.output_dim, 256, 160, 320, 32, 128, 128)
        self.maxpool_5a = self.maxPool3d_5a_2x2 = MaxPool3dTFPadding(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding='SAME')
        self.mixed_5b = InceptionBlock(self.mixed_4f.output_dim, 256, 160, 320, 32, 128, 128)
        self.mixed_5c = InceptionBlock(self.mixed_5b.output_dim, 384, 192, 384, 48, 128, 128)
        self.fc = nn.Linear(self.mixed_5c.output_dim, num_classes)
        self.text_module = Sentence_Embedding(num_classes, token_to_word_path=dict_path)

    def _space_to_depth(self, input):
        """3D space to depth trick for TPU optimization.
      """
        B, C, T, H, W = input.shape
        input = input.view(B, C, T // 2, 2, H // 2, 2, W // 2, 2)
        input = input.permute(0, 3, 5, 7, 1, 2, 4, 6)
        input = input.contiguous().view(B, 8 * C, T // 2, H // 2, W // 2)
        return input

    def forward(self, inputs):
        """Defines the S3DG base architecture."""
        if self.space_to_depth:
            inputs = self._space_to_depth(inputs)
        net = self.conv1(inputs)
        if self.space_to_depth:
            net = net[:, :, 1:, 1:, 1:]
        net = self.maxpool_2a(net)
        net = self.conv_2b(net)
        net = self.conv_2c(net)
        if self.gating:
            net = self.gating(net)
        net = self.maxpool_3a(net)
        net = self.mixed_3b(net)
        net = self.mixed_3c(net)
        net = self.maxpool_4a(net)
        net = self.mixed_4b(net)
        net = self.mixed_4c(net)
        net = self.mixed_4d(net)
        net = self.mixed_4e(net)
        net = self.mixed_4f(net)
        net = self.maxpool_5a(net)
        net = self.mixed_5b(net)
        net = self.mixed_5c(net)
        net = th.mean(net, dim=[2, 3, 4])
        return {'video_embedding': self.fc(net), 'mixed_5c': net}


def set_seed(seed=43211):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    if torch.backends.cudnn.enabled:
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True


class MMDataset(Dataset):
    """
    A generic multi-modal dataset.
        Args:
            `meta_processor`: a meta processor,
                handling loading meta data and return video_id and text_id.
            `video_processor`: a video processor,
                handling e.g., decoding, loading .np files.
            `text_processor`: a text processor,
                handling e.g., tokenization.
            `aligner`: combine the video and text feature
                as one training example.
    """

    def __init__(self, meta_processor, video_processor, text_processor, align_processor):
        self.split = meta_processor.split
        self.meta_processor = meta_processor
        self.video_processor = video_processor
        self.text_processor = text_processor
        self.align_processor = align_processor

    def __len__(self):
        return len(self.meta_processor)

    def __getitem__(self, idx):
        if self.split == 'test':
            set_seed(idx)
        video_id, text_id = self.meta_processor[idx]
        video_feature = self.video_processor(video_id)
        text_feature = self.text_processor(text_id)
        output = self.align_processor(video_id, video_feature, text_feature)
        output.update({'idx': idx})
        return output

    def collater(self, samples):
        """This collator is deprecated.
        set self.collator = MMDataset.collater.
        see collator in FairseqMMDataset.
        """
        if len(samples) == 0:
            return {}
        if isinstance(samples[0], dict):
            batch = OrderedDict()
            for key in samples[0]:
                if samples[0][key] is not None:
                    batch[key] = default_collate([sample[key] for sample in samples])
            return batch
        else:
            return default_collate(samples)

    def print_example(self, output):
        None
        if hasattr(self.align_processor, 'subsampling') and self.align_processor.subsampling is not None and self.align_processor.subsampling > 1:
            for key in output:
                if torch.is_tensor(output[key]):
                    output[key] = output[key][0]
        tokenizer = None
        if hasattr(self.text_processor, 'tokenizer'):
            tokenizer = self.text_processor.tokenizer
        elif hasattr(self.align_processor, 'tokenizer'):
            tokenizer = self.align_processor.tokenizer
        if tokenizer is not None:
            caps = output['caps'].tolist()
            if isinstance(caps[0], list):
                caps = caps[0]
            None
            None
        for key, value in output.items():
            if torch.is_tensor(value):
                if len(value.size()) >= 3:
                    None
                    None
                    None
                else:
                    None
        None


class Task(object):
    """
    A task refers to one generic training task (e.g., training one model).
    """

    @classmethod
    def config_task(cls, config):
        """
        determine whether to load a hard-coded task or config from a generic one.
        via if a task string is available in config.
        """
        if config.task is not None:
            task_cls = getattr(tasks, config.task)
            return task_cls(config)
        else:
            return Task(config)

    def __init__(self, config):
        self.config = config
        self.train_data = None
        self.val_data = None
        self.test_data = None
        self.model = None
        self.loss_fn = None
        self.eval_fn = None

    def build_dataset(self):
        """TODO (huxu): move processor breakdown to MMDataset."""
        """fill-in `self.train_data`, `self.val_data` and `self.test_data`."""
        meta_processor_cls = getattr(processors, self.config.dataset.meta_processor)
        video_processor_cls = getattr(processors, self.config.dataset.video_processor)
        text_processor_cls = getattr(processors, self.config.dataset.text_processor)
        aligner_cls = getattr(processors, self.config.dataset.aligner)
        if self.config.dataset.train_path is not None:
            self.config.dataset.split = 'train'
            meta_processor = meta_processor_cls(self.config.dataset)
            video_processor = video_processor_cls(self.config.dataset)
            text_processor = text_processor_cls(self.config.dataset)
            aligner = aligner_cls(self.config.dataset)
            self.train_data = MMDataset(meta_processor, video_processor, text_processor, aligner)
            None
            output = self.train_data[0]
            self.train_data.print_example(output)
        if self.config.dataset.val_path is not None:
            self.config.dataset.split = 'valid'
            meta_processor = meta_processor_cls(self.config.dataset)
            video_processor = video_processor_cls(self.config.dataset)
            text_processor = text_processor_cls(self.config.dataset)
            aligner = aligner_cls(self.config.dataset)
            self.val_data = MMDataset(meta_processor, video_processor, text_processor, aligner)
            None
            output = self.val_data[0]
            self.val_data.print_example(output)
        if self.config.dataset.split == 'test':
            meta_processor = meta_processor_cls(self.config.dataset)
            video_processor = video_processor_cls(self.config.dataset)
            text_processor = text_processor_cls(self.config.dataset)
            self.test_data = MMDataset(meta_processor, video_processor, text_processor, aligner)
            None
            output = self.test_data[0]
            self.test_data.print_example(output)

    def build_model(self, checkpoint=None):
        if self.model is None:
            model_cls = getattr(models, self.config.model.model_cls)
            self.model = model_cls(self.config)
        if checkpoint is not None:
            self.load_checkpoint(checkpoint)
        return self.model

    def load_checkpoint(self, checkpoint):
        if self.model is None:
            raise ValueError('model is not initialized.')
        state_dict = torch.load(checkpoint)
        state_dict = self._trim_state_dict(state_dict)
        self.model.load_state_dict(state_dict, strict=False)
        if next(self.model.parameters()).dtype == torch.float16:
            self.model = self.model.float()
        return self.model

    def _trim_state_dict(self, state_dict):
        from collections import OrderedDict
        if 'state_dict' in state_dict:
            state_dict = state_dict['state_dict']
        if 'model' in state_dict:
            state_dict = state_dict['model']
        ret_state_dict = OrderedDict()
        for key, value in state_dict.items():
            if key.startswith('mmmodel'):
                key = key[len('mmmodel.'):]
            ret_state_dict[key] = value
        return ret_state_dict

    def build_loss(self):
        if self.loss_fn is None and self.config.loss is not None:
            loss_cls = getattr(losses, self.config.loss.loss_cls)
            self.loss_fn = loss_cls()
        return self.loss_fn

    def flat_subsample(self, tensor):
        size = tensor.size()
        if len(size) >= 2:
            batch_size = size[0] * size[1]
            expanded_size = (batch_size,) + size[2:] if len(size) > 2 else (batch_size,)
            tensor = tensor.view(expanded_size)
        return tensor

    def reshape_subsample(self, sample):
        if hasattr(self.config.dataset, 'subsampling') and self.config.dataset.subsampling is not None and self.config.dataset.subsampling > 1:
            for key in sample:
                if torch.is_tensor(sample[key]):
                    sample[key] = self.flat_subsample(sample[key])
        return sample

    def __call__(self, model, sample):
        loss = None
        loss_scalar = float('inf')
        sample = self.reshape_subsample(sample)
        outputs = self.model(**sample)
        sample.update(outputs)
        if self.loss_fn is not None:
            loss = self.loss_fn(**sample)
            loss_scalar = loss.item()
        batch_size = sample['caps'].size(0)
        sample_size = 1
        return {'loss': loss, 'loss_scalar': loss_scalar, 'max_len': self.config.dataset.max_len, 'batch_size': batch_size, 'sample_size': sample_size}

    def build_dataloader(self):
        """only used for trainer that lacks building loaders."""
        raise NotImplementedError


def recursive_config(config_path):
    """allows for stacking of configs in any depth."""
    config = OmegaConf.load(config_path)
    if config.includes is not None:
        includes = config.includes
        config.pop('includes')
        base_config = recursive_config(includes)
        config = OmegaConf.merge(base_config, config)
    return config


class MMPTModel(nn.Module):
    """An e2e wrapper of inference model.
    """

    @classmethod
    def from_pretrained(cls, config, checkpoint='checkpoint_best.pt'):
        config = recursive_config(config)
        mmtask = Task.config_task(config)
        checkpoint_path = os.path.join(config.eval.save_path, checkpoint)
        mmtask.build_model(checkpoint=checkpoint_path)
        video_encoder = S3D('pretrained_models/s3d_dict.npy', 512)
        video_encoder.load_state_dict(torch.load('pretrained_models/s3d_howto100m.pth'))
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name, use_fast=config.dataset.use_fast)
        aligner = Aligner(config.dataset)
        return MMPTModel(config, mmtask.model, video_encoder), tokenizer, aligner

    def __init__(self, config, model, video_encoder, **kwargs):
        super().__init__()
        self.max_video_len = config.dataset.max_video_len
        self.video_encoder = video_encoder
        self.model = model

    def forward(self, video_frames, caps, cmasks, return_score=False):
        bsz = video_frames.size(0)
        assert bsz == 1, 'only bsz=1 is supported now.'
        seq_len = video_frames.size(1)
        video_frames = video_frames.view(-1, *video_frames.size()[2:])
        vfeats = self.video_encoder(video_frames.permute(0, 4, 1, 2, 3))
        vfeats = vfeats['video_embedding']
        vfeats = vfeats.view(bsz, seq_len, vfeats.size(-1))
        padding = torch.zeros(bsz, self.max_video_len - seq_len, vfeats.size(-1))
        vfeats = torch.cat([vfeats, padding], dim=1)
        vmasks = torch.cat([torch.ones((bsz, seq_len), dtype=torch.bool), torch.zeros((bsz, self.max_video_len - seq_len), dtype=torch.bool)], dim=1)
        output = self.model(caps, cmasks, vfeats, vmasks)
        if return_score:
            output = {'score': torch.bmm(output['pooled_video'][:, None, :], output['pooled_text'][:, :, None]).squeeze(-1).squeeze(-1)}
        return output


class MMFusion(nn.Module):
    """a MMPT wrapper class for MMBert style models.
    TODO: move isolated mask to a subclass.
    """

    def __init__(self, config, **kwargs):
        super().__init__()
        transformer_config = AutoConfig.from_pretrained(config.dataset.bert_name)
        self.hidden_size = transformer_config.hidden_size
        self.is_train = False
        if config.dataset.train_path is not None:
            self.is_train = True
        self.num_hidden_layers = transformer_config.num_hidden_layers
        self.last_iso_layer = 0
        if config.dataset.num_iso_layer is not None:
            self.last_iso_layer = config.dataset.num_iso_layer - 1 + 1
        if config.model.mm_encoder_cls is not None:
            mm_encoder_cls = getattr(transformermodel, config.model.mm_encoder_cls)
            model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
            model_config.max_video_len = config.dataset.max_video_len
            model_config.use_seg_emb = config.model.use_seg_emb
            self.mm_encoder = mm_encoder_cls.from_pretrained(config.dataset.bert_name, config=model_config)
        elif config.model.video_encoder_cls is not None and config.model.text_encoder_cls is not None:
            video_encoder_cls = getattr(transformermodel, config.model.video_encoder_cls)
            model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
            model_config.max_video_len = config.dataset.max_video_len
            if hasattr(model_config, 'num_layers'):
                model_config.num_layers = config.model.num_hidden_video_layers
            else:
                model_config.num_hidden_layers = config.model.num_hidden_video_layers
            self.video_encoder = video_encoder_cls.from_pretrained(config.dataset.bert_name, config=model_config)
            text_encoder_cls = getattr(transformermodel, config.model.text_encoder_cls)
            self.text_encoder = text_encoder_cls.from_pretrained(config.dataset.bert_name)
        else:
            raise ValueError('the encoder must be either MM or two backbones.')

    def forward(self, caps, cmasks, vfeats, vmasks, **kwargs):
        raise NotImplementedError('Please derive MMFusion module.')

    def _mm_on_the_fly(self, cmasks, vmasks, attention_mask):
        """helper function for mask, seg_ids and token_type_ids."""
        if attention_mask is None:
            attention_mask = self._mm_attention_mask(cmasks, vmasks)
        """
        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
        | first sequence    | second sequence |
        """
        token_type_ids = torch.cat([torch.zeros((vmasks.size(0), vmasks.size(1) + 2), dtype=torch.long, device=vmasks.device), torch.ones((cmasks.size(0), cmasks.size(1) - 2), dtype=torch.long, device=cmasks.device)], dim=1)
        return attention_mask, token_type_ids

    def _mm_attention_mask(self, cmasks, vmasks):
        assert cmasks.size(0) == vmasks.size(0), '{}, {}, {}, {}'.format(str(cmasks.size()), str(vmasks.size()), str(cmasks.size(0)), str(vmasks.size(0)))
        mm_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)
        if self.last_iso_layer == 0:
            return mm_mask
        else:
            batch_size = cmasks.size(0)
            iso_mask = self._make_iso_mask(batch_size, cmasks, vmasks)
            mm_mask = mm_mask[:, None, :].repeat(1, mm_mask.size(-1), 1)
            iso_mm_masks = []
            iso_mask = iso_mask[:, None, :, :].repeat(1, self.last_iso_layer, 1, 1)
            iso_mm_masks.append(iso_mask)
            if self.last_iso_layer < self.num_hidden_layers:
                mm_mask = mm_mask[:, None, :, :].repeat(1, self.num_hidden_layers - self.last_iso_layer, 1, 1)
                iso_mm_masks.append(mm_mask)
            iso_mm_masks = torch.cat(iso_mm_masks, dim=1)
            return iso_mm_masks

    def _make_iso_mask(self, batch_size, cmasks, vmasks):
        cls_self_mask = torch.cat([torch.ones((batch_size, 1), dtype=torch.bool, device=cmasks.device), torch.zeros((batch_size, cmasks.size(1) + vmasks.size(1) - 1), dtype=torch.bool, device=cmasks.device)], dim=1)
        iso_video_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=cmasks.device), vmasks, cmasks[:, 1:2], torch.zeros((batch_size, cmasks.size(1) - 2), dtype=torch.bool, device=cmasks.device)], dim=1)
        iso_text_mask = torch.cat([torch.zeros((batch_size, 2 + vmasks.size(1)), dtype=torch.bool, device=cmasks.device), cmasks[:, 2:]], dim=1)
        cls_self_mask = cls_self_mask[:, None, :]
        iso_video_mask = iso_video_mask[:, None, :].repeat(1, vmasks.size(1) + 1, 1)
        iso_text_mask = iso_text_mask[:, None, :].repeat(1, cmasks.size(1) - 2, 1)
        return torch.cat([cls_self_mask, iso_video_mask, iso_text_mask], dim=1)

    def _pooling_vt_layer(self, layered_sequence_output, cmasks, vmasks):
        layer_idx = self.last_iso_layer if self.last_iso_layer > 0 else self.num_hidden_layers
        hidden_state = layered_sequence_output[layer_idx]
        batch_size = cmasks.size(0)
        text_offset = vmasks.size(1) + 2
        video_outputs = hidden_state[:, 1:text_offset]
        video_attention_mask = torch.cat([vmasks, torch.ones((batch_size, 1), dtype=torch.bool, device=vmasks.device)], dim=1)
        assert video_outputs.size(1) == video_attention_mask.size(1)
        pooled_video = torch.sum(video_outputs * video_attention_mask.unsqueeze(-1), dim=1) / video_attention_mask.sum(1, keepdim=True)
        text_attention_mask = cmasks[:, 2:]
        text_outputs = hidden_state[:, text_offset:]
        assert text_outputs.size(1) == text_attention_mask.size(1)
        pooled_text = torch.sum(text_outputs * text_attention_mask.unsqueeze(-1), dim=1) / text_attention_mask.sum(1, keepdim=True)
        return pooled_video, pooled_text


class MMFusionMFMMLM(MMFusion):
    """forward function for MFM and MLM."""

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, video_label=None, text_label=None, **kwargs):
        output_hidden_states = False if self.is_train else True
        target_vfeats, non_masked_frame_mask = None, None
        if video_label is not None:
            target_vfeats = vfeats.masked_select(video_label.unsqueeze(-1)).view(-1, vfeats.size(-1))
            vfeats[video_label] = 0.0
            non_masked_frame_mask = vmasks.clone()
            non_masked_frame_mask[video_label] = False
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, masked_frame_labels=video_label, target_video_hidden_states=target_vfeats, non_masked_frame_mask=non_masked_frame_mask, masked_lm_labels=text_label, output_hidden_states=output_hidden_states)
        video_logits, text_logits = outputs[0], outputs[1]
        if self.is_train:
            return {'video_logits': video_logits, 'text_logits': text_logits}
        pooled_video, pooled_text = self._pooling_vt_layer(outputs[2], cmasks, vmasks)
        return {'pooled_video': pooled_video, 'pooled_text': pooled_text}


class BertMFMMLMPredictionHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.bias = nn.Parameter(torch.zeros(config.vocab_size))
        self.decoder.bias = self.bias

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        video_logits, text_logits = None, None
        if video_hidden_states is not None:
            video_hidden_states = self.transform(video_hidden_states)
            non_masked_frame_logits = torch.mm(video_hidden_states, non_masked_frame_hidden_states.transpose(1, 0))
            masked_frame_logits = torch.bmm(video_hidden_states.unsqueeze(1), target_video_hidden_states.unsqueeze(-1)).squeeze(-1)
            video_logits = torch.cat([masked_frame_logits, non_masked_frame_logits], dim=1)
        if text_hidden_states is not None:
            text_hidden_states = self.transform(text_hidden_states)
            text_logits = self.decoder(text_hidden_states)
        return video_logits, text_logits


class MFMMLMHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.predictions = BertMFMMLMPredictionHead(config)

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        video_logits, text_logits = self.predictions(video_hidden_states, target_video_hidden_states, non_masked_frame_hidden_states, text_hidden_states)
        return video_logits, text_logits


class VideoTokenMLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        input_dim = config.input_dim if hasattr(config, 'input_dim') else 512
        self.linear1 = nn.Linear(input_dim, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size)
        self.activation = ACT2FN[config.hidden_act]
        self.linear2 = nn.Linear(config.hidden_size, config.hidden_size)

    def forward(self, hidden_states):
        hidden_states = self.linear1(hidden_states)
        hidden_states = self.activation(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        hidden_states = self.linear2(hidden_states)
        return hidden_states


class BertMTMPredictionHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        non_masked_frame_hidden_states = non_masked_frame_hidden_states.transpose(1, 0)
        video_logits, text_logits = None, None
        if video_hidden_states is not None:
            video_hidden_states = self.transform(video_hidden_states)
            masked_frame_logits = torch.bmm(video_hidden_states.unsqueeze(1), target_video_hidden_states.unsqueeze(-1)).squeeze(-1)
            non_masked_frame_logits = torch.mm(video_hidden_states, non_masked_frame_hidden_states)
            video_on_vocab_logits = self.decoder(video_hidden_states)
            video_logits = torch.cat([masked_frame_logits, non_masked_frame_logits, video_on_vocab_logits], dim=1)
        if text_hidden_states is not None:
            text_hidden_states = self.transform(text_hidden_states)
            text_on_vocab_logits = self.decoder(text_hidden_states)
            text_on_video_logits = torch.mm(text_hidden_states, non_masked_frame_hidden_states)
            text_logits = torch.cat([text_on_vocab_logits, text_on_video_logits], dim=1)
        return video_logits, text_logits


class MTMHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.predictions = BertMTMPredictionHead(config)

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        video_logits, text_logits = self.predictions(video_hidden_states, target_video_hidden_states, non_masked_frame_hidden_states, text_hidden_states)
        return video_logits, text_logits


class MMFusionMTM(MMFusionMFMMLM):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        """
        For reproducibility:
        self.mm_encoder will be initialized then discarded.
        """
        model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
        model_config.max_video_len = config.dataset.max_video_len
        model_config.use_seg_emb = config.model.use_seg_emb
        self.mm_encoder = MMBertForMTM.from_pretrained(config.dataset.bert_name, config=model_config)


class MMFusionShare(MMFusion):
    """A retrival wrapper using mm_encoder as both video/text backbone.
    TODO: move formally.
    """

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, video_label=None, text_label=None, output_hidden_states=False, **kwargs):
        pooled_video = self.forward_video(vfeats, vmasks, caps, cmasks, output_hidden_states)
        pooled_text = self.forward_text(caps, cmasks, output_hidden_states)
        return {'pooled_video': pooled_video, 'pooled_text': pooled_text}

    def forward_video(self, vfeats, vmasks, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = caps[:, :2]
        attention_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:2]], dim=1)
        token_type_ids = torch.zeros((vmasks.size(0), vmasks.size(1) + 2), dtype=torch.long, device=vmasks.device)
        outputs = self.mm_encoder(input_ids=input_ids, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        video_outputs = outputs[0]
        if output_hidden_states:
            return video_outputs
        batch_size = cmasks.size(0)
        video_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=vmasks.device), vmasks, torch.ones((batch_size, 1), dtype=torch.bool, device=vmasks.device)], dim=1)
        assert video_outputs.size(1) == video_attention_mask.size(1)
        video_attention_mask = video_attention_mask.type(video_outputs.dtype) / video_attention_mask.sum(1, keepdim=True)
        pooled_video = torch.bmm(video_outputs.transpose(2, 1), video_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_video

    def forward_text(self, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = torch.cat([caps[:, :1], caps[:, 2:]], dim=1)
        attention_mask = torch.cat([cmasks[:, :1], cmasks[:, 2:]], dim=1)
        token_type_ids = torch.cat([torch.zeros((cmasks.size(0), 1), dtype=torch.long, device=cmasks.device), torch.ones((cmasks.size(0), cmasks.size(1) - 2), dtype=torch.long, device=cmasks.device)], dim=1)
        outputs = self.mm_encoder(input_ids=input_ids, input_video_embeds=None, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        text_outputs = outputs[0]
        if output_hidden_states:
            return text_outputs
        batch_size = caps.size(0)
        text_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=cmasks.device), cmasks[:, 2:]], dim=1)
        assert text_outputs.size(1) == text_attention_mask.size(1)
        text_attention_mask = text_attention_mask.type(text_outputs.dtype) / text_attention_mask.sum(1, keepdim=True)
        pooled_text = torch.bmm(text_outputs.transpose(2, 1), text_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_text


class MMFusionSeparate(MMFusionShare):

    def forward_video(self, vfeats, vmasks, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = caps[:, :2]
        attention_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:2]], dim=1)
        token_type_ids = torch.zeros((vmasks.size(0), vmasks.size(1) + 2), dtype=torch.long, device=vmasks.device)
        outputs = self.video_encoder(input_ids=input_ids, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        video_outputs = outputs[0]
        if output_hidden_states:
            return video_outputs
        batch_size = cmasks.size(0)
        video_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=vmasks.device), vmasks, torch.ones((batch_size, 1), dtype=torch.bool, device=vmasks.device)], dim=1)
        assert video_outputs.size(1) == video_attention_mask.size(1)
        video_attention_mask = video_attention_mask.type(video_outputs.dtype) / video_attention_mask.sum(1, keepdim=True)
        pooled_video = torch.bmm(video_outputs.transpose(2, 1), video_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_video

    def forward_text(self, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = torch.cat([caps[:, :1], caps[:, 2:]], dim=1)
        attention_mask = torch.cat([cmasks[:, :1], cmasks[:, 2:]], dim=1)
        token_type_ids = torch.zeros((cmasks.size(0), cmasks.size(1) - 1), dtype=torch.long, device=cmasks.device)
        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        text_outputs = outputs[0]
        if output_hidden_states:
            return text_outputs
        batch_size = caps.size(0)
        text_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=cmasks.device), cmasks[:, 2:]], dim=1)
        assert text_outputs.size(1) == text_attention_mask.size(1)
        text_attention_mask = text_attention_mask.type(text_outputs.dtype) / text_attention_mask.sum(1, keepdim=True)
        pooled_text = torch.bmm(text_outputs.transpose(2, 1), text_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_text


class MMFusionJoint(MMFusion):
    """fine-tuning wrapper for retrival task."""

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, video_label=None, text_label=None, **kwargs):
        output_hidden_states = True
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        separate_forward_split = None if self.is_train else vmasks.size(1) + 2
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states, separate_forward_split=separate_forward_split)
        pooled_video, pooled_text = self._pooling_vt_layer(outputs[2], cmasks, vmasks)
        return {'pooled_video': pooled_video, 'pooled_text': pooled_text}


class MMFusionActionSegmentation(MMFusion):
    """Fine-tuning wrapper for action segmentation.
    TODO: rename this for VLM.
    """

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, **kwargs):
        caps = caps.view(-1, caps.size(-1))
        cmasks = cmasks.view(-1, cmasks.size(-1))
        vfeats = vfeats.view(-1, vfeats.size(2), vfeats.size(3))
        vmasks = vmasks.view(-1, vmasks.size(-1))
        attention_mask = attention_mask.view(-1, attention_mask.size(2), attention_mask.size(3)) if attention_mask is not None else None
        output_hidden_states = True
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        logits = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states)
        return {'logits': logits[0][:, 1:vmasks.size(1) + 1]}


class MMFusionActionLocalization(MMFusion):
    """fine-tuning model for retrival task."""

    def __init__(self, config, **kwargs):
        super().__init__(config)
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, **kwargs):
        caps = caps.squeeze(0)
        cmasks = cmasks.squeeze(0)
        vfeats = vfeats.squeeze(0)
        vmasks = vmasks.squeeze(0)
        attention_mask = attention_mask.squeeze(0) if attention_mask is not None else None
        output_hidden_states = True
        dummy_vfeats = torch.zeros((caps.size(0), 1, vfeats.size(-1)), device=vfeats.device, dtype=vfeats.dtype)
        dummy_vmasks = torch.ones((caps.size(0), 1), dtype=torch.bool, device=vfeats.device)
        dummy_caps = torch.LongTensor([[self.cls_token_id, self.sep_token_id, self.pad_token_id, self.sep_token_id]]).repeat(vfeats.size(0), 1)
        dummy_cmasks = torch.BoolTensor([[0, 1, 0, 1]]).repeat(vfeats.size(0), 1)
        attention_mask, token_type_ids = self._mm_on_the_fly(dummy_cmasks, vmasks, None)
        outputs = self.mm_encoder(input_ids=dummy_caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states)
        layer_idx = self.last_iso_layer if self.last_iso_layer > 0 else self.num_hidden_layers
        video_seq = outputs[2][layer_idx][:, 1:vmasks.size(1) + 1].masked_select(vmasks.unsqueeze(-1)).view(-1, self.hidden_size)
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, dummy_vmasks, None)
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=dummy_vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states)
        _, pooled_text = self._pooling_vt_layer(outputs[2], cmasks, dummy_vmasks)
        logits = torch.mm(video_seq, pooled_text.transpose(1, 0))
        return {'logits': logits}


class MMFusionSeparateActionSegmentation(MMFusionSeparate):
    """Fine-tuning wrapper for action segmentation."""

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, **kwargs):
        caps = caps.view(-1, caps.size(-1))
        cmasks = cmasks.view(-1, cmasks.size(-1))
        vfeats = vfeats.view(-1, vfeats.size(2), vfeats.size(3))
        vmasks = vmasks.view(-1, vmasks.size(-1))
        logits = self.forward_video(vfeats, vmasks, caps, cmasks, output_hidden_states=True)
        return {'logits': logits[:, 1:vmasks.size(1) + 1]}


class MMFusionSeparateActionLocalization(MMFusionSeparate):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id

    def forward(self, caps, cmasks, vfeats, vmasks, **kwargs):
        caps = caps.squeeze(0)
        cmasks = cmasks.squeeze(0)
        vfeats = vfeats.squeeze(0)
        vmasks = vmasks.squeeze(0)
        dummy_caps = torch.LongTensor([[self.cls_token_id, self.sep_token_id, self.pad_token_id, self.sep_token_id]]).repeat(vfeats.size(0), 1)
        dummy_cmasks = torch.BoolTensor([[0, 1, 0, 1]]).repeat(vfeats.size(0), 1)
        outputs = self.forward_video(vfeats, vmasks, dummy_caps, dummy_cmasks, output_hidden_states=True)
        video_seq = outputs[:, 1:vmasks.size(1) + 1].masked_select(vmasks.unsqueeze(-1)).view(-1, self.hidden_size)
        pooled_text = self.forward_text(caps, cmasks, output_hidden_states=False)
        logits = torch.mm(video_seq, pooled_text.transpose(1, 0))
        return {'logits': logits}


class MMFusionShareActionLocalization(MMFusionShare):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id

    def forward(self, caps, cmasks, vfeats, vmasks, **kwargs):
        caps = caps.squeeze(0)
        cmasks = cmasks.squeeze(0)
        vfeats = vfeats.squeeze(0)
        vmasks = vmasks.squeeze(0)
        dummy_caps = torch.LongTensor([[self.cls_token_id, self.sep_token_id, self.pad_token_id, self.sep_token_id]]).repeat(vfeats.size(0), 1)
        dummy_cmasks = torch.BoolTensor([[0, 1, 0, 1]]).repeat(vfeats.size(0), 1)
        outputs = self.forward_video(vfeats, vmasks, dummy_caps, dummy_cmasks, output_hidden_states=True)
        video_seq = outputs[:, 1:vmasks.size(1) + 1].masked_select(vmasks.unsqueeze(-1)).view(-1, self.hidden_size)
        pooled_text = self.forward_text(caps, cmasks, output_hidden_states=False)
        logits = torch.mm(video_seq, pooled_text.transpose(1, 0))
        return {'logits': logits}


class MMFusionNLG(MMFusion):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        if config.model.max_decode_length is not None:
            self.max_length = min(config.model.max_decode_length, config.dataset.max_len - config.dataset.max_video_len - 3)
        else:
            self.max_length = config.dataset.max_len - config.dataset.max_video_len - 3
        self.gen_param = config.gen_param if config.gen_param is not None else {}

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask, video_label=None, text_label=None, **kwargs):
        """use pre-trained LM header for generation."""
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, masked_lm_labels=text_label)
        return {'logits': outputs[0]}

    @torch.no_grad()
    def generate(self, caps, cmasks, vfeats, vmasks, attention_mask=None, bos_token_id=None, eos_token_id=None, **kwargs):
        assert caps.size(1) == 3
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        output = self.mm_encoder.generate(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, bos_token_id=bos_token_id, eos_token_id=eos_token_id, max_length=self.max_length, **self.gen_param)
        return output


class AlignHead(nn.Module):
    """this will load pre-trained weights for NSP, which is desirable."""

    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, dropout_pooled_output):
        logits = self.seq_relationship(dropout_pooled_output)
        return logits


def get_mask_from_lengths(lengths):
    max_len = torch.max(lengths).item()
    ids = torch.arange(0, max_len, out=torch.LongTensor(max_len))
    mask = ids < lengths.unsqueeze(1)
    return mask


class GlobalAvgPool(torch.nn.Module):

    def __init__(self):
        super(GlobalAvgPool, self).__init__()

    def forward(self, x, lengths=None):
        """Average pooling across time steps (dim=1) with optionally lengths.
        Args:
            x: torch.Tensor of shape (N, T, ...)
            lengths: None or torch.Tensor of shape (N,)
            dim: dimension to pool
        """
        if lengths is None:
            return x.mean(dim=1, keepdim=False)
        else:
            mask = get_mask_from_lengths(lengths).type(x.type())
            mask_shape = list(mask.size()) + [(1) for _ in range(x.ndimension() - 2)]
            mask = mask.reshape(*mask_shape)
            numer = (x * mask).sum(dim=1, keepdim=False)
            denom = mask.sum(dim=1, keepdim=False)
            return numer / denom


class AdaptiveMask(nn.Module):
    """Soft masking function for adaptive size.
    It masks out the last K values of an input. The masking value
    goes from 1 to 0 gradually, so K can be learned with
    back-propagation.
    Args:
        max_size: maximum size (i.e. input dimension)
        ramp_size: size of the ramp going from 0 to 1
        init_val: initial size proportion not to be masked out
        shape: learn multiple sizes independent of each other
    """

    def __init__(self, max_size, ramp_size, init_val=0, shape=(1,)):
        nn.Module.__init__(self)
        self._max_size = max_size
        self._ramp_size = ramp_size
        self.current_val = nn.Parameter(torch.zeros(*shape) + init_val)
        mask_template = torch.linspace(1 - max_size, 0, steps=max_size)
        self.register_buffer('mask_template', mask_template)

    def forward(self, x):
        mask = self.mask_template.float() + self.current_val.float() * self._max_size
        mask = mask / self._ramp_size + 1
        mask = mask.clamp(0, 1)
        if x.size(-1) < self._max_size:
            mask = mask.narrow(-1, self._max_size - x.size(-1), x.size(-1))
        x = (x * mask).type_as(x)
        return x

    def get_current_max_size(self, include_ramp=True):
        current_size = math.ceil(self.current_val.max().item() * self._max_size)
        if include_ramp:
            current_size += self._ramp_size
        current_size = max(0, min(self._max_size, current_size))
        return current_size

    def get_current_avg_size(self, include_ramp=True):
        current_size = math.ceil(self.current_val.float().mean().item() * self._max_size)
        if include_ramp:
            current_size += self._ramp_size
        current_size = max(0, min(self._max_size, current_size))
        return current_size

    def clamp_param(self):
        """this need to be called after each update"""
        self.current_val.data.clamp_(0, 1)


class AdaptiveSpan(nn.Module):
    """Adaptive attention span for Transformerself.
    This module learns an attention span length from data for each
    self-attention head.
    Args:
        attn_span: maximum attention span
        adapt_span_loss: loss coefficient for the span length
        adapt_span_ramp: length of the masking ramp
        adapt_span_init: initial size ratio
        adapt_span_cache: adapt cache size to reduce memory usage
    """

    def __init__(self, attn_span, adapt_span_ramp, adapt_span_init, n_head, adapt_span_layer, **kargs):
        nn.Module.__init__(self)
        self._max_span = attn_span
        self._n_head = n_head
        self._adapt_span_layer = adapt_span_layer
        if self._adapt_span_layer:
            self._mask = AdaptiveMask(max_size=self._max_span, ramp_size=adapt_span_ramp, init_val=adapt_span_init)
        else:
            self._mask = AdaptiveMask(max_size=self._max_span, ramp_size=adapt_span_ramp, init_val=adapt_span_init, shape=(n_head, 1, 1))

    def forward(self, attn, normalize=True):
        """mask attention with the right span"""
        self.clamp_param()
        if self._adapt_span_layer:
            attn = self._mask(attn)
        else:
            B = attn.size(0)
            M = attn.size(1)
            attn = attn.reshape(B // self._n_head, self._n_head, M, -1)
            attn = self._mask(attn)
            attn = attn.view(B, M, -1)
        return attn

    def get_trim_len(self):
        """how much of memory can be trimmed to reduce computation"""
        L = self._max_span
        trim_len = min(L - 1, L - self._mask.get_current_max_size())
        trim_len = math.floor(trim_len / 64) * 64
        return trim_len

    def trim_memory(self, query, key, value, key_pe):
        """trim out unnecessary memory beforehand to reduce computation"""
        trim_len = self.get_trim_len()
        cache_size = key.size(1) - query.size(1)
        trim_len_cache = trim_len - (self._max_span - cache_size)
        if trim_len_cache > 0:
            key = key[:, trim_len_cache:, :]
            value = value[:, trim_len_cache:, :]
        elif trim_len_cache < 0:
            key = F.pad(key, [0, 0, -trim_len_cache, 0])
            value = F.pad(value, [0, 0, -trim_len_cache, 0])
        if trim_len > 0:
            if key_pe is not None:
                key_pe = key_pe[:, :, trim_len:]
        return key, value, key_pe

    def get_cache_size(self):
        """determine how long the cache should be"""
        trim_len = self.get_trim_len()
        return min(self._max_span, self._max_span - trim_len + 64)

    def get_loss(self):
        """a loss term for regularizing the span length"""
        return self._max_span * self._mask.current_val.float().mean()

    def get_current_max_span(self):
        return self._mask.get_current_max_size()

    def get_current_avg_span(self):
        return self._mask.get_current_avg_size()

    def clamp_param(self):
        self._mask.clamp_param()


def _skew(X, pad_value):
    """shift every row 1 step to right"""
    B, M, L = X.size()
    X = F.pad(X, (0, M + 1), value=pad_value)
    X = X.view(B, -1)
    X = X[:, :-M]
    X = X.view(B, M, M + L)
    return X


def _unskew(X):
    """reverse _skew operation"""
    B, M, L = X.size()
    L -= M
    X = X.view(B, -1)
    X = F.pad(X, (0, M))
    X = X.view(B, M, M + L + 1)
    X = X[:, :, :L]
    return X


class SeqAttention(nn.Module):
    """Sequential self-attention layer.
    Each token will attend to its previous fixed number of steps.
    Note that attention doesn't include the current step itself.
    """

    def __init__(self, d_model, n_head, attn_span, dropout, adapt_span_layer, **kargs):
        nn.Module.__init__(self)
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
        self.attn_span = attn_span
        self.adaptive_span = AdaptiveSpan(attn_span=attn_span, n_head=n_head, adapt_span_layer=adapt_span_layer, **kargs)

    def forward(self, query, key, value, key_pe):
        key, value, key_pe = self.adaptive_span.trim_memory(query, key, value, key_pe)
        attn_cont = torch.matmul(query, key.transpose(-1, -2))
        attn_cont = _unskew(attn_cont)
        attn_pos = torch.matmul(query, key_pe)
        attn = attn_cont + attn_pos
        attn = attn / math.sqrt(self.d_model)
        attn = F.softmax(attn.float(), dim=-1).type_as(attn)
        attn = self.adaptive_span(attn)
        attn = self.dropout(attn)
        attn_cont = _skew(attn, 0)
        out = torch.matmul(attn_cont, value)
        return out

    def get_cache_size(self):
        return self.adaptive_span.get_cache_size()


class MultiHeadSeqAttention(nn.Module):

    def __init__(self, d_model, n_head, **kargs):
        nn.Module.__init__(self)
        assert d_model % n_head == 0
        self.n_head = n_head
        self.head_dim = d_model // n_head
        self.attn = SeqAttention(d_model=self.head_dim, n_head=n_head, **kargs)
        self.proj_query = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_query.weight)
        self.proj_out = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_out.weight)
        self.proj_val = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_val.weight)
        self.proj_key = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_key.weight)

    def head_reshape(self, x):
        K = self.n_head
        D = self.head_dim
        x = x.view(x.size()[:-1] + (K, D))
        x = x.transpose(1, 2).contiguous()
        x = x.view(-1, x.size(-2), x.size(-1))
        return x

    def forward(self, query, key, value, key_pe):
        B = query.size(0)
        K = self.n_head
        D = self.head_dim
        M = query.size(1)
        query = self.proj_query(query)
        query = self.head_reshape(query)
        value = self.proj_val(value)
        value = self.head_reshape(value)
        key = self.proj_key(key)
        key = self.head_reshape(key)
        out = self.attn(query, key, value, key_pe)
        out = out.view(B, K, M, D)
        out = out.transpose(1, 2).contiguous()
        out = out.view(B, M, -1)
        out = self.proj_out(out)
        return out


class FeedForwardLayer(nn.Module):

    def __init__(self, d_model, d_inner, dropout, **kargs):
        nn.Module.__init__(self)
        self.fc1 = nn.Linear(d_model, d_inner)
        self.fc2 = nn.Linear(d_inner, d_model)
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        self.dropout = nn.Dropout(dropout)

    def forward(self, h):
        h1 = F.relu(self.fc1(h))
        h1 = self.dropout(h1)
        h2 = self.fc2(h1)
        return h2


def LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, export=False):
    if torch.jit.is_scripting() or torch.jit.is_tracing():
        export = True
    if not export and torch.cuda.is_available() and has_fused_layernorm:
        return FusedLayerNorm(normalized_shape, eps, elementwise_affine)
    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)


class TransformerSeqLayer(nn.Module):

    def __init__(self, d_model, **kargs):
        nn.Module.__init__(self)
        self.attn = MultiHeadSeqAttention(d_model=d_model, **kargs)
        self.norm1 = LayerNorm(d_model)
        self.ff = FeedForwardLayer(d_model=d_model, **kargs)
        self.norm2 = LayerNorm(d_model)

    def forward(self, h, h_cache, key_pe):
        h_all = torch.cat([h_cache, h], dim=1)
        attn_out = self.attn(h, h_all, h_all, key_pe)
        h = self.norm1(h + attn_out)
        if self.ff is not None:
            ff_out = self.ff(h)
            out = self.norm2(h + ff_out)
        else:
            out = h
        return out

    def get_cache_size(self):
        return self.attn.attn.get_cache_size()


class TransformerSeq(nn.Module):

    def __init__(self, vocab_size, d_model, n_head, n_layer, attn_span, emb_dropout, aux_loss_scaler, adapt_span_layer, **kargs):
        nn.Module.__init__(self)
        self.in_emb = nn.Embedding(vocab_size, d_model)
        nn.init.normal_(self.in_emb.weight, mean=0, std=d_model ** -0.5)
        self.out_emb = nn.Linear(d_model, vocab_size)
        self.aux_loss_scaler = aux_loss_scaler
        if emb_dropout > 0:
            self.emb_dropout = nn.Dropout(emb_dropout)
        else:
            self.emb_dropout = None
        self.key_pe = nn.Parameter(torch.randn(1, d_model // n_head, attn_span))
        self.layers = nn.ModuleList()
        self.layers.extend(TransformerSeqLayer(d_model=d_model, n_head=n_head, attn_span=attn_span, adapt_span_layer=adapt_span_layer, **kargs) for _ in range(n_layer))

    def forward(self, x, h_cache, target=None):
        block_size = x.size(1)
        h = self.in_emb(x)
        if self.emb_dropout is not None:
            h = self.emb_dropout(h)
        h_cache_next = []
        for l, layer in enumerate(self.layers):
            cache_size = layer.attn.attn.get_cache_size()
            if cache_size > block_size:
                h_cache_next_l = torch.cat([h_cache[l][:, -cache_size + block_size:, :], h], dim=1).detach()
            else:
                h_cache_next_l = h[:, -cache_size:, :].detach()
            h_cache_next.append(h_cache_next_l)
            h = layer(h, h_cache[l], self.key_pe)
        if self.emb_dropout is not None:
            h = self.emb_dropout(h)
        out = F.log_softmax(self.out_emb(h).float(), dim=-1).type_as(h)
        dummy_loss = None
        return out, h_cache_next, dummy_loss

    def get_aux_loss(self):
        loss = 0.0
        for layer in self.layers:
            loss += layer.attn.attn.adaptive_span.get_loss()
        return self.aux_loss_scaler * loss

    def get_current_max_span(self):
        max_span = 0.0
        for layer in self.layers:
            max_span = max(max_span, layer.attn.attn.adaptive_span.get_current_max_span())
        return max_span

    def get_current_avg_span(self):
        avg_span = 0.0
        for layer in self.layers:
            avg_span += layer.attn.attn.adaptive_span.get_current_avg_span()
        return avg_span / len(self.layers)


class HeadSelectionLoss(_Loss):

    def __init__(self, args):
        super().__init__()
        self.args = args
        self.kl_weight = getattr(args, 'kl_weight', 0.0)

    def forward(self, head_samples, sample_sizes, prior=0.5, eps=1e-07):
        """
        head_scores: (num_tasks, num_layers, num_heads)
        sample_sizes: (num_tasks, )
        """
        kl_loss = (head_samples * (torch.log(head_samples + eps) - math.log(prior))).sum(-1).sum(-1)
        kl_loss /= torch.numel(head_samples) / head_samples.size(0)
        kl_loss = self.kl_weight * torch.matmul(kl_loss, sample_sizes)
        return kl_loss


class AttnHeadSelector(nn.Module):
    """
    Latent variable modeling of attention head selection
    """

    def __init__(self, num_tasks, num_layers, total_num_heads, num_heads, select_strategy='group', head_select_temp=5.0):
        super(AttnHeadSelector, self).__init__()
        self.num_tasks = num_tasks
        self.num_layers = num_layers
        self.total_num_heads = total_num_heads
        self.num_heads = num_heads
        self.select_strategy = select_strategy
        self.temp = head_select_temp
        self.head_logits = torch.nn.Parameter(torch.Tensor(self.num_tasks, self.num_layers, total_num_heads), requires_grad=True)
        nn.init.uniform_(self.head_logits, a=math.log(0.01), b=math.log(1.0))

    def gumbel_sample(self, logits, tau=1.0):
        gumbels1 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels2 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels1 = (logits + gumbels1 - gumbels2) / tau
        y_soft = gumbels1.sigmoid()
        return y_soft

    def subset_select(self, y_soft, topk, dim=-1):
        top_values, top_inds = torch.topk(y_soft, k=topk, dim=dim)
        top_ret = 1.0 - top_values.detach() + top_values
        return top_inds.detach(), top_ret

    def group_selet(self, y_soft, topk, dim=-1):
        top_values, top_inds = torch.max(y_soft.view(self.num_tasks, self.num_layers, -1, topk), dim=2)
        top_inds = top_inds * topk + torch.arange(topk, device=top_inds.device).unsqueeze(0).unsqueeze(1)
        top_ret = 1.0 - top_values.detach() + top_values
        return top_inds.detach(), top_ret

    def head_select(self, task_ids=None):
        self.head_samples = self.gumbel_sample(self.head_logits, tau=self.temp)
        if self.select_strategy == 'subset':
            self.subset_heads, self.subset_weights = self.subset_select(self.head_samples, topk=self.num_heads)
        elif self.select_strategy == 'group':
            self.subset_heads, self.subset_weights = self.group_selet(self.head_samples, topk=self.num_heads)
        else:
            raise ValueError('{} is not supported'.format(self.select_strategy))
        self.batch_subset = self.subset_heads[task_ids, :, :]
        self.batch_weights = self.subset_weights[task_ids, :, :]

    def forward(self, layer_idx):
        assert layer_idx is not None
        batch_subset = self.batch_subset[:, layer_idx, :]
        batch_weights = self.batch_weights[:, layer_idx, :]
        return batch_subset, batch_weights


class PatchEmbed(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        if isinstance(img_size, int):
            img_size = img_size, img_size
        if isinstance(patch_size, int):
            patch_size = patch_size, patch_size
        num_patches = img_size[1] // patch_size[1] * (img_size[0] // patch_size[0])
        self.patch_shape = img_size[0] // patch_size[0], img_size[1] // patch_size[1]
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        self.conv = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.conv(x).flatten(2).transpose(1, 2)
        return x


class LinearNorm(torch.nn.Module):

    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):
        super(LinearNorm, self).__init__()
        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)
        torch.nn.init.xavier_uniform_(self.linear_layer.weight, gain=torch.nn.init.calculate_gain(w_init_gain))

    def forward(self, x):
        return self.linear_layer(x)


class ConvNorm(torch.nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=None, dilation=1, bias=True, w_init_gain='linear'):
        super(ConvNorm, self).__init__()
        if padding is None:
            assert kernel_size % 2 == 1
            padding = int(dilation * (kernel_size - 1) / 2)
        self.conv = torch.nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)
        torch.nn.init.xavier_uniform_(self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))

    def forward(self, signal):
        conv_signal = self.conv(signal)
        return conv_signal


class LocationLayer(nn.Module):

    def __init__(self, attention_n_filters, attention_kernel_size, attention_dim):
        super(LocationLayer, self).__init__()
        padding = int((attention_kernel_size - 1) / 2)
        self.location_conv = ConvNorm(2, attention_n_filters, kernel_size=attention_kernel_size, padding=padding, bias=False, stride=1, dilation=1)
        self.location_dense = LinearNorm(attention_n_filters, attention_dim, bias=False, w_init_gain='tanh')

    def forward(self, attention_weights_cat):
        processed_attention = self.location_conv(attention_weights_cat)
        processed_attention = processed_attention.transpose(1, 2)
        processed_attention = self.location_dense(processed_attention)
        return processed_attention


class Attention(nn.Module):

    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size):
        super(Attention, self).__init__()
        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim, bias=False, w_init_gain='tanh')
        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False, w_init_gain='tanh')
        self.v = LinearNorm(attention_dim, 1, bias=False)
        self.location_layer = LocationLayer(attention_location_n_filters, attention_location_kernel_size, attention_dim)
        self.score_mask_value = -float('inf')

    def get_alignment_energies(self, query, processed_memory, attention_weights_cat):
        """
        PARAMS
        ------
        query: decoder output (batch, n_mel_channels * n_frames_per_step)
        processed_memory: processed encoder outputs (B, T_in, attention_dim)
        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)

        RETURNS
        -------
        alignment (batch, max_time)
        """
        processed_query = self.query_layer(query.unsqueeze(1))
        processed_attention_weights = self.location_layer(attention_weights_cat)
        energies = self.v(torch.tanh(processed_query + processed_attention_weights + processed_memory))
        energies = energies.squeeze(-1)
        return energies

    def forward(self, attention_hidden_state, memory, processed_memory, attention_weights_cat, mask):
        """
        PARAMS
        ------
        attention_hidden_state: attention rnn last output
        memory: encoder outputs
        processed_memory: processed encoder outputs
        attention_weights_cat: previous and cummulative attention weights
        mask: binary mask for padded data
        """
        alignment = self.get_alignment_energies(attention_hidden_state, processed_memory, attention_weights_cat)
        if mask is not None:
            alignment.data.masked_fill_(mask, self.score_mask_value)
        attention_weights = F.softmax(alignment, dim=1)
        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)
        attention_context = attention_context.squeeze(1)
        return attention_context, attention_weights


class RelativePositionBias(nn.Module):

    def __init__(self, window_size, num_heads):
        super().__init__()
        self.window_size = window_size
        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3
        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))
        coords_h = torch.arange(window_size[0])
        coords_w = torch.arange(window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += window_size[0] - 1
        relative_coords[:, :, 1] += window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * window_size[1] - 1
        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)
        relative_position_index[1:, 1:] = relative_coords.sum(-1)
        relative_position_index[0, 0:] = self.num_relative_distance - 3
        relative_position_index[0:, 0] = self.num_relative_distance - 2
        relative_position_index[0, 0] = self.num_relative_distance - 1
        self.register_buffer('relative_position_index', relative_position_index)

    def forward(self):
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)
        return relative_position_bias.permute(2, 0, 1).contiguous()


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.drop_prob == 0.0 or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        output = x.div(keep_prob) * random_tensor
        return output

    def extra_repr(self) ->str:
        return 'p={}'.format(self.drop_prob)


class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0, drop_path=0.0, init_values=None, window_size=None):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(nn.Linear(dim, mlp_hidden_dim), nn.GELU(), nn.Linear(mlp_hidden_dim, dim), nn.Dropout(drop))
        if init_values > 0:
            self.gamma_1 = nn.Parameter(init_values * torch.ones(dim), requires_grad=True)
            self.gamma_2 = nn.Parameter(init_values * torch.ones(dim), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward(self, x, rel_pos_bias=None):
        None
        if self.gamma_1 is None:
            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))
            fc_feature = self.drop_path(self.mlp(self.norm2(x)))
            x = x + fc_feature
        else:
            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))
            fc_feature = self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
            x = x + fc_feature
        return x, fc_feature


def deprecation_warning(message, stacklevel=3):
    warnings.warn(message, stacklevel=stacklevel)


def gelu(x: 'torch.Tensor') ->torch.Tensor:
    return torch.nn.functional.gelu(x.float()).type_as(x)


def gelu_accurate(x):
    if not hasattr(gelu_accurate, '_a'):
        gelu_accurate._a = math.sqrt(2 / math.pi)
    return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))


def relu_squared(x: 'torch.Tensor'):
    return F.relu(x).pow(2)


def get_activation_fn(activation: 'str') ->Callable:
    """Returns the activation function corresponding to `activation`"""
    if activation == 'relu':
        return F.relu
    elif activation == 'relu_squared':
        return relu_squared
    elif activation == 'gelu':
        return gelu
    elif activation == 'gelu_fast':
        deprecation_warning('--activation-fn=gelu_fast has been renamed to gelu_accurate')
        return gelu_accurate
    elif activation == 'gelu_accurate':
        return gelu_accurate
    elif activation == 'tanh':
        return torch.tanh
    elif activation == 'linear':
        return lambda x: x
    elif activation == 'swish':
        return torch.nn.SiLU
    else:
        raise RuntimeError('--activation-fn {} not supported'.format(activation))


class ConvolutionModule(torch.nn.Module):
    """Convolution block used in the conformer block"""

    def __init__(self, embed_dim, channels, depthwise_kernel_size, dropout, activation_fn='swish', bias=False, export=False):
        """
        Args:
            embed_dim: Embedding dimension
            channels: Number of channels in depthwise conv layers
            depthwise_kernel_size: Depthwise conv layer kernel size
            dropout: dropout value
            activation_fn: Activation function to use after depthwise convolution kernel
            bias: If bias should be added to conv layers
            export: If layernorm should be exported to jit
        """
        super(ConvolutionModule, self).__init__()
        assert (depthwise_kernel_size - 1) % 2 == 0, "kernel_size should be a odd number for 'SAME' padding"
        self.layer_norm = LayerNorm(embed_dim, export=export)
        self.pointwise_conv1 = torch.nn.Conv1d(embed_dim, 2 * channels, kernel_size=1, stride=1, padding=0, bias=bias)
        self.glu = torch.nn.GLU(dim=1)
        self.depthwise_conv = torch.nn.Conv1d(channels, channels, depthwise_kernel_size, stride=1, padding=(depthwise_kernel_size - 1) // 2, groups=channels, bias=bias)
        self.batch_norm = torch.nn.BatchNorm1d(channels)
        self.activation = get_activation_fn(activation_fn)(channels)
        self.pointwise_conv2 = torch.nn.Conv1d(channels, embed_dim, kernel_size=1, stride=1, padding=0, bias=bias)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: Input of shape B X T X C
        Returns:
          Tensor of shape B X T X C
        """
        x = self.layer_norm(x)
        x = x.transpose(1, 2)
        x = self.pointwise_conv1(x)
        x = self.glu(x)
        x = self.depthwise_conv(x)
        x = self.batch_norm(x)
        x = self.activation(x)
        x = self.pointwise_conv2(x)
        x = self.dropout(x)
        return x.transpose(1, 2)


class ESPNETMultiHeadedAttention(nn.Module):
    """Multi-Head Attention layer.
    Args:
        n_head: The number of heads.
        n_feat: The number of features.
        dropout: Dropout rate.
    """

    def __init__(self, n_feat, n_head, dropout):
        """Construct an MultiHeadedAttention object."""
        super(ESPNETMultiHeadedAttention, self).__init__()
        assert n_feat % n_head == 0
        self.d_k = n_feat // n_head
        self.h = n_head
        self.linear_q = nn.Linear(n_feat, n_feat)
        self.linear_k = nn.Linear(n_feat, n_feat)
        self.linear_v = nn.Linear(n_feat, n_feat)
        self.linear_out = nn.Linear(n_feat, n_feat)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward_qkv(self, query, key, value, **kwargs):
        """Transform query, key and value.
        Args:
            query: Query tensor  B X T1 X C
            key: Key tensor B X T2 X C
            value: Value tensor  B X T2 X C
        Returns:
            torch.Tensor: Transformed query tensor  B X n_head X T1 X d_k
            torch.Tensor: Transformed key tensor B X n_head X T2 X d_k
            torch.Tensor: Transformed value tensor  B X n_head X T2 X d_k
        """
        n_batch = query.size(0)
        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        return q, k, v

    def forward_attention(self, value, scores, mask):
        """Compute attention context vector.
        Args:
            value: Transformed value B X n_head X T2 X d_k.
            scores: Attention score  B X n_head X T1 X T2
            mask: Mask  T2 X B
        Returns:
            torch.Tensor: Transformed value  B X T1 X d_model
                weighted by the attention score  B X T1 X T2
        """
        n_batch = value.size(0)
        if mask is not None:
            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(2), float('-inf'))
            self.attn = torch.softmax(scores, dim=-1)
        else:
            self.attn = torch.softmax(scores, dim=-1)
        p_attn = self.dropout(self.attn)
        x = torch.matmul(p_attn, value)
        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)
        return self.linear_out(x)

    def forward(self, query, key, value, key_padding_mask=None, **kwargs):
        """Compute scaled dot product attention.
        Args:
            query (torch.Tensor): Query tensor T X B X C
            key (torch.Tensor): Key tensor T X B X C
            value (torch.Tensor): Value tensor T X B X C
            mask (torch.Tensor): Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X D.
        """
        query = query.transpose(0, 1)
        key = key.transpose(0, 1)
        value = value.transpose(0, 1)
        q, k, v = self.forward_qkv(query, key, value)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        scores = self.forward_attention(v, scores, key_padding_mask)
        scores = scores.transpose(0, 1)
        return scores, None


class FeedForwardModule(torch.nn.Module):
    """Positionwise feed forward layer used in conformer"""

    def __init__(self, input_feat, hidden_units, dropout1, dropout2, activation_fn='swish', bias=True):
        """
        Args:
            input_feat: Input feature dimension
            hidden_units: Hidden unit dimension
            dropout1: dropout value for layer1
            dropout2: dropout value for layer2
            activation_fn: Name of activation function
            bias: If linear layers should have bias
        """
        super(FeedForwardModule, self).__init__()
        self.layer_norm = LayerNorm(input_feat)
        self.w_1 = torch.nn.Linear(input_feat, hidden_units, bias=bias)
        self.w_2 = torch.nn.Linear(hidden_units, input_feat, bias=bias)
        self.dropout1 = torch.nn.Dropout(dropout1)
        self.dropout2 = torch.nn.Dropout(dropout2)
        self.activation = get_activation_fn(activation_fn)(hidden_units)

    def forward(self, x):
        """
        Args:
            x: Input Tensor of shape  T X B X C
        Returns:
            Tensor of shape T X B X C
        """
        x = self.layer_norm(x)
        x = self.w_1(x)
        x = self.activation(x)
        x = self.dropout1(x)
        x = self.w_2(x)
        return self.dropout2(x)


logger = logging.getLogger('fairseq_cli.validate')


class FairseqDropout(nn.Module):

    def __init__(self, p, module_name=None):
        super().__init__()
        self.p = p
        self.module_name = module_name
        self.apply_during_inference = False

    def forward(self, x, inplace: 'bool'=False):
        if self.p > 0 and (self.training or self.apply_during_inference):
            return F.dropout(x, p=self.p, training=True, inplace=inplace)
        else:
            return x

    def make_generation_fast_(self, name: 'str', retain_dropout: 'bool'=False, retain_dropout_modules: 'Optional[List[str]]'=None, **kwargs):
        if retain_dropout:
            if retain_dropout_modules is not None and self.module_name is None:
                logger.warning('Cannot enable dropout during inference for module {} because module_name was not set'.format(name))
            elif retain_dropout_modules is None or self.module_name in retain_dropout_modules:
                logger.info('Enabling dropout during inference for module: {}'.format(name))
                self.apply_during_inference = True
            else:
                logger.info('Disabling dropout for module: {}'.format(name))


class FairseqDecoder(nn.Module):
    """Base class for decoders."""

    def __init__(self, dictionary):
        super().__init__()
        self.dictionary = dictionary
        self.onnx_trace = False
        self.adaptive_softmax = None

    def forward(self, prev_output_tokens, encoder_out=None, **kwargs):
        """
        Args:
            prev_output_tokens (LongTensor): shifted output tokens of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (dict, optional): output from the encoder, used for
                encoder-side attention

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, **kwargs)
        x = self.output_layer(x)
        return x, extra

    def extract_features(self, prev_output_tokens, encoder_out=None, **kwargs):
        """
        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        raise NotImplementedError

    def output_layer(self, features, **kwargs):
        """
        Project features to the default output size, e.g., vocabulary size.

        Args:
            features (Tensor): features returned by *extract_features*.
        """
        raise NotImplementedError

    def get_normalized_probs(self, net_output: 'Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]]', log_probs: 'bool', sample: 'Optional[Dict[str, Tensor]]'=None):
        """Get normalized probabilities (or log probs) from a net's output."""
        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)

    def get_normalized_probs_scriptable(self, net_output: 'Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]]', log_probs: 'bool', sample: 'Optional[Dict[str, Tensor]]'=None):
        """Get normalized probabilities (or log probs) from a net's output."""
        if hasattr(self, 'adaptive_softmax') and self.adaptive_softmax is not None:
            if sample is not None:
                assert 'target' in sample
                target = sample['target']
            else:
                target = None
            out = self.adaptive_softmax.get_log_prob(net_output[0], target=target)
            return out.exp_() if not log_probs else out
        logits = net_output[0]
        if log_probs:
            return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
        else:
            return utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace)

    def max_positions(self):
        """Maximum input length supported by the decoder."""
        return 1000000.0

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade old state dicts to work with newer code."""
        return state_dict

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True


class FairseqIncrementalState(object):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.init_incremental_state()

    def init_incremental_state(self):
        self._incremental_state_id = str(uuid.uuid4())

    def _get_full_incremental_state_key(self, key: 'str') ->str:
        return '{}.{}'.format(self._incremental_state_id, key)

    def get_incremental_state(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]', key: 'str') ->Optional[Dict[str, Optional[Tensor]]]:
        """Helper for getting incremental state for an nn.Module."""
        full_key = self._get_full_incremental_state_key(key)
        if incremental_state is None or full_key not in incremental_state:
            return None
        return incremental_state[full_key]

    def set_incremental_state(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]', key: 'str', value: 'Dict[str, Optional[Tensor]]') ->Optional[Dict[str, Dict[str, Optional[Tensor]]]]:
        """Helper for setting incremental state for an nn.Module."""
        if incremental_state is not None:
            full_key = self._get_full_incremental_state_key(key)
            incremental_state[full_key] = value
        return incremental_state


def with_incremental_state(cls):
    cls.__bases__ = (FairseqIncrementalState,) + tuple(b for b in cls.__bases__ if b != FairseqIncrementalState)
    return cls


@with_incremental_state
class FairseqIncrementalDecoder(FairseqDecoder):
    """Base class for incremental decoders.

    Incremental decoding is a special mode at inference time where the Model
    only receives a single timestep of input corresponding to the previous
    output token (for teacher forcing) and must produce the next output
    *incrementally*. Thus the model must cache any long-term state that is
    needed about the sequence, e.g., hidden states, convolutional states, etc.

    Compared to the standard :class:`FairseqDecoder` interface, the incremental
    decoder interface allows :func:`forward` functions to take an extra keyword
    argument (*incremental_state*) that can be used to cache state across
    time-steps.

    The :class:`FairseqIncrementalDecoder` interface also defines the
    :func:`reorder_incremental_state` method, which is used during beam search
    to select and reorder the incremental state based on the selection of beams.

    To learn more about how incremental decoding works, refer to `this blog
    <http://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/>`_.
    """

    def __init__(self, dictionary):
        super().__init__(dictionary)

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):
        """
        Args:
            prev_output_tokens (LongTensor): shifted output tokens of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (dict, optional): output from the encoder, used for
                encoder-side attention
            incremental_state (dict, optional): dictionary used for storing
                state during :ref:`Incremental decoding`

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        raise NotImplementedError

    def extract_features(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):
        """
        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        raise NotImplementedError

    def reorder_incremental_state(self, incremental_state: 'Dict[str, Dict[str, Optional[Tensor]]]', new_order: 'Tensor'):
        """Reorder incremental state.

        This will be called when the order of the input has changed from the
        previous time step. A typical use case is beam search, where the input
        order changes between time steps based on the selection of beams.
        """
        pass

    def reorder_incremental_state_scripting(self, incremental_state: 'Dict[str, Dict[str, Optional[Tensor]]]', new_order: 'Tensor'):
        """Main entry point for reordering the incremental state.

        Due to limitations in TorchScript, we call this function in
        :class:`fairseq.sequence_generator.SequenceGenerator` instead of
        calling :func:`reorder_incremental_state` directly.
        """
        for module in self.modules():
            if hasattr(module, 'reorder_incremental_state'):
                result = module.reorder_incremental_state(incremental_state, new_order)
                if result is not None:
                    incremental_state = result

    def set_beam_size(self, beam_size):
        """Sets the beam size in the decoder and all children."""
        if getattr(self, '_beam_size', -1) != beam_size:
            seen = set()

            def apply_set_beam_size(module):
                if module != self and hasattr(module, 'set_beam_size') and module not in seen:
                    seen.add(module)
                    module.set_beam_size(beam_size)
            self.apply(apply_set_beam_size)
            self._beam_size = beam_size


def _mask_for_xformers(mask: 'Tensor', to_dtype: 'Optional[torch.dtype]'=None):
    """
    call to pytorch multihead accepts three mask types:
        - ByteTensor where non-zero means to mask
        - FloatTensor which is an additive mask
        - BoolTensor where True means to mask
    xFormers currently accepts boolean and additive maks. For boolean masks
    the values have opposite meaning. For a BoolTensor True mean to keep the value.
    """
    float_types = [torch.float, torch.float16]
    additive = mask.dtype in float_types
    to_dtype = mask.dtype if to_dtype is None else to_dtype
    to_additive = to_dtype in float_types
    if additive:
        if to_additive:
            return mask
        mask = mask < 0
    if to_additive:
        new_mask = torch.zeros_like(mask, dtype=to_dtype)
        new_mask = new_mask.masked_fill_(mask, -float('inf'))
        return new_mask
    mask = ~mask
    mask = mask
    return mask


def quant_noise(module, p, block_size):
    """
    Wraps modules and applies quantization noise to the weights for
    subsequent quantization with Iterative Product Quantization as
    described in "Training with Quantization Noise for Extreme Model Compression"

    Args:
        - module: nn.Module
        - p: amount of Quantization Noise
        - block_size: size of the blocks for subsequent quantization with iPQ

    Remarks:
        - Module weights must have the right sizes wrt the block size
        - Only Linear, Embedding and Conv2d modules are supported for the moment
        - For more detail on how to quantize by blocks with convolutional weights,
          see "And the Bit Goes Down: Revisiting the Quantization of Neural Networks"
        - We implement the simplest form of noise here as stated in the paper
          which consists in randomly dropping blocks
    """
    if p <= 0:
        return module
    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))
    is_conv = module.weight.ndim == 4
    if not is_conv:
        assert module.weight.size(1) % block_size == 0, 'Input features must be a multiple of block sizes'
    elif module.kernel_size == (1, 1):
        assert module.in_channels % block_size == 0, 'Input channels must be a multiple of block sizes'
    else:
        k = module.kernel_size[0] * module.kernel_size[1]
        assert k % block_size == 0, 'Kernel size must be a multiple of block size'

    def _forward_pre_hook(mod, input):
        if mod.training:
            if not is_conv:
                weight = mod.weight
                in_features = weight.size(1)
                out_features = weight.size(0)
                mask = torch.zeros(in_features // block_size * out_features, device=weight.device)
                mask.bernoulli_(p)
                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)
            else:
                weight = mod.weight
                in_channels = mod.in_channels
                out_channels = mod.out_channels
                if mod.kernel_size == (1, 1):
                    mask = torch.zeros(int(in_channels // block_size * out_channels), device=weight.device)
                    mask.bernoulli_(p)
                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)
                else:
                    mask = torch.zeros(weight.size(0), weight.size(1), device=weight.device)
                    mask.bernoulli_(p)
                    mask = mask.unsqueeze(2).unsqueeze(3).repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])
            mask = mask
            s = 1 / (1 - p)
            mod.weight.data = s * weight.masked_fill(mask, 0)
    module.register_forward_pre_hook(_forward_pre_hook)
    return module


class MultiheadAttention(FairseqIncrementalDecoder):
    """Multi-headed attention.

    See "Attention Is All You Need" for more details.
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, dictionary=None, q_noise=0.0, qn_block_size=8, xformers_att_config: 'Optional[str]'=None, xformers_blocksparse_layout: 'Optional[torch.Tensor]'=None, xformers_blocksparse_blocksize: 'Optional[int]'=16):
        super().__init__(dictionary)
        xformers_att_config = utils.eval_str_dict(xformers_att_config)
        self.use_xformers = xformers_att_config is not None
        if self.use_xformers and not _xformers_available:
            raise ImportError('\n\n  Please install xFormers.')
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim
        self.num_heads = num_heads
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, 'embed_dim must be divisible by num_heads'
        self.scaling = self.head_dim ** -0.5
        self.self_attention = self_attention
        self.encoder_decoder_attention = encoder_decoder_attention
        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and value to be of the same size'
        self.k_proj = quant_noise(nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.v_proj = quant_noise(nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.q_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        if add_bias_kv:
            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))
            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None
        self.add_zero_attn = add_zero_attn
        self.beam_size = 1
        self.reset_parameters()
        if self.use_xformers:
            xformers_att_config['dropout'] = xformers_att_config.get('dropout', dropout)
            xformers_att_config['num_heads'] = xformers_att_config.get('num_heads', num_heads)
            if xformers_blocksparse_layout is not None:
                xformers_att_config['block_size'] = xformers_blocksparse_blocksize
                xformers_att_config['layout'] = xformers_blocksparse_layout
                xformers_att_config['name'] = 'blocksparse'
            self.attention = build_attention(xformers_att_config)
        self.onnx_trace = False
        self.skip_embed_dim_check = False
        self.init_incremental_state()

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def reset_parameters(self):
        if self.qkv_same_dim:
            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))
        else:
            nn.init.xavier_uniform_(self.k_proj.weight)
            nn.init.xavier_uniform_(self.v_proj.weight)
            nn.init.xavier_uniform_(self.q_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.out_proj.bias is not None:
            nn.init.constant_(self.out_proj.bias, 0.0)
        if self.bias_k is not None:
            nn.init.xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            nn.init.xavier_normal_(self.bias_v)

    def _get_reserve_head_index(self, num_heads_to_keep: 'int'):
        k_proj_heads_norm = []
        q_proj_heads_norm = []
        v_proj_heads_norm = []
        for i in range(self.num_heads):
            start_idx = i * self.head_dim
            end_idx = (i + 1) * self.head_dim
            k_proj_heads_norm.append(torch.sum(torch.abs(self.k_proj.weight[start_idx:end_idx,])).tolist() + torch.sum(torch.abs(self.k_proj.bias[start_idx:end_idx])).tolist())
            q_proj_heads_norm.append(torch.sum(torch.abs(self.q_proj.weight[start_idx:end_idx,])).tolist() + torch.sum(torch.abs(self.q_proj.bias[start_idx:end_idx])).tolist())
            v_proj_heads_norm.append(torch.sum(torch.abs(self.v_proj.weight[start_idx:end_idx,])).tolist() + torch.sum(torch.abs(self.v_proj.bias[start_idx:end_idx])).tolist())
        heads_norm = []
        for i in range(self.num_heads):
            heads_norm.append(k_proj_heads_norm[i] + q_proj_heads_norm[i] + v_proj_heads_norm[i])
        sorted_head_index = sorted(range(self.num_heads), key=lambda k: heads_norm[k], reverse=True)
        reserve_head_index = []
        for i in range(num_heads_to_keep):
            start = sorted_head_index[i] * self.head_dim
            end = (sorted_head_index[i] + 1) * self.head_dim
            reserve_head_index.append((start, end))
        return reserve_head_index

    def _adaptive_prune_heads(self, reserve_head_index: 'List[Tuple[int, int]]'):
        new_q_weight = []
        new_q_bias = []
        new_k_weight = []
        new_k_bias = []
        new_v_weight = []
        new_v_bias = []
        new_out_proj_weight = []
        for ele in reserve_head_index:
            start_idx, end_idx = ele
            new_q_weight.append(self.q_proj.weight[start_idx:end_idx,])
            new_q_bias.append(self.q_proj.bias[start_idx:end_idx])
            new_k_weight.append(self.k_proj.weight[start_idx:end_idx,])
            new_k_bias.append(self.k_proj.bias[start_idx:end_idx])
            new_v_weight.append(self.v_proj.weight[start_idx:end_idx,])
            new_v_bias.append(self.v_proj.bias[start_idx:end_idx])
            new_out_proj_weight.append(self.out_proj.weight[:, start_idx:end_idx])
        new_q_weight = torch.cat(new_q_weight).detach()
        new_k_weight = torch.cat(new_k_weight).detach()
        new_v_weight = torch.cat(new_v_weight).detach()
        new_out_proj_weight = torch.cat(new_out_proj_weight, dim=-1).detach()
        new_q_weight.requires_grad = True
        new_k_weight.requires_grad = True
        new_v_weight.requires_grad = True
        new_out_proj_weight.requires_grad = True
        new_q_bias = torch.cat(new_q_bias).detach()
        new_q_bias.requires_grad = True
        new_k_bias = torch.cat(new_k_bias).detach()
        new_k_bias.requires_grad = True
        new_v_bias = torch.cat(new_v_bias).detach()
        new_v_bias.requires_grad = True
        self.q_proj.weight = torch.nn.Parameter(new_q_weight)
        self.q_proj.bias = torch.nn.Parameter(new_q_bias)
        self.k_proj.weight = torch.nn.Parameter(new_k_weight)
        self.k_proj.bias = torch.nn.Parameter(new_k_bias)
        self.v_proj.weight = torch.nn.Parameter(new_v_weight)
        self.v_proj.bias = torch.nn.Parameter(new_v_bias)
        self.out_proj.weight = torch.nn.Parameter(new_out_proj_weight)
        self.num_heads = len(reserve_head_index)
        self.embed_dim = self.head_dim * self.num_heads
        self.q_proj.out_features = self.embed_dim
        self.k_proj.out_features = self.embed_dim
        self.v_proj.out_features = self.embed_dim

    def _set_skip_embed_dim_check(self):
        self.skip_embed_dim_check = True

    def _pad_masks(self, key_padding_mask: 'Optional[Tensor]', attn_mask: 'Optional[Tensor]') ->Tuple[Optional[Tensor], Optional[Tensor]]:
        if attn_mask is not None:
            shape = attn_mask.size()[:-1] + torch.Size([1])
            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(shape)], dim=-1)
        if key_padding_mask is not None:
            shape = key_padding_mask.size()[:-1] + torch.Size([1])
            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(shape)], dim=-1)
        return key_padding_mask, attn_mask

    def _add_bias(self, k: 'Tensor', v: 'Tensor', key_padding_mask: 'Optional[Tensor]', attn_mask: 'Optional[Tensor]', bsz: 'int') ->Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:
        assert self.bias_k is not None
        assert self.bias_v is not None
        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])
        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])
        key_padding_mask, attn_mask = self._pad_masks(key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        return k, v, key_padding_mask, attn_mask

    def _append_zero_attn(self, k: 'Tensor', v: 'Tensor', key_padding_mask: 'Optional[Tensor]', attn_mask: 'Optional[Tensor]') ->Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:
        zero_attn_shape = k.size()[:-2] + torch.Size([1]) + k.size()[-1:]
        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=-2)
        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=-2)
        key_padding_mask, attn_mask = self._pad_masks(key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        return k, v, key_padding_mask, attn_mask

    def _xformers_attn_forward(self, query, key: 'Optional[Tensor]', value: 'Optional[Tensor]', key_padding_mask: 'Optional[Tensor]'=None, need_weights: 'bool'=True, attn_mask: 'Optional[Tensor]'=None) ->Tuple[Tensor, Optional[Tensor]]:
        tgt_len, bsz, embed_dim = query.size()
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == tgt_len
        if self.self_attention:
            key = query
            value = query
        elif self.encoder_decoder_attention:
            value = key
        q = self.q_proj(query)
        k = self.k_proj(key)
        v = self.v_proj(value)
        if self.bias_k is not None:
            assert self.bias_v is not None
            k, v, attn_mask, key_padding_mask = self._add_bias(k, v, attn_mask, key_padding_mask, bsz)

        def fold_heads(x):
            return x.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)

        def split_heads(x):
            return x.contiguous().view(-1, bsz, self.num_heads, self.head_dim).transpose(0, 1).transpose(1, 2)
        massage = split_heads if self.attention.requires_head_dimension else fold_heads
        q = massage(q)
        if k is not None:
            k = massage(k)
        if v is not None:
            v = massage(v)
        if self.add_zero_attn:
            k, v, key_padding_mask, attn_mask = self._append_zero_attn(k=k, v=v, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        kwargs = {}
        if attn_mask is not None and self.attention.supports_attention_mask:
            attn_mask = _mask_for_xformers(attn_mask, to_dtype=q.dtype)
            kwargs['att_mask'] = attn_mask
        if key_padding_mask is not None:
            key_padding_mask = _mask_for_xformers(key_padding_mask, to_dtype=torch.bool)
            if not self.attention.requires_separate_masks:
                attn_mask = maybe_merge_masks(attn_mask, key_padding_mask, batch_size=bsz, src_len=k.size(-2), tgt_len=q.size(-2), num_heads=self.num_heads)
                key_padding_mask = None
                kwargs['att_mask'] = attn_mask
            if self.attention.supports_key_padding_mask:
                kwargs['key_padding_mask'] = key_padding_mask
        y = self.attention(q, k, v, **kwargs)
        y = y.view(bsz, self.num_heads, tgt_len, self.head_dim).transpose(1, 2).flatten(start_dim=2, end_dim=3).transpose(0, 1)
        assert list(y.size()) == [tgt_len, bsz, embed_dim]
        y = self.out_proj(y)
        return y, None

    def forward(self, query: 'Tensor', key: 'Optional[Tensor]', value: 'Optional[Tensor]', key_padding_mask: 'Optional[Tensor]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, need_weights: 'bool'=True, static_kv: 'bool'=False, attn_mask: 'Optional[Tensor]'=None, before_softmax: 'bool'=False, need_head_weights: 'bool'=False) ->Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time x Batch x Channel

        Args:
            key_padding_mask (ByteTensor, optional): mask to exclude
                keys that are pads, of shape `(batch, src_len)`, where
                padding elements are indicated by 1s.
            need_weights (bool, optional): return the attention weights,
                averaged over heads (default: False).
            attn_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
            before_softmax (bool, optional): return the raw attention
                weights and values before the attention softmax.
            need_head_weights (bool, optional): return the attention
                weights for each head. Implies *need_weights*. Default:
                return the average attention weights over all heads.
        """
        if need_head_weights:
            need_weights = True
        is_tpu = query.device.type == 'xla'
        tgt_len, bsz, embed_dim = query.size()
        src_len = tgt_len
        if not self.skip_embed_dim_check:
            assert embed_dim == self.embed_dim, f'query dim {embed_dim} != {self.embed_dim}'
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        if key is not None:
            src_len, key_bsz, _ = key.size()
            if not torch.jit.is_scripting():
                assert value is not None
                assert src_len, key_bsz == value.shape[:2]
        if not self.onnx_trace and not is_tpu and incremental_state is None and not static_kv and not torch.jit.is_scripting() and not self.skip_embed_dim_check:
            assert key is not None and value is not None
            if self.use_xformers:
                return self._xformers_attn_forward(query, key, value, key_padding_mask, need_weights, attn_mask)
            else:
                return F.multi_head_attention_forward(query, key, value, self.embed_dim, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask.bool() if key_padding_mask is not None else None, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight)
        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if saved_state is not None and 'prev_key' in saved_state:
                if static_kv:
                    assert self.encoder_decoder_attention and not self.self_attention
                    key = value = None
        else:
            saved_state = None
        if self.self_attention:
            q = self.q_proj(query)
            k = self.k_proj(query)
            v = self.v_proj(query)
        elif self.encoder_decoder_attention:
            q = self.q_proj(query)
            if key is None:
                assert value is None
                k = v = None
            else:
                if self.beam_size > 1 and bsz == key.size(1):
                    key = key.view(key.size(0), -1, self.beam_size, key.size(2))[:, :, 0, :]
                    if key_padding_mask is not None:
                        key_padding_mask = key_padding_mask.view(-1, self.beam_size, key_padding_mask.size(1))[:, 0, :]
                k = self.k_proj(key)
                v = self.v_proj(key)
        else:
            assert key is not None and value is not None
            q = self.q_proj(query)
            k = self.k_proj(key)
            v = self.v_proj(value)
        q *= self.scaling
        if self.bias_k is not None:
            assert self.bias_v is not None
            k, v, attn_mask, key_padding_mask = self._add_bias(k, v, attn_mask, key_padding_mask, bsz)
        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        kv_bsz = bsz
        if k is not None:
            kv_bsz = k.size(1)
            k = k.contiguous().view(-1, kv_bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if v is not None:
            v = v.contiguous().view(-1, kv_bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if saved_state is not None:
            if 'prev_key' in saved_state:
                _prev_key = saved_state['prev_key']
                assert _prev_key is not None
                kv_bsz = _prev_key.size(0)
                prev_key = _prev_key.view(kv_bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    k = prev_key
                else:
                    assert k is not None
                    k = torch.cat([prev_key, k], dim=1)
                src_len = k.size(1)
            if 'prev_value' in saved_state:
                _prev_value = saved_state['prev_value']
                assert _prev_value is not None
                assert kv_bsz == _prev_value.size(0)
                prev_value = _prev_value.view(kv_bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    v = prev_value
                else:
                    assert v is not None
                    v = torch.cat([prev_value, v], dim=1)
            prev_key_padding_mask: 'Optional[Tensor]' = None
            if 'prev_key_padding_mask' in saved_state:
                prev_key_padding_mask = saved_state['prev_key_padding_mask']
            assert k is not None and v is not None
            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=kv_bsz, src_len=k.size(1), static_kv=static_kv)
            saved_state['prev_key'] = k.view(kv_bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_value'] = v.view(kv_bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_key_padding_mask'] = key_padding_mask
            assert incremental_state is not None
            incremental_state = self._set_input_buffer(incremental_state, saved_state)
        assert k is not None
        assert k.size(1) == src_len
        if key_padding_mask is not None and key_padding_mask.dim() == 0:
            key_padding_mask = None
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == kv_bsz
            assert key_padding_mask.size(1) == src_len
        if self.add_zero_attn:
            assert v is not None
            src_len += 1
            k, v, key_padding_mask, attn_mask = self._append_zero_attn(k=k, v=v, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        if self.encoder_decoder_attention and bsz != kv_bsz:
            attn_weights = torch.einsum('bxhtd,bhsd->bxhts', q.view((kv_bsz, -1, self.num_heads) + q.size()[1:]), k.view((kv_bsz, self.num_heads) + k.size()[1:]))
            attn_weights = attn_weights.reshape((-1,) + attn_weights.size()[-2:])
        else:
            attn_weights = torch.bmm(q, k.transpose(1, 2))
        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(0)
            if self.onnx_trace:
                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)
            attn_weights += attn_mask
        if key_padding_mask is not None:
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            if not is_tpu:
                attn_weights = attn_weights.view(kv_bsz, -1, self.num_heads, tgt_len, src_len)
                attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3), float('-inf'))
            else:
                attn_weights = attn_weights.transpose(0, 2)
                attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))
                attn_weights = attn_weights.transpose(0, 2)
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        if before_softmax:
            return attn_weights, v
        attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)
        attn_weights = attn_weights_float.type_as(attn_weights)
        attn_probs = self.dropout_module(attn_weights)
        assert v is not None
        attn: 'Optional[Tensor]' = None
        if self.encoder_decoder_attention and bsz != kv_bsz:
            attn = torch.einsum('bxhts,bhsd->bxhtd', attn_probs.view((kv_bsz, -1, self.num_heads) + attn_probs.size()[1:]), v.view((kv_bsz, self.num_heads) + v.size()[1:]))
            attn = attn.reshape((-1,) + attn.size()[-2:])
        else:
            attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
        if self.onnx_trace and attn.size(1) == 1:
            attn = attn.contiguous().view(tgt_len, bsz, self.embed_dim)
        else:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)
        attn = self.out_proj(attn)
        attn_weights: 'Optional[Tensor]' = None
        if need_weights:
            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)
            if not need_head_weights:
                attn_weights = attn_weights.mean(dim=0)
        return attn, attn_weights

    @staticmethod
    def _append_prev_key_padding_mask(key_padding_mask: 'Optional[Tensor]', prev_key_padding_mask: 'Optional[Tensor]', batch_size: 'int', src_len: 'int', static_kv: 'bool') ->Optional[Tensor]:
        if prev_key_padding_mask is not None and static_kv:
            new_key_padding_mask = prev_key_padding_mask
        elif prev_key_padding_mask is not None and key_padding_mask is not None:
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), key_padding_mask.float()], dim=1)
        elif prev_key_padding_mask is not None:
            if src_len > prev_key_padding_mask.size(1):
                filler = torch.zeros((batch_size, src_len - prev_key_padding_mask.size(1)), device=prev_key_padding_mask.device)
                new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), filler.float()], dim=1)
            else:
                new_key_padding_mask = prev_key_padding_mask.float()
        elif key_padding_mask is not None:
            if src_len > key_padding_mask.size(1):
                filler = torch.zeros((batch_size, src_len - key_padding_mask.size(1)), device=key_padding_mask.device)
                new_key_padding_mask = torch.cat([filler.float(), key_padding_mask.float()], dim=1)
            else:
                new_key_padding_mask = key_padding_mask.float()
        else:
            new_key_padding_mask = prev_key_padding_mask
        return new_key_padding_mask

    @torch.jit.export
    def reorder_incremental_state(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]', new_order: 'Tensor'):
        """Reorder buffered internal state (for incremental generation)."""
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            for k in input_buffer.keys():
                input_buffer_k = input_buffer[k]
                if input_buffer_k is not None:
                    if self.encoder_decoder_attention:
                        if input_buffer_k.size(0) * self.beam_size == new_order.size(0):
                            return incremental_state
                        elif self.beam_size > 1:
                            input_buffer[k] = input_buffer_k.index_select(0, new_order.reshape(-1, self.beam_size)[:, 0] // self.beam_size)
                        else:
                            input_buffer[k] = input_buffer_k.index_select(0, new_order)
                    else:
                        input_buffer[k] = input_buffer_k.index_select(0, new_order)
            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
        return incremental_state

    def set_beam_size(self, beam_size):
        """Used for effiecient beamable enc-dec attention"""
        self.beam_size = beam_size

    def _get_input_buffer(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]') ->Dict[str, Optional[Tensor]]:
        result = self.get_incremental_state(incremental_state, 'attn_state')
        if result is not None:
            return result
        else:
            empty_result: 'Dict[str, Optional[Tensor]]' = {}
            return empty_result

    def _set_input_buffer(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]', buffer: 'Dict[str, Optional[Tensor]]'):
        return self.set_incremental_state(incremental_state, 'attn_state', buffer)

    def apply_sparse_mask(self, attn_weights, tgt_len: 'int', src_len: 'int', bsz: 'int'):
        return attn_weights

    def upgrade_state_dict_named(self, state_dict, name):
        prefix = name + '.' if name != '' else ''
        items_to_add = {}
        keys_to_remove = []
        for k in state_dict.keys():
            if k.endswith(prefix + 'in_proj_weight'):
                dim = int(state_dict[k].shape[0] / 3)
                items_to_add[prefix + 'q_proj.weight'] = state_dict[k][:dim]
                items_to_add[prefix + 'k_proj.weight'] = state_dict[k][dim:2 * dim]
                items_to_add[prefix + 'v_proj.weight'] = state_dict[k][2 * dim:]
                keys_to_remove.append(k)
                k_bias = prefix + 'in_proj_bias'
                if k_bias in state_dict.keys():
                    dim = int(state_dict[k].shape[0] / 3)
                    items_to_add[prefix + 'q_proj.bias'] = state_dict[k_bias][:dim]
                    items_to_add[prefix + 'k_proj.bias'] = state_dict[k_bias][dim:2 * dim]
                    items_to_add[prefix + 'v_proj.bias'] = state_dict[k_bias][2 * dim:]
                    keys_to_remove.append(prefix + 'in_proj_bias')
        for k in keys_to_remove:
            del state_dict[k]
        for key, value in items_to_add.items():
            state_dict[key] = value


class RelPositionMultiHeadedAttention(ESPNETMultiHeadedAttention):
    """Multi-Head Attention layer with relative position encoding.
    Paper: https://arxiv.org/abs/1901.02860
    Args:
        n_head: The number of heads.
        n_feat: The number of features.
        dropout: Dropout rate.
        zero_triu: Whether to zero the upper triangular part of attention matrix.
    """

    def __init__(self, n_feat, n_head, dropout, zero_triu=False):
        """Construct an RelPositionMultiHeadedAttention object."""
        super().__init__(n_feat, n_head, dropout)
        self.zero_triu = zero_triu
        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)
        self.pos_bias_u = nn.Parameter(torch.zeros(self.h, self.d_k))
        self.pos_bias_v = nn.Parameter(torch.zeros(self.h, self.d_k))
        torch.nn.init.xavier_uniform_(self.pos_bias_u)
        torch.nn.init.xavier_uniform_(self.pos_bias_v)

    def rel_shift(self, x):
        """Compute relative positional encoding.
        Args:
            x: Input tensor B X n_head X T X 2T-1
        Returns:
            torch.Tensor: Output tensor.
        """
        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)
        x_padded = torch.cat([zero_pad, x], dim=-1)
        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))
        x = x_padded[:, :, 1:].view_as(x)[:, :, :, :x.size(-1) // 2 + 1]
        if self.zero_triu:
            ones = torch.ones((x.size(2), x.size(3)), device=x.device)
            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]
        return x

    def forward(self, query, key, value, pos_emb, key_padding_mask=None, **kwargs):
        """Compute scaled dot product attention.
        Args:
            query: Query tensor T X B X C
            key: Key tensor T X B X C
            value: Value tensor T X B X C
            pos_emb: Positional embedding tensor B X 2T-1 X C
            key_padding_mask: Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X C.
        """
        query = query.transpose(0, 1)
        key = key.transpose(0, 1)
        value = value.transpose(0, 1)
        pos_emb = pos_emb.transpose(0, 1)
        q, k, v = self.forward_qkv(query, key, value)
        q = q.transpose(1, 2)
        n_batch_pos = pos_emb.size(0)
        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)
        p = p.transpose(1, 2)
        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)
        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)
        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))
        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
        matrix_bd = self.rel_shift(matrix_bd)
        scores = (matrix_ac + matrix_bd) / math.sqrt(self.d_k)
        scores = self.forward_attention(v, scores, key_padding_mask)
        scores = scores.transpose(0, 1)
        return scores, None


class RotaryPositionalEmbedding(torch.nn.Module):

    def __init__(self, dim, base=10000, precision=torch.half):
        """Rotary positional embedding
        Reference : https://blog.eleuther.ai/rotary-embeddings/
        Paper: https://arxiv.org/pdf/2104.09864.pdf
        Args:
            dim: Dimension of embedding
            base: Base value for exponential
            precision: precision to use for numerical values
        """
        super().__init__()
        inv_freq = 1.0 / base ** (torch.arange(0, dim, 2).float() / dim)
        self.register_buffer('inv_freq', inv_freq)
        self.seq_len_cached = 0
        self.cos_cached = torch.empty(self.seq_len_cached, 1, 1, dim)
        self.sin_cached = torch.empty(self.seq_len_cached, 1, 1, dim)
        self.precision = precision

    def forward(self, x, seq_len: 'int'=0):
        """
        Args:
            x: Input x with T X B X C
            seq_len: Sequence length of input x
        """
        if seq_len > self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            self.cos_cached = emb.cos().view(emb.size(0), 1, 1, emb.size(1))
            self.sin_cached = emb.sin().view(emb.size(0), 1, 1, emb.size(1))
        return self.cos_cached, self.sin_cached


def rotate_half(x):
    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=x1.ndim - 1)


def apply_rotary_pos_emb(q, k, cos, sin, offset: 'int'=0):
    cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
    return q * cos + rotate_half(q) * sin, k * cos + rotate_half(k) * sin


class RotaryPositionMultiHeadedAttention(ESPNETMultiHeadedAttention):

    def __init__(self, n_feat, n_head, dropout, precision, rotary_emd_base=10000):
        """Construct an RotaryPositionMultiHeadedAttention object."""
        super().__init__(n_feat, n_head, dropout)
        precision = torch.float
        self.rotary_ndims = self.d_k
        if precision == 'fp16':
            precision = torch.half
        self.rotary_emb = RotaryPositionalEmbedding(self.rotary_ndims, base=rotary_emd_base, precision=precision)

    def forward(self, query, key, value, key_padding_mask=None, **kwargs):
        """Compute rotary position attention.
        Args:
            query: Query tensor T X B X C
            key: Key tensor T X B X C
            value: Value tensor T X B X C
            key_padding_mask: Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X D.
        Notes:
            Assumes self attn
        """
        T, B, C = value.size()
        query = query.view(T, B, self.h, self.d_k)
        key = key.view(T, B, self.h, self.d_k)
        value = value.view(T, B, self.h, self.d_k)
        cos, sin = self.rotary_emb(value, seq_len=T)
        query, key = apply_rotary_pos_emb(query, key, cos, sin, offset=0)
        query = query.view(T, B, self.h * self.d_k)
        key = key.view(T, B, self.h * self.d_k)
        value = value.view(T, B, self.h * self.d_k)
        query = query.transpose(0, 1)
        key = key.transpose(0, 1)
        value = value.transpose(0, 1)
        q, k, v = self.forward_qkv(query, key, value)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        scores = self.forward_attention(v, scores, key_padding_mask)
        scores = scores.transpose(0, 1)
        return scores, None


class ConformerEncoderLayer(torch.nn.Module):
    """Conformer block based on https://arxiv.org/abs/2005.08100. We currently don't support relative positional encoding in MHA"""

    def __init__(self, embed_dim, ffn_embed_dim, attention_heads, dropout, use_fp16, depthwise_conv_kernel_size=31, activation_fn='swish', attn_type=None, pos_enc_type='abs'):
        """
        Args:
            embed_dim: Input embedding dimension
            ffn_embed_dim: FFN layer dimension
            attention_heads: Number of attention heads in MHA
            dropout: dropout value
            depthwise_conv_kernel_size: Size of kernel in depthwise conv layer in convolution module
            activation_fn: Activation function name to use in convulation block and feed forward block
            attn_type: MHA implementation from ESPNET vs fairseq
            pos_enc_type: Positional encoding type - abs, rope, rel_pos
        """
        self.pos_enc_type = pos_enc_type
        super(ConformerEncoderLayer, self).__init__()
        self.ffn1 = FeedForwardModule(embed_dim, ffn_embed_dim, dropout, dropout)
        self.self_attn_layer_norm = LayerNorm(embed_dim, export=False)
        self.self_attn_dropout = torch.nn.Dropout(dropout)
        if attn_type == 'espnet':
            if self.pos_enc_type == 'rel_pos':
                self.self_attn = RelPositionMultiHeadedAttention(embed_dim, attention_heads, dropout=dropout)
            elif self.pos_enc_type == 'rope':
                self.self_attn = RotaryPositionMultiHeadedAttention(embed_dim, attention_heads, dropout=dropout, precision=use_fp16)
            elif self.pos_enc_type == 'abs':
                self.self_attn = ESPNETMultiHeadedAttention(embed_dim, attention_heads, dropout=dropout)
            else:
                raise Exception(f'Unsupported attention type {self.pos_enc_type}')
        else:
            self.self_attn = MultiheadAttention(embed_dim, attention_heads, dropout=dropout)
        self.conv_module = ConvolutionModule(embed_dim=embed_dim, channels=embed_dim, depthwise_kernel_size=depthwise_conv_kernel_size, dropout=dropout, activation_fn=activation_fn)
        self.ffn2 = FeedForwardModule(embed_dim, ffn_embed_dim, dropout, dropout, activation_fn=activation_fn)
        self.final_layer_norm = LayerNorm(embed_dim, export=False)

    def forward(self, x, encoder_padding_mask: 'Optional[torch.Tensor]', position_emb: 'Optional[torch.Tensor]'=None):
        """
        Args:
            x: Tensor of shape T X B X C
            encoder_padding_mask: Optional mask tensor
            positions:
        Returns:
            Tensor of shape T X B X C
        """
        residual = x
        x = self.ffn1(x)
        x = x * 0.5 + residual
        residual = x
        x = self.self_attn_layer_norm(x)
        if self.pos_enc_type == 'rel_pos':
            x, attn = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, pos_emb=position_emb, need_weights=False)
        else:
            x, attn = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, need_weights=False)
        x = self.self_attn_dropout(x)
        x = x + residual
        residual = x
        x = x.transpose(0, 1)
        x = self.conv_module(x)
        x = x.transpose(0, 1)
        x = residual + x
        residual = x
        x = self.ffn2(x)
        layer_result = x
        x = x * 0.5 + residual
        x = self.final_layer_norm(x)
        return x, (attn, layer_result)


class ConformerWav2Vec2EncoderLayer(ConformerEncoderLayer):
    """Encoder layer for Wav2vec2 encoder"""

    def forward(self, x: 'torch.Tensor', self_attn_mask: 'torch.Tensor'=None, self_attn_padding_mask: 'torch.Tensor'=None, need_weights: 'bool'=False, att_args=None, position_emb=None):
        return super().forward(x, self_attn_padding_mask, position_emb)


class SamePad(nn.Module):

    def __init__(self, kernel_size, causal=False):
        super().__init__()
        if causal:
            self.remove = kernel_size - 1
        else:
            self.remove = 1 if kernel_size % 2 == 0 else 0

    def forward(self, x):
        if self.remove > 0:
            x = x[:, :, :-self.remove]
        return x


class TransformerSentenceEncoderLayer(nn.Module):
    """
    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained
    models.
    """

    def __init__(self, embedding_dim: 'int'=768, ffn_embedding_dim: 'int'=3072, num_attention_heads: 'int'=8, dropout: 'float'=0.1, attention_dropout: 'float'=0.1, activation_dropout: 'float'=0.1, activation_fn: 'str'='relu', export: 'bool'=False, q_noise: 'float'=0.0, qn_block_size: 'int'=8, init_fn: 'Callable'=None) ->None:
        super().__init__()
        if init_fn is not None:
            init_fn()
        self.embedding_dim = embedding_dim
        self.num_attention_heads = num_attention_heads
        self.attention_dropout = attention_dropout
        self.q_noise = q_noise
        self.qn_block_size = qn_block_size
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.activation_dropout_module = FairseqDropout(activation_dropout, module_name=self.__class__.__name__)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.self_attn = self.build_self_attention(self.embedding_dim, num_attention_heads, dropout=attention_dropout, self_attention=True, q_noise=q_noise, qn_block_size=qn_block_size)
        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)
        self.fc1 = self.build_fc1(self.embedding_dim, ffn_embedding_dim, q_noise=q_noise, qn_block_size=qn_block_size)
        self.fc2 = self.build_fc2(ffn_embedding_dim, self.embedding_dim, q_noise=q_noise, qn_block_size=qn_block_size)
        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_self_attention(self, embed_dim, num_attention_heads, dropout, self_attention, q_noise, qn_block_size):
        return MultiheadAttention(embed_dim, num_attention_heads, dropout=dropout, self_attention=True, q_noise=q_noise, qn_block_size=qn_block_size)

    def forward(self, x: 'torch.Tensor', self_attn_mask: 'Optional[torch.Tensor]'=None, self_attn_padding_mask: 'Optional[torch.Tensor]'=None):
        """
        LayerNorm is applied either before or after the self-attention/ffn
        modules similar to the original Transformer implementation.
        """
        residual = x
        x, attn = self.self_attn(query=x, key=x, value=x, key_padding_mask=self_attn_padding_mask, need_weights=False, attn_mask=self_attn_mask)
        x = self.dropout_module(x)
        x = residual + x
        x = self.self_attn_layer_norm(x)
        residual = x
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = residual + x
        x = self.final_layer_norm(x)
        return x, attn


class AdapterFast(nn.Module):

    def __init__(self, adapter_num, input_dim, hidden_dim, act_fn):
        """
        Implements adapter modules directly with 3D tensor weight as parameters
        and without using ModuleList orto speed up training throughput.
        """
        super().__init__()
        self.adapter_num = adapter_num
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.W_a = nn.Parameter(torch.empty(adapter_num, hidden_dim, input_dim))
        self.W_b = nn.Parameter(torch.empty(adapter_num, input_dim, hidden_dim))
        self.b_a = nn.Parameter(torch.empty(adapter_num, hidden_dim))
        self.b_b = nn.Parameter(torch.empty(adapter_num, input_dim))
        self.ln_W = nn.Parameter(torch.empty(adapter_num, input_dim))
        self.ln_b = nn.Parameter(torch.empty(adapter_num, input_dim))
        self.act_fn = nn.Identity()
        if act_fn == 'relu':
            self.act_fn = nn.ReLU()
        elif act_fn == 'gelu':
            self.act_fn = nn.GELU()
        elif act_fn == 'selu':
            self.act_fn = nn.SELU()
        else:
            raise ValueError(f'unsupported {act_fn}')
        self.input_dim = input_dim
        self.reset_parameters()

    def reset_parameters(self):
        for ii in range(self.adapter_num):
            nn.init.kaiming_uniform_(self.W_a[ii], a=math.sqrt(5))
            nn.init.kaiming_uniform_(self.W_b[ii], a=math.sqrt(5))
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_a[ii])
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(self.b_a[ii], -bound, bound)
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.W_b[ii])
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            nn.init.uniform_(self.b_b[ii], -bound, bound)
        nn.init.ones_(self.ln_W)
        nn.init.zeros_(self.ln_b)

    def forward(self, x, adapter_id):
        ii = adapter_id
        h = x
        h = F.layer_norm(h, (self.input_dim,), self.ln_W[ii], self.ln_b[ii])
        h = F.linear(h, self.W_a[ii], self.b_a[ii])
        h = self.act_fn(h)
        h = F.linear(h, self.W_b[ii], self.b_b[ii])
        outputs = h
        return outputs

    def extra_repr(self):
        return 'adapter={}, input_dim={}, hidden_dim={}'.format(self.adapter_num, self.input_dim, self.hidden_dim)


class TransformerSentenceEncoderWithAdapterLayer(TransformerSentenceEncoderLayer):
    """
    Implements a Transformer Encoder Layer with adapters used in BERT/XLM style pre-trained
    models. An adapter module is added along with vanilla Transformer module.
    """

    def __init__(self, embedding_dim: 'float'=768, ffn_embedding_dim: 'float'=3072, num_attention_heads: 'int'=8, dropout: 'float'=0.1, attention_dropout: 'float'=0.1, activation_dropout: 'float'=0.1, activation_fn: 'str'='relu', layer_norm_first: 'bool'=False, adapter_num=201, adapter_dim=64, adapter_act_fn='relu') ->None:
        super().__init__(embedding_dim=embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, layer_norm_first=layer_norm_first)
        self.adapter_num = adapter_num
        self.adapter_dim = adapter_dim
        self.adapter_layer = AdapterFast(adapter_num, self.embedding_dim, self.adapter_dim, adapter_act_fn)

    def forward(self, x: 'torch.Tensor', self_attn_mask: 'torch.Tensor'=None, self_attn_padding_mask: 'torch.Tensor'=None, need_weights: 'bool'=False, att_args=None, corpus_key=None):
        x, (attn, layer_result) = super().forward(x=x, self_attn_mask=self_attn_mask, self_attn_padding_mask=self_attn_padding_mask, need_weights=need_weights, att_args=att_args)
        assert corpus_key is not None
        assert len(set(corpus_key)) == 1, f'corpus_key items are not same {corpus_key}'
        y = self.adapter_layer(x, corpus_key[0])
        x = x + y
        return x, (attn, layer_result)


class TransposeLast(nn.Module):

    def __init__(self, deconstruct_idx=None, tranpose_dim=-2):
        super().__init__()
        self.deconstruct_idx = deconstruct_idx
        self.tranpose_dim = tranpose_dim

    def forward(self, x):
        if self.deconstruct_idx is not None:
            x = x[self.deconstruct_idx]
        return x.transpose(self.tranpose_dim, -1)


def split_non_tensors(mixed: 'Union[torch.Tensor, Tuple[Any]]') ->Tuple[Tuple[torch.Tensor], Dict[str, List[Any]]]:
    """
    Usage::

        x = torch.Tensor([1])
        y = torch.Tensor([2])
        tensors, packed_non_tensors = split_non_tensors((x, y, None, 3))
        recon = unpack_non_tensors(tensors, packed_non_tensors)
        assert recon == (x, y, None, 3)
    """
    if isinstance(mixed, torch.Tensor):
        return (mixed,), None
    tensors = []
    packed_non_tensors = {'is_tensor': [], 'objects': []}
    for o in mixed:
        if isinstance(o, torch.Tensor):
            packed_non_tensors['is_tensor'].append(True)
            tensors.append(o)
        else:
            packed_non_tensors['is_tensor'].append(False)
            packed_non_tensors['objects'].append(o)
    return tuple(tensors), packed_non_tensors


def unpack_kwargs(kwarg_keys: 'List[str]', flat_args: 'List[Any]') ->Tuple[List[Any], Dict[str, Any]]:
    if len(kwarg_keys) == 0:
        return flat_args, {}
    args = flat_args[:-len(kwarg_keys)]
    kwargs = {k: v for k, v in zip(kwarg_keys, flat_args[-len(kwarg_keys):])}
    return args, kwargs


def unpack_non_tensors(tensors: 'Tuple[torch.Tensor]', packed_non_tensors: 'Dict[str, List[Any]]') ->Tuple[Any]:
    if packed_non_tensors is None:
        return tensors
    assert isinstance(packed_non_tensors, dict)
    mixed = []
    is_tensor_list = packed_non_tensors['is_tensor']
    objects = packed_non_tensors['objects']
    assert len(tensors) + len(objects) == len(is_tensor_list)
    obj_i = tnsr_i = 0
    for is_tensor in is_tensor_list:
        if is_tensor:
            mixed.append(tensors[tnsr_i])
            tnsr_i += 1
        else:
            mixed.append(objects[obj_i])
            obj_i += 1
    return tuple(mixed)


class CheckpointFunction(torch.autograd.Function):
    """Similar to the torch version, but support non-Tensor outputs.

    The caller is expected to provide a dict (*parent_ctx_dict*) that will hold
    the non-Tensor outputs. These should be combined with the Tensor *outputs*
    by calling ``unpack_non_tensors``.
    """

    @staticmethod
    def forward(ctx, run_function, parent_ctx_dict, kwarg_keys, *args):
        if torch.is_grad_enabled():
            checkpoint.check_backward_validity(args)
        ctx.run_function = run_function
        ctx.kwarg_keys = kwarg_keys
        ctx.fwd_rng_state = utils.get_rng_state()
        tensor_inputs, packed_non_tensor_inputs = split_non_tensors(args)
        if parent_ctx_dict['offload']:
            ctx.fwd_device = tuple(x.device for x in tensor_inputs)
            ctx.grad_requirements = tuple(x.requires_grad for x in tensor_inputs)
            tensor_inputs = tuple(x for x in tensor_inputs)
        else:
            ctx.fwd_device, ctx.grad_requirements = None, None
        ctx.save_for_backward(*tensor_inputs)
        ctx.packed_non_tensor_inputs = packed_non_tensor_inputs
        with torch.no_grad():
            unpacked_args, unpacked_kwargs = unpack_kwargs(kwarg_keys, args)
            outputs = run_function(*unpacked_args, **unpacked_kwargs)
        if isinstance(outputs, torch.Tensor):
            return outputs
        else:
            outputs, packed_non_tensor_outputs = split_non_tensors(outputs)
            parent_ctx_dict['packed_non_tensor_outputs'] = packed_non_tensor_outputs
            return outputs

    @staticmethod
    def backward(ctx, *args):
        if not torch.autograd._is_checkpoint_valid():
            raise RuntimeError('Checkpointing is not compatible with .grad(), please use .backward() if possible')
        tensor_inputs: 'Tuple' = ctx.saved_tensors
        tensor_inputs = checkpoint.detach_variable(tensor_inputs)
        if ctx.fwd_device is not None:
            tensor_inputs = [t for i, t in enumerate(tensor_inputs)]
            for i, need_grad in enumerate(ctx.grad_requirements):
                tensor_inputs[i].requires_grad = need_grad
        inputs = unpack_non_tensors(tensor_inputs, ctx.packed_non_tensor_inputs)
        bwd_rng_state = utils.get_rng_state()
        utils.set_rng_state(ctx.fwd_rng_state)
        with torch.enable_grad():
            unpacked_args, unpacked_kwargs = unpack_kwargs(ctx.kwarg_keys, inputs)
            outputs = ctx.run_function(*unpacked_args, **unpacked_kwargs)
            tensor_outputs, _ = split_non_tensors(outputs)
        utils.set_rng_state(bwd_rng_state)
        outputs_with_grad = []
        args_with_grad = []
        for i in range(len(tensor_outputs)):
            if tensor_outputs[i].requires_grad:
                outputs_with_grad.append(tensor_outputs[i])
                args_with_grad.append(args[i])
        if len(outputs_with_grad) == 0:
            raise RuntimeError('None of the outputs have requires_grad=True, this checkpoint() is not necessary')
        torch.autograd.backward(outputs_with_grad, args_with_grad)
        grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else None for inp in inputs)
        return (None, None, None) + grads


def pack_kwargs(*args, **kwargs) ->Tuple[List[str], List[Any]]:
    """
    Usage::

        kwarg_keys, flat_args = pack_kwargs(1, 2, a=3, b=4)
        args, kwargs = unpack_kwargs(kwarg_keys, flat_args)
        assert args == [1, 2]
        assert kwargs == {"a": 3, "b": 4}
    """
    kwarg_keys = []
    flat_args = list(args)
    for k, v in kwargs.items():
        kwarg_keys.append(k)
        flat_args.append(v)
    return kwarg_keys, flat_args


def _checkpointed_forward(original_forward, offload_to_cpu, *args, **kwargs):
    kwarg_keys, flat_args = pack_kwargs(*args, **kwargs)
    parent_ctx_dict = {'offload': offload_to_cpu}
    output = CheckpointFunction.apply(original_forward, parent_ctx_dict, kwarg_keys, *flat_args)
    if isinstance(output, torch.Tensor):
        return output
    else:
        packed_non_tensor_outputs = parent_ctx_dict['packed_non_tensor_outputs']
        if packed_non_tensor_outputs:
            output = unpack_non_tensors(output, packed_non_tensor_outputs)
        return output


def checkpoint_wrapper(m, offload_to_cpu=False):
    """
    A friendlier wrapper for performing activation checkpointing.

    Compared to the PyTorch version, this version:
    - wraps an nn.Module, so that all subsequent calls will use checkpointing
    - handles keyword arguments in the forward
    - handles non-Tensor outputs from the forward

    Usage::

        checkpointed_module = checkpoint_wrapper(my_module, offload_to_cpu=True)
        a, b = checkpointed_module(x, y=3, z=torch.Tensor([1]))
    """
    assert not hasattr(m, 'precheckpoint_forward'), 'checkpoint function has already been applied?'
    m.precheckpoint_forward = m.forward
    m.forward = functools.partial(_checkpointed_forward, m.precheckpoint_forward, offload_to_cpu)
    return m


def fsdp_wrap(module, min_num_params: 'Optional[int]'=None, **kwargs):
    """
    Helper to wrap layers/modules in FSDP. This falls back to a no-op if
    fairscale is not available.

    Args:
        module (nn.Module): module to (maybe) wrap
        min_num_params (int, Optional): minimum number of layer params to wrap
    """
    try:
        if min_num_params is not None:
            num_params = sum(p.numel() for p in module.parameters())
            if num_params >= min_num_params:
                return wrap(module, **kwargs)
            else:
                return module
        else:
            return wrap(module, **kwargs)
    except ImportError:
        return module


def is_xla_tensor(tensor):
    return torch.is_tensor(tensor) and tensor.device.type == 'xla'


def index_put(tensor, indices, value):
    if is_xla_tensor(tensor):
        for _ in range(indices.dim(), tensor.dim()):
            indices = indices.unsqueeze(-1)
        if indices.size(-1) < tensor.size(-1):
            indices = indices.expand_as(tensor)
        tensor = torch.mul(tensor, ~indices) + torch.mul(value, indices)
    else:
        tensor[indices] = value
    return tensor


def init_bert_params(module):
    """
    Initialize the weights specific to the BERT Model.
    This overrides the default initializations depending on the specified arguments.
        1. If normal_init_linear_weights is set then weights of linear
           layer will be initialized using the normal distribution and
           bais will be set to the specified value.
        2. If normal_init_embed_weights is set then weights of embedding
           layer will be initialized using the normal distribution.
        3. If normal_init_proj_weights is set then weights of
           in_project_weight for MultiHeadAttention initialized using
           the normal distribution (to be validated).
    """

    def normal_(data):
        data.copy_(data.cpu().normal_(mean=0.0, std=0.02))
    if isinstance(module, nn.Linear):
        normal_(module.weight.data)
        if module.bias is not None:
            module.bias.data.zero_()
    if isinstance(module, nn.Embedding):
        normal_(module.weight.data)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
    if isinstance(module, MultiheadAttention):
        normal_(module.q_proj.weight.data)
        normal_(module.k_proj.weight.data)
        normal_(module.v_proj.weight.data)


def make_conv_pos(e, k, g, is_batch_norm=False):
    pos_conv = nn.Conv1d(e, e, kernel_size=k, padding=k // 2, groups=g)
    dropout = 0
    std = math.sqrt(4 * (1.0 - dropout) / (k * e))
    nn.init.normal_(pos_conv.weight, mean=0, std=std)
    nn.init.constant_(pos_conv.bias, 0)
    if not is_batch_norm:
        pos_conv = nn.utils.weight_norm(pos_conv, name='weight', dim=2)
        pos_conv = nn.Sequential(pos_conv, SamePad(k), nn.GELU())
    else:
        batch_norm = nn.BatchNorm1d(e)
        pos_conv = nn.Sequential(batch_norm, pos_conv, SamePad(k), nn.GELU())
    return pos_conv


def pad_to_multiple(x, multiple, dim=-1, value=0):
    if x is None:
        return None, 0
    tsz = x.size(dim)
    m = tsz / multiple
    remainder = math.ceil(m) * multiple - tsz
    if m.is_integer():
        return x, 0
    pad_offset = (0,) * (-1 - dim) * 2
    return F.pad(x, (*pad_offset, 0, remainder), value=value), remainder


class TransformerEncoder(nn.Module):

    def build_encoder_layer(self, args: 'Wav2Vec2Config', **kwargs):
        if args.layer_type == 'transformer':
            layer = TransformerSentenceEncoderLayer(embedding_dim=self.embedding_dim, ffn_embedding_dim=args.encoder_ffn_embed_dim, num_attention_heads=args.encoder_attention_heads, dropout=self.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, activation_fn=args.activation_fn, layer_norm_first=args.layer_norm_first)
        elif args.layer_type == 'conformer':
            layer = ConformerWav2Vec2EncoderLayer(embed_dim=self.embedding_dim, ffn_embed_dim=args.encoder_ffn_embed_dim, attention_heads=args.encoder_attention_heads, dropout=args.dropout, depthwise_conv_kernel_size=args.depthwise_conv_kernel_size, activation_fn='swish', attn_type=args.attn_type, use_fp16=args.fp16, pos_enc_type='abs')
        elif args.layer_type == 'trf_adp':
            use_adp = False
            if args.adp_trf_idx == 'all':
                use_adp = True
            else:
                adp_trf_idx = list(range(*[int(g) for g in args.adp_trf_idx.split(':')]))
                if kwargs.get('layer_idx', None) in adp_trf_idx:
                    use_adp = True
            if use_adp:
                layer = TransformerSentenceEncoderWithAdapterLayer(embedding_dim=self.embedding_dim, ffn_embedding_dim=args.encoder_ffn_embed_dim, num_attention_heads=args.encoder_attention_heads, dropout=self.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, activation_fn=args.activation_fn, layer_norm_first=args.layer_norm_first, adapter_num=args.adp_num, adapter_dim=args.adp_dim, adapter_act_fn=args.adp_act_fn)
            else:
                layer = TransformerSentenceEncoderLayer(embedding_dim=self.embedding_dim, ffn_embedding_dim=args.encoder_ffn_embed_dim, num_attention_heads=args.encoder_attention_heads, dropout=self.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, activation_fn=args.activation_fn, layer_norm_first=args.layer_norm_first)
        layer = fsdp_wrap(layer)
        if args.checkpoint_activations:
            layer = checkpoint_wrapper(layer)
        return layer

    def __init__(self, args: 'Wav2Vec2Config', skip_pos_conv: 'bool'=False, override_encoder_layer: 'int'=None):
        super().__init__()
        self.dropout = args.dropout
        self.embedding_dim = args.encoder_embed_dim
        self.required_seq_len_multiple = args.required_seq_len_multiple
        pos_conv_depth = getattr(args, 'pos_conv_depth', 1)
        if pos_conv_depth > 1:
            num_layers = args.pos_conv_depth
            k = max(3, args.conv_pos // num_layers)

            def make_conv_block(e, k, g, l):
                return nn.Sequential(*[nn.Sequential(nn.Conv1d(e, e, kernel_size=k, padding=k // 2, groups=g), SamePad(k), TransposeLast(), LayerNorm(e, elementwise_affine=False), TransposeLast(), nn.GELU()) for _ in range(l)])
            self.pos_conv = make_conv_block(self.embedding_dim, k, args.conv_pos_groups, num_layers)
        elif skip_pos_conv:
            self.pos_conv = None
        else:
            self.pos_conv = make_conv_pos(self.embedding_dim, args.conv_pos, args.conv_pos_groups, is_batch_norm=args.conv_pos_batch_norm if hasattr(args, 'conv_pos_batch_norm') else False)
        if override_encoder_layer is None:
            encoder_layers = args.encoder_layers
        else:
            encoder_layers = override_encoder_layer
        self.layers = nn.ModuleList([self.build_encoder_layer(args, layer_idx=ii) for ii in range(encoder_layers)])
        self.layer_norm_first = args.layer_norm_first
        self.layer_norm = LayerNorm(self.embedding_dim)
        self.layerdrop = args.encoder_layerdrop
        self.apply(init_bert_params)

    def forward(self, x, padding_mask=None, layer=None, corpus_key=None):
        x, layer_results = self.extract_features(x, padding_mask, layer, corpus_key=corpus_key)
        if self.layer_norm_first and layer is None:
            x = self.layer_norm(x)
        return x, layer_results

    def extract_features(self, x, padding_mask=None, tgt_layer=None, min_layer=0, corpus_key=None):
        if padding_mask is not None:
            x = index_put(x, padding_mask, 0)
        if self.pos_conv is not None:
            x_conv = self.pos_conv(x.transpose(1, 2))
            x_conv = x_conv.transpose(1, 2)
            x = x + x_conv
        if not self.layer_norm_first:
            x = self.layer_norm(x)
        x, pad_length = pad_to_multiple(x, self.required_seq_len_multiple, dim=-2, value=0)
        if pad_length > 0 and padding_mask is None:
            padding_mask = x.new_zeros((x.size(0), x.size(1)), dtype=torch.bool)
            padding_mask[:, -pad_length:] = True
        else:
            padding_mask, _ = pad_to_multiple(padding_mask, self.required_seq_len_multiple, dim=-1, value=True)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = x.transpose(0, 1)
        layer_results = []
        r = None
        for i, layer in enumerate(self.layers):
            dropout_probability = np.random.random() if self.layerdrop > 0 else 1
            if not self.training or dropout_probability > self.layerdrop:
                layer_check = layer
                if isinstance(layer, FullyShardedDataParallel):
                    layer_check = layer.unwrapped_module
                if corpus_key is None or not isinstance(layer_check, (TransformerSentenceEncoderWithAdapterLayer,)):
                    x, (z, lr) = layer(x, self_attn_padding_mask=padding_mask, need_weights=False)
                else:
                    x, (z, lr) = layer(x, self_attn_padding_mask=padding_mask, need_weights=False, corpus_key=corpus_key)
                if i >= min_layer:
                    layer_results.append((x, z, lr))
            if i == tgt_layer:
                r = x
                break
        if r is not None:
            x = r
        x = x.transpose(0, 1)
        if pad_length > 0:
            x = x[:, :-pad_length]

            def undo_pad(a, b, c):
                return a[:-pad_length], b[:-pad_length] if b is not None else b, c[:-pad_length]
            layer_results = [undo_pad(*u) for u in layer_results]
        return x, layer_results

    def max_positions(self):
        """Maximum output length supported by the encoder."""
        return self.args.max_positions

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade a (possibly old) state dict for new versions of fairseq."""
        return state_dict


class AltAttention(nn.Module):

    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, cosine_attention=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.cosine_attention = cosine_attention
        if cosine_attention:
            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)

    def forward(self, x, padding_mask=None, alibi_bias=None):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        dtype = q.dtype
        if self.cosine_attention:
            attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)
            logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1.0 / 0.01))).exp()
            attn = attn * logit_scale
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
        if alibi_bias is not None:
            attn = attn.type_as(alibi_bias)
            attn[:, :alibi_bias.size(1)] += alibi_bias
        if padding_mask is not None and padding_mask.any():
            attn = attn.masked_fill(padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
        attn = attn.softmax(dim=-1, dtype=torch.float32)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2)
        x = x.reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class AltBlock(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop=0.0, attn_drop=0.0, mlp_drop=0.0, post_mlp_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm, layer_norm_first=True, ffn_targets=False, cosine_attention=False):
        super().__init__()
        self.layer_norm_first = layer_norm_first
        self.ffn_targets = ffn_targets
        self.norm1 = norm_layer(dim)
        self.attn = AltAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, cosine_attention=cosine_attention)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=mlp_drop)
        self.post_mlp_dropout = nn.Dropout(post_mlp_drop, inplace=False)

    def forward(self, x, padding_mask=None, alibi_bias=None):
        if self.layer_norm_first:
            x = x + self.drop_path(self.attn(self.norm1(x), padding_mask, alibi_bias))
            r = x = self.mlp(self.norm2(x))
            t = x
            x = r + self.drop_path(self.post_mlp_dropout(x))
            if not self.ffn_targets:
                t = x
        else:
            x = x + self.drop_path(self.attn(x, padding_mask, alibi_bias))
            r = x = self.norm1(x)
            x = self.mlp(x)
            t = x
            x = self.norm2(r + self.drop_path(self.post_mlp_dropout(x)))
            if not self.ffn_targets:
                t = x
        return x, t


class GradMultiply(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, scale):
        ctx.scale = scale
        res = x.new(x)
        return res

    @staticmethod
    def backward(ctx, grad):
        return grad * ctx.scale, None


MaskInfo = namedtuple('MaskInfo', ['x_unmasked', 'mask', 'ids_restore', 'ids_keep'])


MaskSeed = namedtuple('MaskSeed', ['seed', 'update', 'ids'])


def _learned_alibi_bias(alibi_bias, batch_size, time_steps, heads, scale, dtype, device):
    assert alibi_bias.size(1) == heads, alibi_bias.shape
    assert alibi_bias.dtype == dtype, alibi_bias.dtype
    assert alibi_bias.device == device, alibi_bias.device
    if alibi_bias.size(-1) < time_steps:
        psz = math.ceil((time_steps - alibi_bias.size(-1)) / 2)
        alibi_bias = F.pad(alibi_bias, (psz, psz, psz, psz), mode='replicate')
    alibi_bias = alibi_bias.expand(batch_size, -1, -1, -1) * scale
    return alibi_bias[..., :time_steps, :time_steps]


def compute_mask_indices(shape: 'Tuple[int, int]', padding_mask: 'Optional[torch.Tensor]', mask_prob: 'float', mask_length: 'int', mask_type: 'str'='static', mask_other: 'float'=0.0, min_masks: 'int'=0, no_overlap: 'bool'=False, min_space: 'int'=0, require_same_masks: 'bool'=True, mask_dropout: 'float'=0.0, add_masks: 'bool'=False, seed: 'Optional[int]'=None, epoch: 'Optional[int]'=None, indices: 'Optional[torch.Tensor]'=None, idc_select_ver: 'int'=1, num_mask_ver: 'int'=2) ->np.ndarray:
    """
    Computes random mask spans for a given shape

    Args:
        shape: the the shape for which to compute masks.
            should be of size 2 where first element is batch size and 2nd is timesteps
        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements
        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by
            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.
            however due to overlaps, the actual number will be smaller (unless no_overlap is True)
        mask_type: how to compute mask lengths
            static = fixed size
            uniform = sample from uniform distribution [mask_other, mask_length*2]
            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element
            poisson = sample from possion distribution with lambda = mask length
        min_masks: minimum number of masked spans
        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping
        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans
        require_same_masks: if true, will randomly drop out masks until same amount of masks remains in each sample
        mask_dropout: randomly dropout this percentage of masks in each example
    """
    bsz, all_sz = shape
    mask = np.full((bsz, all_sz), False)
    if num_mask_ver == 1:
        all_num_mask = int(mask_prob * all_sz / float(mask_length) + np.random.rand())
        all_num_mask = max(min_masks, all_num_mask)
    mask_idcs = []
    for i in range(bsz):
        if seed is not None and epoch is not None and indices is not None:
            seed_i = int(hash((seed, epoch, indices[i].item())) % 1000000.0)
        else:
            seed_i = None
        rng = np.random.default_rng(seed_i)
        if padding_mask is not None:
            sz = all_sz - padding_mask[i].long().sum().item()
            assert sz >= 0, sz
        else:
            sz = all_sz
        if num_mask_ver == 1:
            if padding_mask is not None:
                num_mask = int(mask_prob * sz / float(mask_length) + np.random.rand())
                num_mask = max(min_masks, num_mask)
            else:
                num_mask = all_num_mask
        elif num_mask_ver == 2:
            num_mask = int(mask_prob * sz / float(mask_length) + rng.random())
            num_mask = max(min_masks, num_mask)
        else:
            raise ValueError()
        if mask_type == 'static':
            lengths = np.full(num_mask, mask_length)
        elif mask_type == 'uniform':
            lengths = rng.randint(mask_other, mask_length * 2 + 1, size=num_mask)
        elif mask_type == 'normal':
            lengths = rng.normal(mask_length, mask_other, size=num_mask)
            lengths = [max(1, int(round(x))) for x in lengths]
        elif mask_type == 'poisson':
            lengths = rng.poisson(mask_length, size=num_mask)
            lengths = [int(round(x)) for x in lengths]
        else:
            raise Exception('unknown mask selection ' + mask_type)
        if sum(lengths) == 0:
            if mask_type == 'static':
                raise ValueError(f'this should never happens')
            else:
                lengths = [min(mask_length, sz - 1)]
        if no_overlap:
            mask_idc = []

            def arrange(s, e, length, keep_length):
                span_start = rng.randint(s, e - length)
                mask_idc.extend(span_start + i for i in range(length))
                new_parts = []
                if span_start - s - min_space >= keep_length:
                    new_parts.append((s, span_start - min_space + 1))
                if e - span_start - length - min_space > keep_length:
                    new_parts.append((span_start + length + min_space, e))
                return new_parts
            parts = [(0, sz)]
            min_length = min(lengths)
            for length in sorted(lengths, reverse=True):
                lens = np.fromiter((e - s if e - s >= length + min_space else 0 for s, e in parts), np.int)
                l_sum = np.sum(lens)
                if l_sum == 0:
                    break
                probs = lens / np.sum(lens)
                c = rng.choice(len(parts), p=probs)
                s, e = parts.pop(c)
                parts.extend(arrange(s, e, length, min_length))
            mask_idc = np.asarray(mask_idc)
        else:
            if idc_select_ver == 1:
                min_len = min(lengths)
                if sz - min_len <= num_mask:
                    min_len = sz - num_mask - 1
                mask_idc = rng.choice(sz - min_len, num_mask, replace=False)
            elif idc_select_ver == 2:
                mask_idc = rng.choice(sz, num_mask, replace=False)
            else:
                raise ValueError()
            mask_idc = np.asarray([(mask_idc[j] + offset) for j in range(len(mask_idc)) for offset in range(lengths[j])])
        mask_idc = np.unique(mask_idc[mask_idc < sz])
        if len(mask_idc) >= sz:
            raise ValueError(f'the entire sequence is masked. sz={sz}; mask_idc[mask_idc]; index={indices[i] if indices is not None else None}')
        mask_idcs.append(mask_idc)
    target_len = None
    if require_same_masks:
        if add_masks:
            target_len = max([len(m) for m in mask_idcs])
        else:
            target_len = min([len(m) for m in mask_idcs])
    for i, mask_idc in enumerate(mask_idcs):
        if target_len is not None and len(mask_idc) > target_len:
            mask_idc = rng.choice(mask_idc, target_len, replace=False)
        mask[i, mask_idc] = True
        if target_len is not None and len(mask_idc) < target_len:
            unmasked = np.flatnonzero(~mask[i])
            to_mask = rng.choice(unmasked, target_len - len(mask_idc), replace=False)
            mask[i, to_mask] = True
        if mask_dropout > 0:
            masked = np.flatnonzero(mask[i])
            num_holes = np.rint(len(masked) * mask_dropout).astype(int)
            to_drop = rng.choice(masked, num_holes, replace=False)
            mask[i, to_drop] = False
    return mask


def gather_unmasked(x: 'torch.Tensor', mask_info: 'MaskInfo') ->torch.Tensor:
    return torch.gather(x, dim=1, index=mask_info.ids_keep)


def gather_unmasked_mask(x: 'torch.Tensor', mask_info: 'MaskInfo') ->torch.Tensor:
    return torch.gather(x, dim=1, index=mask_info.ids_keep[..., 0])


def masked_alibi(alibi_bias, mask_indices, orig_B, orig_T):
    alibi_bias = alibi_bias.view(orig_B, -1, orig_T, orig_T)
    H = alibi_bias.size(1)
    alibi_mask = mask_indices.unsqueeze(1)
    alibi_bias = alibi_bias.masked_select(alibi_mask.unsqueeze(-1))
    alibi_bias = alibi_bias.view(orig_B, H, -1, orig_T)
    M = alibi_bias.size(-2)
    alibi_bias = alibi_bias.masked_select(alibi_mask.unsqueeze(-2))
    alibi_bias = alibi_bias.view(-1, M, M)
    return alibi_bias


def random_masking(x, mask_ratio, mask_seed: 'Optional[MaskSeed]'):
    N, L, D = x.shape
    len_keep = int(L * (1 - mask_ratio))
    generator = None
    if mask_seed is not None:
        seed = int(hash((mask_seed.seed, mask_seed.update, mask_seed.ids.sum().item())) % 1000000.0)
        generator = torch.Generator(device=x.device)
        generator.manual_seed(seed)
    noise = torch.rand(N, L, generator=generator, device=x.device)
    ids_shuffle = noise.argsort(dim=1)
    ids_restore = ids_shuffle.argsort(dim=1)
    ids_keep = ids_shuffle[:, :len_keep]
    ids_keep = ids_keep.unsqueeze(-1).expand(-1, -1, D)
    x_unmasked = torch.gather(x, dim=1, index=ids_keep)
    mask = torch.ones([N, L], dtype=x.dtype, device=x.device)
    mask[:, :len_keep] = 0
    mask = torch.gather(mask, dim=1, index=ids_restore)
    ids_restore = ids_restore.unsqueeze(-1).expand(-1, -1, D)
    return MaskInfo(x_unmasked=x_unmasked, mask=mask, ids_restore=ids_restore, ids_keep=ids_keep)


class ModalitySpecificEncoder(nn.Module):

    def __init__(self, modality_cfg: 'D2vModalityConfig', embed_dim: 'int', local_encoder: 'nn.Module', project_features: 'nn.Module', fixed_positional_encoder: 'Optional[nn.Module]', relative_positional_encoder: 'Optional[nn.Module]', context_encoder: 'nn.Module', decoder: 'nn.Module', get_alibi_bias: 'Optional[Callable[[int, int, str, str], torch.Tensor]]'):
        super().__init__()
        self.modality_cfg = modality_cfg
        self.local_encoder = local_encoder
        self.project_features = project_features
        self.fixed_positional_encoder = fixed_positional_encoder
        self.relative_positional_encoder = relative_positional_encoder
        self.context_encoder = context_encoder
        self.decoder = decoder
        self.get_alibi_bias = get_alibi_bias if modality_cfg.use_alibi_encoder else None
        self.local_grad_mult = self.modality_cfg.local_grad_mult
        self.extra_tokens = None
        if modality_cfg.num_extra_tokens > 0:
            self.extra_tokens = nn.Parameter(torch.zeros(1, modality_cfg.num_extra_tokens, embed_dim))
            if not modality_cfg.init_extra_token_zero:
                nn.init.normal_(self.extra_tokens)
            elif self.extra_tokens.size(1) > 1:
                nn.init.normal_(self.extra_tokens[:, 1:])
        self.alibi_scale = None
        if self.get_alibi_bias is not None:
            self.alibi_scale = nn.Parameter(torch.full((modality_cfg.prenet_depth + modality_cfg.model_depth if modality_cfg.learned_alibi_scale_per_layer else 1, 1, self.modality_cfg.num_alibi_heads if modality_cfg.learned_alibi_scale_per_head else 1, 1, 1), modality_cfg.alibi_scale, dtype=torch.float), requires_grad=modality_cfg.learned_alibi_scale)
        if modality_cfg.learned_alibi and self.get_alibi_bias is not None:
            assert modality_cfg.alibi_max_pos is not None
            alibi_bias = self.get_alibi_bias(batch_size=1, time_steps=modality_cfg.alibi_max_pos, heads=modality_cfg.num_alibi_heads, scale=1.0, dtype=torch.float, device='cpu')
            self.alibi_bias = nn.Parameter(alibi_bias)
            self.get_alibi_bias = partial(_learned_alibi_bias, alibi_bias=self.alibi_bias)

    def upgrade_state_dict_named(self, state_dict, name):
        k = f'{name}.alibi_scale'
        if k in state_dict and state_dict[k].dim() == 4:
            state_dict[k] = state_dict[k].unsqueeze(0)
        return state_dict

    def convert_padding_mask(self, x, padding_mask):
        return padding_mask

    def decoder_input(self, x, mask_info: 'MaskInfo'):
        inp_drop = self.modality_cfg.decoder.input_dropout
        if inp_drop > 0:
            x = F.dropout(x, inp_drop, training=self.training, inplace=True)
        num_extra = self.modality_cfg.num_extra_tokens
        if mask_info is not None:
            num_masked = mask_info.ids_restore.shape[1] - x.shape[1] + num_extra
            mask_tokens = x.new_empty(x.size(0), num_masked, x.size(-1)).normal_(0, self.modality_cfg.mask_noise_std)
            x_ = torch.cat([x[:, num_extra:], mask_tokens], dim=1)
            x = torch.gather(x_, dim=1, index=mask_info.ids_restore)
            if self.modality_cfg.decoder.add_positions_masked:
                assert self.fixed_positional_encoder is not None
                pos = self.fixed_positional_encoder(x, None)
                x = x + pos * mask_info.mask.unsqueeze(-1)
        else:
            x = x[:, num_extra:]
        if self.modality_cfg.decoder.add_positions_all:
            assert self.fixed_positional_encoder is not None
            x = x + self.fixed_positional_encoder(x, None)
        return x, mask_info

    def local_features(self, features):
        if self.local_grad_mult > 0:
            if self.local_grad_mult == 1.0:
                x = self.local_encoder(features)
            else:
                x = GradMultiply.apply(self.local_encoder(features), self.local_grad_mult)
        else:
            with torch.no_grad():
                x = self.local_encoder(features)
        x = self.project_features(x)
        return x

    def contextualized_features(self, x, padding_mask, mask, remove_masked, clone_batch: 'int'=1, mask_seeds: 'Optional[torch.Tensor]'=None, precomputed_mask=None):
        if padding_mask is not None:
            padding_mask = self.convert_padding_mask(x, padding_mask)
        local_features = x
        if mask and clone_batch == 1:
            local_features = local_features.clone()
        orig_B, orig_T, _ = x.shape
        pre_mask_B = orig_B
        mask_info = None
        x_pos = None
        if self.fixed_positional_encoder is not None:
            x = x + self.fixed_positional_encoder(x, padding_mask)
        if mask:
            if clone_batch > 1:
                x = x.repeat_interleave(clone_batch, 0)
                if mask_seeds is not None:
                    clone_hash = [int(hash((mask_seeds.seed, ind)) % 10000000000.0) for ind in range(clone_batch - 1)]
                    clone_hash = torch.tensor([0] + clone_hash).long().view(1, -1)
                    id = mask_seeds.ids
                    id = id.repeat_interleave(clone_batch, 0)
                    id = id.view(-1, clone_batch) + clone_hash
                    id = id.view(-1)
                    mask_seeds = MaskSeed(seed=mask_seeds.seed, update=mask_seeds.update, ids=id)
                if padding_mask is not None:
                    padding_mask = padding_mask.repeat_interleave(clone_batch, 0)
            x, mask_info = self.compute_mask(x, padding_mask, mask_seed=mask_seeds, apply=self.relative_positional_encoder is not None or not remove_masked, precomputed_mask=precomputed_mask)
        if self.relative_positional_encoder is not None:
            x_pos = self.relative_positional_encoder(x)
        masked_padding_mask = padding_mask
        if mask and remove_masked:
            x = mask_info.x_unmasked
            if x_pos is not None:
                x = x + gather_unmasked(x_pos, mask_info)
            if padding_mask is not None and padding_mask.any():
                masked_padding_mask = gather_unmasked_mask(padding_mask, mask_info)
                if not masked_padding_mask.any():
                    masked_padding_mask = None
            else:
                masked_padding_mask = None
        elif x_pos is not None:
            x = x + x_pos
        alibi_bias = None
        alibi_scale = self.alibi_scale
        if self.get_alibi_bias is not None:
            alibi_bias = self.get_alibi_bias(batch_size=pre_mask_B, time_steps=orig_T, heads=self.modality_cfg.num_alibi_heads, dtype=torch.float32, device=x.device)
            if alibi_scale is not None:
                alibi_scale = alibi_scale.clamp_min(0)
                if alibi_scale.size(0) == 1:
                    alibi_bias = alibi_bias * alibi_scale.squeeze(0).type_as(alibi_bias)
                    alibi_scale = None
            if clone_batch > 1:
                alibi_bias = alibi_bias.repeat_interleave(clone_batch, 0)
            if mask_info is not None and remove_masked:
                alibi_bias = masked_alibi(alibi_bias, mask_info)
        if self.extra_tokens is not None:
            num = self.extra_tokens.size(1)
            x = torch.cat([self.extra_tokens.expand(x.size(0), -1, -1), x], dim=1)
            if masked_padding_mask is not None:
                masked_padding_mask = F.pad(masked_padding_mask, (num, 0))
            if alibi_bias is not None:
                alibi_bias = F.pad(alibi_bias, (num, 0, num, 0))
        x = self.context_encoder(x, masked_padding_mask, alibi_bias, alibi_scale[:self.modality_cfg.prenet_depth] if alibi_scale is not None else None)
        return {'x': x, 'local_features': local_features, 'padding_mask': masked_padding_mask, 'alibi_bias': alibi_bias, 'alibi_scale': alibi_scale[self.modality_cfg.prenet_depth:] if alibi_scale is not None and alibi_scale.size(0) > 1 else alibi_scale, 'encoder_mask': mask_info}

    def forward(self, features, padding_mask, mask: 'bool', remove_masked: 'bool', clone_batch: 'int'=1, mask_seeds: 'Optional[torch.Tensor]'=None, precomputed_mask=None):
        x = self.local_features(features)
        return self.contextualized_features(x, padding_mask, mask, remove_masked, clone_batch, mask_seeds, precomputed_mask)

    def reset_parameters(self):
        pass

    def compute_mask(self, x, padding_mask, mask_seed: 'Optional[MaskSeed]', apply, precomputed_mask):
        if precomputed_mask is not None:
            mask = precomputed_mask
            mask_info = self.make_maskinfo(x, mask)
        else:
            B, T, C = x.shape
            cfg = self.modality_cfg
            mask_prob = cfg.mask_prob
            if cfg.mask_prob_min is not None and cfg.mask_prob_min >= 0 and cfg.mask_prob_min < mask_prob:
                mask_prob = np.random.uniform(cfg.mask_prob_min, mask_prob)
            if mask_prob > 0:
                if cfg.mask_length == 1:
                    mask_info = random_masking(x, mask_prob, mask_seed)
                else:
                    if self.modality_cfg.inverse_mask:
                        mask_prob = 1 - mask_prob
                    mask = compute_mask_indices((B, T), padding_mask, mask_prob, cfg.mask_length, min_masks=1, require_same_masks=True, mask_dropout=cfg.mask_dropout, add_masks=cfg.add_masks, seed=mask_seed.seed if mask_seed is not None else None, epoch=mask_seed.update if mask_seed is not None else None, indices=mask_seed.ids if mask_seed is not None else None)
                    mask = torch.from_numpy(mask)
                    if self.modality_cfg.inverse_mask:
                        mask = 1 - mask
                    mask_info = self.make_maskinfo(x, mask)
            else:
                mask_info = None
        if apply:
            x = self.apply_mask(x, mask_info)
        return x, mask_info

    def make_maskinfo(self, x, mask, shape=None):
        if shape is None:
            B, T, D = x.shape
        else:
            B, T, D = shape
        mask = mask
        ids_shuffle = mask.argsort(dim=1)
        ids_restore = ids_shuffle.argsort(dim=1).unsqueeze(-1).expand(-1, -1, D)
        len_keep = T - mask[0].sum()
        if self.modality_cfg.keep_masked_pct > 0:
            len_keep += round((T - int(len_keep)) * self.modality_cfg.keep_masked_pct)
        ids_keep = ids_shuffle[:, :len_keep]
        if shape is not None:
            x_unmasked = None
        else:
            ids_keep = ids_keep.unsqueeze(-1).expand(-1, -1, D)
            x_unmasked = torch.gather(x, dim=1, index=ids_keep)
        mask_info = MaskInfo(x_unmasked=x_unmasked, mask=mask, ids_restore=ids_restore, ids_keep=ids_keep)
        return mask_info

    def apply_mask(self, x, mask_info):
        cfg = self.modality_cfg
        B, T, C = x.shape
        if mask_info is not None:
            mask = mask_info.mask
            if cfg.encoder_zero_mask:
                x = x * (1 - mask.type_as(x).unsqueeze(-1))
            else:
                num_masks = mask.sum().item()
                masks = x.new_empty(num_masks, x.size(-1)).normal_(0, cfg.mask_noise_std)
                x = index_put(x, mask, masks)
        if cfg.mask_channel_prob > 0:
            mask_channel = compute_mask_indices((B, C), None, cfg.mask_channel_prob, cfg.mask_channel_length)
            mask_channel = torch.from_numpy(mask_channel).unsqueeze(1).expand(-1, T, -1)
            x = index_put(x, mask_channel, 0)
        return x

    def remove_pretraining_modules(self, keep_decoder=False):
        if not keep_decoder:
            self.decoder = None


class BlockEncoder(nn.Module):

    def __init__(self, blocks, norm_layer, layer_norm_first, layerdrop, dropout):
        super().__init__()
        self.blocks = blocks
        self.norm = norm_layer
        self.layer_norm_first = layer_norm_first
        self.layerdrop = layerdrop
        self.dropout = nn.Dropout(dropout, inplace=True)

    def forward(self, x, padding_mask, alibi_bias, alibi_scale):
        if self.norm is not None and not self.layer_norm_first:
            x = self.norm(x)
        x = self.dropout(x)
        for i, blk in enumerate(self.blocks):
            if not self.training or self.layerdrop == 0 or np.random.random() > self.layerdrop:
                ab = alibi_bias
                if ab is not None and alibi_scale is not None:
                    scale = alibi_scale[i] if alibi_scale.size(0) > 1 else alibi_scale.squeeze(0)
                    ab = ab * scale.type_as(ab)
                x, _ = blk(x, padding_mask, ab)
        if self.norm is not None and self.layer_norm_first:
            x = self.norm(x)
        return x


class DecoderBase(nn.Module):
    decoder_cfg: 'D2vDecoderConfig'

    def __init__(self, cfg: 'D2vDecoderConfig'):
        super().__init__()
        self.decoder_cfg = cfg

    def reset_parameters(self):
        for mod in self.proj.modules():
            if isinstance(mod, nn.Linear):
                mod.reset_parameters()

    def add_residual(self, x, residual, i, mask_info):
        if residual is None or not self.decoder_cfg.decoder_residual or residual.size(1) != x.size(1):
            return x
        ret = x + residual
        return ret


class SamePad2d(nn.Module):

    def __init__(self, kernel_size):
        super().__init__()
        self.remove = 1 if kernel_size % 2 == 0 else 0

    def forward(self, x):
        assert len(x.size()) == 4
        if self.remove > 0:
            x = x[:, :, :-self.remove, :-self.remove]
        return x


class Decoder2d(DecoderBase):

    def __init__(self, cfg: 'D2vDecoderConfig', input_dim, h_size, w_size):
        super().__init__(cfg)
        self.h_size = h_size
        self.w_size = w_size

        def make_block(in_dim):
            block = [nn.Conv2d(in_dim, cfg.decoder_dim, kernel_size=cfg.decoder_kernel, padding=cfg.decoder_kernel // 2, groups=cfg.decoder_groups), SamePad2d(cfg.decoder_kernel), TransposeLast(tranpose_dim=-3), LayerNorm(cfg.decoder_dim, elementwise_affine=False), TransposeLast(tranpose_dim=-3), nn.GELU()]
            return nn.Sequential(*block)
        self.blocks = nn.Sequential(*[make_block(input_dim if i == 0 else cfg.decoder_dim) for i in range(cfg.decoder_layers)])
        self.proj = nn.Linear(cfg.decoder_dim, input_dim)

    def forward(self, x, mask_info):
        B, T, C = x.shape
        x = x.transpose(1, 2).reshape(B, C, self.h_size, self.w_size)
        residual = x
        for i, layer in enumerate(self.blocks):
            x = layer(x)
            x = self.add_residual(x, residual, i, mask_info)
            residual = x
        x = x.reshape(B, -1, T).transpose(1, 2)
        x = self.proj(x)
        return x


class EncDecAttention(nn.Module):

    def __init__(self, q_dim, kv_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, cosine_attention=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = q_dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.q_proj = nn.Linear(q_dim, q_dim, bias=qkv_bias)
        self.kv_proj = nn.Linear(kv_dim, 2 * q_dim, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(q_dim, q_dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.cosine_attention = cosine_attention
        if cosine_attention:
            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)

    def forward(self, q, kv, padding_mask=None, alibi_bias=None):
        B, N, C = q.shape
        q = self.q_proj(q).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        kv = self.kv_proj(kv).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        k, v = kv[0], kv[1]
        dtype = q.dtype
        if self.cosine_attention:
            attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)
            logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1.0 / 0.01))).exp()
            attn = attn * logit_scale
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
        if alibi_bias is not None:
            attn = attn.type_as(alibi_bias)
            attn[:, :alibi_bias.size(1)] += alibi_bias
        if padding_mask is not None and padding_mask.any():
            attn = attn.masked_fill(padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
        attn = attn.softmax(dim=-1, dtype=torch.float32)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2)
        x = x.reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class EncDecBlock(nn.Module):

    def __init__(self, q_dim, kv_dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop=0.0, attn_drop=0.0, mlp_drop=0.0, post_mlp_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm, layer_norm_first=True, cosine_attention=False, first_residual=True):
        super().__init__()
        self.layer_norm_first = layer_norm_first
        self.norm1 = norm_layer(q_dim)
        self.attn = EncDecAttention(q_dim, kv_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, cosine_attention=cosine_attention)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(q_dim)
        mlp_hidden_dim = int(q_dim * mlp_ratio)
        self.mlp = Mlp(in_features=q_dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=mlp_drop)
        self.post_mlp_dropout = nn.Dropout(post_mlp_drop, inplace=False)
        self.first_residual = first_residual

    def forward(self, q, kv, padding_mask=None, alibi_bias=None):
        r = q if self.first_residual else 0
        if self.layer_norm_first:
            x = r + self.drop_path(self.attn(self.norm1(q), kv, padding_mask, alibi_bias))
            r = x = self.mlp(self.norm2(x))
            x = r + self.drop_path(self.post_mlp_dropout(x))
        else:
            x = r + self.drop_path(self.attn(q, kv, padding_mask, alibi_bias))
            r = x = self.norm1(x)
            x = self.mlp(x)
            x = self.norm2(r + self.drop_path(self.post_mlp_dropout(x)))
        return x


class EncDecTransformerDecoder(nn.Module):

    def __init__(self, cfg: 'D2vDecoderConfig', input_dim):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, cfg.decoder_dim)
        self.blocks = nn.Sequential(*[EncDecBlock(q_dim=cfg.decoder_dim, kv_dim=input_dim, num_heads=8, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, mlp_drop=0.0, post_mlp_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm, layer_norm_first=False, cosine_attention=False, first_residual=i > 0) for i in range(cfg.decoder_layers)])
        self.proj = nn.Linear(cfg.decoder_dim, input_dim)

    def reset_parameters(self):
        self.apply(init_bert_params)

    def forward(self, x, kv):
        x = self.input_proj(x)
        for i, layer in enumerate(self.blocks):
            x = layer(x, kv)
        x = self.proj(x)
        return x


class FixedPositionalEncoder(nn.Module):

    def __init__(self, pos_embed):
        super().__init__()
        self.positions = pos_embed

    def forward(self, x, padding_mask):
        return self.positions


def Linear(in_features, out_features, bias=True):
    m = nn.Linear(in_features, out_features, bias)
    nn.init.xavier_uniform_(m.weight)
    if bias:
        nn.init.constant_(m.bias, 0.0)
    return m


class LearnedPositionalEmbedding(nn.Embedding):
    """
    This module learns positional embeddings up to a fixed maximum size.
    Padding ids are ignored by either offsetting based on padding_idx
    or by setting padding_idx to None and ensuring that the appropriate
    position ids are passed to the forward function.
    """

    def __init__(self, num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'int'):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        self.onnx_trace = False
        if self.padding_idx is not None:
            self.max_positions = self.num_embeddings - self.padding_idx - 1
        else:
            self.max_positions = self.num_embeddings

    def forward(self, input: 'Tensor', incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, positions: 'Optional[Tensor]'=None):
        """Input is expected to be of size [bsz x seqlen]."""
        assert positions is None or self.padding_idx is None, 'If positions is pre-computed then padding_idx should not be set.'
        if positions is None:
            if incremental_state is not None:
                positions = torch.zeros((1, 1), device=input.device, dtype=input.dtype).fill_(int(self.padding_idx + input.size(1)))
            else:
                positions = utils.make_positions(input, self.padding_idx, onnx_trace=self.onnx_trace)
        return F.embedding(positions, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)


class SinusoidalPositionalEmbedding(nn.Module):
    """This module produces sinusoidal positional embeddings of any length.

    Padding symbols are ignored.
    """

    def __init__(self, embedding_dim, padding_idx, init_size=1024, auto_expand=True):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.padding_idx = padding_idx if padding_idx is not None else 0
        self.register_buffer('weights', SinusoidalPositionalEmbedding.get_embedding(init_size, embedding_dim, padding_idx), persistent=False)
        self.max_positions = int(100000.0)
        self.auto_expand = auto_expand
        self.onnx_trace = False

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):
        deprecated_keys = ['weights', '_float_tensor']
        for key in deprecated_keys:
            if prefix + key in state_dict:
                del state_dict[prefix + key]
        super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)

    @staticmethod
    def get_embedding(num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'Optional[int]'=None):
        """Build sinusoidal embeddings.

        This matches the implementation in tensor2tensor, but differs slightly
        from the description in Section 3.5 of "Attention Is All You Need".
        """
        half_dim = embedding_dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)
        if embedding_dim % 2 == 1:
            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
        if padding_idx is not None:
            emb[padding_idx, :] = 0
        return emb

    def forward(self, input, incremental_state: 'Optional[Any]'=None, timestep: 'Optional[Tensor]'=None, positions: 'Optional[Any]'=None):
        """Input is expected to be of size [bsz x seqlen]."""
        bspair = torch.onnx.operators.shape_as_tensor(input)
        bsz, seq_len = bspair[0], bspair[1]
        max_pos = self.padding_idx + 1 + seq_len
        weights = self.weights
        if max_pos > self.weights.size(0):
            weights = SinusoidalPositionalEmbedding.get_embedding(max_pos, self.embedding_dim, self.padding_idx)
            if self.auto_expand:
                self.weights = weights
        if incremental_state is not None:
            pos = timestep.view(-1)[0] + 1 if timestep is not None else seq_len
            if self.onnx_trace:
                return weights.index_select(index=self.padding_idx + pos, dim=0).unsqueeze(1).repeat(bsz, 1, 1)
            return weights[self.padding_idx + pos, :].expand(bsz, 1, -1)
        positions = utils.make_positions(input, self.padding_idx, onnx_trace=self.onnx_trace)
        if self.onnx_trace:
            flat_embeddings = weights.detach().index_select(0, positions.view(-1))
            embedding_shape = torch.cat((bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long)))
            embeddings = torch.onnx.operators.reshape_from_tensor_shape(flat_embeddings, embedding_shape)
            return embeddings
        return weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()


def PositionalEmbedding(num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'int', learned: 'bool'=False, auto_expand: 'bool'=True):
    if learned:
        if padding_idx is not None:
            num_embeddings = num_embeddings + padding_idx + 1
        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)
        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)
        if padding_idx is not None:
            nn.init.constant_(m.weight[padding_idx], 0)
    else:
        m = SinusoidalPositionalEmbedding(embedding_dim, padding_idx, init_size=num_embeddings + padding_idx + 1, auto_expand=auto_expand)
    return m


DEFAULT_MAX_SOURCE_POSITIONS = 1024


DEFAULT_MAX_TARGET_POSITIONS = 1024


DEFAULT_MIN_PARAMS_TO_WRAP = int(100000000.0)


_NAME_PARSER = '(decoder|encoder|quant_noise)_(.*)'


def safe_getattr(obj, k, default=None):
    """Returns obj[k] if it exists and is not None, otherwise returns default."""
    if OmegaConf.is_config(obj):
        return obj[k] if k in obj and obj[k] is not None else default
    return getattr(obj, k, default)


def safe_hasattr(obj, k):
    """Returns True if the given key exists and is not None."""
    return getattr(obj, k, None) is not None


class TransformerDecoderLayerBase(nn.Module):
    """Decoder layer block.

    In the original paper each operation (multi-head attention, encoder
    attention or FFN) is postprocessed with: `dropout -> add residual ->
    layernorm`. In the tensor2tensor code they suggest that learning is more
    robust when preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.decoder.normalize_before* to ``True``.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).
    """

    def __init__(self, cfg, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):
        super().__init__()
        self.embed_dim = cfg.decoder.embed_dim
        self.dropout_module = FairseqDropout(cfg.dropout, module_name=self.__class__.__name__)
        self.quant_noise = cfg.quant_noise.pq
        self.quant_noise_block_size = cfg.quant_noise.pq_block_size
        self.cross_self_attention = cfg.cross_self_attention
        self.self_attn = self.build_self_attention(self.embed_dim, cfg, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn)
        self.attn_ln = LayerNorm(self.embed_dim) if utils.safe_getattr(cfg, 'scale_attn', False) else None
        self.nh = self.self_attn.num_heads
        self.head_dim = self.self_attn.head_dim
        scale_heads = utils.safe_getattr(cfg, 'scale_heads', False)
        self.c_attn = nn.Parameter(torch.ones((self.nh,)), requires_grad=True) if scale_heads else None
        self.activation_fn = utils.get_activation_fn(activation=cfg.activation_fn)
        activation_dropout_p = cfg.activation_dropout
        if activation_dropout_p == 0:
            activation_dropout_p = cfg.relu_dropout or 0
        self.activation_dropout_module = FairseqDropout(float(activation_dropout_p), module_name=self.__class__.__name__)
        self.normalize_before = cfg.decoder.normalize_before
        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        if no_encoder_attn:
            self.encoder_attn = None
            self.encoder_attn_layer_norm = None
        else:
            self.encoder_attn = self.build_encoder_attention(self.embed_dim, cfg)
            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.ffn_layernorm = LayerNorm(cfg.decoder.ffn_embed_dim) if utils.safe_getattr(cfg, 'scale_fc', False) else None
        self.w_resid = nn.Parameter(torch.ones(self.embed_dim), requires_grad=True) if utils.safe_getattr(cfg, 'scale_resids', False) else None
        self.fc1 = self.build_fc1(self.embed_dim, cfg.decoder.ffn_embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.fc2 = self.build_fc2(cfg.decoder.ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.final_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.need_attn = True
        self.onnx_trace = False

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_self_attention(self, embed_dim, cfg, add_bias_kv=False, add_zero_attn=False):
        return MultiheadAttention(embed_dim, cfg.decoder.attention_heads, dropout=cfg.attention_dropout, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=not cfg.cross_self_attention, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, xformers_att_config=cfg.decoder.xformers_att_config)

    def build_encoder_attention(self, embed_dim, cfg):
        return MultiheadAttention(embed_dim, cfg.decoder.attention_heads, kdim=cfg.encoder.embed_dim, vdim=cfg.encoder.embed_dim, dropout=cfg.attention_dropout, encoder_decoder_attention=True, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, xformers_att_config=cfg.encoder.xformers_att_config)

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def residual_connection(self, x, residual):
        return residual + x

    def forward(self, x, encoder_out: 'Optional[torch.Tensor]'=None, encoder_padding_mask: 'Optional[torch.Tensor]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, prev_self_attn_state: 'Optional[List[torch.Tensor]]'=None, prev_attn_state: 'Optional[List[torch.Tensor]]'=None, self_attn_mask: 'Optional[torch.Tensor]'=None, self_attn_padding_mask: 'Optional[torch.Tensor]'=None, need_attn: 'bool'=False, need_head_weights: 'bool'=False):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor, optional): binary
                ByteTensor of shape `(batch, src_len)` where padding
                elements are indicated by ``1``.
            need_attn (bool, optional): return attention weights
            need_head_weights (bool, optional): return attention weights
                for each head (default: return average over heads).

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if need_head_weights:
            need_attn = True
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if prev_self_attn_state is not None:
            prev_key, prev_value = prev_self_attn_state[:2]
            saved_state: 'Dict[str, Optional[Tensor]]' = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_self_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_self_attn_state[2]
            assert incremental_state is not None
            self.self_attn._set_input_buffer(incremental_state, saved_state)
        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)
        if self.cross_self_attention and not (incremental_state is not None and _self_attn_input_buffer is not None and 'prev_key' in _self_attn_input_buffer):
            if self_attn_mask is not None:
                assert encoder_out is not None
                self_attn_mask = torch.cat((x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1)
            if self_attn_padding_mask is not None:
                if encoder_padding_mask is None:
                    assert encoder_out is not None
                    encoder_padding_mask = self_attn_padding_mask.new_zeros(encoder_out.size(1), encoder_out.size(0))
                self_attn_padding_mask = torch.cat((encoder_padding_mask, self_attn_padding_mask), dim=1)
            assert encoder_out is not None
            y = torch.cat((encoder_out, x), dim=0)
        else:
            y = x
        x, attn = self.self_attn(query=x, key=y, value=y, key_padding_mask=self_attn_padding_mask, incremental_state=incremental_state, need_weights=False, attn_mask=self_attn_mask)
        if self.c_attn is not None:
            tgt_len, bsz = x.size(0), x.size(1)
            x = x.view(tgt_len, bsz, self.nh, self.head_dim)
            x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)
            x = x.reshape(tgt_len, bsz, self.embed_dim)
        if self.attn_ln is not None:
            x = self.attn_ln(x)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if self.encoder_attn is not None and encoder_out is not None:
            residual = x
            if self.normalize_before:
                x = self.encoder_attn_layer_norm(x)
            if prev_attn_state is not None:
                prev_key, prev_value = prev_attn_state[:2]
                saved_state: 'Dict[str, Optional[Tensor]]' = {'prev_key': prev_key, 'prev_value': prev_value}
                if len(prev_attn_state) >= 3:
                    saved_state['prev_key_padding_mask'] = prev_attn_state[2]
                assert incremental_state is not None
                self.encoder_attn._set_input_buffer(incremental_state, saved_state)
            x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
            x = self.dropout_module(x)
            x = self.residual_connection(x, residual)
            if not self.normalize_before:
                x = self.encoder_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        if self.ffn_layernorm is not None:
            x = self.ffn_layernorm(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        if self.w_resid is not None:
            residual = torch.mul(self.w_resid, residual)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.onnx_trace and incremental_state is not None:
            saved_state = self.self_attn._get_input_buffer(incremental_state)
            assert saved_state is not None
            if self_attn_padding_mask is not None:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value'], saved_state['prev_key_padding_mask']]
            else:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value']]
            return x, attn, self_attn_state
        return x, attn, None

    def make_generation_fast_(self, need_attn: 'bool'=False, **kwargs):
        self.need_attn = need_attn


class TransformerDecoderLayer(TransformerDecoderLayerBase):

    def __init__(self, args, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):
        super().__init__(TransformerConfig.from_namespace(args), no_encoder_attn=no_encoder_attn, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn)
        self.args = args

    def build_self_attention(self, embed_dim, args, add_bias_kv=False, add_zero_attn=False):
        return super().build_self_attention(embed_dim, TransformerConfig.from_namespace(args), add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn)

    def build_encoder_attention(self, embed_dim, args):
        return super().build_encoder_attention(embed_dim, TransformerConfig.from_namespace(args))


class TransformerDecoder(FairseqIncrementalDecoder):
    """
    Transformer decoder consisting of *args.decoder_layers* layers. Each layer
    is a :class:`TransformerDecoderLayer`.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): decoding dictionary
        embed_tokens (torch.nn.Embedding): output embedding
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).
    """

    def __init__(self, cfg: 'Wav2Vec2Seq2SeqConfig', dictionary, embed_tokens, no_encoder_attn=False):
        super().__init__(dictionary)
        self.dropout = cfg.decoder_dropout
        self.share_input_output_embed = cfg.share_decoder_input_output_embed
        input_embed_dim = embed_tokens.embedding_dim
        embed_dim = cfg.decoder_embed_dim
        self.output_embed_dim = cfg.decoder_embed_dim
        self.layerdrop = cfg.decoder_layerdrop
        self.padding_idx = embed_tokens.padding_idx
        self.max_target_positions = cfg.max_target_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None
        self.embed_positions = PositionalEmbedding(cfg.max_target_positions, embed_dim, self.padding_idx, learned=cfg.decoder_learned_pos) if not cfg.no_token_positional_embeddings else None
        transformer_cfg = copy.deepcopy(cfg)
        with open_dict(transformer_cfg):
            transformer_cfg.dropout = transformer_cfg.decoder_dropout
            transformer_cfg.attention_dropout = transformer_cfg.decoder_attention_dropout
            transformer_cfg.activation_dropout = transformer_cfg.decoder_activation_dropout
        self.layers = nn.ModuleList([])
        self.layers.extend([TransformerDecoderLayer(transformer_cfg, no_encoder_attn) for _ in range(transformer_cfg.decoder_layers)])
        if not self.share_input_output_embed:
            self.embed_out = nn.Parameter(torch.Tensor(len(dictionary), self.output_embed_dim))
            nn.init.normal_(self.embed_out, mean=0, std=self.output_embed_dim ** -0.5)
        if transformer_cfg.decoder_normalize_before:
            self.layer_norm = LayerNorm(embed_dim)
        else:
            self.layer_norm = None

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused):
        """
        Args:
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (Tensor, optional): output from the encoder, used for
                encoder-side attention
            incremental_state (dict): dictionary used for storing state during
                :ref:`Incremental decoding`

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        if type(prev_output_tokens) == list:
            max_len = max(len(x) for x in prev_output_tokens)
            tmp = torch.zeros([len(prev_output_tokens), max_len], device=prev_output_tokens[0].device)
            for i, p in enumerate(prev_output_tokens):
                tmp[i, :len(p)] = p
            prev_output_tokens = tmp
        prev_output_tokens = prev_output_tokens.long()
        x, extra = self.extract_features(prev_output_tokens, encoder_out, incremental_state)
        x = self.output_layer(x)
        return x, extra

    def extract_features(self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused):
        """
        Similar to *forward* but only return features.

        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state) if self.embed_positions is not None else None
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = x.transpose(0, 1)
        attn = None
        inner_states = [x]
        self_attn_padding_mask = None
        if prev_output_tokens.eq(self.padding_idx).any():
            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)
        for layer in self.layers:
            dropout_probability = np.random.random()
            if not self.training or dropout_probability > self.layerdrop:
                x, attn, _ = layer(x, encoder_out['encoder_out'] if encoder_out is not None else None, encoder_out['padding_mask'] if encoder_out is not None else None, incremental_state, self_attn_mask=self.buffered_future_mask(x) if incremental_state is None else None, self_attn_padding_mask=self_attn_padding_mask)
                inner_states.append(x)
        if self.layer_norm:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        return x, {'attn': attn, 'inner_states': inner_states}

    def output_layer(self, features, **kwargs):
        """Project features to the vocabulary size."""
        if self.share_input_output_embed:
            return F.linear(features, self.embed_tokens.weight)
        else:
            return F.linear(features, self.embed_out)

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        if self.embed_positions is None:
            return self.max_target_positions
        return min(self.max_target_positions, self.embed_positions.max_positions)

    def buffered_future_mask(self, tensor):
        dim = tensor.size(0)
        if not hasattr(self, '_future_mask') or self._future_mask is None or self._future_mask.device != tensor.device or self._future_mask.size(0) < dim:
            self._future_mask = torch.triu(utils.fill_with_neg_inf(tensor.new(dim, dim)), 1)
        return self._future_mask[:dim, :dim]

    def upgrade_state_dict_named(self, state_dict, name):
        return state_dict


def compute_block_mask_2d(shape: 'Tuple[int, int]', mask_prob: 'float', mask_length: 'int', mask_prob_adjust: 'float'=0, inverse_mask: 'bool'=False, require_same_masks: 'bool'=True, expand_adjcent: 'bool'=False, mask_dropout: 'float'=0, non_overlapping: 'bool'=False) ->torch.Tensor:
    assert mask_length > 1
    B, L = shape
    d = int(L ** 0.5)
    if inverse_mask:
        mask_prob = 1 - mask_prob
    if non_overlapping:
        sz = math.ceil(d / mask_length)
        inp_len = sz * sz
        inp = torch.zeros((B, 1, sz, sz))
        w = torch.ones((1, 1, mask_length, mask_length))
        mask_inds = torch.multinomial(1 - inp.view(B, -1), int(inp_len * (mask_prob + mask_prob_adjust) * (1 + mask_dropout)), replacement=False)
        inp.view(B, -1).scatter_(1, mask_inds, 1)
        mask = torch.nn.functional.conv_transpose2d(inp, w, stride=mask_length).squeeze(1)
        if mask.size(-1) > d:
            mask = mask[..., :d, :d]
    else:
        mask = torch.zeros((B, d, d))
        mask_inds = torch.randint(0, L, size=(B, int(L * ((mask_prob + mask_prob_adjust) / mask_length ** 2) * (1 + mask_dropout))))
        mask.view(B, -1).scatter_(1, mask_inds, 1)
        centers = mask.nonzero(as_tuple=True)
        inds = [], [], []
        offset = mask_length // 2
        for i in range(mask_length):
            for j in range(mask_length):
                k1 = i - offset
                k2 = j - offset
                inds[0].append(centers[0])
                inds[1].append(centers[1] + k1)
                inds[2].append(centers[2] + k2)
        i0 = torch.cat(inds[0])
        i1 = torch.cat(inds[1]).clamp_(min=0, max=d - 1)
        i2 = torch.cat(inds[2]).clamp_(min=0, max=d - 1)
        mask[i0, i1, i2] = 1

    def get_nbs(b, m, w):
        all_nbs = torch.nn.functional.conv2d(m.unsqueeze(1), w, padding='same')
        all_nbs = all_nbs.clamp_max_(1).view(b, -1)
        return all_nbs
    if require_same_masks and expand_adjcent:
        w = torch.zeros((1, 1, 3, 3))
        w[..., 0, 1] = 1
        w[..., 2, 1] = 1
        w[..., 1, 0] = 1
        w[..., 1, 2] = 1
        all_nbs = get_nbs(B, mask, w)
    mask = mask.reshape(B, -1)
    if require_same_masks:
        n_masks = mask.sum(dim=-1)
        final_target_len = int(L * mask_prob)
        target_len = int(final_target_len * (1 + mask_dropout))
        for i in range(len(mask)):
            n = n_masks[i]
            m = mask[i]
            r = 0
            while expand_adjcent and n < target_len:
                if r == 0:
                    nbs = all_nbs[i]
                else:
                    nbs = get_nbs(1, m.view(1, d, d), w).flatten()
                cands = 1 - m + nbs > 1
                cand_sz = int(cands.sum().item())
                assert cand_sz > 0, f'{nbs} {cand_sz}'
                to_mask = torch.multinomial(cands.float(), min(cand_sz, int(target_len - n)), replacement=False)
                m[to_mask] = 1
                assert to_mask.numel() > 0
                n += to_mask.numel()
                r += 1
            if n > final_target_len:
                to_unmask = torch.multinomial(m, int(n - final_target_len), replacement=False)
                m[to_unmask] = 0
            elif n < final_target_len:
                to_mask = torch.multinomial(1 - m, int(final_target_len - n), replacement=False)
                m[to_mask] = 1
    if inverse_mask:
        mask = 1 - mask
    return mask


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000 ** omega
    pos = pos.reshape(-1)
    out = np.einsum('m,d->md', pos, omega)
    emb_sin = np.sin(out)
    emb_cos = np.cos(out)
    emb = np.concatenate([emb_sin, emb_cos], axis=1)
    return emb


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])
    emb = np.concatenate([emb_h, emb_w], axis=1)
    return emb


def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_alibi(max_positions: 'int', attention_heads: 'int'):

    def get_slopes(n):

        def get_slopes_power_of_2(n):
            start = 2 ** -2 ** -(math.log2(n) - 3)
            ratio = start
            return [(start * ratio ** i) for i in range(n)]
        if math.log2(n).is_integer():
            return get_slopes_power_of_2(n)
        else:
            closest_power_of_2 = 2 ** math.floor(math.log2(n))
            return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2 * closest_power_of_2)[0::2][:n - closest_power_of_2]
    maxpos = max_positions
    attn_heads = attention_heads
    slopes = torch.Tensor(get_slopes(attn_heads))
    pos_bias = torch.abs(torch.arange(maxpos).unsqueeze(0) - torch.arange(maxpos).unsqueeze(1)) * -1
    alibi_bias = slopes.unsqueeze(1).unsqueeze(1) * pos_bias.unsqueeze(0).expand(attn_heads, -1, -1)
    return alibi_bias


def get_alibi_bias(alibi_biases, batch_size, time_steps, heads, dtype, device, dims=1, distance='manhattan'):
    cache_key = f'{dims}_{heads}_{distance}'
    buffered = alibi_biases.get(cache_key, None)
    target_size = heads * batch_size
    if buffered is None or buffered.size(0) < target_size or buffered.size(1) < time_steps or buffered.dtype != dtype or buffered.device != device:
        bt = max(time_steps, buffered.size(1) if buffered is not None else 0)
        bn = max(target_size, buffered.size(0) if buffered is not None else 0) // heads
        buffered = get_alibi(bt, heads, dims=dims, distance=distance).repeat(bn, 1, 1)
        alibi_biases[cache_key] = buffered
    b = buffered[:target_size, :time_steps, :time_steps]
    b = b.view(batch_size, heads, time_steps, time_steps)
    return b


class ImageEncoder(ModalitySpecificEncoder):
    modality_cfg: 'D2vImageConfig'

    def __init__(self, modality_cfg: 'D2vImageConfig', embed_dim: 'int', make_block: 'Callable[[float, Optional[int], Optional[int]], nn.ModuleList]', norm_layer: 'Callable[[int], nn.LayerNorm]', layer_norm_first: 'bool', alibi_biases: 'Dict', task: 'Optional[FairseqTask]'):
        img_size = to_2tuple(modality_cfg.input_size)
        patch_size = to_2tuple(modality_cfg.patch_size)
        num_patches = img_size[1] // patch_size[1] * (img_size[0] // patch_size[0])
        local_encoder = PatchEmbed(modality_cfg.input_size, modality_cfg.patch_size, modality_cfg.in_chans, modality_cfg.embed_dim)
        w = local_encoder.proj.weight.data
        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))
        if modality_cfg.embed_dim != embed_dim:
            local_encoder = nn.Sequential(local_encoder, nn.Linear(modality_cfg.embed_dim, embed_dim))
        project_features = nn.Identity()
        pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim), requires_grad=False)
        side_n = int(num_patches ** 0.5)
        emb = get_2d_sincos_pos_embed(pos_embed.shape[-1], side_n, cls_token=False)
        pos_embed.data.copy_(torch.from_numpy(emb).float().unsqueeze(0))
        fixed_positional_encoder = FixedPositionalEncoder(pos_embed) if modality_cfg.fixed_positions else None
        dpr = np.linspace(modality_cfg.start_drop_path_rate, modality_cfg.end_drop_path_rate, modality_cfg.prenet_depth)
        context_encoder = BlockEncoder(nn.ModuleList(make_block(dpr[i]) for i in range(modality_cfg.prenet_depth)), norm_layer(embed_dim) if not layer_norm_first else None, layer_norm_first, modality_cfg.prenet_layerdrop, modality_cfg.prenet_dropout)
        if modality_cfg.transformer_decoder:
            if modality_cfg.enc_dec_transformer:
                decoder = EncDecTransformerDecoder(modality_cfg.decoder, embed_dim)
            else:
                dec_enc = BlockEncoder(nn.ModuleList(make_block(0, modality_cfg.decoder.decoder_dim, 8) for _ in range(modality_cfg.decoder.decoder_layers)), None, layer_norm_first, 0, 0)
                decoder = TransformerDecoder(modality_cfg.decoder, embed_dim, dec_enc)
        else:
            decoder = Decoder2d(modality_cfg.decoder, embed_dim, side_n, side_n) if modality_cfg.decoder is not None else None
        alibi_bias_fn = partial(get_alibi_bias, alibi_biases=alibi_biases, heads=modality_cfg.num_alibi_heads, dims=modality_cfg.alibi_dims, distance=modality_cfg.alibi_distance)
        super().__init__(modality_cfg=modality_cfg, embed_dim=embed_dim, local_encoder=local_encoder, project_features=project_features, fixed_positional_encoder=fixed_positional_encoder, relative_positional_encoder=None, context_encoder=context_encoder, decoder=decoder, get_alibi_bias=alibi_bias_fn)

    def reset_parameters(self):
        super().reset_parameters()
        if self.decoder is not None:
            self.decoder.reset_parameters()

    @torch.no_grad()
    def patchify(self, imgs):
        """
        imgs: (N, 3, H, W)
        x: (N, L, patch_size**2 *3)
        """
        p = self.modality_cfg.patch_size
        h = w = imgs.shape[2] // p
        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))
        x = torch.einsum('nchpwq->nhwpqc', x)
        x = x.reshape(shape=(imgs.shape[0], h * w, p ** 2 * 3))
        return x

    @torch.no_grad()
    def unpatchify(self, x):
        """
        x: (N, L, patch_size**2 *3)
        imgs: (N, 3, H, W)
        """
        p = self.modality_cfg.patch_size
        h = w = int(x.shape[1] ** 0.5)
        assert h * w == x.shape[1]
        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))
        x = torch.einsum('nhwpqc->nchpwq', x)
        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))
        return imgs

    def compute_mask(self, x, padding_mask, mask_seed: 'Optional[MaskSeed]', apply, shape=None, precomputed_mask=None):
        mlen = self.modality_cfg.mask_length
        if mlen <= 1:
            return super().compute_mask(x, padding_mask, mask_seed, apply, precomputed_mask)
        if precomputed_mask is not None:
            mask = precomputed_mask
        else:
            if shape is not None:
                B, L, D = shape
            else:
                B, L, D = x.shape
            mask = compute_block_mask_2d(shape=(B, L), mask_prob=self.modality_cfg.mask_prob, mask_length=self.modality_cfg.mask_length, mask_prob_adjust=self.modality_cfg.mask_prob_adjust, inverse_mask=self.modality_cfg.inverse_mask, require_same_masks=True, mask_dropout=self.modality_cfg.mask_dropout)
        mask_info = self.make_maskinfo(x, mask, shape)
        if apply:
            x = self.apply_mask(x, mask_info)
        return x, mask_info

    def decoder_input(self, x, mask_info):
        if not self.modality_cfg.transformer_decoder or not self.modality_cfg.enc_dec_transformer:
            return super().decoder_input(x, mask_info)
        inp_drop = self.modality_cfg.decoder.input_dropout
        if inp_drop > 0:
            x = F.dropout(x, inp_drop, training=self.training, inplace=True)
        kv = x[:, self.modality_cfg.num_extra_tokens:]
        assert self.fixed_positional_encoder is not None
        pos = self.fixed_positional_encoder(x, None).expand(x.size(0), -1, -1)
        mask = mask_info.mask.bool()
        if self.modality_cfg.decoder.add_positions_all:
            kv = kv + pos[~mask].view(kv.shape)
        q = pos[mask].view(x.size(0), -1, x.size(-1))
        return q, kv


class TextFeatPositionalEncoder(nn.Module):
    """
    Original encoder expects (B, T) long input. This module wraps it to take
    local_encoder output which are (B, T, D) float tensors
    """

    def __init__(self, pos_encoder):
        super().__init__()
        self.pos_encoder = pos_encoder

    def forward(self, x, padding_mask):
        return self.pos_encoder(x[..., 0])


class Decoder1d(DecoderBase):

    def __init__(self, cfg: 'D2vDecoderConfig', input_dim):
        super().__init__(cfg)

        def make_block(in_dim):
            block = [nn.Conv1d(in_dim, cfg.decoder_dim, kernel_size=cfg.decoder_kernel, padding=cfg.decoder_kernel // 2, groups=cfg.decoder_groups), SamePad(cfg.decoder_kernel), TransposeLast(), LayerNorm(cfg.decoder_dim, elementwise_affine=False), TransposeLast(), nn.GELU()]
            return nn.Sequential(*block)
        self.blocks = nn.Sequential(*[make_block(input_dim if i == 0 else cfg.decoder_dim) for i in range(cfg.decoder_layers)])
        projs = []
        curr_dim = cfg.decoder_dim
        for i in range(cfg.projection_layers - 1):
            next_dim = int(curr_dim * cfg.projection_ratio) if i == 0 else curr_dim
            projs.append(nn.Linear(curr_dim, next_dim))
            projs.append(nn.GELU())
            curr_dim = next_dim
        projs.append(nn.Linear(curr_dim, input_dim))
        if len(projs) == 1:
            self.proj = projs[0]
        else:
            self.proj = nn.Sequential(*projs)

    def forward(self, x, mask_info):
        x = x.transpose(1, 2)
        residual = x
        for i, layer in enumerate(self.blocks):
            x = layer(x)
            x = self.add_residual(x, residual, i, mask_info)
            residual = x
        x = x.transpose(1, 2)
        x = self.proj(x)
        return x


class TextLocalEncoder(nn.Module):

    def __init__(self, vocab_size, embed_dim, max_source_positions, pad_idx, no_scale_embedding, layernorm_embedding, dropout, no_token_positional_embeddings, learned_pos):
        super().__init__()
        self.pad_idx = pad_idx
        self.dropout_module = FairseqDropout(dropout)
        self.embed_tokens = nn.Embedding(vocab_size, embed_dim, pad_idx)
        self.embed_scale = 1.0 if no_scale_embedding else math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(max_source_positions, embed_dim, pad_idx, learned=learned_pos) if not no_token_positional_embeddings else None
        self.embed_scale = 1.0 if no_scale_embedding else math.sqrt(embed_dim)
        self.layernorm_embedding = None
        if layernorm_embedding:
            self.layernorm_embedding = LayerNorm(embed_dim)

    def forward(self, src_tokens):
        x = self.embed_scale * self.embed_tokens(src_tokens)
        if self.embed_positions is not None:
            x = x + self.embed_positions(src_tokens)
        if self.layernorm_embedding is not None:
            x = self.layernorm_embedding(x)
        x = self.dropout_module(x)
        return x


class TextEncoder(ModalitySpecificEncoder):
    modality_cfg: 'D2vTextConfig'

    def __init__(self, modality_cfg: 'D2vTextConfig', embed_dim: 'int', make_block: 'Callable[[float], nn.ModuleList]', norm_layer: 'Callable[[int], nn.LayerNorm]', layer_norm_first: 'bool', alibi_biases: 'Dict', task: 'Optional[FairseqTask]'):
        self.pad_idx = task.source_dictionary.pad()
        self.vocab_size = len(task.source_dictionary)
        local_encoder = TextLocalEncoder(vocab_size=self.vocab_size, embed_dim=embed_dim, max_source_positions=modality_cfg.max_source_positions, pad_idx=self.pad_idx, no_scale_embedding=modality_cfg.no_scale_embedding, layernorm_embedding=modality_cfg.layernorm_embedding, dropout=modality_cfg.dropout, no_token_positional_embeddings=modality_cfg.no_token_positional_embeddings, learned_pos=modality_cfg.learned_pos)
        dpr = np.linspace(modality_cfg.start_drop_path_rate, modality_cfg.end_drop_path_rate, modality_cfg.prenet_depth)
        context_encoder = BlockEncoder(nn.ModuleList(make_block(dpr[i]) for i in range(modality_cfg.prenet_depth)), norm_layer(embed_dim) if not layer_norm_first and modality_cfg.prenet_depth > 0 else None, layer_norm_first, modality_cfg.prenet_layerdrop, modality_cfg.prenet_dropout if modality_cfg.prenet_depth > 0 else 0.0)
        decoder = Decoder1d(modality_cfg.decoder, embed_dim) if modality_cfg.decoder is not None else None
        alibi_bias_fn = partial(get_alibi_bias, alibi_biases=alibi_biases)
        super().__init__(modality_cfg=modality_cfg, embed_dim=embed_dim, local_encoder=local_encoder, project_features=nn.Identity(), fixed_positional_encoder=None, relative_positional_encoder=None, context_encoder=context_encoder, decoder=decoder, get_alibi_bias=alibi_bias_fn)

    def reset_parameters(self):
        super().reset_parameters()

    def convert_padding_mask(self, x, padding_mask):
        if padding_mask is None or padding_mask.size(1) == x.size(1):
            return padding_mask
        diff = self.downsample - padding_mask.size(1) % self.downsample
        if 0 < diff < self.downsample:
            padding_mask = F.pad(padding_mask, (0, diff), value=True)
        padding_mask = padding_mask.view(padding_mask.size(0), -1, self.downsample)
        padding_mask = padding_mask.all(-1)
        if padding_mask.size(1) > x.size(1):
            padding_mask = padding_mask[:, :x.size(1)]
        assert x.size(1) == padding_mask.size(1), f'{x.size(1), padding_mask.size(1), diff, self.downsample}'
        return padding_mask


class BaseRanker(nn.Module):

    def __init__(self, args, task):
        super().__init__()
        self.separator_token = task.dictionary.eos()
        self.padding_idx = task.dictionary.pad()

    def forward(self, src_tokens):
        raise NotImplementedError

    def get_segment_labels(self, src_tokens):
        segment_boundary = (src_tokens == self.separator_token).long()
        segment_labels = segment_boundary.cumsum(dim=1) - segment_boundary - (src_tokens == self.padding_idx).long()
        return segment_labels

    def get_positions(self, src_tokens, segment_labels):
        segment_positions = torch.arange(src_tokens.shape[1]).repeat(src_tokens.shape[0], 1)
        segment_boundary = (src_tokens == self.separator_token).long()
        _, col_idx = (segment_positions * segment_boundary).nonzero(as_tuple=True)
        col_idx = torch.cat([torch.zeros(1).type_as(col_idx), col_idx])
        offset = torch.cat([torch.zeros(1).type_as(segment_boundary), segment_boundary.sum(dim=1).cumsum(dim=0)[:-1]])
        segment_positions -= col_idx[segment_labels + offset.unsqueeze(1)] * (segment_labels != 0)
        padding_mask = src_tokens.ne(self.padding_idx)
        segment_positions = (segment_positions + 1) * padding_mask.type_as(segment_positions) + self.padding_idx
        return segment_positions


class RobertaClassificationHead(nn.Module):
    """Head for sentence-level classification tasks."""

    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, q_noise=0, qn_block_size=8, do_spectral_norm=False):
        super().__init__()
        self.dense = nn.Linear(input_dim, inner_dim)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.dropout = nn.Dropout(p=pooler_dropout)
        self.out_proj = apply_quant_noise_(nn.Linear(inner_dim, num_classes), q_noise, qn_block_size)
        if do_spectral_norm:
            if q_noise != 0:
                raise NotImplementedError('Attempting to use Spectral Normalization with Quant Noise. This is not officially supported')
            self.out_proj = torch.nn.utils.spectral_norm(self.out_proj)

    def forward(self, features, **kwargs):
        x = features[:, 0, :]
        x = self.dropout(x)
        x = self.dense(x)
        x = self.activation_fn(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class LayerDropModuleList(nn.ModuleList):
    """
    A LayerDrop implementation based on :class:`torch.nn.ModuleList`.

    We refresh the choice of which layers to drop every time we iterate
    over the LayerDropModuleList instance. During evaluation we always
    iterate over all layers.

    Usage::

        layers = LayerDropList(p=0.5, modules=[layer1, layer2, layer3])
        for layer in layers:  # this might iterate over layers 1 and 3
            x = layer(x)
        for layer in layers:  # this might iterate over all layers
            x = layer(x)
        for layer in layers:  # this might not iterate over any layers
            x = layer(x)

    Args:
        p (float): probability of dropping out each layer
        modules (iterable, optional): an iterable of modules to add
    """

    def __init__(self, p, modules=None):
        super().__init__(modules)
        self.p = p

    def __iter__(self):
        dropout_probs = torch.empty(len(self)).uniform_()
        for i, m in enumerate(super().__iter__()):
            if not self.training or dropout_probs[i] > self.p:
                yield m


class TransformerSentenceEncoder(nn.Module):
    """
    Implementation for a Bi-directional Transformer based Sentence Encoder used
    in BERT/XLM style pre-trained models.

    This first computes the token embedding using the token embedding matrix,
    position embeddings (if specified) and segment embeddings
    (if specified). After applying the specified number of
    TransformerEncoderLayers, it outputs all the internal states of the
    encoder as well as the final representation associated with the first
    token (usually CLS token).

    Input:
        - tokens: B x T matrix representing sentences
        - segment_labels: B x T matrix representing segment label for tokens

    Output:
        - a tuple of the following:
            - a list of internal model states used to compute the
              predictions where each tensor has shape T x B x C
            - sentence representation associated with first input token
              in format B x C.
    """

    def __init__(self, padding_idx: 'int', vocab_size: 'int', num_encoder_layers: 'int'=6, embedding_dim: 'int'=768, ffn_embedding_dim: 'int'=3072, num_attention_heads: 'int'=8, dropout: 'float'=0.1, attention_dropout: 'float'=0.1, activation_dropout: 'float'=0.1, layerdrop: 'float'=0.0, max_seq_len: 'int'=256, num_segments: 'int'=2, use_position_embeddings: 'bool'=True, offset_positions_by_padding: 'bool'=True, encoder_normalize_before: 'bool'=False, apply_bert_init: 'bool'=False, activation_fn: 'str'='relu', learned_pos_embedding: 'bool'=True, embed_scale: 'float'=None, freeze_embeddings: 'bool'=False, n_trans_layers_to_freeze: 'int'=0, export: 'bool'=False, traceable: 'bool'=False, q_noise: 'float'=0.0, qn_block_size: 'int'=8) ->None:
        super().__init__()
        self.padding_idx = padding_idx
        self.vocab_size = vocab_size
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.layerdrop = layerdrop
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim
        self.num_segments = num_segments
        self.use_position_embeddings = use_position_embeddings
        self.apply_bert_init = apply_bert_init
        self.learned_pos_embedding = learned_pos_embedding
        self.traceable = traceable
        self.embed_tokens = self.build_embedding(self.vocab_size, self.embedding_dim, self.padding_idx)
        self.embed_scale = embed_scale
        if q_noise > 0:
            self.quant_noise = apply_quant_noise_(nn.Linear(self.embedding_dim, self.embedding_dim, bias=False), q_noise, qn_block_size)
        else:
            self.quant_noise = None
        self.segment_embeddings = nn.Embedding(self.num_segments, self.embedding_dim, padding_idx=None) if self.num_segments > 0 else None
        self.embed_positions = PositionalEmbedding(self.max_seq_len, self.embedding_dim, padding_idx=self.padding_idx if offset_positions_by_padding else None, learned=self.learned_pos_embedding) if self.use_position_embeddings else None
        if encoder_normalize_before:
            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)
        else:
            self.emb_layer_norm = None
        if self.layerdrop > 0.0:
            self.layers = LayerDropModuleList(p=self.layerdrop)
        else:
            self.layers = nn.ModuleList([])
        self.layers.extend([self.build_transformer_sentence_encoder_layer(embedding_dim=self.embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=self.dropout_module.p, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, export=export, q_noise=q_noise, qn_block_size=qn_block_size) for _ in range(num_encoder_layers)])
        if self.apply_bert_init:
            self.apply(init_bert_params)

        def freeze_module_params(m):
            if m is not None:
                for p in m.parameters():
                    p.requires_grad = False
        if freeze_embeddings:
            freeze_module_params(self.embed_tokens)
            freeze_module_params(self.segment_embeddings)
            freeze_module_params(self.embed_positions)
            freeze_module_params(self.emb_layer_norm)
        for layer in range(n_trans_layers_to_freeze):
            freeze_module_params(self.layers[layer])

    def build_embedding(self, vocab_size, embedding_dim, padding_idx):
        return nn.Embedding(vocab_size, embedding_dim, padding_idx)

    def build_transformer_sentence_encoder_layer(self, embedding_dim, ffn_embedding_dim, num_attention_heads, dropout, attention_dropout, activation_dropout, activation_fn, export, q_noise, qn_block_size):
        return TransformerSentenceEncoderLayer(embedding_dim=embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, export=export, q_noise=q_noise, qn_block_size=qn_block_size)

    def forward(self, tokens: 'torch.Tensor', segment_labels: 'torch.Tensor'=None, last_state_only: 'bool'=False, positions: 'Optional[torch.Tensor]'=None, token_embeddings: 'Optional[torch.Tensor]'=None, attn_mask: 'Optional[torch.Tensor]'=None) ->Tuple[torch.Tensor, torch.Tensor]:
        is_tpu = tokens.device.type == 'xla'
        padding_mask = tokens.eq(self.padding_idx)
        if not self.traceable and not is_tpu and not padding_mask.any():
            padding_mask = None
        if token_embeddings is not None:
            x = token_embeddings
        else:
            x = self.embed_tokens(tokens)
        if self.embed_scale is not None:
            x = x * self.embed_scale
        if self.embed_positions is not None:
            x = x + self.embed_positions(tokens, positions=positions)
        if self.segment_embeddings is not None and segment_labels is not None:
            x = x + self.segment_embeddings(segment_labels)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        if self.emb_layer_norm is not None:
            x = self.emb_layer_norm(x)
        x = self.dropout_module(x)
        if padding_mask is not None:
            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))
        x = x.transpose(0, 1)
        inner_states = []
        if not last_state_only:
            inner_states.append(x)
        for layer in self.layers:
            x, _ = layer(x, self_attn_padding_mask=padding_mask, self_attn_mask=attn_mask)
            if not last_state_only:
                inner_states.append(x)
        sentence_rep = x[0, :, :]
        if last_state_only:
            inner_states = [x]
        if self.traceable:
            return torch.stack(inner_states), sentence_rep
        else:
            return inner_states, sentence_rep


def update_init_roberta_model_state(state):
    """
   update the state_dict of a Roberta model for initializing
   weights of the BertRanker
   """
    for k in list(state.keys()):
        if '.lm_head.' in k or 'version' in k:
            del state[k]
            continue
        assert k.startswith('encoder.sentence_encoder.') or k.startswith('decoder.sentence_encoder.'), f'Cannot recognize parameter name {k}'
        if 'layernorm_embedding' in k:
            new_k = k.replace('.layernorm_embedding.', '.emb_layer_norm.')
            state[new_k[25:]] = state[k]
        else:
            state[k[25:]] = state[k]
        del state[k]


class BertRanker(BaseRanker):

    def __init__(self, args, task):
        super(BertRanker, self).__init__(args, task)
        init_model = getattr(args, 'pretrained_model', '')
        self.joint_layers = nn.ModuleList()
        if os.path.isfile(init_model):
            None
            x = hub_utils.from_pretrained(os.path.dirname(init_model), checkpoint_file=os.path.basename(init_model))
            in_state_dict = x['models'][0].state_dict()
            init_args = x['args'].model
            num_positional_emb = init_args.max_positions + task.dictionary.pad() + 1
            self.model = TransformerSentenceEncoder(padding_idx=task.dictionary.pad(), vocab_size=len(task.dictionary), num_encoder_layers=getattr(args, 'encoder_layers', init_args.encoder_layers), embedding_dim=init_args.encoder_embed_dim, ffn_embedding_dim=init_args.encoder_ffn_embed_dim, num_attention_heads=init_args.encoder_attention_heads, dropout=init_args.dropout, attention_dropout=init_args.attention_dropout, activation_dropout=init_args.activation_dropout, num_segments=2, max_seq_len=num_positional_emb, offset_positions_by_padding=False, encoder_normalize_before=True, apply_bert_init=True, activation_fn=init_args.activation_fn, freeze_embeddings=args.freeze_embeddings, n_trans_layers_to_freeze=args.n_trans_layers_to_freeze)
            if args.freeze_embeddings:
                for p in self.model.segment_embeddings.parameters():
                    p.requires_grad = False
            update_init_roberta_model_state(in_state_dict)
            None
            self.model.load_state_dict(in_state_dict, strict=False)
            ffn_embedding_dim = init_args.encoder_ffn_embed_dim
            num_attention_heads = init_args.encoder_attention_heads
            dropout = init_args.dropout
            attention_dropout = init_args.attention_dropout
            activation_dropout = init_args.activation_dropout
            activation_fn = init_args.activation_fn
            classifier_embed_dim = getattr(args, 'embed_dim', init_args.encoder_embed_dim)
            if classifier_embed_dim != init_args.encoder_embed_dim:
                self.transform_layer = nn.Linear(init_args.encoder_embed_dim, classifier_embed_dim)
        else:
            self.model = TransformerSentenceEncoder(padding_idx=task.dictionary.pad(), vocab_size=len(task.dictionary), num_encoder_layers=args.encoder_layers, embedding_dim=args.embed_dim, ffn_embedding_dim=args.ffn_embed_dim, num_attention_heads=args.attention_heads, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, max_seq_len=task.max_positions() if task.max_positions() else args.tokens_per_sample, num_segments=2, offset_positions_by_padding=False, encoder_normalize_before=args.encoder_normalize_before, apply_bert_init=args.apply_bert_init, activation_fn=args.activation_fn)
            classifier_embed_dim = args.embed_dim
            ffn_embedding_dim = args.ffn_embed_dim
            num_attention_heads = args.attention_heads
            dropout = args.dropout
            attention_dropout = args.attention_dropout
            activation_dropout = args.activation_dropout
            activation_fn = args.activation_fn
        self.joint_classification = args.joint_classification
        if args.joint_classification == 'sent':
            if args.joint_normalize_before:
                self.joint_layer_norm = LayerNorm(classifier_embed_dim)
            else:
                self.joint_layer_norm = None
            self.joint_layers = nn.ModuleList([TransformerSentenceEncoderLayer(embedding_dim=classifier_embed_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn) for _ in range(args.num_joint_layers)])
        self.classifier = RobertaClassificationHead(classifier_embed_dim, classifier_embed_dim, 1, 'tanh', args.classifier_dropout)

    def forward(self, src_tokens, src_lengths):
        segment_labels = self.get_segment_labels(src_tokens)
        positions = self.get_positions(src_tokens, segment_labels)
        inner_states, _ = self.model(tokens=src_tokens, segment_labels=segment_labels, last_state_only=True, positions=positions)
        return inner_states[-1].transpose(0, 1)

    def sentence_forward(self, encoder_out, src_tokens=None, sentence_rep='head'):
        if sentence_rep == 'head':
            x = encoder_out[:, :1, :]
        else:
            assert src_tokens is not None, 'meanpool requires src_tokens input'
            segment_labels = self.get_segment_labels(src_tokens)
            padding_mask = src_tokens.ne(self.padding_idx)
            encoder_mask = segment_labels * padding_mask.type_as(segment_labels)
            if sentence_rep == 'meanpool':
                ntokens = torch.sum(encoder_mask, dim=1, keepdim=True)
                x = torch.sum(encoder_out * encoder_mask.unsqueeze(2), dim=1, keepdim=True) / ntokens.unsqueeze(2).type_as(encoder_out)
            else:
                encoder_out[(encoder_mask == 0).unsqueeze(2).repeat(1, 1, encoder_out.shape[-1])] = -float('inf')
                x, _ = torch.max(encoder_out, dim=1, keepdim=True)
        if hasattr(self, 'transform_layer'):
            x = self.transform_layer(x)
        return x

    def joint_forward(self, x):
        if self.joint_layer_norm:
            x = self.joint_layer_norm(x.transpose(0, 1))
            x = x.transpose(0, 1)
        for layer in self.joint_layers:
            x, _ = layer(x, self_attn_padding_mask=None)
        return x

    def classification_forward(self, x):
        return self.classifier(x)


class Predictor(nn.Module):

    def __init__(self, n_tokens, emb_dim):
        super(Predictor, self).__init__()
        self.n_tokens = n_tokens
        self.emb_dim = emb_dim
        self.padding_token = n_tokens
        self.emb = nn.Embedding(n_tokens + 1, emb_dim, padding_idx=self.padding_token)

    def inflate_input(self, batch):
        """ get a sequence of tokens, predict their durations
        and inflate them accordingly """
        batch_durs = self.forward(batch)
        batch_durs = torch.exp(batch_durs) - 1
        batch_durs = batch_durs.round()
        output = []
        for seq, durs in zip(batch, batch_durs):
            inflated_seq = []
            for token, n in zip(seq, durs):
                if token == self.padding_token:
                    break
                n = int(n.item())
                token = int(token.item())
                inflated_seq.extend([token for _ in range(n)])
            output.append(inflated_seq)
        output = torch.LongTensor(output)
        return output


def bin2freq(x, f0_min, f0_max, bins, mode):
    n_bins = len(bins) + 1
    assert x.shape[-1] == n_bins
    bins = torch.cat([torch.tensor([f0_min]), bins])
    if mode == 'mean':
        f0 = (x * bins).sum(-1, keepdims=True) / x.sum(-1, keepdims=True)
    elif mode == 'argmax':
        idx = F.one_hot(x.argmax(-1), num_classes=n_bins)
        f0 = (idx * bins).sum(-1, keepdims=True)
    else:
        raise NotImplementedError()
    return f0[..., 0]


class CnnPredictor(nn.Module):

    def __init__(self, n_tokens, emb_dim, channels, kernel, dropout, n_layers, spk_emb, gst_emb, n_bins, f0_pred, f0_log, f0_norm):
        super(CnnPredictor, self).__init__()
        self.n_tokens = n_tokens
        self.emb_dim = emb_dim
        self.f0_log = f0_log
        self.f0_pred = f0_pred
        self.padding_token = n_tokens
        self.f0_norm = f0_norm
        self.token_emb = nn.Embedding(n_tokens + 1, emb_dim, padding_idx=self.padding_token)
        self.spk_emb = spk_emb
        self.gst_emb = nn.Embedding(20, gst_emb)
        self.setup = False
        feats = emb_dim + gst_emb
        layers = [nn.Sequential(Rearrange('b t c -> b c t'), nn.Conv1d(feats, channels, kernel_size=kernel, padding=(kernel - 1) // 2), Rearrange('b c t -> b t c'), nn.ReLU(), nn.LayerNorm(channels), nn.Dropout(dropout))]
        for _ in range(n_layers - 1):
            layers += [nn.Sequential(Rearrange('b t c -> b c t'), nn.Conv1d(channels, channels, kernel_size=kernel, padding=(kernel - 1) // 2), Rearrange('b c t -> b t c'), nn.ReLU(), nn.LayerNorm(channels), nn.Dropout(dropout))]
        self.conv_layer = nn.ModuleList(layers)
        self.proj = nn.Linear(channels, n_bins)

    def forward(self, x, gst=None):
        x = self.token_emb(x)
        feats = [x]
        if gst is not None:
            gst = self.gst_emb(gst)
            gst = rearrange(gst, 'b c -> b c 1')
            gst = F.interpolate(gst, x.shape[1])
            gst = rearrange(gst, 'b c t -> b t c')
            feats.append(gst)
        x = torch.cat(feats, dim=-1)
        for i, conv in enumerate(self.conv_layer):
            if i != 0:
                x = conv(x) + x
            else:
                x = conv(x)
        x = self.proj(x)
        x = x.squeeze(-1)
        if self.f0_pred == 'mean':
            x = torch.sigmoid(x)
        elif self.f0_pred == 'argmax':
            x = torch.softmax(x, dim=-1)
        else:
            raise NotImplementedError
        return x

    def setup_f0_stats(self, f0_min, f0_max, f0_bins, speaker_stats):
        self.f0_min = f0_min
        self.f0_max = f0_max
        self.f0_bins = f0_bins
        self.speaker_stats = speaker_stats
        self.setup = True

    def inference(self, x, spk_id=None, gst=None):
        assert self.setup == True, 'make sure that `setup_f0_stats` was called before inference!'
        probs = self(x, gst)
        f0 = bin2freq(probs, self.f0_min, self.f0_max, self.f0_bins, self.f0_pred)
        for i in range(f0.shape[0]):
            mean = self.speaker_stats[spk_id[i].item()].mean_log if self.f0_log else self.speaker_stats[spk_id[i].item()].mean
            std = self.speaker_stats[spk_id[i].item()].std_log if self.f0_log else self.speaker_stats[spk_id[i].item()].std
            if self.f0_norm == 'mean':
                f0[i] = f0[i] + mean
            if self.f0_norm == 'meanstd':
                f0[i] = f0[i] * std + mean
        if self.f0_log:
            f0 = f0.exp()
        return f0


class LaserTransformerEncoder(TransformerEncoder):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, src_tokens, *args, **kwargs):
        encoder_out = super().forward(src_tokens, *args, **kwargs)
        x = encoder_out['encoder_out'][0]
        padding_mask = src_tokens.eq(self.padding_idx).t().unsqueeze(-1)
        if padding_mask.any():
            x = x.float().masked_fill_(padding_mask, float('-inf')).type_as(x)
        sentemb = x.max(dim=0)[0]
        return {'sentemb': [sentemb]}

    @torch.jit.export
    def reorder_encoder_out(self, encoder_out: 'Dict[str, List[Tensor]]', new_order):
        """
        Same as the one in transformer.py, with new_sentemb
        """
        if len(encoder_out['sentemb']) == 0:
            new_sentemb = []
        else:
            new_sentemb = [encoder_out['sentemb'][0].index_select(0, new_order)]
        return {'sentemb': new_sentemb}


class LaserTransformerDecoder(TransformerDecoder):

    def __init__(self, args, dictionary, *kargs, **kwargs):
        self.num_langs = kwargs.get('num_langs', 1)
        self.lang_embed_dim = kwargs.get('lang_embed_dim', 0)
        kwargs.pop('num_langs', None)
        kwargs.pop('lang_embed_dim', None)
        super().__init__(args, dictionary, *kargs, **kwargs, no_encoder_attn=True)
        if self.lang_embed_dim == 0:
            self.embed_lang = None
        else:
            self.embed_lang = nn.Embedding(self.num_langs, self.lang_embed_dim)
            nn.init.uniform_(self.embed_lang.weight, -0.1, 0.1)
        if self.output_projection is not None:
            laser_output_embed_dim = self.output_embed_dim + self.lang_embed_dim + args.encoder_embed_dim
            self.output_projection = nn.Linear(laser_output_embed_dim, len(dictionary), bias=False)
            nn.init.normal_(self.output_projection.weight, mean=0, std=laser_output_embed_dim ** -0.5)

    def build_decoder_layer(self, args, no_encoder_attn=False):
        decoder_embed_dim = args.decoder_embed_dim
        args.decoder_embed_dim = decoder_embed_dim + self.lang_embed_dim + args.encoder_embed_dim
        res = TransformerDecoderLayer(args, no_encoder_attn=True)
        args.decoder_embed_dim = decoder_embed_dim
        return res

    def extract_features(self, prev_output_tokens, encoder_out: 'Optional[Dict[str, List[Tensor]]]', incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, full_context_alignment: 'bool'=False, alignment_layer: 'Optional[int]'=None, alignment_heads: 'Optional[int]'=None, lang_id: 'Optional[int]'=None):
        """
        Similar to *forward* but only return features.

        Includes several features from "Jointly Learning to Align and
        Translate with Transformer Models" (Garg et al., EMNLP 2019).

        Args:
            full_context_alignment (bool, optional): don't apply
                auto-regressive mask to self-attention (default: False).
            alignment_layer (int, optional): return mean alignment over
                heads at this layer (default: last layer).
            alignment_heads (int, optional): only average alignment over
                this many heads (default: all heads).

        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        if alignment_layer is None:
            alignment_layer = self.num_layers - 1
        positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state) if self.embed_positions is not None else None
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        bsz, seqlen = prev_output_tokens.size()
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        if self.layernorm_embedding is not None:
            x = self.layernorm_embedding(x)
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        if self.embed_lang is not None:
            lang_ids = prev_output_tokens.data.new_full((bsz,), lang_id)
            langemb = self.embed_lang(lang_ids)
            langemb = langemb.unsqueeze(0)
            repeat_vals = [x.shape[0] // langemb.shape[0]] + [-1] * (len(langemb.shape) - 1)
            x = torch.cat((x, langemb.expand(*repeat_vals)), dim=-1)
        sentemb = encoder_out['sentemb'][0]
        sentemb = sentemb.unsqueeze(0)
        repeat_vals = [x.shape[0] // sentemb.shape[0]] + [-1] * (len(sentemb.shape) - 1)
        x = torch.cat((x, sentemb.expand(*repeat_vals)), dim=-1)
        self_attn_padding_mask: 'Optional[Tensor]' = None
        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)
        attn: 'Optional[Tensor]' = None
        inner_states: 'List[Optional[Tensor]]' = [x]
        for idx, layer in enumerate(self.layers):
            if incremental_state is None and not full_context_alignment:
                self_attn_mask = self.buffered_future_mask(x)
            else:
                self_attn_mask = None
            x, layer_attn, _ = layer(x, None, None, incremental_state, self_attn_mask=self_attn_mask, self_attn_padding_mask=self_attn_padding_mask, need_attn=bool(idx == alignment_layer), need_head_weights=bool(idx == alignment_layer))
            inner_states.append(x)
            if layer_attn is not None and idx == alignment_layer:
                attn = layer_attn.float()
        if attn is not None:
            if alignment_heads is not None:
                attn = attn[:alignment_heads]
            attn = attn.mean(dim=0)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)
        return x, {'attn': [attn], 'inner_states': inner_states}

    def forward(self, prev_output_tokens, encoder_out: 'Optional[Dict[str, List[Tensor]]]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, features_only: 'bool'=False, alignment_layer: 'Optional[int]'=None, alignment_heads: 'Optional[int]'=None, src_lengths: 'Optional[Any]'=None, return_all_hiddens: 'bool'=False, lang_id: 'Optional[int]'=None):
        """
        Args:
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (optional): output from the encoder, used for
                encoder-side attention
            incremental_state (dict): dictionary used for storing state during
                :ref:`Incremental decoding`
            features_only (bool, optional): only return features without
                applying output layer (default: False).

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        assert lang_id is not None
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state, alignment_layer=alignment_layer, alignment_heads=alignment_heads, lang_id=lang_id)
        if not features_only:
            x = self.output_layer(x)
        return x, extra


class LatentLayersKLLoss(_Loss):

    def __init__(self, args):
        super().__init__()
        self.args = args

    def forward(self, layer_samples, lang_idx, update_num, sample_size):
        prior = self.args.prior
        samples = layer_samples[lang_idx]
        eps = 1e-07
        if prior == 'uniform':
            kl_loss = (samples * (torch.log(samples + eps) - math.log(0.5))).sum(-1)
        elif prior == 'agged_posterior':
            y_t = torch.stack([x.detach() for x in layer_samples], dim=0)
            agged_q = torch.sum(y_t, dim=0)
            row_norm = agged_q.sum(-1)
            normed_agg_q = agged_q / row_norm
            kl_loss = (samples * (torch.log(samples + eps) - torch.log(normed_agg_q + eps))).sum(-1)
        else:
            raise NotImplementedError('The specified prior is not implemented.')
        kl_loss /= layer_samples[0].size()[0]
        kl_weight = min(self.args.sparsity_weight, (update_num - self.args.soft_update) * self.args.sparsity_weight / self.args.anneal_updates)
        kl_loss *= kl_weight * sample_size
        return kl_loss


class LatentLayersSparsityLoss(_Loss):

    def __init__(self, args):
        super().__init__()
        self.args = args

    def is_valid(self, update_num):
        if self.args.target_layers <= 0:
            return False
        return update_num > self.args.soft_update + self.args.anneal_updates

    def forward(self, layer_samples_list, update_num, sample_size):
        batch_loss = 0
        share_loss = 0
        global_sparsity_loss = 0
        layer_samples = torch.stack(layer_samples_list, dim=0)
        if (self.args.target_layers > 0 or self.args.share_weight > 0) and update_num > self.args.soft_update + self.args.anneal_updates:
            if update_num < self.args.anneal_updates + self.args.soft_update:
                weight_anneal = 0
            elif update_num < 2 * self.args.anneal_updates + self.args.soft_update:
                weight_anneal = (update_num - self.args.soft_update - self.args.anneal_updates) * self.args.share_weight / self.args.anneal_updates
            else:
                weight_anneal = 1
            layer_utilization = torch.sum(layer_samples, dim=0)
            layer_utilization /= layer_samples.size()[0]
            if self.args.share_weight > 0:
                share_loss = sum(-1.0 * v * math.log(v) for v in layer_utilization if v > 0)
                batch_loss += weight_anneal * self.args.share_weight * sample_size * share_loss
            if self.args.target_layers > 0:
                expeted_layers = sum(layer_utilization)
                global_sparsity_loss = (expeted_layers - self.args.target_layers) ** 2
                batch_loss += weight_anneal * self.args.share_weight * sample_size * global_sparsity_loss
        return batch_loss


class TransformerEncoderLayerBase(nn.Module):
    """Encoder layer block.

    In the original paper each operation (multi-head attention or FFN) is
    postprocessed with: `dropout -> add residual -> layernorm`. In the
    tensor2tensor code they suggest that learning is more robust when
    preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.encoder.normalize_before* to ``True``.

    Args:
        cfg (argparse.Namespace): parsed command-line arguments
    """

    def __init__(self, cfg, return_fc=False):
        super().__init__()
        self.cfg = cfg
        self.return_fc = return_fc
        self.embed_dim = cfg.encoder.embed_dim
        self.quant_noise = cfg.quant_noise.pq
        self.quant_noise_block_size = cfg.quant_noise.pq_block_size
        self.self_attn = self.build_self_attention(self.embed_dim, cfg)
        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.dropout_module = FairseqDropout(cfg.dropout, module_name=self.__class__.__name__)
        self.activation_fn = utils.get_activation_fn(activation=cfg.activation_fn)
        activation_dropout_p = cfg.activation_dropout
        if activation_dropout_p == 0:
            activation_dropout_p = cfg.relu_dropout or 0
        self.activation_dropout_module = FairseqDropout(float(activation_dropout_p), module_name=self.__class__.__name__)
        self.normalize_before = cfg.encoder.normalize_before
        self.fc1 = self.build_fc1(self.embed_dim, cfg.encoder.ffn_embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.fc2 = self.build_fc2(cfg.encoder.ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.final_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size)

    def _get_fc_rank(self, remove_num: 'int') ->List[int]:
        f1_filter_param = []
        for i in range(self.fc1.out_features):
            f1_filter_param.append(torch.sum(torch.abs(self.fc1.weight[i])) + torch.sum(torch.abs(self.fc2.weight[:, i])) + torch.abs(self.fc1.bias[i]))
        return sorted(range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False)[0:remove_num]

    def _prune_fc_layer(self, remove_index: 'List[int]'):
        new_fc1_weight = []
        new_fc1_bias = []
        for i in range(self.fc1.out_features):
            if i not in remove_index:
                new_fc1_weight.append(self.fc1.weight[i])
                new_fc1_bias.append(self.fc1.bias[i])
        new_fc1_weight = torch.stack(new_fc1_weight).detach()
        new_fc1_weight.requires_grad = True
        new_fc1_bias = torch.stack(new_fc1_bias).detach()
        new_fc1_bias.requires_grad = True
        self.fc1 = quant_noise(nn.Linear(self.fc1.in_features, self.fc1.out_features - len(remove_index)), p=self.quant_noise, block_size=self.quant_noise_block_size)
        self.fc1.weight = torch.nn.Parameter(new_fc1_weight)
        self.fc1.bias = torch.nn.Parameter(new_fc1_bias)
        new_fc2_weight = []
        new_fc2_bias = []
        for i in range(self.fc2.in_features):
            if i not in remove_index:
                new_fc2_weight.append(self.fc2.weight[:, i])
        new_fc2_bias = self.fc2.bias.detach()
        new_fc2_weight = torch.stack(new_fc2_weight, dim=-1).detach()
        new_fc2_weight.requires_grad = True
        new_fc2_bias = self.fc2.bias.detach()
        new_fc2_bias.requires_grad = True
        self.fc2 = quant_noise(nn.Linear(self.fc2.in_features - len(remove_index), self.fc2.out_features), p=self.quant_noise, block_size=self.quant_noise_block_size)
        self.fc2.weight = torch.nn.Parameter(new_fc2_weight)
        self.fc2.bias = torch.nn.Parameter(new_fc2_bias)

    def build_self_attention(self, embed_dim, cfg):
        return MultiheadAttention(embed_dim, cfg.encoder.attention_heads, dropout=cfg.attention_dropout, self_attention=True, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, xformers_att_config=cfg.encoder.xformers_att_config)

    def residual_connection(self, x, residual):
        return residual + x

    def upgrade_state_dict_named(self, state_dict, name):
        """
        Rename layer norm states from `...layer_norms.0.weight` to
        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to
        `...final_layer_norm.weight`
        """
        layer_norm_map = {'0': 'self_attn_layer_norm', '1': 'final_layer_norm'}
        for old, new in layer_norm_map.items():
            for m in ('weight', 'bias'):
                k = '{}.layer_norms.{}.{}'.format(name, old, m)
                if k in state_dict:
                    state_dict['{}.{}.{}'.format(name, new, m)] = state_dict[k]
                    del state_dict[k]

    def forward(self, x, encoder_padding_mask: 'Optional[Tensor]', attn_mask: 'Optional[Tensor]'=None):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, seq_len)` where padding elements are indicated by ``1``.
            attn_mask (ByteTensor): binary tensor of shape `(tgt_len, src_len)`,
                where `tgt_len` is the length of output and `src_len` is the
                length of input, though here both are equal to `seq_len`.
                `attn_mask[tgt_i, src_j] = 1` means that when calculating the
                embedding for `tgt_i`, we exclude (mask out) `src_j`. This is
                useful for strided self-attention.

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if attn_mask is not None:
            attn_mask = attn_mask.masked_fill(attn_mask, -100000000.0 if x.dtype == torch.float32 else -10000.0)
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, need_weights=False, attn_mask=attn_mask)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        fc_result = x
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.return_fc and not torch.jit.is_scripting():
            return x, fc_result
        return x


class TransformerEncoderLayer(TransformerEncoderLayerBase):

    def __init__(self, args):
        super().__init__(TransformerConfig.from_namespace(args))
        self.args = args

    def build_self_attention(self, embed_dim, args):
        return super().build_self_attention(embed_dim, TransformerConfig.from_namespace(args))


class LatentTransformerEncoderLayer(TransformerEncoderLayer):
    """Encoder layer with each (non_residual) block weighted by samples of Bernouli
    or Gumbel Signmoid samples.

    Args:
        args (argparse.Namespace): parsed command-line arguments from standard
            TransformerEncoderLayer.
        idx (int): layer index (used to retrieve samples).
        layer_select (LayerSelect, optional): instance of LayerSelect module with logits
            parameters and sampling method.
    """

    def __init__(self, args, idx, layer_select=None):
        super().__init__(args)
        self.idx = idx
        self.layer_select = layer_select

    def residual_connection(self, x, residual):
        return residual + x * self.layer_select(self.idx)


class LayerSelect(nn.Module):
    """Compute samples (from a Gumbel-Sigmoid distribution) which is used as
    either (soft) weighting or (hard) selection of residual connection.
    https://arxiv.org/abs/2009.13102
    """

    def __init__(self, num_layers, num_logits, soft_select=False, sampling_tau=5.0):
        super(LayerSelect, self).__init__()
        self.layer_logits = torch.nn.Parameter(torch.Tensor(num_logits, num_layers), requires_grad=True)
        self.hard_select = not soft_select
        self.tau = sampling_tau
        self.detach_grad = False
        self.layer_samples = [None] * num_logits

    def sample(self, logit_idx):
        """To leverage the efficiency of distributed training, samples for all
        layers are computed at once for each logit_idx. Logits are parameters
        learnt independent of each other.

        Args:
            logit_idx: The index of logit parameters used for sampling.
        """
        assert logit_idx is not None
        self.samples = self._gumbel_sigmoid(self.layer_logits[logit_idx, :].detach() if self.detach_grad else self.layer_logits[logit_idx, :], dim=-1, tau=self.tau, hard=self.hard_select)
        self.layer_samples[logit_idx] = self.samples

    def forward(self, i):
        sample = self.samples[i]
        return sample

    def _gumbel_sigmoid(self, logits, tau=1, hard=False, eps=1e-10, dim=-1, threshold=0.5):
        gumbels1 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels2 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels1 = (logits + gumbels1 - gumbels2) / tau
        y_soft = gumbels1.sigmoid()
        if hard:
            y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).masked_fill(y_soft > threshold, 1.0)
            ret = y_hard - y_soft.detach() + y_soft
        else:
            ret = y_soft
        return ret


class LatentTransformerEncoder(TransformerEncoder):
    """Latent depth (https://arxiv.org/abs/2009.13102) implemented in
    TransformerEncoder.
    """

    def __init__(self, args, dictionary, embed_tokens, num_logits=1):
        self.num_logits = num_logits
        self.num_layers = args.encoder_layers
        super().__init__(args, dictionary, embed_tokens)
        self.layer_select = LayerSelect(num_layers=self.num_layers, num_logits=self.num_logits, soft_select=getattr(args, 'soft_select', False), sampling_tau=getattr(args, 'sampling_tau', 5.0))
        self.lang_idx = None
        self.layers = nn.ModuleList([self._build_encoder_layer(args, idx) for idx in range(args.encoder_layers)])

    def set_lang_idx(self, lang_idx):
        self.lang_idx = lang_idx

    def _build_encoder_layer(self, args, idx=None):
        return LatentTransformerEncoderLayer(args, idx, layer_select=self.layer_select)

    def forward(self, src_tokens, src_lengths, return_all_hiddens: 'bool'=False):
        self.layer_select.sample(self.lang_idx)
        return super().forward(src_tokens, src_lengths, return_all_hiddens)


class LatentTransformerDecoderLayer(TransformerDecoderLayer):
    """Decoder layer with each (non_residual) block weighted by samples of Bernouli
    or Gumbel Signmoid samples.

    Args:
        args (argparse.Namespace): parsed command-line arguments from standard
            TransformerDecoderLayer.
        idx (int): layer index (used to retrieve samples).
        layer_select (LayerSelect, optional): instance of LayerSelect module with logits
            parameters and sampling method.
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).

    """

    def __init__(self, args, idx, layer_select=None, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):
        super().__init__(args, no_encoder_attn, add_bias_kv, add_zero_attn)
        self.idx = idx
        self.layer_select = layer_select

    def residual_connection(self, x, residual):
        return residual + x * self.layer_select(self.idx)


class LatentTransformerDecoder(TransformerDecoder):
    """Latent depth (https://arxiv.org/abs/2009.13102) implemented in
    TransformerDecoder.
    """

    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False, num_logits=1):
        self.num_logits = num_logits
        self.num_layers = args.decoder_layers
        super().__init__(args, dictionary, embed_tokens, no_encoder_attn=no_encoder_attn)
        self.layer_select = LayerSelect(num_layers=self.num_layers, num_logits=self.num_logits, soft_select=getattr(args, 'soft_select', False), sampling_tau=getattr(args, 'sampling_tau', 5.0))
        self.lang_idx = None
        self.layers = nn.ModuleList([self._build_decoder_layer(args, no_encoder_attn, idx) for idx in range(args.decoder_layers)])

    def set_lang_idx(self, lang_idx):
        self.lang_idx = lang_idx

    def _build_decoder_layer(self, args, no_encoder_attn=False, idx=None):
        return LatentTransformerDecoderLayer(args, idx, layer_select=self.layer_select, no_encoder_attn=no_encoder_attn)

    def forward(self, prev_output_tokens, encoder_out: 'Optional[EncoderOut]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, features_only: 'bool'=False, alignment_layer: 'Optional[int]'=None, alignment_heads: 'Optional[int]'=None, src_lengths: 'Optional[Any]'=None, return_all_hiddens: 'bool'=False):
        self.layer_select.sample(self.lang_idx)
        return super().forward(prev_output_tokens=prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state, features_only=features_only, alignment_layer=alignment_layer, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens)


@with_incremental_state
class MultiheadLinearAttention(nn.Module):
    """Multi-headed linformer attention.

    Projects the key and values down to the compressed dimension, before computing self-attention.

    See "Linformer: Self-Attention with Linear Complexity" for more details.
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, compressed=1, max_seq_len=256, shared_kv_compressed=0, shared_compress_layer=None, freeze_compress=0):
        super().__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, 'embed_dim must be divisible by num_heads'
        self.scaling = self.head_dim ** -0.5
        self.self_attention = self_attention
        self.encoder_decoder_attention = encoder_decoder_attention
        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and value to be of the same size'
        self.k_proj = quant_noise(nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.v_proj = quant_noise(nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.q_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        if shared_compress_layer is None:
            self.compress_seq_len = max_seq_len // compressed
            self.compress_k = nn.Linear(max_seq_len, self.compress_seq_len, bias=False)
            if shared_kv_compressed == 0:
                self.compress_v = nn.Linear(max_seq_len, self.compress_seq_len, bias=False)
            self.layerwise_sharing = False
        else:
            self.compress_k = shared_compress_layer
            if shared_kv_compressed == 0:
                self.compress_v = shared_compress_layer
            self.layerwise_sharing = True
        self.shared_kv_compressed = shared_kv_compressed
        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        if add_bias_kv:
            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))
            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None
        self.add_zero_attn = add_zero_attn
        self.reset_parameters()
        if freeze_compress == 1:
            self.compress_k.weight.requires_grad = False
            if shared_kv_compressed == 0:
                self.compress_v.weight.requires_grad = False
        self.onnx_trace = False

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def reset_parameters(self):
        if self.qkv_same_dim:
            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))
            if not self.layerwise_sharing:
                nn.init.xavier_uniform_(self.compress_k.weight, gain=1 / math.sqrt(2))
                if self.shared_kv_compressed == 0:
                    nn.init.xavier_uniform_(self.compress_v.weight, gain=1 / math.sqrt(2))
        else:
            nn.init.xavier_uniform_(self.k_proj.weight)
            nn.init.xavier_uniform_(self.v_proj.weight)
            nn.init.xavier_uniform_(self.q_proj.weight)
            if not self.layerwise_sharing:
                nn.init.xavier_uniform_(self.compress_k.weight)
                if self.shared_kv_compressed == 0:
                    nn.init.xavier_uniform_(self.compress_v.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.out_proj.bias is not None:
            nn.init.constant_(self.out_proj.bias, 0.0)
        if self.bias_k is not None:
            nn.init.xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            nn.init.xavier_normal_(self.bias_v)

    def forward(self, query, key: 'Optional[Tensor]', value: 'Optional[Tensor]', key_padding_mask: 'Optional[Tensor]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, need_weights: 'bool'=True, static_kv: 'bool'=False, attn_mask: 'Optional[Tensor]'=None, before_softmax: 'bool'=False, need_head_weights: 'bool'=False) ->Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time x Batch x Channel

        Args:
            key_padding_mask (ByteTensor, optional): mask to exclude
                keys that are pads, of shape `(batch, src_len)`, where
                padding elements are indicated by 1s.
            need_weights (bool, optional): return the attention weights,
                averaged over heads (default: False).
            attn_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
            before_softmax (bool, optional): return the raw attention
                weights and values before the attention softmax.
            need_head_weights (bool, optional): return the attention
                weights for each head. Implies *need_weights*. Default:
                return the average attention weights over all heads.
        """
        if need_head_weights:
            need_weights = True
        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.embed_dim
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if saved_state is not None and 'prev_key' in saved_state:
                if static_kv:
                    assert self.encoder_decoder_attention and not self.self_attention
                    key = value = None
        else:
            saved_state = None
        if self.self_attention:
            q = self.q_proj(query)
            k_input = query.permute(1, 2, 0).contiguous()
            k_input = F.linear(k_input, self.compress_k.weight[:, 0:tgt_len]).permute(2, 0, 1).contiguous()
            k = self.k_proj(k_input)
            v_input = query.permute(1, 2, 0).contiguous()
            if self.shared_kv_compressed == 0:
                v_input = F.linear(v_input, self.compress_v.weight[:, 0:tgt_len]).permute(2, 0, 1).contiguous()
            if self.shared_kv_compressed == 1:
                v_input = F.linear(v_input, self.compress_k.weight[:, 0:tgt_len]).permute(2, 0, 1).contiguous()
            v = self.v_proj(v_input)
        elif self.encoder_decoder_attention:
            q = self.q_proj(query)
            if key is None:
                assert value is None
                k = v = None
            else:
                k = self.k_proj(key)
                v = self.v_proj(key)
        else:
            assert key is not None and value is not None
            q = self.q_proj(query)
            k = self.k_proj(key)
            v = self.v_proj(value)
        q *= self.scaling
        if self.bias_k is not None:
            assert self.bias_v is not None
            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)
            if key_padding_mask is not None:
                key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)
        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if k is not None:
            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if v is not None:
            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if saved_state is not None:
            if 'prev_key' in saved_state:
                _prev_key = saved_state['prev_key']
                assert _prev_key is not None
                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    k = prev_key
                else:
                    assert k is not None
                    k = torch.cat([prev_key, k], dim=1)
            if 'prev_value' in saved_state:
                _prev_value = saved_state['prev_value']
                assert _prev_value is not None
                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    v = prev_value
                else:
                    assert v is not None
                    v = torch.cat([prev_value, v], dim=1)
            prev_key_padding_mask: 'Optional[Tensor]' = None
            if 'prev_key_padding_mask' in saved_state:
                prev_key_padding_mask = saved_state['prev_key_padding_mask']
            assert k is not None and v is not None
            key_padding_mask = MultiheadLinearAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)
            saved_state['prev_key'] = k.view(bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_value'] = v.view(bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_key_padding_mask'] = key_padding_mask
            assert incremental_state is not None
            incremental_state = self._set_input_buffer(incremental_state, saved_state)
        assert k is not None
        src_len = k.size(1)
        if self.add_zero_attn:
            assert v is not None
            src_len += 1
            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)
            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)
            if attn_mask is not None:
                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        attn_weights = MultiheadLinearAttention.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(0)
            if self.onnx_trace:
                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)
            attn_weights += attn_mask
        if before_softmax:
            return attn_weights, v
        attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)
        attn_weights = attn_weights_float.type_as(attn_weights)
        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)
        assert v is not None
        attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
        if self.onnx_trace and attn.size(1) == 1:
            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)
        else:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
        attn = self.out_proj(attn)
        attn_weights: 'Optional[Tensor]' = None
        if need_weights:
            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)
            if not need_head_weights:
                attn_weights = attn_weights.mean(dim=0)
        return attn, attn_weights

    @staticmethod
    def _append_prev_key_padding_mask(key_padding_mask: 'Optional[Tensor]', prev_key_padding_mask: 'Optional[Tensor]', batch_size: 'int', src_len: 'int', static_kv: 'bool') ->Optional[Tensor]:
        if prev_key_padding_mask is not None and static_kv:
            new_key_padding_mask = prev_key_padding_mask
        elif prev_key_padding_mask is not None and key_padding_mask is not None:
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), key_padding_mask.float()], dim=1)
        elif prev_key_padding_mask is not None:
            filler = torch.zeros((batch_size, src_len - prev_key_padding_mask.size(1)), device=prev_key_padding_mask.device)
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), filler.float()], dim=1)
        elif key_padding_mask is not None:
            filler = torch.zeros((batch_size, src_len - key_padding_mask.size(1)), device=key_padding_mask.device)
            new_key_padding_mask = torch.cat([filler.float(), key_padding_mask.float()], dim=1)
        else:
            new_key_padding_mask = prev_key_padding_mask
        return new_key_padding_mask

    @torch.jit.export
    def reorder_incremental_state(self, incremental_state: 'Dict[str, Dict[str, Optional[Tensor]]]', new_order: 'Tensor'):
        """Reorder buffered internal state (for incremental generation)."""
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            for k in input_buffer.keys():
                input_buffer_k = input_buffer[k]
                if input_buffer_k is not None:
                    if self.encoder_decoder_attention and input_buffer_k.size(0) == new_order.size(0):
                        break
                    input_buffer[k] = input_buffer_k.index_select(0, new_order)
            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
        return incremental_state

    def _get_input_buffer(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]') ->Dict[str, Optional[Tensor]]:
        result = self.get_incremental_state(incremental_state, 'attn_state')
        if result is not None:
            return result
        else:
            empty_result: 'Dict[str, Optional[Tensor]]' = {}
            return empty_result

    def _set_input_buffer(self, incremental_state: 'Dict[str, Dict[str, Optional[Tensor]]]', buffer: 'Dict[str, Optional[Tensor]]'):
        return self.set_incremental_state(incremental_state, 'attn_state', buffer)

    def apply_sparse_mask(attn_weights, tgt_len: 'int', src_len: 'int', bsz: 'int'):
        return attn_weights

    def upgrade_state_dict_named(self, state_dict, name):
        prefix = name + '.' if name != '' else ''
        items_to_add = {}
        keys_to_remove = []
        for k in state_dict.keys():
            if k.endswith(prefix + 'in_proj_weight'):
                dim = int(state_dict[k].shape[0] / 3)
                items_to_add[prefix + 'q_proj.weight'] = state_dict[k][:dim]
                items_to_add[prefix + 'k_proj.weight'] = state_dict[k][dim:2 * dim]
                items_to_add[prefix + 'v_proj.weight'] = state_dict[k][2 * dim:]
                keys_to_remove.append(k)
                k_bias = prefix + 'in_proj_bias'
                if k_bias in state_dict.keys():
                    dim = int(state_dict[k].shape[0] / 3)
                    items_to_add[prefix + 'q_proj.bias'] = state_dict[k_bias][:dim]
                    items_to_add[prefix + 'k_proj.bias'] = state_dict[k_bias][dim:2 * dim]
                    items_to_add[prefix + 'v_proj.bias'] = state_dict[k_bias][2 * dim:]
                    keys_to_remove.append(prefix + 'in_proj_bias')
        for k in keys_to_remove:
            del state_dict[k]
        for key, value in items_to_add.items():
            state_dict[key] = value


class LinformerTransformerEncoderLayer(TransformerEncoderLayer):
    """
    Implements a Linformer Encoder Layer used in BERT/XLM style pre-trained
    models.
    """

    def __init__(self, args, shared_compress_layer):
        self.shared_compress_layer = [shared_compress_layer]
        super().__init__(args)
        self.register_buffer('version', torch.tensor(2))

    def build_self_attention(self, embed_dim, args):
        return MultiheadLinearAttention(embed_dim, args.encoder_attention_heads, dropout=args.dropout, self_attention=True, q_noise=args.quant_noise_pq, qn_block_size=args.quant_noise_pq_block_size, compressed=args.compressed, max_seq_len=args.max_positions, shared_kv_compressed=args.shared_kv_compressed, shared_compress_layer=self.shared_compress_layer[0], freeze_compress=args.freeze_compress)

    def upgrade_state_dict_named(self, state_dict, name):
        super().upgrade_state_dict_named(state_dict, name)
        prefix = name + '.' if name != '' else ''
        if utils.item(state_dict.get(f'{prefix}version', torch.tensor(1))) < 2:
            state_dict[f'{prefix}version'] = torch.tensor(1)
            if f'{prefix}shared_compress_layer.weight' in state_dict:
                self.shared_compress_layer = [torch.nn.Linear(self.shared_compress_layer[0].weight.size(1), self.shared_compress_layer[0].weight.size(0))]
                self.self_attn = self.build_self_attention(self.embed_dim, self.args)
                del state_dict[f'{prefix}shared_compress_layer.weight']
                if f'{prefix}shared_compress_layer.bias' in state_dict:
                    del state_dict[f'{prefix}shared_compress_layer.bias']


class LinformerTransformerEncoder(TransformerEncoder):
    """
    Implementation for a Bi-directional Linformer based Sentence Encoder used
    in BERT/XLM style pre-trained models.

    This first computes the token embedding using the token embedding matrix,
    position embeddings (if specified) and segment embeddings
    (if specified). After applying the specified number of
    LinformerEncoderLayers, it outputs all the internal states of the
    encoder as well as the final representation associated with the first
    token (usually CLS token).

    Input:
        - tokens: B x T matrix representing sentences
        - segment_labels: B x T matrix representing segment label for tokens

    Output:
        - a tuple of the following:
            - a list of internal model states used to compute the
              predictions where each tensor has shape T x B x C
            - sentence representation associated with first input token
              in format B x C.
    """

    def __init__(self, args, dictionary, embed_tokens):
        self.compress_layer = None
        super().__init__(args, dictionary, embed_tokens)

    def build_encoder_layer(self, args):
        if self.args.shared_layer_kv_compressed == 1 and self.compress_layer is None:
            compress_layer = nn.Linear(self.args.max_positions, self.args.max_positions // self.args.compressed)
            nn.init.xavier_uniform_(compress_layer.weight, gain=1 / math.sqrt(2))
            if self.args.freeze_compress == 1:
                compress_layer.weight.requires_grad = False
            self.compress_layer = compress_layer
        return LinformerTransformerEncoderLayer(args, self.compress_layer)


class TransformerPointerGeneratorEncoder(TransformerEncoder):
    """
    Transformer encoder consisting of *args.encoder_layers* layers. Each layer
    is a :class:`TransformerEncoderLayer`. The pointer-generator variant adds
    the source tokens to the encoder output as these are otherwise not passed
    to the decoder.
    """

    def forward(self, src_tokens, src_lengths: 'Optional[Tensor]'=None, return_all_hiddens: 'bool'=False, token_embeddings: 'Optional[Tensor]'=None):
        """
        Runs the `forward()` method of the parent Transformer class. Then adds
        the source tokens into the encoder output tuple.

        While it might be more elegant that the model would pass the source
        tokens to the `forward()` method of the decoder too, this would require
        changes to `SequenceGenerator`.

        Args:
            src_tokens (torch.LongTensor): tokens in the source language of
                shape `(batch, src_len)`
            src_lengths (torch.LongTensor): lengths of each source sentence of
                shape `(batch)`
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).
            token_embeddings (torch.Tensor, optional): precomputed embeddings
                default `None` will recompute embeddings

        Returns:
            namedtuple:
                - **encoder_out** (Tensor): the last encoder layer's output of
                  shape `(src_len, batch, embed_dim)`
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
                - **encoder_embedding** (Tensor): the (scaled) embedding lookup
                  of shape `(batch, src_len, embed_dim)`
                - **encoder_states** (List[Tensor]): all intermediate
                  hidden states of shape `(src_len, batch, embed_dim)`.
                  Only populated if *return_all_hiddens* is True.
                - **src_tokens** (Tensor): input token ids of shape
                  `(batch, src_len)`
        """
        encoder_out = self.forward_scriptable(src_tokens, src_lengths, return_all_hiddens, token_embeddings)
        return {'encoder_out': encoder_out['encoder_out'], 'encoder_padding_mask': encoder_out['encoder_padding_mask'], 'encoder_embedding': encoder_out['encoder_embedding'], 'encoder_states': encoder_out['encoder_states'], 'src_tokens': [src_tokens], 'src_lengths': []}


class TransformerPointerGeneratorDecoder(TransformerDecoder):
    """
    Transformer decoder consisting of *args.decoder_layers* layers. Each layer
    is a :class:`TransformerDecoderLayer`. The pointer-generator variant mixes
    the output probabilities with an attention distribution in the output layer.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): decoding dictionary
        embed_tokens (torch.nn.Embedding): output embedding
    """

    def __init__(self, args, dictionary, embed_tokens):
        super().__init__(args, dictionary, embed_tokens, no_encoder_attn=False)
        self.alignment_heads = args.alignment_heads
        self.alignment_layer = args.alignment_layer
        input_embed_dim = embed_tokens.embedding_dim
        p_gen_input_size = input_embed_dim + self.output_embed_dim
        self.project_p_gens = nn.Linear(p_gen_input_size, 1)
        nn.init.zeros_(self.project_p_gens.bias)
        self.num_types = len(dictionary)
        self.num_oov_types = args.source_position_markers
        self.num_embeddings = self.num_types - self.num_oov_types
        self.force_p_gen = args.force_generation

    def forward(self, prev_output_tokens, encoder_out: 'Optional[Dict[str, List[Tensor]]]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, features_only: 'bool'=False, alignment_layer: 'Optional[int]'=0, alignment_heads: 'Optional[int]'=1, src_lengths: 'Optional[Any]'=None, return_all_hiddens: 'bool'=False):
        """
        Args:
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (optional): output from the encoder, used for
                encoder-side attention
            incremental_state (dict, optional): dictionary used for storing
                state during :ref:`Incremental decoding`
            features_only (bool, optional): only return features without
                applying output layer (default: False)
            alignment_layer (int, optional): 0-based index of the layer to be
                used for pointing (default: 0)
            alignment_heads (int, optional): number of attention heads to be
                used for pointing (default: 1)

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state, alignment_layer=self.alignment_layer, alignment_heads=self.alignment_heads)
        if not features_only:
            if incremental_state is not None:
                prev_output_tokens = prev_output_tokens[:, -1:]
            prev_output_embed = self.embed_tokens(prev_output_tokens)
            prev_output_embed *= self.embed_scale
            predictors = torch.cat((prev_output_embed, x), 2)
            p_gens = self.project_p_gens(predictors)
            p_gens = torch.sigmoid(p_gens.float())
            attn: 'Optional[Tensor]' = extra['attn'][0]
            assert encoder_out is not None
            assert attn is not None
            x = self.output_layer(x, attn, encoder_out['src_tokens'][0], p_gens)
        return x, extra

    def output_layer(self, features: 'Tensor', attn: 'Tensor', src_tokens: 'Tensor', p_gens: 'Tensor') ->Tensor:
        """
        Project features to the vocabulary size and mix with the attention
        distributions.
        """
        if self.force_p_gen is not None:
            p_gens = self.force_p_gen
        if self.adaptive_softmax is None:
            logits = self.output_projection(features)
        else:
            logits = features
        batch_size = logits.shape[0]
        output_length = logits.shape[1]
        assert logits.shape[2] == self.num_embeddings
        assert src_tokens.shape[0] == batch_size
        src_length = src_tokens.shape[1]
        gen_dists = self.get_normalized_probs_scriptable((logits, None), log_probs=False, sample=None)
        gen_dists = torch.mul(gen_dists, p_gens)
        padding_size = batch_size, output_length, self.num_oov_types
        padding = gen_dists.new_zeros(padding_size)
        gen_dists = torch.cat((gen_dists, padding), 2)
        assert gen_dists.shape[2] == self.num_types
        attn = torch.mul(attn.float(), 1 - p_gens)
        index = src_tokens[:, None, :]
        index = index.expand(batch_size, output_length, src_length)
        attn_dists_size = batch_size, output_length, self.num_types
        attn_dists = attn.new_zeros(attn_dists_size)
        attn_dists.scatter_add_(2, index, attn.float())
        return gen_dists + attn_dists

    def get_normalized_probs(self, net_output: 'Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]]', log_probs: 'bool', sample: 'Optional[Dict[str, Tensor]]'=None):
        """
        Get normalized probabilities (or log probs) from a net's output.
        Pointer-generator network output is already normalized.
        """
        probs = net_output[0]
        return probs.clamp(1e-10, 1.0).log() if log_probs else probs


def Embedding(num_embeddings, embedding_dim, padding_idx):
    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)
    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)
    nn.init.constant_(m.weight[padding_idx], 0)
    return m


class TransformerMonotonicEncoderLayer(TransformerEncoderLayer):

    def forward(self, x, encoder_padding_mask):
        seq_len, _, _ = x.size()
        attn_mask = x.new_ones([seq_len, seq_len]).triu(1)
        attn_mask = attn_mask.masked_fill(attn_mask.bool(), float('-inf'))
        return super().forward(x, encoder_padding_mask, attn_mask)


class TransformerMonotonicEncoder(TransformerEncoder):

    def __init__(self, args, dictionary, embed_tokens):
        super().__init__(args, dictionary, embed_tokens)
        self.dictionary = dictionary
        self.layers = nn.ModuleList([])
        self.layers.extend([TransformerMonotonicEncoderLayer(args) for i in range(args.encoder_layers)])


class TransformerMonotonicDecoderLayer(TransformerDecoderLayer):

    def __init__(self, args):
        super().__init__(args)
        assert args.simul_type is not None, 'A --simul-type is needed.'
        self.encoder_attn = build_monotonic_attention(args)

    def prune_incremental_state(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'):
        input_buffer = self.self_attn._get_input_buffer(incremental_state)
        for key in ['prev_key', 'prev_value']:
            input_buffer_key = input_buffer[key]
            assert input_buffer_key is not None
            if input_buffer_key.size(2) > 1:
                input_buffer[key] = input_buffer_key[:, :, :-1, :]
            else:
                typed_empty_dict: 'Dict[str, Optional[Tensor]]' = {}
                input_buffer = typed_empty_dict
                break
        assert incremental_state is not None
        self.self_attn._set_input_buffer(incremental_state, input_buffer)

    def forward(self, x, encoder_out: 'Optional[Tensor]'=None, encoder_padding_mask: 'Optional[Tensor]'=None, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, prev_self_attn_state: 'Optional[List[Tensor]]'=None, prev_attn_state: 'Optional[List[Tensor]]'=None, self_attn_mask: 'Optional[Tensor]'=None, self_attn_padding_mask: 'Optional[Tensor]'=None, need_attn: 'bool'=False, need_head_weights: 'bool'=False):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor, optional): binary
                ByteTensor of shape `(batch, src_len)` where padding
                elements are indicated by ``1``.
            need_attn (bool, optional): return attention weights
            need_head_weights (bool, optional): return attention weights
                for each head (default: return average over heads).

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if need_head_weights:
            need_attn = True
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if prev_self_attn_state is not None:
            prev_key, prev_value = prev_self_attn_state[:2]
            saved_state: 'Dict[str, Optional[Tensor]]' = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_self_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_self_attn_state[2]
            assert incremental_state is not None
            self.self_attn._set_input_buffer(incremental_state, saved_state)
        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)
        if self.cross_self_attention and not (incremental_state is not None and _self_attn_input_buffer is not None and 'prev_key' in _self_attn_input_buffer):
            if self_attn_mask is not None:
                assert encoder_out is not None
                self_attn_mask = torch.cat((x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1)
            if self_attn_padding_mask is not None:
                if encoder_padding_mask is None:
                    assert encoder_out is not None
                    encoder_padding_mask = self_attn_padding_mask.new_zeros(encoder_out.size(1), encoder_out.size(0))
                self_attn_padding_mask = torch.cat((encoder_padding_mask, self_attn_padding_mask), dim=1)
            assert encoder_out is not None
            y = torch.cat((encoder_out, x), dim=0)
        else:
            y = x
        x, attn = self.self_attn(query=x, key=y, value=y, key_padding_mask=self_attn_padding_mask, incremental_state=incremental_state, need_weights=False, attn_mask=self_attn_mask)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        assert self.encoder_attn is not None
        residual = x
        if self.normalize_before:
            x = self.encoder_attn_layer_norm(x)
        if prev_attn_state is not None:
            prev_key, prev_value = prev_attn_state[:2]
            saved_state: 'Dict[str, Optional[Tensor]]' = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_attn_state[2]
            assert incremental_state is not None
            self.encoder_attn._set_input_buffer(incremental_state, saved_state)
        x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.encoder_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.onnx_trace and incremental_state is not None:
            saved_state = self.self_attn._get_input_buffer(incremental_state)
            assert saved_state is not None
            if self_attn_padding_mask is not None:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value'], saved_state['prev_key_padding_mask']]
            else:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value']]
            return x, attn, self_attn_state
        return x, attn, None


TransformerMonotonicDecoderOut = NamedTuple('TransformerMonotonicDecoderOut', [('action', int), ('p_choose', Optional[Tensor]), ('attn_list', Optional[List[Optional[Dict[str, Tensor]]]]), ('encoder_out', Optional[Dict[str, List[Tensor]]]), ('encoder_padding_mask', Optional[Tensor])])


class TransformerMonotonicDecoder(TransformerDecoder):
    """
    Transformer decoder consisting of *args.decoder_layers* layers. Each layer
    is a :class:`TransformerDecoderLayer`.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): decoding dictionary
        embed_tokens (torch.nn.Embedding): output embedding
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).
    """

    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):
        super().__init__(args, dictionary, embed_tokens, no_encoder_attn=False)
        self.dictionary = dictionary
        self.layers = nn.ModuleList([])
        self.layers.extend([TransformerMonotonicDecoderLayer(args) for _ in range(args.decoder_layers)])
        self.policy_criterion = getattr(args, 'policy_criterion', 'any')
        self.num_updates = None

    def set_num_updates(self, num_updates):
        self.num_updates = num_updates

    def pre_attention(self, prev_output_tokens, encoder_out_dict: 'Dict[str, List[Tensor]]', incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None):
        positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state) if self.embed_positions is not None else None
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        encoder_out = encoder_out_dict['encoder_out'][0]
        if 'encoder_padding_mask' in encoder_out_dict:
            encoder_padding_mask = encoder_out_dict['encoder_padding_mask'][0] if encoder_out_dict['encoder_padding_mask'] and len(encoder_out_dict['encoder_padding_mask']) > 0 else None
        else:
            encoder_padding_mask = None
        return x, encoder_out, encoder_padding_mask

    def post_attention(self, x):
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)
        return x

    def clean_cache(self, incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]', end_id: 'Optional[int]'=None):
        """
        Clean cache in the monotonic layers.
        The cache is generated because of a forward pass of decoder has run but no prediction,
        so that the self attention key value in decoder is written in the incremental state.
        end_id is the last idx of the layers
        """
        if end_id is None:
            end_id = len(self.layers)
        for index, layer in enumerate(self.layers):
            if index < end_id:
                layer.prune_incremental_state(incremental_state)

    def extract_features(self, prev_output_tokens, encoder_out: 'Optional[Dict[str, List[Tensor]]]', incremental_state: 'Optional[Dict[str, Dict[str, Optional[Tensor]]]]'=None, full_context_alignment: 'bool'=False, alignment_layer: 'Optional[int]'=None, alignment_heads: 'Optional[int]'=None):
        """
        Similar to *forward* but only return features.

        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        assert encoder_out is not None
        x, encoder_outs, encoder_padding_mask = self.pre_attention(prev_output_tokens, encoder_out, incremental_state)
        attn = None
        inner_states = [x]
        attn_list: 'List[Optional[Dict[str, Tensor]]]' = []
        p_choose = torch.tensor([1.0])
        for i, layer in enumerate(self.layers):
            x, attn, _ = layer(x=x, encoder_out=encoder_outs, encoder_padding_mask=encoder_padding_mask, incremental_state=incremental_state, self_attn_mask=self.buffered_future_mask(x) if incremental_state is None else None)
            inner_states.append(x)
            attn_list.append(attn)
            if incremental_state is not None:
                if_online = incremental_state['online']['only']
                assert if_online is not None
                if if_online:
                    assert attn is not None
                    if self.policy_criterion == 'any':
                        head_read = layer.encoder_attn._get_monotonic_buffer(incremental_state)['head_read']
                        assert head_read is not None
                        if head_read.any():
                            self.clean_cache(incremental_state, i + 1)
                            return x, TransformerMonotonicDecoderOut(action=0, p_choose=p_choose, attn_list=None, encoder_out=None, encoder_padding_mask=None)
        x = self.post_attention(x)
        return x, TransformerMonotonicDecoderOut(action=1, p_choose=p_choose, attn_list=attn_list, encoder_out=encoder_out, encoder_padding_mask=encoder_padding_mask)


class BLSTM(nn.Module):

    def __init__(self, dim, layers=2, bi=True):
        super().__init__()
        klass = nn.LSTM
        self.lstm = klass(bidirectional=bi, num_layers=layers, hidden_size=dim, input_size=dim)
        self.linear = None
        if bi:
            self.linear = nn.Linear(2 * dim, dim)

    def forward(self, x, hidden=None):
        x, hidden = self.lstm(x, hidden)
        if self.linear:
            x = self.linear(x)
        return x, hidden


def capture_init(init):
    """capture_init.

    Decorate `__init__` with this, and you can then
    recover the *args and **kwargs passed to it in `self._init_args_kwargs`
    """

    @functools.wraps(init)
    def __init__(self, *args, **kwargs):
        self._init_args_kwargs = args, kwargs
        init(self, *args, **kwargs)
    return __init__


def sinc(t):
    """sinc.

    :param t: the input tensor
    """
    return th.where(t == 0, th.tensor(1.0, device=t.device, dtype=t.dtype), th.sin(t) / t)


def kernel_downsample2(zeros=56):
    """kernel_downsample2.

    """
    win = th.hann_window(4 * zeros + 1, periodic=False)
    winodd = win[1::2]
    t = th.linspace(-zeros + 0.5, zeros - 0.5, 2 * zeros)
    t.mul_(math.pi)
    kernel = (sinc(t) * winodd).view(1, 1, -1)
    return kernel


def downsample2(x, zeros=56):
    """
    Downsampling the input by 2 using sinc interpolation.
    Smith, Julius, and Phil Gossett. "A flexible sampling-rate conversion method."
    ICASSP'84. IEEE International Conference on Acoustics, Speech, and Signal Processing.
    Vol. 9. IEEE, 1984.
    """
    if x.shape[-1] % 2 != 0:
        x = F.pad(x, (0, 1))
    xeven = x[..., ::2]
    xodd = x[..., 1::2]
    *other, time = xodd.shape
    kernel = kernel_downsample2(zeros)
    out = xeven + F.conv1d(xodd.view(-1, 1, time), kernel, padding=zeros)[..., :-1].view(*other, time)
    return out.view(*other, -1).mul(0.5)


def rescale_conv(conv, reference):
    std = conv.weight.std().detach()
    scale = (std / reference) ** 0.5
    conv.weight.data /= scale
    if conv.bias is not None:
        conv.bias.data /= scale


def rescale_module(module, reference):
    for sub in module.modules():
        if isinstance(sub, (nn.Conv1d, nn.ConvTranspose1d)):
            rescale_conv(sub, reference)


def kernel_upsample2(zeros=56):
    """kernel_upsample2.

    """
    win = th.hann_window(4 * zeros + 1, periodic=False)
    winodd = win[1::2]
    t = th.linspace(-zeros + 0.5, zeros - 0.5, 2 * zeros)
    t *= math.pi
    kernel = (sinc(t) * winodd).view(1, 1, -1)
    return kernel


def upsample2(x, zeros=56):
    """
    Upsampling the input by 2 using sinc interpolation.
    Smith, Julius, and Phil Gossett. "A flexible sampling-rate conversion method."
    ICASSP'84. IEEE International Conference on Acoustics, Speech, and Signal Processing.
    Vol. 9. IEEE, 1984.
    """
    *other, time = x.shape
    kernel = kernel_upsample2(zeros)
    out = F.conv1d(x.view(-1, 1, time), kernel, padding=zeros)[..., 1:].view(*other, time)
    y = th.stack([x, out], dim=-1)
    return y.view(*other, -1)


class Demucs(nn.Module):
    """
    Demucs speech enhancement model.
    Args:
        - chin (int): number of input channels.
        - chout (int): number of output channels.
        - hidden (int): number of initial hidden channels.
        - depth (int): number of layers.
        - kernel_size (int): kernel size for each layer.
        - stride (int): stride for each layer.
        - causal (bool): if false, uses BiLSTM instead of LSTM.
        - resample (int): amount of resampling to apply to the input/output.
            Can be one of 1, 2 or 4.
        - growth (float): number of channels is multiplied by this for every layer.
        - max_hidden (int): maximum number of channels. Can be useful to
            control the size/speed of the model.
        - normalize (bool): if true, normalize the input.
        - glu (bool): if true uses GLU instead of ReLU in 1x1 convolutions.
        - rescale (float): controls custom weight initialization.
            See https://arxiv.org/abs/1911.13254.
        - floor (float): stability flooring when normalizing.

    """

    @capture_init
    def __init__(self, chin=1, chout=1, hidden=48, depth=5, kernel_size=8, stride=4, causal=True, resample=4, growth=2, max_hidden=10000, normalize=True, glu=True, rescale=0.1, floor=0.001):
        super().__init__()
        if resample not in [1, 2, 4]:
            raise ValueError('Resample should be 1, 2 or 4.')
        self.chin = chin
        self.chout = chout
        self.hidden = hidden
        self.depth = depth
        self.kernel_size = kernel_size
        self.stride = stride
        self.causal = causal
        self.floor = floor
        self.resample = resample
        self.normalize = normalize
        self.encoder = nn.ModuleList()
        self.decoder = nn.ModuleList()
        activation = nn.GLU(1) if glu else nn.ReLU()
        ch_scale = 2 if glu else 1
        for index in range(depth):
            encode = []
            encode += [nn.Conv1d(chin, hidden, kernel_size, stride), nn.ReLU(), nn.Conv1d(hidden, hidden * ch_scale, 1), activation]
            self.encoder.append(nn.Sequential(*encode))
            decode = []
            decode += [nn.Conv1d(hidden, ch_scale * hidden, 1), activation, nn.ConvTranspose1d(hidden, chout, kernel_size, stride)]
            if index > 0:
                decode.append(nn.ReLU())
            self.decoder.insert(0, nn.Sequential(*decode))
            chout = hidden
            chin = hidden
            hidden = min(int(growth * hidden), max_hidden)
        self.lstm = BLSTM(chin, bi=not causal)
        if rescale:
            rescale_module(self, reference=rescale)

    def valid_length(self, length):
        """
        Return the nearest valid length to use with the model so that
        there is no time steps left over in a convolutions, e.g. for all
        layers, size of the input - kernel_size % stride = 0.

        If the mixture has a valid length, the estimated sources
        will have exactly the same length.
        """
        length = math.ceil(length * self.resample)
        for _ in range(self.depth):
            length = math.ceil((length - self.kernel_size) / self.stride) + 1
            length = max(length, 1)
        for _ in range(self.depth):
            length = (length - 1) * self.stride + self.kernel_size
        length = int(math.ceil(length / self.resample))
        return int(length)

    @property
    def total_stride(self):
        return self.stride ** self.depth // self.resample

    def forward(self, mix):
        if mix.dim() == 2:
            mix = mix.unsqueeze(1)
        if self.normalize:
            mono = mix.mean(dim=1, keepdim=True)
            std = mono.std(dim=-1, keepdim=True)
            mix = mix / (self.floor + std)
        else:
            std = 1
        length = mix.shape[-1]
        x = mix
        x = F.pad(x, (0, self.valid_length(length) - length))
        if self.resample == 2:
            x = upsample2(x)
        elif self.resample == 4:
            x = upsample2(x)
            x = upsample2(x)
        skips = []
        for encode in self.encoder:
            x = encode(x)
            skips.append(x)
        x = x.permute(2, 0, 1)
        x, _ = self.lstm(x)
        x = x.permute(1, 2, 0)
        for decode in self.decoder:
            skip = skips.pop(-1)
            x = x + skip[..., :x.shape[-1]]
            x = decode(x)
        if self.resample == 2:
            x = downsample2(x)
        elif self.resample == 4:
            x = downsample2(x)
            x = downsample2(x)
        x = x[..., :length]
        return std * x


class SpeechEmbedder(nn.Module):

    def __init__(self, hp):
        super(SpeechEmbedder, self).__init__()
        self.lstm = nn.LSTM(hp['num_mels'], hp['lstm_hidden'], num_layers=hp['lstm_layers'], batch_first=True)
        self.proj = LinearNorm(hp)
        self.hp = hp

    def forward(self, mel):
        mels = mel.unfold(1, self.hp['window'], self.hp['stride'])
        mels = mels.permute(1, 2, 0)
        x, _ = self.lstm(mels)
        x = x[:, -1, :]
        x = self.proj(x)
        x = x / torch.norm(x, p=2, dim=1, keepdim=True)
        x = x.mean(dim=0)
        if x.norm(p=2) != 0:
            x = x / x.norm(p=2)
        return x


EMBEDDER_PARAMS = {'num_mels': 40, 'n_fft': 512, 'emb_dim': 256, 'lstm_hidden': 768, 'lstm_layers': 3, 'window': 80, 'stride': 40}


def set_requires_grad(nets, requires_grad=False):
    """Set requies_grad=Fasle for all the networks to avoid unnecessary
    computations
    Parameters:
        nets (network list)   -- a list of networks
        requires_grad (bool)  -- whether the networks require gradients or not
    """
    if not isinstance(nets, list):
        nets = [nets]
    for net in nets:
        if net is not None:
            for param in net.parameters():
                param.requires_grad = requires_grad


class SpkrEmbedder(nn.Module):
    RATE = 16000

    def __init__(self, embedder_path, embedder_params=EMBEDDER_PARAMS, rate=16000, hop_length=160, win_length=400, pad=False):
        super(SpkrEmbedder, self).__init__()
        embedder_pt = torch.load(embedder_path, map_location='cpu')
        self.embedder = SpeechEmbedder(embedder_params)
        self.embedder.load_state_dict(embedder_pt)
        self.embedder.eval()
        set_requires_grad(self.embedder, requires_grad=False)
        self.embedder_params = embedder_params
        self.register_buffer('mel_basis', torch.from_numpy(librosa.filters.mel(sr=self.RATE, n_fft=self.embedder_params['n_fft'], n_mels=self.embedder_params['num_mels'])))
        self.resample = None
        if rate != self.RATE:
            self.resample = torchaudio.transforms.Resample(rate, self.RATE)
        self.hop_length = hop_length
        self.win_length = win_length
        self.pad = pad

    def get_mel(self, y):
        if self.pad and y.shape[-1] < 14000:
            y = F.pad(y, (0, 14000 - y.shape[-1]))
        window = torch.hann_window(self.win_length)
        y = torch.stft(y, n_fft=self.embedder_params['n_fft'], hop_length=self.hop_length, win_length=self.win_length, window=window)
        magnitudes = torch.norm(y, dim=-1, p=2) ** 2
        mel = torch.log10(self.mel_basis @ magnitudes + 1e-06)
        return mel

    def forward(self, inputs):
        dvecs = []
        for wav in inputs:
            mel = self.get_mel(wav)
            if mel.dim() == 3:
                mel = mel.squeeze(0)
            dvecs += [self.embedder(mel)]
        dvecs = torch.stack(dvecs)
        dvec = torch.mean(dvecs, dim=0)
        dvec = dvec / torch.norm(dvec)
        return dvec


class BenchmarkingBase(nn.Module):

    def __init__(self):
        nn.Module.__init__(self)
        self.s2x_task = None

    def warm_up(self, sample, repeat):
        """Warm up the model"""
        for _i in range(repeat):
            self.forward(sample)
        logger.info(f'Model warmed up by running inference {repeat} times')

    def benchmark_run_time(self, dataset, repeat):
        """Benchmark average runtime for the model by calling benchmark_run_time_single_sample function"""
        logger.info('Starting run time benchmarking')
        time_elapsed = 0
        for i, sample in enumerate(dataset):
            time_elapsed += self.benchmark_run_time_single_sample(sample, repeat=repeat)
            if i % 100 == 0:
                logger.info(f'Benchmarked run time for {i}/{len(dataset)} samples')
        total_time_elapsed = time_elapsed / len(dataset)
        return total_time_elapsed

    def benchmark_run_time_single_sample(self, sample, repeat):
        """Benchmark average runtime for a single sample using timeit library. Units are seconds"""
        timer = timeit.Timer(lambda : self.forward(sample))
        time_elapsed = timer.timeit(repeat)
        return time_elapsed / repeat

    def count_flops(self, dataset, repeat):
        """Use PYPAPI library to count average flops for model inference.
        Note: It only works if the model is being run on cpu"""
        logger.info('Starting flop counter')
        high.start_counters([events.PAPI_DP_OPS])
        for i, sample in enumerate(dataset):
            for _r in range(repeat):
                self.forward(sample)
            if i % 100 == 0:
                logger.info(f'Counted flops for {i}/{len(dataset)} samples')
        flops = high.stop_counters()
        flops = round(flops[0] / (repeat * len(dataset)))
        return flops

    def max_memory(self, dataset, repeat):
        """Compute average max memory consumed by model inference. Units are MiB"""
        logger.info('Starting memory benchmarking')
        total_memory = 0
        for i, sample in enumerate(dataset):
            for _r in range(repeat):
                total_memory += max(memory_usage((self.forward, (sample,), {})))
            if i % 100 == 0:
                logger.info(f'Benchmarked memory for {i}/{len(dataset)} samples')
        total_memory = total_memory / (repeat * len(dataset))
        return total_memory

    def gather_all_metrics(self, dataset, repeat):
        run_time = self.benchmark_run_time(dataset, repeat)
        max_memory = self.max_memory(dataset, repeat)
        flops = self.count_flops(dataset, repeat)
        return run_time, max_memory, flops

    def dump_final_speech_output(self, dataset, output_dir, resample_fn, sample_rate, prefix=None):
        for i, sample in enumerate(dataset):
            hypo = self.forward(sample)[0]

            def to_np(x):
                return x.detach().cpu().numpy()
            try:
                wave_preds = to_np(resample_fn(hypo['waveform']))
                sf.write(f'{output_dir}/{prefix}_{i}_pred.wav', wave_preds, sample_rate)
            except Exception as e:
                raise Exception(f' Encountered {e} - Invalid waveform. Make sure the model outputs a waveform')


ARCH_MODEL_REGISTRY = {}


def eval_str_list(x, type=float):
    if x is None:
        return None
    if isinstance(x, str):
        x = eval(x)
    try:
        return list(map(type, x))
    except TypeError:
        return [type(x)]


def interpret_dc_type(field_type):
    if isinstance(field_type, str):
        raise RuntimeError('field should be a type')
    if field_type == Any:
        return str
    typestring = str(field_type)
    if re.match('(typing.|^)Union\\[(.*), NoneType\\]$', typestring) or typestring.startswith('typing.Optional'):
        return field_type.__args__[0]
    return field_type


def gen_parser_from_dataclass(parser: 'ArgumentParser', dataclass_instance: 'FairseqDataclass', delete_default: 'bool'=False, with_prefix: 'Optional[str]'=None) ->None:
    """
    convert a dataclass instance to tailing parser arguments.

    If `with_prefix` is provided, prefix all the keys in the resulting parser with it. It means that we are
    building a flat namespace from a structured dataclass (see transformer_config.py for example).
    """

    def argparse_name(name: 'str'):
        if name == 'data' and (with_prefix is None or with_prefix == ''):
            return name
        if name == '_name':
            return None
        full_name = '--' + name.replace('_', '-')
        if with_prefix is not None and with_prefix != '':
            full_name = with_prefix + '-' + full_name[2:]
        return full_name

    def get_kwargs_from_dc(dataclass_instance: 'FairseqDataclass', k: 'str') ->Dict[str, Any]:
        """k: dataclass attributes"""
        kwargs = {}
        field_type = dataclass_instance._get_type(k)
        inter_type = interpret_dc_type(field_type)
        field_default = dataclass_instance._get_default(k)
        if isinstance(inter_type, type) and issubclass(inter_type, Enum):
            field_choices = [t.value for t in list(inter_type)]
        else:
            field_choices = None
        field_help = dataclass_instance._get_help(k)
        field_const = dataclass_instance._get_argparse_const(k)
        if isinstance(field_default, str) and field_default.startswith('${'):
            kwargs['default'] = field_default
        else:
            if field_default is MISSING:
                kwargs['required'] = True
            if field_choices is not None:
                kwargs['choices'] = field_choices
            if isinstance(inter_type, type) and (issubclass(inter_type, List) or issubclass(inter_type, Tuple)) or ('List' in str(inter_type) or 'Tuple' in str(inter_type)):
                if 'int' in str(inter_type):
                    kwargs['type'] = lambda x: eval_str_list(x, int)
                elif 'float' in str(inter_type):
                    kwargs['type'] = lambda x: eval_str_list(x, float)
                elif 'str' in str(inter_type):
                    kwargs['type'] = lambda x: eval_str_list(x, str)
                else:
                    raise NotImplementedError('parsing of type ' + str(inter_type) + ' is not implemented')
                if field_default is not MISSING:
                    kwargs['default'] = ','.join(map(str, field_default)) if field_default is not None else None
            elif isinstance(inter_type, type) and issubclass(inter_type, Enum) or 'Enum' in str(inter_type):
                kwargs['type'] = str
                if field_default is not MISSING:
                    if isinstance(field_default, Enum):
                        kwargs['default'] = field_default.value
                    else:
                        kwargs['default'] = field_default
            elif inter_type is bool:
                kwargs['action'] = 'store_false' if field_default is True else 'store_true'
                kwargs['default'] = field_default
            else:
                kwargs['type'] = inter_type
                if field_default is not MISSING:
                    kwargs['default'] = field_default
        if with_prefix is not None and with_prefix != '' and field_help is not None:
            field_help = with_prefix[2:] + ': ' + field_help
        kwargs['help'] = field_help
        if field_const is not None:
            kwargs['const'] = field_const
            kwargs['nargs'] = '?'
        return kwargs
    for k in dataclass_instance._get_all_attributes():
        field_name = argparse_name(dataclass_instance._get_name(k))
        field_type = dataclass_instance._get_type(k)
        if field_name is None:
            continue
        elif inspect.isclass(field_type) and issubclass(field_type, FairseqDataclass):
            prefix = None
            if with_prefix is not None:
                prefix = field_name
            gen_parser_from_dataclass(parser, field_type(), delete_default, prefix)
            continue
        kwargs = get_kwargs_from_dc(dataclass_instance, k)
        field_args = [field_name]
        alias = dataclass_instance._get_argparse_alias(k)
        if alias is not None:
            field_args.append(alias)
        if 'default' in kwargs:
            if isinstance(kwargs['default'], str) and kwargs['default'].startswith('${'):
                if kwargs['help'] is None:
                    continue
                else:
                    del kwargs['default']
            if delete_default and 'default' in kwargs:
                del kwargs['default']
        try:
            parser.add_argument(*field_args, **kwargs)
        except ArgumentError:
            pass


class FairseqCriterion(_Loss):

    def __init__(self, task):
        super().__init__()
        self.task = task
        if hasattr(task, 'target_dictionary'):
            tgt_dict = task.target_dictionary
            self.padding_idx = tgt_dict.pad() if tgt_dict is not None else -100

    @classmethod
    def add_args(cls, parser):
        """Add criterion-specific arguments to the parser."""
        dc = getattr(cls, '__dataclass', None)
        if dc is not None:
            gen_parser_from_dataclass(parser, dc())

    @classmethod
    def build_criterion(cls, cfg: 'FairseqDataclass', task):
        """Construct a criterion from command-line args."""
        init_args = {}
        for p in inspect.signature(cls).parameters.values():
            if p.kind == p.POSITIONAL_ONLY or p.kind == p.VAR_POSITIONAL or p.kind == p.VAR_KEYWORD:
                raise NotImplementedError('{} not supported'.format(p.kind))
            assert p.kind in {p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY}
            if p.name == 'task':
                init_args['task'] = task
            elif p.name == 'cfg':
                init_args['cfg'] = cfg
            elif hasattr(cfg, p.name):
                init_args[p.name] = getattr(cfg, p.name)
            elif p.default != p.empty:
                pass
            else:
                raise NotImplementedError('Unable to infer Criterion arguments, please implement {}.build_criterion'.format(cls.__name__))
        return cls(**init_args)

    def forward(self, model, sample, reduce=True):
        """Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """
        raise NotImplementedError

    @staticmethod
    def aggregate_logging_outputs(logging_outputs: 'List[Dict[str, Any]]') ->Dict[str, Any]:
        """Aggregate logging outputs from data parallel training."""
        utils.deprecation_warning('The aggregate_logging_outputs API is deprecated. Please use the reduce_metrics API instead.')
        raise NotImplementedError

    @classmethod
    def reduce_metrics(cls, logging_outputs: 'List[Dict[str, Any]]') ->None:
        """Aggregate logging outputs from data parallel training."""
        utils.deprecation_warning('Criterions should implement the reduce_metrics API. Falling back to deprecated aggregate_logging_outputs API.')
        agg_logging_outputs = cls.aggregate_logging_outputs(logging_outputs)
        for k, v in agg_logging_outputs.items():
            if k in {'nsentences', 'ntokens', 'sample_size'}:
                continue
            metrics.log_scalar(k, v)

    @staticmethod
    def logging_outputs_can_be_summed() ->bool:
        """
        Whether the logging outputs returned by `forward` can be summed
        across workers prior to calling `reduce_metrics`. Setting this
        to True will improves distributed training speed.
        """
        return False


class FairseqOptimizer(object):

    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg

    @classmethod
    def add_args(cls, parser):
        """Add optimizer-specific arguments to the parser."""
        dc = getattr(cls, '__dataclass', None)
        if dc is not None:
            gen_parser_from_dataclass(parser, dc())

    @property
    def optimizer(self):
        """Return a torch.optim.optimizer.Optimizer instance."""
        if not hasattr(self, '_optimizer'):
            raise NotImplementedError
        if not isinstance(self._optimizer, torch.optim.Optimizer):
            raise ValueError('_optimizer must be an instance of torch.optim.Optimizer')
        return self._optimizer

    @optimizer.setter
    def optimizer(self, optimizer):
        """Reset optimizer instance."""
        if not hasattr(self, '_optimizer'):
            raise NotImplementedError
        if not isinstance(self._optimizer, torch.optim.Optimizer):
            raise ValueError('_optimizer must be an instance of torch.optim.Optimizer')
        self._optimizer = optimizer

    @property
    def optimizer_config(self):
        """
        Return a kwarg dictionary that will be used to override optimizer
        args stored in checkpoints. This allows us to load a checkpoint and
        resume training using a different set of optimizer args, e.g., with a
        different learning rate.
        """
        raise NotImplementedError

    @property
    def params(self):
        """Return an iterable of the parameters held by the optimizer."""
        for param_group in self.param_groups:
            for p in param_group['params']:
                yield p

    @property
    def param_groups(self):
        return self.optimizer.param_groups

    def __getstate__(self):
        return self._optimizer.__getstate__()

    def get_lr(self):
        """Return the current learning rate."""
        return self.param_groups[0]['lr']

    def set_lr(self, lr):
        """Set the learning rate."""
        for param_group in self.param_groups:
            param_group['lr'] = lr

    def state_dict(self):
        """Return the optimizer's state dict."""
        return self.optimizer.state_dict()

    def load_state_dict(self, state_dict, optimizer_overrides=None):
        """Load an optimizer state dict.

        In general we should prefer the configuration of the existing optimizer
        instance (e.g., learning rate) over that found in the state_dict. This
        allows us to resume training from a checkpoint using a new set of
        optimizer args.
        """
        self.optimizer.load_state_dict(state_dict)
        if optimizer_overrides is not None and len(optimizer_overrides) > 0:
            for group in self.param_groups:
                group.update(optimizer_overrides)

    def backward(self, loss):
        """Computes the sum of gradients of the given tensor w.r.t. graph leaves."""
        loss.backward()

    def all_reduce_grads(self, module):
        """Manually all-reduce gradients (if required)."""
        if hasattr(module, 'all_reduce_grads'):
            module.all_reduce_grads()

    def multiply_grads(self, c):
        """Multiplies grads by a constant *c*."""
        per_device_and_dtype_grads = defaultdict(lambda : defaultdict(list))
        for p in self.params:
            if p.grad is not None:
                if p.grad.is_sparse:
                    p.grad.data.mul_(c if torch.is_tensor(c) else c)
                else:
                    per_device_and_dtype_grads[p.grad.device][p.grad.dtype].append(p.grad.data)
        for device, per_dtype_grads in per_device_and_dtype_grads.items():
            for grads in per_dtype_grads.values():
                torch._foreach_mul_(grads, c if torch.is_tensor(c) else c)

    def clip_grad_norm(self, max_norm, aggregate_norm_fn=None):
        """Clips gradient norm."""
        return utils.clip_grad_norm_(self.params, max_norm, aggregate_norm_fn)

    def step(self, closure=None, scale=1.0, groups=None):
        """Performs a single optimization step."""
        if self.supports_step_with_scale:
            if self.supports_groups:
                self.optimizer.step(closure, scale=scale, groups=groups)
            else:
                self.optimizer.step(closure, scale=scale)
        else:
            if scale != 1.0:
                self.multiply_grads(1.0 / scale)
            if self.supports_groups:
                self.optimizer.step(closure, groups=groups)
            else:
                self.optimizer.step(closure)

    def zero_grad(self):
        """Clears the gradients of all optimized parameters."""
        for p in self.params:
            p.grad = None
        self.optimizer.zero_grad()

    @property
    def supports_memory_efficient_fp16(self):
        if hasattr(self.optimizer, 'supports_memory_efficient_fp16'):
            return self.optimizer.supports_memory_efficient_fp16
        return False

    @property
    def supports_step_with_scale(self):
        if hasattr(self.optimizer, 'supports_step_with_scale'):
            return self.optimizer.supports_step_with_scale
        return False

    @property
    def supports_groups(self):
        if hasattr(self.optimizer, 'supports_groups'):
            return self.optimizer.supports_groups
        return False

    @property
    def supports_flat_params(self):
        """
        Whether the optimizer supports collapsing of the model
        parameters/gradients into a single contiguous Tensor.
        """
        if hasattr(self.optimizer, 'supports_flat_params'):
            return self.optimizer.supports_flat_params
        return False

    def average_params(self):
        pass

    def broadcast_global_state_dict(self, state_dict):
        """
        Broadcasts a global state dict to all ranks.
        Useful for optimizers that shard state between ranks.
        """
        if hasattr(self.optimizer, 'broadcast_global_state_dict'):
            return self.optimizer.broadcast_global_state_dict(state_dict)
        else:
            return state_dict


class FairseqLRScheduler(object):

    def __init__(self, cfg, optimizer):
        super().__init__()
        if optimizer is not None and not isinstance(optimizer, FairseqOptimizer):
            raise ValueError('optimizer must be an instance of FairseqOptimizer')
        self.cfg = cfg
        self.optimizer = optimizer
        self.best = None

    @classmethod
    def add_args(cls, parser):
        """Add arguments to the parser for this LR scheduler."""
        dc = getattr(cls, '__dataclass', None)
        if dc is not None:
            gen_parser_from_dataclass(parser, dc())

    def state_dict(self):
        """Return the LR scheduler state dict."""
        return {'best': self.best}

    def load_state_dict(self, state_dict):
        """Load an LR scheduler state dict."""
        self.best = state_dict['best']

    def step_begin_epoch(self, epoch):
        """Update the learning rate at the beginning of the given epoch."""
        pass

    def step(self, epoch, val_loss=None):
        """Update the learning rate at the end of the given epoch."""
        if val_loss is not None:
            if self.best is None:
                self.best = val_loss
            else:
                self.best = min(self.best, val_loss)

    def step_update(self, num_updates):
        """Update the learning rate after each update."""
        return self.optimizer.get_lr()


TASK_REGISTRY = {}


def _set_legacy_defaults(args, cls):
    """Helper to set default arguments based on *add_args*."""
    if not hasattr(cls, 'add_args'):
        return
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS, allow_abbrev=False)
    cls.add_args(parser)
    defaults = argparse.Namespace()
    for action in parser._actions:
        if action.dest is not argparse.SUPPRESS:
            if not hasattr(defaults, action.dest):
                if action.default is not argparse.SUPPRESS:
                    setattr(defaults, action.dest, action.default)
    for key, default_value in vars(defaults).items():
        if not hasattr(args, key):
            setattr(args, key, default_value)


class omegaconf_no_object_check:

    def __init__(self):
        if hasattr(_utils, 'is_primitive_type'):
            self.old_is_primitive = _utils.is_primitive_type
        else:
            self.old_is_primitive = _utils.is_primitive_type_annotation

    def __enter__(self):
        if hasattr(_utils, 'is_primitive_type'):
            _utils.is_primitive_type = lambda _: True
        else:
            _utils.is_primitive_type_annotation = lambda _: True

    def __exit__(self, type, value, traceback):
        if hasattr(_utils, 'is_primitive_type'):
            _utils.is_primitive_type = self.old_is_primitive
        else:
            _utils.is_primitive_type_annotation = self.old_is_primitive


ARCH_MODEL_NAME_REGISTRY = {}


REGISTRIES = {}


TASK_DATACLASS_REGISTRY = {}


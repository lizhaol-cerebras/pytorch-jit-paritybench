import sys
_module = sys.modules[__name__]
del sys
abstractions2 = _module
abstractions3 = _module
examples = _module
beautiful_cartpole = _module
beautiful_cifar = _module
beautiful_mnist = _module
beautiful_mnist_multigpu = _module
coder = _module
compile_efficientnet = _module
compile_tensorflow = _module
conversation = _module
efficientnet = _module
flux1 = _module
gpt2 = _module
handcode_opt = _module
hlb_cifar10 = _module
llama = _module
llama3 = _module
export = _module
train_gpt2 = _module
mamba = _module
mask_rcnn = _module
mixtral = _module
dataloader = _module
helpers = _module
initializers = _module
losses = _module
lr_schedulers = _module
metrics = _module
model_eval = _module
model_spec = _module
model_train = _module
mnist_gan = _module
openelm = _module
compile2 = _module
compile3 = _module
beautiful_mnist_mlx = _module
beautiful_mnist_torch = _module
lightupbutton = _module
sdv2 = _module
sdxl = _module
serious_mnist = _module
simple_conv_bn = _module
so_vits_svc = _module
preprocess = _module
stable_diffusion = _module
stunning_mnist = _module
train_efficientnet = _module
train_resnet = _module
transformer = _module
vgg7 = _module
waifu2x = _module
vit = _module
vits = _module
compile = _module
whisper = _module
yolov3 = _module
yolov8 = _module
coreml_ane = _module
dcompile = _module
hwx_parse = _module
struct_recover = _module
new_patch = _module
ane = _module
testconv = _module
ops_ane = _module
benchmark_matmul = _module
archprobe = _module
assembly = _module
assembly_arm64 = _module
assembly_ptx = _module
assembly_rdna = _module
test = _module
asm = _module
augment = _module
cstyle = _module
graph_hip = _module
hsa_driver = _module
hsa_graph = _module
ops_hip = _module
ops_hsa = _module
ops_rhip = _module
ops_webgl = _module
ops_webgpu = _module
rdna = _module
triton = _module
datasets = _module
coco = _module
fake_imagenet_from_mnist = _module
imagenet = _module
imagenet_download = _module
kits19 = _module
librispeech = _module
openimages = _module
preprocess_imagenet = _module
squad = _module
wikipedia = _module
wikipedia_download = _module
adreno = _module
disk_read_speed = _module
invoke_bug = _module
invoke_bug_2 = _module
run = _module
run_3 = _module
dump_cache = _module
export_model = _module
f16_w_uint32 = _module
amx = _module
cuda_matmul = _module
fuzz_matmul = _module
gemm = _module
hip_matmul = _module
intel_xmx = _module
jax_pmatmul = _module
metal_conv = _module
metal_matmul = _module
metal_matvec = _module
mlx_matmul = _module
real_pmatmul = _module
simple_conv = _module
simple_matmul = _module
simple_matvec = _module
tf_gemm = _module
tinygrad_nv_matmul = _module
torch_gemm = _module
triton_nv_matmul = _module
tvm_gemm = _module
gradcheck = _module
hip_events = _module
hip_ioctl = _module
test_kfd_2 = _module
test_pm4 = _module
test_sdma_fun = _module
introspection = _module
sentencepiece_model_pb2 = _module
lr_scheduler = _module
mcts_search = _module
amddriver = _module
amdgpu = _module
driver = _module
gpu = _module
mockgpu = _module
nvdriver = _module
nvgpu = _module
bert = _module
clip = _module
convnext = _module
inception = _module
resnet = _module
retinanet = _module
rnnt = _module
t5 = _module
unet = _module
unet3d = _module
multitensor = _module
nv_ioctl = _module
onnx = _module
onnx_ops = _module
extract_dataset = _module
extract_policynet = _module
extract_sa_pairs = _module
get_action_space = _module
pretrain_valuenet = _module
rl = _module
run_qnet = _module
search = _module
test_beam_search = _module
test_helpers = _module
test_net = _module
test_time_linearizer = _module
msm_kgsl = _module
opencl_ioctl = _module
qcom_opencl_interop = _module
resnet_mlx = _module
resnet_tinygrad = _module
ring_copy = _module
thneed = _module
threefry = _module
to_movement_ops = _module
training = _module
transfer_speed = _module
setup = _module
sz = _module
external_benchmark_hip_compile = _module
external_benchmark_load_stable_diffusion = _module
external_benchmark_multitensor_allreduce = _module
external_benchmark_openpilot = _module
external_benchmark_resnet = _module
external_benchmark_schedule = _module
external_cl_half_max = _module
external_gpu_fail_osx = _module
external_hip_compiler_bug = _module
external_jit_failure = _module
external_llama_eval = _module
external_metal_compile_fail = _module
external_model_benchmark = _module
external_multi_gpu = _module
external_osx_profiling = _module
external_test_amd = _module
external_test_datasets = _module
external_test_embedding = _module
external_test_example = _module
external_test_hcq = _module
external_test_hip_compile = _module
external_test_hsa_driver = _module
external_test_image = _module
external_test_jit_on_models = _module
external_test_llama3_ff = _module
external_test_lm_head = _module
external_test_losses = _module
external_test_mamba = _module
external_test_metrics = _module
external_test_mnist_data_select = _module
external_test_nv = _module
external_test_onnx_backend = _module
external_test_opt = _module
external_test_optim = _module
external_test_speed_llama = _module
external_test_speed_theoretical = _module
external_test_valid_remove = _module
external_test_whisper_librispeech = _module
external_test_yolo = _module
external_test_yolov8 = _module
fuzz_graph = _module
fuzz_kfd = _module
fuzz_linearizer = _module
fuzz_schedule = _module
fuzz_shapetracker = _module
fuzz_shapetracker_math = _module
fuzz_symbolic = _module
fuzz_uops = _module
graph_batchnorm = _module
external_benchmark_bert = _module
external_test_checkpoint_loading = _module
create_pretraining_data = _module
external_test_preprocessing_part = _module
pick_eval_samples = _module
tokenization = _module
lars_optimizer = _module
lars_util = _module
dice = _module
kits19 = _module
process_replay = _module
reset = _module
speed_beam_v_hcopt = _module
speed_compare_cuda_nv = _module
speed_compare_cuda_ptx = _module
verify_kernel = _module
imported = _module
test_indexing = _module
models = _module
test_bert = _module
test_efficientnet = _module
test_end2end = _module
test_mnist = _module
test_onnx = _module
test_real_world = _module
test_resnet = _module
test_rnnt = _module
test_train = _module
test_waifu2x = _module
test_whisper = _module
test_arange = _module
test_assign = _module
test_compile_failures = _module
test_const_folding = _module
test_conv = _module
test_conv_shapetracker = _module
test_copy_speed = _module
test_device_speed = _module
test_dtype = _module
test_dtype_alu = _module
test_fusion_op = _module
test_fuzz_shape_ops = _module
test_gc = _module
test_graph = _module
test_hcq = _module
test_image_dtype = _module
test_jit = _module
test_kernel_cache = _module
test_lazybuffer = _module
test_linearizer = _module
test_linearizer_dumb = _module
test_linearizer_failures = _module
test_linearizer_overflows = _module
test_masked_st = _module
test_metal = _module
test_method_cache = _module
test_multitensor = _module
test_net_speed = _module
test_nn = _module
test_ocl = _module
test_ops = _module
test_optim = _module
test_pickle = _module
test_profiler = _module
test_randomness = _module
test_rearrange_einops = _module
test_renderer_failures = _module
test_sample = _module
test_schedule = _module
test_search = _module
test_setitem = _module
test_specific_conv = _module
test_speed_v_torch = _module
test_subbuffer = _module
test_symbolic_jit = _module
test_symbolic_ops = _module
test_symbolic_shapetracker = _module
test_tensor = _module
test_tensor_data = _module
test_tensor_variable = _module
test_tiny = _module
test_to_numpy = _module
test_transcendental = _module
test_uop_graph = _module
test_uops = _module
test_uops_stats = _module
test_viz = _module
test_winograd = _module
test_zero_copy = _module
testextra = _module
test_export_model = _module
test_lr_scheduler = _module
unit = _module
test_device = _module
test_disk_cache = _module
test_disk_tensor = _module
test_elf = _module
test_gguf = _module
test_graph_rewrite = _module
test_pattern_matcher = _module
test_qcom = _module
test_shapetracker = _module
test_shapetracker_math = _module
test_shm_tensor = _module
test_simplify_valid_idx = _module
test_tqdm = _module
test_transcendental_helpers = _module
test_uop_resolve = _module
test_uop_symbolic = _module
test_uop_vmin_vmax = _module
test_verify_ast = _module
test_view = _module
tinygrad = _module
codegen = _module
kernel = _module
linearize = _module
lowerer = _module
transcendental = _module
uopgraph = _module
device = _module
dtype = _module
engine = _module
fuse = _module
jit = _module
lazy = _module
memory = _module
realize = _module
schedule = _module
function = _module
multi = _module
nn = _module
optim = _module
state = _module
ops = _module
renderer = _module
llvmir = _module
ptx = _module
runtime = _module
amd_gpu = _module
comgr = _module
cuda = _module
hip = _module
hsa = _module
io_uring = _module
kfd = _module
kgsl = _module
libc = _module
nv_gpu = _module
nvrtc = _module
opencl = _module
qcom_dsp = _module
graph = _module
clang = _module
hcq = _module
metal = _module
ops_amd = _module
ops_clang = _module
ops_cloud = _module
ops_cuda = _module
ops_disk = _module
ops_dsp = _module
ops_gpu = _module
ops_llvm = _module
ops_metal = _module
ops_npy = _module
ops_nv = _module
ops_python = _module
ops_qcom = _module
support = _module
compiler_cuda = _module
compiler_hip = _module
elf = _module
shape = _module
shapetracker = _module
view = _module
tensor = _module
serve = _module

from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import numpy as np


import math


import time


from typing import List


from typing import Dict


from typing import Optional


from typing import Union


from typing import Tuple


from typing import Callable


from torch.nn import functional as F


from torchvision import transforms as T


from torchvision.transforms import functional as Ft


import random


import torch


from torchvision.utils import make_grid


from torchvision.utils import save_image


from typing import cast


from functools import partial


from torch import nn


from torch import optim


from collections import namedtuple


from typing import Any


from itertools import chain


from collections import defaultdict


import functools


from scipy import signal


from scipy import ndimage


import torch.nn.functional as F


import torch.mps


import re


import torchaudio


import torch.nn as nn


import scipy.ndimage


from torch.utils.data import Dataset


from torchvision import transforms


import copy


import warnings


from math import prod


from typing import get_args


from collections import OrderedDict


import itertools


import inspect


import string


from typing import ClassVar


from typing import Type


from typing import Sequence


from typing import DefaultDict


from typing import Literal


class Model(nn.Module):

    def __init__(self):
        super().__init__()
        self.c1 = nn.Conv2d(1, 32, 5)
        self.c2 = nn.Conv2d(32, 32, 5)
        self.bn1 = nn.BatchNorm2d(32)
        self.m1 = nn.MaxPool2d(2)
        self.c3 = nn.Conv2d(32, 64, 3)
        self.c4 = nn.Conv2d(64, 64, 3)
        self.bn2 = nn.BatchNorm2d(64)
        self.m2 = nn.MaxPool2d(2)
        self.lin = nn.Linear(576, 10)

    def forward(self, x):
        x = nn.functional.relu(self.c1(x))
        x = nn.functional.relu(self.c2(x), 0)
        x = self.m1(self.bn1(x))
        x = nn.functional.relu(self.c3(x), 0)
        x = nn.functional.relu(self.c4(x), 0)
        x = self.m2(self.bn2(x))
        return self.lin(torch.flatten(x, 1))


class Block(nn.Module):

    def __init__(self, in_dims, dims, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_dims, dims, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm(dims)
        self.conv2 = nn.Conv2d(dims, dims, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm(dims)
        self.downsample = []
        if stride != 1:
            self.downsample = [nn.Conv2d(in_dims, dims, kernel_size=1, stride=stride, bias=False), nn.BatchNorm(dims)]

    def __call__(self, x):
        out = nn.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        for l in self.downsample:
            x = l(x)
        out += x
        out = nn.relu(out)
        return out


class ResNet(nn.Module):

    def __init__(self, block, num_blocks, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 64, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 128, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 256, 512, num_blocks[3], stride=2)
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, block, in_dims, dims, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(in_dims, dims, stride))
            in_dims = dims
        return layers

    def __call__(self, x):
        x = nn.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        for l in (self.layer1 + self.layer2 + self.layer3 + self.layer4):
            x = l(x)
        x = mx.mean(x, axis=[1, 2])
        x = self.fc(x)
        return x


def to_one_hot(array, layout, channel_axis):
    if len(array.shape) >= 5:
        array = torch.squeeze(array, dim=channel_axis)
    array = F.one_hot(array.long(), num_classes=3)
    if layout == 'NCDHW':
        array = array.permute(0, 4, 1, 2, 3).float()
    return array


class Dice:

    def __init__(self, to_onehot_y: 'bool'=True, to_onehot_x: 'bool'=False, use_softmax: 'bool'=True, use_argmax: 'bool'=False, include_background: 'bool'=False, layout: 'str'='NCDHW'):
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        self.to_onehot_x = to_onehot_x
        self.use_softmax = use_softmax
        self.use_argmax = use_argmax
        self.smooth_nr = 1e-06
        self.smooth_dr = 1e-06
        self.layout = layout

    def __call__(self, prediction, target):
        if self.layout == 'NCDHW':
            channel_axis = 1
            reduce_axis = list(range(2, len(prediction.shape)))
        else:
            channel_axis = -1
            reduce_axis = list(range(1, len(prediction.shape) - 1))
        num_pred_ch = prediction.shape[channel_axis]
        if self.use_softmax:
            prediction = torch.softmax(prediction, dim=channel_axis)
        elif self.use_argmax:
            prediction = torch.argmax(prediction, dim=channel_axis)
        if self.to_onehot_y:
            target = to_one_hot(target, self.layout, channel_axis)
        if self.to_onehot_x:
            prediction = to_one_hot(prediction, self.layout, channel_axis)
        if not self.include_background:
            assert num_pred_ch > 1, f'To exclude background the prediction needs more than one channel. Got {num_pred_ch}.'
            if self.layout == 'NCDHW':
                target = target[:, 1:]
                prediction = prediction[:, 1:]
            else:
                target = target[..., 1:]
                prediction = prediction[..., 1:]
        assert target.shape == prediction.shape, f'Target and prediction shape do not match. Target: ({target.shape}), prediction: ({prediction.shape}).'
        intersection = torch.sum(target * prediction, dim=reduce_axis)
        target_sum = torch.sum(target, dim=reduce_axis)
        prediction_sum = torch.sum(prediction, dim=reduce_axis)
        return (2.0 * intersection + self.smooth_nr) / (target_sum + prediction_sum + self.smooth_dr)


class DiceCELoss(nn.Module):

    def __init__(self, to_onehot_y, use_softmax, layout, include_background):
        super(DiceCELoss, self).__init__()
        self.dice = Dice(to_onehot_y=to_onehot_y, use_softmax=use_softmax, layout=layout, include_background=include_background)
        self.cross_entropy = nn.CrossEntropyLoss()

    def forward(self, y_pred, y_true):
        cross_entropy = self.cross_entropy(y_pred, torch.squeeze(y_true, dim=1).long())
        dice = torch.mean(1.0 - self.dice(y_pred, y_true))
        return (dice + cross_entropy) / 2



from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import numpy as np


import torch


from torch import nn


from torch.backends import cudnn


from torch.autograd import Variable


import random


import torchvision.utils as vutils


import math


from torch.optim import lr_scheduler


import torch.optim as optim


import torch.nn.functional as F


import torchvision.transforms as v_transforms


import torchvision.utils as v_utils


import torchvision.datasets as v_datasets


from torch.utils.data import DataLoader


from torch.utils.data import TensorDataset


from torch.utils.data import Dataset


import logging


from torchvision import datasets


from torchvision import transforms


import scipy.io as sio


from torch.utils import data


import torchvision.transforms as standard_transforms


import torch.nn as nn


import torch.nn.init as init


import torchvision.transforms as transforms


from sklearn.utils.class_weight import compute_class_weight


import time


class BinaryCrossEntropy(nn.Module):

    def __init__(self):
        super().__init__()
        self.loss = nn.BCELoss()

    def forward(self, logits, labels):
        loss = self.loss(logits, labels)
        return loss


class CrossEntropyLoss(nn.Module):

    def __init__(self, config=None):
        super(CrossEntropyLoss, self).__init__()
        if config == None:
            self.loss = nn.CrossEntropyLoss()
        else:
            class_weights = np.load(config.class_weights)
            self.loss = nn.CrossEntropyLoss(ignore_index=config.ignore_index, weight=torch.from_numpy(class_weights.astype(np.float32)), size_average=True, reduce=True)

    def forward(self, inputs, targets):
        return self.loss(inputs, targets)


class CrossEntropyLoss2d(nn.Module):

    def __init__(self, weight=None, size_average=True):
        super(CrossEntropyLoss2d, self).__init__()
        self.loss = nn.CrossEntropyLoss()

    def forward(self, logits, labels):
        loss = self.loss(logits, labels)
        return loss


class HuberLoss(nn.Module):

    def __init__(self):
        super().__init__()
        self.loss = nn.SmoothL1Loss()

    def forward(self, logits, labels):
        loss = self.loss(logits, labels)
        return loss


class LearnedGroupConv(nn.Module):
    global_progress = 0.0

    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, condense_factor=None, dropout_rate=0.0):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.condense_factor = condense_factor
        self.groups = groups
        self.dropout_rate = dropout_rate
        assert self.in_channels % self.groups == 0, 'group value is not divisible by input channels'
        assert self.in_channels % self.condense_factor == 0, 'condensation factor is not divisible by input channels'
        assert self.out_channels % self.groups == 0, 'group value is not divisible by output channels'
        self.batch_norm = nn.BatchNorm2d(in_channels)
        self.relu = nn.ReLU(inplace=True)
        if self.dropout_rate > 0:
            self.dropout = nn.Dropout(self.dropout_rate, inplace=False)
        self.conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=1, bias=False)
        self.register_buffer('_count', torch.zeros(1))
        self.register_buffer('_stage', torch.zeros(1))
        self.register_buffer('_mask', torch.ones(self.conv.weight.size()))

    def forward(self, x):
        out = self.batch_norm(x)
        out = self.relu(out)
        if self.dropout_rate > 0:
            out = self.dropout(out)
        self.check_if_drop()
        weight = self.conv.weight * self.mask
        out_conv = F.conv2d(input=out, weight=weight, bias=None, stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=1)
        return out_conv
    """
    Paper: Sec 3.1: Condensation procedure: number of epochs for each condensing stage: M/2(C-1)
    Paper: Sec 3.1: Condensation factor: allow each group to select R/C of inputs.
    - During training a fraction of (Câˆ’1)/C connections are removed after each of the C-1 condensing stages
    - we remove columns in Fg (by zeroing them out) if their L1-norm is small compared to the L1-norm of other columns.
    """

    def check_if_drop(self):
        current_progress = LearnedGroupConv.global_progress
        delta = 0
        for i in range(self.condense_factor - 1):
            if current_progress * 2 < (i + 1) / (self.condense_factor - 1):
                stage = i
                break
        else:
            stage = self.condense_factor - 1
        if not self.reach_stage(stage):
            self.stage = stage
            delta = self.in_channels // self.condense_factor
            None
        if delta > 0:
            self.drop(delta)
        return

    def drop(self, delta):
        weight = self.conv.weight * self.mask
        None
        assert weight.size()[-1] == 1
        weight = weight.abs().squeeze()
        assert weight.size()[0] == self.out_channels
        assert weight.size()[1] == self.in_channels
        d_out = self.out_channels // self.groups
        None
        weight = weight.view(d_out, self.groups, self.in_channels)
        None
        weight = weight.transpose(0, 1).contiguous()
        None
        weight = weight.view(self.out_channels, self.in_channels)
        None
        for i in range(self.groups):
            wi = weight[i * d_out:(i + 1) * d_out, :]
            di = wi.sum(0).sort()[1][self.count:self.count + delta]
            for d in di.data:
                self._mask[i::self.groups, d, :, :].fill_(0)
        self.count = self.count + delta

    def reach_stage(self, stage):
        return (self._stage >= stage).all()

    @property
    def count(self):
        return int(self._count[0])

    @count.setter
    def count(self, val):
        self._count.fill_(val)

    @property
    def stage(self):
        return int(self._stage[0])

    @stage.setter
    def stage(self, val):
        self._stage.fill_(val)

    @property
    def mask(self):
        return Variable(self._mask)


class DenseLayer(nn.Module):

    def __init__(self, in_channels, growth_rate, config):
        super().__init__()
        self.config = config
        self.conv_bottleneck = self.config.conv_bottleneck
        self.group1x1 = self.config.group1x1
        self.group3x3 = self.config.group3x3
        self.condense_factor = self.config.condense_factor
        self.dropout_rate = self.config.dropout_rate
        self.conv_1 = LearnedGroupConv(in_channels=in_channels, out_channels=self.conv_bottleneck * growth_rate, kernel_size=1, groups=self.group1x1, condense_factor=self.condense_factor, dropout_rate=self.dropout_rate)
        self.batch_norm = nn.BatchNorm2d(self.conv_bottleneck * growth_rate)
        self.relu = nn.ReLU(inplace=True)
        self.conv_2 = nn.Conv2d(in_channels=self.conv_bottleneck * growth_rate, out_channels=growth_rate, kernel_size=3, padding=1, stride=1, groups=self.group3x3, bias=False)

    def forward(self, x):
        out = self.conv_1(x)
        out = self.batch_norm(out)
        out = self.relu(out)
        out = self.conv_2(out)
        return torch.cat([x, out], 1)


class DenseBlock(nn.Sequential):

    def __init__(self, num_layers, in_channels, growth_rate, config):
        super().__init__()
        for layer_id in range(num_layers):
            layer = DenseLayer(in_channels=in_channels + layer_id * growth_rate, growth_rate=growth_rate, config=config)
            self.add_module('dense_layer_%d' % (layer_id + 1), layer)


def init_model_weights(m):
    for m in m.modules():
        if isinstance(m, nn.Conv2d):
            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            m.weight.data.normal_(0, math.sqrt(2.0 / n))
        elif isinstance(m, nn.BatchNorm2d):
            m.weight.data.fill_(1)
            m.bias.data.zero_()
        elif isinstance(m, nn.Linear):
            m.bias.data.zero_()


class CondenseNet(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.stages = self.config.stages
        self.growth_rate = self.config.growth_rate
        assert len(self.stages) == len(self.growth_rate)
        self.init_stride = self.config.init_stride
        self.pool_size = self.config.pool_size
        self.num_classes = self.config.num_classes
        self.progress = 0.0
        self.num_filters = 2 * self.growth_rate[0]
        """
        Initializing layers
        """
        self.transition_pool = nn.AvgPool2d(kernel_size=2, stride=2)
        self.pool = nn.AvgPool2d(self.pool_size)
        self.relu = nn.ReLU(inplace=True)
        self.init_conv = nn.Conv2d(in_channels=self.config.input_channels, out_channels=self.num_filters, kernel_size=3, stride=self.init_stride, padding=1, bias=False)
        self.denseblock_one = DenseBlock(num_layers=self.stages[0], in_channels=self.num_filters, growth_rate=self.growth_rate[0], config=self.config)
        self.num_filters += self.stages[0] * self.growth_rate[0]
        self.denseblock_two = DenseBlock(num_layers=self.stages[1], in_channels=self.num_filters, growth_rate=self.growth_rate[1], config=self.config)
        self.num_filters += self.stages[1] * self.growth_rate[1]
        self.denseblock_three = DenseBlock(num_layers=self.stages[2], in_channels=self.num_filters, growth_rate=self.growth_rate[2], config=self.config)
        self.num_filters += self.stages[2] * self.growth_rate[2]
        self.batch_norm = nn.BatchNorm2d(self.num_filters)
        self.classifier = nn.Linear(self.num_filters, self.num_classes)
        self.apply(init_model_weights)

    def forward(self, x, progress=None):
        if progress:
            LearnedGroupConv.global_progress = progress
        x = self.init_conv(x)
        x = self.denseblock_one(x)
        x = self.transition_pool(x)
        x = self.denseblock_two(x)
        x = self.transition_pool(x)
        x = self.denseblock_three(x)
        x = self.batch_norm(x)
        x = self.relu(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        out = self.classifier(x)
        return out


class non_bottleneck_1d(nn.Module):

    def __init__(self, n_channel, drop_rate, dilated):
        super().__init__()
        self.conv3x1_1 = nn.Conv2d(n_channel, n_channel, (3, 1), stride=1, padding=(1, 0), bias=True)
        self.conv1x3_1 = nn.Conv2d(n_channel, n_channel, (1, 3), stride=1, padding=(0, 1), bias=True)
        self.conv3x1_2 = nn.Conv2d(n_channel, n_channel, (3, 1), stride=1, padding=(1 * dilated, 0), bias=True, dilation=(dilated, 1))
        self.conv1x3_2 = nn.Conv2d(n_channel, n_channel, (1, 3), stride=1, padding=(0, 1 * dilated), bias=True, dilation=(1, dilated))
        self.bn1 = nn.BatchNorm2d(n_channel, eps=0.001)
        self.bn2 = nn.BatchNorm2d(n_channel, eps=0.001)
        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout2d(drop_rate)

    def forward(self, input):
        output = self.conv3x1_1(input)
        output = self.relu(output)
        output = self.conv1x3_1(output)
        output = self.bn1(output)
        output = self.relu(output)
        output = self.conv3x1_2(output)
        output = self.relu(output)
        output = self.conv1x3_2(output)
        output = self.bn2(output)
        if self.dropout.p != 0:
            output = self.dropout(output)
        return self.relu(output + input)


class DownsamplerBlock(nn.Module):

    def __init__(self, in_channel, out_channel):
        super().__init__()
        self.conv = nn.Conv2d(in_channel, out_channel - in_channel, (3, 3), stride=2, padding=1, bias=True)
        self.pool = nn.MaxPool2d(2, stride=2)
        self.bn = nn.BatchNorm2d(out_channel, eps=0.001)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, input):
        output = torch.cat([self.conv(input), self.pool(input)], 1)
        output = self.bn(output)
        return self.relu(output)


class UpsamplerBlock(nn.Module):

    def __init__(self, in_channel, out_channel):
        super().__init__()
        self.conv = nn.ConvTranspose2d(in_channel, out_channel, 3, stride=2, padding=1, output_padding=1, bias=True)
        self.bn = nn.BatchNorm2d(out_channel, eps=0.001)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, input):
        output = self.conv(input)
        output = self.bn(output)
        return self.relu(output)


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)


class Discriminator(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.relu = nn.LeakyReLU(self.config.relu_slope, inplace=True)
        self.conv1 = nn.Conv2d(in_channels=self.config.input_channels, out_channels=self.config.num_filt_d, kernel_size=4, stride=2, padding=1, bias=False)
        self.conv2 = nn.Conv2d(in_channels=self.config.num_filt_d, out_channels=self.config.num_filt_d * 2, kernel_size=4, stride=2, padding=1, bias=False)
        self.batch_norm1 = nn.BatchNorm2d(self.config.num_filt_d * 2)
        self.conv3 = nn.Conv2d(in_channels=self.config.num_filt_d * 2, out_channels=self.config.num_filt_d * 4, kernel_size=4, stride=2, padding=1, bias=False)
        self.batch_norm2 = nn.BatchNorm2d(self.config.num_filt_d * 4)
        self.conv4 = nn.Conv2d(in_channels=self.config.num_filt_d * 4, out_channels=self.config.num_filt_d * 8, kernel_size=4, stride=2, padding=1, bias=False)
        self.batch_norm3 = nn.BatchNorm2d(self.config.num_filt_d * 8)
        self.conv5 = nn.Conv2d(in_channels=self.config.num_filt_d * 8, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False)
        self.out = nn.Sigmoid()
        self.apply(weights_init)

    def forward(self, x):
        out = self.conv1(x)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.batch_norm1(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.batch_norm2(out)
        out = self.relu(out)
        out = self.conv4(out)
        out = self.batch_norm3(out)
        out = self.relu(out)
        out = self.conv5(out)
        out = self.out(out)
        return out.view(-1, 1).squeeze(1)


class Generator(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.relu = nn.ReLU(inplace=True)
        self.deconv1 = nn.ConvTranspose2d(in_channels=self.config.g_input_size, out_channels=self.config.num_filt_g * 8, kernel_size=4, stride=1, padding=0, bias=False)
        self.batch_norm1 = nn.BatchNorm2d(self.config.num_filt_g * 8)
        self.deconv2 = nn.ConvTranspose2d(in_channels=self.config.num_filt_g * 8, out_channels=self.config.num_filt_g * 4, kernel_size=4, stride=2, padding=1, bias=False)
        self.batch_norm2 = nn.BatchNorm2d(self.config.num_filt_g * 4)
        self.deconv3 = nn.ConvTranspose2d(in_channels=self.config.num_filt_g * 4, out_channels=self.config.num_filt_g * 2, kernel_size=4, stride=2, padding=1, bias=False)
        self.batch_norm3 = nn.BatchNorm2d(self.config.num_filt_g * 2)
        self.deconv4 = nn.ConvTranspose2d(in_channels=self.config.num_filt_g * 2, out_channels=self.config.num_filt_g, kernel_size=4, stride=2, padding=1, bias=False)
        self.batch_norm4 = nn.BatchNorm2d(self.config.num_filt_g)
        self.deconv5 = nn.ConvTranspose2d(in_channels=self.config.num_filt_g, out_channels=self.config.input_channels, kernel_size=4, stride=2, padding=1, bias=False)
        self.out = nn.Tanh()
        self.apply(weights_init)

    def forward(self, x):
        out = self.deconv1(x)
        out = self.batch_norm1(out)
        out = self.relu(out)
        out = self.deconv2(out)
        out = self.batch_norm2(out)
        out = self.relu(out)
        out = self.deconv3(out)
        out = self.batch_norm3(out)
        out = self.relu(out)
        out = self.deconv4(out)
        out = self.batch_norm4(out)
        out = self.relu(out)
        out = self.deconv5(out)
        out = self.out(out)
        return out


class DQN(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_channels=self.config.input_channels, out_channels=self.config.conv_filters[0], kernel_size=5, stride=2, padding=0, bias=True)
        self.bn1 = nn.BatchNorm2d(self.config.conv_filters[0])
        self.conv2 = nn.Conv2d(in_channels=self.config.conv_filters[0], out_channels=self.config.conv_filters[1], kernel_size=5, stride=2, padding=0, bias=True)
        self.bn2 = nn.BatchNorm2d(self.config.conv_filters[1])
        self.conv3 = nn.Conv2d(in_channels=self.config.conv_filters[1], out_channels=self.config.conv_filters[2], kernel_size=5, stride=2, padding=0, bias=True)
        self.bn3 = nn.BatchNorm2d(self.config.conv_filters[2])
        self.linear = nn.Linear(448, self.config.num_classes)
        self.apply(weights_init)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.bn3(x)
        x = self.relu(x)
        out = self.linear(x.view(x.size(0), -1))
        return out


class ERF(nn.Module):

    def __init__(self, config, encoder=None):
        super().__init__()
        self.config = config
        self.num_classes = self.config.num_classes
        if encoder == None:
            self.encoder_flag = True
            self.encoder_layers = nn.ModuleList()
            self.initial_block = DownsamplerBlock(self.config.input_channels, 16)
            self.encoder_layers.append(DownsamplerBlock(in_channel=16, out_channel=64))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0.03, dilated=1))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0.03, dilated=1))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0.03, dilated=1))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0.03, dilated=1))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0.03, dilated=1))
            self.encoder_layers.append(DownsamplerBlock(in_channel=64, out_channel=128))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=2))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=4))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=8))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=16))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=2))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=4))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=8))
            self.encoder_layers.append(non_bottleneck_1d(n_channel=128, drop_rate=0.3, dilated=16))
        else:
            self.encoder_flag = False
            self.encoder = encoder
        self.decoder_layers = nn.ModuleList()
        self.decoder_layers.append(UpsamplerBlock(in_channel=128, out_channel=64))
        self.decoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0, dilated=1))
        self.decoder_layers.append(non_bottleneck_1d(n_channel=64, drop_rate=0, dilated=1))
        self.decoder_layers.append(UpsamplerBlock(in_channel=64, out_channel=16))
        self.decoder_layers.append(non_bottleneck_1d(n_channel=16, drop_rate=0, dilated=1))
        self.decoder_layers.append(non_bottleneck_1d(n_channel=16, drop_rate=0, dilated=1))
        self.output_conv = nn.ConvTranspose2d(in_channels=16, out_channels=self.num_classes, kernel_size=2, stride=2, padding=0, output_padding=0, bias=True)

    def forward(self, x):
        if self.encoder_flag:
            output = self.initial_block(x)
            for layer in self.encoder_layers:
                output = layer(output)
        else:
            output = self.encoder(x)
        for layer in self.decoder_layers:
            output = layer(output)
        output = self.output_conv(output)
        return output


class Classifier(nn.Module):

    def __init__(self, num_classes):
        super().__init__()
        self.linear = nn.Linear(128, num_classes)

    def forward(self, input):
        output = input.view(input.size(0), 128)
        output = self.linear(output)
        return output


class Encoder(nn.Module):

    def __init__(self):
        super().__init__()
        self.initial_block = DownsamplerBlock(3, 16)
        self.layers = nn.ModuleList()
        self.layers.append(DownsamplerBlock(16, 64))
        for x in range(0, 5):
            self.layers.append(non_bottleneck_1d(64, 0.1, 1))
        self.layers.append(DownsamplerBlock(64, 128))
        for x in range(0, 2):
            self.layers.append(non_bottleneck_1d(128, 0.1, 2))
            self.layers.append(non_bottleneck_1d(128, 0.1, 4))
            self.layers.append(non_bottleneck_1d(128, 0.1, 8))
            self.layers.append(non_bottleneck_1d(128, 0.1, 16))

    def forward(self, input):
        output = self.initial_block(input)
        for layer in self.layers:
            output = layer(output)
        return output


class Features(nn.Module):

    def __init__(self):
        super().__init__()
        self.encoder = Encoder()
        self.extralayer1 = nn.MaxPool2d(2, stride=2)
        self.extralayer2 = nn.AvgPool2d(14, 1, 0)

    def forward(self, input):
        output = self.encoder(input)
        output = self.extralayer1(output)
        output = self.extralayer2(output)
        return output


class ERFNet(nn.Module):

    def __init__(self, num_classes):
        super().__init__()
        self.features = Features()
        self.classifier = Classifier(num_classes)

    def forward(self, input):
        output = self.features(input)
        output = self.classifier(output)
        return output


class Example(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.relu = nn.ReLU(inplace=True)
        self.conv = nn.Conv2d(in_channels=self.config.input_channels, out_channels=self.config.num_filters, kernel_size=3, stride=1, padding=1, bias=False)
        self.apply(weights_init)

    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        out = x.view(x.size(0), -1)
        return out


class Mnist(nn.Module):

    def __init__(self):
        super(Mnist, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)
        self.apply(weights_init)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


import torch
from torch.nn import MSELoss, ReLU
from types import SimpleNamespace


TESTCASES = [
    # (nn.Module, init_args, forward_args)
    (BinaryCrossEntropy,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (Classifier,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 128])], {})),
    (CrossEntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (CrossEntropyLoss2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (DenseBlock,
     lambda: ([], {'num_layers': 1, 'in_channels': 4, 'growth_rate': 4, 'config': SimpleNamespace(conv_bottleneck=4, group1x1=4, group3x3=4, condense_factor=4, dropout_rate=0.5)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (DenseLayer,
     lambda: ([], {'in_channels': 4, 'growth_rate': 4, 'config': SimpleNamespace(conv_bottleneck=4, group1x1=4, group3x3=4, condense_factor=4, dropout_rate=0.5)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Discriminator,
     lambda: ([], {'config': SimpleNamespace(relu_slope=4, input_channels=4, num_filt_d=4)}),
     lambda: ([torch.rand([4, 4, 64, 64])], {})),
    (ERF,
     lambda: ([], {'config': SimpleNamespace(num_classes=4, input_channels=4)}),
     lambda: ([torch.rand([4, 4, 64, 64])], {})),
    (Encoder,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {})),
    (Example,
     lambda: ([], {'config': SimpleNamespace(input_channels=4, num_filters=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (Features,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 256, 256])], {})),
    (Generator,
     lambda: ([], {'config': SimpleNamespace(g_input_size=4, num_filt_g=4, input_channels=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (HuberLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {})),
    (UpsamplerBlock,
     lambda: ([], {'in_channel': 4, 'out_channel': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
    (non_bottleneck_1d,
     lambda: ([], {'n_channel': 4, 'drop_rate': 0.5, 'dilated': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {})),
]


import sys
_module = sys.modules[__name__]
del sys
conftest = _module
gray_test = _module
conftest = _module
generate_examples = _module
image_registration = _module
conf = _module
kornia = _module
_2d = _module
base = _module
geometric = _module
affine = _module
center_crop = _module
crop = _module
elastic_transform = _module
fisheye = _module
horizontal_flip = _module
pad = _module
perspective = _module
resize = _module
resized_crop = _module
rotation = _module
shear = _module
thin_plate_spline = _module
translate = _module
vertical_flip = _module
intensity = _module
auto_contrast = _module
base = _module
box_blur = _module
brightness = _module
channel_dropout = _module
channel_shuffle = _module
clahe = _module
color_jiggle = _module
color_jitter = _module
contrast = _module
denormalize = _module
dissolving = _module
equalize = _module
erasing = _module
gamma = _module
gaussian_blur = _module
gaussian_illumination = _module
gaussian_noise = _module
grayscale = _module
hue = _module
invert = _module
jpeg = _module
linear_illumination = _module
median_blur = _module
motion_blur = _module
normalize = _module
planckian_jitter = _module
plasma = _module
posterize = _module
random_rain = _module
random_rgb_shift = _module
random_snow = _module
salt_pepper_noise = _module
saturation = _module
sharpness = _module
solarize = _module
mix = _module
base = _module
cutmix = _module
jigsaw = _module
mixup = _module
mosaic = _module
transplantation = _module
_3d = _module
base = _module
affine = _module
center_crop = _module
crop = _module
depthical_flip = _module
horizontal_flip = _module
perspective = _module
rotation = _module
vertical_flip = _module
equalize = _module
motion_blur = _module
augmentation = _module
auto = _module
autoaugment = _module
autoaugment = _module
ops = _module
base = _module
operations = _module
base = _module
policy = _module
rand_augment = _module
rand_augment = _module
trivial_augment = _module
trivial_augment = _module
base = _module
container = _module
augment = _module
base = _module
dispatcher = _module
image = _module
ops = _module
params = _module
patch = _module
video = _module
affine = _module
channel_dropout = _module
color_jiggle = _module
color_jitter = _module
crop = _module
cutmix = _module
gaussian_blur = _module
gaussian_illumination = _module
jigsaw = _module
jpeg = _module
linear_illumination = _module
mixup = _module
mosaic = _module
motion_blur = _module
perspective = _module
plain_uniform = _module
planckian_jitter = _module
posterize = _module
probability = _module
random_rain = _module
rectangle_earase = _module
resize = _module
salt_pepper_noise = _module
shear = _module
translate = _module
affine = _module
crop = _module
motion_blur = _module
perspective = _module
rotation = _module
random_generator = _module
base = _module
utils = _module
helpers = _module
param_validation = _module
color = _module
_colormap_data = _module
colormap = _module
gray = _module
hls = _module
hsv = _module
lab = _module
luv = _module
raw = _module
rgb = _module
sepia = _module
xyz = _module
ycbcr = _module
yuv = _module
config = _module
constants = _module
contrib = _module
classification = _module
connected_components = _module
diamond_square = _module
distance_transform = _module
edge_detection = _module
extract_patches = _module
face_detection = _module
histogram_matching = _module
image_stitching = _module
kmeans = _module
lambda_module = _module
models = _module
base = _module
common = _module
efficient_vit = _module
backbone = _module
model = _module
nn = _module
act = _module
norm = _module
ops = _module
list = _module
network = _module
rt_detr = _module
architecture = _module
hgnetv2 = _module
hybrid_encoder = _module
resnet_d = _module
rtdetr_head = _module
model = _module
post_processor = _module
sam = _module
common = _module
image_encoder = _module
mask_decoder = _module
prompt_encoder = _module
transformer = _module
model = _module
structures = _module
tiny_vit = _module
object_detection = _module
visual_prompter = _module
vit = _module
vit_mobile = _module
core = _module
_backend = _module
check = _module
external = _module
mixin = _module
image_module = _module
onnx = _module
module = _module
tensor_wrapper = _module
enhance = _module
adjust = _module
core = _module
equalization = _module
histogram = _module
integral = _module
jpeg = _module
normalize = _module
rescale = _module
shift_rgb = _module
zca = _module
feature = _module
adalam = _module
adalam = _module
core = _module
ransac = _module
utils = _module
affine_shape = _module
dedode = _module
decoder = _module
dedode = _module
dedode_models = _module
descriptor = _module
detector = _module
encoder = _module
dinov2 = _module
layers = _module
attention = _module
block = _module
dino_head = _module
drop_path = _module
layer_scale = _module
mlp = _module
patch_embed = _module
swiglu_ffn = _module
utils = _module
vgg = _module
defmo = _module
disk = _module
_unets = _module
blocks = _module
unet = _module
detector = _module
disk = _module
structs = _module
hardnet = _module
hynet = _module
integrated = _module
keynet = _module
laf = _module
lightglue = _module
lightglue_onnx = _module
lightglue = _module
download = _module
keypoints = _module
loftr = _module
resnet_fpn = _module
loftr = _module
loftr_module = _module
fine_preprocess = _module
linear_attention = _module
transformer = _module
coarse_matching = _module
fine_matching = _module
geometry = _module
position_encoding = _module
supervision = _module
matching = _module
mkd = _module
orientation = _module
responses = _module
scale_space_detector = _module
siftdesc = _module
sold2 = _module
backbones = _module
sold2 = _module
sold2_detector = _module
sosnet = _module
steerers = _module
tfeat = _module
filters = _module
bilateral = _module
blur = _module
blur_pool = _module
canny = _module
dexined = _module
dissolving = _module
filter = _module
gaussian = _module
guided = _module
in_range = _module
kernels = _module
kernels_geometry = _module
laplacian = _module
median = _module
motion = _module
sobel = _module
unsharp = _module
bbox = _module
boxes = _module
calibration = _module
distort = _module
pnp = _module
undistort = _module
camera = _module
distortion_affine = _module
distortion_kannala_brandt = _module
perspective = _module
pinhole = _module
projection_orthographic = _module
projection_z1 = _module
stereo = _module
conversions = _module
depth = _module
epipolar = _module
_metrics = _module
essential = _module
fundamental = _module
numeric = _module
projection = _module
scene = _module
triangulation = _module
homography = _module
keypoints = _module
liegroup = _module
_utils = _module
se2 = _module
se3 = _module
so2 = _module
so3 = _module
linalg = _module
line = _module
plane = _module
pose = _module
quaternion = _module
ransac = _module
ray = _module
solvers = _module
polynomial_solver = _module
subpix = _module
dsnt = _module
nms = _module
spatial_soft_argmax = _module
transform = _module
affwarp = _module
crop2d = _module
crop3d = _module
elastic_transform = _module
flips = _module
homography_warper = _module
image_registrator = _module
imgwarp = _module
pyramid = _module
thin_plate_spline = _module
vector = _module
grad_estimator = _module
ste = _module
image = _module
io = _module
io = _module
losses = _module
_utils = _module
cauchy = _module
charbonnier = _module
depth_smooth = _module
dice = _module
divergence = _module
focal = _module
geman_mcclure = _module
hausdorff = _module
lovasz_hinge = _module
lovasz_softmax = _module
ms_ssim = _module
psnr = _module
ssim = _module
ssim3d = _module
total_variation = _module
tversky = _module
welsch = _module
metrics = _module
accuracy = _module
average_meter = _module
confusion_matrix = _module
endpoint_error = _module
mean_average_precision = _module
mean_iou = _module
psnr = _module
ssim = _module
ssim3d = _module
_hf_models = _module
hf_onnx_community = _module
preprocessor = _module
base = _module
depth_estimation = _module
base = _module
depth_anything = _module
detection = _module
base = _module
rtdetr = _module
base = _module
dexined = _module
segmentation = _module
base = _module
segmentation_models = _module
super_resolution = _module
base = _module
rrdbnet = _module
small_sr = _module
tracking = _module
boxmot_tracker = _module
utils = _module
morphology = _module
morphology = _module
nerf = _module
camera_utils = _module
data_utils = _module
nerf_model = _module
nerf_solver = _module
positional_encoder = _module
samplers = _module
volume_renderer = _module
sequential = _module
sensors = _module
camera_model = _module
distortion_model = _module
projection_model = _module
testing = _module
planar_tracker = _module
transpiler = _module
transpiler = _module
_compat = _module
draw = _module
grid = _module
helpers = _module
image = _module
image_print = _module
memory = _module
misc = _module
one_hot = _module
pointcloud_io = _module
sample = _module
x = _module
callbacks = _module
trainer = _module
trainers = _module
utils = _module
datasets = _module
base = _module
casts = _module
error = _module
create = _module
linalg = _module
overwrite = _module
tests = _module
test_augmentation = _module
test_augmentation_3d = _module
test_augmentation_mix = _module
test_auto_operation = _module
test_backward = _module
test_backward_3d = _module
test_base = _module
test_container = _module
test_dist_mapper = _module
test_motionblur = _module
test_perspective_rand = _module
test_random_generator = _module
test_random_generator_3d = _module
benchmark = _module
test_colormap = _module
test_gray = _module
test_hls = _module
test_hsv = _module
test_lab = _module
test_luv = _module
test_raw = _module
test_rgb = _module
test_sepia = _module
test_xyz = _module
test_ycbcr = _module
test_yuv = _module
test_efficient_vit = _module
test_rt_detr = _module
test_sam = _module
test_tiny_vit = _module
test_classification_head = _module
test_combine_tensor_patch = _module
test_connected_component = _module
test_conv_distance_transformer = _module
test_diamond_square = _module
test_edge_detector = _module
test_extract_tensor_patch = _module
test_face_detection = _module
test_hist_match = _module
test_image_stitcher = _module
test_kmeans = _module
test_lambda_module = _module
test_mobile_vit = _module
test_object_detector = _module
test_prompter = _module
test_vision_transformer = _module
test_check = _module
test_lazyloader = _module
test_module = _module
test_tensor_wrapper = _module
test_adjust = _module
test_core = _module
test_equalization = _module
test_histogram = _module
test_integral = _module
test_jpeg = _module
test_normalize = _module
test_shift_rgb = _module
test_zca = _module
test_affine_shape_estimator = _module
test_dedode = _module
test_defmo = _module
test_disk = _module
test_hardnet = _module
test_hynet = _module
test_integrated = _module
test_keynet = _module
test_laf = _module
test_lightglue_onnx = _module
test_local_features_orientation = _module
test_loftr = _module
test_matching = _module
test_mkd = _module
test_responces_local_features = _module
test_scale_space_detector = _module
test_siftdesc = _module
test_sold2 = _module
test_sosnet = _module
test_steerer = _module
test_tfeat = _module
test_bilateral = _module
test_blur = _module
test_blur_pool = _module
test_canny = _module
test_dissolving = _module
test_filters = _module
test_gaussian = _module
test_guided = _module
test_hanning = _module
test_in_range = _module
test_laplacian = _module
test_median = _module
test_motion = _module
test_sobel = _module
test_unsharp_mask = _module
test_distort = _module
test_pnp = _module
test_undistort = _module
test_distortion = _module
test_perspective = _module
test_pinhole = _module
test_projections = _module
test_stereo = _module
test_epipolar_metrics = _module
test_essential = _module
test_fundamental = _module
test_numeric = _module
test_projection = _module
test_triangulation = _module
test_se2 = _module
test_se3 = _module
test_so2 = _module
test_so3 = _module
test_polynomial_solver = _module
test_dsnt = _module
test_nms = _module
test_spatial_softargmax = _module
test_bbox = _module
test_boxes = _module
test_conversions = _module
test_depth = _module
test_depth_warper = _module
test_homography = _module
test_linalg = _module
test_line = _module
test_plane = _module
test_pose = _module
test_quaternion = _module
test_ransac = _module
test_vector = _module
test_affine = _module
test_crop2d = _module
test_crop3d = _module
test_elastic_transform = _module
test_flip = _module
test_homography_warper = _module
test_image_registrator = _module
test_imgwarp = _module
test_imgwarp3d = _module
test_pyramid = _module
test_thin_plate_spline = _module
test_ste = _module
test_image = _module
test_conversions = _module
test_focal = _module
test_soft_argmax2d = _module
test_warp = _module
test_io_image = _module
test_cauchy = _module
test_charbonnier = _module
test_depth_smoothness = _module
test_dice = _module
test_divergence = _module
test_focal_loss = _module
test_geman_macclure = _module
test_hd = _module
test_lovaz_hinge = _module
test_lovaz_softmax = _module
test_psnr = _module
test_ssim = _module
test_total_variation = _module
test_tversky = _module
test_welcsh = _module
test_aepe = _module
test_confusion = _module
test_map = _module
test_mean_iou = _module
test_psnr_metric = _module
test_ssim3d = _module
box_filtering = _module
test_bottom_hat = _module
test_closing = _module
test_dilation = _module
test_erosion = _module
test_gradient = _module
test_opening = _module
test_top_hat = _module
test_camera_utils = _module
test_data_utils = _module
test_nerf_model = _module
test_nerf_solver = _module
test_positional_encoder = _module
test_rays = _module
test_renderer = _module
test_sequential = _module
test_utils = _module
test_imgwarp_speed = _module
test_project_points_speed = _module
test_camera_model = _module
test_distortion_model = _module
test_projection_model = _module
smoke_test = _module
test_planar_tracking = _module
test_draw = _module
test_grid = _module
test_helpers = _module
test_image_utils = _module
test_memory = _module
test_misc = _module
test_one_hot = _module
test_pointcloud_io = _module
test_print = _module
test_detection = _module
test_image_classification = _module
test_segmentation = _module
test_x = _module

from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from functools import partial


from itertools import product


import numpy as np


import math


from typing import Optional


import matplotlib as mpl


import matplotlib.pyplot as plt


import inspect


from typing import Any


from typing import Dict


from torch import float16


from torch import float32


from torch import float64


from typing import Tuple


from typing import Union


from typing import List


from torch import Tensor


from typing import cast


from typing import Sequence


from typing import Iterator


from torch.distributions import Categorical


from typing import Callable


from typing import Type


from typing import TypeVar


from torch import nn


from torch.autograd import Function


from torch.distributions import Bernoulli


from torch.distributions import RelaxedBernoulli


from torch import Size


from enum import Enum


from torch.distributions import Distribution


import warnings


from collections import OrderedDict


from itertools import zip_longest


import copy


from abc import ABCMeta


from abc import abstractmethod


from typing import Generic


from itertools import cycle


from itertools import islice


from torch.distributions import Uniform


from torch.distributions import Beta


from functools import wraps


from torch.nn.functional import interpolate


from typing import ClassVar


import logging


from enum import EnumMeta


import torch.nn.functional as F


from math import ceil


from warnings import warn


from torch.nn.modules.utils import _pair


from abc import ABC


from typing import Literal


from typing import NamedTuple


from torch.nn.utils.fusion import fuse_conv_bn_weights


import re


from math import pi


import itertools


from torch.utils import checkpoint


import collections


import torch.utils.checkpoint


from torch.nn.init import trunc_normal_


from torch.nn.utils import weight_norm


from types import SimpleNamespace


from torch.hub import HASH_REGEX


from torch.hub import download_url_to_file


from torch.hub import get_dir


from torch.nn import Dropout


import functools


from torch.nn.functional import pixel_shuffle


from torch.nn.functional import softmax


from math import sqrt


from torch.linalg import qr as linalg_qr


from typing import Iterable


from typing import overload


import uuid


from torch import optim


from torch.utils.dlpack import from_dlpack


from torch.utils.dlpack import to_dlpack


from torch.nn.functional import mse_loss as mse


from typing import TYPE_CHECKING


from torch.utils.data import BatchSampler


from torch.utils.data import DataLoader


from torch.utils.data import Dataset


from torch.utils.data import RandomSampler


from torch.utils.data import SequentialSampler


from torch.nn import functional as F


from copy import deepcopy


from torch.autograd import gradcheck


from torch.testing import assert_close as _assert_close


from types import ModuleType


from typing import ContextManager


from inspect import isclass


from inspect import isfunction


from torch.linalg import inv_ex


from math import inf


from torch.optim import Optimizer


from torch.optim import lr_scheduler


import torch.nn


from torch.distributions import Normal


from torch.utils import benchmark


import random


from torch.nn.functional import mse_loss


from torch.testing import assert_close


from numpy.testing import assert_almost_equal


from time import time


T = TypeVar('T')


zeros = torch.zeros


def _boxes_to_polygons(xmin: 'torch.Tensor', ymin: 'torch.Tensor', width: 'torch.Tensor', height: 'torch.Tensor') ->torch.Tensor:
    if not xmin.ndim == ymin.ndim == width.ndim == height.ndim == 2:
        raise ValueError('We expect to create a batch of 2D boxes (quadrilaterals) in vertices format (B, N, 4, 2)')
    polygons = zeros((xmin.shape[0], xmin.shape[1], 4, 2), device=xmin.device, dtype=xmin.dtype)
    polygons[..., 0] = xmin.unsqueeze(-1)
    polygons[..., 1] = ymin.unsqueeze(-1)
    polygons[..., 1, 0] += width - 1
    polygons[..., 2, 0] += width - 1
    polygons[..., 2, 1] += height - 1
    polygons[..., 3, 1] += height - 1
    return polygons


def validate_bbox(boxes: 'torch.Tensor') ->bool:
    """Validate if a 2D bounding box usable or not. This function checks if the boxes are rectangular or not.

    Args:
        boxes: a tensor containing the coordinates of the bounding boxes to be extracted. The tensor must have the shape
            of Bx4x2, where each box is defined in the following ``clockwise`` order: top-left, top-right, bottom-right,
            bottom-left. The coordinates must be in the x, y order.
    """
    if not (len(boxes.shape) in [3, 4] and boxes.shape[-2:] == torch.Size([4, 2])):
        return False
    if len(boxes.shape) == 4:
        boxes = boxes.view(-1, 4, 2)
    x_tl, y_tl = boxes[..., 0, 0], boxes[..., 0, 1]
    x_tr, y_tr = boxes[..., 1, 0], boxes[..., 1, 1]
    x_br, y_br = boxes[..., 2, 0], boxes[..., 2, 1]
    x_bl, y_bl = boxes[..., 3, 0], boxes[..., 3, 1]
    width_t, width_b = x_tr - x_tl + 1, x_br - x_bl + 1
    height_t, height_b = y_tr - y_tl + 1, y_br - y_bl + 1
    if not torch.allclose(width_t, width_b, atol=0.0001):
        return False
    if not torch.allclose(height_t, height_b, atol=0.0001):
        return False
    return True


def _boxes_to_quadrilaterals(boxes: 'torch.Tensor', mode: 'str'='xyxy', validate_boxes: 'bool'=True) ->torch.Tensor:
    """Convert from boxes to quadrilaterals."""
    mode = mode.lower()
    if mode.startswith('vertices'):
        batched = boxes.ndim == 4
        if not (3 <= boxes.ndim <= 4 and boxes.shape[-2:] == torch.Size([4, 2])):
            raise ValueError(f'Boxes shape must be (N, 4, 2) or (B, N, 4, 2) when {mode} mode. Got {boxes.shape}.')
    elif mode.startswith('xy'):
        batched = boxes.ndim == 3
        if not (2 <= boxes.ndim <= 3 and boxes.shape[-1] == 4):
            raise ValueError(f'Boxes shape must be (N, 4) or (B, N, 4) when {mode} mode. Got {boxes.shape}.')
    else:
        raise ValueError(f'Unknown mode {mode}')
    boxes = boxes if boxes.is_floating_point() else boxes.float()
    boxes = boxes if batched else boxes.unsqueeze(0)
    if mode.startswith('vertices'):
        if mode == 'vertices':
            quadrilaterals = boxes.clone()
            quadrilaterals[..., 1:3, 0] = quadrilaterals[..., 1:3, 0] - 1
            quadrilaterals[..., 2:, 1] = quadrilaterals[..., 2:, 1] - 1
        elif mode == 'vertices_plus':
            quadrilaterals = boxes.clone()
        else:
            raise ValueError(f'Unknown mode {mode}')
        not validate_boxes or validate_bbox(quadrilaterals)
    elif mode.startswith('xy'):
        if mode == 'xyxy':
            height, width = boxes[..., 3] - boxes[..., 1], boxes[..., 2] - boxes[..., 0]
        elif mode == 'xyxy_plus':
            height, width = boxes[..., 3] - boxes[..., 1] + 1, boxes[..., 2] - boxes[..., 0] + 1
        elif mode == 'xywh':
            height, width = boxes[..., 3], boxes[..., 2]
        else:
            raise ValueError(f'Unknown mode {mode}')
        if validate_boxes:
            if (width <= 0).any():
                raise ValueError('Some boxes have negative widths or 0.')
            if (height <= 0).any():
                raise ValueError('Some boxes have negative heights or 0.')
        xmin, ymin = boxes[..., 0], boxes[..., 1]
        quadrilaterals = _boxes_to_polygons(xmin, ymin, width, height)
    else:
        raise ValueError(f'Unknown mode {mode}')
    quadrilaterals = quadrilaterals if batched else quadrilaterals.squeeze(0)
    return quadrilaterals


def _is_floating_point_dtype(dtype: 'torch.dtype') ->bool:
    return dtype in (torch.float16, torch.float32, torch.float64, torch.bfloat16, torch.half)


where = torch.where


def convert_points_from_homogeneous(points: 'Tensor', eps: 'float'=1e-08) ->Tensor:
    """Function that converts points from homogeneous to Euclidean space.

    Args:
        points: the points to be transformed of shape :math:`(B, N, D)`.
        eps: to avoid division by zero.

    Returns:
        the points in Euclidean space :math:`(B, N, D-1)`.

    Examples:
        >>> input = tensor([[0., 0., 1.]])
        >>> convert_points_from_homogeneous(input)
        tensor([[0., 0.]])
    """
    if not isinstance(points, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(points)}')
    if len(points.shape) < 2:
        raise ValueError(f'Input must be at least a 2D tensor. Got {points.shape}')
    z_vec: 'Tensor' = points[..., -1:]
    mask: 'Tensor' = torch.abs(z_vec) > eps
    scale = where(mask, 1.0 / (z_vec + eps), torch.ones_like(z_vec))
    return scale * points[..., :-1]


pad = torch.nn.functional.pad


def convert_points_to_homogeneous(points: 'Tensor') ->Tensor:
    """Function that converts points from Euclidean to homogeneous space.

    Args:
        points: the points to be transformed with shape :math:`(*, N, D)`.

    Returns:
        the points in homogeneous coordinates :math:`(*, N, D+1)`.

    Examples:
        >>> input = tensor([[0., 0.]])
        >>> convert_points_to_homogeneous(input)
        tensor([[0., 0., 1.]])
    """
    if not isinstance(points, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(points)}')
    if len(points.shape) < 2:
        raise ValueError(f'Input must be at least a 2D tensor. Got {points.shape}')
    return pad(points, [0, 1], 'constant', 1.0)


def transform_points(trans_01: 'Tensor', points_1: 'Tensor') ->Tensor:
    """Function that applies transformations to a set of points.

    Args:
        trans_01: tensor for transformations of shape
          :math:`(B, D+1, D+1)`.
        points_1: tensor of points of shape :math:`(B, N, D)`.
    Returns:
        a tensor of N-dimensional points.

    Shape:
        - Output: :math:`(B, N, D)`

    Examples:

        >>> points_1 = torch.rand(2, 4, 3)  # BxNx3
        >>> trans_01 = torch.eye(4).view(1, 4, 4)  # Bx4x4
        >>> points_0 = transform_points(trans_01, points_1)  # BxNx3
    """
    KORNIA_CHECK_IS_TENSOR(trans_01)
    KORNIA_CHECK_IS_TENSOR(points_1)
    if not trans_01.shape[0] == points_1.shape[0] and trans_01.shape[0] != 1:
        raise ValueError(f'Input batch size must be the same for both tensors or 1. Got {trans_01.shape} and {points_1.shape}')
    if not trans_01.shape[-1] == points_1.shape[-1] + 1:
        raise ValueError(f'Last input dimensions must differ by one unit Got{trans_01} and {points_1}')
    shape_inp = list(points_1.shape)
    points_1 = points_1.reshape(-1, points_1.shape[-2], points_1.shape[-1])
    trans_01 = trans_01.reshape(-1, trans_01.shape[-2], trans_01.shape[-1])
    trans_01 = torch.repeat_interleave(trans_01, repeats=int(points_1.shape[0] // trans_01.shape[0]), dim=0)
    points_1_h = convert_points_to_homogeneous(points_1)
    points_0_h = torch.bmm(points_1_h, trans_01.permute(0, 2, 1))
    points_0_h = torch.squeeze(points_0_h, dim=-1)
    points_0 = convert_points_from_homogeneous(points_0_h)
    shape_inp[-2] = points_0.shape[-2]
    shape_inp[-1] = points_0.shape[-1]
    points_0 = points_0.reshape(shape_inp)
    return points_0


def _transform_boxes(boxes: 'torch.Tensor', M: 'torch.Tensor') ->torch.Tensor:
    """Transforms 3D and 2D in kornia format by applying the transformation matrix M. Boxes and the transformation
    matrix could be batched or not.

    Args:
        boxes: 2D quadrilaterals or 3D hexahedrons in kornia format.
        M: the transformation matrix of shape :math:`(3, 3)` or :math:`(B, 3, 3)` for 2D and :math:`(4, 4)` or
            :math:`(B, 4, 4)` for 3D hexahedron.
    """
    M = M if M.is_floating_point() else M.float()
    boxes_per_batch, n_points_per_box, coordinates_dimension = boxes.shape[-3:]
    if boxes_per_batch == 0:
        return boxes
    points = boxes.view(-1, n_points_per_box * boxes_per_batch, coordinates_dimension)
    M = M if M.ndim == 3 else M.unsqueeze(0)
    if points.shape[0] != M.shape[0]:
        raise ValueError(f'Batch size mismatch. Got {points.shape[0]} for boxes and {M.shape[0]} for the transformation matrix.')
    transformed_boxes: 'torch.Tensor' = transform_points(M, points)
    transformed_boxes = transformed_boxes.view_as(boxes)
    return transformed_boxes


eye = torch.eye


def eye_like(n: 'int', input: 'Tensor', shared_memory: 'bool'=False) ->Tensor:
    """Return a 2-D tensor with ones on the diagonal and zeros elsewhere with the same batch size as the input.

    Args:
        n: the number of rows :math:`(N)`.
        input: image tensor that will determine the batch size of the output matrix.
          The expected shape is :math:`(B, *)`.
        shared_memory: when set, all samples in the batch will share the same memory.

    Returns:
       The identity matrix with the same batch size as the input :math:`(B, N, N)`.

    Notes:
        When the dimension to expand is of size 1, using torch.expand(...) yields the same tensor as torch.repeat(...)
        without using extra memory. Thus, when the tensor obtained by this method will be later assigned -
        use this method with shared_memory=False, otherwise, prefer using it with shared_memory=True.
    """
    if n <= 0:
        raise AssertionError(type(n), n)
    if len(input.shape) < 1:
        raise AssertionError(input.shape)
    identity = eye(n, device=input.device).type(input.dtype)
    return identity[None].expand(input.shape[0], n, n) if shared_memory else identity[None].repeat(input.shape[0], 1, 1)


def _merge_keypoint_list(keypoints: 'List[Tensor]') ->Tensor:
    raise NotImplementedError


class Keypoints:
    """2D Keypoints containing Nx2 or BxNx2 points.

    Args:
        keypoints: Raw tensor or a list of Tensors with the Nx2 coordinates
        raise_if_not_floating_point: will raise if the Tensor isn't float
    """

    def __init__(self, keypoints: 'Union[Tensor, List[Tensor]]', raise_if_not_floating_point: 'bool'=True) ->None:
        self._N: 'Optional[List[int]]' = None
        if isinstance(keypoints, list):
            keypoints, self._N = _merge_keypoint_list(keypoints)
        if not isinstance(keypoints, Tensor):
            raise TypeError(f'Input keypoints is not a Tensor. Got: {type(keypoints)}.')
        if not keypoints.is_floating_point():
            if raise_if_not_floating_point:
                raise ValueError(f'Coordinates must be in floating point. Got {keypoints.dtype}')
            keypoints = keypoints.float()
        if len(keypoints.shape) == 0:
            keypoints = keypoints.reshape((-1, 2))
        if not (2 <= keypoints.ndim <= 3 and keypoints.shape[-1:] == (2,)):
            raise ValueError(f'Keypoints shape must be (N, 2) or (B, N, 2). Got {keypoints.shape}.')
        self._is_batched = False if keypoints.ndim == 2 else True
        self._data = keypoints

    def __getitem__(self, key: 'Union[slice, int, Tensor]') ->'Keypoints':
        new_obj = type(self)(self._data[key], False)
        return new_obj

    def __setitem__(self, key: 'Union[slice, int, Tensor]', value: "'Keypoints'") ->'Keypoints':
        self._data[key] = value._data
        return self

    @property
    def shape(self) ->Union[Tuple[int, ...], Size]:
        return self.data.shape

    @property
    def data(self) ->Tensor:
        return self._data

    @property
    def device(self) ->torch.device:
        """Returns keypoints device."""
        return self._data.device

    @property
    def dtype(self) ->torch.dtype:
        """Returns keypoints dtype."""
        return self._data.dtype

    def index_put(self, indices: 'Union[Tuple[Tensor, ...], List[Tensor]]', values: "Union[Tensor, 'Keypoints']", inplace: 'bool'=False) ->'Keypoints':
        if inplace:
            _data = self._data
        else:
            _data = self._data.clone()
        if isinstance(values, Keypoints):
            _data.index_put_(indices, values.data)
        else:
            _data.index_put_(indices, values)
        if inplace:
            return self
        obj = self.clone()
        obj._data = _data
        return obj

    def pad(self, padding_size: 'Tensor') ->'Keypoints':
        """Pad a bounding keypoints.

        Args:
            padding_size: (B, 4)
        """
        if not (len(padding_size.shape) == 2 and padding_size.size(1) == 4):
            raise RuntimeError(f'Expected padding_size as (B, 4). Got {padding_size.shape}.')
        self._data[..., 0] += padding_size[..., :1]
        self._data[..., 1] += padding_size[..., 2:3]
        return self

    def unpad(self, padding_size: 'Tensor') ->'Keypoints':
        """Pad a bounding keypoints.

        Args:
            padding_size: (B, 4)
        """
        if not (len(padding_size.shape) == 2 and padding_size.size(1) == 4):
            raise RuntimeError(f'Expected padding_size as (B, 4). Got {padding_size.shape}.')
        self._data[..., 0] -= padding_size[..., :1]
        self._data[..., 1] -= padding_size[..., 2:3]
        return self

    def transform_keypoints(self, M: 'Tensor', inplace: 'bool'=False) ->'Keypoints':
        """Apply a transformation matrix to the 2D keypoints.

        Args:
            M: The transformation matrix to be applied, shape of :math:`(3, 3)` or :math:`(B, 3, 3)`.
            inplace: do transform in-place and return self.

        Returns:
            The transformed keypoints.
        """
        if not 2 <= M.ndim <= 3 or M.shape[-2:] != (3, 3):
            raise ValueError(f'The transformation matrix shape must be (3, 3) or (B, 3, 3). Got {M.shape}.')
        transformed_boxes = transform_points(M, self._data)
        if inplace:
            self._data = transformed_boxes
            return self
        return Keypoints(transformed_boxes, False)

    def transform_keypoints_(self, M: 'Tensor') ->'Keypoints':
        """Inplace version of :func:`Keypoints.transform_keypoints`"""
        return self.transform_keypoints(M, inplace=True)

    @classmethod
    def from_tensor(cls, keypoints: 'Tensor') ->'Keypoints':
        return cls(keypoints)

    def to_tensor(self, as_padded_sequence: 'bool'=False) ->Union[Tensor, List[Tensor]]:
        """Cast :class:`Keypoints` to a tensor. ``mode`` controls which 2D keypoints format should be use to
        represent keypoints in the tensor.

        Args:
            as_padded_sequence: whether to keep the pads for a list of keypoints. This parameter is only valid
                if the keypoints are from a keypoint list.

        Returns:
            Keypoints tensor :math:`(B, N, 2)`
        """
        if as_padded_sequence:
            raise NotImplementedError
        return self._data

    def clone(self) ->'Keypoints':
        return Keypoints(self._data.clone(), False)

    def type(self, dtype: 'torch.dtype') ->'Keypoints':
        self._data = self._data.type(dtype)
        return self


class _PostInitInjectionMetaClass(type):
    """To inject the ``__post_init__`` function after the creation of each instance."""

    def __call__(cls: 'Type[T]', *args: Any, **kwargs: Any) ->T:
        obj = type.__call__(cls, *args, **kwargs)
        obj.__post_init__()
        return obj


class RandomGeneratorBase(Module, metaclass=_PostInitInjectionMetaClass):
    """Base class for generating random augmentation parameters."""
    device: 'Optional[Device]' = None
    dtype: 'torch.dtype'

    def __init__(self) ->None:
        super().__init__()

    def __post_init__(self) ->None:
        self.set_rng_device_and_dtype()

    def set_rng_device_and_dtype(self, device: 'torch.device'=torch.device('cpu'), dtype: 'torch.dtype'=torch.float32) ->None:
        """Change the random generation device and dtype.

        Note:
            The generated random numbers are not reproducible across different devices and dtypes.
        """
        if self.device != device or self.dtype != dtype:
            self.make_samplers(device, dtype)
            self.device = device
            self.dtype = dtype

    def to(self, *args: Any, **kwargs: Any) ->'RandomGeneratorBase':
        device, dtype, _, _ = torch._C._nn._parse_to(*args, **kwargs)
        self.set_rng_device_and_dtype(device=device, dtype=dtype)
        return self

    def make_samplers(self, device: 'torch.device', dtype: 'torch.dtype') ->None:
        raise NotImplementedError

    def forward(self, batch_shape: 'Tuple[int, ...]', same_on_batch: 'bool'=False) ->Dict[str, Tensor]:
        raise NotImplementedError


def _adapted_rsampling(shape: 'Union[Tuple[int, ...], torch.Size]', dist: 'torch.distributions.Distribution', same_on_batch: 'Optional[bool]'=False) ->Tensor:
    """The uniform reparameterized sampling function that accepts 'same_on_batch'.

    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]). By default,
    same_on_batch is set to False.
    """
    if isinstance(shape, tuple):
        shape = torch.Size(shape)
    if same_on_batch:
        rsample_size = torch.Size((1, *shape[1:]))
        rsample = dist.rsample(rsample_size)
        return rsample.repeat(shape[0], *([1] * (len(rsample.shape) - 1)))
    return dist.rsample(shape)


def _adapted_sampling(shape: 'Union[Tuple[int, ...], torch.Size]', dist: 'torch.distributions.Distribution', same_on_batch: 'Optional[bool]'=False) ->Tensor:
    """The uniform sampling function that accepts 'same_on_batch'.

    If same_on_batch is True, all values generated will be exactly same given a batch_size (shape[0]). By default,
    same_on_batch is set to False.
    """
    if isinstance(shape, tuple):
        shape = torch.Size(shape)
    if same_on_batch:
        return dist.sample(torch.Size((1, *shape[1:]))).repeat(shape[0], *([1] * (len(shape) - 1)))
    return dist.sample(shape)


def _apply_transform_unimplemented(self: 'Module', *input: Any) ->Tensor:
    """Defines the computation performed at every call.

    Should be overridden by all subclasses.
    """
    raise NotImplementedError(f'Module [{type(self).__name__}] is missing the required "apply_tranform" function')


def _transform_output_shape(output: 'Tensor', shape: 'Tuple[int, ...]', *, reference_shape: Optional[Tensor]=None) ->Tensor:
    """Collapse the broadcasted batch dimensions an input tensor to be the specified shape.
    Args:
        input: Tensor
        shape: List/tuple of int

    Returns:
        Tensor
    """
    out_tensor = output.clone()
    for dim in range(len(out_tensor.shape) - len(shape)):
        idx = 0
        if reference_shape is not None and out_tensor.shape[0] == reference_shape[0] != 1 and len(shape) > 2:
            idx = 1
        if out_tensor.shape[idx] != 1:
            raise AssertionError(f'Dimension {dim} of input is expected to be 1, got {out_tensor.shape[idx]}')
        out_tensor = out_tensor.squeeze(idx)
    return out_tensor


def deepcopy_dict(params: 'Dict[str, Any]') ->Dict[str, Any]:
    """Perform deep copy on any dict.

    Support tensor copying here.
    """
    out = {}
    for k, v in params.items():
        if isinstance(v, Tensor):
            out.update({k: v.clone()})
        else:
            out.update({k: v})
    return out


def override_parameters(params: 'Dict[str, Any]', params_override: 'Optional[Dict[str, Any]]'=None, if_none_exist: 'str'='ignore', in_place: 'bool'=False) ->Dict[str, Any]:
    """Override params dict w.r.t params_override.

    Args:
        params: source parameters.
        params_override: key-values to override the source parameters.
        if_none_exist: behaviour if the key in `params_override` does not exist in `params`.
            'raise' | 'ignore'.
        in_place: if to override in-place or not.
    """
    if params_override is None:
        return params
    out = params if in_place else deepcopy_dict(params)
    for k, v in params_override.items():
        if k in params_override:
            out[k] = v
        elif if_none_exist == 'ignore':
            pass
        elif if_none_exist == 'raise':
            raise RuntimeError(f'Param `{k}` not existed in `{params_override}`.')
        else:
            raise ValueError(f'`{if_none_exist}` is not a valid option.')
    return out


tensor = torch.tensor


class _BasicAugmentationBase(Module):
    """_BasicAugmentationBase base class for customized augmentation implementations.

    Plain augmentation base class without the functionality of transformation matrix calculations.
    By default, the random computations will be happened on CPU with ``torch.get_default_dtype()``.
    To change this behaviour, please use ``set_rng_device_and_dtype``.

    For automatically generating the corresponding ``__repr__`` with full customized parameters, you may need to
    implement ``_param_generator`` by inheriting ``RandomGeneratorBase`` for generating random parameters and
    put all static parameters inside ``self.flags``. You may take the advantage of ``PlainUniformGenerator`` to
    generate simple uniform parameters with less boilerplate code.

    Args:
        p: probability for applying an augmentation. This param controls the augmentation probabilities element-wise.
        p_batch: probability for applying an augmentation to a batch. This param controls the augmentation
          probabilities batch-wise.
        same_on_batch: apply the same transformation across the batch.
        keepdim: whether to keep the output shape the same as input ``True`` or broadcast it to
          the batch form ``False``.
    """
    ONNX_EXPORTABLE = False

    def __init__(self, p: 'float'=0.5, p_batch: 'float'=1.0, same_on_batch: 'bool'=False, keepdim: 'bool'=False) ->None:
        super().__init__()
        self.p = p
        self.p_batch = p_batch
        self.same_on_batch = same_on_batch
        self.keepdim = keepdim
        self._params: 'Dict[str, Tensor]' = {}
        self._p_gen: 'Distribution'
        self._p_batch_gen: 'Distribution'
        if p != 0.0 or p != 1.0:
            self._p_gen = Bernoulli(self.p)
        if p_batch != 0.0 or p_batch != 1.0:
            self._p_batch_gen = Bernoulli(self.p_batch)
        self._param_generator: 'Optional[RandomGeneratorBase]' = None
        self.flags: 'Dict[str, Any]' = {}
        self.set_rng_device_and_dtype(torch.device('cpu'), torch.get_default_dtype())
    apply_transform: 'Callable[..., Tensor]' = _apply_transform_unimplemented

    def to(self, *args: Any, **kwargs: Any) ->'_BasicAugmentationBase':
        """Set the device and dtype for the random number generator."""
        device, dtype, _, _ = torch._C._nn._parse_to(*args, **kwargs)
        self.set_rng_device_and_dtype(device, dtype)
        return super()

    def __repr__(self) ->str:
        txt = f'p={self.p}, p_batch={self.p_batch}, same_on_batch={self.same_on_batch}'
        if isinstance(self._param_generator, RandomGeneratorBase):
            txt = f'{self._param_generator!s}, {txt}'
        for k, v in self.flags.items():
            if isinstance(v, Enum):
                txt += f', {k}={v.name.lower()}'
            else:
                txt += f', {k}={v}'
        return f'{self.__class__.__name__}({txt})'

    def __unpack_input__(self, input: 'Tensor') ->Tensor:
        return input

    def transform_tensor(self, input: 'Tensor', *, shape: Optional[Tensor]=None, match_channel: bool=True) ->Tensor:
        """Standardize input tensors."""
        raise NotImplementedError

    def validate_tensor(self, input: 'Tensor') ->None:
        """Check if the input tensor is formatted as expected."""
        raise NotImplementedError

    def transform_output_tensor(self, output: 'Tensor', output_shape: 'Tuple[int, ...]') ->Tensor:
        """Standardize output tensors."""
        return _transform_output_shape(output, output_shape) if self.keepdim else output

    def generate_parameters(self, batch_shape: 'Tuple[int, ...]') ->Dict[str, Tensor]:
        if self._param_generator is not None:
            return self._param_generator(batch_shape, self.same_on_batch)
        return {}

    def set_rng_device_and_dtype(self, device: 'torch.device', dtype: 'torch.dtype') ->None:
        """Change the random generation device and dtype.

        Note:
            The generated random numbers are not reproducible across different devices and dtypes.
        """
        self.device = device
        self.dtype = dtype
        if self._param_generator is not None:
            self._param_generator.set_rng_device_and_dtype(device, dtype)

    def __batch_prob_generator__(self, batch_shape: 'Tuple[int, ...]', p: 'float', p_batch: 'float', same_on_batch: 'bool') ->Tensor:
        batch_prob: 'Tensor'
        if p_batch == 1:
            batch_prob = zeros(1) + 1
        elif p_batch == 0:
            batch_prob = zeros(1)
        elif isinstance(self._p_batch_gen, (RelaxedBernoulli,)):
            batch_prob = _adapted_rsampling((1,), self._p_batch_gen, same_on_batch)
        else:
            batch_prob = _adapted_sampling((1,), self._p_batch_gen, same_on_batch)
        if batch_prob.sum() == 1:
            elem_prob: 'Tensor'
            if p == 1:
                elem_prob = zeros(batch_shape[0]) + 1
            elif p == 0:
                elem_prob = zeros(batch_shape[0])
            elif isinstance(self._p_gen, (RelaxedBernoulli,)):
                elem_prob = _adapted_rsampling((batch_shape[0],), self._p_gen, same_on_batch)
            else:
                elem_prob = _adapted_sampling((batch_shape[0],), self._p_gen, same_on_batch)
            batch_prob = batch_prob * elem_prob
        else:
            batch_prob = batch_prob.repeat(batch_shape[0])
        if len(batch_prob.shape) == 2:
            return batch_prob[..., 0]
        return batch_prob

    def _process_kwargs_to_params_and_flags(self, params: 'Optional[Dict[str, Tensor]]'=None, flags: 'Optional[Dict[str, Any]]'=None, **kwargs: Any) ->Tuple[Dict[str, Tensor], Dict[str, Any]]:
        save_kwargs = kwargs['save_kwargs'] if 'save_kwargs' in kwargs else False
        params = self._params if params is None else params
        flags = self.flags if flags is None else flags
        if save_kwargs:
            params = override_parameters(params, kwargs, in_place=True)
            self._params = params
        else:
            self._params = params
            params = override_parameters(params, kwargs, in_place=False)
        flags = override_parameters(flags, kwargs, in_place=False)
        return params, flags

    def forward_parameters(self, batch_shape: 'Tuple[int, ...]') ->Dict[str, Tensor]:
        batch_prob = self.__batch_prob_generator__(batch_shape, self.p, self.p_batch, self.same_on_batch)
        to_apply = batch_prob > 0.5
        _params = self.generate_parameters(torch.Size((int(to_apply.sum().item()), *batch_shape[1:])))
        if _params is None:
            _params = {}
        _params['batch_prob'] = batch_prob
        input_size = tensor(batch_shape, dtype=torch.long)
        _params.update({'forward_input_shape': input_size})
        return _params

    def apply_func(self, input: 'Tensor', params: 'Dict[str, Tensor]', flags: 'Dict[str, Any]') ->Tensor:
        return self.apply_transform(input, params, flags)

    def forward(self, input: 'Tensor', params: 'Optional[Dict[str, Tensor]]'=None, **kwargs: Any) ->Tensor:
        """Perform forward operations.

        Args:
            input: the input tensor.
            params: the corresponding parameters for an operation.
                If None, a new parameter suite will be generated.
            **kwargs: key-value pairs to override the parameters and flags.

        Note:
            By default, all the overwriting parameters in kwargs will not be recorded
            as in ``self._params``. If you wish it to be recorded, you may pass
            ``save_kwargs=True`` additionally.
        """
        in_tensor = self.__unpack_input__(input)
        input_shape = in_tensor.shape
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = in_tensor.shape
        if params is None:
            params = self.forward_parameters(batch_shape)
        if 'batch_prob' not in params:
            params['batch_prob'] = tensor([True] * batch_shape[0])
        params, flags = self._process_kwargs_to_params_and_flags(params, self.flags, **kwargs)
        output = self.apply_func(in_tensor, params, flags)
        return self.transform_output_tensor(output, input_shape) if self.keepdim else output


def torch_version() ->str:
    """Parse the `torch.__version__` variable and removes +cu*/cpu."""
    return torch.__version__.split('+')[0]


version = ''


def torch_version_ge(major: 'int', minor: 'int', patch: 'Optional[int]'=None) ->bool:
    _version = version.parse(torch_version())
    if patch is None:
        return _version >= version.parse(f'{major}.{minor}')
    else:
        return _version >= version.parse(f'{major}.{minor}.{patch}')


def is_autocast_enabled(both: 'bool'=True) ->bool:
    """Check if torch autocast is enabled.

    Args:
        both: if True will consider autocast region for both types of devices

    Returns:
        Return a Bool,
        will always return False for a torch without support, otherwise will be: if both is True
        `torch.is_autocast_enabled() or torch.is_autocast_enabled('cpu')`. If both is False will return just
        `torch.is_autocast_enabled()`.
    """
    if TYPE_CHECKING:
        return False
    if not torch_version_ge(1, 10, 2):
        return False
    if both:
        if torch_version_ge(2, 4):
            return torch.is_autocast_enabled() or torch.is_autocast_enabled('cpu')
        else:
            return torch.is_autocast_enabled() or torch.is_autocast_cpu_enabled()
    return torch.is_autocast_enabled()


class OperationBase(Module):
    """Base class of differentiable augmentation operations.

    Args:
        operation: Kornia augmentation module.
        initial_magnitude: targeted magnitude parameter name and its initial magnitude value.
            The magnitude parameter name shall align with the attribute inside the random_generator
            in each augmentation. If None, the augmentation will be randomly applied according to
            the augmentation sampling range.
        temperature: temperature for RelaxedBernoulli distribution used during training.
        is_batch_operation: determine if to obtain the probability from `p` or `p_batch`.
            Set to True for most non-shape-persistent operations (e.g. cropping).
    """

    def __init__(self, operation: '_AugmentationBase', initial_magnitude: 'Optional[List[Tuple[str, Optional[float]]]]'=None, temperature: 'float'=0.1, is_batch_operation: 'bool'=False, magnitude_fn: 'Optional[Callable[[Tensor], Tensor]]'=None, gradient_estimator: 'Optional[Type[Function]]'=None, symmetric_megnitude: 'bool'=False) ->None:
        super().__init__()
        if not isinstance(operation, _AugmentationBase):
            raise ValueError(f'Only Kornia augmentations supported. Got {operation}.')
        self.op = operation
        self._init_magnitude(initial_magnitude)
        self.probability_range = 1e-07, 1 - 1e-07
        self._is_batch_operation = is_batch_operation
        if is_batch_operation:
            self._probability = nn.Parameter(torch.empty(1).fill_(self.op.p_batch))
        else:
            self._probability = nn.Parameter(torch.empty(1).fill_(self.op.p))
        if temperature < 0:
            raise ValueError(f'Expect temperature value greater than 0. Got {temperature}.')
        self.register_buffer('temperature', torch.empty(1).fill_(temperature))
        self.symmetric_megnitude = symmetric_megnitude
        self._magnitude_fn = self._init_magnitude_fn(magnitude_fn)
        self._gradient_estimator = gradient_estimator

    def _init_magnitude_fn(self, magnitude_fn: 'Optional[Callable[[Tensor], Tensor]]') ->Callable[[Tensor], Tensor]:

        def _identity(x: 'Tensor') ->Tensor:
            return x

        def _random_flip(fn: 'Callable[[Tensor], Tensor]') ->Callable[[Tensor], Tensor]:

            def f(x: 'Tensor') ->Tensor:
                flip = torch.rand((x.shape[0],), device=x.device) > 0.5
                return fn(x) * flip
            return f
        if magnitude_fn is None:
            magnitude_fn = _identity
        if self.symmetric_megnitude:
            return _random_flip(magnitude_fn)
        return magnitude_fn

    def _init_magnitude(self, initial_magnitude: 'Optional[List[Tuple[str, Optional[float]]]]') ->None:
        if isinstance(initial_magnitude, (list, tuple)):
            if not all(isinstance(ini_mag, (list, tuple)) and len(ini_mag) == 2 for ini_mag in initial_magnitude):
                raise ValueError(f'`initial_magnitude` shall be a list of 2-element tuples. Got {initial_magnitude}')
            if len(initial_magnitude) != 1:
                raise NotImplementedError('Multi magnitudes operations are not yet supported.')
        if initial_magnitude is None:
            self._factor_name = None
            self._magnitude = None
            self.magnitude_range = None
        else:
            self._factor_name = initial_magnitude[0][0]
            if self.op._param_generator is not None:
                self.magnitude_range = getattr(self.op._param_generator, self._factor_name)
            else:
                raise ValueError(f'No valid magnitude `{self._factor_name}` found in `{self.op._param_generator}`.')
            self._magnitude = None
            if initial_magnitude[0][1] is not None:
                self._magnitude = nn.Parameter(torch.empty(1).fill_(initial_magnitude[0][1]))

    def _update_probability_gen(self, relaxation: 'bool') ->None:
        if relaxation:
            if self._is_batch_operation:
                self.op._p_batch_gen = RelaxedBernoulli(self.temperature, self.probability)
            else:
                self.op._p_gen = RelaxedBernoulli(self.temperature, self.probability)
        elif self._is_batch_operation:
            self.op._p_batch_gen = Bernoulli(self.probability)
        else:
            self.op._p_gen = Bernoulli(self.probability)

    def train(self: 'T', mode: 'bool'=True) ->T:
        self._update_probability_gen(relaxation=mode)
        return super().train(mode=mode)

    def eval(self: 'T') ->T:
        return self.train(False)

    def forward_parameters(self, batch_shape: 'torch.Size', mag: 'Optional[Tensor]'=None) ->Dict[str, Tensor]:
        if mag is None:
            mag = self.magnitude
        self._update_probability_gen(relaxation=True)
        params = self.op.forward_parameters(batch_shape)
        if mag is not None:
            if self._factor_name is None:
                raise RuntimeError('No factor found in the params while `mag` is provided.')
            params[self._factor_name] = params[self._factor_name].zero_() + mag
        if self._factor_name is not None:
            params[self._factor_name] = self._magnitude_fn(params[self._factor_name])
        return params

    def forward(self, input: 'Tensor', params: 'Optional[Dict[str, Tensor]]'=None) ->Tensor:
        if params is None:
            params = self.forward_parameters(input.shape)
        batch_prob = params['batch_prob'][(...,) + (None,) * (len(input.shape) - 1)]
        if self._gradient_estimator is not None:
            with torch.no_grad():
                output = self.op(input, params=params)
            output = batch_prob * output + (1 - batch_prob) * input
            if self.magnitude is None:
                return self._gradient_estimator.apply(input, output)
            return self._gradient_estimator.apply(self.magnitude, output)
        return batch_prob * self.op(input, params=params) + (1 - batch_prob) * input

    @property
    def transform_matrix(self) ->Optional[Tensor]:
        if hasattr(self.op, 'transform_matrix'):
            return self.op.transform_matrix
        return None

    @property
    def magnitude(self) ->Optional[Tensor]:
        if self._magnitude is None:
            return None
        mag = self._magnitude
        if self.magnitude_range is not None:
            return mag.clamp(*self.magnitude_range)
        return mag

    @property
    def probability(self) ->Tensor:
        p = self._probability.clamp(*self.probability_range)
        return p


class ParamItem(NamedTuple):
    name: 'str'
    data: "Optional[Union[Dict[str, Tensor], List['ParamItem']]]"


class BasicSequentialBase(nn.Sequential):
    """BasicSequential for creating kornia modulized processing pipeline.

    Args:
        *args : a list of kornia augmentation and image operation modules.
    """

    def __init__(self, *args: Module) ->None:
        _args = OrderedDict()
        for idx, mod in enumerate(args):
            if not isinstance(mod, Module):
                raise NotImplementedError(f'Only Module are supported at this moment. Got {mod}.')
            _args.update({f'{mod.__class__.__name__}_{idx}': mod})
        super().__init__(_args)
        self._params: 'Optional[List[ParamItem]]' = None

    def get_submodule(self, target: 'str') ->Module:
        """Get submodule.

        This code is taken from torch 1.9.0 since it is not introduced
        back to torch 1.7.1. We included this for maintaining more
        backward torch versions.

        Args:
            target: The fully-qualified string name of the submodule
                to look for. (See above example for how to specify a
                fully-qualified string.)

        Returns:
            Module: The submodule referenced by ``target``

        Raises:
            AttributeError: If the target string references an invalid
                path or resolves to something that is not an
                ``Module``
        """
        if len(target) == 0:
            return self
        atoms: 'List[str]' = target.split('.')
        mod = self
        for item in atoms:
            if not hasattr(mod, item):
                raise AttributeError(mod._get_name() + ' has no attribute `' + item + '`')
            mod = getattr(mod, item)
            if not isinstance(mod, Module):
                raise AttributeError('`' + item + '` is not an Module')
        return mod

    def clear_state(self) ->None:
        """Reset self._params state to None."""
        self._params = None

    def forward_parameters(self, batch_shape: 'torch.Size') ->List[ParamItem]:
        raise NotImplementedError

    def get_children_by_indices(self, indices: 'Tensor') ->Iterator[Tuple[str, Module]]:
        modules = list(self.named_children())
        for idx in indices:
            yield modules[idx]

    def get_children_by_params(self, params: 'List[ParamItem]') ->Iterator[Tuple[str, Module]]:
        modules = list(self.named_children())
        for param in params:
            yield modules[list(dict(self.named_children()).keys()).index(param.name)]

    def get_params_by_module(self, named_modules: 'Iterator[Tuple[str, Module]]') ->Iterator[ParamItem]:
        for name, _ in named_modules:
            yield ParamItem(name, None)


class SequentialBase(BasicSequentialBase):
    """SequentialBase for creating kornia modulized processing pipeline.

    Args:
        *args : a list of kornia augmentation and image operation modules.
        same_on_batch: apply the same transformation across the batch.
            If None, it will not overwrite the function-wise settings.
        return_transform: if ``True`` return the matrix describing the transformation
            applied to each. If None, it will not overwrite the function-wise settings.
        keepdim: whether to keep the output shape the same as input (True) or broadcast it
            to the batch form (False). If None, it will not overwrite the function-wise settings.
    """

    def __init__(self, *args: Module, same_on_batch: Optional[bool]=None, keepdim: Optional[bool]=None) ->None:
        super().__init__(*args)
        self._same_on_batch = same_on_batch
        self._keepdim = keepdim
        self.update_attribute(same_on_batch, keepdim=keepdim)

    def update_attribute(self, same_on_batch: 'Optional[bool]'=None, return_transform: 'Optional[bool]'=None, keepdim: 'Optional[bool]'=None) ->None:
        for mod in self.children():
            if isinstance(mod, (_AugmentationBase, K.MixAugmentationBaseV2)):
                if same_on_batch is not None:
                    mod.same_on_batch = same_on_batch
                if keepdim is not None:
                    mod.keepdim = keepdim
            if isinstance(mod, SequentialBase):
                mod.update_attribute(same_on_batch, return_transform, keepdim)

    @property
    def same_on_batch(self) ->Optional[bool]:
        return self._same_on_batch

    @same_on_batch.setter
    def same_on_batch(self, same_on_batch: 'Optional[bool]') ->None:
        self._same_on_batch = same_on_batch
        self.update_attribute(same_on_batch=same_on_batch)

    @property
    def keepdim(self) ->Optional[bool]:
        return self._keepdim

    @keepdim.setter
    def keepdim(self, keepdim: 'Optional[bool]') ->None:
        self._keepdim = keepdim
        self.update_attribute(keepdim=keepdim)

    def autofill_dim(self, input: 'Tensor', dim_range: 'Tuple[int, int]'=(2, 4)) ->Tuple[torch.Size, torch.Size]:
        """Fill tensor dim to the upper bound of dim_range.

        If input tensor dim is smaller than the lower bound of dim_range, an error will be thrown out.
        """
        ori_shape = input.shape
        if len(ori_shape) < dim_range[0] or len(ori_shape) > dim_range[1]:
            raise RuntimeError(f'input shape expected to be in {dim_range} while got {ori_shape}.')
        while len(input.shape) < dim_range[1]:
            input = input[None]
        return ori_shape, input.shape


class SequentialOpsInterface(Generic[T], metaclass=ABCMeta):
    """Abstract interface for applying and inversing transformations."""

    @classmethod
    def get_instance_module_param(cls, param: 'ParamItem') ->Dict[str, Tensor]:
        if isinstance(param, ParamItem) and isinstance(param.data, dict):
            _params = param.data
        else:
            raise TypeError(f'Expected param (ParamItem.data) be a dictionary. Gotcha {param}.')
        return _params

    @classmethod
    def get_sequential_module_param(cls, param: 'ParamItem') ->List[ParamItem]:
        if isinstance(param, ParamItem) and isinstance(param.data, list):
            _params = param.data
        else:
            raise TypeError(f'Expected param (ParamItem.data) be a list. Gotcha {param}.')
        return _params

    @classmethod
    @abstractmethod
    def transform(cls, input: 'T', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->T:
        """Apply a transformation with respect to the parameters.

        Args:
            input: the input tensor.
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        raise NotImplementedError

    @classmethod
    @abstractmethod
    def inverse(cls, input: 'T', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->T:
        """Inverse a transformation with respect to the parameters.

        Args:
            input: the input tensor.
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        raise NotImplementedError


class _KORNIA_EnumMeta(EnumMeta):

    def __iter__(self) ->Iterator[Enum]:
        return super().__iter__()

    def __contains__(self, other: 'TKEnum[Enum]') ->bool:
        if isinstance(other, str):
            return any(val.name.upper() == other.upper() for val in self)
        elif isinstance(other, int):
            return any(val.value == other for val in self)
        return any(val == other for val in self)

    def __repr__(self) ->str:
        return ' | '.join(f'{self.__name__}.{val.name}' for val in self)


def _get(cls: 'Type[T]', value: 'TKEnum[T]') ->T:
    if isinstance(value, str):
        return cls[value.upper()]
    elif isinstance(value, int):
        return cls(value)
    elif isinstance(value, cls):
        return value
    raise TypeError(f'The `.get` method from `{cls}` expects a value with type `str`, `int` or `{cls}`. Gotcha {type(value)}')


class DataKey(Enum, metaclass=_KORNIA_EnumMeta):
    IMAGE = 0
    INPUT = 0
    MASK = 1
    BBOX = 2
    BBOX_XYXY = 3
    BBOX_XYWH = 4
    KEYPOINTS = 5
    LABEL = 6
    CLASS = 6

    @classmethod
    def get(cls, value: "TKEnum['DataKey']") ->'DataKey':
        return _get(cls, value)


class InputSequentialOps(SequentialOpsInterface[Tensor]):

    @classmethod
    def transform(cls, input: 'Tensor', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Tensor:
        if isinstance(module, (_AugmentationBase, K.MixAugmentationBaseV2)):
            input = module(input, params=cls.get_instance_module_param(param), data_keys=[DataKey.INPUT], **extra_args)
        elif isinstance(module, (K.container.ImageSequentialBase,)):
            input = module.transform_inputs(input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            input = module(input, params=cls.get_instance_module_param(param))
        else:
            if param.data is not None:
                raise AssertionError(f'Non-augmentaion operation {param.name} require empty parameters. Got {param}.')
            input = module(input)
        return input

    @classmethod
    def inverse(cls, input: 'Tensor', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Tensor:
        if isinstance(module, K.GeometricAugmentationBase2D):
            input = module.inverse(input, params=cls.get_instance_module_param(param), **extra_args)
        elif isinstance(module, (K.GeometricAugmentationBase3D,)):
            raise NotImplementedError('The support for 3d inverse operations are not yet supported. You are welcome to file a PR in our repo.')
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            return InputSequentialOps.inverse(input, module=module.op, param=param, extra_args=extra_args)
        elif isinstance(module, K.ImageSequential) and not module.is_intensity_only():
            input = module.inverse_inputs(input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, K.container.ImageSequentialBase):
            input = module.inverse_inputs(input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        return input


class KeypointSequentialOps(SequentialOpsInterface[Keypoints]):
    """Apply and inverse transformations for keypoints tensors.

    This is for transform keypoints in the format (B, N, 2).
    """

    @classmethod
    def transform(cls, input: 'Keypoints', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Keypoints:
        """Apply a transformation with respect to the parameters.

        Args:
            input: the input tensor, (B, N, 4, 2) or (B, 4, 2).
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        _input = input.clone()
        if isinstance(module, (K.GeometricAugmentationBase2D,)):
            _input = module.transform_keypoints(_input, cls.get_instance_module_param(param), module.flags, transform=module.transform_matrix, **extra_args)
        elif isinstance(module, (K.GeometricAugmentationBase3D,)):
            raise NotImplementedError('The support for 3d keypoint operations are not yet supported. You are welcome to file a PR in our repo.')
        elif isinstance(module, K.ImageSequential) and not module.is_intensity_only():
            _input = module.transform_keypoints(_input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, K.container.ImageSequentialBase):
            _input = module.transform_keypoints(_input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            return KeypointSequentialOps.transform(input, module=module.op, param=param, extra_args=extra_args)
        return _input

    @classmethod
    def inverse(cls, input: 'Keypoints', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Keypoints:
        """Inverse a transformation with respect to the parameters.

        Args:
            input: the input tensor.
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        _input = input.clone()
        if isinstance(module, (K.GeometricAugmentationBase2D,)):
            if module.transform_matrix is None:
                raise ValueError(f'No valid transformation matrix found in {module.__class__}.')
            transform = module.compute_inverse_transformation(module.transform_matrix)
            _input = module.inverse_keypoints(_input, cls.get_instance_module_param(param), module.flags, transform=transform, **extra_args)
        elif isinstance(module, (K.GeometricAugmentationBase3D,)):
            raise NotImplementedError('The support for 3d keypoint operations are not yet supported. You are welcome to file a PR in our repo.')
        elif isinstance(module, K.ImageSequential) and not module.is_intensity_only():
            _input = module.inverse_keypoints(_input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, K.container.ImageSequentialBase):
            _input = module.inverse_keypoints(_input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            return KeypointSequentialOps.inverse(input, module=module.op, param=param, extra_args=extra_args)
        return _input


class MaskSequentialOps(SequentialOpsInterface[Tensor]):
    """Apply and inverse transformations for mask tensors."""

    @classmethod
    def transform(cls, input: 'Tensor', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Tensor:
        """Apply a transformation with respect to the parameters.

        Args:
            input: the input tensor.
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        if isinstance(module, (K.GeometricAugmentationBase2D,)):
            input = module.transform_masks(input, params=cls.get_instance_module_param(param), flags=module.flags, transform=module.transform_matrix, **extra_args)
        elif isinstance(module, (K.GeometricAugmentationBase3D,)):
            raise NotImplementedError('The support for 3d mask operations are not yet supported. You are welcome to file a PR in our repo.')
        elif isinstance(module, K.RandomTransplantation):
            input = module(input, params=cls.get_instance_module_param(param), data_keys=[DataKey.MASK], **extra_args)
        elif isinstance(module, _AugmentationBase):
            input = module.transform_masks(input, params=cls.get_instance_module_param(param), flags=module.flags, **extra_args)
        elif isinstance(module, K.ImageSequential) and not module.is_intensity_only():
            input = module.transform_masks(input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, K.container.ImageSequentialBase):
            input = module.transform_masks(input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            input = MaskSequentialOps.transform(input, module=module.op, param=param, extra_args=extra_args)
        return input

    @classmethod
    def transform_list(cls, input: 'List[Tensor]', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->List[Tensor]:
        """Apply a transformation with respect to the parameters.

        Args:
            input: list of input tensors.
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        if isinstance(module, (K.GeometricAugmentationBase2D,)):
            tfm_input = []
            params = cls.get_instance_module_param(param)
            params_i = copy.deepcopy(params)
            for i, inp in enumerate(input):
                params_i['batch_prob'] = params['batch_prob'][i]
                tfm_inp = module.transform_masks(inp, params=params_i, flags=module.flags, transform=module.transform_matrix, **extra_args)
                tfm_input.append(tfm_inp)
            input = tfm_input
        elif isinstance(module, (K.GeometricAugmentationBase3D,)):
            raise NotImplementedError('The support for 3d mask operations are not yet supported. You are welcome to file a PR in our repo.')
        elif isinstance(module, _AugmentationBase):
            tfm_input = []
            params = cls.get_instance_module_param(param)
            params_i = copy.deepcopy(params)
            for i, inp in enumerate(input):
                params_i['batch_prob'] = params['batch_prob'][i]
                tfm_inp = module.transform_masks(inp, params=params_i, flags=module.flags, **extra_args)
                tfm_input.append(tfm_inp)
            input = tfm_input
        elif isinstance(module, K.ImageSequential) and not module.is_intensity_only():
            tfm_input = []
            seq_params = cls.get_sequential_module_param(param)
            for inp in input:
                tfm_inp = module.transform_masks(inp, params=seq_params, extra_args=extra_args)
                tfm_input.append(tfm_inp)
            input = tfm_input
        elif isinstance(module, K.container.ImageSequentialBase):
            tfm_input = []
            seq_params = cls.get_sequential_module_param(param)
            for inp in input:
                tfm_inp = module.transform_masks(inp, params=seq_params, extra_args=extra_args)
                tfm_input.append(tfm_inp)
            input = tfm_input
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            raise NotImplementedError('The support for list of masks under auto operations are not yet supported. You are welcome to file a PR in our repo.')
        return input

    @classmethod
    def inverse(cls, input: 'Tensor', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Tensor:
        """Inverse a transformation with respect to the parameters.

        Args:
            input: the input tensor.
            module: any torch Module but only kornia augmentation modules will count
                to apply transformations.
            param: the corresponding parameters to the module.
        """
        if isinstance(module, (K.GeometricAugmentationBase2D,)):
            if module.transform_matrix is None:
                raise ValueError(f'No valid transformation matrix found in {module.__class__}.')
            transform = module.compute_inverse_transformation(module.transform_matrix)
            input = module.inverse_masks(input, params=cls.get_instance_module_param(param), flags=module.flags, transform=transform, **extra_args)
        elif isinstance(module, (K.GeometricAugmentationBase3D,)):
            raise NotImplementedError('The support for 3d mask operations are not yet supported. You are welcome to file a PR in our repo.')
        elif isinstance(module, K.container.ImageSequentialBase):
            input = module.inverse_masks(input, params=cls.get_sequential_module_param(param), extra_args=extra_args)
        elif isinstance(module, (K.auto.operations.OperationBase,)):
            input = MaskSequentialOps.inverse(input, module=module.op, param=param, extra_args=extra_args)
        return input


def _transform_input3d(input: 'Tensor') ->Tensor:
    """Reshape an input tensor to be (*, C, D, H, W). Accept either (D, H, W), (C, D, H, W) or (*, C, D, H, W).
    Args:
        input: Tensor

    Returns:
        Tensor
    """
    if not torch.is_tensor(input):
        raise TypeError(f'Input type is not a Tensor. Got {type(input)}')
    if len(input.shape) not in [3, 4, 5]:
        raise ValueError(f'Input size must have a shape of either (D, H, W), (C, D, H, W) or (*, C, D, H, W). Got {input.shape}')
    if len(input.shape) == 3:
        input = input.unsqueeze(0)
    if len(input.shape) == 4:
        input = input.unsqueeze(0)
    return input


def _transform_input3d_by_shape(input: 'Tensor', reference_shape: 'Tensor', match_channel: 'bool'=True) ->Tensor:
    """Reshape an input tensor to have the same dimensions as the reference_shape.

    Arguments
        input: tensor to be transformed
        reference_shape: shape used as reference
        match_channel: if True, C_{src} == C_{ref}. otherwise, no constrain. C =1 by default
    """
    B = reference_shape[-5] if len(reference_shape) >= 5 else None
    C = reference_shape[-4] if len(reference_shape) >= 4 else None
    if len(input.shape) == 3:
        input = input.unsqueeze(0)
    if len(input.shape) == 4 and B == input.shape[-4]:
        input = input.unsqueeze(2)
    if match_channel and C:
        if not input.shape[-4] == C:
            raise ValueError('The C dimension of tensor did not match with the reference tensor.')
    elif match_channel and C is None:
        raise ValueError('The reference tensor do not have a C dimension!')
    return input


def _validate_input_dtype(input: 'Tensor', accepted_dtypes: 'List[torch.dtype]') ->None:
    """Check if the dtype of the input tensor is in the range of accepted_dtypes
    Args:
        input: Tensor
        accepted_dtypes: List. e.g. [torch.float32, torch.float64]
    """
    if input.dtype not in accepted_dtypes:
        raise TypeError(f'Expected input of {accepted_dtypes}. Got {input.dtype}')


class ClassSequentialOps(SequentialOpsInterface[Tensor]):
    """Apply and inverse transformations for class labels if needed."""

    @classmethod
    def transform(cls, input: 'Tensor', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Tensor:
        if isinstance(module, K.MixAugmentationBaseV2):
            raise NotImplementedError('The support for class labels for mix augmentations that change the class label is not yet supported.')
        return input

    @classmethod
    def inverse(cls, input: 'Tensor', module: 'Module', param: 'ParamItem', extra_args: 'Dict[str, Any]'={}) ->Tensor:
        return input


class ChannelsOrder(Enum):
    """Enum that represents the channels order of an image."""
    CHANNELS_FIRST = 0
    CHANNELS_LAST = 1


class ColorSpace(Enum):
    """Enum that represents the color space of an image."""
    UNKNOWN = 0
    GRAY = 1
    RGB = 2
    BGR = 3


DLPack = Any


class ImageLoadType(Enum):
    """Enum to specify the desired image type."""
    UNCHANGED = 0
    GRAY8 = 1
    RGB8 = 2
    RGBA8 = 3
    GRAY32 = 4
    RGB32 = 5


def KORNIA_CHECK(condition: 'bool', msg: 'Optional[str]'=None, raises: 'bool'=True) ->bool:
    """Check any arbitrary boolean condition.

    Args:
        condition: the condition to evaluate.
        msg: message to show in the exception.
        raises: bool indicating whether an exception should be raised upon failure.

    Raises:
        Exception: if the condition is met and raises is True.

    Example:
        >>> x = torch.rand(2, 3, 3)
        >>> KORNIA_CHECK(x.shape[-2:] == (3, 3), "Invalid homography")
        True
    """
    if not condition:
        if raises:
            raise Exception(f'{condition} not true.\n{msg}')
        return False
    return True


def KORNIA_CHECK_SHAPE(x: 'Tensor', shape: 'list[str]', raises: 'bool'=True) ->bool:
    """Check whether a tensor has a specified shape.

    The shape can be specified with a implicit or explicit list of strings.
    The guard also check whether the variable is a type `Tensor`.

    Args:
        x: the tensor to evaluate.
        shape: a list with strings with the expected shape.
        raises: bool indicating whether an exception should be raised upon failure.

    Raises:
        Exception: if the input tensor is has not the expected shape and raises is True.

    Example:
        >>> x = torch.rand(2, 3, 4, 4)
        >>> KORNIA_CHECK_SHAPE(x, ["B", "C", "H", "W"])  # implicit
        True

        >>> x = torch.rand(2, 3, 4, 4)
        >>> KORNIA_CHECK_SHAPE(x, ["2", "3", "H", "W"])  # explicit
        True
    """
    if '*' == shape[0]:
        shape_to_check = shape[1:]
        x_shape_to_check = x.shape[-len(shape) + 1:]
    elif '*' == shape[-1]:
        shape_to_check = shape[:-1]
        x_shape_to_check = x.shape[:len(shape) - 1]
    else:
        shape_to_check = shape
        x_shape_to_check = x.shape
    if len(x_shape_to_check) != len(shape_to_check):
        if raises:
            raise TypeError(f'{x} shape must be [{shape}]. Got {x.shape}')
        else:
            return False
    for i in range(len(x_shape_to_check)):
        dim_: 'str' = shape_to_check[i]
        if not dim_.isnumeric():
            continue
        dim = int(dim_)
        if x_shape_to_check[i] != dim:
            if raises:
                raise TypeError(f'{x} shape must be [{shape}]. Got {x.shape}')
            else:
                return False
    return True


def image_to_tensor(image: 'Any', keepdim: 'bool'=True) ->Tensor:
    """Convert a numpy image to a PyTorch 4d tensor image.

    Args:
        image: image of the form :math:`(H, W, C)`, :math:`(H, W)` or
            :math:`(B, H, W, C)`.
        keepdim: If ``False`` unsqueeze the input image to match the shape
            :math:`(B, H, W, C)`.

    Returns:
        tensor of the form :math:`(B, C, H, W)` if keepdim is ``False``,
            :math:`(C, H, W)` otherwise.

    Example:
        >>> img = np.ones((3, 3))
        >>> image_to_tensor(img).shape
        torch.Size([1, 3, 3])

        >>> img = np.ones((4, 4, 1))
        >>> image_to_tensor(img).shape
        torch.Size([1, 4, 4])

        >>> img = np.ones((4, 4, 3))
        >>> image_to_tensor(img, keepdim=False).shape
        torch.Size([1, 3, 4, 4])
    """
    if len(image.shape) > 4 or len(image.shape) < 2:
        raise ValueError('Input size must be a two, three or four dimensional array')
    input_shape = image.shape
    tensor: 'Tensor' = torch.from_numpy(image)
    if len(input_shape) == 2:
        tensor = tensor.unsqueeze(0)
    elif len(input_shape) == 3:
        tensor = tensor.permute(2, 0, 1)
    elif len(input_shape) == 4:
        tensor = tensor.permute(0, 3, 1, 2)
        keepdim = True
    else:
        raise ValueError(f'Cannot process image with shape {input_shape}')
    return tensor.unsqueeze(0) if not keepdim else tensor


def _load_image_to_tensor(path_file: 'Path', device: 'Device') ->Tensor:
    """Read an image file and decode using the Kornia Rust backend.

    The decoded image is returned as numpy array with shape HxWxC.

    Args:
        path_file: Path to a valid image file.
        device: the device where you want to get your image placed.

    Return:
        Image tensor with shape :math:`(3,H,W)`.
    """
    if path_file.suffix.lower() in ['.jpg', '.jpeg']:
        img = kornia_rs.read_image_jpeg(str(path_file))
    else:
        img = kornia_rs.read_image_any(str(path_file))
    img_t = image_to_tensor(img, keepdim=True)
    dev = device if isinstance(device, torch.device) or device is None else torch.device(device)
    return img_t


def _to_float32(image: 'Tensor') ->Tensor:
    """Convert an image tensor to float32."""
    KORNIA_CHECK(image.dtype == torch.uint8)
    return image.float() / 255.0


def _to_uint8(image: 'Tensor') ->Tensor:
    """Convert an image tensor to uint8."""
    KORNIA_CHECK(image.dtype == torch.float32)
    return image.mul(255.0).byte()


def load_image(path_file: 'str | Path', desired_type: 'ImageLoadType'=ImageLoadType.RGB32, device: 'Device'='cpu') ->Tensor:
    """Read an image file and decode using the Kornia Rust backend.

    Args:
        path_file: Path to a valid image file.
        desired_type: the desired image type, defined by color space and dtype.
        device: the device where you want to get your image placed.

    Return:
        Image tensor with shape :math:`(3,H,W)`.
    """
    if not isinstance(path_file, Path):
        path_file = Path(path_file)
    image: 'Tensor' = _load_image_to_tensor(path_file, device)
    if desired_type == ImageLoadType.UNCHANGED:
        return image
    elif desired_type == ImageLoadType.GRAY8:
        if image.shape[0] == 1 and image.dtype == torch.uint8:
            return image
        elif image.shape[0] == 3 and image.dtype == torch.uint8:
            gray8 = kornia.color.rgb_to_grayscale(image)
            return gray8
        elif image.shape[0] == 4 and image.dtype == torch.uint8:
            gray32 = kornia.color.rgb_to_grayscale(kornia.color.rgba_to_rgb(_to_float32(image)))
            return _to_uint8(gray32)
    elif desired_type == ImageLoadType.RGB8:
        if image.shape[0] == 3 and image.dtype == torch.uint8:
            return image
        elif image.shape[0] == 1 and image.dtype == torch.uint8:
            rgb8 = kornia.color.grayscale_to_rgb(image)
            return rgb8
    elif desired_type == ImageLoadType.RGBA8:
        if image.shape[0] == 3 and image.dtype == torch.uint8:
            rgba32 = kornia.color.rgb_to_rgba(_to_float32(image), 0.0)
            return _to_uint8(rgba32)
    elif desired_type == ImageLoadType.GRAY32:
        if image.shape[0] == 1 and image.dtype == torch.uint8:
            return _to_float32(image)
        elif image.shape[0] == 3 and image.dtype == torch.uint8:
            gray32 = kornia.color.rgb_to_grayscale(_to_float32(image))
            return gray32
        elif image.shape[0] == 4 and image.dtype == torch.uint8:
            gray32 = kornia.color.rgb_to_grayscale(kornia.color.rgba_to_rgb(_to_float32(image)))
            return gray32
    elif desired_type == ImageLoadType.RGB32:
        if image.shape[0] == 3 and image.dtype == torch.uint8:
            return _to_float32(image)
        elif image.shape[0] == 1 and image.dtype == torch.uint8:
            rgb32 = kornia.color.grayscale_to_rgb(_to_float32(image))
            return rgb32
    raise NotImplementedError(f'Unknown type: {desired_type}')


np_ndarray = Any


def tensor_to_image(tensor: 'Tensor', keepdim: 'bool'=False, force_contiguous: 'bool'=False) ->Any:
    """Converts a PyTorch tensor image to a numpy image.

    In case the tensor is in the GPU, it will be copied back to CPU.

    Args:
        tensor: image of the form :math:`(H, W)`, :math:`(C, H, W)` or
            :math:`(B, C, H, W)`.
        keepdim: If ``False`` squeeze the input image to match the shape
            :math:`(H, W, C)` or :math:`(H, W)`.
        force_contiguous: If ``True`` call `contiguous` to the tensor before

    Returns:
        image of the form :math:`(H, W)`, :math:`(H, W, C)` or :math:`(B, H, W, C)`.

    Example:
        >>> img = torch.ones(1, 3, 3)
        >>> tensor_to_image(img).shape
        (3, 3)

        >>> img = torch.ones(3, 4, 4)
        >>> tensor_to_image(img).shape
        (4, 4, 3)
    """
    if not isinstance(tensor, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(tensor)}')
    if len(tensor.shape) > 4 or len(tensor.shape) < 2:
        raise ValueError('Input size must be a two, three or four dimensional tensor')
    input_shape = tensor.shape
    image = tensor.cpu().detach()
    if len(input_shape) == 2:
        pass
    elif len(input_shape) == 3:
        if input_shape[0] == 1:
            image = image.squeeze()
        else:
            image = image.permute(1, 2, 0)
    elif len(input_shape) == 4:
        image = image.permute(0, 2, 3, 1)
        if input_shape[0] == 1 and not keepdim:
            image = image.squeeze(0)
        if input_shape[1] == 1:
            image = image.squeeze(-1)
    else:
        raise ValueError(f'Cannot process tensor with shape {input_shape}')
    if force_contiguous:
        image = image.contiguous()
    return image.numpy()


def write_image(path_file: 'str | Path', image: 'Tensor') ->None:
    """Save an image file using the Kornia Rust backend.

    For now, we only support the writing of JPEG of the following types: RGB8.

    Args:
        path_file: Path to a valid image file.
        image: Image tensor with shape :math:`(3,H,W)`, `(1,H,W)` and `(H,W)`.

    Return:
        None.
    """
    if not isinstance(path_file, Path):
        path_file = Path(path_file)
    KORNIA_CHECK(path_file.suffix in ['.jpg', '.jpeg'], f'Invalid file extension: {path_file}, only .jpg and .jpeg are supported.')
    if image.dim() == 3 and image.shape[0] == 1 or image.dim() == 2:
        image = image.squeeze(0)
        image = image[None, :, :].repeat(3, 1, 1)
    KORNIA_CHECK(image.dim() == 3 and image.shape[0] == 3, f'Invalid image shape: {image.shape}')
    KORNIA_CHECK(image.dtype == torch.uint8, f'Invalid image dtype: {image.dtype}')
    img_np = tensor_to_image(image, keepdim=True, force_contiguous=True)
    kornia_rs.write_image_jpeg(str(path_file), img_np)


from_numpy = torch.from_numpy


def make_grid(tensor: 'Tensor', n_row: 'Optional[int]'=None, padding: 'int'=2) ->Tensor:
    """Convert a batched tensor to one image with padding in between.

    Args:
        tensor: A batched tensor of shape (B, C, H, W).
        n_row: Number of images displayed in each row of the grid.
        padding: The amount of padding to add between images.

    Returns:
        Tensor: The combined image grid.
    """
    if not isinstance(tensor, torch.Tensor):
        raise TypeError('Input tensor must be a PyTorch tensor.')
    B, C, H, W = tensor.shape
    if n_row is None:
        n_row = int(torch.sqrt(torch.tensor(B, dtype=torch.float32)).ceil())
    n_col = (B + n_row - 1) // n_row
    padded_H = H + padding
    padded_W = W + padding
    combined_H = n_row * padded_H - padding
    combined_W = n_col * padded_W - padding
    pad_value = 0
    combined_image = torch.full((C, combined_H, combined_W), pad_value, dtype=tensor.dtype)
    for idx in range(B):
        row = idx // n_col
        col = idx % n_col
        top = row * padded_H
        left = col * padded_W
        combined_image[:, top:top + H, left:left + W] = tensor[idx]
    return combined_image


class ImageModuleMixIn:
    """A MixIn that handles image-based operations.

    This modules accepts multiple input and output data types, provides end-to-end visualization, file saving features.
    Note that this MixIn fits the classes that return one image tensor only.
    """
    _output_image: 'Any'

    def convert_input_output(self, input_names_to_handle: 'Optional[List[Any]]'=None, output_type: 'str'='tensor') ->Callable[[Any], Any]:
        """Decorator to convert input and output types for a function.

        Args:
            input_names_to_handle: List of input names to convert, if None, handle all inputs.
            output_type: Desired output type ('tensor', 'numpy', or 'pil').

        Returns:
            Callable: Decorated function with converted input and output types.
        """

        def decorator(func: 'Callable[[Any], Any]') ->Callable[[Any], Any]:

            @wraps(func)
            def wrapper(*args: Any, **kwargs: Any) ->Union[Any, List[Any]]:
                if input_names_to_handle is None:
                    args = tuple(self.to_tensor(arg) if self._is_valid_arg(arg) else arg for arg in args)
                    kwargs = {k: (self.to_tensor(v) if self._is_valid_arg(v) else v) for k, v in kwargs.items()}
                else:
                    args = list(args)
                    for i, (arg, name) in enumerate(zip(args, func.__code__.co_varnames)):
                        if name in input_names_to_handle:
                            args[i] = self.to_tensor(arg)
                    for name, value in kwargs.items():
                        if name in input_names_to_handle:
                            kwargs[name] = self.to_tensor(value)
                tensor_outputs = func(*args, **kwargs)
                if not isinstance(tensor_outputs, (tuple,)):
                    tensor_outputs = tensor_outputs,
                outputs = []
                for output in tensor_outputs:
                    if output_type == 'tensor':
                        outputs.append(output)
                    elif output_type == 'numpy':
                        outputs.append(self.to_numpy(output))
                    elif output_type == 'pil':
                        outputs.append(self.to_pil(output))
                    else:
                        raise ValueError("Output type not supported. Choose from 'tensor', 'numpy', or 'pil'.")
                return outputs if len(outputs) > 1 else outputs[0]
            return wrapper
        return decorator

    def _is_valid_arg(self, arg: 'Any') ->bool:
        """Check if the argument is a valid type for conversion.

        Args:
            arg: The argument to check.

        Returns:
            bool: True if valid, False otherwise.
        """
        if isinstance(arg, (str,)) and os.path.exists(arg):
            return True
        if isinstance(arg, (Tensor,)):
            return True
        if isinstance(arg, (np.ndarray,)):
            return True
        if isinstance(arg, Image.Image):
            return True
        return False

    def to_tensor(self, x: 'Any') ->Tensor:
        """Convert input to tensor.

        Supports image path, numpy array, PIL image, and raw tensor.

        Args:
            x: The input to convert.

        Returns:
            Tensor: The converted tensor.
        """
        if isinstance(x, (str,)):
            return load_image(x, ImageLoadType.UNCHANGED) / 255
        if isinstance(x, (Tensor,)):
            return x
        if isinstance(x, (np.ndarray,)):
            return image_to_tensor(x) / 255
        if isinstance(x, (Image.Image,)):
            return from_numpy(np.array(x)).permute(2, 0, 1).float() / 255
        raise TypeError('Input type not supported')

    def to_numpy(self, x: 'Any') ->'np.array':
        """Convert input to numpy array.

        Args:
            x: The input to convert.

        Returns:
            np.array: The converted numpy array.
        """
        if isinstance(x, (Tensor,)):
            return x.cpu().detach().numpy()
        if isinstance(x, (np.ndarray,)):
            return x
        if isinstance(x, (Image.Image,)):
            return np.array(x)
        raise TypeError('Input type not supported')

    def to_pil(self, x: 'Any') ->'Image.Image':
        """Convert input to PIL image.

        Args:
            x: The input to convert.

        Returns:
            Image.Image: The converted PIL image.
        """
        if isinstance(x, (Tensor,)):
            x = x.cpu().detach() * 255
            if x.dim() == 3:
                x = x.permute(1, 2, 0)
                return Image.fromarray(x.byte().numpy())
            elif x.dim() == 4:
                x = x.permute(0, 2, 3, 1)
                return [Image.fromarray(_x.byte().numpy()) for _x in x]
            else:
                raise NotImplementedError
        if isinstance(x, (np.ndarray,)):
            raise NotImplementedError
        if isinstance(x, (Image.Image,)):
            return x
        raise TypeError('Input type not supported')

    def _detach_tensor_to_cpu(self, output_image: 'Union[Tensor, List[Tensor], Tuple[Tensor]]') ->Union[Tensor, List[Tensor], Tuple[Tensor]]:
        if isinstance(output_image, (Tensor,)):
            return output_image.detach().cpu()
        if isinstance(output_image, (list, tuple)):
            return type(output_image)([self._detach_tensor_to_cpu(out) for out in output_image])
        raise RuntimeError(f'Unexpected object {output_image} with a type of `{type(output_image)}`')

    def show(self, n_row: 'Optional[int]'=None, backend: 'str'='pil', display: 'bool'=True) ->Optional[Any]:
        """Returns PIL images.

        Args:
            n_row: Number of images displayed in each row of the grid.
            backend: visualization backend. Only PIL is supported now.
        """
        if self._output_image is None:
            raise ValueError('No pre-computed images found. Needs to execute first.')
        if len(self._output_image.shape) == 3:
            out_image = self._output_image
        elif len(self._output_image.shape) == 4:
            if n_row is None:
                n_row = math.ceil(self._output_image.shape[0] ** 0.5)
            out_image = make_grid(self._output_image, n_row, padding=2)
        else:
            raise ValueError
        if backend == 'pil' and display:
            Image.fromarray((out_image.permute(1, 2, 0).squeeze().numpy() * 255).astype(np.uint8)).show()
            return None
        if backend == 'pil':
            return Image.fromarray((out_image.permute(1, 2, 0).squeeze().numpy() * 255).astype(np.uint8))
        raise ValueError(f'Unsupported backend `{backend}`.')

    def save(self, name: 'Optional[str]'=None, n_row: 'Optional[int]'=None) ->None:
        """Saves the output image(s) to a directory.

        Args:
            name: Directory to save the images.
            n_row: Number of images displayed in each row of the grid.
        """
        if name is None:
            name = f"Kornia-{datetime.datetime.now(tz=datetime.timezone.utc).strftime('%Y%m%d%H%M%S')!s}.jpg"
        if len(self._output_image.shape) == 3:
            out_image = self._output_image
        if len(self._output_image.shape) == 4:
            if n_row is None:
                n_row = math.ceil(self._output_image.shape[0] ** 0.5)
            out_image = make_grid(self._output_image, n_row, padding=2)
        write_image(name, out_image.mul(255.0).byte())


class InstallationMode(str, Enum):
    ASK = 'ASK'
    AUTO = 'AUTO'
    RAISE = 'RAISE'

    def __eq__(self, other: 'object') ->bool:
        if isinstance(other, str):
            return self.value.lower() == other.lower()
        return super().__eq__(other)


class LazyLoaderConfig:
    _installation_mode: 'InstallationMode' = InstallationMode.ASK

    @property
    def installation_mode(self) ->InstallationMode:
        return self._installation_mode

    @installation_mode.setter
    def installation_mode(self, value: 'str') ->None:
        if isinstance(value, str):
            try:
                self._installation_mode = InstallationMode(value.upper())
            except ValueError:
                raise ValueError(f'{value} is not a valid InstallationMode. Choose from: {list(InstallationMode)}')
        elif isinstance(value, InstallationMode):
            self._installation_mode = value
        else:
            raise TypeError('installation_mode must be a string or InstallationMode Enum.')


logger = logging.getLogger(__name__)


class LazyLoader:
    """A class that implements lazy loading for Python modules.

    This class defers the import of a module until an attribute of the module is accessed.
    It helps in reducing the initial load time and memory usage of a script, especially when
    dealing with large or optional dependencies that might not be used in every execution.

    Attributes:
        module_name: The name of the module to be lazily loaded.
        module: The actual module object, initialized to None and loaded upon first access.
    """
    auto_install: 'bool' = False

    def __init__(self, module_name: 'str', dev_dependency: 'bool'=False) ->None:
        """Initializes the LazyLoader with the name of the module.

        Args:
            module_name: The name of the module to be lazily loaded.
            dev_dependency: If the dependency is required in the dev environment.
                If True, the module will be loaded in the dev environment.
                If False, the module will not be loaded in the dev environment.
        """
        self.module_name = module_name
        self.module: 'Optional[ModuleType]' = None
        self.dev_dependency = dev_dependency

    def _install_package(self, module_name: 'str') ->None:
        logger.info(f'Installing `{module_name}` ...')
        subprocess.run([sys.executable, '-m', 'pip', 'install', '-U', module_name], shell=False, check=False)

    def _load(self) ->None:
        """Loads the module if it hasn't been loaded yet.

        This method is called internally when an attribute of the module is accessed for the first time. It attempts to
        import the module and raises an ImportError with a custom message if the module is not installed.
        """
        if not self.dev_dependency:
            if '--doctest-modules' in sys.argv:
                logger.info(f"Doctest detected, skipping loading of '{self.module_name}'")
                return
            try:
                if __sphinx_build__:
                    logger.info(f"Sphinx detected, skipping loading of '{self.module_name}'")
                    return
            except NameError:
                pass
        if self.module is None:
            try:
                self.module = importlib.import_module(self.module_name)
            except ImportError as e:
                if kornia_config.lazyloader.installation_mode == InstallationMode.AUTO or self.auto_install:
                    self._install_package(self.module_name)
                elif kornia_config.lazyloader.installation_mode == InstallationMode.ASK:
                    to_ask = True
                    if_install = input(f"Optional dependency '{self.module_name}' is not installed. You may silent this prompt by `kornia_config.lazyloader.installation_mode = 'auto'`. Do you wish to install the dependency? [Y]es, [N]o, [A]ll.")
                    while to_ask:
                        if if_install.lower() == 'y' or if_install.lower() == 'yes':
                            self._install_package(self.module_name)
                            self.module = importlib.import_module(self.module_name)
                            to_ask = False
                        elif if_install.lower() == 'a' or if_install.lower() == 'all':
                            self.auto_install = True
                            self._install_package(self.module_name)
                            self.module = importlib.import_module(self.module_name)
                            to_ask = False
                        elif if_install.lower() == 'n' or if_install.lower() == 'no':
                            raise ImportError(f"Optional dependency '{self.module_name}' is not installed. Please install it to use this functionality.") from e
                        else:
                            if_install = input("Invalid input. Please enter 'Y', 'N', or 'A'.")
                elif kornia_config.lazyloader.installation_mode == InstallationMode.RAISE:
                    raise ImportError(f"Optional dependency '{self.module_name}' is not installed. Please install it to use this functionality.") from e
                self.module = importlib.import_module(self.module_name)

    def __getattr__(self, item: 'str') ->object:
        """Loads the module (if not already loaded) and returns the requested attribute.

        This method is called when an attribute of the LazyLoader instance is accessed.
        It ensures that the module is loaded and then returns the requested attribute.

        Args:
            item: The name of the attribute to be accessed.

        Returns:
            The requested attribute of the loaded module.
        """
        self._load()
        return getattr(self.module, item)

    def __dir__(self) ->List[str]:
        """Loads the module (if not already loaded) and returns the list of attributes of the module.

        This method is called when the built-in dir() function is used on the LazyLoader instance.
        It ensures that the module is loaded and then returns the list of attributes of the module.

        Returns:
            list: The list of attributes of the loaded module.
        """
        self._load()
        return dir(self.module)


onnx = LazyLoader('onnx', dev_dependency=True)


rand = torch.rand


class ImageModuleForSequentialMixIn(ImageModuleMixIn):
    _disable_features: 'bool' = False

    @property
    def disable_features(self) ->bool:
        return self._disable_features

    @disable_features.setter
    def disable_features(self, value: 'bool'=True) ->None:
        self._disable_features = value

    def disable_item_features(self, *args: Module) ->None:
        for arg in args:
            if isinstance(arg, (ImageModule,)):
                arg.disable_features = True


def _get_new_batch_shape(param: 'ParamItem', batch_shape: 'torch.Size') ->torch.Size:
    """Get the new batch shape if the augmentation changes the image size.

    Note:
       Augmentations that change the image size must provide the parameter `output_size`.
    """
    if param.data is None:
        return batch_shape
    if isinstance(param.data, list):
        for p in param.data:
            batch_shape = _get_new_batch_shape(p, batch_shape)
    elif 'output_size' in param.data:
        if not (param.data['batch_prob'] > 0.5)[0]:
            return batch_shape
        new_batch_shape = list(batch_shape)
        new_batch_shape[-2:] = param.data['output_size'][0]
        batch_shape = torch.Size(new_batch_shape)
    return batch_shape


as_tensor = torch.as_tensor


MaskDataType = Union[Tensor, List[Tensor]]


class PatchParamItem(NamedTuple):
    indices: 'List[int]'
    param: 'ParamItem'


concatenate = torch.cat


def _check_patch_fit(original_size: 'Tuple[int, int]', window_size: 'Tuple[int, int]', stride: 'Tuple[int, int]') ->bool:
    remainder_vertical = (original_size[0] - window_size[0]) % stride[0]
    remainder_horizontal = (original_size[1] - window_size[1]) % stride[1]
    if remainder_horizontal != 0 or remainder_vertical != 0:
        return False
    return True


def _extract_tensor_patchesnd(input: 'Tensor', window_sizes: 'Tuple[int, ...]', strides: 'Tuple[int, ...]') ->Tensor:
    batch_size, num_channels = input.size()[:2]
    dims = range(2, input.dim())
    for dim, patch_size, stride in zip(dims, window_sizes, strides):
        input = input.unfold(dim, patch_size, stride)
    input = input.permute(0, *dims, 1, *(dim + len(dims) for dim in dims)).contiguous()
    return input.view(batch_size, -1, num_channels, *window_sizes)


FullPadType = Tuple[int, int, int, int]


def compute_padding(original_size: 'Union[int, Tuple[int, int]]', window_size: 'Union[int, Tuple[int, int]]', stride: 'Optional[Union[int, Tuple[int, int]]]'=None) ->FullPadType:
    """Compute required padding to ensure chaining of :func:`extract_tensor_patches` and
    :func:`combine_tensor_patches` produces expected result.

    Args:
        original_size: the size of the original tensor.
        window_size: the size of the sliding window used while extracting patches.
        stride: The stride of the sliding window. Optional: if not specified, window_size will be used.

    Return:
        The required padding as a tuple of four ints: (top, bottom, left, right)

    Example:
        >>> image = torch.arange(12).view(1, 1, 4, 3)
        >>> padding = compute_padding((4,3), (3,3))
        >>> out = extract_tensor_patches(image, window_size=(3, 3), stride=(3, 3), padding=padding)
        >>> combine_tensor_patches(out, original_size=(4, 3), window_size=(3, 3), stride=(3, 3), unpadding=padding)
        tensor([[[[ 0,  1,  2],
                  [ 3,  4,  5],
                  [ 6,  7,  8],
                  [ 9, 10, 11]]]])

    .. note::
        This function will be implicitly used in :func:`extract_tensor_patches` and :func:`combine_tensor_patches` if
        `allow_auto_(un)padding` is set to True.
    """
    original_size = cast(Tuple[int, int], _pair(original_size))
    window_size = cast(Tuple[int, int], _pair(window_size))
    if stride is None:
        stride = window_size
    stride = cast(Tuple[int, int], _pair(stride))
    remainder_vertical = (original_size[0] - window_size[0]) % stride[0]
    remainder_horizontal = (original_size[1] - window_size[1]) % stride[1]
    if remainder_vertical != 0:
        vertical_padding = stride[0] - remainder_vertical
    else:
        vertical_padding = 0
    if remainder_horizontal != 0:
        horizontal_padding = stride[1] - remainder_horizontal
    else:
        horizontal_padding = 0
    if vertical_padding % 2 == 0:
        top_padding = bottom_padding = vertical_padding // 2
    else:
        top_padding = vertical_padding // 2
        bottom_padding = ceil(vertical_padding / 2)
    if horizontal_padding % 2 == 0:
        left_padding = right_padding = horizontal_padding // 2
    else:
        left_padding = horizontal_padding // 2
        right_padding = ceil(horizontal_padding / 2)
    padding = int(top_padding), int(bottom_padding), int(left_padding), int(right_padding)
    return cast(FullPadType, padding)


TuplePadType = Union[Tuple[int, int], FullPadType]


def create_padding_tuple(padding: 'PadType', unpadding: 'bool'=False) ->FullPadType:
    padding = cast(TuplePadType, _pair(padding))
    if len(padding) not in [2, 4]:
        raise AssertionError(f"{'Unpadding' if unpadding else 'Padding'} must be either an int, tuple of two ints or tuple of four ints")
    if len(padding) == 2:
        pad_vert = _pair(padding[0])
        pad_horz = _pair(padding[1])
    else:
        pad_vert = padding[:2]
        pad_horz = padding[2:]
    padding = cast(FullPadType, pad_horz + pad_vert)
    return padding


def extract_tensor_patches(input: 'Tensor', window_size: 'Union[int, Tuple[int, int]]', stride: 'Union[int, Tuple[int, int]]'=1, padding: 'PadType'=0, allow_auto_padding: 'bool'=False) ->Tensor:
    """Function that extract patches from tensors and stacks them.

    See :class:`~kornia.contrib.ExtractTensorPatches` for details.

    Args:
        input: tensor image where to extract the patches with shape :math:`(B, C, H, W)`.
        window_size: the size of the sliding window and the output patch size.
        stride: stride of the sliding window.
        padding: Zero-padding added to both side of the input.
        allow_auto_adding: whether to allow automatic padding if the window and stride do not fit into the image.

    Returns:
        the tensor with the extracted patches with shape :math:`(B, N, C, H_{out}, W_{out})`.

    Examples:
        >>> input = torch.arange(9.).view(1, 1, 3, 3)
        >>> patches = extract_tensor_patches(input, (2, 3))
        >>> input
        tensor([[[[0., 1., 2.],
                  [3., 4., 5.],
                  [6., 7., 8.]]]])
        >>> patches[:, -1]
        tensor([[[[3., 4., 5.],
                  [6., 7., 8.]]]])
    """
    if not torch.is_tensor(input):
        raise TypeError(f'Input input type is not a Tensor. Got {type(input)}')
    if len(input.shape) != 4:
        raise ValueError(f'Invalid input shape, we expect BxCxHxW. Got: {input.shape}')
    window_size = cast(Tuple[int, int], _pair(window_size))
    stride = cast(Tuple[int, int], _pair(stride))
    original_size = input.shape[-2], input.shape[-1]
    if not padding:
        if not _check_patch_fit(original_size, window_size, stride):
            if not allow_auto_padding:
                warn(f'The window will not fit into the image. \nWindow size: {window_size}\nStride: {stride}\nImage size: {original_size}\nThis means that the final incomplete patches will be dropped. By enabling `allow_auto_padding`, the input will be padded to fit the window and stride.')
            else:
                padding = compute_padding(original_size=original_size, window_size=window_size, stride=stride)
    if padding:
        padding = create_padding_tuple(padding)
        input = pad(input, padding)
    return _extract_tensor_patchesnd(input, window_size, stride)


class Resample(Enum, metaclass=_KORNIA_EnumMeta):
    NEAREST = 0
    BILINEAR = 1
    BICUBIC = 2

    @classmethod
    def get(cls, value: "TKEnum['Resample']") ->'Resample':
        return _get(cls, value)


def _transform_input(input: 'Tensor') ->Tensor:
    """Reshape an input tensor to be (*, C, H, W). Accept either (H, W), (C, H, W) or (*, C, H, W).
    Args:
        input: Tensor

    Returns:
        Tensor
    """
    if not torch.is_tensor(input):
        raise TypeError(f'Input type is not a Tensor. Got {type(input)}')
    if len(input.shape) not in [2, 3, 4]:
        raise ValueError(f'Input size must have a shape of either (H, W), (C, H, W) or (*, C, H, W). Got {input.shape}')
    if len(input.shape) == 2:
        input = input.unsqueeze(0)
    if len(input.shape) == 3:
        input = input.unsqueeze(0)
    return input


def _transform_input_by_shape(input: 'Tensor', reference_shape: 'Tensor', match_channel: 'bool'=True) ->Tensor:
    """Reshape an input tensor to have the same dimensions as the reference_shape.

    Arguments
        input: tensor to be transformed
        reference_shape: shape used as reference
        match_channel: if True, C_{src} == C_{ref}. otherwise, no constrain. C =1 by default
    """
    B = reference_shape[-4] if len(reference_shape) >= 4 else None
    C = reference_shape[-3] if len(reference_shape) >= 3 else None
    if len(input.shape) == 2:
        input = input.unsqueeze(0)
    if len(input.shape) == 3:
        input = input.unsqueeze(1) if B == input.shape[-3] else input.unsqueeze(0)
    if match_channel and C:
        if not input.shape[-3] == C:
            raise ValueError('The C dimension of tensor did not match with the reference tensor.')
    elif match_channel and C is None:
        raise ValueError('The reference tensor do not have a C dimension!')
    return input


def _boxes3d_to_polygons3d(xmin: 'torch.Tensor', ymin: 'torch.Tensor', zmin: 'torch.Tensor', width: 'torch.Tensor', height: 'torch.Tensor', depth: 'torch.Tensor') ->torch.Tensor:
    if not xmin.ndim == ymin.ndim == zmin.ndim == width.ndim == height.ndim == depth.ndim == 2:
        raise ValueError('We expect to create a batch of 3D boxes (hexahedrons) in vertices format (B, N, 8, 3)')
    front_vertices = zeros((xmin.shape[0], xmin.shape[1], 4, 3), device=xmin.device, dtype=xmin.dtype)
    front_vertices[..., 0] = xmin.unsqueeze(-1)
    front_vertices[..., 1] = ymin.unsqueeze(-1)
    front_vertices[..., 2] = zmin.unsqueeze(-1)
    front_vertices[..., 1, 0] += width - 1
    front_vertices[..., 2, 0] += width - 1
    front_vertices[..., 2, 1] += height - 1
    front_vertices[..., 3, 1] += height - 1
    back_vertices = front_vertices.clone()
    back_vertices[..., 2] += depth.unsqueeze(-1) - 1
    polygons3d = torch.cat([front_vertices, back_vertices], dim=-2)
    return polygons3d


stack = torch.stack


class Keypoints3D:
    """3D Keypoints containing Nx3 or BxNx3 points.

    Args:
        keypoints: Raw tensor or a list of Tensors with the Nx3 coordinates
        raise_if_not_floating_point: will raise if the Tensor isn't float
    """

    def __init__(self, keypoints: 'Union[Tensor, List[Tensor]]', raise_if_not_floating_point: 'bool'=True) ->None:
        self._N: 'Optional[List[int]]' = None
        if isinstance(keypoints, list):
            keypoints, self._N = _merge_keypoint_list(keypoints)
        if not isinstance(keypoints, Tensor):
            raise TypeError(f'Input keypoints is not a Tensor. Got: {type(keypoints)}.')
        if not keypoints.is_floating_point():
            if raise_if_not_floating_point:
                raise ValueError(f'Coordinates must be in floating point. Got {keypoints.dtype}')
            keypoints = keypoints.float()
        if len(keypoints.shape) == 0:
            keypoints = keypoints.reshape((-1, 3))
        if not (2 <= keypoints.ndim <= 3 and keypoints.shape[-1:] == (3,)):
            raise ValueError(f'Keypoints shape must be (N, 3) or (B, N, 3). Got {keypoints.shape}.')
        self._is_batched = False if keypoints.ndim == 2 else True
        self._data = keypoints

    def __getitem__(self, key: 'Union[slice, int, Tensor]') ->'Keypoints3D':
        new_obj = type(self)(self._data[key], False)
        return new_obj

    def __setitem__(self, key: 'Union[slice, int, Tensor]', value: "'Keypoints3D'") ->'Keypoints3D':
        self._data[key] = value._data
        return self

    @property
    def shape(self) ->Size:
        return self.data.shape

    @property
    def data(self) ->Tensor:
        return self._data

    def pad(self, padding_size: 'Tensor') ->'Keypoints3D':
        """Pad a bounding keypoints.

        Args:
            padding_size: (B, 6)
        """
        raise NotImplementedError

    def unpad(self, padding_size: 'Tensor') ->'Keypoints3D':
        """Pad a bounding keypoints.

        Args:
            padding_size: (B, 6)
        """
        raise NotImplementedError

    def transform_keypoints(self, M: 'Tensor', inplace: 'bool'=False) ->'Keypoints3D':
        """Apply a transformation matrix to the 2D keypoints.

        Args:
            M: The transformation matrix to be applied, shape of :math:`(3, 3)` or :math:`(B, 3, 3)`.
            inplace: do transform in-place and return self.

        Returns:
            The transformed keypoints.
        """
        raise NotImplementedError

    def transform_keypoints_(self, M: 'Tensor') ->'Keypoints3D':
        """Inplace version of :func:`Keypoints.transform_keypoints`"""
        return self.transform_keypoints(M, inplace=True)

    @classmethod
    def from_tensor(cls, keypoints: 'Tensor') ->'Keypoints3D':
        return cls(keypoints)

    def to_tensor(self, as_padded_sequence: 'bool'=False) ->Union[Tensor, List[Tensor]]:
        """Cast :class:`Keypoints` to a tensor. ``mode`` controls which 2D keypoints format should be use to
        represent keypoints in the tensor.

        Args:
            as_padded_sequence: whether to keep the pads for a list of keypoints. This parameter is only valid
                if the keypoints are from a keypoint list.

        Returns:
            Keypoints tensor :math:`(B, N, 3)`
        """
        if as_padded_sequence:
            raise NotImplementedError
        return self._data

    def clone(self) ->'Keypoints3D':
        return Keypoints3D(self._data.clone(), False)


class TransformMatrixMinIn:
    """Enables computation matrix computation."""
    _valid_ops_for_transform_computation: 'Tuple[Any, ...]' = ()
    _transformation_matrix_arg: 'str' = 'silent'

    def __init__(self, *args, **kwargs) ->None:
        super().__init__(*args, **kwargs)
        self._transform_matrix: 'Optional[Tensor]' = None
        self._transform_matrices: 'List[Optional[Tensor]]' = []

    def _parse_transformation_matrix_mode(self, transformation_matrix_mode: 'str') ->None:
        _valid_transformation_matrix_args = {'silence', 'silent', 'rigid', 'skip'}
        if transformation_matrix_mode not in _valid_transformation_matrix_args:
            raise ValueError(f'`transformation_matrix` has to be one of {_valid_transformation_matrix_args}. Got {transformation_matrix_mode}.')
        self._transformation_matrix_arg = transformation_matrix_mode

    @property
    def transform_matrix(self) ->Optional[Tensor]:
        if self._transform_matrix is None and len(self._transform_matrices) != 0:
            self._transform_matrix = self._transform_matrices[0]
            for mat in self._transform_matrices[1:]:
                self._update_transform_matrix(mat)
        return self._transform_matrix

    def _update_transform_matrix_for_valid_op(self, module: 'Module') ->None:
        raise NotImplementedError(module)

    def _update_transform_matrix_by_module(self, module: 'Module') ->None:
        if self._transformation_matrix_arg == 'skip':
            return
        if isinstance(module, self._valid_ops_for_transform_computation):
            self._update_transform_matrix_for_valid_op(module)
        elif self._transformation_matrix_arg == 'rigid':
            raise RuntimeError(f'Non-rigid module `{module}` is not supported under `rigid` computation mode. Please either update the module or change the `transformation_matrix` argument.')

    def _update_transform_matrix(self, transform_matrix: 'Optional[Tensor]') ->None:
        if self._transform_matrix is None:
            self._transform_matrix = transform_matrix
        else:
            self._transform_matrix = transform_matrix @ self._transform_matrix

    def _reset_transform_matrix_state(self) ->None:
        self._transform_matrix = None
        self._transform_matrices = []


class VideoKeypoints(Keypoints):
    temporal_channel_size: 'int'

    @classmethod
    def from_tensor(cls, boxes: 'Union[Tensor, List[Tensor]]', validate_boxes: 'bool'=True) ->'VideoKeypoints':
        if isinstance(boxes, (list,)) or (boxes.dim() != 4 or boxes.shape[-1] != 2):
            raise ValueError('Input box type is not yet supported. Please input an `BxTxNx2` tensor directly.')
        temporal_channel_size = boxes.size(1)
        out = cls(boxes.view(boxes.size(0) * boxes.size(1), -1, boxes.size(3)))
        out.temporal_channel_size = temporal_channel_size
        return out

    def to_tensor(self) ->Tensor:
        out = super().to_tensor(as_padded_sequence=False)
        out = cast(Tensor, out)
        return out.view(-1, self.temporal_channel_size, *out.shape[1:])

    def transform_keypoints(self, M: 'Tensor', inplace: 'bool'=False) ->'VideoKeypoints':
        out = super().transform_keypoints(M, inplace=inplace)
        if inplace:
            return self
        out = VideoKeypoints(out.data, False)
        out.temporal_channel_size = self.temporal_channel_size
        return out

    def clone(self) ->'VideoKeypoints':
        out = VideoKeypoints(self._data.clone(), False)
        out.temporal_channel_size = self.temporal_channel_size
        return out


_BOXES_OPTIONS = {DataKey.BBOX, DataKey.BBOX_XYXY, DataKey.BBOX_XYWH}


_CLS_OPTIONS = {DataKey.CLASS, DataKey.LABEL}


_IMG_OPTIONS = {DataKey.INPUT, DataKey.IMAGE}


_KEYPOINTS_OPTIONS = {DataKey.KEYPOINTS}


_MSK_OPTIONS = {DataKey.MASK}


def apply_colormap(input_tensor: 'Tensor', colormap: 'ColorMap') ->Tensor:
    """Apply to a gray tensor a colormap.

    .. image:: _static/img/apply_colormap.png

    Args:
        input_tensor: the input tensor of image.
        colormap: the colormap desired to be applied to the input tensor.

    Returns:
        A RGB tensor with the applied color map into the input_tensor.

    Raises:
        ValueError: If `colormap` is not a ColorMap object.

    .. note::
        The input tensor must be integer values in the range of [0-255] or float values in the range of [0-1].

    Example:
        >>> input_tensor = torch.tensor([[[0, 1, 2], [15, 25, 33], [128, 158, 188]]])
        >>> colormap = ColorMap(base=ColorMapType.autumn)
        >>> apply_colormap(input_tensor, colormap)
        tensor([[[[1.0000, 1.0000, 1.0000],
                  [1.0000, 1.0000, 1.0000],
                  [1.0000, 1.0000, 1.0000]],
        <BLANKLINE>
                 [[0.0000, 0.0159, 0.0159],
                  [0.0635, 0.1111, 0.1429],
                  [0.5079, 0.6190, 0.7302]],
        <BLANKLINE>
                 [[0.0000, 0.0000, 0.0000],
                  [0.0000, 0.0000, 0.0000],
                  [0.0000, 0.0000, 0.0000]]]])
    """
    KORNIA_CHECK(isinstance(input_tensor, Tensor), f'`input_tensor` must be a Tensor. Got: {type(input_tensor)}')
    valid_types = [torch.half, torch.float, torch.double, torch.uint8, torch.int, torch.long, torch.short]
    KORNIA_CHECK(input_tensor.dtype in valid_types, f'`input_tensor` must be a {valid_types}. Got: {input_tensor.dtype}')
    KORNIA_CHECK(len(input_tensor.shape) in (3, 4), 'Wrong input tensor dimension.')
    if len(input_tensor.shape) == 3:
        input_tensor = input_tensor.unsqueeze_(0)
    B, C, H, W = input_tensor.shape
    input_tensor = input_tensor.reshape(B, C, -1)
    max_value = 1.0 if input_tensor.max() <= 1.0 else 255.0
    input_tensor = input_tensor.float().div_(max_value)
    colors = colormap.colors.permute(1, 0)
    num_colors, channels_cmap = colors.shape
    keys = torch.linspace(0.0, 1.0, num_colors - 1, device=input_tensor.device, dtype=input_tensor.dtype)
    indices = torch.bucketize(input_tensor, keys).unsqueeze(-1).expand(-1, -1, -1, 3)
    output = torch.gather(colors.expand(B, C, -1, -1), 2, indices)
    output = output.permute(0, 1, 3, 2).reshape(B, C * channels_cmap, H, W)
    return output


class ApplyColorMap(Module):
    """Class for applying a colormap to images.

    .. image:: _static/img/ApplyColorMap.png

    Args:
        colormap: Either the name of a built-in colormap or a ColorMap object.
        num_colors: Number of colors in the colormap. Default is 256.
        device: The device to put the generated colormap on.
        dtype: The data type of the generated colormap.

    Returns:
        A RGB tensor with the applied color map into the input_tensor

    Raises:
        ValueError: If `colormap` is not a ColorMap object.

    .. note::
        The input tensor must be integer values in the range of [0-255] or float values in the range of [0-1].

    Example:
        >>> input_tensor = torch.tensor([[[0, 1, 2], [15, 25, 33], [128, 158, 188]]])
        >>> colormap = ColorMap(base=ColorMapType.autumn)
        >>> ApplyColorMap(colormap=colormap)(input_tensor)
        tensor([[[[1.0000, 1.0000, 1.0000],
                  [1.0000, 1.0000, 1.0000],
                  [1.0000, 1.0000, 1.0000]],
        <BLANKLINE>
                 [[0.0000, 0.0159, 0.0159],
                  [0.0635, 0.1111, 0.1429],
                  [0.5079, 0.6190, 0.7302]],
        <BLANKLINE>
                 [[0.0000, 0.0000, 0.0000],
                  [0.0000, 0.0000, 0.0000],
                  [0.0000, 0.0000, 0.0000]]]])
    """

    def __init__(self, colormap: 'ColorMap') ->None:
        super().__init__()
        self.colormap = colormap

    def forward(self, input_tensor: 'Tensor') ->Tensor:
        """Applies the colormap to the input tensor.

        Args:
            input_tensor: The input tensor representing the grayscale image.

        .. note::
        The input tensor must be integer values in the range of [0-255] or float values in the range of [0-1].

        Returns:
            The output tensor representing the image with the applied colormap.
        """
        return apply_colormap(input_tensor, self.colormap)


def grayscale_to_rgb(image: 'Tensor') ->Tensor:
    """Convert a grayscale image to RGB version of image.

    .. image:: _static/img/grayscale_to_rgb.png

    The image data is assumed to be in the range of (0, 1).

    Args:
        image: grayscale image tensor to be converted to RGB with shape :math:`(*,1,H,W)`.

    Returns:
        RGB version of the image with shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.randn(2, 1, 4, 5)
        >>> gray = grayscale_to_rgb(input) # 2x3x4x5
    """
    KORNIA_CHECK_IS_TENSOR(image)
    if len(image.shape) < 3 or image.shape[-3] != 1:
        raise ValueError(f'Input size must have a shape of (*, 1, H, W). Got {image.shape}.')
    return concatenate([image, image, image], -3)


class GrayscaleToRgb(Module):
    """Module to convert a grayscale image to RGB version of image.

    The image data is assumed to be in the range of (0, 1).

    Shape:
        - image: :math:`(*, 1, H, W)`
        - output: :math:`(*, 3, H, W)`

    reference:
        https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

    Example:
        >>> input = torch.rand(2, 1, 4, 5)
        >>> rgb = GrayscaleToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 1, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return grayscale_to_rgb(image)


def rgb_to_grayscale(image: 'Tensor', rgb_weights: 'Optional[Tensor]'=None) ->Tensor:
    """Convert a RGB image to grayscale version of image.

    .. image:: _static/img/rgb_to_grayscale.png

    The image data is assumed to be in the range of (0, 1).

    Args:
        image: RGB image to be converted to grayscale with shape :math:`(*,3,H,W)`.
        rgb_weights: Weights that will be applied on each channel (RGB).
            The sum of the weights should add up to one.
    Returns:
        grayscale version of the image with shape :math:`(*,1,H,W)`.

    .. note::
       See a working example `here <https://kornia.github.io/tutorials/nbs/color_conversions.html>`__.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> gray = rgb_to_grayscale(input) # 2x1x4x5
    """
    KORNIA_CHECK_IS_TENSOR(image)
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    if rgb_weights is None:
        if image.dtype == torch.uint8:
            rgb_weights = torch.tensor([76, 150, 29], device=image.device, dtype=torch.uint8)
        elif image.dtype in (torch.float16, torch.float32, torch.float64):
            rgb_weights = torch.tensor([0.299, 0.587, 0.114], device=image.device, dtype=image.dtype)
        else:
            raise TypeError(f'Unknown data type: {image.dtype}')
    else:
        rgb_weights = rgb_weights
    r: 'Tensor' = image[..., 0:1, :, :]
    g: 'Tensor' = image[..., 1:2, :, :]
    b: 'Tensor' = image[..., 2:3, :, :]
    w_r, w_g, w_b = rgb_weights.unbind()
    return w_r * r + w_g * g + w_b * b


class RgbToGrayscale(Module):
    """Module to convert a RGB image to grayscale version of image.

    The image data is assumed to be in the range of (0, 1).

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 1, H, W)`

    reference:
        https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> gray = RgbToGrayscale()
        >>> output = gray(input)  # 2x1x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 1, -1, -1]

    def __init__(self, rgb_weights: 'Optional[Tensor]'=None) ->None:
        super().__init__()
        if rgb_weights is None:
            rgb_weights = Tensor([0.299, 0.587, 0.114])
        self.rgb_weights = rgb_weights

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_grayscale(image, rgb_weights=self.rgb_weights)


def bgr_to_rgb(image: 'Tensor') ->Tensor:
    """Convert a BGR image to RGB.

    Args:
        image: BGR Image to be converted to BGR of shape :math:`(*,3,H,W)`.

    Returns:
        RGB version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = bgr_to_rgb(input) # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W).Got {image.shape}')
    out: 'Tensor' = image.flip(-3)
    return out


def bgr_to_grayscale(image: 'Tensor') ->Tensor:
    """Convert a BGR image to grayscale.

    The image data is assumed to be in the range of (0, 1). First flips to RGB, then converts.

    Args:
        image: BGR image to be converted to grayscale with shape :math:`(*,3,H,W)`.

    Returns:
        grayscale version of the image with shape :math:`(*,1,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> gray = bgr_to_grayscale(input) # 2x1x4x5
    """
    KORNIA_CHECK_IS_TENSOR(image)
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    image_rgb: 'Tensor' = bgr_to_rgb(image)
    return rgb_to_grayscale(image_rgb)


class BgrToGrayscale(Module):
    """Module to convert a BGR image to grayscale version of image.

    The image data is assumed to be in the range of (0, 1). First flips to RGB, then converts.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 1, H, W)`

    reference:
        https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> gray = BgrToGrayscale()
        >>> output = gray(input)  # 2x1x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 1, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return bgr_to_grayscale(image)


def rgb_to_hls(image: 'Tensor', eps: 'float'=1e-08) ->Tensor:
    """Convert an RGB image to HLS.

    .. image:: _static/img/rgb_to_hls.png

    The image data is assumed to be in the range of (0, 1).

    NOTE: this method cannot be compiled with JIT in pytohrch < 1.7.0

    Args:
        image: RGB image to be converted to HLS with shape :math:`(*, 3, H, W)`.
        eps: epsilon value to avoid div by zero.

    Returns:
        HLS version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_hls(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    _RGB2HSL_IDX = tensor([[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype)
    _img_max: 'tuple[Tensor, Tensor]' = image.max(-3)
    maxc = _img_max[0]
    imax = _img_max[1]
    minc: 'Tensor' = image.min(-3)[0]
    if image.requires_grad:
        l_ = maxc + minc
        s = maxc - minc
        h = l_
        image_hls = l_
    else:
        image_hls = torch.empty_like(image)
        h, l_, s = image_hls[..., 0, :, :], image_hls[..., 1, :, :], image_hls[..., 2, :, :]
        torch.add(maxc, minc, out=l_)
        torch.sub(maxc, minc, out=s)
    im = image / (s + eps).unsqueeze(-3)
    s /= where(l_ < 1.0, l_, 2.0 - l_) + eps
    l_ /= 2
    r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
    cond = imax[..., None, :, :] == _RGB2HSL_IDX
    if image.requires_grad:
        h = (g - b) % 6 * cond[..., 0, :, :]
    else:
        torch.mul((g - b) % 6, cond[..., 0, :, :], out=h)
    h += (b - r + 2) * cond[..., 1, :, :]
    h += (r - g + 4) * cond[..., 2, :, :]
    h *= math.pi / 3.0
    if image.requires_grad:
        return stack([h, l_, s], -3)
    return image_hls


class RgbToHls(Module):
    """Convert an image from RGB to HLS.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        HLS version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> hls = RgbToHls()
        >>> output = hls(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_hls(image)


def hls_to_rgb(image: 'Tensor') ->Tensor:
    """Convert a HLS image to RGB.

    The image data is assumed to be in the range of (0, 1).

    Args:
        image: HLS image to be converted to RGB with shape :math:`(*, 3, H, W)`.

    Returns:
        RGB version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = hls_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    _HLS2RGB = tensor([[[0.0]], [[8.0]], [[4.0]]], device=image.device, dtype=image.dtype)
    im: 'Tensor' = image.unsqueeze(-4)
    h_ch: 'Tensor' = im[..., 0, :, :]
    l_ch: 'Tensor' = im[..., 1, :, :]
    s_ch: 'Tensor' = im[..., 2, :, :]
    h_ch = h_ch * (6 / math.pi)
    a = s_ch * torch.min(l_ch, 1.0 - l_ch)
    k: 'Tensor' = (h_ch + _HLS2RGB) % 12
    mink = torch.min(k - 3.0, 9.0 - k)
    return torch.addcmul(l_ch, a, mink.clamp_(min=-1.0, max=1.0), value=-1)


class HlsToRgb(Module):
    """Convert an image from HLS to RGB.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        RGB version of the image.

    Shape:
        - input: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Reference:
        https://en.wikipedia.org/wiki/HSL_and_HSV

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = HlsToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return hls_to_rgb(image)


def rgb_to_hsv(image: 'torch.Tensor', eps: 'float'=1e-08) ->torch.Tensor:
    """Convert an image from RGB to HSV.

    .. image:: _static/img/rgb_to_hsv.png

    The image data is assumed to be in the range of (0, 1).

    Args:
        image: RGB Image to be converted to HSV with shape of :math:`(*, 3, H, W)`.
        eps: scalar to enforce numarical stability.

    Returns:
        HSV version of the image with shape of :math:`(*, 3, H, W)`.
        The H channel values are in the range 0..2pi. S and V are in the range 0..1.

    .. note::
       See a working example `here <https://kornia.github.io/tutorials/nbs/color_conversions.html>`__.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_hsv(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    max_rgb, argmax_rgb = image.max(-3)
    min_rgb, argmin_rgb = image.min(-3)
    deltac = max_rgb - min_rgb
    v = max_rgb
    s = deltac / (max_rgb + eps)
    deltac = torch.where(deltac == 0, torch.ones_like(deltac), deltac)
    rc, gc, bc = torch.unbind(max_rgb.unsqueeze(-3) - image, dim=-3)
    h1 = bc - gc
    h2 = rc - bc + 2.0 * deltac
    h3 = gc - rc + 4.0 * deltac
    h = torch.stack((h1, h2, h3), dim=-3) / deltac.unsqueeze(-3)
    h = torch.gather(h, dim=-3, index=argmax_rgb.unsqueeze(-3)).squeeze(-3)
    h = h / 6.0 % 1.0
    h = 2.0 * math.pi * h
    return torch.stack((h, s, v), dim=-3)


class RgbToHsv(Module):
    """Convert an image from RGB to HSV.

    The image data is assumed to be in the range of (0, 1).

    Args:
        eps: scalar to enforce numarical stability.

    Returns:
        HSV version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> hsv = RgbToHsv()
        >>> output = hsv(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def __init__(self, eps: 'float'=1e-06) ->None:
        super().__init__()
        self.eps = eps

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return rgb_to_hsv(image, self.eps)


def hsv_to_rgb(image: 'torch.Tensor') ->torch.Tensor:
    """Convert an image from HSV to RGB.

    The H channel values are assumed to be in the range 0..2pi. S and V are in the range 0..1.

    Args:
        image: HSV Image to be converted to HSV with shape of :math:`(*, 3, H, W)`.

    Returns:
        RGB version of the image with shape of :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = hsv_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    h: 'torch.Tensor' = image[..., 0, :, :] / (2 * math.pi)
    s: 'torch.Tensor' = image[..., 1, :, :]
    v: 'torch.Tensor' = image[..., 2, :, :]
    hi: 'torch.Tensor' = torch.floor(h * 6) % 6
    f: 'torch.Tensor' = h * 6 % 6 - hi
    one: 'torch.Tensor' = torch.tensor(1.0, device=image.device, dtype=image.dtype)
    p: 'torch.Tensor' = v * (one - s)
    q: 'torch.Tensor' = v * (one - f * s)
    t: 'torch.Tensor' = v * (one - (one - f) * s)
    hi = hi.long()
    indices: 'torch.Tensor' = torch.stack([hi, hi + 6, hi + 12], dim=-3)
    out = torch.stack((v, q, p, p, t, v, t, v, v, q, p, p, p, p, t, v, v, q), dim=-3)
    out = torch.gather(out, -3, indices)
    return out


class HsvToRgb(Module):
    """Convert an image from HSV to RGB.

    H channel values are assumed to be in the range 0..2pi. S and V are in the range 0..1.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = HsvToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return hsv_to_rgb(image)


def rgb_to_linear_rgb(image: 'Tensor') ->Tensor:
    """Convert an sRGB image to linear RGB. Used in colorspace conversions.

    .. image:: _static/img/rgb_to_linear_rgb.png

    Args:
        image: sRGB Image to be converted to linear RGB of shape :math:`(*,3,H,W)`.

    Returns:
        linear RGB version of the image with shape of :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_linear_rgb(input) # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W).Got {image.shape}')
    lin_rgb: 'Tensor' = torch.where(image > 0.04045, torch.pow((image + 0.055) / 1.055, 2.4), image / 12.92)
    return lin_rgb


def rgb_to_xyz(image: 'Tensor') ->Tensor:
    """Convert a RGB image to XYZ.

    .. image:: _static/img/rgb_to_xyz.png

    Args:
        image: RGB Image to be converted to XYZ with shape :math:`(*, 3, H, W)`.

    Returns:
         XYZ version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_xyz(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    r: 'Tensor' = image[..., 0, :, :]
    g: 'Tensor' = image[..., 1, :, :]
    b: 'Tensor' = image[..., 2, :, :]
    x: 'Tensor' = 0.412453 * r + 0.35758 * g + 0.180423 * b
    y: 'Tensor' = 0.212671 * r + 0.71516 * g + 0.072169 * b
    z: 'Tensor' = 0.019334 * r + 0.119193 * g + 0.950227 * b
    out: 'Tensor' = torch.stack([x, y, z], -3)
    return out


def rgb_to_lab(image: 'torch.Tensor') ->torch.Tensor:
    """Convert a RGB image to Lab.

    .. image:: _static/img/rgb_to_lab.png

    The input RGB image is assumed to be in the range of :math:`[0, 1]`. Lab
    color is computed using the D65 illuminant and Observer 2.

    Args:
        image: RGB Image to be converted to Lab with shape :math:`(*, 3, H, W)`.

    Returns:
        Lab version of the image with shape :math:`(*, 3, H, W)`.
        The L channel values are in the range 0..100. a and b are in the range -128..127.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_lab(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    lin_rgb = rgb_to_linear_rgb(image)
    xyz_im: 'torch.Tensor' = rgb_to_xyz(lin_rgb)
    xyz_ref_white = torch.tensor([0.95047, 1.0, 1.08883], device=xyz_im.device, dtype=xyz_im.dtype)[..., :, None, None]
    xyz_normalized = torch.div(xyz_im, xyz_ref_white)
    threshold = 0.008856
    power = torch.pow(xyz_normalized.clamp(min=threshold), 1 / 3.0)
    scale = 7.787 * xyz_normalized + 4.0 / 29.0
    xyz_int = torch.where(xyz_normalized > threshold, power, scale)
    x: 'torch.Tensor' = xyz_int[..., 0, :, :]
    y: 'torch.Tensor' = xyz_int[..., 1, :, :]
    z: 'torch.Tensor' = xyz_int[..., 2, :, :]
    L: 'torch.Tensor' = 116.0 * y - 16.0
    a: 'torch.Tensor' = 500.0 * (x - y)
    _b: 'torch.Tensor' = 200.0 * (y - z)
    out: 'torch.Tensor' = torch.stack([L, a, _b], dim=-3)
    return out


class RgbToLab(Module):
    """Convert an image from RGB to Lab.

    The image data is assumed to be in the range of :math:`[0, 1]`. Lab
    color is computed using the D65 illuminant and Observer 2.

    Returns:
        Lab version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> lab = RgbToLab()
        >>> output = lab(input)  # 2x3x4x5

    Reference:
        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

        [2] https://www.easyrgb.com/en/math.php

        [3] https://github.com/torch/image/blob/dc061b98fb7e946e00034a5fc73e883a299edc7f/generic/image.c#L1467
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return rgb_to_lab(image)


def linear_rgb_to_rgb(image: 'Tensor') ->Tensor:
    """Convert a linear RGB image to sRGB. Used in colorspace conversions.

    Args:
        image: linear RGB Image to be converted to sRGB of shape :math:`(*,3,H,W)`.

    Returns:
        sRGB version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = linear_rgb_to_rgb(input) # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W).Got {image.shape}')
    threshold = 0.0031308
    rgb: 'Tensor' = torch.where(image > threshold, 1.055 * torch.pow(image.clamp(min=threshold), 1 / 2.4) - 0.055, 12.92 * image)
    return rgb


def xyz_to_rgb(image: 'Tensor') ->Tensor:
    """Convert a XYZ image to RGB.

    Args:
        image: XYZ Image to be converted to RGB with shape :math:`(*, 3, H, W)`.

    Returns:
        RGB version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = xyz_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    x: 'Tensor' = image[..., 0, :, :]
    y: 'Tensor' = image[..., 1, :, :]
    z: 'Tensor' = image[..., 2, :, :]
    r: 'Tensor' = 3.2404813432005266 * x + -1.5371515162713185 * y + -0.4985363261688878 * z
    g: 'Tensor' = -0.9692549499965682 * x + 1.8759900014898907 * y + 0.0415559265582928 * z
    b: 'Tensor' = 0.0556466391351772 * x + -0.2040413383665112 * y + 1.0573110696453443 * z
    out: 'Tensor' = torch.stack([r, g, b], dim=-3)
    return out


def lab_to_rgb(image: 'torch.Tensor', clip: 'bool'=True) ->torch.Tensor:
    """Convert a Lab image to RGB.

    The L channel is assumed to be in the range of :math:`[0, 100]`.
    a and b channels are in the range of :math:`[-128, 127]`.

    Args:
        image: Lab image to be converted to RGB with shape :math:`(*, 3, H, W)`.
        clip: Whether to apply clipping to insure output RGB values in range :math:`[0, 1]`.

    Returns:
        Lab version of the image with shape :math:`(*, 3, H, W)`.
        The output RGB image are in the range of :math:`[0, 1]`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = lab_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    L: 'torch.Tensor' = image[..., 0, :, :]
    a: 'torch.Tensor' = image[..., 1, :, :]
    _b: 'torch.Tensor' = image[..., 2, :, :]
    fy = (L + 16.0) / 116.0
    fx = a / 500.0 + fy
    fz = fy - _b / 200.0
    fz = fz.clamp(min=0.0)
    fxyz = torch.stack([fx, fy, fz], dim=-3)
    power = torch.pow(fxyz, 3.0)
    scale = (fxyz - 4.0 / 29.0) / 7.787
    xyz = torch.where(fxyz > 0.2068966, power, scale)
    xyz_ref_white = torch.tensor([0.95047, 1.0, 1.08883], device=xyz.device, dtype=xyz.dtype)[..., :, None, None]
    xyz_im = xyz * xyz_ref_white
    rgbs_im: 'torch.Tensor' = xyz_to_rgb(xyz_im)
    rgb_im = linear_rgb_to_rgb(rgbs_im)
    if clip:
        rgb_im = torch.clamp(rgb_im, min=0.0, max=1.0)
    return rgb_im


class LabToRgb(Module):
    """Convert an image from Lab to RGB.

    Returns:
        RGB version of the image. Range may not be in :math:`[0, 1]`.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = LabToRgb()
        >>> output = rgb(input)  # 2x3x4x5

    References:
        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

        [2] https://www.easyrgb.com/en/math.php

        [3] https://github.com/torch/image/blob/dc061b98fb7e946e00034a5fc73e883a299edc7f/generic/image.c#L1518
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'torch.Tensor', clip: 'bool'=True) ->torch.Tensor:
        return lab_to_rgb(image, clip)


def rgb_to_luv(image: 'torch.Tensor', eps: 'float'=1e-12) ->torch.Tensor:
    """Convert a RGB image to Luv.

    .. image:: _static/img/rgb_to_luv.png

    The image data is assumed to be in the range of :math:`[0, 1]`. Luv
    color is computed using the D65 illuminant and Observer 2.

    Args:
        image: RGB Image to be converted to Luv with shape :math:`(*, 3, H, W)`.
        eps: for numerically stability when dividing.

    Returns:
        Luv version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_luv(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    lin_rgb = rgb_to_linear_rgb(image)
    xyz_im: 'torch.Tensor' = rgb_to_xyz(lin_rgb)
    x: 'torch.Tensor' = xyz_im[..., 0, :, :]
    y: 'torch.Tensor' = xyz_im[..., 1, :, :]
    z: 'torch.Tensor' = xyz_im[..., 2, :, :]
    threshold = 0.008856
    L: 'torch.Tensor' = torch.where(y > threshold, 116.0 * torch.pow(y.clamp(min=threshold), 1.0 / 3.0) - 16.0, 903.3 * y)
    xyz_ref_white: 'tuple[float, float, float]' = (0.95047, 1.0, 1.08883)
    u_w: 'float' = 4 * xyz_ref_white[0] / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])
    v_w: 'float' = 9 * xyz_ref_white[1] / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])
    u_p: 'torch.Tensor' = 4 * x / (x + 15 * y + 3 * z + eps)
    v_p: 'torch.Tensor' = 9 * y / (x + 15 * y + 3 * z + eps)
    u: 'torch.Tensor' = 13 * L * (u_p - u_w)
    v: 'torch.Tensor' = 13 * L * (v_p - v_w)
    out = torch.stack([L, u, v], dim=-3)
    return out


class RgbToLuv(Module):
    """Convert an image from RGB to Luv.

    The image data is assumed to be in the range of :math:`[0, 1]`. Luv
    color is computed using the D65 illuminant and Observer 2.

    Returns:
        Luv version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> luv = RgbToLuv()
        >>> output = luv(input)  # 2x3x4x5

    Reference:
        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

        [2] https://www.easyrgb.com/en/math.php

        [3] http://www.poynton.com/ColorFAQ.html
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return rgb_to_luv(image)


def luv_to_rgb(image: 'torch.Tensor', eps: 'float'=1e-12) ->torch.Tensor:
    """Convert a Luv image to RGB.

    Args:
        image: Luv image to be converted to RGB with shape :math:`(*, 3, H, W)`.
        eps: for numerically stability when dividing.

    Returns:
        Luv version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = luv_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    L: 'torch.Tensor' = image[..., 0, :, :]
    u: 'torch.Tensor' = image[..., 1, :, :]
    v: 'torch.Tensor' = image[..., 2, :, :]
    y: 'torch.Tensor' = torch.where(L > 7.999625, torch.pow((L + 16) / 116, 3.0), L / 903.3)
    xyz_ref_white: 'tuple[float, float, float]' = (0.95047, 1.0, 1.08883)
    u_w: 'float' = 4 * xyz_ref_white[0] / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])
    v_w: 'float' = 9 * xyz_ref_white[1] / (xyz_ref_white[0] + 15 * xyz_ref_white[1] + 3 * xyz_ref_white[2])
    a: 'torch.Tensor' = u_w + u / (13 * L + eps)
    d: 'torch.Tensor' = v_w + v / (13 * L + eps)
    c: 'torch.Tensor' = 3 * y * (5 * d - 3)
    z: 'torch.Tensor' = ((a - 4) * c - 15 * a * d * y) / (12 * d + eps)
    x: 'torch.Tensor' = -(c / (d + eps) + 3.0 * z)
    xyz_im: 'torch.Tensor' = torch.stack([x, y, z], -3)
    rgbs_im: 'torch.Tensor' = xyz_to_rgb(xyz_im)
    rgb_im = linear_rgb_to_rgb(rgbs_im)
    return rgb_im


class LuvToRgb(Module):
    """Convert an image from Luv to RGB.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = LuvToRgb()
        >>> output = rgb(input)  # 2x3x4x5

    References:
        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

        [2] https://www.easyrgb.com/en/math.php

        [3] http://www.poynton.com/ColorFAQ.html
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return luv_to_rgb(image)


class CFA(Enum):
    """Define the configuration of the color filter array.

    So far only bayer images is supported and the enum sets the pixel order for bayer. Note that this can change due
    to things like rotations and cropping of images. Take care if including the translations in pipeline.
    This implementations is optimized to be reasonably fast, look better than simple nearest neighbour.
    On top of this care is taken to make it reversible going raw -> rgb -> raw. the raw samples remain intact
    during conversion and only unknown samples are interpolated.

    The names are based on the OpenCV convention where the BG indicates pixel 1,1 (counting from 0,0) is
    blue and its neighbour to the right is green. In that case the top left pixel is red. Other options are GB, RG and
    GR

    reference:
        https://en.wikipedia.org/wiki/Color_filter_array
    """
    BG = 0
    GB = 1
    RG = 2
    GR = 3


def raw_to_rgb(image: 'torch.Tensor', cfa: 'CFA') ->torch.Tensor:
    """Convert a raw bayer image to RGB version of image.

    We are assuming a CFA with 2 green, 1 red, 1 blue. A bilinear interpolation is used for R/G and a fix convolution
    for the green pixels. To simplify calculations we expect the Height Width to be evenly divisible by 2.

    The image data is assumed to be in the range of (0, 1). Image H/W is assumed to be evenly divisible by 2.
    for simplicity reasons

    Args:
        image: raw image to be converted to RGB with shape :math:`(*,1,H,W)`.
        cfa: The configuration of the color filter.
    Returns:
        RGB version of the image with shape :math:`(*,3,H,W)`.

    Example:
        >>> rawinput = torch.randn(2, 1, 4, 6)
        >>> rgb = raw_to_rgb(rawinput, CFA.RG) # 2x3x4x6
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if image.dim() < 3 or image.size(-3) != 1:
        raise ValueError(f'Input size must have a shape of (*, 1, H, W). Got {image.shape}.')
    if len(image.shape) < 2 or image.shape[-2] % 2 == 1 or image.shape[-1] % 2 == 1:
        raise ValueError(f'Input H&W must be evenly disible by 2. Got {image.shape}')
    imagesize = image.size()
    image = image.view(-1, 1, image.shape[-2], image.shape[-1])
    if cfa == CFA.BG:
        r = image[..., :, ::2, ::2]
        b = image[..., :, 1::2, 1::2]
        rpad = 0, 1, 0, 1
        bpad = 1, 0, 1, 0
    elif cfa == CFA.GB:
        r = image[..., :, ::2, 1::2]
        b = image[..., :, 1::2, ::2]
        rpad = 1, 0, 0, 1
        bpad = 0, 1, 1, 0
    elif cfa == CFA.RG:
        r = image[..., :, 1::2, 1::2]
        b = image[..., :, ::2, ::2]
        rpad = 1, 0, 1, 0
        bpad = 0, 1, 0, 1
    elif cfa == CFA.GR:
        r = image[..., :, 1::2, ::2]
        b = image[..., :, ::2, 1::2]
        rpad = 0, 1, 1, 0
        bpad = 1, 0, 0, 1
    else:
        raise ValueError(f'Unsupported CFA Got {cfa}.')
    rpadded = torch.nn.functional.pad(r, list(rpad), 'replicate')
    bpadded = torch.nn.functional.pad(b, list(bpad), 'replicate')
    gpadded = torch.nn.functional.pad(image, [1, 1, 1, 1], 'reflect')
    r_up = torch.nn.functional.interpolate(rpadded, size=(image.shape[-2] + 1, image.shape[-1] + 1), mode='bilinear', align_corners=True)
    b_up = torch.nn.functional.interpolate(bpadded, size=(image.shape[-2] + 1, image.shape[-1] + 1), mode='bilinear', align_corners=True)
    r_up = torch.nn.functional.pad(r_up, [(-x) for x in rpad])
    b_up = torch.nn.functional.pad(b_up, [(-x) for x in bpad])
    kernel = torch.tensor([[[[0.0, 0.25, 0.0], [0.25, 0.0, 0.25], [0.0, 0.25, 0.0]]]], dtype=image.dtype, device=image.device)
    g_up = torch.nn.functional.conv2d(gpadded, kernel)
    if cfa == CFA.BG:
        g_up[:, :, ::2, 1::2] = image[:, :, ::2, 1::2]
        g_up[:, :, 1::2, ::2] = image[:, :, 1::2, ::2]
    elif cfa == CFA.GB:
        g_up[:, :, ::2, ::2] = image[:, :, ::2, ::2]
        g_up[:, :, 1::2, 1::2] = image[:, :, 1::2, 1::2]
    elif cfa == CFA.RG:
        g_up[:, :, 1::2, ::2] = image[:, :, 1::2, ::2]
        g_up[:, :, ::2, 1::2] = image[:, :, ::2, 1::2]
    elif cfa == CFA.GR:
        g_up[:, :, 1::2, 1::2] = image[:, :, 1::2, 1::2]
        g_up[:, :, ::2, ::2] = image[:, :, ::2, ::2]
    else:
        raise ValueError(f'Unsupported CFA Got {cfa}.')
    r_up = r_up.view(imagesize)
    g_up = g_up.view(imagesize)
    b_up = b_up.view(imagesize)
    rgb: 'torch.Tensor' = torch.cat([r_up, g_up, b_up], dim=-3)
    return rgb


class RawToRgb(Module):
    """Module to convert a bayer raw image to RGB version of image.

    The image data is assumed to be in the range of (0, 1).

    Shape:
        - image: :math:`(*, 1, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> rawinput = torch.rand(2, 1, 4, 6)
        >>> rgb = RawToRgb(CFA.RG)
        >>> output = rgb(rawinput)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 1, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def __init__(self, cfa: 'CFA') ->None:
        super().__init__()
        self.cfa = cfa

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return raw_to_rgb(image, cfa=self.cfa)


def rgb_to_raw(image: 'torch.Tensor', cfa: 'CFA') ->torch.Tensor:
    """Convert a RGB image to RAW version of image with the specified color filter array.

    The image data is assumed to be in the range of (0, 1).

    Args:
        image: RGB image to be converted to bayer raw with shape :math:`(*,3,H,W)`.
        cfa: Which color filter array do we want the output to mimic. I.e. which pixels are red/green/blue.

    Returns:
        raw version of the image with shape :math:`(*,1,H,W)`.

    Example:
        >>> rgbinput = torch.rand(2, 3, 4, 6)
        >>> raw = rgb_to_raw(rgbinput, CFA.BG) # 2x1x4x6
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    output: 'torch.Tensor' = image[..., 1:2, :, :].clone()
    if cfa == CFA.BG:
        output[..., :, ::2, ::2] = image[..., 0:1, ::2, ::2]
        output[..., :, 1::2, 1::2] = image[..., 2:3, 1::2, 1::2]
    elif cfa == CFA.GB:
        output[..., :, ::2, 1::2] = image[..., 0:1, ::2, 1::2]
        output[..., :, 1::2, ::2] = image[..., 2:3, 1::2, ::2]
    elif cfa == CFA.RG:
        output[..., :, 1::2, 1::2] = image[..., 0:1, 1::2, 1::2]
        output[..., :, ::2, ::2] = image[..., 2:3, ::2, ::2]
    elif cfa == CFA.GR:
        output[..., :, 1::2, ::2] = image[..., 0:1, 1::2, ::2]
        output[..., :, ::2, 1::2] = image[..., 2:3, ::2, 1::2]
    return output


class RgbToRaw(Module):
    """Module to convert a RGB image to bayer raw version of image.

    The image data is assumed to be in the range of (0, 1).

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 1, H, W)`

    reference:
        https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html

    Example:
        >>> rgbinput = torch.rand(2, 3, 4, 6)
        >>> raw = RgbToRaw(CFA.GB)
        >>> output = raw(rgbinput)  # 2x1x4x6
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 1, -1, -1]

    def __init__(self, cfa: 'CFA') ->None:
        super().__init__()
        self.cfa = cfa

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        return rgb_to_raw(image, cfa=self.cfa)


def raw_to_rgb_2x2_downscaled(image: 'Tensor', cfa: 'CFA') ->Tensor:
    """Convert the raw bayer image to RGB version of it and resize width and height by half.

    This is done efficiently by converting each superpixel of bayer image to the corresponding rgb triplet.
    R and B channels of the raw image are left as are, while two G channels of raw image are averaged to obtain the
    output G channel.

    We are assuming a CFA with 2 green, 1 red, 1 blue.
    The image data is assumed to be in the range of (0, 1). Image H/W is assumed to be evenly divisible by 2
    for simplicity reasons.

    Args:
        image: raw image to be converted to RGB and downscaled with shape :math:`(*,1,H,W)`.
        cfa: The configuration of the color filter.

    Returns:
        downscaled RGB version of the image with shape :math:`(*,3,\\frac{H}{2},\\frac{W}{2})`.

    Example:
        >>> rawinput = torch.randn(2, 1, 4, 6)
        >>> rgb = raw_to_rgb_2x2_downscaled(rawinput, CFA.RG) # 2x3x2x3
    """
    KORNIA_CHECK(isinstance(image, Tensor), 'Input type is not a torch.Tensor')
    KORNIA_CHECK_SHAPE(image, ['*', '1', 'H', 'W'])
    KORNIA_CHECK(image.shape[-2] % 2 == 0 and image.shape[-1] % 2 == 0, f'Input H&W must be evenly disible by 2. Got {image.shape}')
    if cfa == CFA.BG:
        r = image[..., :, ::2, ::2]
        b = image[..., :, 1::2, 1::2]
        g1 = image[..., :, ::2, 1::2]
        g2 = image[..., :, 1::2, ::2]
    elif cfa == CFA.GB:
        r = image[..., :, ::2, 1::2]
        b = image[..., :, 1::2, ::2]
        g1 = image[..., :, ::2, ::2]
        g2 = image[..., :, 1::2, 1::2]
    elif cfa == CFA.RG:
        r = image[..., :, 1::2, 1::2]
        b = image[..., :, ::2, ::2]
        g1 = image[..., :, 1::2, ::2]
        g2 = image[..., :, ::2, 1::2]
    elif cfa == CFA.GR:
        r = image[..., :, 1::2, ::2]
        b = image[..., :, ::2, 1::2]
        g1 = image[..., :, 1::2, 1::2]
        g2 = image[..., :, ::2, ::2]
    else:
        raise ValueError(f'Unsupported CFA Got {cfa}.')
    rgb: 'Tensor' = concatenate([r, (g1 + g2) / 2, b], dim=-3)
    return rgb


class RawToRgb2x2Downscaled(Module):
    """Module version of the :func:`raw_to_rgb_2x2_downscaled()` function.

    The image width and height have to be divisible by two. The image
    data is assumed to be in the range of (0, 1).

    Shape:
        - image: :math:`(*, 1, H, W)`
        - output: :math:`(*, 3, \\frac{H}{2}, \\frac{W}{2})`

    Example:
        >>> rawinput = torch.rand(2, 1, 4, 6)
        >>> rgb_downscale = RawToRgb2x2Downscaled(CFA.RG)
        >>> output = rgb_downscale(rawinput)  # 2x3x2x3
    """

    def __init__(self, cfa: 'CFA') ->None:
        super().__init__()
        self.cfa = cfa

    def forward(self, image: 'Tensor') ->Tensor:
        return raw_to_rgb_2x2_downscaled(image, cfa=self.cfa)


class BgrToRgb(Module):
    """Convert image from BGR to RGB.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = BgrToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return bgr_to_rgb(image)


def rgb_to_bgr(image: 'Tensor') ->Tensor:
    """Convert a RGB image to BGR.

    .. image:: _static/img/rgb_to_bgr.png

    Args:
        image: RGB Image to be converted to BGRof of shape :math:`(*,3,H,W)`.

    Returns:
        BGR version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_bgr(input) # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W).Got {image.shape}')
    return bgr_to_rgb(image)


class RgbToBgr(Module):
    """Convert an image from RGB to BGR.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        BGR version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> bgr = RgbToBgr()
        >>> output = bgr(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_bgr(image)


def rgb_to_rgba(image: 'Tensor', alpha_val: 'Union[float, Tensor]') ->Tensor:
    """Convert an image from RGB to RGBA.

    Args:
        image: RGB Image to be converted to RGBA of shape :math:`(*,3,H,W)`.
        alpha_val (float, Tensor): A float number for the alpha value or a tensor
          of shape :math:`(*,1,H,W)`.

    Returns:
        RGBA version of the image with shape :math:`(*,4,H,W)`.

    .. note:: The current functionality is NOT supported by Torchscript.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_rgba(input, 1.) # 2x4x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W).Got {image.shape}')
    if not isinstance(alpha_val, (float, Tensor)):
        raise TypeError(f'alpha_val type is not a float or Tensor. Got {type(alpha_val)}')
    r, g, b = torch.chunk(image, image.shape[-3], dim=-3)
    a: 'Tensor' = cast(Tensor, alpha_val)
    if isinstance(alpha_val, float):
        a = torch.full_like(r, fill_value=float(alpha_val))
    return torch.cat([r, g, b, a], dim=-3)


class RgbToRgba(Module):
    """Convert an image from RGB to RGBA.

    Add an alpha channel to existing RGB image.

    Args:
        alpha_val: A float number for the alpha value or a tensor
          of shape :math:`(*,1,H,W)`.

    Returns:
        Tensor: RGBA version of the image with shape :math:`(*,4,H,W)`.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 4, H, W)`

    .. note:: The current functionality is NOT supported by Torchscript.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgba = RgbToRgba(1.)
        >>> output = rgba(input)  # 2x4x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 4, -1, -1]

    def __init__(self, alpha_val: 'Union[float, Tensor]') ->None:
        super().__init__()
        self.alpha_val = alpha_val

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_rgba(image, self.alpha_val)


class BgrToRgba(Module):
    """Convert an image from BGR to RGBA.

    Add an alpha channel to existing RGB image.

    Args:
        alpha_val: A float number for the alpha value or a tensor
          of shape :math:`(*,1,H,W)`.

    Returns:
        RGBA version of the image with shape :math:`(*,4,H,W)`.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 4, H, W)`

    .. note:: The current functionality is NOT supported by Torchscript.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgba = BgrToRgba(1.)
        >>> output = rgba(input)  # 2x4x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 4, -1, -1]

    def __init__(self, alpha_val: 'Union[float, Tensor]') ->None:
        super().__init__()
        self.alpha_val = alpha_val

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_rgba(image, self.alpha_val)


def rgba_to_rgb(image: 'Tensor') ->Tensor:
    """Convert an image from RGBA to RGB.

    Args:
        image: RGBA Image to be converted to RGB of shape :math:`(*,4,H,W)`.

    Returns:
        RGB version of the image with shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 4, 4, 5)
        >>> output = rgba_to_rgb(input) # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 4:
        raise ValueError(f'Input size must have a shape of (*, 4, H, W).Got {image.shape}')
    r, g, b, a = torch.chunk(image, image.shape[-3], dim=-3)
    a_one = torch.tensor(1.0) - a
    r_new: 'Tensor' = a_one * r + a * r
    g_new: 'Tensor' = a_one * g + a * g
    b_new: 'Tensor' = a_one * b + a * b
    return torch.cat([r_new, g_new, b_new], dim=-3)


class RgbaToRgb(Module):
    """Convert an image from RGBA to RGB.

    Remove an alpha channel from RGB image.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 4, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 4, 4, 5)
        >>> rgba = RgbaToRgb()
        >>> output = rgba(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 4, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgba_to_rgb(image)


def rgba_to_bgr(image: 'Tensor') ->Tensor:
    """Convert an image from RGBA to BGR.

    Args:
        image: RGBA Image to be converted to BGR of shape :math:`(*,4,H,W)`.

    Returns:
        RGB version of the image with shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 4, 4, 5)
        >>> output = rgba_to_bgr(input) # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 4:
        raise ValueError(f'Input size must have a shape of (*, 4, H, W).Got {image.shape}')
    x_rgb: 'Tensor' = rgba_to_rgb(image)
    return rgb_to_bgr(x_rgb)


class RgbaToBgr(Module):
    """Convert an image from RGBA to BGR.

    Remove an alpha channel from BGR image.

    Returns:
        BGR version of the image.

    Shape:
        - image: :math:`(*, 4, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 4, 4, 5)
        >>> rgba = RgbaToBgr()
        >>> output = rgba(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 4, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgba_to_bgr(image)


class RgbToLinearRgb(Module):
    """Convert an image from sRGB to linear RGB.

    Reverses the gamma correction of sRGB to get linear RGB values for colorspace conversions.
    The image data is assumed to be in the range of :math:`[0, 1]`

    Returns:
        Linear RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb_lin = RgbToLinearRgb()
        >>> output = rgb_lin(input)  # 2x3x4x5

    References:
        [1] https://stackoverflow.com/questions/35952564/convert-rgb-to-srgb

        [2] https://www.cambridgeincolour.com/tutorials/gamma-correction.htm

        [3] https://en.wikipedia.org/wiki/SRGB
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_linear_rgb(image)


class LinearRgbToRgb(Module):
    """Convert a linear RGB image to sRGB.

    Applies gamma correction to linear RGB values, at the end of colorspace conversions, to get sRGB.

    Returns:
        sRGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> srgb = LinearRgbToRgb()
        >>> output = srgb(input)  # 2x3x4x5

    References:
        [1] https://stackoverflow.com/questions/35952564/convert-rgb-to-srgb

        [2] https://www.cambridgeincolour.com/tutorials/gamma-correction.htm

        [3] https://en.wikipedia.org/wiki/SRGB
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return linear_rgb_to_rgb(image)


def KORNIA_CHECK_IS_COLOR(x: 'Tensor', msg: 'Optional[str]'=None, raises: 'bool'=True) ->bool:
    """Check whether an image tensor is a color images.

    Args:
        x: image tensor to evaluate.
        msg: message to show in the exception.
        raises: bool indicating whether an exception should be raised upon failure.

    Raises:
        TypeException: if all the input tensor has not a shape :math:`(3,H,W)` and raises is True.

    Example:
        >>> img = torch.rand(2, 3, 4, 4)
        >>> KORNIA_CHECK_IS_COLOR(img, "Image is not color")
        True
    """
    if len(x.shape) < 3 or x.shape[-3] != 3:
        if raises:
            raise TypeError(f'Not a color tensor. Got: {type(x)}.\n{msg}')
        return False
    return True


def normals_to_rgb255(image: 'Tensor') ->Tensor:
    """Convert surface normals to RGB [0, 255] for visualization purposes.

    Args:
        image: surface normals to be converted to RGB with quantization of shape :math:`(*,3,H,W)`.

    Returns:
        RGB version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = normals_to_rgb255(input) # 2x3x4x5
    """
    KORNIA_CHECK_IS_COLOR(image)
    rgb255 = (0.5 * (image + 1.0)).clip(0.0, 1.0) * 255
    return rgb255


class NormalsToRgb255(Module):
    """Convert surface normals to RGB [0, 255] for visualization purposes.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = NormalsToRgb255()
        >>> output = rgb(input)  # 2x3x4x5
    """

    def forward(self, image: 'Tensor') ->Tensor:
        return normals_to_rgb255(image)


def rgb_to_rgb255(image: 'Tensor') ->Tensor:
    """Convert an image from RGB to RGB [0, 255] for visualization purposes.

    Args:
        image: RGB Image to be converted to RGB [0, 255] of shape :math:`(*,3,H,W)`.

    Returns:
        RGB version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_rgb255(input) # 2x3x4x5
    """
    KORNIA_CHECK_IS_COLOR(image)
    rgb255 = (image * 255).clip(0.0, 255.0)
    return rgb255


class RgbToRgb255(Module):
    """Convert an image from RGB to RGB [0, 255] for visualization purposes.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = RgbToRgb255()
        >>> output = rgb(input)  # 2x3x4x5
    """

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_rgb255(image)


def rgb255_to_rgb(image: 'Tensor') ->Tensor:
    """Convert an image from RGB [0, 255] to RGB for visualization purposes.

    Args:
        image: RGB Image to be converted to RGB of shape :math:`(*,3,H,W)`.

    Returns:
        RGB version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb255_to_rgb(input) # 2x3x4x5
    """
    KORNIA_CHECK_IS_COLOR(image)
    rgb = image / 255.0
    return rgb


class Rgb255ToRgb(Module):
    """Convert an image from RGB [0, 255] to RGB for visualization purposes.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = Rgb255ToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb255_to_rgb(image)


def normalize(data: 'Tensor', mean: 'Tensor', std: 'Tensor') ->Tensor:
    """Normalize an image/video tensor with mean and standard deviation.

    .. math::
        \\text{input[channel] = (input[channel] - mean[channel]) / std[channel]}

    Where `mean` is :math:`(M_1, ..., M_n)` and `std` :math:`(S_1, ..., S_n)` for `n` channels,

    Args:
        data: Image tensor of size :math:`(B, C, *)`.
        mean: Mean for each channel.
        std: Standard deviations for each channel.

    Return:
        Normalised tensor with same size as input :math:`(B, C, *)`.

    Examples:
        >>> x = torch.rand(1, 4, 3, 3)
        >>> out = normalize(x, torch.tensor([0.0]), torch.tensor([255.]))
        >>> out.shape
        torch.Size([1, 4, 3, 3])

        >>> x = torch.rand(1, 4, 3, 3)
        >>> mean = torch.zeros(4)
        >>> std = 255. * torch.ones(4)
        >>> out = normalize(x, mean, std)
        >>> out.shape
        torch.Size([1, 4, 3, 3])
    """
    shape = data.shape
    if torch.onnx.is_in_onnx_export():
        if not isinstance(mean, Tensor) or not isinstance(std, Tensor):
            raise ValueError('Only tensor is accepted when converting to ONNX.')
        if mean.shape[0] != 1 or std.shape[0] != 1:
            raise ValueError(f'Batch dimension must be one for broadcasting when converting to ONNX.Try changing mean shape and std shape from ({mean.shape}, {std.shape}) to (1, C) or (1, C, 1, 1).')
    else:
        if isinstance(mean, float):
            mean = torch.tensor([mean] * shape[1], device=data.device, dtype=data.dtype)
        if isinstance(std, float):
            std = torch.tensor([std] * shape[1], device=data.device, dtype=data.dtype)
        if mean.shape and mean.shape[0] != 1:
            if mean.shape[0] != data.shape[1] and mean.shape[:2] != data.shape[:2]:
                raise ValueError(f'mean length and number of channels do not match. Got {mean.shape} and {data.shape}.')
        if std.shape and std.shape[0] != 1:
            if std.shape[0] != data.shape[1] and std.shape[:2] != data.shape[:2]:
                raise ValueError(f'std length and number of channels do not match. Got {std.shape} and {data.shape}.')
        mean = torch.as_tensor(mean, device=data.device, dtype=data.dtype)
        std = torch.as_tensor(std, device=data.device, dtype=data.dtype)
    mean = mean[..., None]
    std = std[..., None]
    out: 'Tensor' = (data.view(shape[0], shape[1], -1) - mean) / std
    return out.view(shape)


def rgb255_to_normals(image: 'Tensor') ->Tensor:
    """Convert an image from RGB [0, 255] to surface normals for visualization purposes.

    Args:
        image: RGB Image to be converted to surface normals of shape :math:`(*,3,H,W)`.

    Returns:
        surface normals version of the image with shape of shape :math:`(*,3,H,W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb255_to_normals(input) # 2x3x4x5
    """
    KORNIA_CHECK_IS_COLOR(image)
    normals = normalize(image / 255.0 * 2.0 - 1.0, dim=-3, p=2.0)
    return normals


class Rgb255ToNormals(Module):
    """Convert an image from RGB [0, 255] to surface normals for visualization purposes.

    Returns:
        surface normals version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> normals = Rgb255ToNormals()
        >>> output = normals(input)  # 2x3x4x5
    """

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb255_to_normals(image)


def sepia_from_rgb(input: 'Tensor', rescale: 'bool'=True, eps: 'float'=1e-06) ->Tensor:
    """Apply to a tensor the sepia filter.

    Args:
        input: the input tensor with shape of :math:`(*, C, H, W)`.
        rescale: If True, the output tensor will be rescaled (max values be 1. or 255).
        eps: scalar to enforce numerical stability.

    Returns:
        Tensor: The sepia tensor of same size and numbers of channels
        as the input with shape :math:`(*, C, H, W)`.

    Example:
        >>> input = torch.ones(3, 1, 1)
        >>> sepia_from_rgb(input, rescale=False)
        tensor([[[1.3510]],
        <BLANKLINE>
                [[1.2030]],
        <BLANKLINE>
                [[0.9370]]])
    """
    if len(input.shape) < 3 or input.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {input.shape}')
    r = input[..., 0, :, :]
    g = input[..., 1, :, :]
    b = input[..., 2, :, :]
    r_out = 0.393 * r + 0.769 * g + 0.189 * b
    g_out = 0.349 * r + 0.686 * g + 0.168 * b
    b_out = 0.272 * r + 0.534 * g + 0.131 * b
    sepia_out = torch.stack([r_out, g_out, b_out], dim=-3)
    if rescale:
        max_values = sepia_out.amax(dim=-1).amax(dim=-1)
        sepia_out = sepia_out / (max_values[..., None, None] + eps)
    return sepia_out


class Sepia(Module):
    """Module that apply the sepia filter to tensors.

    Args:
        input: the input tensor with shape of :math:`(*, C, H, W)`.
        rescale: If True, the output tensor will be rescaled (max values be 1. or 255).
        eps: scalar to enforce numerical stability.

    Returns:
        Tensor: The sepia tensor of same size and numbers of channels
        as the input with shape :math:`(*, C, H, W)`.

    Example:
        >>>
        >>> input = torch.ones(3, 1, 1)
        >>> Sepia(rescale=False)(input)
        tensor([[[1.3510]],
        <BLANKLINE>
                [[1.2030]],
        <BLANKLINE>
                [[0.9370]]])
    """

    def __init__(self, rescale: 'bool'=True, eps: 'float'=1e-06) ->None:
        self.rescale = rescale
        self.eps = eps
        super().__init__()

    def __repr__(self) ->str:
        return self.__class__.__name__ + f'(rescale={self.rescale}, eps={self.eps})'

    def forward(self, input: 'Tensor') ->Tensor:
        return sepia_from_rgb(input, rescale=self.rescale, eps=self.eps)


class RgbToXyz(Module):
    """Convert an image from RGB to XYZ.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        XYZ version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> xyz = RgbToXyz()
        >>> output = xyz(input)  # 2x3x4x5

    Reference:
        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_xyz(image)


class XyzToRgb(Module):
    """Converts an image from XYZ to RGB.

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = XyzToRgb()
        >>> output = rgb(input)  # 2x3x4x5

    Reference:
        [1] https://docs.opencv.org/4.0.1/de/d25/imgproc_color_conversions.html
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return xyz_to_rgb(image)


def _rgb_to_y(r: 'Tensor', g: 'Tensor', b: 'Tensor') ->Tensor:
    y: 'Tensor' = 0.299 * r + 0.587 * g + 0.114 * b
    return y


def rgb_to_ycbcr(image: 'Tensor') ->Tensor:
    """Convert an RGB image to YCbCr.

    .. image:: _static/img/rgb_to_ycbcr.png

    Args:
        image: RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.

    Returns:
        YCbCr version of the image with shape :math:`(*, 3, H, W)`.

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    r: 'Tensor' = image[..., 0, :, :]
    g: 'Tensor' = image[..., 1, :, :]
    b: 'Tensor' = image[..., 2, :, :]
    delta: 'float' = 0.5
    y: 'Tensor' = _rgb_to_y(r, g, b)
    cb: 'Tensor' = (b - y) * 0.564 + delta
    cr: 'Tensor' = (r - y) * 0.713 + delta
    return torch.stack([y, cb, cr], -3)


class RgbToYcbcr(Module):
    """Convert an image from RGB to YCbCr.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        YCbCr version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> ycbcr = RgbToYcbcr()
        >>> output = ycbcr(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return rgb_to_ycbcr(image)


def ycbcr_to_rgb(image: 'Tensor') ->Tensor:
    """Convert an YCbCr image to RGB.

    The image data is assumed to be in the range of (0, 1).

    Args:
        image: YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.

    Returns:
        RGB version of the image with shape :math:`(*, 3, H, W)`.

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    y: 'Tensor' = image[..., 0, :, :]
    cb: 'Tensor' = image[..., 1, :, :]
    cr: 'Tensor' = image[..., 2, :, :]
    delta: 'float' = 0.5
    cb_shifted: 'Tensor' = cb - delta
    cr_shifted: 'Tensor' = cr - delta
    r: 'Tensor' = y + 1.403 * cr_shifted
    g: 'Tensor' = y - 0.714 * cr_shifted - 0.344 * cb_shifted
    b: 'Tensor' = y + 1.773 * cb_shifted
    return torch.stack([r, g, b], -3).clamp(0, 1)


class YcbcrToRgb(Module):
    """Convert an image from YCbCr to Rgb.

    The image data is assumed to be in the range of (0, 1).

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = YcbcrToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, image: 'Tensor') ->Tensor:
        return ycbcr_to_rgb(image)


def rgb_to_yuv(image: 'Tensor') ->Tensor:
    """Convert an RGB image to YUV.

    .. image:: _static/img/rgb_to_yuv.png

    The image data is assumed to be in the range of :math:`(0, 1)`. The range of the output is of
    :math:`(0, 1)` to luma and the ranges of U and V are :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`,
    respectively.

    The YUV model adopted here follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Args:
        image: RGB Image to be converted to YUV with shape :math:`(*, 3, H, W)`.

    Returns:
        YUV version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_yuv(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    r: 'Tensor' = image[..., 0, :, :]
    g: 'Tensor' = image[..., 1, :, :]
    b: 'Tensor' = image[..., 2, :, :]
    y: 'Tensor' = 0.299 * r + 0.587 * g + 0.114 * b
    u: 'Tensor' = -0.147 * r - 0.289 * g + 0.436 * b
    v: 'Tensor' = 0.615 * r - 0.515 * g - 0.1 * b
    out: 'Tensor' = torch.stack([y, u, v], -3)
    return out


class RgbToYuv(Module):
    """Convert an image from RGB to YUV.

    The image data is assumed to be in the range of :math:`(0, 1)`.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Returns:
        YUV version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> yuv = RgbToYuv()
        >>> output = yuv(input)  # 2x3x4x5

    Reference::
        [1] https://es.wikipedia.org/wiki/YUV#RGB_a_Y'UV
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, input: 'Tensor') ->Tensor:
        return rgb_to_yuv(input)


def rgb_to_yuv420(image: 'Tensor') ->tuple[Tensor, Tensor]:
    """Convert an RGB image to YUV 420 (subsampled).

    Input need to be padded to be evenly divisible by 2 horizontal and vertical.

    The image data is assumed to be in the range of :math:`(0, 1)`. The range of the output is of :math:`(0, 1)` to
    luma and the ranges of U and V are :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    The YUV model adopted here follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Args:
        image: RGB Image to be converted to YUV with shape :math:`(*, 3, H, W)`.

    Returns:
        A Tensor containing the Y plane with shape :math:`(*, 1, H, W)`
        A Tensor containing the UV planes with shape :math:`(*, 2, H/2, W/2)`

    Example:
        >>> input = torch.rand(2, 3, 4, 6)
        >>> output = rgb_to_yuv420(input)  # (2x1x4x6, 2x2x2x3)
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    if len(image.shape) < 2 or image.shape[-2] % 2 == 1 or image.shape[-1] % 2 == 1:
        raise ValueError(f'Input H&W must be evenly disible by 2. Got {image.shape}')
    yuvimage = rgb_to_yuv(image)
    return yuvimage[..., :1, :, :], yuvimage[..., 1:3, :, :].unfold(-2, 2, 2).unfold(-2, 2, 2).mean((-1, -2))


class RgbToYuv420(Module):
    """Convert an image from RGB to YUV420.

    Width and Height evenly divisible by 2.

    The image data is assumed to be in the range of :math:`(0, 1)`.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Returns:
        YUV420 version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 1, H, W)` and :math:`(*, 2, H/2, W/2)`

    Examples:
        >>> yuvinput = torch.rand(2, 3, 4, 6)
        >>> yuv = RgbToYuv420()
        >>> output = yuv(yuvinput)  # # (2x1x4x6, 2x1x2x3)

    Reference::
        [1] https://es.wikipedia.org/wiki/YUV#RGB_a_Y'UV
    """
    ONNX_EXPORTABLE = False

    def forward(self, yuvinput: 'Tensor') ->tuple[Tensor, Tensor]:
        return rgb_to_yuv420(yuvinput)


def rgb_to_yuv422(image: 'Tensor') ->tuple[Tensor, Tensor]:
    """Convert an RGB image to YUV 422 (subsampled).

    Input need to be padded to be evenly divisible by 2 vertical.

    The image data is assumed to be in the range of :math:`(0, 1)`. The range of the output is of
    :math:`(0, 1)` to luma and the ranges of U and V are :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`,
    respectively.

    The YUV model adopted here follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Args:
        image: RGB Image to be converted to YUV with shape :math:`(*, 3, H, W)`.

    Returns:
       A Tensor containing the Y plane with shape :math:`(*, 1, H, W)`
       A Tensor containing the UV planes with shape :math:`(*, 2, H, W/2)`

    Example:
        >>> input = torch.rand(2, 3, 4, 6)
        >>> output = rgb_to_yuv420(input)  # (2x1x4x6, 2x1x4x3)
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    if len(image.shape) < 2 or image.shape[-2] % 2 == 1 or image.shape[-1] % 2 == 1:
        raise ValueError(f'Input H&W must be evenly disible by 2. Got {image.shape}')
    yuvimage = rgb_to_yuv(image)
    return yuvimage[..., :1, :, :], yuvimage[..., 1:3, :, :].unfold(-1, 2, 2).mean(-1)


class RgbToYuv422(Module):
    """Convert an image from RGB to YUV422.

    Width must be evenly disvisible by 2.

    The image data is assumed to be in the range of :math:`(0, 1)`.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Returns:
        YUV422 version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 1, H, W)` and :math:`(*, 2, H, W/2)`

    Examples:
        >>> yuvinput = torch.rand(2, 3, 4, 6)
        >>> yuv = RgbToYuv422()
        >>> output = yuv(yuvinput)  # # (2x1x4x6, 2x2x4x3)

    Reference::
        [1] https://es.wikipedia.org/wiki/YUV#RGB_a_Y'UV
    """
    ONNX_EXPORTABLE = False

    def forward(self, yuvinput: 'Tensor') ->tuple[Tensor, Tensor]:
        return rgb_to_yuv422(yuvinput)


def yuv_to_rgb(image: 'Tensor') ->Tensor:
    """Convert an YUV image to RGB.

    The image data is assumed to be in the range of :math:`(0, 1)` for luma (Y). The ranges of U and V are
    :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Args:
        image: YUV Image to be converted to RGB with shape :math:`(*, 3, H, W)`.

    Returns:
        RGB version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = yuv_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(image)}')
    if image.dim() < 3 or image.shape[-3] != 3:
        raise ValueError(f'Input size must have a shape of (*, 3, H, W). Got {image.shape}')
    y: 'Tensor' = image[..., 0, :, :]
    u: 'Tensor' = image[..., 1, :, :]
    v: 'Tensor' = image[..., 2, :, :]
    r: 'Tensor' = y + 1.14 * v
    g: 'Tensor' = y + -0.396 * u - 0.581 * v
    b: 'Tensor' = y + 2.029 * u
    out: 'Tensor' = torch.stack([r, g, b], -3)
    return out


class YuvToRgb(Module):
    """Convert an image from YUV to RGB.

    The image data is assumed to be in the range of :math:`(0, 1)` for luma (Y). The ranges of U and V are
    :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Returns:
        RGB version of the image.

    Shape:
        - image: :math:`(*, 3, H, W)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> rgb = YuvToRgb()
        >>> output = rgb(input)  # 2x3x4x5
    """
    ONNX_DEFAULT_INPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]
    ONNX_DEFAULT_OUTPUTSHAPE: 'ClassVar[list[int]]' = [-1, 3, -1, -1]

    def forward(self, input: 'Tensor') ->Tensor:
        return yuv_to_rgb(input)


def yuv420_to_rgb(imagey: 'Tensor', imageuv: 'Tensor') ->Tensor:
    """Convert an YUV420 image to RGB.

    Input need to be padded to be evenly divisible by 2 horizontal and vertical.

    The image data is assumed to be in the range of :math:`(0, 1)` for luma (Y). The ranges of U and V are
    :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Args:
        imagey: Y (luma) Image plane to be converted to RGB with shape :math:`(*, 1, H, W)`.
        imageuv: UV (chroma) Image planes to be converted to RGB with shape :math:`(*, 2, H/2, W/2)`.

    Returns:
        RGB version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> inputy = torch.rand(2, 1, 4, 6)
        >>> inputuv = torch.rand(2, 2, 2, 3)
        >>> output = yuv420_to_rgb(inputy, inputuv)  # 2x3x4x6
    """
    if not isinstance(imagey, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(imagey)}')
    if not isinstance(imageuv, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(imageuv)}')
    if len(imagey.shape) < 3 or imagey.shape[-3] != 1:
        raise ValueError(f'Input imagey size must have a shape of (*, 1, H, W). Got {imagey.shape}')
    if len(imageuv.shape) < 3 or imageuv.shape[-3] != 2:
        raise ValueError(f'Input imageuv size must have a shape of (*, 2, H/2, W/2). Got {imageuv.shape}')
    if len(imagey.shape) < 2 or imagey.shape[-2] % 2 == 1 or imagey.shape[-1] % 2 == 1:
        raise ValueError(f'Input H&W must be evenly disible by 2. Got {imagey.shape}')
    if len(imageuv.shape) < 2 or len(imagey.shape) < 2 or imagey.shape[-2] / imageuv.shape[-2] != 2 or imagey.shape[-1] / imageuv.shape[-1] != 2:
        raise ValueError(f'Input imageuv H&W must be half the size of the luma plane. Got {imagey.shape} and {imageuv.shape}')
    yuv444image = torch.cat([imagey, imageuv.repeat_interleave(2, dim=-1).repeat_interleave(2, dim=-2)], dim=-3)
    return yuv_to_rgb(yuv444image)


class Yuv420ToRgb(Module):
    """Convert an image from YUV to RGB.

    Width and Height must be evenly divisible by 2.

    The image data is assumed to be in the range of :math:`(0, 1)` for luma (Y). The ranges of U and V are
    :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Returns:
        RGB version of the image.

    Shape:
        - imagey: :math:`(*, 1, H, W)`
        - imageuv: :math:`(*, 2, H/2, W/2)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> inputy = torch.rand(2, 1, 4, 6)
        >>> inputuv = torch.rand(2, 2, 2, 3)
        >>> rgb = Yuv420ToRgb()
        >>> output = rgb(inputy, inputuv)  # 2x3x4x6
    """
    ONNX_EXPORTABLE = False

    def forward(self, inputy: 'Tensor', inputuv: 'Tensor') ->Tensor:
        return yuv420_to_rgb(inputy, inputuv)


def yuv422_to_rgb(imagey: 'Tensor', imageuv: 'Tensor') ->Tensor:
    """Convert an YUV422 image to RGB.

    Input need to be padded to be evenly divisible by 2 vertical.

    The image data is assumed to be in the range of :math:`(0, 1)` for luma (Y). The ranges of U and V are
    :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Args:
        imagey: Y (luma) Image plane to be converted to RGB with shape :math:`(*, 1, H, W)`.
        imageuv: UV (luma) Image planes to be converted to RGB with shape :math:`(*, 2, H, W/2)`.

    Returns:
        RGB version of the image with shape :math:`(*, 3, H, W)`.

    Example:
        >>> inputy = torch.rand(2, 1, 4, 6)
        >>> inputuv = torch.rand(2, 2, 2, 3)
        >>> output = yuv420_to_rgb(inputy, inputuv)  # 2x3x4x5
    """
    if not isinstance(imagey, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(imagey)}')
    if not isinstance(imageuv, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(imageuv)}')
    if len(imagey.shape) < 3 or imagey.shape[-3] != 1:
        raise ValueError(f'Input imagey size must have a shape of (*, 1, H, W). Got {imagey.shape}')
    if len(imageuv.shape) < 3 or imageuv.shape[-3] != 2:
        raise ValueError(f'Input imageuv size must have a shape of (*, 2, H, W/2). Got {imageuv.shape}')
    if len(imagey.shape) < 2 or imagey.shape[-2] % 2 == 1 or imagey.shape[-1] % 2 == 1:
        raise ValueError(f'Input H&W must be evenly disible by 2. Got {imagey.shape}')
    if len(imageuv.shape) < 2 or len(imagey.shape) < 2 or imagey.shape[-1] / imageuv.shape[-1] != 2:
        raise ValueError(f'Input imageuv W must be half the size of the luma plane. Got {imagey.shape} and {imageuv.shape}')
    yuv444image = torch.cat([imagey, imageuv.repeat_interleave(2, dim=-1)], dim=-3)
    return yuv_to_rgb(yuv444image)


class Yuv422ToRgb(Module):
    """Convert an image from YUV to RGB.

    Width must be evenly divisible by 2.

    The image data is assumed to be in the range of :math:`(0, 1)` for luma (Y). The ranges of U and V are
    :math:`(-0.436, 0.436)` and :math:`(-0.615, 0.615)`, respectively.

    YUV formula follows M/PAL values (see
    `BT.470-5 <https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.470-5-199802-S!!PDF-E.pdf>`_, Table 2,
    items 2.5 and 2.6).

    Returns:
        RGB version of the image.

    Shape:
        - imagey: :math:`(*, 1, H, W)`
        - imageuv: :math:`(*, 2, H, W/2)`
        - output: :math:`(*, 3, H, W)`

    Examples:
        >>> inputy = torch.rand(2, 1, 4, 6)
        >>> inputuv = torch.rand(2, 2, 4, 3)
        >>> rgb = Yuv422ToRgb()
        >>> output = rgb(inputy, inputuv)  # 2x3x4x6
    """
    ONNX_EXPORTABLE = False

    def forward(self, inputy: 'Tensor', inputuv: 'Tensor') ->Tensor:
        return yuv422_to_rgb(inputy, inputuv)


class ClassificationHead(nn.Module):
    """Module to be used as a classification head.

    Args:
        embed_size: the logits tensor coming from the networks.
        num_classes: an integer representing the numbers of classes to classify.

    Example:
        >>> feat = torch.rand(1, 256, 256)
        >>> head = ClassificationHead(256, 10)
        >>> head(feat).shape
        torch.Size([1, 10])
    """

    def __init__(self, embed_size: 'int'=768, num_classes: 'int'=10) ->None:
        super().__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.linear = nn.Linear(embed_size, num_classes)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        out = x.mean(-2)
        return self.linear(self.norm(out))


def create_meshgrid(height: 'int', width: 'int', normalized_coordinates: 'bool'=True, device: 'Optional[torch.device]'=None, dtype: 'Optional[torch.dtype]'=None) ->Tensor:
    """Generate a coordinate grid for an image.

    When the flag ``normalized_coordinates`` is set to True, the grid is
    normalized to be in the range :math:`[-1,1]` to be consistent with the pytorch
    function :py:func:`torch.nn.functional.grid_sample`.

    Args:
        height: the image height (rows).
        width: the image width (cols).
        normalized_coordinates: whether to normalize
          coordinates in the range :math:`[-1,1]` in order to be consistent with the
          PyTorch function :py:func:`torch.nn.functional.grid_sample`.
        device: the device on which the grid will be generated.
        dtype: the data type of the generated grid.

    Return:
        grid tensor with shape :math:`(1, H, W, 2)`.

    Example:
        >>> create_meshgrid(2, 2)
        tensor([[[[-1., -1.],
                  [ 1., -1.]],
        <BLANKLINE>
                 [[-1.,  1.],
                  [ 1.,  1.]]]])

        >>> create_meshgrid(2, 2, normalized_coordinates=False)
        tensor([[[[0., 0.],
                  [1., 0.]],
        <BLANKLINE>
                 [[0., 1.],
                  [1., 1.]]]])
    """
    xs: 'Tensor' = torch.linspace(0, width - 1, width, device=device, dtype=dtype)
    ys: 'Tensor' = torch.linspace(0, height - 1, height, device=device, dtype=dtype)
    if normalized_coordinates:
        xs = (xs / (width - 1) - 0.5) * 2
        ys = (ys / (height - 1) - 0.5) * 2
    base_grid: 'Tensor' = stack(torch_meshgrid([xs, ys], indexing='ij'), dim=-1)
    return base_grid.permute(1, 0, 2).unsqueeze(0)


_VALID_BEHAVIOUR = {'conv', 'corr'}


_VALID_BORDERS = {'constant', 'reflect', 'replicate', 'circular'}


_VALID_PADDING = {'valid', 'same'}


def normalize_kernel2d(input: 'Tensor') ->Tensor:
    """Normalize both derivative and smoothing kernel."""
    KORNIA_CHECK_SHAPE(input, ['*', 'H', 'W'])
    norm = input.abs().sum(dim=-1).sum(dim=-1)
    return input / norm[..., None, None]


def filter2d(input: 'Tensor', kernel: 'Tensor', border_type: 'str'='reflect', normalized: 'bool'=False, padding: 'str'='same', behaviour: 'str'='corr') ->Tensor:
    """Convolve a tensor with a 2d kernel.

    The function applies a given kernel to a tensor. The kernel is applied
    independently at each depth channel of the tensor. Before applying the
    kernel, the function applies padding according to the specified mode so
    that the output remains in the same shape.

    Args:
        input: the input tensor with shape of
          :math:`(B, C, H, W)`.
        kernel: the kernel to be convolved with the input
          tensor. The kernel shape must be :math:`(1, kH, kW)` or :math:`(B, kH, kW)`.
        border_type: the padding mode to be applied before convolving.
          The expected modes are: ``'constant'``, ``'reflect'``,
          ``'replicate'`` or ``'circular'``.
        normalized: If True, kernel will be L1 normalized.
        padding: This defines the type of padding.
          2 modes available ``'same'`` or ``'valid'``.
        behaviour: defines the convolution mode -- correlation (default), using pytorch conv2d,
        or true convolution (kernel is flipped). 2 modes available ``'corr'`` or ``'conv'``.


    Return:
        Tensor: the convolved tensor of same size and numbers of channels
        as the input with shape :math:`(B, C, H, W)`.

    Example:
        >>> input = torch.tensor([[[
        ...    [0., 0., 0., 0., 0.],
        ...    [0., 0., 0., 0., 0.],
        ...    [0., 0., 5., 0., 0.],
        ...    [0., 0., 0., 0., 0.],
        ...    [0., 0., 0., 0., 0.],]]])
        >>> kernel = torch.ones(1, 3, 3)
        >>> filter2d(input, kernel, padding='same')
        tensor([[[[0., 0., 0., 0., 0.],
                  [0., 5., 5., 5., 0.],
                  [0., 5., 5., 5., 0.],
                  [0., 5., 5., 5., 0.],
                  [0., 0., 0., 0., 0.]]]])
    """
    KORNIA_CHECK_IS_TENSOR(input)
    KORNIA_CHECK_SHAPE(input, ['B', 'C', 'H', 'W'])
    KORNIA_CHECK_IS_TENSOR(kernel)
    KORNIA_CHECK_SHAPE(kernel, ['B', 'H', 'W'])
    KORNIA_CHECK(str(border_type).lower() in _VALID_BORDERS, f'Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}')
    KORNIA_CHECK(str(padding).lower() in _VALID_PADDING, f'Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}')
    KORNIA_CHECK(str(behaviour).lower() in _VALID_BEHAVIOUR, f'Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}')
    b, c, h, w = input.shape
    if str(behaviour).lower() == 'conv':
        tmp_kernel = kernel.flip((-2, -1))[:, None, ...]
    else:
        tmp_kernel = kernel[:, None, ...]
    if normalized:
        tmp_kernel = normalize_kernel2d(tmp_kernel)
    tmp_kernel = tmp_kernel.expand(-1, c, -1, -1)
    height, width = tmp_kernel.shape[-2:]
    if padding == 'same':
        padding_shape: 'list[int]' = _compute_padding([height, width])
        input = pad(input, padding_shape, mode=border_type)
    tmp_kernel = tmp_kernel.reshape(-1, 1, height, width)
    input = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))
    output = F.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)
    if padding == 'same':
        out = output.view(b, c, h, w)
    else:
        out = output.view(b, c, h - height + 1, w - width + 1)
    return out


def distance_transform(image: 'torch.Tensor', kernel_size: 'int'=3, h: 'float'=0.35) ->torch.Tensor:
    """Approximates the Manhattan distance transform of images using cascaded convolution operations.

    The value at each pixel in the output represents the distance to the nearest non-zero pixel in the image image.
    It uses the method described in :cite:`pham2021dtlayer`.
    The transformation is applied independently across the channel dimension of the images.

    Args:
        image: Image with shape :math:`(B,C,H,W)`.
        kernel_size: size of the convolution kernel.
        h: value that influence the approximation of the min function.

    Returns:
        tensor with shape :math:`(B,C,H,W)`.

    Example:
        >>> tensor = torch.zeros(1, 1, 5, 5)
        >>> tensor[:,:, 1, 2] = 1
        >>> dt = kornia.contrib.distance_transform(tensor)
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError(f'image type is not a torch.Tensor. Got {type(image)}')
    if not len(image.shape) == 4:
        raise ValueError(f'Invalid image shape, we expect BxCxHxW. Got: {image.shape}')
    if kernel_size % 2 == 0:
        raise ValueError('Kernel size must be an odd number.')
    n_iters: 'int' = math.ceil(max(image.shape[2], image.shape[3]) / math.floor(kernel_size / 2))
    grid = create_meshgrid(kernel_size, kernel_size, normalized_coordinates=False, device=image.device, dtype=image.dtype)
    grid -= math.floor(kernel_size / 2)
    kernel = torch.hypot(grid[0, :, :, 0], grid[0, :, :, 1])
    kernel = torch.exp(kernel / -h).unsqueeze(0)
    out = torch.zeros_like(image)
    boundary = image.clone()
    signal_ones = torch.ones_like(boundary)
    for i in range(n_iters):
        cdt = filter2d(boundary, kernel, border_type='replicate')
        cdt = -h * torch.log(cdt)
        cdt = torch.nan_to_num(cdt, posinf=0.0)
        mask = torch.where(cdt > 0, 1.0, 0.0)
        if mask.sum() == 0:
            break
        offset: 'int' = i * kernel_size // 2
        out += (offset + cdt) * mask
        boundary = torch.where(mask == 1, signal_ones, boundary)
    return out


class DistanceTransform(nn.Module):
    """Module that approximates the Manhattan (city block) distance transform of images using convolutions.

    Args:
        kernel_size: size of the convolution kernel.
        h: value that influence the approximation of the min function.
    """

    def __init__(self, kernel_size: 'int'=3, h: 'float'=0.35) ->None:
        super().__init__()
        self.kernel_size = kernel_size
        self.h = h

    def forward(self, image: 'torch.Tensor') ->torch.Tensor:
        if image.shape[1] > 1:
            image_in = image.view(-1, 1, image.shape[-2], image.shape[-1])
        else:
            image_in = image
        return distance_transform(image_in, self.kernel_size, self.h).view_as(image)


class ModelBaseMixin:
    name: 'str' = 'model'

    def _tensor_to_type(self, output: 'Union[Tensor, List[Tensor]]', output_type: 'str', is_batch: 'bool'=False) ->Union[Tensor, List[Tensor], List['Image.Image']]:
        """Converts the output tensor to the desired type.

        Args:
            output: The output tensor or list of tensors.
            output_type: The desired output type. Accepted values are "torch" and "pil".
            is_batch: If True, the output is expected to be a batch of tensors.

        Returns:
            The converted output tensor or list of tensors.

        Raises:
            RuntimeError: If the output type is not supported.
        """
        if output_type == 'torch':
            if is_batch and not isinstance(output, Tensor):
                return stack(output)
            elif is_batch and isinstance(output, Tensor):
                return output
            elif not is_batch and isinstance(output, Tensor):
                return list(output)
            elif not is_batch and not isinstance(output, Tensor):
                return output
            return output
        elif output_type == 'pil':
            out = [Image.fromarray((tensor_to_image(out_img) * 255).astype(np.uint8)) for out_img in output]
            return list(out)
        raise RuntimeError(f'Unsupported output type `{output_type}`.')

    def _save_outputs(self, outputs: 'Union[Tensor, List[Tensor]]', directory: 'Optional[str]'=None, suffix: 'str'='') ->None:
        """Save the output image(s) to a directory.

        Args:
            outputs: output tensor.
            directory: directory to save the images.
        """
        if directory is None:
            name = f"{self.name}_{datetime.datetime.now(tz=datetime.timezone.utc).strftime('%Y%m%d%H%M%S')!s}"
            directory = os.path.join('kornia_outputs', name)
        os.makedirs(directory, exist_ok=True)
        for i, out_image in enumerate(outputs):
            write_image(os.path.join(directory, f'{str(i).zfill(6)}{suffix}.jpg'), out_image.mul(255.0).byte())
        logger.info(f'Outputs are saved in {directory}')


class ExtractTensorPatches(Module):
    """Module that extract patches from tensors and stack them.

    In the simplest case, the output value of the operator with input size
    :math:`(B, C, H, W)` is :math:`(B, N, C, H_{out}, W_{out})`.

    where
      - :math:`B` is the batch size.
      - :math:`N` denotes the total number of extracted patches stacked in
      - :math:`C` denotes the number of input channels.
      - :math:`H`, :math:`W` the input height and width of the input in pixels.
      - :math:`H_{out}`, :math:`W_{out}` denote to denote to the patch size
        defined in the function signature.
        left-right and top-bottom order.

    * :attr:`window_size` is the size of the sliding window and controls the
      shape of the output tensor and defines the shape of the output patch.
    * :attr:`stride` controls the stride to apply to the sliding window and
      regulates the overlapping between the extracted patches.
    * :attr:`padding` controls the amount of implicit zeros-paddings on both
      sizes at each dimension.
    * :attr:`allow_auto_padding` allows automatic calculation of the padding required
      to fit the window and stride into the image.

    The parameters :attr:`window_size`, :attr:`stride` and :attr:`padding` can
    be either:

        - a single ``int`` -- in which case the same value is used for the
          height and width dimension.
        - a ``tuple`` of two ints -- in which case, the first `int` is used for
          the height dimension, and the second `int` for the width dimension.

    :attr:`padding` can also be a ``tuple`` of four ints -- in which case, the
    first two ints are for the height dimension while the last two ints are for
    the width dimension.

    Args:
        input: tensor image where to extract the patches with shape :math:`(B, C, H, W)`.
        window_size: the size of the sliding window and the output patch size.
        stride: stride of the sliding window.
        padding: Zero-padding added to both side of the input.
        allow_auto_adding: whether to allow automatic padding if the window and stride do not fit into the image.

    Shape:
        - Input: :math:`(B, C, H, W)`
        - Output: :math:`(B, N, C, H_{out}, W_{out})`

    Returns:
        the tensor with the extracted patches.

    Examples:
        >>> input = torch.arange(9.).view(1, 1, 3, 3)
        >>> patches = extract_tensor_patches(input, (2, 3))
        >>> input
        tensor([[[[0., 1., 2.],
                  [3., 4., 5.],
                  [6., 7., 8.]]]])
        >>> patches[:, -1]
        tensor([[[[3., 4., 5.],
                  [6., 7., 8.]]]])
    """

    def __init__(self, window_size: 'Union[int, Tuple[int, int]]', stride: 'Union[int, Tuple[int, int]]'=1, padding: 'PadType'=0, allow_auto_padding: 'bool'=False) ->None:
        super().__init__()
        self.window_size: 'Union[int, Tuple[int, int]]' = window_size
        self.stride: 'Union[int, Tuple[int, int]]' = stride
        self.padding: 'PadType' = padding
        self.allow_auto_padding: 'bool' = allow_auto_padding

    def forward(self, input: 'Tensor') ->Tensor:
        return extract_tensor_patches(input, self.window_size, stride=self.stride, padding=self.padding, allow_auto_padding=self.allow_auto_padding)


def combine_tensor_patches(patches: 'Tensor', original_size: 'Union[int, Tuple[int, int]]', window_size: 'Union[int, Tuple[int, int]]', stride: 'Union[int, Tuple[int, int]]', allow_auto_unpadding: 'bool'=False, unpadding: 'PadType'=0, eps: 'float'=1e-08) ->Tensor:
    """Restore input from patches.

    See :class:`~kornia.contrib.CombineTensorPatches` for details.

    Args:
        patches: patched tensor with shape :math:`(B, N, C, H_{out}, W_{out})`.
        original_size: the size of the original tensor and the output size.
        window_size: the size of the sliding window used while extracting patches.
        stride: stride of the sliding window.
        unpadding: remove the padding added to both side of the input.
        allow_auto_unpadding: whether to allow automatic unpadding of the input
            if the window and stride do not fit into the original_size.
        eps: small value used to prevent division by zero.

    Return:
        The combined patches in an image tensor with shape :math:`(B, C, H, W)`.

    Example:
        >>> out = extract_tensor_patches(torch.arange(16).view(1, 1, 4, 4), window_size=(2, 2), stride=(2, 2))
        >>> combine_tensor_patches(out, original_size=(4, 4), window_size=(2, 2), stride=(2, 2))
        tensor([[[[ 0,  1,  2,  3],
                  [ 4,  5,  6,  7],
                  [ 8,  9, 10, 11],
                  [12, 13, 14, 15]]]])

    .. note::
        This function is supposed to be used in conjunction with :func:`extract_tensor_patches`.
    """
    if patches.ndim != 5:
        raise ValueError(f'Invalid input shape, we expect BxNxCxHxW. Got: {patches.shape}')
    original_size = cast(Tuple[int, int], _pair(original_size))
    window_size = cast(Tuple[int, int], _pair(window_size))
    stride = cast(Tuple[int, int], _pair(stride))
    if (stride[0] > window_size[0]) | (stride[1] > window_size[1]):
        raise AssertionError(f'Stride={stride} should be less than or equal to Window size={window_size}, information is missing')
    if not unpadding:
        if not _check_patch_fit(original_size, window_size, stride):
            if not allow_auto_unpadding:
                warn(f'The window will not fit into the image. \nWindow size: {window_size}\nStride: {stride}\nImage size: {original_size}\nThis means we probably cannot correctly recombine patches. By enabling `allow_auto_unpadding`, the input will be unpadded to fit the window and stride.\nIf the patches have been obtained through `extract_tensor_patches` with the correct padding or the argument `allow_auto_padding`, this will result in a correct reconstruction.')
            else:
                unpadding = compute_padding(original_size=original_size, window_size=window_size, stride=stride)
    if unpadding:
        unpadding = create_padding_tuple(unpadding)
        unpadding = cast(FullPadType, unpadding)
    ones = torch.ones(patches.shape[0], patches.shape[2], original_size[0], original_size[1], device=patches.device, dtype=patches.dtype)
    if unpadding:
        ones = pad(ones, pad=unpadding)
    restored_size = ones.shape[2:]
    patches = patches.permute(0, 2, 3, 4, 1)
    patches = patches.reshape(patches.shape[0], -1, patches.shape[-1])
    int_flag = 0
    if not torch.is_floating_point(patches):
        int_flag = 1
        dtype = patches.dtype
        patches = patches.float()
        ones = ones.float()
    unfold_ones = F.unfold(ones, kernel_size=window_size, stride=stride)
    norm_map = F.fold(input=unfold_ones, output_size=restored_size, kernel_size=window_size, stride=stride)
    if unpadding:
        norm_map = pad(norm_map, [(-i) for i in unpadding])
    saturated_restored_tensor = F.fold(input=patches, output_size=restored_size, kernel_size=window_size, stride=stride)
    if unpadding:
        saturated_restored_tensor = pad(saturated_restored_tensor, [(-i) for i in unpadding])
    restored_tensor = saturated_restored_tensor / (norm_map + eps)
    if int_flag:
        restored_tensor = restored_tensor
    return restored_tensor


class CombineTensorPatches(Module):
    """Module that combines patches back into full tensors.

    In the simplest case, the output value of the operator with input size
    :math:`(B, N, C, H_{out}, W_{out})` is :math:`(B, C, H, W)`.

    where
      - :math:`B` is the batch size.
      - :math:`N` denotes the total number of extracted patches stacked in
      - :math:`C` denotes the number of input channels.
      - :math:`H`, :math:`W` the input height and width of the input in pixels.
      - :math:`H_{out}`, :math:`W_{out}` denote to denote to the patch size
        defined in the function signature.
        left-right and top-bottom order.


    * :attr:`original_size` is the size of the original image prior to
      extracting tensor patches and defines the shape of the output patch.
    * :attr:`window_size` is the size of the sliding window used while
      extracting tensor patches.
    * :attr:`stride` controls the stride to apply to the sliding window and
      regulates the overlapping between the extracted patches.
    * :attr:`unpadding` is the amount of padding to be removed. If specified,
      this value must be the same as padding used while extracting tensor patches.
    * :attr:`allow_auto_unpadding` allows automatic calculation of the padding required
      to fit the window and stride into the image. This must be used if the
      `allow_auto_padding` flag was used for extracting the patches.


    The parameters :attr:`original_size`, :attr:`window_size`, :attr:`stride`, and :attr:`unpadding` can
    be either:

        - a single ``int`` -- in which case the same value is used for the
          height and width dimension.
        - a ``tuple`` of two ints -- in which case, the first `int` is used for
          the height dimension, and the second `int` for the width dimension.

    :attr:`unpadding` can also be a ``tuple`` of four ints -- in which case, the
    first two ints are for the height dimension while the last two ints are for
    the width dimension.

    Args:
        patches: patched tensor with shape :math:`(B, N, C, H_{out}, W_{out})`.
        original_size: the size of the original tensor and the output size.
        window_size: the size of the sliding window used while extracting patches.
        stride: stride of the sliding window.
        unpadding: remove the padding added to both side of the input.
        allow_auto_unpadding: whether to allow automatic unpadding of the input
            if the window and stride do not fit into the original_size.
        eps: small value used to prevent division by zero.

    Shape:
        - Input: :math:`(B, N, C, H_{out}, W_{out})`
        - Output: :math:`(B, C, H, W)`

    Example:
        >>> out = extract_tensor_patches(torch.arange(16).view(1, 1, 4, 4), window_size=(2, 2), stride=(2, 2))
        >>> combine_tensor_patches(out, original_size=(4, 4), window_size=(2, 2), stride=(2, 2))
        tensor([[[[ 0,  1,  2,  3],
                  [ 4,  5,  6,  7],
                  [ 8,  9, 10, 11],
                  [12, 13, 14, 15]]]])

    .. note::
        This function is supposed to be used in conjunction with :class:`ExtractTensorPatches`.
    """

    def __init__(self, original_size: 'Tuple[int, int]', window_size: 'Union[int, Tuple[int, int]]', stride: 'Optional[Union[int, Tuple[int, int]]]'=None, unpadding: 'PadType'=0, allow_auto_unpadding: 'bool'=False) ->None:
        super().__init__()
        self.original_size: 'Tuple[int, int]' = original_size
        self.window_size: 'Union[int, Tuple[int, int]]' = window_size
        self.stride: 'Union[int, Tuple[int, int]]' = stride if stride is not None else window_size
        self.unpadding: 'PadType' = unpadding
        self.allow_auto_unpadding: 'bool' = allow_auto_unpadding

    def forward(self, input: 'Tensor') ->Tensor:
        return combine_tensor_patches(input, self.original_size, self.window_size, stride=self.stride, unpadding=self.unpadding, allow_auto_unpadding=self.allow_auto_unpadding)


class ConvDPUnit(nn.Sequential):

    def __init__(self, in_channels: 'int', out_channels: 'int', withBNRelu: 'bool'=True) ->None:
        super().__init__()
        self.add_module('conv1', nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=True, groups=1))
        self.add_module('conv2', nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=True, groups=out_channels))
        if withBNRelu:
            self.add_module('bn', nn.BatchNorm2d(out_channels))
            self.add_module('relu', nn.ReLU(inplace=True))


class Conv4layerBlock(nn.Sequential):

    def __init__(self, in_channels: 'int', out_channels: 'int', withBNRelu: 'bool'=True) ->None:
        super().__init__()
        self.add_module('conv1', ConvDPUnit(in_channels, in_channels, True))
        self.add_module('conv2', ConvDPUnit(in_channels, out_channels, withBNRelu))


class Conv_head(nn.Sequential):

    def __init__(self, in_channels: 'int', mid_channels: 'int', out_channels: 'int') ->None:
        super().__init__()
        self.add_module('conv1', nn.Conv2d(in_channels, mid_channels, 3, 2, 1, bias=True, groups=1))
        self.add_module('bn1', nn.BatchNorm2d(mid_channels))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv2', ConvDPUnit(mid_channels, out_channels))


url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'


class YuFaceDetectNet(nn.Module):

    def __init__(self, phase: 'str', pretrained: 'bool') ->None:
        super().__init__()
        self.phase = phase
        self.num_classes = 2
        self.model0 = Conv_head(3, 16, 16)
        self.model1 = Conv4layerBlock(16, 64)
        self.model2 = Conv4layerBlock(64, 64)
        self.model3 = Conv4layerBlock(64, 64)
        self.model4 = Conv4layerBlock(64, 64)
        self.model5 = Conv4layerBlock(64, 64)
        self.model6 = Conv4layerBlock(64, 64)
        self.head = nn.Sequential(Conv4layerBlock(64, 3 * (14 + 2 + 1), False), Conv4layerBlock(64, 2 * (14 + 2 + 1), False), Conv4layerBlock(64, 2 * (14 + 2 + 1), False), Conv4layerBlock(64, 3 * (14 + 2 + 1), False))
        if self.phase == 'train':
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    if m.bias is not None:
                        nn.init.xavier_normal_(m.weight.data)
                        m.bias.data.fill_(0.02)
                    else:
                        m.weight.data.normal_(0, 0.01)
                elif isinstance(m, nn.BatchNorm2d):
                    m.weight.data.fill_(1)
                    m.bias.data.zero_()
        if pretrained:
            pretrained_dict = torch.hub.load_state_dict_from_url(url, map_location=torch.device('cpu'))
            self.load_state_dict(pretrained_dict, strict=True)
        self.eval()

    def forward(self, x: 'torch.Tensor') ->Dict[str, torch.Tensor]:
        detection_sources, head_list = [], []
        x = self.model0(x)
        x = F.max_pool2d(x, 2)
        x = self.model1(x)
        x = self.model2(x)
        x = F.max_pool2d(x, 2)
        x = self.model3(x)
        detection_sources.append(x)
        x = F.max_pool2d(x, 2)
        x = self.model4(x)
        detection_sources.append(x)
        x = F.max_pool2d(x, 2)
        x = self.model5(x)
        detection_sources.append(x)
        x = F.max_pool2d(x, 2)
        x = self.model6(x)
        detection_sources.append(x)
        for i, h in enumerate(self.head):
            x_tmp = h(detection_sources[i])
            head_list.append(x_tmp.permute(0, 2, 3, 1).contiguous())
        head_data = torch.cat([o.view(o.size(0), -1) for o in head_list], 1)
        head_data = head_data.view(head_data.size(0), -1, 17)
        loc_data, conf_data, iou_data = head_data.split((14, 2, 1), dim=-1)
        if self.phase == 'test':
            conf_data = torch.softmax(conf_data, dim=-1)
        else:
            loc_data = loc_data.view(loc_data.size(0), -1, 14)
            conf_data = conf_data.view(conf_data.size(0), -1, self.num_classes)
            iou_data = iou_data.view(iou_data.size(0), -1, 1)
        return {'loc': loc_data, 'conf': conf_data, 'iou': iou_data}


class _PriorBox:

    def __init__(self, min_sizes: 'List[List[int]]', steps: 'List[int]', clip: 'bool', image_size: 'Tuple[int, int]') ->None:
        self.min_sizes = min_sizes
        self.steps = steps
        self.clip = clip
        self.image_size = image_size
        self.device: 'torch.device' = torch.device('cpu')
        self.dtype: 'torch.dtype' = torch.float32
        for i in range(4):
            if self.steps[i] != math.pow(2, i + 3):
                raise ValueError('steps must be [8,16,32,64]')
        self.feature_map_2th = [int(int((self.image_size[0] + 1) / 2) / 2), int(int((self.image_size[1] + 1) / 2) / 2)]
        self.feature_map_3th = [int(self.feature_map_2th[0] / 2), int(self.feature_map_2th[1] / 2)]
        self.feature_map_4th = [int(self.feature_map_3th[0] / 2), int(self.feature_map_3th[1] / 2)]
        self.feature_map_5th = [int(self.feature_map_4th[0] / 2), int(self.feature_map_4th[1] / 2)]
        self.feature_map_6th = [int(self.feature_map_5th[0] / 2), int(self.feature_map_5th[1] / 2)]
        self.feature_maps = [self.feature_map_3th, self.feature_map_4th, self.feature_map_5th, self.feature_map_6th]

    def to(self, device: 'torch.device', dtype: 'torch.dtype') ->'_PriorBox':
        self.device = device
        self.dtype = dtype
        return self

    def __call__(self) ->torch.Tensor:
        anchors: 'List[float]' = []
        for k, f in enumerate(self.feature_maps):
            min_sizes: 'List[int]' = self.min_sizes[k]
            for i in range(f[0]):
                for j in range(f[1]):
                    for min_size in min_sizes:
                        s_kx = min_size / self.image_size[1]
                        s_ky = min_size / self.image_size[0]
                        cx = (j + 0.5) * self.steps[k] / self.image_size[1]
                        cy = (i + 0.5) * self.steps[k] / self.image_size[0]
                        anchors += [cx, cy, s_kx, s_ky]
        output = torch.tensor(anchors, device=self.device, dtype=self.dtype).view(-1, 4)
        if self.clip:
            output = output.clamp(max=1, min=0)
        return output


def _decode(loc: 'torch.Tensor', priors: 'torch.Tensor', variances: 'List[float]') ->torch.Tensor:
    """Decode locations from predictions using priors to undo the encoding we did for offset regression at train
    time.

    Args:
        loc:location predictions for loc layers. Shape: [num_priors,4].
        priors: Prior boxes in center-offset form. Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes.

    Return:
        Tensor containing decoded bounding box predictions.
    """
    boxes = torch.cat((priors[:, 0:2] + loc[:, 0:2] * variances[0] * priors[:, 2:4], priors[:, 2:4] * torch.exp(loc[:, 2:4] * variances[1]), priors[:, 0:2] + loc[:, 4:6] * variances[0] * priors[:, 2:4], priors[:, 0:2] + loc[:, 6:8] * variances[0] * priors[:, 2:4], priors[:, 0:2] + loc[:, 8:10] * variances[0] * priors[:, 2:4], priors[:, 0:2] + loc[:, 10:12] * variances[0] * priors[:, 2:4], priors[:, 0:2] + loc[:, 12:14] * variances[0] * priors[:, 2:4]), 1)
    tmp = boxes[:, 0:2] - boxes[:, 2:4] / 2
    return torch.cat((tmp, boxes[:, 2:4] + tmp, boxes[:, 4:]), dim=-1)


class FaceDetector(nn.Module):
    """Detect faces in a given image using a CNN.

    By default, it uses the method described in :cite:`facedetect-yu`.

    Args:
        top_k: the maximum number of detections to return before the nms.
        confidence_threshold: the threshold used to discard detections.
        nms_threshold: the threshold used by the nms for iou.
        keep_top_k: the maximum number of detections to return after the nms.

    Return:
        A list of B tensors with shape :math:`(N,15)` to be used with :py:class:`kornia.contrib.FaceDetectorResult`.

    Example:
        >>> img = torch.rand(1, 3, 320, 320)
        >>> detect = FaceDetector()
        >>> res = detect(img)
    """

    def __init__(self, top_k: 'int'=5000, confidence_threshold: 'float'=0.3, nms_threshold: 'float'=0.3, keep_top_k: 'int'=750) ->None:
        super().__init__()
        self.top_k = top_k
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.keep_top_k = keep_top_k
        self.config = {'name': 'YuFaceDetectNet', 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]], 'steps': [8, 16, 32, 64], 'variance': [0.1, 0.2], 'clip': False}
        self.min_sizes = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]
        self.steps = [8, 16, 32, 64]
        self.variance = [0.1, 0.2]
        self.clip = False
        self.model = YuFaceDetectNet('test', pretrained=True)
        self.nms = nms_kornia

    def preprocess(self, image: 'torch.Tensor') ->torch.Tensor:
        return image

    def postprocess(self, data: 'Dict[str, torch.Tensor]', height: 'int', width: 'int') ->List[torch.Tensor]:
        loc, conf, iou = data['loc'], data['conf'], data['iou']
        scale = torch.tensor([width, height, width, height, width, height, width, height, width, height, width, height, width, height], device=loc.device, dtype=loc.dtype)
        priors = _PriorBox(self.min_sizes, self.steps, self.clip, image_size=(height, width))
        priors = priors
        batched_dets: 'List[torch.Tensor]' = []
        for batch_elem in range(loc.shape[0]):
            boxes = _decode(loc[batch_elem], priors(), self.variance)
            boxes = boxes * scale
            cls_scores, iou_scores = conf[batch_elem, :, 1], iou[batch_elem, :, 0]
            scores = (cls_scores * iou_scores.clamp(0.0, 1.0)).sqrt()
            inds = scores > self.confidence_threshold
            boxes, scores = boxes[inds], scores[inds]
            order = scores.sort(descending=True)[1][:self.top_k]
            boxes, scores = boxes[order], scores[order]
            dets = torch.cat((boxes, scores[:, None]), dim=-1)
            keep = self.nms(boxes[:, :4], scores, self.nms_threshold)
            if len(keep) > 0:
                dets = dets[keep, :]
            batched_dets.append(dets[:self.keep_top_k])
        return batched_dets

    def forward(self, image: 'torch.Tensor') ->List[torch.Tensor]:
        """Detect faces in a given batch of images.

        Args:
            image: batch of images :math:`(B,3,H,W)`

        Return:
            List[torch.Tensor]: list with the boxes found on each image. :math:`Bx(N,15)`.
        """
        img = self.preprocess(image)
        out = self.model(img)
        return self.postprocess(out, img.shape[-2], img.shape[-1])


INF = 1000000000.0


def compute_max_candidates(p_m0: 'Tensor', p_m1: 'Tensor') ->Tensor:
    """Compute the max candidates of all pairs within a batch.

    Args:
        p_m0, p_m1 (torch.Tensor): padded masks
    """
    h0s, w0s = p_m0.sum(1).max(-1)[0], p_m0.sum(-1).max(-1)[0]
    h1s, w1s = p_m1.sum(1).max(-1)[0], p_m1.sum(-1).max(-1)[0]
    max_cand = torch.sum(torch.min(torch.stack([h0s * w0s, h1s * w1s], -1), -1)[0])
    return max_cand


def mask_border(m: 'Tensor', b: 'int', v: 'Union[Tensor, float, bool]') ->None:
    """Mask borders with value
    Args:
        m (torch.Tensor): [N, H0, W0, H1, W1]
        b (int)
        v (m.dtype)
    """
    if b <= 0:
        return
    m[:, :b] = v
    m[:, :, :b] = v
    m[:, :, :, :b] = v
    m[:, :, :, :, :b] = v
    m[:, -b:] = v
    m[:, :, -b:] = v
    m[:, :, :, -b:] = v
    m[:, :, :, :, -b:] = v


def mask_border_with_padding(m: 'Tensor', bd: 'int', v: 'Union[Tensor, float, bool]', p_m0: 'Tensor', p_m1: 'Tensor') ->None:
    if bd <= 0:
        return
    m[:, :bd] = v
    m[:, :, :bd] = v
    m[:, :, :, :bd] = v
    m[:, :, :, :, :bd] = v
    h0s, w0s = p_m0.sum(1).max(-1)[0].int(), p_m0.sum(-1).max(-1)[0].int()
    h1s, w1s = p_m1.sum(1).max(-1)[0].int(), p_m1.sum(-1).max(-1)[0].int()
    for b_idx, (h0, w0, h1, w1) in enumerate(zip(h0s, w0s, h1s, w1s)):
        m[b_idx, h0 - bd:] = v
        m[b_idx, :, w0 - bd:] = v
        m[b_idx, :, :, h1 - bd:] = v
        m[b_idx, :, :, :, w1 - bd:] = v


class FineMatching(Module):
    """FineMatching with s2d paradigm."""

    def __init__(self) ->None:
        super().__init__()

    def forward(self, feat_f0: 'Tensor', feat_f1: 'Tensor', data: 'dict[str, Any]') ->None:
        """
        Args:
            feat0 (torch.Tensor): [M, WW, C]
            feat1 (torch.Tensor): [M, WW, C]
            data (dict)
        Update:
            data (dict):{
                'expec_f' (torch.Tensor): [M, 3],
                'mkpts0_f' (torch.Tensor): [M, 2],
                'mkpts1_f' (torch.Tensor): [M, 2]}
        """
        M, WW, C = feat_f0.shape
        W = int(math.sqrt(WW))
        scale = data['hw0_i'][0] / data['hw0_f'][0]
        self.M, self.W, self.WW, self.C, self.scale = M, W, WW, C, scale
        if M == 0:
            if self.training:
                raise ValueError('M >0, when training, see coarse_matching.py')
            data.update({'expec_f': torch.empty(0, 3, device=feat_f0.device, dtype=feat_f0.dtype), 'mkpts0_f': data['mkpts0_c'], 'mkpts1_f': data['mkpts1_c']})
            return
        feat_f0_picked = feat_f0[:, WW // 2, :]
        sim_matrix = torch.einsum('mc,mrc->mr', feat_f0_picked, feat_f1)
        softmax_temp = 1.0 / C ** 0.5
        heatmap = torch.softmax(softmax_temp * sim_matrix, dim=1).view(-1, W, W)
        coords_normalized = dsnt.spatial_expectation2d(heatmap[None], True)[0]
        grid_normalized = create_meshgrid(W, W, normalized_coordinates=True, device=heatmap.device, dtype=heatmap.dtype).reshape(1, -1, 2)
        var = torch.sum(grid_normalized ** 2 * heatmap.view(-1, WW, 1), dim=1) - coords_normalized ** 2
        std = torch.sum(torch.sqrt(torch.clamp(var, min=1e-10)), -1)
        data.update({'expec_f': torch.cat([coords_normalized, std.unsqueeze(1)], -1)})
        self.get_fine_match(coords_normalized, data)

    @torch.no_grad()
    def get_fine_match(self, coords_normed: 'Tensor', data: 'dict[str, Any]') ->None:
        W, _, _, scale = self.W, self.WW, self.C, self.scale
        mkpts0_f = data['mkpts0_c']
        scale1 = scale * data['scale1'][data['b_ids']] if 'scale0' in data else scale
        mkpts1_f = data['mkpts1_c'] + (coords_normed * (W // 2) * scale1)[:len(data['mconf'])]
        data.update({'mkpts0_f': mkpts0_f, 'mkpts1_f': mkpts1_f})


class FinePreprocess(Module):

    def __init__(self, config: 'Dict[str, Any]') ->None:
        super().__init__()
        self.config = config
        self.cat_c_feat = config['fine_concat_coarse_feat']
        self.W = self.config['fine_window_size']
        d_model_c = self.config['coarse']['d_model']
        d_model_f = self.config['fine']['d_model']
        self.d_model_f = d_model_f
        if self.cat_c_feat:
            self.down_proj = nn.Linear(d_model_c, d_model_f, bias=True)
            self.merge_feat = nn.Linear(2 * d_model_f, d_model_f, bias=True)
        self._reset_parameters()

    def _reset_parameters(self) ->None:
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.kaiming_normal_(p, mode='fan_out', nonlinearity='relu')

    def forward(self, feat_f0: 'Tensor', feat_f1: 'Tensor', feat_c0: 'Tensor', feat_c1: 'Tensor', data: 'Dict[str, Any]') ->Tuple[Tensor, Tensor]:
        W = self.W
        stride = data['hw0_f'][0] // data['hw0_c'][0]
        data.update({'W': W})
        if data['b_ids'].shape[0] == 0:
            feat0 = torch.empty(0, self.W ** 2, self.d_model_f, device=feat_f0.device)
            feat1 = torch.empty(0, self.W ** 2, self.d_model_f, device=feat_f0.device)
            return feat0, feat1
        feat_f0_unfold = F.unfold(feat_f0, kernel_size=(W, W), stride=stride, padding=W // 2)
        n0, cww0, l0 = feat_f0_unfold.shape
        c0 = cww0 // (W * W)
        feat_f0_unfold = feat_f0_unfold.reshape(n0, c0, -1, l0).permute(0, 3, 2, 1)
        feat_f1_unfold = F.unfold(feat_f1, kernel_size=(W, W), stride=stride, padding=W // 2)
        n1, cww1, l1 = feat_f1_unfold.shape
        c1 = cww1 // (W * W)
        feat_f1_unfold = feat_f1_unfold.reshape(n1, c1, -1, l1).permute(0, 3, 2, 1)
        feat_f0_unfold = feat_f0_unfold[data['b_ids'], data['i_ids']]
        feat_f1_unfold = feat_f1_unfold[data['b_ids'], data['j_ids']]
        if self.cat_c_feat:
            feat_c_win = self.down_proj(torch.cat([feat_c0[data['b_ids'], data['i_ids']], feat_c1[data['b_ids'], data['j_ids']]], 0))
            feat_cf_win = self.merge_feat(torch.cat([torch.cat([feat_f0_unfold, feat_f1_unfold], 0), feat_c_win.unsqueeze(1).repeat(1, W ** 2, 1)], -1))
            feat_f0_unfold, feat_f1_unfold = torch.chunk(feat_cf_win, 2, dim=0)
        return feat_f0_unfold, feat_f1_unfold


class FullAttention(Module):

    def __init__(self, use_dropout: 'bool'=False, attention_dropout: 'float'=0.1) ->None:
        super().__init__()
        self.use_dropout = use_dropout
        self.dropout = Dropout(attention_dropout)

    def forward(self, queries: 'Tensor', keys: 'Tensor', values: 'Tensor', q_mask: 'Optional[Tensor]'=None, kv_mask: 'Optional[Tensor]'=None) ->Tensor:
        """Multi-head scaled dot-product attention, a.k.a full attention.

        Args:
            queries: [N, L, H, D]
            keys: [N, S, H, D]
            values: [N, S, H, D]
            q_mask: [N, L]
            kv_mask: [N, S]
        Returns:
            queried_values: (N, L, H, D)
        """
        QK = torch.einsum('nlhd,nshd->nlsh', queries, keys)
        if kv_mask is not None and q_mask is not None:
            QK.masked_fill_(~(q_mask[:, :, None, None] * kv_mask[:, None, :, None]), float('-inf'))
        softmax_temp = 1.0 / queries.size(3) ** 0.5
        A = torch.softmax(softmax_temp * QK, dim=2)
        if self.use_dropout:
            A = self.dropout(A)
        queried_values = torch.einsum('nlsh,nshd->nlhd', A, values)
        return queried_values.contiguous()


def elu_feature_map(x: 'Tensor') ->Tensor:
    return torch.nn.functional.elu(x) + 1


class LinearAttention(Module):

    def __init__(self, eps: 'float'=1e-06) ->None:
        super().__init__()
        self.feature_map = elu_feature_map
        self.eps = eps

    def forward(self, queries: 'Tensor', keys: 'Tensor', values: 'Tensor', q_mask: 'Optional[Tensor]'=None, kv_mask: 'Optional[Tensor]'=None) ->Tensor:
        """Multi-Head linear attention proposed in "Transformers are RNNs"
        Args:
            queries: [N, L, H, D]
            keys: [N, S, H, D]
            values: [N, S, H, D]
            q_mask: [N, L]
            kv_mask: [N, S]
        Returns:
            queried_values: (N, L, H, D)
        """
        Q = self.feature_map(queries)
        K = self.feature_map(keys)
        if q_mask is not None:
            Q = Q * q_mask[:, :, None, None]
        if kv_mask is not None:
            K = K * kv_mask[:, :, None, None]
            values = values * kv_mask[:, :, None, None]
        v_length = values.size(1)
        values = values / v_length
        KV = torch.einsum('nshd,nshv->nhdv', K, values)
        Z = 1 / (torch.einsum('nlhd,nhd->nlh', Q, K.sum(dim=1)) + self.eps)
        queried_values = torch.einsum('nlhd,nhdv,nlh->nlhv', Q, KV, Z) * v_length
        return queried_values.contiguous()


class LoFTREncoderLayer(Module):

    def __init__(self, d_model: 'int', nhead: 'int', attention: "Optional[Literal['linear']]"='linear') ->None:
        super().__init__()
        self.dim = d_model // nhead
        self.nhead = nhead
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.attention = LinearAttention() if attention == 'linear' else FullAttention()
        self.merge = nn.Linear(d_model, d_model, bias=False)
        self.mlp = nn.Sequential(nn.Linear(d_model * 2, d_model * 2, bias=False), nn.ReLU(True), nn.Linear(d_model * 2, d_model, bias=False))
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x: 'Tensor', source: 'Tensor', x_mask: 'Optional[Tensor]'=None, source_mask: 'Optional[Tensor]'=None) ->Tensor:
        """
        Args:
            x: [N, L, C]
            source: [N, S, C]
            x_mask: [N, L] (optional)
            source_mask: [N, S] (optional)
        """
        bs = x.size(0)
        query, key, value = x, source, source
        query = self.q_proj(query).view(bs, -1, self.nhead, self.dim)
        key = self.k_proj(key).view(bs, -1, self.nhead, self.dim)
        value = self.v_proj(value).view(bs, -1, self.nhead, self.dim)
        message = self.attention(query, key, value, q_mask=x_mask, kv_mask=source_mask)
        message = self.merge(message.view(bs, -1, self.nhead * self.dim))
        message = self.norm1(message)
        message = self.mlp(torch.cat([x, message], dim=2))
        message = self.norm2(message)
        return x + message


class LocalFeatureTransformer(Module):
    """A Local Feature Transformer (LoFTR) module."""

    def __init__(self, config: 'dict[str, Any]') ->None:
        super().__init__()
        self.config = config
        self.d_model = config['d_model']
        self.nhead = config['nhead']
        self.layer_names = config['layer_names']
        encoder_layer = LoFTREncoderLayer(config['d_model'], config['nhead'], config['attention'])
        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(len(self.layer_names))])
        self._reset_parameters()

    def _reset_parameters(self) ->None:
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, feat0: 'Tensor', feat1: 'Tensor', mask0: 'None | Tensor'=None, mask1: 'None | Tensor'=None) ->tuple[Tensor, Tensor]:
        """
        Args:
            feat0: [N, L, C]
            feat1: [N, S, C]
            mask0: [N, L] (optional)
            mask1: [N, S] (optional)
        """
        if self.d_model != feat0.size(2):
            msg = 'the feature number of src and transformer must be equal'
            raise ValueError(msg)
        for layer, name in zip(self.layers, self.layer_names):
            if name == 'self':
                feat0 = layer(feat0, feat0, mask0, mask0)
                feat1 = layer(feat1, feat1, mask1, mask1)
            elif name == 'cross':
                feat0 = layer(feat0, feat1, mask0, mask1)
                feat1 = layer(feat1, feat0, mask1, mask0)
            else:
                raise KeyError
        return feat0, feat1


cos = torch.cos


sin = torch.sin


class PositionEncodingSine(Module):
    """This is a sinusoidal position encoding that generalized to 2-dimensional images."""
    pe: 'Tensor'

    def __init__(self, d_model: 'int', max_shape: 'Tuple[int, int]'=(256, 256), temp_bug_fix: 'bool'=True) ->None:
        """
        Args:
            max_shape (tuple): for 1/8 featmap, the max length of 256 corresponds to 2048 pixels
            temp_bug_fix (bool): As noted in this [issue](https://github.com/zju3dv/LoFTR/issues/41),
                the original implementation of LoFTR includes a bug in the pos-enc impl, which has little impact
                on the final performance. For now, we keep both impls for backward compatibility.
                We will remove the buggy impl after re-training all variants of our released models.
        """
        super().__init__()
        self.d_model = d_model
        self.temp_bug_fix = temp_bug_fix
        pe = self._create_position_encoding(max_shape)
        self.register_buffer('pe', pe, persistent=False)

    def _create_position_encoding(self, max_shape: 'Tuple[int, int]') ->Tensor:
        """Creates a position encoding from scratch.

        For 1/8 feature map (which is standard): If the input image size is H, W (both divisible by 8), the max_shape
        should be (H//8, W//8).
        """
        pe = zeros((self.d_model, *max_shape))
        y_position = torch.ones(max_shape).cumsum(0).float().unsqueeze(0)
        x_position = torch.ones(max_shape).cumsum(1).float().unsqueeze(0)
        if self.temp_bug_fix:
            div_term = torch.exp(torch.arange(0, self.d_model // 2, 2).float() * (-math.log(10000.0) / (self.d_model // 2)))
        else:
            div_term = torch.exp(torch.arange(0, self.d_model // 2, 2).float() * (-math.log(10000.0) / self.d_model // 2))
        div_term = div_term[:, None, None]
        pe[0::4, :, :] = sin(x_position * div_term)
        pe[1::4, :, :] = cos(x_position * div_term)
        pe[2::4, :, :] = sin(y_position * div_term)
        pe[3::4, :, :] = cos(y_position * div_term)
        return pe.unsqueeze(0)

    def update_position_encoding_size(self, max_shape: 'Tuple[int, int]') ->None:
        """Updates position encoding to new max_shape.

        For 1/8 feature map (which is standard): If the input image size is H, W (both divisible by 8), the max_shape
        should be (H//8, W//8).
        """
        self.pe = self._create_position_encoding(max_shape)

    def forward(self, x: 'Tensor') ->Tensor:
        """
        Args:
            x: [N, C, H, W]
        """
        if x.size(2) > self.pe.size(2) or x.size(3) > self.pe.size(3):
            max_shape = max(x.size(2), self.pe.size(2)), max(x.size(3), self.pe.size(3))
            self.update_position_encoding_size(max_shape)
        return x + self.pe[:, :, :x.size(2), :x.size(3)]


def conv1x1(in_planes: 'int', out_planes: 'int', stride: 'int'=1) ->nn.Conv2d:
    """1x1 convolution without padding."""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False)


def conv3x3(in_planes: 'int', out_planes: 'int', stride: 'int'=1) ->nn.Conv2d:
    """3x3 convolution with padding."""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)


class BasicBlock(Module):

    def __init__(self, in_planes: 'int', planes: 'int', stride: 'int'=1) ->None:
        super().__init__()
        self.conv1 = conv3x3(in_planes, planes, stride)
        self.conv2 = conv3x3(planes, planes)
        self.bn1 = nn.BatchNorm2d(planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        if stride == 1:
            self.downsample = None
        else:
            self.downsample = nn.Sequential(conv1x1(in_planes, planes, stride=stride), nn.BatchNorm2d(planes))

    def forward(self, x: 'Tensor') ->Tensor:
        y = x
        y = self.relu(self.bn1(self.conv1(y)))
        y = self.bn2(self.conv2(y))
        if self.downsample is not None:
            x = self.downsample(x)
        return self.relu(x + y)


class ResNetFPN_16_4(nn.Module):
    """ResNet+FPN, output resolution are 1/16 and 1/4.

    Each block has 2 layers.
    """

    def __init__(self, config: 'Dict[str, Any]') ->None:
        super().__init__()
        block = BasicBlock
        initial_dim = config['initial_dim']
        block_dims = config['block_dims']
        self.in_planes = initial_dim
        self.conv1 = nn.Conv2d(1, initial_dim, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(initial_dim)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(block, block_dims[0], stride=1)
        self.layer2 = self._make_layer(block, block_dims[1], stride=2)
        self.layer3 = self._make_layer(block, block_dims[2], stride=2)
        self.layer4 = self._make_layer(block, block_dims[3], stride=2)
        self.layer4_outconv = conv1x1(block_dims[3], block_dims[3])
        self.layer3_outconv = conv1x1(block_dims[2], block_dims[3])
        self.layer3_outconv2 = nn.Sequential(conv3x3(block_dims[3], block_dims[3]), nn.BatchNorm2d(block_dims[3]), nn.LeakyReLU(), conv3x3(block_dims[3], block_dims[2]))
        self.layer2_outconv = conv1x1(block_dims[1], block_dims[2])
        self.layer2_outconv2 = nn.Sequential(conv3x3(block_dims[2], block_dims[2]), nn.BatchNorm2d(block_dims[2]), nn.LeakyReLU(), conv3x3(block_dims[2], block_dims[1]))
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block: 'Union[Type[BasicBlock], Module]', dim: 'int', stride: 'int'=1) ->nn.Sequential:
        layer1 = block(self.in_planes, dim, stride=stride)
        layer2 = block(dim, dim, stride=1)
        layers = layer1, layer2
        self.in_planes = dim
        return nn.Sequential(*layers)

    def forward(self, x: 'Tensor') ->List[Tensor]:
        x0 = self.relu(self.bn1(self.conv1(x)))
        x1 = self.layer1(x0)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)
        x4_out = self.layer4_outconv(x4)
        x4_out_2x = F.interpolate(x4_out, scale_factor=2.0, mode='bilinear', align_corners=True)
        x3_out = self.layer3_outconv(x3)
        x3_out = self.layer3_outconv2(x3_out + x4_out_2x)
        x3_out_2x = F.interpolate(x3_out, scale_factor=2.0, mode='bilinear', align_corners=True)
        x2_out = self.layer2_outconv(x2)
        x2_out = self.layer2_outconv2(x2_out + x3_out_2x)
        return [x4_out, x2_out]


class ResNetFPN_8_2(nn.Module):
    """ResNet+FPN, output resolution are 1/8 and 1/2.

    Each block has 2 layers.
    """

    def __init__(self, config: 'Dict[str, Any]') ->None:
        super().__init__()
        block = BasicBlock
        initial_dim = config['initial_dim']
        block_dims = config['block_dims']
        self.in_planes = initial_dim
        self.conv1 = nn.Conv2d(1, initial_dim, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(initial_dim)
        self.relu = nn.ReLU(inplace=True)
        self.layer1 = self._make_layer(block, block_dims[0], stride=1)
        self.layer2 = self._make_layer(block, block_dims[1], stride=2)
        self.layer3 = self._make_layer(block, block_dims[2], stride=2)
        self.layer3_outconv = conv1x1(block_dims[2], block_dims[2])
        self.layer2_outconv = conv1x1(block_dims[1], block_dims[2])
        self.layer2_outconv2 = nn.Sequential(conv3x3(block_dims[2], block_dims[2]), nn.BatchNorm2d(block_dims[2]), nn.LeakyReLU(), conv3x3(block_dims[2], block_dims[1]))
        self.layer1_outconv = conv1x1(block_dims[0], block_dims[1])
        self.layer1_outconv2 = nn.Sequential(conv3x3(block_dims[1], block_dims[1]), nn.BatchNorm2d(block_dims[1]), nn.LeakyReLU(), conv3x3(block_dims[1], block_dims[0]))
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block: 'Union[Type[BasicBlock], Module]', dim: 'int', stride: 'int'=1) ->nn.Sequential:
        layer1 = block(self.in_planes, dim, stride=stride)
        layer2 = block(dim, dim, stride=1)
        layers = layer1, layer2
        self.in_planes = dim
        return nn.Sequential(*layers)

    def forward(self, x: 'Tensor') ->List[Tensor]:
        x0 = self.relu(self.bn1(self.conv1(x)))
        x1 = self.layer1(x0)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x3_out = self.layer3_outconv(x3)
        x2_out = self.layer2_outconv(x2)
        x3_out_2x = F.interpolate(x3_out, size=x2_out.shape[2:], mode='bilinear', align_corners=True)
        x2_out = self.layer2_outconv2(x2_out + x3_out_2x)
        x1_out = self.layer1_outconv(x1)
        x2_out_2x = F.interpolate(x2_out, size=x1_out.shape[2:], mode='bilinear', align_corners=True)
        x1_out = self.layer1_outconv2(x1_out + x2_out_2x)
        return [x3_out, x1_out]


default_cfg = {'backbone_type': 'ResNetFPN', 'resolution': (8, 2), 'fine_window_size': 5, 'fine_concat_coarse_feat': True, 'resnetfpn': {'initial_dim': 128, 'block_dims': [128, 196, 256]}, 'coarse': {'d_model': 256, 'd_ffn': 256, 'nhead': 8, 'layer_names': ['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross'], 'attention': 'linear', 'temp_bug_fix': False}, 'match_coarse': {'thr': 0.2, 'border_rm': 2, 'match_type': 'dual_softmax', 'dsmax_temperature': 0.1, 'skh_iters': 3, 'skh_init_bin_score': 1.0, 'skh_prefilter': True, 'train_coarse_percent': 0.4, 'train_pad_num_gt_min': 200}, 'fine': {'d_model': 128, 'd_ffn': 128, 'nhead': 8, 'layer_names': ['self', 'cross'], 'attention': 'linear'}}


def _side_to_image_size(side_size: 'int', aspect_ratio: 'float', side: 'str'='short') ->Tuple[int, int]:
    if side not in ('short', 'long', 'vert', 'horz'):
        raise ValueError(f"side can be one of 'short', 'long', 'vert', and 'horz'. Got '{side}'")
    if side == 'vert':
        return side_size, int(side_size * aspect_ratio)
    if side == 'horz':
        return int(side_size / aspect_ratio), side_size
    if (side == 'short') ^ (aspect_ratio < 1.0):
        return side_size, int(side_size * aspect_ratio)
    return int(side_size / aspect_ratio), side_size


def _unpack_2d_ks(kernel_size: 'tuple[int, int] | int') ->tuple[int, int]:
    if isinstance(kernel_size, int):
        ky = kx = kernel_size
    else:
        KORNIA_CHECK(len(kernel_size) == 2, '2D Kernel size should have a length of 2.')
        ky, kx = kernel_size
    ky = int(ky)
    kx = int(kx)
    return ky, kx


def filter2d_separable(input: 'Tensor', kernel_x: 'Tensor', kernel_y: 'Tensor', border_type: 'str'='reflect', normalized: 'bool'=False, padding: 'str'='same') ->Tensor:
    """Convolve a tensor with two 1d kernels, in x and y directions.

    The function applies a given kernel to a tensor. The kernel is applied
    independently at each depth channel of the tensor. Before applying the
    kernel, the function applies padding according to the specified mode so
    that the output remains in the same shape.

    Args:
        input: the input tensor with shape of
          :math:`(B, C, H, W)`.
        kernel_x: the kernel to be convolved with the input
          tensor. The kernel shape must be :math:`(1, kW)` or :math:`(B, kW)`.
        kernel_y: the kernel to be convolved with the input
          tensor. The kernel shape must be :math:`(1, kH)` or :math:`(B, kH)`.
        border_type: the padding mode to be applied before convolving.
          The expected modes are: ``'constant'``, ``'reflect'``,
          ``'replicate'`` or ``'circular'``.
        normalized: If True, kernel will be L1 normalized.
        padding: This defines the type of padding.
          2 modes available ``'same'`` or ``'valid'``.

    Return:
        Tensor: the convolved tensor of same size and numbers of channels
        as the input with shape :math:`(B, C, H, W)`.

    Example:
        >>> input = torch.tensor([[[
        ...    [0., 0., 0., 0., 0.],
        ...    [0., 0., 0., 0., 0.],
        ...    [0., 0., 5., 0., 0.],
        ...    [0., 0., 0., 0., 0.],
        ...    [0., 0., 0., 0., 0.],]]])
        >>> kernel = torch.ones(1, 3)

        >>> filter2d_separable(input, kernel, kernel, padding='same')
        tensor([[[[0., 0., 0., 0., 0.],
                  [0., 5., 5., 5., 0.],
                  [0., 5., 5., 5., 0.],
                  [0., 5., 5., 5., 0.],
                  [0., 0., 0., 0., 0.]]]])
    """
    out_x = filter2d(input, kernel_x[..., None, :], border_type, normalized, padding)
    out = filter2d(out_x, kernel_y[..., None], border_type, normalized, padding)
    return out


Device = Union[str, torch.device, None]


Dtype = Union[torch.dtype, None]


def _check_kernel_size(kernel_size: 'tuple[int, ...] | int', min_value: 'int'=0, allow_even: 'bool'=False) ->None:
    if isinstance(kernel_size, int):
        kernel_size = kernel_size,
    fmt = 'even or odd' if allow_even else 'odd'
    for size in kernel_size:
        KORNIA_CHECK(isinstance(size, int) and ((size % 2 == 1 or allow_even) and size > min_value), f'Kernel size must be an {fmt} integer bigger than {min_value}. Gotcha {size} on {kernel_size}')


def gaussian(window_size: 'int', sigma: 'Tensor | float', *, mean: Optional[Union[Tensor, float]]=None, device: Optional[Device]=None, dtype: Optional[Dtype]=None) ->Tensor:
    """Compute the gaussian values based on the window and sigma values.

    Args:
        window_size: the size which drives the filter amount.
        sigma: gaussian standard deviation. If a tensor, should be in a shape :math:`(B, 1)`
        mean: Mean of the Gaussian function (center). If not provided, it defaults to window_size // 2.
        If a tensor, should be in a shape :math:`(B, 1)`
        device: This value will be used if sigma is a float. Device desired to compute.
        dtype: This value will be used if sigma is a float. Dtype desired for compute.
    Returns:
        A tensor withshape :math:`(B, 	ext{kernel_size})`, with Gaussian values.
    """
    if isinstance(sigma, float):
        sigma = tensor([[sigma]], device=device, dtype=dtype)
    KORNIA_CHECK_IS_TENSOR(sigma)
    KORNIA_CHECK_SHAPE(sigma, ['B', '1'])
    batch_size = sigma.shape[0]
    mean = float(window_size // 2) if mean is None else mean
    if isinstance(mean, float):
        mean = tensor([[mean]], device=sigma.device, dtype=sigma.dtype)
    KORNIA_CHECK_IS_TENSOR(mean)
    KORNIA_CHECK_SHAPE(mean, ['B', '1'])
    x = (torch.arange(window_size, device=sigma.device, dtype=sigma.dtype) - mean).expand(batch_size, -1)
    if window_size % 2 == 0:
        x = x + 0.5
    gauss = torch.exp(-x.pow(2.0) / (2 * sigma.pow(2.0)))
    return gauss / gauss.sum(-1, keepdim=True)


def get_gaussian_kernel1d(kernel_size: 'int', sigma: 'float | Tensor', force_even: 'bool'=False, *, device: Optional[Device]=None, dtype: Optional[Dtype]=None) ->Tensor:
    """Function that returns Gaussian filter coefficients.

    Args:
        kernel_size: filter size. It should be odd and positive.
        sigma: gaussian standard deviation.
        force_even: overrides requirement for odd kernel size.
        device: This value will be used if sigma is a float. Device desired to compute.
        dtype: This value will be used if sigma is a float. Dtype desired for compute.

    Returns:
        gaussian filter coefficients with shape :math:`(B, \\text{kernel_size})`.

    Examples:
        >>> get_gaussian_kernel1d(3, 2.5)
        tensor([[0.3243, 0.3513, 0.3243]])
        >>> get_gaussian_kernel1d(5, 1.5)
        tensor([[0.1201, 0.2339, 0.2921, 0.2339, 0.1201]])
        >>> get_gaussian_kernel1d(5, torch.tensor([[1.5], [0.7]]))
        tensor([[0.1201, 0.2339, 0.2921, 0.2339, 0.1201],
                [0.0096, 0.2054, 0.5699, 0.2054, 0.0096]])
    """
    _check_kernel_size(kernel_size, allow_even=force_even)
    return gaussian(kernel_size, sigma, device=device, dtype=dtype)


def get_gaussian_kernel2d(kernel_size: 'tuple[int, int] | int', sigma: 'tuple[float, float] | Tensor', force_even: 'bool'=False, *, device: Optional[Device]=None, dtype: Optional[Dtype]=None) ->Tensor:
    """Function that returns Gaussian filter matrix coefficients.

    Args:
        kernel_size: filter sizes in the y and x direction. Sizes should be odd and positive.
        sigma: gaussian standard deviation in the y and x.
        force_even: overrides requirement for odd kernel size.
        device: This value will be used if sigma is a float. Device desired to compute.
        dtype: This value will be used if sigma is a float. Dtype desired for compute.

    Returns:
        2D tensor with gaussian filter matrix coefficients.

    Shape:
        - Output: :math:`(B, \\text{kernel_size}_x, \\text{kernel_size}_y)`

    Examples:
        >>> get_gaussian_kernel2d((5, 5), (1.5, 1.5))
        tensor([[[0.0144, 0.0281, 0.0351, 0.0281, 0.0144],
                 [0.0281, 0.0547, 0.0683, 0.0547, 0.0281],
                 [0.0351, 0.0683, 0.0853, 0.0683, 0.0351],
                 [0.0281, 0.0547, 0.0683, 0.0547, 0.0281],
                 [0.0144, 0.0281, 0.0351, 0.0281, 0.0144]]])
        >>> get_gaussian_kernel2d((3, 5), (1.5, 1.5))
        tensor([[[0.0370, 0.0720, 0.0899, 0.0720, 0.0370],
                 [0.0462, 0.0899, 0.1123, 0.0899, 0.0462],
                 [0.0370, 0.0720, 0.0899, 0.0720, 0.0370]]])
        >>> get_gaussian_kernel2d((5, 5), torch.tensor([[1.5, 1.5]]))
        tensor([[[0.0144, 0.0281, 0.0351, 0.0281, 0.0144],
                 [0.0281, 0.0547, 0.0683, 0.0547, 0.0281],
                 [0.0351, 0.0683, 0.0853, 0.0683, 0.0351],
                 [0.0281, 0.0547, 0.0683, 0.0547, 0.0281],
                 [0.0144, 0.0281, 0.0351, 0.0281, 0.0144]]])
    """
    if isinstance(sigma, tuple):
        sigma = tensor([sigma], device=device, dtype=dtype)
    KORNIA_CHECK_IS_TENSOR(sigma)
    KORNIA_CHECK_SHAPE(sigma, ['B', '2'])
    ksize_y, ksize_x = _unpack_2d_ks(kernel_size)
    sigma_y, sigma_x = sigma[:, 0, None], sigma[:, 1, None]
    kernel_y = get_gaussian_kernel1d(ksize_y, sigma_y, force_even, device=device, dtype=dtype)[..., None]
    kernel_x = get_gaussian_kernel1d(ksize_x, sigma_x, force_even, device=device, dtype=dtype)[..., None]
    return kernel_y * kernel_x.view(-1, 1, ksize_x)


def gaussian_blur2d(input: 'Tensor', kernel_size: 'tuple[int, int] | int', sigma: 'tuple[float, float] | Tensor', border_type: 'str'='reflect', separable: 'bool'=True) ->Tensor:
    """Create an operator that blurs a tensor using a Gaussian filter.

    .. image:: _static/img/gaussian_blur2d.png

    The operator smooths the given tensor with a gaussian kernel by convolving
    it to each channel. It supports batched operation.

    Arguments:
        input: the input tensor with shape :math:`(B,C,H,W)`.
        kernel_size: the size of the kernel.
        sigma: the standard deviation of the kernel.
        border_type: the padding mode to be applied before convolving.
          The expected modes are: ``'constant'``, ``'reflect'``,
          ``'replicate'`` or ``'circular'``. Default: ``'reflect'``.
        separable: run as composition of two 1d-convolutions.

    Returns:
        the blurred tensor with shape :math:`(B, C, H, W)`.

    .. note::
       See a working example `here <https://kornia.github.io/tutorials/nbs/gaussian_blur.html>`__.

    Examples:
        >>> input = torch.rand(2, 4, 5, 5)
        >>> output = gaussian_blur2d(input, (3, 3), (1.5, 1.5))
        >>> output.shape
        torch.Size([2, 4, 5, 5])

        >>> output = gaussian_blur2d(input, (3, 3), torch.tensor([[1.5, 1.5]]))
        >>> output.shape
        torch.Size([2, 4, 5, 5])
    """
    KORNIA_CHECK_IS_TENSOR(input)
    if isinstance(sigma, tuple):
        sigma = tensor([sigma], device=input.device, dtype=input.dtype)
    else:
        KORNIA_CHECK_IS_TENSOR(sigma)
        sigma = sigma
    if separable:
        ky, kx = _unpack_2d_ks(kernel_size)
        bs = sigma.shape[0]
        kernel_x = get_gaussian_kernel1d(kx, sigma[:, 1].view(bs, 1))
        kernel_y = get_gaussian_kernel1d(ky, sigma[:, 0].view(bs, 1))
        out = filter2d_separable(input, kernel_x, kernel_y, border_type)
    else:
        kernel = get_gaussian_kernel2d(kernel_size, sigma)
        out = filter2d(input, kernel, border_type)
    return out


def _to_bchw(tensor: 'Tensor') ->Tensor:
    """Convert a PyTorch tensor image to BCHW format.

    Args:
        tensor (torch.Tensor): image of the form :math:`(*, H, W)`.

    Returns:
        input tensor of the form :math:`(B, C, H, W)`.
    """
    if not isinstance(tensor, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(tensor)}')
    if len(tensor.shape) < 2:
        raise ValueError(f'Input size must be a two, three or four dimensional tensor. Got {tensor.shape}')
    if len(tensor.shape) == 2:
        tensor = tensor.unsqueeze(0)
    if len(tensor.shape) == 3:
        tensor = tensor.unsqueeze(0)
    if len(tensor.shape) > 4:
        tensor = tensor.view(-1, tensor.shape[-3], tensor.shape[-2], tensor.shape[-1])
    return tensor


def perform_keep_shape_image(f: 'Callable[..., Tensor]') ->Callable[..., Tensor]:
    """A decorator that enable `f` to be applied to an image of arbitrary leading dimensions `(*, C, H, W)`.

    It works by first viewing the image as `(B, C, H, W)`, applying the function and re-viewing the image as original
    shape.
    """

    @wraps(f)
    def _wrapper(input: 'Tensor', *args: Any, **kwargs: Any) ->Tensor:
        if not isinstance(input, Tensor):
            raise TypeError(f'Input input type is not a Tensor. Got {type(input)}')
        if input.shape.numel() == 0:
            raise ValueError('Invalid input tensor, it is empty.')
        input_shape = input.shape
        input = _to_bchw(input)
        output = f(input, *args, **kwargs)
        if len(input_shape) == 3:
            output = output[0]
        if len(input_shape) == 2:
            output = output[0, 0]
        if len(input_shape) > 4:
            output = output.view(*(input_shape[:-3] + output.shape[-3:]))
        return output
    return _wrapper


@perform_keep_shape_image
def resize(input: 'Tensor', size: 'Union[int, Tuple[int, int]]', interpolation: 'str'='bilinear', align_corners: 'Optional[bool]'=None, side: 'str'='short', antialias: 'bool'=False) ->Tensor:
    """Resize the input Tensor to the given size.

    .. image:: _static/img/resize.png

    Args:
        tensor: The image tensor to be skewed with shape of :math:`(..., H, W)`.
            `...` means there can be any number of dimensions.
        size: Desired output size. If size is a sequence like (h, w),
            output size will be matched to this. If size is an int, smaller edge of the image will
            be matched to this number. i.e, if height > width, then image will be rescaled
            to (size * height / width, size)
        interpolation:  algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` |
            'bicubic' | 'trilinear' | 'area'.
        align_corners: interpolation flag.
        side: Corresponding side if ``size`` is an integer. Can be one of ``'short'``, ``'long'``, ``'vert'``,
            or ``'horz'``.
        antialias: if True, then image will be filtered with Gaussian before downscaling.
            No effect for upscaling.

    Returns:
        The resized tensor with the shape as the specified size.

    Example:
        >>> img = torch.rand(1, 3, 4, 4)
        >>> out = resize(img, (6, 8))
        >>> print(out.shape)
        torch.Size([1, 3, 6, 8])
    """
    if not isinstance(input, Tensor):
        raise TypeError(f'Input tensor type is not a Tensor. Got {type(input)}')
    if len(input.shape) < 2:
        raise ValueError(f'Input tensor must have at least two dimensions. Got {len(input.shape)}')
    input_size = h, w = input.shape[-2:]
    if isinstance(size, int):
        if torch.onnx.is_in_onnx_export():
            warnings.warn('Please pass the size with a tuple when exporting to ONNX to correct the tracing.')
        aspect_ratio = w / h
        size = _side_to_image_size(size, aspect_ratio, side)
    if not torch.onnx.is_in_onnx_export():
        if size == input_size:
            return input
    factors = h / size[0], w / size[1]
    antialias = antialias and max(factors) > 1
    if antialias:
        sigmas = max((factors[0] - 1.0) / 2.0, 0.001), max((factors[1] - 1.0) / 2.0, 0.001)
        ks = int(max(2.0 * 2 * sigmas[0], 3)), int(max(2.0 * 2 * sigmas[1], 3))
        if ks[0] % 2 == 0:
            ks = ks[0] + 1, ks[1]
        if ks[1] % 2 == 0:
            ks = ks[0], ks[1] + 1
        input = gaussian_blur2d(input, ks, sigmas)
    output = torch.nn.functional.interpolate(input, size=size, mode=interpolation, align_corners=align_corners)
    return output


class LoFTR(Module):
    """Module, which finds correspondences between two images.

    This is based on the original code from paper "LoFTR: Detector-Free Local
    Feature Matching with Transformers". See :cite:`LoFTR2021` for more details.

    If the distance matrix dm is not provided, :py:func:`torch.cdist` is used.

    Args:
        config: Dict with initialization parameters. Do not pass it, unless you know what you are doing`.
        pretrained: Download and set pretrained weights to the model. Options: 'outdoor', 'indoor'.
                    'outdoor' is trained on the MegaDepth dataset and 'indoor'
                    on the ScanNet.

    Returns:
        Dictionary with image correspondences and confidence scores.

    Example:
        >>> img1 = torch.rand(1, 1, 320, 200)
        >>> img2 = torch.rand(1, 1, 128, 128)
        >>> input = {"image0": img1, "image1": img2}
        >>> loftr = LoFTR('outdoor')
        >>> out = loftr(input)
    """

    def __init__(self, pretrained: 'Optional[str]'='outdoor', config: 'dict[str, Any]'=default_cfg) ->None:
        super().__init__()
        self.config = config
        if pretrained == 'indoor_new':
            self.config['coarse']['temp_bug_fix'] = True
        self.backbone = build_backbone(config)
        self.pos_encoding = PositionEncodingSine(config['coarse']['d_model'], temp_bug_fix=config['coarse']['temp_bug_fix'])
        self.loftr_coarse = LocalFeatureTransformer(config['coarse'])
        self.coarse_matching = CoarseMatching(config['match_coarse'])
        self.fine_preprocess = FinePreprocess(config)
        self.loftr_fine = LocalFeatureTransformer(config['fine'])
        self.fine_matching = FineMatching()
        self.pretrained = pretrained
        if pretrained is not None:
            if pretrained not in urls.keys():
                raise ValueError(f'pretrained should be None or one of {urls.keys()}')
            pretrained_dict = torch.hub.load_state_dict_from_url(urls[pretrained], map_location=torch.device('cpu'))
            self.load_state_dict(pretrained_dict['state_dict'])
        self.eval()

    def forward(self, data: 'dict[str, Tensor]') ->dict[str, Tensor]:
        """
        Args:
            data: dictionary containing the input data in the following format:

        Keyword Args:
            image0: left image with shape :math:`(N, 1, H1, W1)`.
            image1: right image with shape :math:`(N, 1, H2, W2)`.
            mask0 (optional): left image mask. '0' indicates a padded position :math:`(N, H1, W1)`.
            mask1 (optional): right image mask. '0' indicates a padded position :math:`(N, H2, W2)`.

        Returns:
            - ``keypoints0``, matching keypoints from image0 :math:`(NC, 2)`.
            - ``keypoints1``, matching keypoints from image1 :math:`(NC, 2)`.
            - ``confidence``, confidence score [0, 1] :math:`(NC)`.
            - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.
        """
        _data: 'dict[str, Tensor | int | torch.Size]' = {'bs': data['image0'].size(0), 'hw0_i': data['image0'].shape[2:], 'hw1_i': data['image1'].shape[2:]}
        if _data['hw0_i'] == _data['hw1_i']:
            feats_c, feats_f = self.backbone(torch.cat([data['image0'], data['image1']], dim=0))
            (feat_c0, feat_c1), (feat_f0, feat_f1) = feats_c.split(_data['bs']), feats_f.split(_data['bs'])
        else:
            (feat_c0, feat_f0), (feat_c1, feat_f1) = self.backbone(data['image0']), self.backbone(data['image1'])
        _data.update({'hw0_c': feat_c0.shape[2:], 'hw1_c': feat_c1.shape[2:], 'hw0_f': feat_f0.shape[2:], 'hw1_f': feat_f1.shape[2:]})
        feat_c0 = self.pos_encoding(feat_c0).permute(0, 2, 3, 1)
        n, h, w, c = feat_c0.shape
        feat_c0 = feat_c0.reshape(n, -1, c)
        feat_c1 = self.pos_encoding(feat_c1).permute(0, 2, 3, 1)
        n1, h1, w1, c1 = feat_c1.shape
        feat_c1 = feat_c1.reshape(n1, -1, c1)
        mask_c0 = mask_c1 = None
        if 'mask0' in data:
            mask_c0 = resize(data['mask0'], _data['hw0_c'], interpolation='nearest').flatten(-2)
        if 'mask1' in data:
            mask_c1 = resize(data['mask1'], _data['hw1_c'], interpolation='nearest').flatten(-2)
        feat_c0, feat_c1 = self.loftr_coarse(feat_c0, feat_c1, mask_c0, mask_c1)
        self.coarse_matching(feat_c0, feat_c1, _data, mask_c0=mask_c0, mask_c1=mask_c1)
        feat_f0_unfold, feat_f1_unfold = self.fine_preprocess(feat_f0, feat_f1, feat_c0, feat_c1, _data)
        if feat_f0_unfold.size(0) != 0:
            feat_f0_unfold, feat_f1_unfold = self.loftr_fine(feat_f0_unfold, feat_f1_unfold)
        self.fine_matching(feat_f0_unfold, feat_f1_unfold, _data)
        rename_keys: 'dict[str, str]' = {'mkpts0_f': 'keypoints0', 'mkpts1_f': 'keypoints1', 'mconf': 'confidence', 'b_ids': 'batch_indexes'}
        out: 'dict[str, Tensor]' = {}
        for k, v in rename_keys.items():
            _d = _data[k]
            if isinstance(_d, Tensor):
                out[v] = _d
            else:
                raise TypeError(f'Expected Tensor for item `{k}`. Gotcha {type(_d)}')
        return out

    def load_state_dict(self, state_dict: 'dict[str, Any]', *args: Any, **kwargs: Any) ->Any:
        for k in list(state_dict.keys()):
            if k.startswith('matcher.'):
                state_dict[k.replace('matcher.', '', 1)] = state_dict.pop(k)
        return super().load_state_dict(state_dict, *args, **kwargs)


def KORNIA_CHECK_LAF(laf: 'Tensor', raises: 'bool'=True) ->bool:
    """Check whether a Local Affine Frame (laf) has a valid shape.

    Args:
        laf: local affine frame tensor to evaluate.
        raises: bool indicating whether an exception should be raised upon failure.

    Raises:
        Exception: if the input laf does not have a shape :math:`(B,N,2,3)` and raises is True.

    Example:
        >>> lafs = torch.rand(2, 10, 2, 3)
        >>> KORNIA_CHECK_LAF(lafs)
        True
    """
    return KORNIA_CHECK_SHAPE(laf, ['B', 'N', '2', '3'], raises)


def get_laf_center(LAF: 'Tensor') ->Tensor:
    """Return a center (keypoint) of the LAFs. The convention is that center of 5-pixel image (coordinates from 0
    to 4) is 2, and not 2.5.

    Args:
        LAF: :math:`(B, N, 2, 3)`

    Returns:
        xy :math:`(B, N, 2)`

    Example:
        >>> input = torch.ones(1, 5, 2, 3)  # BxNx2x3
        >>> output = get_laf_center(input)  # BxNx2
    """
    KORNIA_CHECK_LAF(LAF)
    out = LAF[..., 2]
    return out


class LocalFeatureMatcher(Module):
    """Module, which finds correspondences between two images based on local features.

    Args:
        local_feature: Local feature detector. See :class:`~kornia.feature.GFTTAffNetHardNet`.
        matcher: Descriptor matcher, see :class:`~kornia.feature.DescriptorMatcher`.

    Returns:
        Dict[str, Tensor]: Dictionary with image correspondences and confidence scores.

    Example:
        >>> img1 = torch.rand(1, 1, 320, 200)
        >>> img2 = torch.rand(1, 1, 128, 128)
        >>> input = {"image0": img1, "image1": img2}
        >>> gftt_hardnet_matcher = LocalFeatureMatcher(
        ...     GFTTAffNetHardNet(10), kornia.feature.DescriptorMatcher('snn', 0.8)
        ... )
        >>> out = gftt_hardnet_matcher(input)
    """

    def __init__(self, local_feature: 'Module', matcher: 'Module') ->None:
        super().__init__()
        self.local_feature = local_feature
        self.matcher = matcher
        self.eval()

    def extract_features(self, image: 'Tensor', mask: 'Optional[Tensor]'=None) ->Dict[str, Tensor]:
        """Function for feature extraction from simple image."""
        lafs0, resps0, descs0 = self.local_feature(image, mask)
        return {'lafs': lafs0, 'responses': resps0, 'descriptors': descs0}

    def no_match_output(self, device: 'Device', dtype: 'torch.dtype') ->Dict[str, Tensor]:
        return {'keypoints0': torch.empty(0, 2, device=device, dtype=dtype), 'keypoints1': torch.empty(0, 2, device=device, dtype=dtype), 'lafs0': torch.empty(0, 0, 2, 3, device=device, dtype=dtype), 'lafs1': torch.empty(0, 0, 2, 3, device=device, dtype=dtype), 'confidence': torch.empty(0, device=device, dtype=dtype), 'batch_indexes': torch.empty(0, device=device, dtype=torch.long)}

    def forward(self, data: 'Dict[str, Tensor]') ->Dict[str, Tensor]:
        """
        Args:
            data: dictionary containing the input data in the following format:

        Keyword Args:
            image0: left image with shape :math:`(N, 1, H1, W1)`.
            image1: right image with shape :math:`(N, 1, H2, W2)`.
            mask0 (optional): left image mask. '0' indicates a padded position :math:`(N, H1, W1)`.
            mask1 (optional): right image mask. '0' indicates a padded position :math:`(N, H2, W2)`.

        Returns:
            - ``keypoints0``, matching keypoints from image0 :math:`(NC, 2)`.
            - ``keypoints1``, matching keypoints from image1 :math:`(NC, 2)`.
            - ``confidence``, confidence score [0, 1] :math:`(NC)`.
            - ``lafs0``, matching LAFs from image0 :math:`(1, NC, 2, 3)`.
            - ``lafs1``, matching LAFs from image1 :math:`(1, NC, 2, 3)`.
            - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.
        """
        num_image_pairs: 'int' = data['image0'].shape[0]
        if 'lafs0' not in data.keys() or 'descriptors0' not in data.keys():
            feats_dict0: 'Dict[str, Tensor]' = self.extract_features(data['image0'])
            lafs0, descs0 = feats_dict0['lafs'], feats_dict0['descriptors']
        else:
            lafs0, descs0 = data['lafs0'], data['descriptors0']
        if 'lafs1' not in data.keys() or 'descriptors1' not in data.keys():
            feats_dict1: 'Dict[str, Tensor]' = self.extract_features(data['image1'])
            lafs1, descs1 = feats_dict1['lafs'], feats_dict1['descriptors']
        else:
            lafs1, descs1 = data['lafs1'], data['descriptors1']
        keypoints0: 'Tensor' = get_laf_center(lafs0)
        keypoints1: 'Tensor' = get_laf_center(lafs1)
        out_keypoints0: 'List[Tensor]' = []
        out_keypoints1: 'List[Tensor]' = []
        out_confidence: 'List[Tensor]' = []
        out_batch_indexes: 'List[Tensor]' = []
        out_lafs0: 'List[Tensor]' = []
        out_lafs1: 'List[Tensor]' = []
        for batch_idx in range(num_image_pairs):
            dists, idxs = self.matcher(descs0[batch_idx], descs1[batch_idx])
            if len(idxs) == 0:
                continue
            current_keypoints_0 = keypoints0[batch_idx, idxs[:, 0]]
            current_keypoints_1 = keypoints1[batch_idx, idxs[:, 1]]
            current_lafs_0 = lafs0[batch_idx, idxs[:, 0]]
            current_lafs_1 = lafs1[batch_idx, idxs[:, 1]]
            out_confidence.append(1.0 - dists)
            batch_idxs = batch_idx * torch.ones(len(dists), device=keypoints0.device, dtype=torch.long)
            out_keypoints0.append(current_keypoints_0)
            out_keypoints1.append(current_keypoints_1)
            out_lafs0.append(current_lafs_0)
            out_lafs1.append(current_lafs_1)
            out_batch_indexes.append(batch_idxs)
        if len(out_batch_indexes) == 0:
            return self.no_match_output(data['image0'].device, data['image0'].dtype)
        return {'keypoints0': concatenate(out_keypoints0, dim=0).view(-1, 2), 'keypoints1': concatenate(out_keypoints1, dim=0).view(-1, 2), 'lafs0': concatenate(out_lafs0, dim=0).view(1, -1, 2, 3), 'lafs1': concatenate(out_lafs1, dim=0).view(1, -1, 2, 3), 'confidence': concatenate(out_confidence, dim=0).view(-1), 'batch_indexes': concatenate(out_batch_indexes, dim=0).view(-1)}


def _torch_svd_cast(input: 'Tensor') ->Tuple[Tensor, Tensor, Tensor]:
    """Helper function to make torch.svd work with other than fp32/64.

    The function torch.svd is only implemented for fp32/64 which makes
    impossible to be used by fp16 or others. What this function does, is cast
    input data type to fp32, apply torch.svd, and cast back to the input dtype.

    NOTE: in torch 1.8.1 this function is recommended to use as torch.linalg.svd
    """
    dtype = input.dtype
    if dtype not in (torch.float32, torch.float64):
        dtype = torch.float32
    out1, out2, out3H = torch.linalg.svd(input)
    if torch_version_ge(1, 11):
        out3 = out3H.mH
    else:
        out3 = out3H.transpose(-1, -2)
    return out1, out2, out3


ones_like = torch.ones_like


def normalize_points(points: 'Tensor', eps: 'float'=1e-08) ->Tuple[Tensor, Tensor]:
    """Normalizes points (isotropic).

    Computes the transformation matrix such that the two principal moments of the set of points
    are equal to unity, forming an approximately symmetric circular cloud of points of radius 1
    about the origin. Reference: Hartley/Zisserman 4.4.4 pag.107

    This operation is an essential step before applying the DLT algorithm in order to consider
    the result as optimal.

    Args:
       points: Tensor containing the points to be normalized with shape :math:`(B, N, 2)`.
       eps: epsilon value to avoid numerical instabilities.

    Returns:
       tuple containing the normalized points in the shape :math:`(B, N, 2)` and the transformation matrix
       in the shape :math:`(B, 3, 3)`.
    """
    if len(points.shape) != 3:
        raise AssertionError(points.shape)
    if points.shape[-1] != 2:
        raise AssertionError(points.shape)
    x_mean = torch.mean(points, dim=1, keepdim=True)
    scale = (points - x_mean).norm(dim=-1, p=2).mean(dim=-1)
    scale = torch.sqrt(torch.tensor(2.0)) / (scale + eps)
    ones, zeros = ones_like(scale), torch.zeros_like(scale)
    transform = stack([scale, zeros, -scale * x_mean[..., 0, 0], zeros, scale, -scale * x_mean[..., 0, 1], zeros, zeros, ones], dim=-1)
    transform = transform.view(-1, 3, 3)
    points_norm = transform_points(transform, points)
    return points_norm, transform


def normalize_transformation(M: 'Tensor', eps: 'float'=1e-08) ->Tensor:
    """Normalize a given transformation matrix.

    The function trakes the transformation matrix and normalize so that the value in
    the last row and column is one.

    Args:
        M: The transformation to be normalized of any shape with a minimum size of 2x2.
        eps: small value to avoid unstabilities during the backpropagation.

    Returns:
        the normalized transformation matrix with same shape as the input.
    """
    if len(M.shape) < 2:
        raise AssertionError(M.shape)
    norm_val: 'Tensor' = M[..., -1:, -1:]
    return where(norm_val.abs() > eps, M / (norm_val + eps), M)


def safe_inverse_with_mask(A: 'Tensor') ->Tuple[Tensor, Tensor]:
    """Helper function, which avoids crashing because of non-invertable matrix input and outputs the mask of valid
    solution."""
    if not isinstance(A, Tensor):
        raise AssertionError(f'A must be Tensor. Got: {type(A)}.')
    dtype_original = A.dtype
    if dtype_original not in (torch.float32, torch.float64):
        dtype = torch.float32
    else:
        dtype = dtype_original
    inverse, info = inv_ex(A)
    mask = info == 0
    return inverse, mask


def solve_quadratic(coeffs: 'Tensor') ->Tensor:
    """Solve given quadratic equation.

    The function takes the coefficients of quadratic equation and returns the real roots.

    .. math:: coeffs[0]x^2 + coeffs[1]x + coeffs[2] = 0

    Args:
        coeffs : The coefficients of quadratic equation :`(B, 3)`

    Returns:
        A tensor of shape `(B, 2)` containing the real roots to the quadratic equation.

    Example:
        >>> coeffs = torch.tensor([[1., 4., 4.]])
        >>> roots = solve_quadratic(coeffs)

    .. note::
       In cases where a quadratic polynomial has only one real root, the output will be in the format
       [real_root, 0]. And for the complex roots should be represented as 0. This is done to maintain
       a consistent output shape for all cases.
    """
    KORNIA_CHECK_SHAPE(coeffs, ['B', '3'])
    a = coeffs[:, 0]
    b = coeffs[:, 1]
    c = coeffs[:, 2]
    delta = b * b - 4 * a * c
    mask_negative = delta < 0
    mask_zero = delta == 0
    inv_2a = 0.5 / a
    solutions = zeros((coeffs.shape[0], 2), device=coeffs.device, dtype=coeffs.dtype)
    if torch.any(mask_zero):
        solutions[mask_zero, 0] = -b[mask_zero] * inv_2a[mask_zero]
        solutions[mask_zero, 1] = solutions[mask_zero, 0]
    sqrt_delta = torch.sqrt(delta)
    mask = torch.bitwise_and(~mask_negative, ~mask_zero)
    if torch.any(mask):
        solutions[mask, 0] = (-b[mask] + sqrt_delta[mask]) * inv_2a[mask]
        solutions[mask, 1] = (-b[mask] - sqrt_delta[mask]) * inv_2a[mask]
    return solutions


zeros_like = torch.zeros_like


def solve_cubic(coeffs: 'Tensor') ->Tensor:
    """Solve given cubic equation.

    The function takes the coefficients of cubic equation and returns
    the real roots.

    .. math:: coeffs[0]x^3 + coeffs[1]x^2 + coeffs[2]x + coeffs[3] = 0

    Args:
        coeffs : The coefficients cubic equation : `(B, 4)`

    Returns:
        A tensor of shape `(B, 3)` containing the real roots to the cubic equation.

    Example:
        >>> coeffs = torch.tensor([[32., 3., -11., -6.]])
        >>> roots = solve_cubic(coeffs)

    .. note::
       In cases where a cubic polynomial has only one or two real roots, the output for the non-real
       roots should be represented as 0. Thus, the output for a single real root should be in the
       format [real_root, 0, 0], and for two real roots, it should be [real_root_1, real_root_2, 0].
    """
    KORNIA_CHECK_SHAPE(coeffs, ['B', '4'])
    _PI = torch.tensor(math.pi, device=coeffs.device, dtype=coeffs.dtype)
    a = coeffs[:, 0]
    b = coeffs[:, 1]
    c = coeffs[:, 2]
    d = coeffs[:, 3]
    solutions = zeros((len(coeffs), 3), device=a.device, dtype=a.dtype)
    mask_a_zero = a == 0
    mask_b_zero = b == 0
    mask_c_zero = c == 0
    mask_first_order = mask_a_zero & mask_b_zero & ~mask_c_zero
    mask_second_order = mask_a_zero & ~mask_b_zero & ~mask_c_zero
    if torch.any(mask_second_order):
        solutions[mask_second_order, 0:2] = solve_quadratic(coeffs[mask_second_order, 1:])
    if torch.any(mask_first_order):
        solutions[mask_first_order, 0] = torch.tensor(1.0, device=a.device, dtype=a.dtype)
    inv_a = 1.0 / a[~mask_a_zero]
    b_a = inv_a * b[~mask_a_zero]
    b_a2 = b_a * b_a
    c_a = inv_a * c[~mask_a_zero]
    d_a = inv_a * d[~mask_a_zero]
    Q = (3 * c_a - b_a2) / 9
    R = (9 * b_a * c_a - 27 * d_a - 2 * b_a * b_a2) / 54
    Q3 = Q * Q * Q
    D = Q3 + R * R
    b_a_3 = 1.0 / 3.0 * b_a
    a_Q_zero = ones_like(a)
    a_R_zero = ones_like(a)
    a_D_zero = ones_like(a)
    a_Q_zero[~mask_a_zero] = Q
    a_R_zero[~mask_a_zero] = R
    a_D_zero[~mask_a_zero] = D
    mask_Q_zero = (Q == 0) & (R != 0)
    mask_Q_zero_solutions = (a_Q_zero == 0) & (a_R_zero != 0)
    if torch.any(mask_Q_zero):
        x0_Q_zero = torch.pow(2 * R[mask_Q_zero], 1 / 3) - b_a_3[mask_Q_zero]
        solutions[mask_Q_zero_solutions, 0] = x0_Q_zero
    mask_QR_zero = (Q == 0) & (R == 0)
    mask_QR_zero_solutions = (a_Q_zero == 0) & (a_R_zero == 0)
    if torch.any(mask_QR_zero):
        solutions[mask_QR_zero_solutions] = stack([-b_a_3[mask_QR_zero], -b_a_3[mask_QR_zero], -b_a_3[mask_QR_zero]], dim=1)
    mask_D_zero = (D <= 0) & (Q != 0)
    mask_D_zero_solutions = (a_D_zero <= 0) & (a_Q_zero != 0)
    if torch.any(mask_D_zero):
        theta_D_zero = torch.acos(R[mask_D_zero] / torch.sqrt(-Q3[mask_D_zero]))
        sqrt_Q_D_zero = torch.sqrt(-Q[mask_D_zero])
        x0_D_zero = 2 * sqrt_Q_D_zero * cos(theta_D_zero / 3.0) - b_a_3[mask_D_zero]
        x1_D_zero = 2 * sqrt_Q_D_zero * cos((theta_D_zero + 2 * _PI) / 3.0) - b_a_3[mask_D_zero]
        x2_D_zero = 2 * sqrt_Q_D_zero * cos((theta_D_zero + 4 * _PI) / 3.0) - b_a_3[mask_D_zero]
        solutions[mask_D_zero_solutions] = stack([x0_D_zero, x1_D_zero, x2_D_zero], dim=1)
    a_D_positive = zeros_like(a)
    a_D_positive[~mask_a_zero] = D
    mask_D_positive_solution = (a_D_positive > 0) & (a_Q_zero != 0)
    mask_D_positive = (D > 0) & (Q != 0)
    if torch.any(mask_D_positive):
        AD = zeros_like(R)
        BD = zeros_like(R)
        R_abs = torch.abs(R)
        mask_R_positive = R_abs > 1e-16
        if torch.any(mask_R_positive):
            AD[mask_R_positive] = torch.pow(R_abs[mask_R_positive] + torch.sqrt(D[mask_R_positive]), 1 / 3)
            mask_R_positive_ = R < 0
            if torch.any(mask_R_positive_):
                AD[mask_R_positive_] = -AD[mask_R_positive_]
            BD[mask_R_positive] = -Q[mask_R_positive] / AD[mask_R_positive]
        x0_D_positive = AD[mask_D_positive] + BD[mask_D_positive] - b_a_3[mask_D_positive]
        solutions[mask_D_positive_solution, 0] = x0_D_positive
    return solutions


def run_7point(points1: 'Tensor', points2: 'Tensor') ->Tensor:
    """Compute the fundamental matrix using the 7-point algorithm.

    Args:
        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2)`.
        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2)`.

    Returns:
        the computed fundamental matrix with shape :math:`(B, 3*m, 3), Valid values of m are 1, 2 or 3`
    """
    KORNIA_CHECK_SHAPE(points1, ['B', '7', '2'])
    KORNIA_CHECK_SHAPE(points2, ['B', '7', '2'])
    batch_size = points1.shape[0]
    points1_norm, transform1 = normalize_points(points1)
    points2_norm, transform2 = normalize_points(points2)
    x1, y1 = torch.chunk(points1_norm, dim=-1, chunks=2)
    x2, y2 = torch.chunk(points2_norm, dim=-1, chunks=2)
    ones = ones_like(x1)
    X = concatenate([x2 * x1, x2 * y1, x2, y2 * x1, y2 * y1, y2, x1, y1, ones], -1)
    _, _, v = _torch_svd_cast(X)
    f1 = v[..., 7].view(-1, 3, 3)
    f2 = v[..., 8].view(-1, 3, 3)
    coeffs = zeros((batch_size, 4), device=v.device, dtype=v.dtype)
    f1_det = torch.linalg.det(f1)
    f2_det = torch.linalg.det(f2)
    coeffs[:, 0] = f1_det
    coeffs[:, 1] = torch.einsum('bii->b', f2 @ safe_inverse_with_mask(f1)[0]) * f1_det
    coeffs[:, 2] = torch.einsum('bii->b', f1 @ safe_inverse_with_mask(f2)[0]) * f2_det
    coeffs[:, 3] = f2_det
    roots = solve_cubic(coeffs)
    fmatrix = zeros((batch_size, 3, 3, 3), device=v.device, dtype=v.dtype)
    valid_root_mask = (torch.count_nonzero(roots, dim=1) < 3) | (torch.count_nonzero(roots, dim=1) > 1)
    _lambda = roots
    _mu = torch.ones_like(_lambda)
    _s = f1[valid_root_mask, 2, 2].unsqueeze(dim=1) * roots[valid_root_mask] + f2[valid_root_mask, 2, 2].unsqueeze(dim=1)
    _s_non_zero_mask = ~torch.isclose(_s, torch.tensor(0.0, device=v.device, dtype=v.dtype))
    _mu[_s_non_zero_mask] = 1.0 / _s[_s_non_zero_mask]
    _lambda[_s_non_zero_mask] = _lambda[_s_non_zero_mask] * _mu[_s_non_zero_mask]
    f1_expanded = f1.unsqueeze(1).expand(batch_size, 3, 3, 3)
    f2_expanded = f2.unsqueeze(1).expand(batch_size, 3, 3, 3)
    fmatrix[valid_root_mask] = f1_expanded[valid_root_mask] * _lambda[valid_root_mask, :, None, None] + f2_expanded[valid_root_mask] * _mu[valid_root_mask, :, None, None]
    mat_ind = zeros(3, 3, dtype=torch.bool)
    mat_ind[2, 2] = True
    fmatrix[_s_non_zero_mask, mat_ind] = 1.0
    fmatrix[~_s_non_zero_mask, mat_ind] = 0.0
    trans1_exp = transform1[valid_root_mask].unsqueeze(1).expand(-1, fmatrix.shape[2], -1, -1)
    trans2_exp = transform2[valid_root_mask].unsqueeze(1).expand(-1, fmatrix.shape[2], -1, -1)
    fmatrix[valid_root_mask] = torch.matmul(trans2_exp.transpose(-2, -1), torch.matmul(fmatrix[valid_root_mask], trans1_exp))
    return normalize_transformation(fmatrix)


def KORNIA_CHECK_SAME_SHAPE(x: 'Tensor', y: 'Tensor', raises: 'bool'=True) ->bool:
    """Check whether two tensor have the same shape.

    Args:
        x: first tensor to evaluate.
        y: sencod tensor to evaluate.
        msg: message to show in the exception.
        raises: bool indicating whether an exception should be raised upon failure.

    Raises:
        TypeException: if the two tensors have not the same shape and raises is True.

    Example:
        >>> x1 = torch.rand(2, 3, 3)
        >>> x2 = torch.rand(2, 3, 3)
        >>> KORNIA_CHECK_SAME_SHAPE(x1, x2)
        True
    """
    if x.shape != y.shape:
        if raises:
            raise TypeError(f'Not same shape for tensors. Got: {x.shape} and {y.shape}')
        return False
    return True


def run_8point(points1: 'Tensor', points2: 'Tensor', weights: 'Optional[Tensor]'=None) ->Tensor:
    """Compute the fundamental matrix using the DLT formulation.

    The linear system is solved by using the Weighted Least Squares Solution for the 8 Points algorithm.

    Args:
        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2), N>=8`.
        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2), N>=8`.
        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.

    Returns:
        the computed fundamental matrix with shape :math:`(B, 3, 3)`.
    """
    KORNIA_CHECK_SHAPE(points1, ['B', 'N', '2'])
    KORNIA_CHECK_SHAPE(points2, ['B', 'N', '2'])
    KORNIA_CHECK_SAME_SHAPE(points1, points2)
    if points1.shape[1] < 8:
        raise AssertionError(points1.shape)
    if weights is not None:
        KORNIA_CHECK_SHAPE(weights, ['B', 'N'])
        if not weights.shape[1] == points1.shape[1]:
            raise AssertionError(weights.shape)
    points1_norm, transform1 = normalize_points(points1)
    points2_norm, transform2 = normalize_points(points2)
    x1, y1 = torch.chunk(points1_norm, dim=-1, chunks=2)
    x2, y2 = torch.chunk(points2_norm, dim=-1, chunks=2)
    ones = ones_like(x1)
    X = torch.cat([x2 * x1, x2 * y1, x2, y2 * x1, y2 * y1, y2, x1, y1, ones], dim=-1)
    if weights is None:
        X = X.transpose(-2, -1) @ X
    else:
        w_diag = torch.diag_embed(weights)
        X = X.transpose(-2, -1) @ w_diag @ X
    _, _, V = _torch_svd_cast(X)
    F_mat = V[..., -1].view(-1, 3, 3)
    U, S, V = _torch_svd_cast(F_mat)
    rank_mask = torch.tensor([1.0, 1.0, 0.0], device=F_mat.device, dtype=F_mat.dtype)
    F_projected = U @ (torch.diag_embed(S * rank_mask) @ V.transpose(-2, -1))
    F_est = transform2.transpose(-2, -1) @ (F_projected @ transform1)
    return normalize_transformation(F_est)


def find_fundamental(points1: 'Tensor', points2: 'Tensor', weights: 'Optional[Tensor]'=None, method: "Literal['8POINT', '7POINT']"='8POINT') ->Tensor:
    """
    Args:
        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2), N>=8`.
        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2), N>=8`.
        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.
        method: The method to use for computing the fundamental matrix. Supported methods are "7POINT" and "8POINT".

    Returns:
        the computed fundamental matrix with shape :math:`(B, 3*m, 3)`, where `m` number of fundamental matrix.

    Raises:
        ValueError: If an invalid method is provided.

    """
    if method.upper() == '7POINT':
        result = run_7point(points1, points2)
    elif method.upper() == '8POINT':
        result = run_8point(points1, points2, weights)
    else:
        raise ValueError(f"Invalid method: {method}. Supported methods are '7POINT' and '8POINT'.")
    return result


def _extract_device_dtype(tensor_list: 'List[Optional[Any]]') ->Tuple[torch.device, torch.dtype]:
    """Check if all the input are in the same device (only if when they are Tensor).

    If so, it would return a tuple of (device, dtype). Default: (cpu, ``get_default_dtype()``).

    Returns:
        [torch.device, torch.dtype]
    """
    device, dtype = None, None
    for tensor in tensor_list:
        if tensor is not None:
            if not isinstance(tensor, (Tensor,)):
                continue
            _device = tensor.device
            _dtype = tensor.dtype
            if device is None and dtype is None:
                device = _device
                dtype = _dtype
            elif device != _device or dtype != _dtype:
                raise ValueError(f'Passed values are not in the same device and dtype.Got ({device}, {dtype}) and ({_device}, {_dtype}).')
    if device is None:
        device = torch.device('cpu')
    if dtype is None:
        dtype = torch.get_default_dtype()
    return device, dtype


def _torch_solve_cast(A: 'Tensor', B: 'Tensor') ->Tensor:
    """Helper function to make torch.solve work with other than fp32/64.

    For stable operation, the input matrices should be cast to fp64, and the output will be cast back to the input
    dtype.
    """
    out = torch.linalg.solve(A, B)
    return out


def safe_solve_with_mask(B: 'Tensor', A: 'Tensor') ->Tuple[Tensor, Tensor, Tensor]:
    """Helper function, which avoids crashing because of singular matrix input and outputs the mask of valid
    solution."""
    if not torch_version_ge(1, 10):
        sol = _torch_solve_cast(A, B)
        warnings.warn('PyTorch version < 1.10, solve validness mask maybe not correct', RuntimeWarning)
        return sol, sol, torch.ones(len(A), dtype=torch.bool, device=A.device)
    if not isinstance(B, Tensor):
        raise AssertionError(f'B must be Tensor. Got: {type(B)}.')
    dtype: 'torch.dtype' = B.dtype
    if dtype not in (torch.float32, torch.float64):
        dtype = torch.float32
    if TYPE_CHECKING:
        A_LU: 'Tensor'
        pivots: 'Tensor'
        info: 'Tensor'
    elif torch_version_ge(1, 13):
        A_LU, pivots, info = torch.linalg.lu_factor_ex(A)
    else:
        A_LU, pivots, info = torch.lu(A, True, get_infos=True)
    valid_mask: 'Tensor' = info == 0
    n_dim_B = len(B.shape)
    n_dim_A = len(A.shape)
    if n_dim_A - n_dim_B == 1:
        B = B.unsqueeze(-1)
    if TYPE_CHECKING:
        X: 'Tensor'
    elif torch_version_ge(1, 13):
        X = torch.linalg.lu_solve(A_LU, pivots, B)
    else:
        X = torch.lu_solve(B, A_LU, pivots)
    return X, A_LU, valid_mask


def find_homography_dlt(points1: 'torch.Tensor', points2: 'torch.Tensor', weights: 'Optional[torch.Tensor]'=None, solver: 'str'='lu') ->torch.Tensor:
    """Compute the homography matrix using the DLT formulation.

    The linear system is solved by using the Weighted Least Squares Solution for the 4 Points algorithm.

    Args:
        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2)`.
        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2)`.
        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.
        solver: variants: svd, lu.


    Returns:
        the computed homography matrix with shape :math:`(B, 3, 3)`.
    """
    if points1.shape != points2.shape:
        raise AssertionError(points1.shape)
    if points1.shape[1] < 4:
        raise AssertionError(points1.shape)
    KORNIA_CHECK_SHAPE(points1, ['B', 'N', '2'])
    KORNIA_CHECK_SHAPE(points2, ['B', 'N', '2'])
    device, dtype = _extract_device_dtype([points1, points2])
    eps: 'float' = 1e-08
    points1_norm, transform1 = normalize_points(points1)
    points2_norm, transform2 = normalize_points(points2)
    x1, y1 = torch.chunk(points1_norm, dim=-1, chunks=2)
    x2, y2 = torch.chunk(points2_norm, dim=-1, chunks=2)
    ones, zeros = torch.ones_like(x1), torch.zeros_like(x1)
    ax = torch.cat([zeros, zeros, zeros, -x1, -y1, -ones, y2 * x1, y2 * y1, y2], dim=-1)
    ay = torch.cat([x1, y1, ones, zeros, zeros, zeros, -x2 * x1, -x2 * y1, -x2], dim=-1)
    A = torch.cat((ax, ay), dim=-1).reshape(ax.shape[0], -1, ax.shape[-1])
    if weights is None:
        A = A.transpose(-2, -1) @ A
    else:
        if not (len(weights.shape) == 2 and weights.shape == points1.shape[:2]):
            raise AssertionError(weights.shape)
        w_diag = torch.diag_embed(weights.unsqueeze(dim=-1).repeat(1, 1, 2).reshape(weights.shape[0], -1))
        A = A.transpose(-2, -1) @ w_diag @ A
    if solver == 'svd':
        try:
            _, _, V = _torch_svd_cast(A)
        except RuntimeError:
            warnings.warn('SVD did not converge', RuntimeWarning)
            return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)
        H = V[..., -1].view(-1, 3, 3)
    elif solver == 'lu':
        B = torch.ones(A.shape[0], A.shape[1], device=device, dtype=dtype)
        sol, _, _ = safe_solve_with_mask(B, A)
        H = sol.reshape(-1, 3, 3)
    else:
        raise NotImplementedError
    H = safe_inverse_with_mask(transform2)[0] @ (H @ transform1)
    H_norm = H / (H[..., -1:, -1:] + eps)
    return H_norm


def oneway_transfer_error(pts1: 'Tensor', pts2: 'Tensor', H: 'Tensor', squared: 'bool'=True, eps: 'float'=1e-08) ->Tensor:
    """Return transfer error in image 2 for correspondences given the homography matrix.

    Args:
        pts1: correspondences from the left images with shape
          (B, N, 2 or 3). If they are homogeneous, converted automatically.
        pts2: correspondences from the right images with shape
          (B, N, 2 or 3). If they are homogeneous, converted automatically.
        H: Homographies with shape :math:`(B, 3, 3)`.
        squared: if True (default), the squared distance is returned.
        eps: Small constant for safe sqrt.

    Returns:
        the computed distance with shape :math:`(B, N)`.
    """
    KORNIA_CHECK_SHAPE(H, ['B', '3', '3'])
    if pts1.size(-1) == 3:
        pts1 = convert_points_from_homogeneous(pts1)
    if pts2.size(-1) == 3:
        pts2 = convert_points_from_homogeneous(pts2)
    pts1_in_2: 'Tensor' = transform_points(H, pts1)
    error_squared: 'Tensor' = (pts1_in_2 - pts2).pow(2).sum(dim=-1)
    if squared:
        return error_squared
    return (error_squared + eps).sqrt()


def symmetric_transfer_error(pts1: 'Tensor', pts2: 'Tensor', H: 'Tensor', squared: 'bool'=True, eps: 'float'=1e-08) ->Tensor:
    """Return Symmetric transfer error for correspondences given the homography matrix.

    Args:
        pts1: correspondences from the left images with shape
          (B, N, 2 or 3). If they are homogeneous, converted automatically.
        pts2: correspondences from the right images with shape
          (B, N, 2 or 3). If they are homogeneous, converted automatically.
        H: Homographies with shape :math:`(B, 3, 3)`.
        squared: if True (default), the squared distance is returned.
        eps: Small constant for safe sqrt.

    Returns:
        the computed distance with shape :math:`(B, N)`.
    """
    KORNIA_CHECK_SHAPE(H, ['B', '3', '3'])
    if pts1.size(-1) == 3:
        pts1 = convert_points_from_homogeneous(pts1)
    if pts2.size(-1) == 3:
        pts2 = convert_points_from_homogeneous(pts2)
    max_num = torch.finfo(pts1.dtype).max
    H_inv, good_H = safe_inverse_with_mask(H)
    there: 'Tensor' = oneway_transfer_error(pts1, pts2, H, True, eps)
    back: 'Tensor' = oneway_transfer_error(pts2, pts1, H_inv, True, eps)
    good_H_reshape: 'Tensor' = good_H.view(-1, 1).expand_as(there)
    out = (there + back) * good_H_reshape + max_num * ~good_H_reshape
    if squared:
        return out
    return (out + eps).sqrt()


def find_homography_dlt_iterated(points1: 'Tensor', points2: 'Tensor', weights: 'Tensor', soft_inl_th: 'float'=3.0, n_iter: 'int'=5) ->Tensor:
    """Compute the homography matrix using the iteratively-reweighted least squares (IRWLS).

    The linear system is solved by using the Reweighted Least Squares Solution for the 4 Points algorithm.

    Args:
        points1: A set of points in the first image with a tensor shape :math:`(B, N, 2)`.
        points2: A set of points in the second image with a tensor shape :math:`(B, N, 2)`.
        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.
          Used for the first iteration of the IRWLS.
        soft_inl_th: Soft inlier threshold used for weight calculation.
        n_iter: number of iterations.

    Returns:
        the computed homography matrix with shape :math:`(B, 3, 3)`.
    """
    H: 'Tensor' = find_homography_dlt(points1, points2, weights)
    for _ in range(n_iter - 1):
        errors: 'Tensor' = symmetric_transfer_error(points1, points2, H, False)
        weights_new: 'Tensor' = torch.exp(-errors / (2.0 * soft_inl_th ** 2))
        H = find_homography_dlt(points1, points2, weights_new)
    return H


def find_homography_lines_dlt(ls1: 'Tensor', ls2: 'Tensor', weights: 'Optional[Tensor]'=None) ->Tensor:
    """Compute the homography matrix using the DLT formulation for line correspondences.

    See :cite:`homolines2001` for details.

    The linear system is solved by using the Weighted Least Squares Solution for the 4 Line correspondences algorithm.

    Args:
        ls1: A set of line segments in the first image with a tensor shape :math:`(B, N, 2, 2)`.
        ls2: A set of line segments in the second image with a tensor shape :math:`(B, N, 2, 2)`.
        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.

    Returns:
        the computed homography matrix with shape :math:`(B, 3, 3)`.
    """
    if len(ls1.shape) == 3:
        ls1 = ls1[None]
    if len(ls2.shape) == 3:
        ls2 = ls2[None]
    KORNIA_CHECK_SHAPE(ls1, ['B', 'N', '2', '2'])
    KORNIA_CHECK_SHAPE(ls2, ['B', 'N', '2', '2'])
    BS, N = ls1.shape[:2]
    device, dtype = _extract_device_dtype([ls1, ls2])
    points1 = ls1.reshape(BS, 2 * N, 2)
    points2 = ls2.reshape(BS, 2 * N, 2)
    points1_norm, transform1 = normalize_points(points1)
    points2_norm, transform2 = normalize_points(points2)
    lst1, le1 = torch.chunk(points1_norm, dim=1, chunks=2)
    lst2, le2 = torch.chunk(points2_norm, dim=1, chunks=2)
    xs1, ys1 = torch.chunk(lst1, dim=-1, chunks=2)
    xs2, ys2 = torch.chunk(lst2, dim=-1, chunks=2)
    xe1, ye1 = torch.chunk(le1, dim=-1, chunks=2)
    xe2, ye2 = torch.chunk(le2, dim=-1, chunks=2)
    A = ys2 - ye2
    B = xe2 - xs2
    C = xs2 * ye2 - xe2 * ys2
    eps: 'float' = 1e-08
    ax = torch.cat([A * xs1, A * ys1, A, B * xs1, B * ys1, B, C * xs1, C * ys1, C], dim=-1)
    ay = torch.cat([A * xe1, A * ye1, A, B * xe1, B * ye1, B, C * xe1, C * ye1, C], dim=-1)
    A = torch.cat((ax, ay), dim=-1).reshape(ax.shape[0], -1, ax.shape[-1])
    if weights is None:
        A = A.transpose(-2, -1) @ A
    else:
        if not (len(weights.shape) == 2 and weights.shape == ls1.shape[:2]):
            raise AssertionError(weights.shape)
        w_diag = torch.diag_embed(weights.unsqueeze(dim=-1).repeat(1, 1, 2).reshape(weights.shape[0], -1))
        A = A.transpose(-2, -1) @ w_diag @ A
    try:
        _, _, V = _torch_svd_cast(A)
    except RuntimeError:
        warnings.warn('SVD did not converge', RuntimeWarning)
        return torch.empty((points1_norm.size(0), 3, 3), device=device, dtype=dtype)
    H = V[..., -1].view(-1, 3, 3)
    H = safe_inverse_with_mask(transform2)[0] @ (H @ transform1)
    H_norm = H / (H[..., -1:, -1:] + eps)
    return H_norm


def line_segment_transfer_error_one_way(ls1: 'Tensor', ls2: 'Tensor', H: 'Tensor', squared: 'bool'=False) ->Tensor:
    """Return transfer error in image 2 for line segment correspondences given the homography matrix.

    Line segment end points are reprojected into image 2, and point-to-line error is calculated w.r.t. line,
    induced by line segment in image 2. See :cite:`homolines2001` for details.

    Args:
        ls1: line segment correspondences from the left images with shape
          (B, N, 2, 2).
        ls2: line segment correspondences from the right images with shape
          (B, N, 2, 2).
        H: Homographies with shape :math:`(B, 3, 3)`.
        squared: if True (default is False), the squared distance is returned.

    Returns:
        the computed distance with shape :math:`(B, N)`.
    """
    KORNIA_CHECK_SHAPE(H, ['B', '3', '3'])
    KORNIA_CHECK_SHAPE(ls1, ['B', 'N', '2', '2'])
    KORNIA_CHECK_SHAPE(ls2, ['B', 'N', '2', '2'])
    B, N = ls1.shape[:2]
    ps1, pe1 = torch.chunk(ls1, dim=2, chunks=2)
    ps2, pe2 = torch.chunk(ls2, dim=2, chunks=2)
    ps2_h = convert_points_to_homogeneous(ps2)
    pe2_h = convert_points_to_homogeneous(pe2)
    ln2 = ps2_h.cross(pe2_h, dim=3)
    ps1_in2 = convert_points_to_homogeneous(transform_points(H, ps1))
    pe1_in2 = convert_points_to_homogeneous(transform_points(H, pe1))
    er_st1 = (ln2 @ ps1_in2.transpose(-2, -1)).view(B, N).abs()
    er_end1 = (ln2 @ pe1_in2.transpose(-2, -1)).view(B, N).abs()
    error = 0.5 * (er_st1 + er_end1)
    if squared:
        error = error ** 2
    return error


def find_homography_lines_dlt_iterated(ls1: 'Tensor', ls2: 'Tensor', weights: 'Tensor', soft_inl_th: 'float'=4.0, n_iter: 'int'=5) ->Tensor:
    """Compute the homography matrix using the iteratively-reweighted least squares (IRWLS) from line segments. The
    linear system is solved by using the Reweighted Least Squares Solution for the 4 line segments algorithm.

    Args:
        ls1: A set of line segments in the first image with a tensor shape :math:`(B, N, 2, 2)`.
        ls2: A set of line segments in the second image with a tensor shape :math:`(B, N, 2, 2)`.
        weights: Tensor containing the weights per point correspondence with a shape of :math:`(B, N)`.
          Used for the first iteration of the IRWLS.
        soft_inl_th: Soft inlier threshold used for weight calculation.
        n_iter: number of iterations.

    Returns:
        the computed homography matrix with shape :math:`(B, 3, 3)`.
    """
    H: 'Tensor' = find_homography_lines_dlt(ls1, ls2, weights)
    for _ in range(n_iter - 1):
        errors: 'Tensor' = line_segment_transfer_error_one_way(ls1, ls2, H, False)
        weights_new: 'Tensor' = torch.exp(-errors / (2.0 * soft_inl_th ** 2))
        H = find_homography_lines_dlt(ls1, ls2, weights_new)
    return H


def sample_is_valid_for_homography(points1: 'Tensor', points2: 'Tensor') ->Tensor:
    """Function, which implements oriented constraint check from :cite:`Marquez-Neila2015`.

    Analogous to https://github.com/opencv/opencv/blob/4.x/modules/calib3d/src/usac/degeneracy.cpp#L88

    Args:
        points1: A set of points in the first image with a tensor shape :math:`(B, 4, 2)`.
        points2: A set of points in the second image with a tensor shape :math:`(B, 4, 2)`.

    Returns:
        Mask with the minimal sample is good for homography estimation:math:`(B, 3, 3)`.
    """
    if points1.shape != points2.shape:
        raise AssertionError(points1.shape)
    KORNIA_CHECK_SHAPE(points1, ['B', '4', '2'])
    KORNIA_CHECK_SHAPE(points2, ['B', '4', '2'])
    device = points1.device
    idx_perm = torch.tensor([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]], dtype=torch.long, device=device)
    points_src_h = convert_points_to_homogeneous(points1)
    points_dst_h = convert_points_to_homogeneous(points2)
    src_perm = points_src_h[:, idx_perm]
    dst_perm = points_dst_h[:, idx_perm]
    left_sign = (torch.cross(src_perm[..., 1:2, :], src_perm[..., 2:3, :]) @ src_perm[..., 0:1, :].permute(0, 1, 3, 2)).sign()
    right_sign = (torch.cross(dst_perm[..., 1:2, :], dst_perm[..., 2:3, :]) @ dst_perm[..., 0:1, :].permute(0, 1, 3, 2)).sign()
    sample_is_valid = (left_sign == right_sign).view(-1, 4).min(dim=1)[0]
    return sample_is_valid


def symmetrical_epipolar_distance(pts1: 'Tensor', pts2: 'Tensor', Fm: 'Tensor', squared: 'bool'=True, eps: 'float'=1e-08) ->Tensor:
    """Return symmetrical epipolar distance for correspondences given the fundamental matrix.

    Args:
       pts1: correspondences from the left images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
             converted automatically.
       pts2: correspondences from the right images with shape :math:`(*, N, (2|3))`. If they are not homogeneous,
             converted automatically.
       Fm: Fundamental matrices with shape :math:`(*, 3, 3)`. Called Fm to avoid ambiguity with torch.nn.functional.
       squared: if True (default), the squared distance is returned.
       eps: Small constant for safe sqrt.

    Returns:
        the computed Symmetrical distance with shape :math:`(*, N)`.
    """
    if not isinstance(Fm, Tensor):
        raise TypeError(f'Fm type is not a torch.Tensor. Got {type(Fm)}')
    if len(Fm.shape) < 3 or not Fm.shape[-2:] == (3, 3):
        raise ValueError(f'Fm must be a (*, 3, 3) tensor. Got {Fm.shape}')
    if pts1.shape[-1] == 2:
        pts1 = convert_points_to_homogeneous(pts1)
    if pts2.shape[-1] == 2:
        pts2 = convert_points_to_homogeneous(pts2)
    F_t: 'Tensor' = Fm.transpose(dim0=-2, dim1=-1)
    line1_in_2: 'Tensor' = pts1 @ F_t
    line2_in_1: 'Tensor' = pts2 @ Fm
    numerator: 'Tensor' = (pts2 * line1_in_2).sum(dim=-1).pow(2)
    denominator_inv: 'Tensor' = 1.0 / line1_in_2[..., :2].norm(2, dim=-1).pow(2) + 1.0 / line2_in_1[..., :2].norm(2, dim=-1).pow(2)
    out: 'Tensor' = numerator * denominator_inv
    if squared:
        return out
    return (out + eps).sqrt()


class RANSAC(Module):
    """Module for robust geometry estimation with RANSAC. https://en.wikipedia.org/wiki/Random_sample_consensus.

    Args:
        model_type: type of model to estimate: "homography", "fundamental", "fundamental_7pt",
            "homography_from_linesegments".
        inliers_threshold: threshold for the correspondence to be an inlier.
        batch_size: number of generated samples at once.
        max_iterations: maximum batches to generate. Actual number of models to try is ``batch_size * max_iterations``.
        confidence: desired confidence of the result, used for the early stopping.
        max_local_iterations: number of local optimization (polishing) iterations.
    """

    def __init__(self, model_type: 'str'='homography', inl_th: 'float'=2.0, batch_size: 'int'=2048, max_iter: 'int'=10, confidence: 'float'=0.99, max_lo_iters: 'int'=5) ->None:
        super().__init__()
        self.supported_models = ['homography', 'fundamental', 'fundamental_7pt', 'homography_from_linesegments']
        self.inl_th = inl_th
        self.max_iter = max_iter
        self.batch_size = batch_size
        self.model_type = model_type
        self.confidence = confidence
        self.max_lo_iters = max_lo_iters
        self.model_type = model_type
        self.error_fn: 'Callable[..., Tensor]'
        self.minimal_solver: 'Callable[..., Tensor]'
        self.polisher_solver: 'Callable[..., Tensor]'
        if model_type == 'homography':
            self.error_fn = oneway_transfer_error
            self.minimal_solver = find_homography_dlt
            self.polisher_solver = find_homography_dlt_iterated
            self.minimal_sample_size = 4
        elif model_type == 'homography_from_linesegments':
            self.error_fn = line_segment_transfer_error_one_way
            self.minimal_solver = find_homography_lines_dlt
            self.polisher_solver = find_homography_lines_dlt_iterated
            self.minimal_sample_size = 4
        elif model_type == 'fundamental':
            self.error_fn = symmetrical_epipolar_distance
            self.minimal_solver = find_fundamental
            self.minimal_sample_size = 8
            self.polisher_solver = find_fundamental
        elif model_type == 'fundamental_7pt':
            self.error_fn = symmetrical_epipolar_distance
            self.minimal_solver = partial(find_fundamental, method='7POINT')
            self.minimal_sample_size = 7
            self.polisher_solver = find_fundamental
        else:
            raise NotImplementedError(f'{model_type} is unknown. Try one of {self.supported_models}')

    def sample(self, sample_size: 'int', pop_size: 'int', batch_size: 'int', device: 'Device'=torch.device('cpu')) ->Tensor:
        """Minimal sampler, but unlike traditional RANSAC we sample in batches to get benefit of the parallel
        processing, esp.

        on GPU.
        """
        rand = torch.rand(batch_size, pop_size, device=device)
        _, out = rand.topk(k=sample_size, dim=1)
        return out

    @staticmethod
    def max_samples_by_conf(n_inl: 'int', num_tc: 'int', sample_size: 'int', conf: 'float') ->float:
        """Formula to update max_iter in order to stop iterations earlier
        https://en.wikipedia.org/wiki/Random_sample_consensus."""
        eps = 1e-09
        if num_tc <= sample_size:
            return 1.0
        if n_inl == num_tc:
            return 1.0
        return math.log(1.0 - conf) / min(-eps, math.log(max(eps, 1.0 - math.pow(n_inl / num_tc, sample_size))))

    def estimate_model_from_minsample(self, kp1: 'Tensor', kp2: 'Tensor') ->Tensor:
        batch_size, sample_size = kp1.shape[:2]
        H = self.minimal_solver(kp1, kp2, torch.ones(batch_size, sample_size, dtype=kp1.dtype, device=kp1.device))
        return H

    def verify(self, kp1: 'Tensor', kp2: 'Tensor', models: 'Tensor', inl_th: 'float') ->Tuple[Tensor, Tensor, float]:
        if len(kp1.shape) == 2:
            kp1 = kp1[None]
        if len(kp2.shape) == 2:
            kp2 = kp2[None]
        batch_size = models.shape[0]
        if self.model_type == 'homography_from_linesegments':
            errors = self.error_fn(kp1.expand(batch_size, -1, 2, 2), kp2.expand(batch_size, -1, 2, 2), models)
        else:
            errors = self.error_fn(kp1.expand(batch_size, -1, 2), kp2.expand(batch_size, -1, 2), models)
        inl = errors <= inl_th
        models_score = inl.sum(dim=1)
        best_model_idx = models_score.argmax()
        best_model_score = models_score[best_model_idx].item()
        model_best = models[best_model_idx].clone()
        inliers_best = inl[best_model_idx]
        return model_best, inliers_best, best_model_score

    def remove_bad_samples(self, kp1: 'Tensor', kp2: 'Tensor') ->Tuple[Tensor, Tensor]:
        """"""
        if self.model_type == 'homography':
            mask = sample_is_valid_for_homography(kp1, kp2)
            return kp1[mask], kp2[mask]
        return kp1, kp2

    def remove_bad_models(self, models: 'Tensor') ->Tensor:
        main_diagonal = torch.diagonal(models, dim1=1, dim2=2)
        mask = main_diagonal.abs().min(dim=1)[0] > 0.0001
        return models[mask]

    def polish_model(self, kp1: 'Tensor', kp2: 'Tensor', inliers: 'Tensor') ->Tensor:
        kp1_inl = kp1[inliers][None]
        kp2_inl = kp2[inliers][None]
        num_inl = kp1_inl.size(1)
        model = self.polisher_solver(kp1_inl, kp2_inl, torch.ones(1, num_inl, dtype=kp1_inl.dtype, device=kp1_inl.device))
        return model

    def validate_inputs(self, kp1: 'Tensor', kp2: 'Tensor', weights: 'Optional[Tensor]'=None) ->None:
        if self.model_type in ['homography', 'fundamental']:
            KORNIA_CHECK_SHAPE(kp1, ['N', '2'])
            KORNIA_CHECK_SHAPE(kp2, ['N', '2'])
            if not kp1.shape[0] == kp2.shape[0] or kp1.shape[0] < self.minimal_sample_size:
                raise ValueError(f'kp1 and kp2 should be                                  equal shape at least [{self.minimal_sample_size}, 2],                                  got {kp1.shape}, {kp2.shape}')
        if self.model_type == 'homography_from_linesegments':
            KORNIA_CHECK_SHAPE(kp1, ['N', '2', '2'])
            KORNIA_CHECK_SHAPE(kp2, ['N', '2', '2'])
            if not kp1.shape[0] == kp2.shape[0] or kp1.shape[0] < self.minimal_sample_size:
                raise ValueError(f'kp1 and kp2 should be                                  equal shape at least [{self.minimal_sample_size}, 2, 2],                                  got {kp1.shape}, {kp2.shape}')

    def forward(self, kp1: 'Tensor', kp2: 'Tensor', weights: 'Optional[Tensor]'=None) ->Tuple[Tensor, Tensor]:
        """Main forward method to execute the RANSAC algorithm.

        Args:
            kp1: source image keypoints :math:`(N, 2)`.
            kp2: distance image keypoints :math:`(N, 2)`.
            weights: optional correspondences weights. Not used now.

        Returns:
            - Estimated model, shape of :math:`(1, 3, 3)`.
            - The inlier/outlier mask, shape of :math:`(1, N)`, where N is number of input correspondences.
        """
        self.validate_inputs(kp1, kp2, weights)
        best_score_total: 'float' = float(self.minimal_sample_size)
        num_tc: 'int' = len(kp1)
        best_model_total = zeros(3, 3, dtype=kp1.dtype, device=kp1.device)
        inliers_best_total: 'Tensor' = zeros(num_tc, 1, device=kp1.device, dtype=torch.bool)
        for i in range(self.max_iter):
            idxs = self.sample(self.minimal_sample_size, num_tc, self.batch_size, kp1.device)
            kp1_sampled = kp1[idxs]
            kp2_sampled = kp2[idxs]
            kp1_sampled, kp2_sampled = self.remove_bad_samples(kp1_sampled, kp2_sampled)
            if len(kp1_sampled) == 0:
                continue
            models = self.estimate_model_from_minsample(kp1_sampled, kp2_sampled)
            models = self.remove_bad_models(models)
            if models is None or len(models) == 0:
                continue
            model, inliers, model_score = self.verify(kp1, kp2, models, self.inl_th)
            if model_score > best_score_total:
                for lo_step in range(self.max_lo_iters):
                    model_lo = self.polish_model(kp1, kp2, inliers)
                    if model_lo is None or len(model_lo) == 0:
                        continue
                    _, inliers_lo, score_lo = self.verify(kp1, kp2, model_lo, self.inl_th)
                    if score_lo > model_score:
                        model = model_lo.clone()[0]
                        inliers = inliers_lo.clone()
                        model_score = score_lo
                    else:
                        break
                best_model_total = model.clone()
                inliers_best_total = inliers.clone()
                best_score_total = model_score
                new_max_iter = int(self.max_samples_by_conf(int(best_score_total), num_tc, self.minimal_sample_size, self.confidence))
                if (i + 1) * self.batch_size >= new_max_iter:
                    break
        return best_model_total, inliers_best_total


def _fill_and_warp(src: 'Tensor', grid: 'Tensor', mode: 'str', align_corners: 'bool', fill_value: 'Tensor') ->Tensor:
    """Warp a mask of ones, then multiple with fill_value and add to default warp.

    Args:
        src: input tensor of shape :math:`(B, 3, H, W)`.
        grid: grid tensor from `transform_points`.
        mode: interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'``.
        align_corners: interpolation flag.
        fill_value: tensor of shape :math:`(3)` that fills the padding area. Only supported for RGB.

    Returns:
        the warped and filled tensor with shape :math:`(B, 3, H, W)`.
    """
    ones_mask = ones_like(src)
    fill_value = fill_value[None, :, None, None]
    inv_ones_mask = 1 - F.grid_sample(ones_mask, grid, align_corners=align_corners, mode=mode, padding_mode='zeros')
    inv_color_mask = inv_ones_mask * fill_value
    return F.grid_sample(src, grid, align_corners=align_corners, mode=mode, padding_mode='zeros') + inv_color_mask


def _torch_inverse_cast(input: 'Tensor') ->Tensor:
    """Helper function to make torch.inverse work with other than fp32/64.

    The function torch.inverse is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What
    this function does, is cast input data type to fp32, apply torch.inverse, and cast back to the input dtype.
    """
    if not isinstance(input, Tensor):
        raise AssertionError(f'Input must be Tensor. Got: {type(input)}.')
    dtype: 'torch.dtype' = input.dtype
    if dtype not in (torch.float32, torch.float64):
        dtype = torch.float32
    return torch.linalg.inv(input.to(dtype))


def normal_transform_pixel(height: 'int', width: 'int', eps: 'float'=1e-14, device: 'Optional[torch.device]'=None, dtype: 'Optional[torch.dtype]'=None) ->Tensor:
    """Compute the normalization matrix from image size in pixels to [-1, 1].

    Args:
        height image height.
        width: image width.
        eps: epsilon to prevent divide-by-zero errors

    Returns:
        normalized transform with shape :math:`(1, 3, 3)`.
    """
    tr_mat = tensor([[1.0, 0.0, -1.0], [0.0, 1.0, -1.0], [0.0, 0.0, 1.0]], device=device, dtype=dtype)
    width_denom: 'float' = eps if width == 1 else width - 1.0
    height_denom: 'float' = eps if height == 1 else height - 1.0
    tr_mat[0, 0] = tr_mat[0, 0] * 2.0 / width_denom
    tr_mat[1, 1] = tr_mat[1, 1] * 2.0 / height_denom
    return tr_mat.unsqueeze(0)


def normalize_homography(dst_pix_trans_src_pix: 'Tensor', dsize_src: 'tuple[int, int]', dsize_dst: 'tuple[int, int]') ->Tensor:
    """Normalize a given homography in pixels to [-1, 1].

    Args:
        dst_pix_trans_src_pix: homography/ies from source to destination to be
          normalized. :math:`(B, 3, 3)`
        dsize_src: size of the source image (height, width).
        dsize_dst: size of the destination image (height, width).

    Returns:
        the normalized homography of shape :math:`(B, 3, 3)`.
    """
    if not isinstance(dst_pix_trans_src_pix, Tensor):
        raise TypeError(f'Input type is not a Tensor. Got {type(dst_pix_trans_src_pix)}')
    if not (len(dst_pix_trans_src_pix.shape) == 3 or dst_pix_trans_src_pix.shape[-2:] == (3, 3)):
        raise ValueError(f'Input dst_pix_trans_src_pix must be a Bx3x3 tensor. Got {dst_pix_trans_src_pix.shape}')
    src_h, src_w = dsize_src
    dst_h, dst_w = dsize_dst
    src_norm_trans_src_pix: 'Tensor' = normal_transform_pixel(src_h, src_w).to(dst_pix_trans_src_pix)
    src_pix_trans_src_norm = _torch_inverse_cast(src_norm_trans_src_pix)
    dst_norm_trans_dst_pix: 'Tensor' = normal_transform_pixel(dst_h, dst_w).to(dst_pix_trans_src_pix)
    dst_norm_trans_src_norm: 'Tensor' = dst_norm_trans_dst_pix @ (dst_pix_trans_src_pix @ src_pix_trans_src_norm)
    return dst_norm_trans_src_norm


def warp_perspective(src: 'Tensor', M: 'Tensor', dsize: 'tuple[int, int]', mode: 'str'='bilinear', padding_mode: 'str'='zeros', align_corners: 'bool'=True, fill_value: 'Tensor'=zeros(3)) ->Tensor:
    """Apply a perspective transformation to an image.

    The function warp_perspective transforms the source image using
    the specified matrix:

    .. math::
        \\text{dst} (x, y) = \\text{src} \\left(
        \\frac{M^{-1}_{11} x + M^{-1}_{12} y + M^{-1}_{13}}{M^{-1}_{31} x + M^{-1}_{32} y + M^{-1}_{33}} ,
        \\frac{M^{-1}_{21} x + M^{-1}_{22} y + M^{-1}_{23}}{M^{-1}_{31} x + M^{-1}_{32} y + M^{-1}_{33}}
        \\right )

    Args:
        src: input image with shape :math:`(B, C, H, W)`.
        M: transformation matrix with shape :math:`(B, 3, 3)`.
        dsize: size of the output image (height, width).
        mode: interpolation mode to calculate output values ``'bilinear'`` | ``'nearest'``.
        padding_mode: padding mode for outside grid values ``'zeros'`` | ``'border'`` | ``'reflection'`` | ``'fill'``.
        align_corners: interpolation flag.
        fill_value: tensor of shape :math:`(3)` that fills the padding area. Only supported for RGB.

    Returns:
        the warped input image :math:`(B, C, H, W)`.

    Example:
       >>> img = torch.rand(1, 4, 5, 6)
       >>> H = torch.eye(3)[None]
       >>> out = warp_perspective(img, H, (4, 2), align_corners=True)
       >>> print(out.shape)
       torch.Size([1, 4, 4, 2])

    .. note::
        This function is often used in conjunction with :func:`get_perspective_transform`.

    .. note::
        See a working example `here <https://kornia.github.io/tutorials/nbs/warp_perspective.html>`_.
    """
    if not isinstance(src, Tensor):
        raise TypeError(f'Input src type is not a Tensor. Got {type(src)}')
    if not isinstance(M, Tensor):
        raise TypeError(f'Input M type is not a Tensor. Got {type(M)}')
    if not len(src.shape) == 4:
        raise ValueError(f'Input src must be a BxCxHxW tensor. Got {src.shape}')
    if not (len(M.shape) == 3 and M.shape[-2:] == (3, 3)):
        raise ValueError(f'Input M must be a Bx3x3 tensor. Got {M.shape}')
    if padding_mode == 'fill' and fill_value.shape != torch.Size([3]):
        raise ValueError(f'Padding_tensor only supported for 3 channels. Got {fill_value.shape}')
    B, _, H, W = src.size()
    h_out, w_out = dsize
    dst_norm_trans_src_norm: 'Tensor' = normalize_homography(M, (H, W), (h_out, w_out))
    src_norm_trans_dst_norm = _torch_inverse_cast(dst_norm_trans_src_norm)
    grid = create_meshgrid(h_out, w_out, normalized_coordinates=True, device=src.device).expand(B, h_out, w_out, 2)
    grid = transform_points(src_norm_trans_dst_norm[:, None, None], grid)
    if padding_mode == 'fill':
        return _fill_and_warp(src, grid, align_corners=align_corners, mode=mode, fill_value=fill_value)
    return F.grid_sample(src, grid, align_corners=align_corners, mode=mode, padding_mode=padding_mode)


class ImageStitcher(Module):
    """Stitch two images with overlapping fields of view.

    Args:
        matcher: image feature matching module.
        estimator: method to compute homography, either "vanilla" or "ransac".
            "ransac" is slower with a better accuracy.
        blending_method: method to blend two images together.
            Only "naive" is currently supported.

    Note:
        Current implementation requires strict image ordering from left to right.

    .. code-block:: python

        IS = ImageStitcher(KF.LoFTR(pretrained='outdoor'), estimator='ransac').cuda()
        # Compute the stitched result with less GPU memory cost.
        with torch.inference_mode():
            out = IS(img_left, img_right)
        # Show the result
        plt.imshow(K.tensor_to_image(out))
    """

    def __init__(self, matcher: 'Module', estimator: 'str'='ransac', blending_method: 'str'='naive') ->None:
        super().__init__()
        self.matcher = matcher
        self.estimator = estimator
        self.blending_method = blending_method
        if estimator not in ['ransac', 'vanilla']:
            raise NotImplementedError(f'Unsupported estimator {estimator}. Use `ransac` or `vanilla` instead.')
        if estimator == 'ransac':
            self.ransac = RANSAC('homography')

    def _estimate_homography(self, keypoints1: 'Tensor', keypoints2: 'Tensor') ->Tensor:
        """Estimate homography by the matched keypoints.

        Args:
            keypoints1: matched keypoint set from an image, shaped as :math:`(N, 2)`.
            keypoints2: matched keypoint set from the other image, shaped as :math:`(N, 2)`.
        """
        if self.estimator == 'vanilla':
            homo = find_homography_dlt_iterated(keypoints2[None], keypoints1[None], torch.ones_like(keypoints1[None, :, 0]))
        elif self.estimator == 'ransac':
            homo, _ = self.ransac(keypoints2, keypoints1)
            homo = homo[None]
        else:
            raise NotImplementedError(f'Unsupported estimator {self.estimator}. Use `ransac` or `vanilla` instead.')
        return homo

    def estimate_transform(self, *args: Tensor, **kwargs: Tensor) ->Tensor:
        """Compute the corresponding homography."""
        kp1, kp2, idx = kwargs['keypoints0'], kwargs['keypoints1'], kwargs['batch_indexes']
        homos = [self._estimate_homography(kp1[idx == i], kp2[idx == i]) for i in range(len(idx.unique()))]
        if len(homos) == 0:
            raise RuntimeError('Compute homography failed. No matched keypoints found.')
        return concatenate(homos)

    def blend_image(self, src_img: 'Tensor', dst_img: 'Tensor', mask: 'Tensor') ->Tensor:
        """Blend two images together."""
        out: 'Tensor'
        if self.blending_method == 'naive':
            out = where(mask == 1, src_img, dst_img)
        else:
            raise NotImplementedError(f'Unsupported blending method {self.blending_method}. Use `naive`.')
        return out

    def preprocess(self, image_1: 'Tensor', image_2: 'Tensor') ->Dict[str, Tensor]:
        """Preprocess input to the required format."""
        if isinstance(self.matcher, (LoFTR, LocalFeatureMatcher)):
            input_dict = {'image0': rgb_to_grayscale(image_1), 'image1': rgb_to_grayscale(image_2)}
        else:
            raise NotImplementedError(f'The preprocessor for {self.matcher} has not been implemented.')
        return input_dict

    def postprocess(self, image: 'Tensor', mask: 'Tensor') ->Tensor:
        mask_ = mask.sum((0, 1))
        index = int(mask_.bool().any(0).long().argmin().item())
        if index == 0:
            return image
        return image[..., :index]

    def on_matcher(self, data: 'Dict[str, Tensor]') ->Dict[str, Tensor]:
        return self.matcher(data)

    def stitch_pair(self, images_left: 'Tensor', images_right: 'Tensor', mask_left: 'Optional[Tensor]'=None, mask_right: 'Optional[Tensor]'=None) ->Tuple[Tensor, Tensor]:
        input_dict = self.preprocess(images_left, images_right)
        out_shape = images_left.shape[-2], images_left.shape[-1] + images_right.shape[-1]
        correspondences = self.on_matcher(input_dict)
        homo = self.estimate_transform(**correspondences)
        src_img = warp_perspective(images_right, homo, out_shape)
        dst_img = concatenate([images_left, zeros_like(images_right)], -1)
        if mask_left is None:
            mask_left = torch.ones_like(images_left)
        if mask_right is None:
            mask_right = torch.ones_like(images_right)
        src_mask = warp_perspective(mask_right, homo, out_shape, mode='nearest')
        dst_mask = concatenate([mask_left, zeros_like(mask_right)], -1)
        return self.blend_image(src_img, dst_img, src_mask), (dst_mask + src_mask).bool()

    def forward(self, *imgs: Tensor) ->Tensor:
        img_out = imgs[0]
        mask_left = torch.ones_like(img_out)
        for i in range(len(imgs) - 1):
            img_out, mask_left = self.stitch_pair(img_out, imgs[i + 1], mask_left)
        return self.postprocess(img_out, mask_left)


class Lambda(Module):
    """Module to create a lambda function as Module.

    Args:
        fcn: a pointer to any function.

    Example:
        >>> import torch
        >>> import kornia as K
        >>> fcn = Lambda(lambda x: K.geometry.resize(x, (32, 16)))
        >>> fcn(torch.rand(1, 4, 64, 32)).shape
        torch.Size([1, 4, 32, 16])
    """

    def __init__(self, fcn: 'Callable[..., Any]') ->None:
        super().__init__()
        self.fcn = fcn

    def forward(self, x: 'Tensor') ->Any:
        return self.fcn(x)


class ConvNormAct(nn.Sequential):

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'int', stride: 'int'=1, act: 'str'='relu', groups: 'int'=1, conv_naming: 'str'='conv', norm_naming: 'str'='norm', act_naming: 'str'='act') ->None:
        super().__init__()
        if kernel_size % 2 == 0:
            p1 = (kernel_size - 1) // 2
            p2 = kernel_size - 1 - p1
            self.pad = nn.ZeroPad2d((p1, p2, p1, p2))
            padding = 0
        else:
            padding = (kernel_size - 1) // 2
        conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, 1, groups, False)
        norm = nn.BatchNorm2d(out_channels)
        activation = {'relu': nn.ReLU, 'silu': nn.SiLU, 'none': nn.Identity}[act](inplace=True)
        self.__setattr__(conv_naming, conv)
        self.__setattr__(norm_naming, norm)
        self.__setattr__(act_naming, activation)


class MLP(Module):
    """Class to represent a multi-layer perceptron.

    The MLP represents a deep NN of fully connected layers.
    The network is build of user defined sub-units, each with a user defined number of layers.

    Skip connections span between the sub-units.
    The model follows: Ben Mildenhall et el. (2020) at https://arxiv.org/abs/2003.08934.
    """

    def __init__(self, num_dims: 'int', num_units: 'int'=2, num_unit_layers: 'int'=4, num_hidden: 'int'=128) ->None:
        """Constructor method.

        Args:
            num_dims: Number of input dimensions (channels).
            num_units: Number of sub-units.
            num_unit_layers: Number of fully connected layers in each sub-unit.
            num_hidden: Layer hidden dimensions.
        """
        super().__init__()
        self._num_unit_layers = num_unit_layers
        layers = []
        for i in range(num_units):
            num_unit_inp_dims = num_dims if i == 0 else num_hidden + num_dims
            for j in range(num_unit_layers):
                num_layer_inp_dims = num_unit_inp_dims if j == 0 else num_hidden
                layer = nn.Linear(num_layer_inp_dims, num_hidden)
                layers.append(nn.Sequential(layer, nn.ReLU()))
        self._mlp = nn.ModuleList(layers)

    def forward(self, x: 'Tensor') ->Tensor:
        out = x
        x_skip = x
        for i, layer in enumerate(self._mlp):
            if i > 0 and i % self._num_unit_layers == 0:
                out = torch.cat((out, x_skip), dim=-1)
            out = layer(out)
        return out


class DropPath(Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob: 'float'=0.0, scale_by_keep: 'bool'=True) ->None:
        super().__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def forward(self, x: 'Tensor') ->Tensor:
        if self.drop_prob == 0.0 or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
        if keep_prob > 0.0 and self.scale_by_keep:
            random_tensor.div_(keep_prob)
        return x * random_tensor


class LayerNorm2d(nn.LayerNorm):

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        out = x - torch.mean(x, dim=1, keepdim=True)
        out = out / torch.sqrt(torch.square(out).mean(dim=1, keepdim=True) + self.eps)
        if self.elementwise_affine:
            out = out * self.weight.view(1, -1, 1, 1) + self.bias.view(1, -1, 1, 1)
        return out


class ConvBN(nn.Sequential):

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'int', stride: 'int'=1, padding: 'int'=0, groups: 'int'=1, activation: 'type[Module]'=nn.Identity) ->None:
        super().__init__()
        self.c = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = activation()


class MBConv(Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', expansion_ratio: 'float', activation: 'type[Module]'=nn.GELU, drop_path: 'float'=0.0) ->None:
        super().__init__()
        hidden_channels = int(in_channels * expansion_ratio)
        self.conv1 = ConvBN(in_channels, hidden_channels, 1, activation=activation)
        self.conv2 = ConvBN(hidden_channels, hidden_channels, 3, 1, 1, hidden_channels, activation)
        self.conv3 = ConvBN(hidden_channels, out_channels, 1)
        self.drop_path = DropPath(drop_path)
        self.act = activation()

    def forward(self, x: 'Tensor') ->Tensor:
        return self.act(x + self.drop_path(self.conv3(self.conv2(self.conv1(x)))))


class ConvLayer(Module):

    def __init__(self, dim: 'int', depth: 'int', activation: 'type[Module]'=nn.GELU, drop_path: 'float | list[float]'=0.0, downsample: 'Optional[Module]'=None, use_checkpoint: 'bool'=False, conv_expand_ratio: 'float'=4.0) ->None:
        super().__init__()
        self.use_checkpoint = use_checkpoint
        if not isinstance(drop_path, list):
            drop_path = [drop_path] * depth
        self.blocks = nn.ModuleList([MBConv(dim, dim, conv_expand_ratio, activation, drop_path[i]) for i in range(depth)])
        self.downsample = downsample

    def forward(self, x: 'Tensor') ->Tensor:
        for blk in self.blocks:
            x = checkpoint.checkpoint(blk, x) if self.use_checkpoint else blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x


def val2tuple(x: 'Union[list[Any], tuple[Any, ...], Any]', min_len: 'int'=1, idx_repeat: 'int'=-1) ->tuple[Any, ...]:
    x = val2list(x)
    if len(x) > 0:
        x[idx_repeat:idx_repeat] = [x[idx_repeat] for _ in range(min_len - len(x))]
    return tuple(x)


class DSConv(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size=3, stride=1, use_bias=False, norm=('bn2d', 'bn2d'), act_func=('relu6', None)):
        super().__init__()
        use_bias = val2tuple(use_bias, 2)
        norm = val2tuple(norm, 2)
        act_func = val2tuple(act_func, 2)
        self.depth_conv = ConvLayer(in_channels, in_channels, kernel_size, stride, groups=in_channels, norm=norm[0], act_func=act_func[0], use_bias=use_bias[0])
        self.point_conv = ConvLayer(in_channels, out_channels, 1, norm=norm[1], act_func=act_func[1], use_bias=use_bias[1])

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x = self.depth_conv(x)
        x = self.point_conv(x)
        return x


class IdentityLayer(nn.Module):

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        return x


def build_kwargs_from_config(config: 'dict[str, Any]', target_func: 'Any') ->dict[str, Any]:
    valid_keys = list(signature(target_func).parameters)
    kwargs = {}
    for key in config:
        if key in valid_keys:
            kwargs[key] = config[key]
    return kwargs


def build_act(name: 'Optional[str]', **kwargs: dict[str, Any]) ->Union[nn.Module, None]:
    if name in REGISTERED_ACT_DICT:
        act_cls = REGISTERED_ACT_DICT[name]
        args = build_kwargs_from_config(kwargs, act_cls)
        return act_cls(**args)
    return None


def get_same_padding(kernel_size: 'Union[int, tuple[int, ...]]') ->Union[int, tuple[int, ...]]:
    if isinstance(kernel_size, (tuple,)):
        return tuple([get_same_padding(ks) for ks in kernel_size])
    return kernel_size // 2


class ResidualBlock(nn.Module):

    def __init__(self, main: 'nn.Module or None', shortcut: 'nn.Module or None', post_act=None, pre_norm: 'nn.Module or None'=None):
        super().__init__()
        self.pre_norm = pre_norm
        self.main = main
        self.shortcut = shortcut
        self.post_act = build_act(post_act)

    def forward_main(self, x: 'torch.Tensor') ->torch.Tensor:
        if self.pre_norm is None:
            return self.main(x)
        else:
            return self.main(self.pre_norm(x))

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        if self.main is None:
            res = x
        elif self.shortcut is None:
            res = self.forward_main(x)
        else:
            res = self.forward_main(x) + self.shortcut(x)
            if self.post_act:
                res = self.post_act(res)
        return res


class EfficientViTBlock(nn.Module):

    def __init__(self, in_channels: 'int', heads_ratio: 'float'=1.0, dim=32, expand_ratio: 'float'=4, norm='bn2d', act_func='hswish'):
        super().__init__()
        self.context_module = ResidualBlock(LiteMLA(in_channels=in_channels, out_channels=in_channels, heads_ratio=heads_ratio, dim=dim, norm=(None, norm)), IdentityLayer())
        local_module = MBConv(in_channels=in_channels, out_channels=in_channels, expand_ratio=expand_ratio, use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))
        self.local_module = ResidualBlock(local_module, IdentityLayer())

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x = self.context_module(x)
        x = self.local_module(x)
        return x


class OpSequential(nn.Module):

    def __init__(self, op_list: 'list[nn.Module or None]'):
        super().__init__()
        valid_op_list = []
        for op in op_list:
            if op is not None:
                valid_op_list.append(op)
        self.op_list = nn.ModuleList(valid_op_list)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        for op in self.op_list:
            x = op(x)
        return x


class EfficientViTBackbone(nn.Module):

    def __init__(self, width_list: 'list[int]', depth_list: 'list[int]', in_channels: 'int'=3, dim: 'int'=32, expand_ratio: 'float'=4, norm: 'str'='bn2d', act_func: 'str'='hswish') ->None:
        super().__init__()
        self.width_list = []
        input_stem = [ConvLayer(in_channels=in_channels, out_channels=width_list[0], stride=2, norm=norm, act_func=act_func)]
        for _ in range(depth_list[0]):
            block = self.build_local_block(in_channels=width_list[0], out_channels=width_list[0], stride=1, expand_ratio=1, norm=norm, act_func=act_func)
            input_stem.append(ResidualBlock(block, IdentityLayer()))
        in_channels = width_list[0]
        self.input_stem = OpSequential(input_stem)
        self.width_list.append(in_channels)
        stages = []
        for w, d in zip(width_list[1:3], depth_list[1:3]):
            stage = []
            for i in range(d):
                stride = 2 if i == 0 else 1
                block = self.build_local_block(in_channels=in_channels, out_channels=w, stride=stride, expand_ratio=expand_ratio, norm=norm, act_func=act_func)
                block = ResidualBlock(block, IdentityLayer() if stride == 1 else None)
                stage.append(block)
                in_channels = w
            stages.append(OpSequential(stage))
            self.width_list.append(in_channels)
        for w, d in zip(width_list[3:], depth_list[3:]):
            stage = []
            block = self.build_local_block(in_channels=in_channels, out_channels=w, stride=2, expand_ratio=expand_ratio, norm=norm, act_func=act_func, fewer_norm=True)
            stage.append(ResidualBlock(block, None))
            in_channels = w
            for _ in range(d):
                stage.append(EfficientViTBlock(in_channels=in_channels, dim=dim, expand_ratio=expand_ratio, norm=norm, act_func=act_func))
            stages.append(OpSequential(stage))
            self.width_list.append(in_channels)
        self.stages = nn.ModuleList(stages)

    @staticmethod
    def build_local_block(in_channels: 'int', out_channels: 'int', stride: 'int', expand_ratio: 'float', norm: 'str', act_func: 'str', fewer_norm: 'bool'=False) ->nn.Module:
        if expand_ratio == 1:
            block = DSConv(in_channels=in_channels, out_channels=out_channels, stride=stride, use_bias=(True, False) if fewer_norm else False, norm=(None, norm) if fewer_norm else norm, act_func=(act_func, None))
        else:
            block = MBConv(in_channels=in_channels, out_channels=out_channels, stride=stride, expand_ratio=expand_ratio, use_bias=(True, True, False) if fewer_norm else False, norm=(None, None, norm) if fewer_norm else norm, act_func=(act_func, act_func, None))
        return block

    def forward(self, x: 'torch.Tensor') ->dict[str, torch.Tensor]:
        output_dict = {'input': x}
        output_dict['stage0'] = x = self.input_stem(x)
        for stage_id, stage in enumerate(self.stages, 1):
            output_dict['stage%d' % stage_id] = x = stage(x)
        output_dict['stage_final'] = x
        return output_dict


class FusedMBConv(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size=3, stride=1, mid_channels=None, expand_ratio=6, groups=1, use_bias=False, norm=('bn2d', 'bn2d'), act_func=('relu6', None)):
        super().__init__()
        use_bias = val2tuple(use_bias, 2)
        norm = val2tuple(norm, 2)
        act_func = val2tuple(act_func, 2)
        mid_channels = mid_channels or round(in_channels * expand_ratio)
        self.spatial_conv = ConvLayer(in_channels, mid_channels, kernel_size, stride, groups=groups, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0])
        self.point_conv = ConvLayer(mid_channels, out_channels, 1, use_bias=use_bias[1], norm=norm[1], act_func=act_func[1])

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x = self.spatial_conv(x)
        x = self.point_conv(x)
        return x


class ResBlock(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size=3, stride=1, mid_channels=None, expand_ratio=1, use_bias=False, norm=('bn2d', 'bn2d'), act_func=('relu6', None)):
        super().__init__()
        use_bias = val2tuple(use_bias, 2)
        norm = val2tuple(norm, 2)
        act_func = val2tuple(act_func, 2)
        mid_channels = mid_channels or round(in_channels * expand_ratio)
        self.conv1 = ConvLayer(in_channels, mid_channels, kernel_size, stride, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0])
        self.conv2 = ConvLayer(mid_channels, out_channels, kernel_size, 1, use_bias=use_bias[1], norm=norm[1], act_func=act_func[1])

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x = self.conv1(x)
        x = self.conv2(x)
        return x


class EfficientViTLargeBackbone(nn.Module):

    def __init__(self, width_list: 'list[int]', depth_list: 'list[int]', in_channels: 'int'=3, qkv_dim: 'int'=32, norm: 'str'='bn2d', act_func: 'str'='gelu') ->None:
        super().__init__()
        self.width_list = []
        stages = []
        stage0 = [ConvLayer(in_channels=in_channels, out_channels=width_list[0], stride=2, norm=norm, act_func=act_func)]
        for _ in range(depth_list[0]):
            block = self.build_local_block(stage_id=0, in_channels=width_list[0], out_channels=width_list[0], stride=1, expand_ratio=1, norm=norm, act_func=act_func)
            stage0.append(ResidualBlock(block, IdentityLayer()))
        in_channels = width_list[0]
        stages.append(OpSequential(stage0))
        self.width_list.append(in_channels)
        for stage_id, (w, d) in enumerate(zip(width_list[1:4], depth_list[1:4]), start=1):
            stage = []
            for i in range(d + 1):
                stride = 2 if i == 0 else 1
                block = self.build_local_block(stage_id=stage_id, in_channels=in_channels, out_channels=w, stride=stride, expand_ratio=4 if stride == 1 else 16, norm=norm, act_func=act_func, fewer_norm=stage_id > 2)
                block = ResidualBlock(block, IdentityLayer() if stride == 1 else None)
                stage.append(block)
                in_channels = w
            stages.append(OpSequential(stage))
            self.width_list.append(in_channels)
        for stage_id, (w, d) in enumerate(zip(width_list[4:], depth_list[4:]), start=4):
            stage = []
            block = self.build_local_block(stage_id=stage_id, in_channels=in_channels, out_channels=w, stride=2, expand_ratio=24, norm=norm, act_func=act_func, fewer_norm=True)
            stage.append(ResidualBlock(block, None))
            in_channels = w
            for _ in range(d):
                stage.append(EfficientViTBlock(in_channels=in_channels, dim=qkv_dim, expand_ratio=6, norm=norm, act_func=act_func))
            stages.append(OpSequential(stage))
            self.width_list.append(in_channels)
        self.stages = nn.ModuleList(stages)

    @staticmethod
    def build_local_block(stage_id: 'int', in_channels: 'int', out_channels: 'int', stride: 'int', expand_ratio: 'float', norm: 'str', act_func: 'str', fewer_norm: 'bool'=False) ->nn.Module:
        if expand_ratio == 1:
            block = ResBlock(in_channels=in_channels, out_channels=out_channels, stride=stride, use_bias=(True, False) if fewer_norm else False, norm=(None, norm) if fewer_norm else norm, act_func=(act_func, None))
        elif stage_id <= 2:
            block = FusedMBConv(in_channels=in_channels, out_channels=out_channels, stride=stride, expand_ratio=expand_ratio, use_bias=(True, False) if fewer_norm else False, norm=(None, norm) if fewer_norm else norm, act_func=(act_func, None))
        else:
            block = MBConv(in_channels=in_channels, out_channels=out_channels, stride=stride, expand_ratio=expand_ratio, use_bias=(True, True, False) if fewer_norm else False, norm=(None, None, norm) if fewer_norm else norm, act_func=(act_func, act_func, None))
        return block

    def forward(self, x: 'torch.Tensor') ->dict[str, torch.Tensor]:
        output_dict = {'input': x}
        for stage_id, stage in enumerate(self.stages):
            output_dict['stage%d' % stage_id] = x = stage(x)
        output_dict['stage_final'] = x
        return output_dict


class StemBlock(Module):

    def __init__(self, in_channels: 'int', mid_channels: 'int', out_channels: 'int') ->None:
        super().__init__()
        self.stem1 = ConvNormAct(in_channels, mid_channels, 3, 2)
        self.stem2a = ConvNormAct(mid_channels, mid_channels // 2, 2)
        self.stem2b = ConvNormAct(mid_channels // 2, mid_channels, 2)
        self.stem3 = ConvNormAct(mid_channels * 2, mid_channels, 3, 2)
        self.stem4 = ConvNormAct(mid_channels, out_channels, 1)
        self.pool = nn.Sequential(nn.ZeroPad2d((0, 1, 0, 1)), nn.MaxPool2d(2, 1))

    def forward(self, x: 'Tensor') ->Tensor:
        x = self.stem1(x)
        x = concatenate([self.pool(x), self.stem2b(self.stem2a(x))], 1)
        x = self.stem4(self.stem3(x))
        return x


class LightConvNormAct(nn.Sequential):

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'int') ->None:
        super().__init__()
        self.conv1 = ConvNormAct(in_channels, out_channels, 1, act='none')
        self.conv2 = ConvNormAct(out_channels, out_channels, kernel_size, groups=out_channels)


class HGBlock(Module):

    def __init__(self, in_channels: 'int', config: 'StageConfig', identity: 'bool') ->None:
        super().__init__()
        self.identity = identity
        layer_cls = LightConvNormAct if config.light_block else ConvNormAct
        self.layers = nn.ModuleList()
        for i in range(config.layer_num):
            ch_in = in_channels if i == 0 else config.mid_channels
            self.layers.append(layer_cls(ch_in, config.mid_channels, config.kernel_size))
        total_channels = in_channels + config.mid_channels * config.layer_num
        self.aggregation_squeeze_conv = ConvNormAct(total_channels, config.out_channels // 2, 1)
        self.aggregation_excitation_conv = ConvNormAct(config.out_channels // 2, config.out_channels, 1)

    def forward(self, x: 'Tensor') ->Tensor:
        feats = [x]
        for layer in self.layers:
            feats.append(layer(feats[-1]))
        out = concatenate(feats, 1)
        out = self.aggregation_squeeze_conv(out)
        out = self.aggregation_excitation_conv(out)
        return x + out if self.identity else out


class HGStage(nn.Sequential):

    def __init__(self, config: 'StageConfig') ->None:
        super().__init__()
        ch_in = config.in_channels
        self.downsample = ConvNormAct(ch_in, ch_in, 3, 2, 'none', ch_in) if config.downsample else None
        self.blocks = nn.Sequential(HGBlock(ch_in, config, False), *[HGBlock(config.out_channels, config, True) for _ in range(config.num_blocks - 1)])


class StageConfig(NamedTuple):
    in_channels: 'int'
    mid_channels: 'int'
    out_channels: 'int'
    num_blocks: 'int'
    downsample: 'bool'
    light_block: 'bool'
    kernel_size: 'int'
    layer_num: 'int'


class RepVggBlock(Module):

    def __init__(self, in_channels: 'int', out_channels: 'int') ->None:
        super().__init__()
        self.conv1 = ConvNormAct(in_channels, out_channels, 3, act='none')
        self.conv2 = ConvNormAct(in_channels, out_channels, 1, act='none')
        self.act = nn.SiLU(inplace=True)
        self.conv: 'Optional[nn.Conv2d]' = None

    def forward(self, x: 'Tensor') ->Tensor:
        if self.conv is not None:
            out = self.act(self.conv(x))
        else:
            out = self.act(self.conv1(x) + self.conv2(x))
        return out

    @torch.no_grad()
    def optimize_for_deployment(self) ->None:

        def _fuse_conv_bn_weights(m: 'ConvNormAct') ->tuple[nn.Parameter, nn.Parameter]:
            if m.norm.running_mean is None or m.norm.running_var is None:
                raise ValueError
            return fuse_conv_bn_weights(m.conv.weight, m.conv.bias, m.norm.running_mean, m.norm.running_var, m.norm.eps, m.norm.weight, m.norm.bias)
        kernel3x3, bias3x3 = _fuse_conv_bn_weights(self.conv1)
        kernel1x1, bias1x1 = _fuse_conv_bn_weights(self.conv2)
        kernel3x3.add_(pad(kernel1x1, [1, 1, 1, 1]))
        bias3x3.add_(bias1x1)
        self.conv = nn.Conv2d(kernel3x3.shape[1], kernel3x3.shape[0], 3, 1, 1)
        self.conv.weight = kernel3x3
        self.conv.bias = bias3x3


class CSPRepLayer(Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', num_blocks: 'int', expansion: 'float'=1.0) ->None:
        super().__init__()
        hidden_channels = int(out_channels * expansion)
        self.conv1 = ConvNormAct(in_channels, hidden_channels, 1, act='silu')
        self.conv2 = ConvNormAct(in_channels, hidden_channels, 1, act='silu')
        self.bottlenecks = nn.Sequential(*[RepVggBlock(hidden_channels, hidden_channels) for _ in range(num_blocks)])
        self.conv3 = ConvNormAct(hidden_channels, out_channels, 1, act='silu') if hidden_channels != out_channels else nn.Identity()

    def forward(self, x: 'Tensor') ->Tensor:
        return self.conv3(self.bottlenecks(self.conv1(x)) + self.conv2(x))


class AIFI(Module):

    def __init__(self, embed_dim: 'int', num_heads: 'int', dim_feedforward: 'int', dropout: 'float'=0.0) ->None:
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout)
        self.linear1 = nn.Linear(embed_dim, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, embed_dim)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.act = nn.GELU()

    def forward(self, x: 'Tensor') ->Tensor:
        N, C, H, W = x.shape
        x = x.permute(2, 3, 0, 1).flatten(0, 1)
        pos_emb = self.build_2d_sincos_pos_emb(W, H, C, device=x.device, dtype=x.dtype)
        q = k = x + pos_emb
        attn, _ = self.self_attn(q, k, x, need_weights=True)
        x = self.norm1(x + self.dropout1(attn))
        x = self.norm2(x + self.dropout2(self.ffn(x)))
        x = x.view(H, W, N, C).permute(2, 3, 0, 1)
        return x

    def ffn(self, x: 'Tensor') ->Tensor:
        return self.linear2(self.dropout(self.act(self.linear1(x))))

    @staticmethod
    def build_2d_sincos_pos_emb(w: 'int', h: 'int', embed_dim: 'int', temp: 'float'=10000.0, device: 'Optional[torch.device]'=None, dtype: 'Optional[torch.dtype]'=None) ->Tensor:
        """Construct 2D sin-cos positional embeddings.

        Args:
            w: width of the image or feature map
            h: height of the image or feature map
            embed_dim: embedding dimension
            temp: temperature coefficient
            device: device to place the positional embeddings
            dtype: data type of the positional embeddings

        Returns:
            positional embeddings, shape :math:`(H * W, 1, C)`
        """
        xs = torch.arange(w, device=device, dtype=dtype)
        ys = torch.arange(h, device=device, dtype=dtype)
        grid_x, grid_y = torch_meshgrid([xs, ys], indexing='ij')
        pos_dim = embed_dim // 4
        omega = torch.arange(pos_dim, device=device, dtype=dtype) / pos_dim
        omega = 1.0 / temp ** omega
        out_x = grid_x.reshape(-1, 1) * omega.view(1, -1)
        out_y = grid_y.reshape(-1, 1) * omega.view(1, -1)
        pos_emb = concatenate([out_x.sin(), out_x.cos(), out_y.sin(), out_y.cos()], 1)
        return pos_emb.unsqueeze(1)


class FeedForward(Module):

    def __init__(self, dim: 'int', hidden_dim: 'int', dropout: 'float'=0.0) ->None:
        super().__init__()
        self.net = nn.Sequential(nn.Linear(dim, hidden_dim), nn.SiLU(), nn.Dropout(dropout), nn.Linear(hidden_dim, dim), nn.Dropout(dropout))

    def forward(self, x: 'Tensor') ->Tensor:
        return self.net(x)


class MultiHeadAttention(Module):

    def __init__(self, emb_size: 'int', num_heads: 'int', att_drop: 'float', proj_drop: 'float') ->None:
        super().__init__()
        self.emb_size = emb_size
        self.num_heads = num_heads
        head_size = emb_size // num_heads
        self.scale = head_size ** -0.5
        if self.emb_size % self.num_heads:
            raise ValueError(f'Size of embedding inside the transformer decoder must be visible by number of headsfor correct multi-head attention Got: {self.emb_size} embedding size and {self.num_heads} numbers of heads')
        self.qkv = nn.Linear(emb_size, emb_size * 3)
        self.att_drop = nn.Dropout(att_drop)
        self.projection = nn.Linear(emb_size, emb_size)
        self.projection_drop = nn.Dropout(proj_drop)

    def forward(self, x: 'Tensor') ->Tensor:
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        att = torch.einsum('bhqd, bhkd -> bhqk', q, k) * self.scale
        att = att.softmax(dim=-1)
        att = self.att_drop(att)
        out = torch.einsum('bhal, bhlv -> bhav ', att, v)
        out = out.permute(0, 2, 1, 3).contiguous().view(B, N, -1)
        out = self.projection(out)
        out = self.projection_drop(out)
        return out


class ResidualAdd(Module):

    def __init__(self, fn: 'Callable[..., Tensor]') ->None:
        super().__init__()
        self.fn = fn

    def forward(self, x: 'Tensor', **kwargs: Any) ->Tensor:
        res = x
        x = self.fn(x, **kwargs)
        x += res
        return x


class TransformerEncoderBlock(nn.Sequential):

    def __init__(self, embed_dim: 'int', num_heads: 'int', dropout_rate: 'float', dropout_attn: 'float') ->None:
        super().__init__(ResidualAdd(nn.Sequential(nn.LayerNorm(embed_dim, 1e-06), MultiHeadAttention(embed_dim, num_heads, dropout_attn, dropout_rate), nn.Dropout(dropout_rate))), ResidualAdd(nn.Sequential(nn.LayerNorm(embed_dim, 1e-06), FeedForward(embed_dim, embed_dim * 4, embed_dim, dropout_rate=dropout_rate), nn.Dropout(dropout_rate))))


class TransformerEncoder(Module):

    def __init__(self, embed_dim: 'int'=768, depth: 'int'=12, num_heads: 'int'=12, dropout_rate: 'float'=0.0, dropout_attn: 'float'=0.0) ->None:
        super().__init__()
        self.blocks = nn.Sequential(*(TransformerEncoderBlock(embed_dim, num_heads, dropout_rate, dropout_attn) for _ in range(depth)))
        self.results: 'list[Tensor]' = []

    def forward(self, x: 'Tensor') ->Tensor:
        self.results = []
        out = x
        for m in self.blocks.children():
            out = m(out)
            self.results.append(out)
        return out


def _make_shortcut(in_channels: 'int', out_channels: 'int', stride: 'int') ->Module:
    return nn.Sequential(OrderedDict([('pool', nn.AvgPool2d(2, 2)), ('conv', ConvNormAct(in_channels, out_channels, 1, act='none'))])) if stride == 2 else ConvNormAct(in_channels, out_channels, 1, act='none')


class BasicBlockD(Module):
    expansion = 1

    def __init__(self, in_channels: 'int', out_channels: 'int', stride: 'int', shortcut: 'bool') ->None:
        KORNIA_CHECK(stride in {1, 2})
        super().__init__()
        self.convs = nn.Sequential(OrderedDict([('branch2a', ConvNormAct(in_channels, out_channels, 3, stride=stride)), ('branch2b', ConvNormAct(out_channels, out_channels, 3, act='none'))]))
        self.short = nn.Identity() if shortcut else _make_shortcut(in_channels, out_channels, stride)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x: 'Tensor') ->Tensor:
        return self.relu(self.convs(x) + self.short(x))


class BottleneckD(Module):
    expansion = 4

    def __init__(self, in_channels: 'int', out_channels: 'int', stride: 'int', shortcut: 'bool') ->None:
        KORNIA_CHECK(stride in {1, 2})
        super().__init__()
        expanded_out_channels = out_channels * self.expansion
        self.convs = nn.Sequential(OrderedDict([('branch2a', ConvNormAct(in_channels, out_channels, 1)), ('branch2b', ConvNormAct(out_channels, out_channels, 3, stride=stride)), ('branch2c', ConvNormAct(out_channels, expanded_out_channels, 1, act='none'))]))
        self.short = nn.Identity() if shortcut else _make_shortcut(in_channels, expanded_out_channels, stride)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x: 'Tensor') ->Tensor:
        return self.relu(self.convs(x) + self.short(x))


einsum = torch.einsum


class Attention(Module):

    def __init__(self, allow_flash: 'bool') ->None:
        super().__init__()
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn('FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.', stacklevel=2)
        self.enable_flash = allow_flash and FLASH_AVAILABLE
        self.has_sdp = hasattr(F, 'scaled_dot_product_attention')
        if allow_flash and FlashCrossAttention:
            self.flash_ = FlashCrossAttention()
        if self.has_sdp:
            torch.backends.cuda.enable_flash_sdp(allow_flash)

    def forward(self, q: 'Tensor', k: 'Tensor', v: 'Tensor', mask: 'Optional[Tensor]'=None) ->Tensor:
        if self.enable_flash and q.device.type == 'cuda':
            if self.has_sdp:
                args = [x.half().contiguous() for x in [q, k, v]]
                v = F.scaled_dot_product_attention(*args, attn_mask=mask)
                return v if mask is None else v.nan_to_num()
            else:
                KORNIA_CHECK(mask is None)
                q, k, v = (x.transpose(-2, -3).contiguous() for x in [q, k, v])
                m = self.flash_(q.half(), stack([k, v], 2).half())
                return m.transpose(-2, -3).clone()
        elif self.has_sdp:
            args = [x.contiguous() for x in [q, k, v]]
            v = F.scaled_dot_product_attention(*args, attn_mask=mask)
            return v if mask is None else v.nan_to_num()
        else:
            s = q.shape[-1] ** -0.5
            sim = einsum('...id,...jd->...ij', q, k) * s
            if mask is not None:
                sim.masked_fill(~mask, -float('inf'))
            attn = F.softmax(sim, -1)
            return einsum('...ij,...jd->...id', attn, v)


class LayerScale(nn.Module):

    def __init__(self, dim: 'int', init_values: 'Union[float, Tensor]'=1e-05, inplace: 'bool'=False) ->None:
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(dim))

    def forward(self, x: 'Tensor') ->Tensor:
        return x.mul_(self.gamma) if self.inplace else x * self.gamma


class Mlp(nn.Module):

    def __init__(self, in_features: 'int', hidden_features: 'Optional[int]'=None, out_features: 'Optional[int]'=None, act_layer: 'Callable[..., nn.Module]'=nn.GELU, drop: 'float'=0.0, bias: 'bool'=True) ->None:
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)
        self.drop = nn.Dropout(drop)

    def forward(self, x: 'Tensor') ->Tensor:
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def drop_add_residual_stochastic_depth(x: 'Tensor', residual_func: 'Callable[[Tensor], Tensor]', sample_drop_ratio: 'float'=0.0) ->Tensor:
    b, n, d = x.shape
    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)
    brange = torch.randperm(b, device=x.device)[:sample_subset_size]
    x_subset = x[brange]
    residual = residual_func(x_subset)
    x_flat = x.flatten(1)
    residual = residual.flatten(1)
    residual_scale_factor = b / sample_subset_size
    x_plus_residual = torch.index_add(x_flat, 0, brange, residual, alpha=residual_scale_factor)
    return x_plus_residual.view_as(x)


class Block(nn.Module):

    def __init__(self, dim: 'int', num_heads: 'int', mlp_ratio: 'float'=4.0, qkv_bias: 'bool'=False, proj_bias: 'bool'=True, ffn_bias: 'bool'=True, drop: 'float'=0.0, attn_drop: 'float'=0.0, init_values=None, drop_path: 'float'=0.0, act_layer: 'Callable[..., nn.Module]'=nn.GELU, norm_layer: 'Callable[..., nn.Module]'=nn.LayerNorm, attn_class: 'Callable[..., nn.Module]'=Attention, ffn_layer: 'Callable[..., nn.Module]'=Mlp) ->None:
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = attn_class(dim, num_heads=num_heads, qkv_bias=qkv_bias, proj_bias=proj_bias, attn_drop=attn_drop, proj_drop=drop)
        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = ffn_layer(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, bias=ffn_bias)
        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.sample_drop_ratio = drop_path

    def forward(self, x: 'Tensor') ->Tensor:

        def attn_residual_func(x: 'Tensor') ->Tensor:
            return self.ls1(self.attn(self.norm1(x)))

        def ffn_residual_func(x: 'Tensor') ->Tensor:
            return self.ls2(self.mlp(self.norm2(x)))
        if self.training and self.sample_drop_ratio > 0.1:
            x = drop_add_residual_stochastic_depth(x, residual_func=attn_residual_func, sample_drop_ratio=self.sample_drop_ratio)
            x = drop_add_residual_stochastic_depth(x, residual_func=ffn_residual_func, sample_drop_ratio=self.sample_drop_ratio)
        elif self.training and self.sample_drop_ratio > 0.0:
            x = x + self.drop_path1(attn_residual_func(x))
            x = x + self.drop_path1(ffn_residual_func(x))
        else:
            x = x + attn_residual_func(x)
            x = x + ffn_residual_func(x)
        return x


def _deformable_attention_kernel(value: 'Tensor', value_spatial_shapes: 'list[tuple[int, int]]', sampling_locations: 'Tensor', attention_weights: 'Tensor') ->Tensor:
    """Deformable Attention Kernel used in Deformable DETR.

    Described in https://arxiv.org/abs/2010.04159.

    Args:
        value: shape (N, Lv, n_head * C)
        value_spatial_shapes: [(H0, W0), (H1, W1), ...]
        sampling_locations: shape (N, Lq, n_head, n_levels, n_points, 2)
        attention_weights: shape (N, Lq, n_head, n_levels, n_points)

    Returns:
        output, shape (N, Lq, n_head * C)
    """
    bs, _, n_head, c = value.shape
    _, Len_q, _, n_levels, n_points, _ = sampling_locations.shape
    split_shape: 'list[int]' = [(h * w) for h, w in value_spatial_shapes]
    value_list = value.split(split_shape, dim=1)
    sampling_grids = 2 * sampling_locations - 1
    sampling_value_list: 'list[Tensor]' = []
    for level, (h, w) in enumerate(value_spatial_shapes):
        value_l_ = value_list[level].flatten(2).permute(0, 2, 1).reshape(bs * n_head, c, h, w)
        sampling_grid_l_ = sampling_grids[:, :, :, level].permute(0, 2, 1, 3, 4).flatten(0, 1)
        sampling_value_l_ = torch.nn.functional.grid_sample(value_l_, sampling_grid_l_, mode='bilinear', padding_mode='zeros', align_corners=False)
        sampling_value_list.append(sampling_value_l_)
    attention_weights = attention_weights.permute(0, 2, 1, 3, 4).reshape(bs * n_head, 1, Len_q, n_levels * n_points)
    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights).sum(-1).reshape(bs, n_head * c, Len_q)
    return output.permute(0, 2, 1)


class MultiScaleDeformableAttention(Module):
    """Multi-scale Deformable Attention used in Deformable DETR.

    Described in https://arxiv.org/abs/2010.04159.
    """

    def __init__(self, embed_dim: 'int', num_heads: 'int', num_levels: 'int', num_points: 'int') ->None:
        super().__init__()
        self.num_heads = num_heads
        self.num_levels = num_levels
        self.num_points = num_points
        self.total_points = num_heads * num_levels * num_points
        self.head_dim = embed_dim // num_heads
        self.sampling_offsets = nn.Linear(embed_dim, self.total_points * 2)
        self.attention_weights = nn.Linear(embed_dim, self.total_points)
        self.value_proj = nn.Linear(embed_dim, embed_dim)
        self.output_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, query: 'Tensor', reference_points: 'Tensor', value: 'Tensor', value_spatial_shapes: 'list[tuple[int, int]]') ->Tensor:
        """
        Args:
            query: shape (N, Lq, C)
            reference_points: shape (N, Lq, n_levels, 4)
            value: shape (N, Lv, C)
            value_spatial_shapes: [(H0, W0), (H1, W1), ...]

        Returns:
            output, shape (N, Lq, C)
        """
        N, Lenq, _ = query.shape
        _, Len_v, _ = value.shape
        sampling_offsets = self.sampling_offsets(query).reshape(N, Lenq, self.num_heads, self.num_levels, self.num_points, 2)
        attention_weights = self.attention_weights(query).reshape(N, Lenq, self.num_heads, self.num_levels * self.num_points)
        attention_weights = attention_weights.softmax(-1).reshape(N, Lenq, self.num_heads, self.num_levels, self.num_points)
        reference_points_cxcy = reference_points[:, :, None, :, None, :2]
        reference_points_wh = reference_points[:, :, None, :, None, 2:]
        sampling_locations = reference_points_cxcy + sampling_offsets / self.num_points * reference_points_wh * 0.5
        value_buf = self.value_proj(value).reshape(N, Len_v, self.num_heads, self.head_dim)
        out = _deformable_attention_kernel(value_buf, value_spatial_shapes, sampling_locations, attention_weights)
        out = self.output_proj(out)
        return out


class TransformerDecoderLayer(Module):
    """Deformable Transformer Decoder layer used in Deformable DETR.

    Described in: https://arxiv.org/abs/2010.04159.
    """

    def __init__(self, embed_dim: 'int', num_heads: 'int', dropout: 'float', num_levels: 'int', num_points: 'int') ->None:
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True)
        self.dropout1 = nn.Dropout(dropout)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.cross_attn = MultiScaleDeformableAttention(embed_dim, num_heads, num_levels, num_points)
        self.dropout2 = nn.Dropout(dropout)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.linear1 = nn.Linear(embed_dim, embed_dim * 4)
        self.activation = nn.ReLU(inplace=True)
        self.dropout3 = nn.Dropout(dropout)
        self.linear2 = nn.Linear(embed_dim * 4, embed_dim)
        self.dropout4 = nn.Dropout(dropout)
        self.norm3 = nn.LayerNorm(embed_dim)

    def _ffn(self, x: 'Tensor') ->Tensor:
        return self.linear2(self.dropout3(self.activation(self.linear1(x))))

    def forward(self, tgt: 'Tensor', ref_points: 'Tensor', memory: 'Tensor', memory_spatial_shapes: 'list[tuple[int, int]]', memory_level_start_index: 'Optional[list[int]]'=None, attn_mask: 'Optional[Tensor]'=None, memory_mask: 'Optional[Tensor]'=None, query_pos_embed: 'Optional[Tensor]'=None) ->Tensor:
        q = k = tgt + query_pos_embed
        out, _ = self.self_attn(q, k, value=tgt)
        tgt = self.norm1(tgt + self.dropout1(out))
        out = self.cross_attn(tgt + query_pos_embed, ref_points, memory, memory_spatial_shapes)
        tgt = self.norm2(tgt + self.dropout2(out))
        out = self.norm3(tgt + self.dropout4(self._ffn(tgt)))
        return out


def _inverse_sigmoid(x: 'torch.Tensor', eps: 'float'=1e-05) ->torch.Tensor:
    """Inverse sigmoid function.

    Args:
        x: input tensor
        eps: epsilon value for numerical stability

    Returns:
        output tensor
    """
    out = x.clip(min=0.0, max=1.0)
    return torch.log(out.clip(min=eps) / (1.0 - out).clip(min=eps))


class TransformerDecoder(Module):

    def __init__(self, hidden_dim: 'int', decoder_layer: 'nn.Module', num_layers: 'int', eval_idx: 'int'=-1) ->None:
        super().__init__()
        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.eval_idx = eval_idx if eval_idx >= 0 else num_layers + eval_idx

    def forward(self, tgt: 'Tensor', ref_points_unact: 'Tensor', memory: 'Tensor', memory_spatial_shapes: 'list[tuple[int, int]]', memory_level_start_index: 'list[int]', bbox_head: 'nn.ModuleList', score_head: 'nn.ModuleList', query_pos_head: 'nn.Module', attn_mask: 'Optional[Tensor]'=None, memory_mask: 'Optional[Tensor]'=None) ->tuple[Tensor, Tensor]:
        output: 'Tensor' = tgt
        dec_out_bboxes: 'list[Tensor]' = []
        dec_out_logits: 'list[Tensor]' = []
        ref_points_detach = torch.sigmoid(ref_points_unact)
        for i, layer in enumerate(self.layers):
            ref_points_input = ref_points_detach.unsqueeze(2)
            query_pos_embed: 'Tensor' = query_pos_head(ref_points_detach)
            output = layer(output, ref_points_input, memory, memory_spatial_shapes, memory_level_start_index, attn_mask, memory_mask, query_pos_embed)
            inter_ref_bbox = torch.sigmoid(bbox_head[i](output) + _inverse_sigmoid(ref_points_detach))
            if i == self.eval_idx:
                dec_out_logits.append(score_head[i](output))
                dec_out_bboxes.append(inter_ref_bbox)
                break
            ref_points_detach = inter_ref_bbox
        return torch.stack(dec_out_bboxes), torch.stack(dec_out_logits)


def mod(a: 'Tensor', b: 'int') ->Tensor:
    """Compute the modulo operation for two numbers.

    This function calculates the remainder of the division of 'a' by 'b'
    using the formula: a - (a // b) * b, which is equivalent to the modulo operation.

    Args:
        a: The dividend.
        b: The divisor.

    Returns:
        The remainder of a divided by b.

    Example:
        >>> mod(7, 3)
        1
    """
    return a - a // b * b


class MLPBlock(Module):

    def __init__(self, embedding_dim: 'int', mlp_dim: 'int', act: 'type[Module]'=nn.GELU) ->None:
        super().__init__()
        self.lin1 = nn.Linear(embedding_dim, mlp_dim)
        self.lin2 = nn.Linear(mlp_dim, embedding_dim)
        self.act = act()

    def forward(self, x: 'Tensor') ->Tensor:
        return self.lin2(self.act(self.lin1(x)))


class LayerNorm(nn.LayerNorm):

    def __init__(self, *args: Any, **kwargs: Any) ->None:
        super().__init__(*args, **kwargs)
        self.eps = 1e-06


def make_2tuple(x):
    if isinstance(x, tuple):
        KORNIA_CHECK(len(x) == 2)
        return x
    KORNIA_CHECK(isinstance(x, int))
    return x, x


class PatchEmbed(nn.Module):
    """
    2D image to patch embedding: (B,C,H,W) -> (B,N,D)

    Args:
        img_size: Image size.
        patch_size: Patch token size.
        in_chans: Number of input image channels.
        embed_dim: Number of linear projection output channels.
        norm_layer: Normalization layer.
    """

    def __init__(self, img_size: 'Union[int, Tuple[int, int]]'=224, patch_size: 'Union[int, Tuple[int, int]]'=16, in_chans: 'int'=3, embed_dim: 'int'=768, norm_layer: 'Optional[Callable]'=None, flatten_embedding: 'bool'=True) ->None:
        super().__init__()
        image_HW = make_2tuple(img_size)
        patch_HW = make_2tuple(patch_size)
        patch_grid_size = image_HW[0] // patch_HW[0], image_HW[1] // patch_HW[1]
        self.img_size = image_HW
        self.patch_size = patch_HW
        self.patches_resolution = patch_grid_size
        self.num_patches = patch_grid_size[0] * patch_grid_size[1]
        self.in_chans = in_chans
        self.embed_dim = embed_dim
        self.flatten_embedding = flatten_embedding
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_HW, stride=patch_HW)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x: 'Tensor') ->Tensor:
        _, _, H, W = x.shape
        patch_H, patch_W = self.patch_size
        KORNIA_CHECK(H % patch_H == 0, f'Input image height {H} is not a multiple of patch height {patch_H}')
        KORNIA_CHECK(W % patch_W == 0, f'Input image width {W} is not a multiple of patch width: {patch_W}')
        x = self.proj(x)
        H, W = x.size(2), x.size(3)
        x = x.flatten(2).transpose(1, 2)
        x = self.norm(x)
        if not self.flatten_embedding:
            x = x.reshape(-1, H, W, self.embed_dim)
        return x

    def flops(self) ->float:
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class ImageEncoderViT(Module):

    def __init__(self, img_size: 'int'=1024, *, patch_size: int=16, in_chans: int=3, embed_dim: int=768, depth: int=12, num_heads: int=12, mlp_ratio: float=4.0, out_chans: int=256, qkv_bias: bool=True, norm_layer: type[Module]=nn.LayerNorm, act_layer: type[Module]=nn.GELU, use_abs_pos: bool=True, use_rel_pos: bool=False, rel_pos_zero_init: bool=True, window_size: int=0, global_attn_indexes: tuple[int, ...]=()) ->None:
        """
        Args:
            img_size: Input image size.
            patch_size: Patch size.
            in_chans: Number of input image channels.
            embed_dim: Patch embedding dimension.
            depth: Depth of ViT.
            num_heads: Number of attention heads in each ViT block.
            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
            qkv_bias: If True, add a learnable bias to query, key, value.
            norm_layer: Normalization layer.
            act_layer: Activation layer.
            use_abs_pos: If True, use absolute positional embeddings.
            use_rel_pos: If True, add relative positional embeddings to the attention map.
            rel_pos_zero_init: If True, zero initialize relative positional parameters.
            window_size: Window size for window attention blocks.
            global_attn_indexes: Indexes for blocks using global attention.
        """
        super().__init__()
        self.img_size = img_size
        self.patch_embed = PatchEmbed(kernel_size=(patch_size, patch_size), stride=(patch_size, patch_size), in_chans=in_chans, embed_dim=embed_dim)
        self.pos_embed: 'Optional[nn.Parameter]' = None
        if use_abs_pos:
            self.pos_embed = nn.Parameter(zeros(1, img_size // patch_size, img_size // patch_size, embed_dim))
        self.blocks = nn.ModuleList()
        for i in range(depth):
            block = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, norm_layer=norm_layer, act_layer=act_layer, use_rel_pos=use_rel_pos, rel_pos_zero_init=rel_pos_zero_init, window_size=window_size if i not in global_attn_indexes else 0, input_size=(img_size // patch_size, img_size // patch_size))
            self.blocks.append(block)
        self.neck = nn.Sequential(nn.Conv2d(embed_dim, out_chans, kernel_size=1, bias=False), LayerNorm2d(out_chans), nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=False), LayerNorm2d(out_chans))

    def forward(self, x: 'Tensor') ->Tensor:
        x = self.patch_embed(x)
        if self.pos_embed is not None:
            x = x + self.pos_embed
        for blk in self.blocks:
            x = blk(x)
        x = self.neck(x.permute(0, 3, 1, 2))
        return x


class MaskDecoder(Module):

    def __init__(self, *, transformer_dim: int, transformer: Module, num_multimask_outputs: int=3, activation: type[Module]=nn.GELU, iou_head_depth: int=3, iou_head_hidden_dim: int=256) ->None:
        """Predicts masks given an image and prompt embeddings, using a transformer architecture.

        Args:
            transformer_dim: the channel dimension of the transformer
            transformer: the transformer used to predict masks
            num_multimask_outputs: the number of masks to predict when disambiguating masks
            activation: the type of activation to use when upscaling masks
            iou_head_depth: the depth of the MLP used to predict mask quality
            iou_head_hidden_dim: the hidden dimension of the MLP used to predict mask quality
        """
        super().__init__()
        self.transformer_dim = transformer_dim
        self.transformer = transformer
        self.num_multimask_outputs = num_multimask_outputs
        self.iou_token = nn.Embedding(1, transformer_dim)
        self.num_mask_tokens = num_multimask_outputs + 1
        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)
        self.output_upscaling = nn.Sequential(nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2), LayerNorm2d(transformer_dim // 4), activation(), nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2), activation())
        self.output_hypernetworks_mlps = nn.ModuleList([MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3) for i in range(self.num_mask_tokens)])
        self.iou_prediction_head = MLP(transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth)

    def forward(self, image_embeddings: 'Tensor', image_pe: 'Tensor', sparse_prompt_embeddings: 'Tensor', dense_prompt_embeddings: 'Tensor', multimask_output: 'bool') ->tuple[Tensor, Tensor]:
        """Predict masks given image and prompt embeddings.

        Args:
            image_embeddings: the embeddings from the image encoder
            image_pe: positional encoding with the shape of image_embeddings
            sparse_prompt_embeddings: the embeddings of the points and boxes
            dense_prompt_embeddings: the embeddings of the mask inputs
            multimask_output: Whether to return multiple masks or a single mask.

        Returns:
            batched predicted masks
            batched predictions of mask quality
        """
        masks, iou_pred = self.predict_masks(image_embeddings=image_embeddings, image_pe=image_pe, sparse_prompt_embeddings=sparse_prompt_embeddings, dense_prompt_embeddings=dense_prompt_embeddings)
        if multimask_output:
            mask_slice = slice(1, None)
        else:
            mask_slice = slice(0, 1)
        masks = masks[:, mask_slice, :, :]
        iou_pred = iou_pred[:, mask_slice]
        return masks, iou_pred

    def predict_masks(self, image_embeddings: 'Tensor', image_pe: 'Tensor', sparse_prompt_embeddings: 'Tensor', dense_prompt_embeddings: 'Tensor') ->tuple[Tensor, Tensor]:
        """Predicts masks.

        See 'forward' for more details.
        """
        output_tokens = concatenate([self.iou_token.weight, self.mask_tokens.weight], dim=0)
        output_tokens = output_tokens[None, ...].expand(sparse_prompt_embeddings.size(0), -1, -1)
        tokens = concatenate((output_tokens, sparse_prompt_embeddings), dim=1)
        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)
        src = src + dense_prompt_embeddings
        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)
        b, c, h, w = src.shape
        hs, src = self.transformer(src, pos_src, tokens)
        iou_token_out = hs[:, 0, :]
        mask_tokens_out = hs[:, 1:1 + self.num_mask_tokens, :]
        src = src.transpose(1, 2).view(b, c, h, w)
        upscaled_embedding = self.output_upscaling(src)
        hyper_in_list: 'list[Tensor]' = []
        for i in range(self.num_mask_tokens):
            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))
        hyper_in = stack(hyper_in_list, dim=1)
        b, c, h, w = upscaled_embedding.shape
        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)
        iou_pred = self.iou_prediction_head(iou_token_out)
        return masks, iou_pred


class PositionEmbeddingRandom(Module):
    """Positional encoding using random spatial frequencies."""

    def __init__(self, num_pos_feats: 'int'=64, scale: 'Optional[float]'=None) ->None:
        super().__init__()
        if scale is None or scale <= 0.0:
            scale = 1.0
        self.register_buffer('positional_encoding_gaussian_matrix', scale * torch.randn((2, num_pos_feats)))

    def _pe_encoding(self, coords: 'Tensor') ->Tensor:
        """Positionally encode points that are normalized to [0,1]."""
        coords = 2 * coords - 1
        coords = coords @ self.positional_encoding_gaussian_matrix
        coords = 2 * pi * coords
        return concatenate([sin(coords), cos(coords)], dim=-1)

    def forward(self, size: 'tuple[int, int]') ->Tensor:
        """Generate positional encoding for a grid of the specified size."""
        h, w = size
        _dev = self.positional_encoding_gaussian_matrix.device
        device = _dev if isinstance(_dev, torch.device) else None
        grid = torch.ones((h, w), device=device, dtype=torch.float32)
        y_embed = grid.cumsum(dim=0) - 0.5
        x_embed = grid.cumsum(dim=1) - 0.5
        y_embed = y_embed / h
        x_embed = x_embed / w
        pe = self._pe_encoding(stack([x_embed, y_embed], dim=-1))
        return pe.permute(2, 0, 1)

    def forward_with_coords(self, coords_input: 'Tensor', image_size: 'tuple[int, int]') ->Tensor:
        """Positionally encode points that are not normalized to [0,1]."""
        coords = coords_input.clone()
        coords[:, :, 0] = coords[:, :, 0] / image_size[1]
        coords[:, :, 1] = coords[:, :, 1] / image_size[0]
        return self._pe_encoding(coords)


class PromptEncoder(Module):

    def __init__(self, embed_dim: 'int', image_embedding_size: 'tuple[int, int]', input_image_size: 'tuple[int, int]', mask_in_chans: 'int', activation: 'type[Module]'=nn.GELU) ->None:
        """Encodes prompts for input to SAM's mask decoder.

        Args:
            embed_dim: The prompts' embedding dimension
            image_embedding_size: The spatial size of the image embedding, as (H, W).
            input_image_size: The padded size of the image as input to the image encoder, as (H, W).
            mask_in_chans: The number of hidden channels used for encoding input masks.
            activation: The activation to use when encoding input masks.
        """
        super().__init__()
        self.embed_dim = embed_dim
        self.input_image_size = input_image_size
        self.image_embedding_size = image_embedding_size
        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)
        self.num_point_embeddings: 'int' = 4
        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]
        self.point_embeddings = nn.ModuleList(point_embeddings)
        self.not_a_point_embed = nn.Embedding(1, embed_dim)
        self.mask_input_size = 4 * image_embedding_size[0], 4 * image_embedding_size[1]
        self.mask_downscaling = nn.Sequential(nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2), LayerNorm2d(mask_in_chans // 4), activation(), nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2), LayerNorm2d(mask_in_chans), activation(), nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1))
        self.no_mask_embed = nn.Embedding(1, embed_dim)

    def get_dense_pe(self) ->Tensor:
        """Returns the positional encoding used to encode point prompts, applied to a dense set of points the shape
        of the image encoding.

        Returns:
            Positional encoding with shape 1x(embed_dim)x(embedding_h)x(embedding_w)
        """
        return self.pe_layer(self.image_embedding_size)[None, ...]

    def _embed_points(self, points: 'Tensor', labels: 'Tensor', pad: 'bool') ->Tensor:
        """Embeds point prompts."""
        points = points + 0.5
        if pad:
            padding_point = zeros((points.shape[0], 1, 2), device=points.device)
            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)
            points = concatenate([points, padding_point], dim=1)
            labels = concatenate([labels, padding_label], dim=1)
        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)
        point_embedding[labels == -1] = 0.0
        point_embedding[labels == -1] += self.not_a_point_embed.weight
        point_embedding[labels == 0] += self.point_embeddings[0].weight
        point_embedding[labels == 1] += self.point_embeddings[1].weight
        return point_embedding

    def _embed_boxes(self, boxes: 'Tensor') ->Tensor:
        """Embeds box prompts."""
        boxes = boxes + 0.5
        coords = boxes.reshape(-1, 2, 2)
        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)
        corner_embedding[:, 0, :] += self.point_embeddings[2].weight
        corner_embedding[:, 1, :] += self.point_embeddings[3].weight
        return corner_embedding

    def _embed_masks(self, masks: 'Tensor') ->Tensor:
        """Embeds mask inputs."""
        mask_embedding = self.mask_downscaling(masks)
        return mask_embedding

    def _get_batch_size(self, points: 'Optional[tuple[Tensor, Tensor]]', boxes: 'Optional[Tensor]', masks: 'Optional[Tensor]') ->int:
        """Gets the batch size of the output given the batch size of the input prompts."""
        if points is not None:
            return points[0].shape[0]
        elif boxes is not None:
            return boxes.shape[0]
        elif masks is not None:
            return masks.shape[0]
        else:
            return 1

    def _get_device(self) ->Device:
        return self.point_embeddings[0].weight.device

    def forward(self, points: 'Optional[tuple[Tensor, Tensor]]', boxes: 'Optional[Tensor]', masks: 'Optional[Tensor]') ->tuple[Tensor, Tensor]:
        """Embeds different types of prompts, returning both sparse and dense embeddings.

        Args:
            points: point coordinates and labels to embed.
            boxes: boxes to embed
            masks: masks to embed

        Returns:
            - sparse embeddings for the points and boxes, with shape BxNx(embed_dim), where N is determined by the
            number of input points and boxes.
            - dense embeddings for the masks, in the shape Bx(embed_dim)x(embed_H)x(embed_W)
        """
        bs = self._get_batch_size(points, boxes, masks)
        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())
        if points is not None:
            coords, labels = points
            point_embeddings = self._embed_points(coords, labels, pad=boxes is None)
            sparse_embeddings = concatenate([sparse_embeddings, point_embeddings], dim=1)
        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes)
            sparse_embeddings = concatenate([sparse_embeddings, box_embeddings], dim=1)
        if masks is not None:
            dense_embeddings = self._embed_masks(masks)
        else:
            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(bs, -1, self.image_embedding_size[0], self.image_embedding_size[1])
        return sparse_embeddings, dense_embeddings


class TwoWayAttentionBlock(Module):

    def __init__(self, embedding_dim: 'int', num_heads: 'int', mlp_dim: 'int'=2048, activation: 'type[Module]'=nn.ReLU, attention_downsample_rate: 'int'=2, skip_first_layer_pe: 'bool'=False) ->None:
        """A transformer block with four layers: (1) self-attention of sparse inputs, (2) cross attention of sparse
        inputs to dense inputs, (3) mlp block on sparse inputs, and (4) cross attention of dense inputs to sparse
        inputs.

        Args:
            embedding_dim: the channel dimension of the embeddings
            num_heads: the number of heads in the attention layers
            mlp_dim: the hidden dimension of the mlp block
            activation: the activation of the mlp block
            skip_first_layer_pe: skip the PE on the first layer
        """
        super().__init__()
        self.self_attn = Attention(embedding_dim, num_heads)
        self.norm1 = nn.LayerNorm(embedding_dim)
        self.cross_attn_token_to_image = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)
        self.norm2 = nn.LayerNorm(embedding_dim)
        self.mlp = MLPBlock(embedding_dim, mlp_dim, activation)
        self.norm3 = nn.LayerNorm(embedding_dim)
        self.norm4 = nn.LayerNorm(embedding_dim)
        self.cross_attn_image_to_token = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)
        self.skip_first_layer_pe = skip_first_layer_pe

    def forward(self, queries: 'Tensor', keys: 'Tensor', query_pe: 'Tensor', key_pe: 'Tensor') ->tuple[Tensor, Tensor]:
        if self.skip_first_layer_pe:
            queries = self.self_attn(q=queries, k=queries, v=queries)
        else:
            q = queries + query_pe
            attn_out = self.self_attn(q=q, k=q, v=queries)
            queries = queries + attn_out
        queries = self.norm1(queries)
        q = queries + query_pe
        k = keys + key_pe
        attn_out = self.cross_attn_token_to_image(q=q, k=k, v=keys)
        queries = queries + attn_out
        queries = self.norm2(queries)
        mlp_out = self.mlp(queries)
        queries = queries + mlp_out
        queries = self.norm3(queries)
        q = queries + query_pe
        k = keys + key_pe
        attn_out = self.cross_attn_image_to_token(q=k, k=q, v=queries)
        keys = keys + attn_out
        keys = self.norm4(keys)
        return queries, keys


class TwoWayTransformer(Module):

    def __init__(self, depth: 'int', embedding_dim: 'int', num_heads: 'int', mlp_dim: 'int', activation: 'type[Module]'=nn.ReLU, attention_downsample_rate: 'int'=2) ->None:
        """A transformer decoder that attends to an input image using queries whose positional embedding is
        supplied.

        Args:
            depth: number of layers in the transformer
            embedding_dim: the channel dimension for the input embeddings
            num_heads: the number of heads for multihead attention. Must divide embedding_dim
            mlp_dim: the channel dimension internal to the MLP block
            activation: the activation to use in the MLP block
        """
        super().__init__()
        self.depth = depth
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.mlp_dim = mlp_dim
        self.layers = nn.ModuleList()
        for i in range(depth):
            self.layers.append(TwoWayAttentionBlock(embedding_dim=embedding_dim, num_heads=num_heads, mlp_dim=mlp_dim, activation=activation, attention_downsample_rate=attention_downsample_rate, skip_first_layer_pe=i == 0))
        self.final_attn_token_to_image = Attention(embedding_dim, num_heads, downsample_rate=attention_downsample_rate)
        self.norm_final_attn = nn.LayerNorm(embedding_dim)

    def forward(self, image_embedding: 'Tensor', image_pe: 'Tensor', point_embedding: 'Tensor') ->tuple[Tensor, Tensor]:
        """
        Args:
            image_embedding: image to attend to. Should be shape B x embedding_dim x h x w for any h and w.
            image_pe: the positional encoding to add to the image. Must have the same shape as image_embedding.
            point_embedding: the embedding to add to the query points. Must have shape B x N_points x embedding_dim
            for any N_points.

        Returns:
            - the processed point_embedding
            - the processed image_embedding
        """
        bs, c, h, w = image_embedding.shape
        image_embedding = image_embedding.flatten(2).permute(0, 2, 1)
        image_pe = image_pe.flatten(2).permute(0, 2, 1)
        queries = point_embedding
        keys = image_embedding
        for layer in self.layers:
            queries, keys = layer(queries=queries, keys=keys, query_pe=point_embedding, key_pe=image_pe)
        q = queries + point_embedding
        k = keys + image_pe
        attn_out = self.final_attn_token_to_image(q=q, k=k, v=keys)
        queries = queries + attn_out
        queries = self.norm_final_attn(queries)
        return queries, keys


def _make_pair(x: 'int | tuple[int, int]') ->tuple[int, int]:
    return (x, x) if isinstance(x, int) else x


class PatchMerging(Module):

    def __init__(self, input_resolution: 'int | tuple[int, int]', dim: 'int', out_dim: 'int', stride: 'int', activation: 'type[Module]'=nn.GELU) ->None:
        KORNIA_CHECK(stride in (1, 2), 'stride must be either 1 or 2')
        super().__init__()
        self.input_resolution = _make_pair(input_resolution)
        self.conv1 = ConvBN(dim, out_dim, 1, activation=activation)
        self.conv2 = ConvBN(out_dim, out_dim, 3, stride, 1, groups=out_dim, activation=activation)
        self.conv3 = ConvBN(out_dim, out_dim, 1)

    def forward(self, x: 'Tensor') ->Tensor:
        if x.ndim == 3:
            x = x.transpose(1, 2).unflatten(2, self.input_resolution)
        x = self.conv3(self.conv2(self.conv1(x)))
        x = x.flatten(2).transpose(1, 2)
        return x


def window_partition(x: 'Tensor', window_size: 'int') ->tuple[Tensor, tuple[int, int]]:
    """Partition into non-overlapping windows with padding if needed.

    Args:
        x: input tokens with [B, H, W, C].
        window_size: window size.

    Returns:
        windows: windows after partition with [B * num_windows, window_size, window_size, C].
        (Hp, Wp): padded height and width before partition
    """
    B, H, W, C = x.shape
    pad_h = (window_size - H % window_size) % window_size
    pad_w = (window_size - W % window_size) % window_size
    if pad_h > 0 or pad_w > 0:
        x = pad(x, (0, 0, 0, pad_w, 0, pad_h))
    Hp, Wp = H + pad_h, W + pad_w
    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows, (Hp, Wp)


def window_unpartition(windows: 'Tensor', window_size: 'int', pad_hw: 'tuple[int, int]', hw: 'tuple[int, int]') ->Tensor:
    """Window unpartition into original sequences and removing padding.

    Args:
        x: input tokens with [B * num_windows, window_size, window_size, C].
        window_size: window size.
        pad_hw: padded height and width (Hp, Wp).
        hw: original height and width (H, W) before padding.

    Returns:
        x: unpartitioned sequences with [B, H, W, C].
    """
    Hp, Wp = pad_hw
    H, W = hw
    B = windows.shape[0] // (Hp * Wp // window_size // window_size)
    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)
    if Hp > H or Wp > W:
        x = x[:, :H, :W, :].contiguous()
    return x


class TinyViTBlock(Module):

    def __init__(self, dim: 'int', input_resolution: 'int | tuple[int, int]', num_heads: 'int', window_size: 'int'=7, mlp_ratio: 'float'=4.0, drop: 'float'=0.0, drop_path: 'float'=0.0, local_conv_size: 'int'=3, activation: 'type[Module]'=nn.GELU) ->None:
        KORNIA_CHECK(dim % num_heads == 0, 'dim must be divislbe by num_heads')
        super().__init__()
        self.input_resolution = _make_pair(input_resolution)
        self.window_size = window_size
        head_dim = dim // num_heads
        self.attn = Attention(dim, head_dim, num_heads, 1.0, (window_size, window_size))
        self.drop_path1 = DropPath(drop_path)
        self.local_conv = ConvBN(dim, dim, local_conv_size, 1, local_conv_size // 2, dim)
        self.mlp = MLP(dim, int(dim * mlp_ratio), dim, activation, drop)
        self.drop_path2 = DropPath(drop_path)

    def forward(self, x: 'Tensor') ->Tensor:
        H, W = self.input_resolution
        B, L, C = x.shape
        res_x = x
        x = x.view(B, H, W, C)
        x, pad_hw = window_partition(x, self.window_size)
        x = self.attn(x.flatten(1, 2))
        x = window_unpartition(x, self.window_size, pad_hw, (H, W))
        x = x.view(B, L, C)
        x = res_x + self.drop_path1(x)
        x = x.transpose(1, 2).reshape(B, C, H, W)
        x = self.local_conv(x)
        x = x.view(B, C, L).transpose(1, 2)
        x = x + self.drop_path2(self.mlp(x))
        return x


class BasicLayer(Module):

    def __init__(self, dim: 'int', input_resolution: 'int | tuple[int, int]', depth: 'int', num_heads: 'int', window_size: 'int', mlp_ratio: 'float'=4.0, drop: 'float'=0.0, drop_path: 'float | list[float]'=0.0, downsample: 'Optional[Module]'=None, use_checkpoint: 'bool'=False, local_conv_size: 'int'=3, activation: 'type[Module]'=nn.GELU) ->None:
        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.blocks = nn.ModuleList([TinyViTBlock(dim, input_resolution, num_heads, window_size, mlp_ratio, drop, drop_path[i] if isinstance(drop_path, list) else drop_path, local_conv_size, activation) for i in range(depth)])
        self.downsample = downsample

    def forward(self, x: 'Tensor') ->Tensor:
        for blk in self.blocks:
            x = checkpoint.checkpoint(blk, x) if self.use_checkpoint else blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x


import sys
_module = sys.modules[__name__]
del sys
allennlp = _module
commands = _module
_checklist_internal = _module
build_vocab = _module
cached_path = _module
checklist = _module
count_instances = _module
diff = _module
evaluate = _module
find_learning_rate = _module
predict = _module
print_results = _module
push_to_hf = _module
subcommand = _module
test_install = _module
train = _module
common = _module
cached_transformers = _module
checks = _module
file_utils = _module
from_params = _module
lazy = _module
logging = _module
meta = _module
model_card = _module
params = _module
plugins = _module
registrable = _module
sequences = _module
task_card = _module
testing = _module
checklist_test = _module
confidence_check_test = _module
distributed_test = _module
interpret_test = _module
model_test_case = _module
test_case = _module
tqdm = _module
util = _module
confidence_checks = _module
normalization_bias_verification = _module
task_checklists = _module
question_answering_suite = _module
sentiment_analysis_suite = _module
task_suite = _module
textual_entailment_suite = _module
utils = _module
verification_base = _module
data = _module
batch = _module
data_loaders = _module
data_collator = _module
data_loader = _module
multiprocess_data_loader = _module
multitask_data_loader = _module
multitask_epoch_sampler = _module
multitask_scheduler = _module
simple_data_loader = _module
dataset_readers = _module
babi = _module
conll2003 = _module
dataset_reader = _module
dataset_utils = _module
span_utils = _module
interleaving_dataset_reader = _module
multitask = _module
sequence_tagging = _module
sharded_dataset_reader = _module
text_classification_json = _module
fields = _module
adjacency_field = _module
array_field = _module
field = _module
flag_field = _module
index_field = _module
label_field = _module
list_field = _module
metadata_field = _module
multilabel_field = _module
namespace_swapping_field = _module
sequence_field = _module
sequence_label_field = _module
span_field = _module
tensor_field = _module
text_field = _module
transformer_text_field = _module
image_loader = _module
instance = _module
samplers = _module
batch_sampler = _module
bucket_batch_sampler = _module
max_tokens_batch_sampler = _module
token_indexers = _module
elmo_indexer = _module
pretrained_transformer_indexer = _module
pretrained_transformer_mismatched_indexer = _module
single_id_token_indexer = _module
spacy_indexer = _module
token_characters_indexer = _module
token_indexer = _module
tokenizers = _module
character_tokenizer = _module
letters_digits_tokenizer = _module
pretrained_transformer_tokenizer = _module
sentence_splitter = _module
spacy_tokenizer = _module
token_class = _module
tokenizer = _module
whitespace_tokenizer = _module
vocabulary = _module
evaluation = _module
evaluator = _module
postprocessors = _module
serializers = _module
fairness = _module
adversarial_bias_mitigator = _module
bias_direction = _module
bias_direction_wrappers = _module
bias_metrics = _module
bias_mitigator_applicator = _module
bias_mitigator_wrappers = _module
bias_mitigators = _module
bias_utils = _module
fairness_metrics = _module
interpret = _module
attackers = _module
attacker = _module
hotflip = _module
input_reduction = _module
influence_interpreters = _module
influence_interpreter = _module
simple_influence = _module
saliency_interpreters = _module
integrated_gradient = _module
saliency_interpreter = _module
simple_gradient = _module
smooth_gradient = _module
models = _module
archival = _module
basic_classifier = _module
heads = _module
classifier_head = _module
head = _module
model = _module
multitask = _module
simple_tagger = _module
modules = _module
attention = _module
additive_attention = _module
attention = _module
bilinear_attention = _module
cosine_attention = _module
dot_product_attention = _module
linear_attention = _module
scaled_dot_product_attention = _module
augmented_lstm = _module
backbones = _module
backbone = _module
pretrained_transformer_backbone = _module
vilbert_backbone = _module
bimpm_matching = _module
conditional_random_field = _module
conditional_random_field = _module
conditional_random_field_wemission = _module
conditional_random_field_wlannoy = _module
conditional_random_field_wtrans = _module
elmo = _module
elmo_lstm = _module
encoder_base = _module
feedforward = _module
gated_sum = _module
highway = _module
input_variational_dropout = _module
layer_norm = _module
lstm_cell_with_projection = _module
masked_layer_norm = _module
matrix_attention = _module
bilinear_matrix_attention = _module
cosine_matrix_attention = _module
dot_product_matrix_attention = _module
linear_matrix_attention = _module
matrix_attention = _module
scaled_dot_product_matrix_attention = _module
maxout = _module
residual_with_layer_dropout = _module
sampled_softmax_loss = _module
scalar_mix = _module
seq2seq_encoders = _module
compose_encoder = _module
feedforward_encoder = _module
gated_cnn_encoder = _module
pass_through_encoder = _module
pytorch_seq2seq_wrapper = _module
pytorch_transformer_wrapper = _module
seq2seq_encoder = _module
seq2vec_encoders = _module
bert_pooler = _module
boe_encoder = _module
cls_pooler = _module
cnn_encoder = _module
cnn_highway_encoder = _module
pytorch_seq2vec_wrapper = _module
seq2vec_encoder = _module
softmax_loss = _module
span_extractors = _module
bidirectional_endpoint_span_extractor = _module
endpoint_span_extractor = _module
max_pooling_span_extractor = _module
self_attentive_span_extractor = _module
span_extractor = _module
span_extractor_with_span_width_embedding = _module
stacked_alternating_lstm = _module
stacked_bidirectional_lstm = _module
text_field_embedders = _module
basic_text_field_embedder = _module
text_field_embedder = _module
time_distributed = _module
token_embedders = _module
bag_of_word_counts_token_embedder = _module
elmo_token_embedder = _module
embedding = _module
empty_embedder = _module
pass_through_token_embedder = _module
pretrained_transformer_embedder = _module
pretrained_transformer_mismatched_embedder = _module
token_characters_encoder = _module
token_embedder = _module
transformer = _module
activation_layer = _module
attention_module = _module
bimodal_attention = _module
bimodal_connection_layer = _module
bimodal_encoder = _module
layer_norm = _module
output_layer = _module
positional_encoding = _module
t5 = _module
transformer_embeddings = _module
transformer_layer = _module
transformer_module = _module
transformer_pooler = _module
transformer_stack = _module
util = _module
util = _module
vision = _module
grid_embedder = _module
image2image = _module
region_detector = _module
nn = _module
activations = _module
beam_search = _module
checkpoint = _module
checkpoint_wrapper = _module
fairscale_checkpoint_wrapper = _module
chu_liu_edmonds = _module
initializers = _module
module = _module
parallel = _module
ddp_accelerator = _module
fairscale_fsdp_accelerator = _module
sharded_module_mixin = _module
regularizers = _module
regularizer = _module
regularizer_applicator = _module
regularizers = _module
util = _module
predictors = _module
predictor = _module
sentence_tagger = _module
text_classifier = _module
sanity_checks = _module
tools = _module
archive_surgery = _module
create_elmo_embeddings_from_vocab = _module
inspect_cache = _module
training = _module
callbacks = _module
backward = _module
callback = _module
console_logger = _module
log_writer = _module
should_validate = _module
tensorboard = _module
track_epoch = _module
wandb = _module
checkpointer = _module
gradient_descent_trainer = _module
learning_rate_schedulers = _module
combined = _module
cosine = _module
learning_rate_scheduler = _module
linear_with_warmup = _module
noam = _module
polynomial_decay = _module
pytorch_lr_schedulers = _module
slanted_triangular = _module
metric_tracker = _module
metrics = _module
attachment_scores = _module
auc = _module
average = _module
bleu = _module
boolean_accuracy = _module
categorical_accuracy = _module
covariance = _module
entropy = _module
evalb_bracketing_scorer = _module
f1_measure = _module
fbeta_measure = _module
fbeta_multi_label_measure = _module
fbeta_verbose_measure = _module
mean_absolute_error = _module
metric = _module
pearson_correlation = _module
perplexity = _module
rouge = _module
sequence_accuracy = _module
span_based_f1_measure = _module
spearman_correlation = _module
unigram_recall = _module
momentum_schedulers = _module
inverted_triangular = _module
momentum_scheduler = _module
moving_average = _module
no_op_trainer = _module
optimizers = _module
scheduler = _module
trainer = _module
util = _module
version = _module
benchmarks = _module
character_tokenizer_bench = _module
util_bench = _module
resume_daemon = _module
run_with_beaker = _module
build_docs_config = _module
check_links = _module
check_torch_version = _module
close_stale_issues = _module
get_version = _module
ping_issue_assignees = _module
py2md = _module
release_notes = _module
resume_daemon_test = _module
basic_example = _module
py2md_test = _module
train_fixtures = _module
setup = _module
test_fixtures = _module
d = _module
tests = _module
build_vocab_test = _module
cached_path_test = _module
diff_test = _module
evaluate_test = _module
find_learning_rate_test = _module
main_test = _module
no_op_train_test = _module
predict_test = _module
print_results_test = _module
test_install_test = _module
train_test = _module
cached_transformers_test = _module
file_utils_test = _module
from_params_test = _module
logging_test = _module
model_card_test = _module
params_test = _module
plugins_test = _module
push_to_hub_test = _module
registrable_test = _module
sequences_test = _module
task_card_test = _module
testing = _module
util_test = _module
normalization_bias_verification_test = _module
sentiment_analysis_suite_test = _module
task_suite_test = _module
utils_test = _module
multiprocess_data_loader_test = _module
multitask_data_loader_test = _module
multitask_scheduler_test = _module
babi_reader_test = _module
conll2003_test = _module
dataset_reader_test = _module
span_utils_test = _module
interleaving_dataset_reader_test = _module
sequence_tagging_test = _module
sharded_dataset_reader_test = _module
text_classification_json_test = _module
dataset_test = _module
adjacency_field_test = _module
field_test = _module
flag_field_test = _module
index_field_test = _module
label_field_test = _module
list_field_test = _module
metadata_field_test = _module
multilabel_field_test = _module
sequence_label_field_test = _module
span_field_test = _module
tensor_field_test = _module
text_field_test = _module
transformer_text_field_test = _module
image_loader_test = _module
instance_test = _module
bucket_batch_sampler_test = _module
max_tokens_batch_sampler_test = _module
sampler_test = _module
character_token_indexer_test = _module
elmo_indexer_test = _module
pretrained_transformer_indexer_test = _module
pretrained_transformer_mismatched_indexer_test = _module
single_id_token_indexer_test = _module
spacy_indexer_test = _module
character_tokenizer_test = _module
letters_digits_tokenizer_test = _module
pretrained_transformer_tokenizer_test = _module
sentence_splitter_test = _module
spacy_tokenizer_test = _module
vocabulary_test = _module
evaluator_tests = _module
serializer_test = _module
bias_direction_test = _module
bias_metrics_test = _module
bias_mitigators_test = _module
bias_utils_test = _module
fairness_metrics_test = _module
hotflip_test = _module
input_reduction_test = _module
integrated_gradient_test = _module
simple_gradient_test = _module
simple_influence_test = _module
smooth_gradient_test = _module
archival_test = _module
basic_classifier_test = _module
model_test = _module
multitask_test = _module
simple_tagger_test = _module
test_model_test_case = _module
additive_attention_test = _module
attention_test = _module
bilinear_attention_test = _module
cosine_attention_test = _module
dot_product_attention_test = _module
linear_attention_test = _module
scaled_dot_product_attention_test = _module
augmented_lstm_test = _module
bimpm_matching_test = _module
conditional_random_field_test = _module
elmo_test = _module
encoder_base_test = _module
feedforward_test = _module
gated_sum_test = _module
highway_test = _module
lstm_cell_with_projection_test = _module
masked_layer_norm_test = _module
bilinear_matrix_attention_test = _module
cosine_matrix_attention_test = _module
dot_product_matrix_attention_test = _module
linear_matrix_attention_test = _module
matrix_attention_test = _module
scaled_dot_product_matrix_attention_test = _module
maxout_test = _module
residual_with_layer_dropout_test = _module
sampled_softmax_loss_test = _module
scalar_mix_test = _module
seq2seq_encoder_test = _module
compose_encoder_test = _module
feedforward_encoder_test = _module
gated_cnn_encoder_test = _module
pass_through_encoder_test = _module
pytorch_seq2seq_wrapper_test = _module
pytorch_transformer_wrapper_test = _module
seq2vec_encoder_test = _module
bert_pooler_test = _module
boe_encoder_test = _module
cls_pooler_test = _module
cnn_encoder_test = _module
cnn_highway_encoder_test = _module
pytorch_seq2vec_wrapper_test = _module
bidirectional_endpoint_span_extractor_test = _module
endpoint_span_extractor_test = _module
max_pooling_span_extractor_test = _module
self_attentive_span_extractor_test = _module
stacked_alternating_lstm_test = _module
stacked_bidirectional_lstm_test = _module
stacked_elmo_lstm_test = _module
basic_text_field_embedder_test = _module
time_distributed_test = _module
bag_of_word_counts_token_embedder_test = _module
elmo_token_embedder_test = _module
embedding_test = _module
pass_through_embedder_test = _module
pretrained_transformer_embedder_test = _module
pretrained_transformer_mismatched_embedder_test = _module
token_characters_encoder_test = _module
activation_layer_test = _module
bimodal_attention_test = _module
bimodal_encoder_test = _module
output_layer_test = _module
positional_encoding_test = _module
self_attention_test = _module
t5_self_attention_test = _module
t5_test = _module
toolkit_test = _module
transformer_embeddings_test = _module
transformer_layer_test = _module
transformer_module_test = _module
transformer_pooler_test = _module
transformer_stack_test = _module
grid_embedder_test = _module
region_detector_test = _module
beam_search_test = _module
checkpoint_wrapper_test = _module
fairscale_checkpoint_wrapper_test = _module
chu_liu_edmonds_test = _module
initializers_test = _module
fairscale_fsdp_accelerator_test = _module
pretrained_model_initializer_test = _module
regularizers_test = _module
util_test = _module
predictor_test = _module
sentence_tagger_test = _module
text_classifier_test = _module
checkpointer_test = _module
combined_test = _module
cosine_test = _module
learning_rate_scheduler_test = _module
slanted_triangular_test = _module
attachment_scores_test = _module
auc_test = _module
average_test = _module
bleu_test = _module
boolean_accuracy_test = _module
categorical_accuracy_test = _module
covariance_test = _module
entropy_test = _module
evalb_bracketing_scorer_test = _module
f1_measure_test = _module
fbeta_measure_test = _module
fbeta_multi_label_measure_test = _module
fbeta_verbose_measure_test = _module
mean_absolute_error_test = _module
pearson_correlation_test = _module
rouge_test = _module
sequence_accuracy_test = _module
span_based_f1_measure_test = _module
spearman_correlation_test = _module
unigram_recall_test = _module
inverted_triangular_test = _module
moving_average_test = _module
no_op_trainer_test = _module
optimizer_test = _module
trainer_test = _module
tutorials = _module
tagger = _module
basic_allennlp_test = _module
version_test = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import warnings


import logging


from typing import Union


from typing import Dict


from typing import List


from typing import Tuple


from typing import NamedTuple


from typing import cast


import torch


from typing import Any


from typing import Optional


import torch.distributed as dist


import torch.multiprocessing as mp


import re


from torch import cuda


from abc import ABC


from collections import defaultdict


from typing import Callable


from typing import Set


from typing import Iterator


from typing import Iterable


from typing import MutableMapping


import time


import numpy as np


from torch import Tensor


from torch.testing import assert_allclose


import copy


import random


import numpy


from numpy.testing import assert_allclose


from itertools import islice


from itertools import zip_longest


from typing import Generator


from typing import TypeVar


from typing import Sequence


from torch import nn as nn


from abc import abstractmethod


from copy import deepcopy


import torch.nn as nn


from collections import Counter


from collections import deque


from queue import Full


import itertools


import math


from typing import Generic


import torch.nn.functional


import torchvision


from torch import FloatTensor


from torch import IntTensor


from typing import Mapping


import sklearn


import scipy


from scipy.stats import wasserstein_distance


from torch.distributions.categorical import Categorical


from torch.distributions.kl import kl_divergence


from torch import autograd


import torch.autograd as autograd


from torch.nn import Module


from typing import Type


import inspect


from torch.nn.modules.linear import Linear


import torch.nn.functional as F


from torch.nn.parameter import Parameter


from torch.nn import Parameter


from torch.nn.utils.rnn import PackedSequence


from torch.nn.utils.rnn import pack_padded_sequence


from torch.nn.utils.rnn import pad_packed_sequence


from torch.nn.modules import Dropout


from torch.nn import ParameterList


from torch import nn


import torch.nn


from torch.nn import Conv1d


from torch.nn import Linear


from typing import BinaryIO


from torch.nn.functional import embedding


from typing import TYPE_CHECKING


from torch.nn import CrossEntropyLoss


from collections import OrderedDict


import torchvision.ops.boxes as box_ops


from inspect import signature


import functools


from torch.utils.checkpoint import CheckpointFunction


import torch.nn.init


from typing import OrderedDict


from torch.cuda import amp


from torch.nn.utils import clip_grad_norm_


from itertools import chain


from torch.utils.hooks import RemovableHandle


from torch import backends


from typing import Deque


from torch.cuda.amp.grad_scaler import OptState


from sklearn import metrics


import scipy.stats as stats


import torch.optim.lr_scheduler


from torch import allclose


from numpy.testing import assert_almost_equal


from torch.autograd import Variable


from torch.nn.modules.rnn import LSTM


from torch.nn import LSTM


from torch.nn import RNN


from torch.nn import GRU


from torch.nn import Embedding


from numpy.testing import assert_array_almost_equal


from sklearn.metrics import precision_recall_fscore_support


from math import isclose


class ConfigurationError(Exception):
    """
    The exception raised by any AllenNLP object when it's misconfigured
    (e.g. missing properties, invalid properties, unknown properties).
    """

    def __reduce__(self) ->Union[str, Tuple[Any, ...]]:
        return type(self), (self.message,)

    def __init__(self, message: str):
        super().__init__()
        self.message = message

    def __str__(self):
        return self.message


DataArray = TypeVar('DataArray', torch.Tensor, Dict[str, torch.Tensor], Dict[str, Dict[str, torch.Tensor]])


DEFAULT_NON_PADDED_NAMESPACES = '*tags', '*labels'


DEFAULT_OOV_TOKEN = '@@UNKNOWN@@'


DEFAULT_PADDING_TOKEN = '@@PADDING@@'


NAMESPACE_PADDING_FILE = 'non_padded_namespaces.txt'


def _is_encodable(value: str) ->bool:
    """
    We need to filter out environment variables that can't
    be unicode-encoded to avoid a "surrogates not allowed"
    error in jsonnet.
    """
    return value == '' or value.encode('utf-8', 'ignore') != b''


def _environment_variables() ->Dict[str, str]:
    """
    Wraps `os.environ` to filter out non-encodable values.
    """
    return {key: value for key, value in os.environ.items() if _is_encodable(value)}


def _is_dict_free(obj: Any) ->bool:
    """
    Returns False if obj is a dict, or if it's a list with an element that _has_dict.
    """
    if isinstance(obj, dict):
        return False
    elif isinstance(obj, list):
        return all(_is_dict_free(item) for item in obj)
    else:
        return True


def _replace_none(params: Any) ->Any:
    if params == 'None':
        return None
    elif isinstance(params, dict):
        for key, value in params.items():
            params[key] = _replace_none(value)
        return params
    elif isinstance(params, list):
        return [_replace_none(value) for value in params]
    return params


logger = logging.getLogger(__name__)


def _cached_path(args: argparse.Namespace):
    logger.info('Cache directory: %s', args.cache_dir)
    if args.inspect:
        if args.extract_archive or args.force_extract or args.remove:
            raise RuntimeError('cached-path cannot accept --extract-archive, --force-extract, or --remove options when --inspect flag is used.')
        inspect_cache(patterns=args.resources, cache_dir=args.cache_dir)
    elif args.remove:
        if args.extract_archive or args.force_extract or args.inspect:
            raise RuntimeError('cached-path cannot accept --extract-archive, --force-extract, or --inspect options when --remove flag is used.')
        if not args.resources:
            raise RuntimeError("Missing positional argument(s) 'resources'. 'resources' is required when using the --remove option. If you really want to remove everything, pass '*' for 'resources'.")
        reclaimed_space = remove_cache_entries(args.resources, cache_dir=args.cache_dir)
        None
    else:
        for resource in args.resources:
            None


def infer_and_cast(value: Any):
    """
    In some cases we'll be feeding params dicts to functions we don't own;
    for example, PyTorch optimizers. In that case we can't use `pop_int`
    or similar to force casts (which means you can't specify `int` parameters
    using environment variables). This function takes something that looks JSON-like
    and recursively casts things that look like (bool, int, float) to (bool, int, float).
    """
    if isinstance(value, (int, float, bool)):
        return value
    elif isinstance(value, list):
        return [infer_and_cast(item) for item in value]
    elif isinstance(value, dict):
        return {key: infer_and_cast(item) for key, item in value.items()}
    elif isinstance(value, str):
        if value.lower() == 'true':
            return True
        elif value.lower() == 'false':
            return False
        else:
            try:
                return int(value)
            except ValueError:
                pass
            try:
                return float(value)
            except ValueError:
                return value
    else:
        raise ValueError(f'cannot infer type of {value}')


T = TypeVar('T')


def with_overrides(original: T, overrides_dict: Dict[str, Any], prefix: str='') ->T:
    merged: T
    keys: Union[Iterable[str], Iterable[int]]
    if isinstance(original, list):
        merged = [None] * len(original)
        keys = range(len(original))
    elif isinstance(original, dict):
        merged = {}
        keys = chain(original.keys(), (k for k in overrides_dict if '.' not in k and k not in original))
    elif prefix:
        raise ValueError(f"overrides for '{prefix[:-1]}.*' expected list or dict in original, found {type(original)} instead")
    else:
        raise ValueError(f'expected list or dict, found {type(original)} instead')
    used_override_keys: Set[str] = set()
    for key in keys:
        if str(key) in overrides_dict:
            merged[key] = copy.deepcopy(overrides_dict[str(key)])
            used_override_keys.add(str(key))
        else:
            overrides_subdict = {}
            for o_key in overrides_dict:
                if o_key.startswith(f'{key}.'):
                    overrides_subdict[o_key[len(f'{key}.'):]] = overrides_dict[o_key]
                    used_override_keys.add(o_key)
            if overrides_subdict:
                merged[key] = with_overrides(original[key], overrides_subdict, prefix=prefix + f'{key}.')
            else:
                merged[key] = copy.deepcopy(original[key])
    unused_override_keys = [(prefix + key) for key in set(overrides_dict.keys()) - used_override_keys]
    if unused_override_keys:
        raise ValueError(f'overrides dict contains unused keys: {unused_override_keys}')
    return merged


def takes_arg(obj, arg: str) ->bool:
    """
    Checks whether the provided obj takes a certain arg.
    If it's a class, we're really checking whether its constructor does.
    If it's a function or method, we're checking the object itself.
    Otherwise, we raise an error.
    """
    if inspect.isclass(obj):
        signature = inspect.signature(obj.__init__)
    elif inspect.ismethod(obj) or inspect.isfunction(obj):
        signature = inspect.signature(obj)
    else:
        raise ConfigurationError(f'object {obj} is not callable')
    return arg in signature.parameters


def takes_kwargs(obj) ->bool:
    """
    Checks whether a provided object takes in any positional arguments.
    Similar to takes_arg, we do this for both the __init__ function of
    the class or a function / method
    Otherwise, we raise an error
    """
    if inspect.isclass(obj):
        signature = inspect.signature(obj.__init__)
    elif inspect.ismethod(obj) or inspect.isfunction(obj):
        signature = inspect.signature(obj)
    else:
        raise ConfigurationError(f'object {obj} is not callable')
    return any(p.kind == inspect.Parameter.VAR_KEYWORD for p in signature.parameters.values())


def create_extras(cls: Type[T], extras: Dict[str, Any]) ->Dict[str, Any]:
    """
    Given a dictionary of extra arguments, returns a dictionary of
    kwargs that actually are a part of the signature of the cls.from_params
    (or cls) method.
    """
    subextras: Dict[str, Any] = {}
    if hasattr(cls, 'from_params'):
        from_params_method = cls.from_params
    else:
        from_params_method = cls
    if takes_kwargs(from_params_method):
        subextras = extras
    else:
        subextras = {k: v for k, v in extras.items() if takes_arg(from_params_method, k)}
    return subextras


def infer_method_params(cls: Type[T], method: Callable) ->Dict[str, inspect.Parameter]:
    signature = inspect.signature(method)
    parameters = dict(signature.parameters)
    has_kwargs = False
    var_positional_key = None
    for param in parameters.values():
        if param.kind == param.VAR_KEYWORD:
            has_kwargs = True
        elif param.kind == param.VAR_POSITIONAL:
            var_positional_key = param.name
    if var_positional_key:
        del parameters[var_positional_key]
    if not has_kwargs:
        return parameters
    super_class = None
    for super_class_candidate in cls.mro()[1:]:
        if issubclass(super_class_candidate, FromParams):
            super_class = super_class_candidate
            break
    if super_class:
        super_parameters = infer_params(super_class)
    else:
        super_parameters = {}
    return {**super_parameters, **parameters}


def infer_constructor_params(cls: Type[T], constructor: Union[Callable[..., T], Callable[[T], None]]=None) ->Dict[str, inspect.Parameter]:
    if constructor is None:
        constructor = cls.__init__
    return infer_method_params(cls, constructor)


infer_params = infer_constructor_params


_NO_DEFAULT = inspect.Parameter.empty


def can_construct_from_params(type_: Type) ->bool:
    if type_ in [str, int, float, bool]:
        return True
    origin = getattr(type_, '__origin__', None)
    if origin == Lazy:
        return True
    elif origin:
        if hasattr(type_, 'from_params'):
            return True
        args = getattr(type_, '__args__')
        return all(can_construct_from_params(arg) for arg in args)
    return hasattr(type_, 'from_params')


def remove_optional(annotation: type):
    """
    Optional[X] annotations are actually represented as Union[X, NoneType].
    For our purposes, the "Optional" part is not interesting, so here we
    throw it away.
    """
    origin = getattr(annotation, '__origin__', None)
    args = getattr(annotation, '__args__', ())
    if origin == Union:
        return Union[tuple([arg for arg in args if arg != type(None)])]
    else:
        return annotation


def is_base_registrable(cls) ->bool:
    """
    Checks whether this is a class that directly inherits from Registrable, or is a subclass of such
    a class.
    """
    if not issubclass(cls, Registrable):
        return False
    method_resolution_order = inspect.getmro(cls)[1:]
    for base_class in method_resolution_order:
        if issubclass(base_class, Registrable) and base_class is not Registrable:
            return False
    return True


_RegistrableT = TypeVar('_RegistrableT', bound='Registrable')


_SubclassRegistry = Dict[str, Tuple[type, Optional[str]]]


_T = TypeVar('_T')


def _get_suggestion(name: str, available: List[str]) ->Optional[str]:
    for ch, repl_ch in (('_', '-'), ('-', '_')):
        suggestion = name.replace(ch, repl_ch)
        if suggestion in available:
            return suggestion
    for suggestion in available:
        if edit_distance(name, suggestion, transpositions=True) == 1:
            return suggestion
    return None


def replace_cr_with_newline(message: str) ->str:
    """
    TQDM and requests use carriage returns to get the training line to update for each batch
    without adding more lines to the terminal output. Displaying those in a file won't work
    correctly, so we'll just make sure that each batch shows up on its one line.
    """
    message = message.replace('\r', '').replace('\n', '').replace('\x1b[A', '')
    if message and message[-1] != '\n':
        message += '\n'
    return message


class TqdmToLogsWriter(object):

    def __init__(self):
        self.last_message_written_time = 0.0

    def write(self, message):
        file_friendly_message: Optional[str] = None
        if common_logging.FILE_FRIENDLY_LOGGING:
            file_friendly_message = replace_cr_with_newline(message)
            if file_friendly_message.strip():
                sys.stderr.write(file_friendly_message)
        else:
            sys.stderr.write(message)
        now = time()
        if now - self.last_message_written_time >= 10 or '100%' in message:
            if file_friendly_message is None:
                file_friendly_message = replace_cr_with_newline(message)
            for message in file_friendly_message.split('\n'):
                message = message.strip()
                if len(message) > 0:
                    logger.info(message)
                    self.last_message_written_time = now

    def flush(self):
        sys.stderr.flush()


class Tqdm:

    @staticmethod
    def tqdm(*args, **kwargs):
        default_mininterval = 2.0 if common_logging.FILE_FRIENDLY_LOGGING else 0.1
        new_kwargs = {'file': TqdmToLogsWriter(), 'mininterval': default_mininterval, **kwargs}
        return _tqdm(*args, **new_kwargs)

    @staticmethod
    def set_lock(lock):
        _tqdm.set_lock(lock)

    @staticmethod
    def get_lock():
        return _tqdm.get_lock()


def namespace_match(pattern: str, namespace: str):
    """
    Matches a namespace pattern against a namespace string.  For example, `*tags` matches
    `passage_tags` and `question_tags` and `tokens` matches `tokens` but not
    `stemmed_tokens`.
    """
    if pattern[0] == '*' and namespace.endswith(pattern[1:]):
        return True
    elif pattern == namespace:
        return True
    return False


class _NamespaceDependentDefaultDict(defaultdict):
    """
    This is a [defaultdict]
    (https://docs.python.org/2/library/collections.html#collections.defaultdict) where the
    default value is dependent on the key that is passed.

    We use "namespaces" in the :class:`Vocabulary` object to keep track of several different
    mappings from strings to integers, so that we have a consistent API for mapping words, tags,
    labels, characters, or whatever else you want, into integers.  The issue is that some of those
    namespaces (words and characters) should have integers reserved for padding and
    out-of-vocabulary tokens, while others (labels and tags) shouldn't.  This class allows you to
    specify filters on the namespace (the key used in the `defaultdict`), and use different
    default values depending on whether the namespace passes the filter.

    To do filtering, we take a set of `non_padded_namespaces`.  This is a set of strings
    that are either matched exactly against the keys, or treated as suffixes, if the
    string starts with `*`.  In other words, if `*tags` is in `non_padded_namespaces` then
    `passage_tags`, `question_tags`, etc. (anything that ends with `tags`) will have the
    `non_padded` default value.

    # Parameters

    non_padded_namespaces : `Iterable[str]`
        A set / list / tuple of strings describing which namespaces are not padded.  If a namespace
        (key) is missing from this dictionary, we will use :func:`namespace_match` to see whether
        the namespace should be padded.  If the given namespace matches any of the strings in this
        list, we will use `non_padded_function` to initialize the value for that namespace, and
        we will use `padded_function` otherwise.
    padded_function : `Callable[[], Any]`
        A zero-argument function to call to initialize a value for a namespace that `should` be
        padded.
    non_padded_function : `Callable[[], Any]`
        A zero-argument function to call to initialize a value for a namespace that should `not` be
        padded.
    """

    def __init__(self, non_padded_namespaces: Iterable[str], padded_function: Callable[[], Any], non_padded_function: Callable[[], Any]) ->None:
        self._non_padded_namespaces = set(non_padded_namespaces)
        self._padded_function = padded_function
        self._non_padded_function = non_padded_function
        super().__init__()

    def __missing__(self, key: str):
        if any(namespace_match(pattern, key) for pattern in self._non_padded_namespaces):
            value = self._non_padded_function()
        else:
            value = self._padded_function()
        dict.__setitem__(self, key, value)
        return value

    def add_non_padded_namespaces(self, non_padded_namespaces: Set[str]):
        self._non_padded_namespaces.update(non_padded_namespaces)


class _IndexToTokenDefaultDict(_NamespaceDependentDefaultDict):

    def __init__(self, non_padded_namespaces: Set[str], padding_token: str, oov_token: str) ->None:
        super().__init__(non_padded_namespaces, lambda : {(0): padding_token, (1): oov_token}, lambda : {})


_NEW_LINE_REGEX = re.compile('\\n|\\r\\n')


class _TokenToIndexDefaultDict(_NamespaceDependentDefaultDict):

    def __init__(self, non_padded_namespaces: Set[str], padding_token: str, oov_token: str) ->None:
        super().__init__(non_padded_namespaces, lambda : {padding_token: 0, oov_token: 1}, lambda : {})


def _read_pretrained_tokens(embeddings_file_uri: str) ->List[str]:
    logger.info('Reading pretrained tokens from: %s', embeddings_file_uri)
    tokens: List[str] = []
    with EmbeddingsTextFile(embeddings_file_uri) as embeddings_file:
        for line_number, line in enumerate(Tqdm.tqdm(embeddings_file), start=1):
            token_end = line.find(' ')
            if token_end >= 0:
                token = line[:token_end]
                tokens.append(token)
            else:
                line_begin = line[:20] + '...' if len(line) > 20 else line
                logger.warning('Skipping line number %d: %s', line_number, line_begin)
    return tokens


JsonDict = Dict[str, Any]


A = TypeVar('A')


def ensure_list(iterable: Iterable[A]) ->List[A]:
    """
    An Iterable may be a list or a generator.
    This ensures we get a list without making an unnecessary copy.
    """
    if isinstance(iterable, list):
        return iterable
    else:
        return list(iterable)


class LoadStateDictReturnType(NamedTuple):
    missing_keys: List[str]
    unexpected_keys: List[str]


class ShardedModuleMixin:
    """
    Mixin class for sharded data parallel wrappers. Subclasses should implement
    `get_original_module()` which returns a reference the original inner wrapped module.
    """

    def get_original_module(self) ->torch.nn.Module:
        """
        Get the original
        """
        raise NotImplementedError


StateDictType = Union[Dict[str, torch.Tensor], 'OrderedDict[str, torch.Tensor]']


def int_to_device(device: Union[int, torch.device]) ->torch.device:
    if isinstance(device, torch.device):
        return device
    if device < 0:
        return torch.device('cpu')
    return torch.device(device)


_DEFAULT_WEIGHTS = 'best.th'


TextFieldTensors = Dict[str, Dict[str, torch.Tensor]]


def get_forward_arguments(module: torch.nn.Module) ->Set[str]:
    signature = inspect.signature(module.forward)
    return set([arg for arg in signature.parameters if arg != 'self'])


_V = TypeVar('_V', int, float, torch.Tensor)


def is_distributed() ->bool:
    """
    Checks if the distributed process group is available and has been initialized
    """
    return dist.is_available() and dist.is_initialized()


def distributed_device() ->torch.device:
    """
    Get the correct `torch.device` of the current process to use for distributed point-to-point communication.
    """
    if not is_distributed():
        raise RuntimeError("'distributed_device()' can only be called within a distributed process group")
    return int_to_device(-1 if dist.get_backend() != 'nccl' else torch.cuda.current_device())


def dist_reduce(value: _V, reduce_op) ->_V:
    """
    Reduces the given `value` across all distributed worker nodes according the given
    reduction operation.

    If called outside of a distributed context, it will just return `value`.

    # Parameters

    value : `_V`
        The value to reduce across distributed nodes.
    reduce_op : `torch.distributed.ReduceOp`
        The [reduction operation](https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp)
        to use.
    **kwargs : `Any`
        Additional arguments used to construct the tensor that will wrap `value`.

    # Returns

    `_V`
        The final value.
    """
    if not is_distributed():
        return value
    device = distributed_device()
    if isinstance(value, torch.Tensor):
        value_tensor = value.clone()
    else:
        value_tensor = torch.tensor(value, device=device)
    dist.all_reduce(value_tensor, op=reduce_op)
    if isinstance(value, torch.Tensor):
        return value_tensor
    return value_tensor.item()


def dist_reduce_sum(value: _V) ->_V:
    """
    Sums the given `value` across distributed worker nodes.
    This is equivalent to calling `dist_reduce(v, dist.ReduceOp.SUM)`.
    """
    if not is_distributed():
        return value
    return dist_reduce(value, dist.ReduceOp.SUM)


RnnState = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]


RnnStateStorage = Tuple[torch.Tensor, ...]


def get_lengths_from_binary_sequence_mask(mask: torch.BoolTensor) ->torch.LongTensor:
    """
    Compute sequence lengths for each batch element in a tensor using a
    binary mask.

    # Parameters

    mask : `torch.BoolTensor`, required.
        A 2D binary mask of shape (batch_size, sequence_length) to
        calculate the per-batch sequence lengths from.

    # Returns

    `torch.LongTensor`
        A torch.LongTensor of shape (batch_size,) representing the lengths
        of the sequences in the batch.
    """
    return mask.sum(-1)


def sort_batch_by_length(tensor: torch.Tensor, sequence_lengths: torch.Tensor):
    """
    Sort a batch first tensor by some specified lengths.

    # Parameters

    tensor : `torch.FloatTensor`, required.
        A batch first Pytorch tensor.
    sequence_lengths : `torch.LongTensor`, required.
        A tensor representing the lengths of some dimension of the tensor which
        we want to sort by.

    # Returns

    sorted_tensor : `torch.FloatTensor`
        The original tensor sorted along the batch dimension with respect to sequence_lengths.
    sorted_sequence_lengths : `torch.LongTensor`
        The original sequence_lengths sorted by decreasing size.
    restoration_indices : `torch.LongTensor`
        Indices into the sorted_tensor such that
        `sorted_tensor.index_select(0, restoration_indices) == original_tensor`
    permutation_index : `torch.LongTensor`
        The indices used to sort the tensor. This is useful if you want to sort many
        tensors using the same ordering.
    """
    if not isinstance(tensor, torch.Tensor) or not isinstance(sequence_lengths, torch.Tensor):
        raise ConfigurationError('Both the tensor and sequence lengths must be torch.Tensors.')
    sorted_sequence_lengths, permutation_index = sequence_lengths.sort(0, descending=True)
    sorted_tensor = tensor.index_select(0, permutation_index)
    index_range = torch.arange(0, len(sequence_lengths), device=sequence_lengths.device)
    _, reverse_mapping = permutation_index.sort(0, descending=False)
    restoration_indices = index_range.index_select(0, reverse_mapping)
    return sorted_tensor, sorted_sequence_lengths, restoration_indices, permutation_index


class _EncoderBase(torch.nn.Module):
    """
    This abstract class serves as a base for the 3 `Encoder` abstractions in AllenNLP.
    - [`Seq2SeqEncoders`](./seq2seq_encoders/seq2seq_encoder.md)
    - [`Seq2VecEncoders`](./seq2vec_encoders/seq2vec_encoder.md)

    Additionally, this class provides functionality for sorting sequences by length
    so they can be consumed by Pytorch RNN classes, which require their inputs to be
    sorted by length. Finally, it also provides optional statefulness to all of it's
    subclasses by allowing the caching and retrieving of the hidden states of RNNs.
    """

    def __init__(self, stateful: bool=False) ->None:
        super().__init__()
        self.stateful = stateful
        self._states: Optional[RnnStateStorage] = None

    def sort_and_run_forward(self, module: Callable[[PackedSequence, Optional[RnnState]], Tuple[Union[PackedSequence, torch.Tensor], RnnState]], inputs: torch.Tensor, mask: torch.BoolTensor, hidden_state: Optional[RnnState]=None):
        """
        This function exists because Pytorch RNNs require that their inputs be sorted
        before being passed as input. As all of our Seq2xxxEncoders use this functionality,
        it is provided in a base class. This method can be called on any module which
        takes as input a `PackedSequence` and some `hidden_state`, which can either be a
        tuple of tensors or a tensor.

        As all of our Seq2xxxEncoders have different return types, we return `sorted`
        outputs from the module, which is called directly. Additionally, we return the
        indices into the batch dimension required to restore the tensor to it's correct,
        unsorted order and the number of valid batch elements (i.e the number of elements
        in the batch which are not completely masked). This un-sorting and re-padding
        of the module outputs is left to the subclasses because their outputs have different
        types and handling them smoothly here is difficult.

        # Parameters

        module : `Callable[RnnInputs, RnnOutputs]`
            A function to run on the inputs, where
            `RnnInputs: [PackedSequence, Optional[RnnState]]` and
            `RnnOutputs: Tuple[Union[PackedSequence, torch.Tensor], RnnState]`.
            In most cases, this is a `torch.nn.Module`.
        inputs : `torch.Tensor`, required.
            A tensor of shape `(batch_size, sequence_length, embedding_size)` representing
            the inputs to the Encoder.
        mask : `torch.BoolTensor`, required.
            A tensor of shape `(batch_size, sequence_length)`, representing masked and
            non-masked elements of the sequence for each element in the batch.
        hidden_state : `Optional[RnnState]`, (default = `None`).
            A single tensor of shape (num_layers, batch_size, hidden_size) representing the
            state of an RNN with or a tuple of
            tensors of shapes (num_layers, batch_size, hidden_size) and
            (num_layers, batch_size, memory_size), representing the hidden state and memory
            state of an LSTM-like RNN.

        # Returns

        module_output : `Union[torch.Tensor, PackedSequence]`.
            A Tensor or PackedSequence representing the output of the Pytorch Module.
            The batch size dimension will be equal to `num_valid`, as sequences of zero
            length are clipped off before the module is called, as Pytorch cannot handle
            zero length sequences.
        final_states : `Optional[RnnState]`
            A Tensor representing the hidden state of the Pytorch Module. This can either
            be a single tensor of shape (num_layers, num_valid, hidden_size), for instance in
            the case of a GRU, or a tuple of tensors, such as those required for an LSTM.
        restoration_indices : `torch.LongTensor`
            A tensor of shape `(batch_size,)`, describing the re-indexing required to transform
            the outputs back to their original batch order.
        """
        batch_size = mask.size(0)
        num_valid = torch.sum(mask[:, 0]).int().item()
        sequence_lengths = get_lengths_from_binary_sequence_mask(mask)
        sorted_inputs, sorted_sequence_lengths, restoration_indices, sorting_indices = sort_batch_by_length(inputs, sequence_lengths)
        packed_sequence_input = pack_padded_sequence(sorted_inputs[:num_valid, :, :], sorted_sequence_lengths[:num_valid].data.tolist(), batch_first=True)
        if not self.stateful:
            if hidden_state is None:
                initial_states: Any = hidden_state
            elif isinstance(hidden_state, tuple):
                initial_states = [state.index_select(1, sorting_indices)[:, :num_valid, :].contiguous() for state in hidden_state]
            else:
                initial_states = hidden_state.index_select(1, sorting_indices)[:, :num_valid, :].contiguous()
        else:
            initial_states = self._get_initial_states(batch_size, num_valid, sorting_indices)
        module_output, final_states = module(packed_sequence_input, initial_states)
        return module_output, final_states, restoration_indices

    def _get_initial_states(self, batch_size: int, num_valid: int, sorting_indices: torch.LongTensor) ->Optional[RnnState]:
        """
        Returns an initial state for use in an RNN. Additionally, this method handles
        the batch size changing across calls by mutating the state to append initial states
        for new elements in the batch. Finally, it also handles sorting the states
        with respect to the sequence lengths of elements in the batch and removing rows
        which are completely padded. Importantly, this `mutates` the state if the
        current batch size is larger than when it was previously called.

        # Parameters

        batch_size : `int`, required.
            The batch size can change size across calls to stateful RNNs, so we need
            to know if we need to expand or shrink the states before returning them.
            Expanded states will be set to zero.
        num_valid : `int`, required.
            The batch may contain completely padded sequences which get removed before
            the sequence is passed through the encoder. We also need to clip these off
            of the state too.
        sorting_indices `torch.LongTensor`, required.
            Pytorch RNNs take sequences sorted by length. When we return the states to be
            used for a given call to `module.forward`, we need the states to match up to
            the sorted sequences, so before returning them, we sort the states using the
            same indices used to sort the sequences.

        # Returns

        This method has a complex return type because it has to deal with the first time it
        is called, when it has no state, and the fact that types of RNN have heterogeneous
        states.

        If it is the first time the module has been called, it returns `None`, regardless
        of the type of the `Module`.

        Otherwise, for LSTMs, it returns a tuple of `torch.Tensors` with shape
        `(num_layers, num_valid, state_size)` and `(num_layers, num_valid, memory_size)`
        respectively, or for GRUs, it returns a single `torch.Tensor` of shape
        `(num_layers, num_valid, state_size)`.
        """
        if self._states is None:
            return None
        if batch_size > self._states[0].size(1):
            num_states_to_concat = batch_size - self._states[0].size(1)
            resized_states = []
            for state in self._states:
                zeros = state.new_zeros(state.size(0), num_states_to_concat, state.size(2))
                resized_states.append(torch.cat([state, zeros], 1))
            self._states = tuple(resized_states)
            correctly_shaped_states = self._states
        elif batch_size < self._states[0].size(1):
            correctly_shaped_states = tuple(state[:, :batch_size, :] for state in self._states)
        else:
            correctly_shaped_states = self._states
        if len(self._states) == 1:
            correctly_shaped_state = correctly_shaped_states[0]
            sorted_state = correctly_shaped_state.index_select(1, sorting_indices)
            return sorted_state[:, :num_valid, :].contiguous()
        else:
            sorted_states = [state.index_select(1, sorting_indices) for state in correctly_shaped_states]
            return tuple(state[:, :num_valid, :].contiguous() for state in sorted_states)

    def _update_states(self, final_states: RnnStateStorage, restoration_indices: torch.LongTensor) ->None:
        """
        After the RNN has run forward, the states need to be updated.
        This method just sets the state to the updated new state, performing
        several pieces of book-keeping along the way - namely, unsorting the
        states and ensuring that the states of completely padded sequences are
        not updated. Finally, it also detaches the state variable from the
        computational graph, such that the graph can be garbage collected after
        each batch iteration.

        # Parameters

        final_states : `RnnStateStorage`, required.
            The hidden states returned as output from the RNN.
        restoration_indices : `torch.LongTensor`, required.
            The indices that invert the sorting used in `sort_and_run_forward`
            to order the states with respect to the lengths of the sequences in
            the batch.
        """
        new_unsorted_states = [state.index_select(1, restoration_indices) for state in final_states]
        if self._states is None:
            self._states = tuple(state.data for state in new_unsorted_states)
        else:
            current_state_batch_size = self._states[0].size(1)
            new_state_batch_size = final_states[0].size(1)
            used_new_rows_mask = [(state[0, :, :].sum(-1) != 0.0).float().view(1, new_state_batch_size, 1) for state in new_unsorted_states]
            new_states = []
            if current_state_batch_size > new_state_batch_size:
                for old_state, new_state, used_mask in zip(self._states, new_unsorted_states, used_new_rows_mask):
                    masked_old_state = old_state[:, :new_state_batch_size, :] * (1 - used_mask)
                    old_state[:, :new_state_batch_size, :] = new_state + masked_old_state
                    new_states.append(old_state.detach())
            else:
                new_states = []
                for old_state, new_state, used_mask in zip(self._states, new_unsorted_states, used_new_rows_mask):
                    masked_old_state = old_state * (1 - used_mask)
                    new_state += masked_old_state
                    new_states.append(new_state.detach())
            self._states = tuple(new_states)

    def reset_states(self, mask: torch.BoolTensor=None) ->None:
        """
        Resets the internal states of a stateful encoder.

        # Parameters

        mask : `torch.BoolTensor`, optional.
            A tensor of shape `(batch_size,)` indicating which states should
            be reset. If not provided, all states will be reset.
        """
        if mask is None:
            self._states = None
        else:
            mask_batch_size = mask.size(0)
            mask = mask.view(1, mask_batch_size, 1)
            new_states = []
            assert self._states is not None
            for old_state in self._states:
                old_state_batch_size = old_state.size(1)
                if old_state_batch_size != mask_batch_size:
                    raise ValueError(f'Trying to reset states using mask with incorrect batch size. Expected batch size: {old_state_batch_size}. Provided batch size: {mask_batch_size}.')
                new_state = ~mask * old_state
                new_states.append(new_state.detach())
            self._states = tuple(new_states)


TypedStringSpan = Tuple[str, Tuple[int, int]]


TAGS_TO_SPANS_FUNCTION_TYPE = Callable[[List[str], Optional[List[str]]], List[TypedStringSpan]]


class InvalidTagSequence(Exception):

    def __init__(self, tag_sequence=None):
        super().__init__()
        self.tag_sequence = tag_sequence

    def __str__(self):
        return ' '.join(self.tag_sequence)


def bio_tags_to_spans(tag_sequence: List[str], classes_to_ignore: List[str]=None) ->List[TypedStringSpan]:
    """
    Given a sequence corresponding to BIO tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are also included (i.e those which do not start with a "B-LABEL"),
    as otherwise it is possible to get a perfect precision score whilst still predicting
    ill-formed spans in addition to the correct spans. This function works properly when
    the spans are unlabeled (i.e., your labels are simply "B", "I", and "O").

    # Parameters

    tag_sequence : `List[str]`, required.
        The integer class labels for a sequence.
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
        Note that the label `does not` contain any BIO tag prefixes.
    """
    classes_to_ignore = classes_to_ignore or []
    spans: Set[Tuple[str, Tuple[int, int]]] = set()
    span_start = 0
    span_end = 0
    active_conll_tag = None
    for index, string_tag in enumerate(tag_sequence):
        bio_tag = string_tag[0]
        if bio_tag not in ['B', 'I', 'O']:
            raise InvalidTagSequence(tag_sequence)
        conll_tag = string_tag[2:]
        if bio_tag == 'O' or conll_tag in classes_to_ignore:
            if active_conll_tag is not None:
                spans.add((active_conll_tag, (span_start, span_end)))
            active_conll_tag = None
            continue
        elif bio_tag == 'B':
            if active_conll_tag is not None:
                spans.add((active_conll_tag, (span_start, span_end)))
            active_conll_tag = conll_tag
            span_start = index
            span_end = index
        elif bio_tag == 'I' and conll_tag == active_conll_tag:
            span_end += 1
        else:
            if active_conll_tag is not None:
                spans.add((active_conll_tag, (span_start, span_end)))
            active_conll_tag = conll_tag
            span_start = index
            span_end = index
    if active_conll_tag is not None:
        spans.add((active_conll_tag, (span_start, span_end)))
    return list(spans)


def bioul_tags_to_spans(tag_sequence: List[str], classes_to_ignore: List[str]=None) ->List[TypedStringSpan]:
    """
    Given a sequence corresponding to BIOUL tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.
    This function works properly when the spans are unlabeled (i.e., your labels are
    simply "B", "I", "O", "U", and "L").

    # Parameters

    tag_sequence : `List[str]`, required.
        The tag sequence encoded in BIOUL, e.g. ["B-PER", "L-PER", "O"].
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
    """
    spans = []
    classes_to_ignore = classes_to_ignore or []
    index = 0
    while index < len(tag_sequence):
        label = tag_sequence[index]
        if label[0] == 'U':
            spans.append((label.partition('-')[2], (index, index)))
        elif label[0] == 'B':
            start = index
            while label[0] != 'L':
                index += 1
                if index >= len(tag_sequence):
                    raise InvalidTagSequence(tag_sequence)
                label = tag_sequence[index]
                if not (label[0] == 'I' or label[0] == 'L'):
                    raise InvalidTagSequence(tag_sequence)
            spans.append((label.partition('-')[2], (start, index)))
        elif label != 'O':
            raise InvalidTagSequence(tag_sequence)
        index += 1
    return [span for span in spans if span[0] not in classes_to_ignore]


def bmes_tags_to_spans(tag_sequence: List[str], classes_to_ignore: List[str]=None) ->List[TypedStringSpan]:
    """
    Given a sequence corresponding to BMES tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are also included (i.e those which do not start with a "B-LABEL"),
    as otherwise it is possible to get a perfect precision score whilst still predicting
    ill-formed spans in addition to the correct spans.
    This function works properly when the spans are unlabeled (i.e., your labels are
    simply "B", "M", "E" and "S").

    # Parameters

    tag_sequence : `List[str]`, required.
        The integer class labels for a sequence.
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
        Note that the label `does not` contain any BIO tag prefixes.
    """

    def extract_bmes_tag_label(text):
        bmes_tag = text[0]
        label = text[2:]
        return bmes_tag, label
    spans: List[Tuple[str, List[int]]] = []
    prev_bmes_tag: Optional[str] = None
    for index, tag in enumerate(tag_sequence):
        bmes_tag, label = extract_bmes_tag_label(tag)
        if bmes_tag in ('B', 'S'):
            spans.append((label, [index, index]))
        elif bmes_tag in ('M', 'E') and prev_bmes_tag in ('B', 'M') and spans[-1][0] == label:
            spans[-1][1][1] = index
        else:
            spans.append((label, [index, index]))
        prev_bmes_tag = bmes_tag
    classes_to_ignore = classes_to_ignore or []
    return [(span[0], (span[1][0], span[1][1])) for span in spans if span[0] not in classes_to_ignore]


def _iob1_start_of_chunk(prev_bio_tag: Optional[str], prev_conll_tag: Optional[str], curr_bio_tag: str, curr_conll_tag: str) ->bool:
    if curr_bio_tag == 'B':
        return True
    if curr_bio_tag == 'I' and prev_bio_tag == 'O':
        return True
    if curr_bio_tag != 'O' and prev_conll_tag != curr_conll_tag:
        return True
    return False


def iob1_tags_to_spans(tag_sequence: List[str], classes_to_ignore: List[str]=None) ->List[TypedStringSpan]:
    """
    Given a sequence corresponding to IOB1 tags, extracts spans.
    Spans are inclusive and can be of zero length, representing a single word span.
    Ill-formed spans are also included (i.e., those where "B-LABEL" is not preceded
    by "I-LABEL" or "B-LABEL").

    # Parameters

    tag_sequence : `List[str]`, required.
        The integer class labels for a sequence.
    classes_to_ignore : `List[str]`, optional (default = `None`).
        A list of string class labels `excluding` the bio tag
        which should be ignored when extracting spans.

    # Returns

    spans : `List[TypedStringSpan]`
        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).
        Note that the label `does not` contain any BIO tag prefixes.
    """
    classes_to_ignore = classes_to_ignore or []
    spans: Set[Tuple[str, Tuple[int, int]]] = set()
    span_start = 0
    span_end = 0
    active_conll_tag = None
    prev_bio_tag = None
    prev_conll_tag = None
    for index, string_tag in enumerate(tag_sequence):
        curr_bio_tag = string_tag[0]
        curr_conll_tag = string_tag[2:]
        if curr_bio_tag not in ['B', 'I', 'O']:
            raise InvalidTagSequence(tag_sequence)
        if curr_bio_tag == 'O' or curr_conll_tag in classes_to_ignore:
            if active_conll_tag is not None:
                spans.add((active_conll_tag, (span_start, span_end)))
            active_conll_tag = None
        elif _iob1_start_of_chunk(prev_bio_tag, prev_conll_tag, curr_bio_tag, curr_conll_tag):
            if active_conll_tag is not None:
                spans.add((active_conll_tag, (span_start, span_end)))
            active_conll_tag = curr_conll_tag
            span_start = index
            span_end = index
        else:
            span_end += 1
        prev_bio_tag = string_tag[0]
        prev_conll_tag = string_tag[2:]
    if active_conll_tag is not None:
        spans.add((active_conll_tag, (span_start, span_end)))
    return list(spans)


class TimeDistributed(torch.nn.Module):
    """
    Given an input shaped like `(batch_size, time_steps, [rest])` and a `Module` that takes
    inputs like `(batch_size, [rest])`, `TimeDistributed` reshapes the input to be
    `(batch_size * time_steps, [rest])`, applies the contained `Module`, then reshapes it back.

    Note that while the above gives shapes with `batch_size` first, this `Module` also works if
    `batch_size` is second - we always just combine the first two dimensions, then split them.

    It also reshapes keyword arguments unless they are not tensors or their name is specified in
    the optional `pass_through` iterable.
    """

    def __init__(self, module):
        super().__init__()
        self._module = module

    def forward(self, *inputs, pass_through: List[str]=None, **kwargs):
        pass_through = pass_through or []
        reshaped_inputs = [self._reshape_tensor(input_tensor) for input_tensor in inputs]
        some_input = None
        if inputs:
            some_input = inputs[-1]
        reshaped_kwargs = {}
        for key, value in kwargs.items():
            if isinstance(value, torch.Tensor) and key not in pass_through:
                if some_input is None:
                    some_input = value
                value = self._reshape_tensor(value)
            reshaped_kwargs[key] = value
        reshaped_outputs = self._module(*reshaped_inputs, **reshaped_kwargs)
        if some_input is None:
            raise RuntimeError('No input tensor to time-distribute')
        tuple_output = True
        if not isinstance(reshaped_outputs, tuple):
            tuple_output = False
            reshaped_outputs = reshaped_outputs,
        outputs = []
        for reshaped_output in reshaped_outputs:
            new_size = some_input.size()[:2] + reshaped_output.size()[1:]
            outputs.append(reshaped_output.contiguous().view(new_size))
        if not tuple_output:
            outputs = outputs[0]
        return outputs

    @staticmethod
    def _reshape_tensor(input_tensor):
        input_size = input_tensor.size()
        if len(input_size) <= 2:
            raise RuntimeError(f'No dimension to distribute: {input_size}')
        squashed_shape = [-1] + list(input_size[2:])
        return input_tensor.contiguous().view(*squashed_shape)


def check_dimensions_match(dimension_1: int, dimension_2: int, dim_1_name: str, dim_2_name: str) ->None:
    if dimension_1 != dimension_2:
        raise ConfigurationError(f'{dim_1_name} must match {dim_2_name}, but got {dimension_1} and {dimension_2} instead')


def get_text_field_mask(text_field_tensors: Dict[str, Dict[str, torch.Tensor]], num_wrapping_dims: int=0, padding_id: int=0) ->torch.BoolTensor:
    """
    Takes the dictionary of tensors produced by a `TextField` and returns a mask
    with 0 where the tokens are padding, and 1 otherwise. `padding_id` specifies the id of padding tokens.
    We also handle `TextFields` wrapped by an arbitrary number of `ListFields`, where the number of wrapping
    `ListFields` is given by `num_wrapping_dims`.

    If `num_wrapping_dims == 0`, the returned mask has shape `(batch_size, num_tokens)`.
    If `num_wrapping_dims > 0` then the returned mask has `num_wrapping_dims` extra
    dimensions, so the shape will be `(batch_size, ..., num_tokens)`.

    There could be several entries in the tensor dictionary with different shapes (e.g., one for
    word ids, one for character ids).  In order to get a token mask, we use the tensor in
    the dictionary with the lowest number of dimensions.  After subtracting `num_wrapping_dims`,
    if this tensor has two dimensions we assume it has shape `(batch_size, ..., num_tokens)`,
    and use it for the mask.  If instead it has three dimensions, we assume it has shape
    `(batch_size, ..., num_tokens, num_features)`, and sum over the last dimension to produce
    the mask.  Most frequently this will be a character id tensor, but it could also be a
    featurized representation of each token, etc.

    If the input `text_field_tensors` contains the "mask" key, this is returned instead of inferring the mask.
    """
    masks = []
    for indexer_name, indexer_tensors in text_field_tensors.items():
        if 'mask' in indexer_tensors:
            masks.append(indexer_tensors['mask'].bool())
    if len(masks) == 1:
        return masks[0]
    elif len(masks) > 1:
        raise ValueError('found two mask outputs; not sure which to use!')
    tensor_dims = [(tensor.dim(), tensor) for indexer_output in text_field_tensors.values() for tensor in indexer_output.values()]
    tensor_dims.sort(key=lambda x: x[0])
    smallest_dim = tensor_dims[0][0] - num_wrapping_dims
    if smallest_dim == 2:
        token_tensor = tensor_dims[0][1]
        return token_tensor != padding_id
    elif smallest_dim == 3:
        character_tensor = tensor_dims[0][1]
        return (character_tensor != padding_id).any(dim=-1)
    else:
        raise ValueError('Expected a tensor with dimension 2 or 3, found {}'.format(smallest_dim))


def tiny_value_of_dtype(dtype: torch.dtype):
    """
    Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical
    issues such as division by zero.
    This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.
    Only supports floating point dtypes.
    """
    if not dtype.is_floating_point:
        raise TypeError('Only supports floating point dtypes.')
    if dtype == torch.float or dtype == torch.double:
        return 1e-13
    elif dtype == torch.half:
        return 0.0001
    else:
        raise TypeError('Does not support dtype ' + str(dtype))


def sequence_cross_entropy_with_logits(logits: torch.FloatTensor, targets: torch.LongTensor, weights: Union[torch.FloatTensor, torch.BoolTensor], average: str='batch', label_smoothing: float=None, gamma: float=None, alpha: Union[float, List[float], torch.FloatTensor]=None) ->torch.FloatTensor:
    """
    Computes the cross entropy loss of a sequence, weighted with respect to
    some user provided weights. Note that the weighting here is not the same as
    in the `torch.nn.CrossEntropyLoss()` criterion, which is weighting
    classes; here we are weighting the loss contribution from particular elements
    in the sequence. This allows loss computations for models which use padding.

    # Parameters

    logits : `torch.FloatTensor`, required.
        A `torch.FloatTensor` of size (batch_size, sequence_length, num_classes)
        which contains the unnormalized probability for each class.
    targets : `torch.LongTensor`, required.
        A `torch.LongTensor` of size (batch, sequence_length) which contains the
        index of the true class for each corresponding step.
    weights : `Union[torch.FloatTensor, torch.BoolTensor]`, required.
        A `torch.FloatTensor` of size (batch, sequence_length)
    average: `str`, optional (default = `"batch"`)
        If "batch", average the loss across the batches. If "token", average
        the loss across each item in the input. If `None`, return a vector
        of losses per batch element.
    label_smoothing : `float`, optional (default = `None`)
        Whether or not to apply label smoothing to the cross-entropy loss.
        For example, with a label smoothing value of 0.2, a 4 class classification
        target would look like `[0.05, 0.05, 0.85, 0.05]` if the 3rd class was
        the correct label.
    gamma : `float`, optional (default = `None`)
        Focal loss[*] focusing parameter `gamma` to reduces the relative loss for
        well-classified examples and put more focus on hard. The greater value
        `gamma` is, the more focus on hard examples.
    alpha : `Union[float, List[float]]`, optional (default = `None`)
        Focal loss[*] weighting factor `alpha` to balance between classes. Can be
        used independently with `gamma`. If a single `float` is provided, it
        is assumed binary case using `alpha` and `1 - alpha` for positive and
        negative respectively. If a list of `float` is provided, with the same
        length as the number of classes, the weights will match the classes.
        [*] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, "Focal Loss for
        Dense Object Detection," 2017 IEEE International Conference on Computer
        Vision (ICCV), Venice, 2017, pp. 2999-3007.

    # Returns

    `torch.FloatTensor`
        A torch.FloatTensor representing the cross entropy loss.
        If `average=="batch"` or `average=="token"`, the returned loss is a scalar.
        If `average is None`, the returned loss is a vector of shape (batch_size,).

    """
    if average not in {None, 'token', 'batch'}:
        raise ValueError(f"Got average f{average}, expected one of None, 'token', or 'batch'")
    weights = weights
    non_batch_dims = tuple(range(1, len(weights.shape)))
    weights_batch_sum = weights.sum(dim=non_batch_dims)
    logits_flat = logits.view(-1, logits.size(-1))
    log_probs_flat = torch.nn.functional.log_softmax(logits_flat, dim=-1)
    targets_flat = targets.view(-1, 1).long()
    if gamma:
        probs_flat = log_probs_flat.exp()
        probs_flat = torch.gather(probs_flat, dim=1, index=targets_flat)
        focal_factor = (1.0 - probs_flat) ** gamma
        focal_factor = focal_factor.view(*targets.size())
        weights = weights * focal_factor
    if alpha is not None:
        if isinstance(alpha, (float, int)):
            alpha_factor = torch.tensor([1.0 - float(alpha), float(alpha)], dtype=weights.dtype, device=weights.device)
        elif isinstance(alpha, (list, numpy.ndarray, torch.Tensor)):
            alpha_factor = torch.tensor(alpha, dtype=weights.dtype, device=weights.device)
            if not alpha_factor.size():
                alpha_factor = alpha_factor.view(1)
                alpha_factor = torch.cat([1 - alpha_factor, alpha_factor])
        else:
            raise TypeError('alpha must be float, list of float, or torch.FloatTensor, {} provided.'.format(type(alpha)))
        alpha_factor = torch.gather(alpha_factor, dim=0, index=targets_flat.view(-1)).view(*targets.size())
        weights = weights * alpha_factor
    if label_smoothing is not None and label_smoothing > 0.0:
        num_classes = logits.size(-1)
        smoothing_value = label_smoothing / num_classes
        smoothed_targets = torch.full_like(log_probs_flat, smoothing_value).scatter_(-1, targets_flat, 1.0 - label_smoothing + smoothing_value)
        negative_log_likelihood_flat = -log_probs_flat * smoothed_targets
        negative_log_likelihood_flat = negative_log_likelihood_flat.sum(-1, keepdim=True)
    else:
        negative_log_likelihood_flat = -torch.gather(log_probs_flat, dim=1, index=targets_flat)
    negative_log_likelihood = negative_log_likelihood_flat.view(*targets.size())
    negative_log_likelihood = negative_log_likelihood * weights
    if average == 'batch':
        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype))
        num_non_empty_sequences = (weights_batch_sum > 0).sum() + tiny_value_of_dtype(negative_log_likelihood.dtype)
        return per_batch_loss.sum() / num_non_empty_sequences
    elif average == 'token':
        return negative_log_likelihood.sum() / (weights_batch_sum.sum() + tiny_value_of_dtype(negative_log_likelihood.dtype))
    else:
        per_batch_loss = negative_log_likelihood.sum(non_batch_dims) / (weights_batch_sum + tiny_value_of_dtype(negative_log_likelihood.dtype))
        return per_batch_loss


def info_value_of_dtype(dtype: torch.dtype):
    """
    Returns the `finfo` or `iinfo` object of a given PyTorch data type. Does not allow torch.bool.
    """
    if dtype == torch.bool:
        raise TypeError('Does not support torch.bool')
    elif dtype.is_floating_point:
        return torch.finfo(dtype)
    else:
        return torch.iinfo(dtype)


def min_value_of_dtype(dtype: torch.dtype):
    """
    Returns the minimum value of a given PyTorch data type. Does not allow torch.bool.
    """
    return info_value_of_dtype(dtype).min


def masked_softmax(vector: torch.Tensor, mask: torch.BoolTensor, dim: int=-1, memory_efficient: bool=False) ->torch.Tensor:
    """
    `torch.nn.functional.softmax(vector)` does not work if some elements of `vector` should be
    masked.  This performs a softmax on just the non-masked portions of `vector`.  Passing
    `None` in for the mask is also acceptable; you'll just get a regular softmax.

    `vector` can have an arbitrary number of dimensions; the only requirement is that `mask` is
    broadcastable to `vector's` shape.  If `mask` has fewer dimensions than `vector`, we will
    unsqueeze on dimension 1 until they match.  If you need a different unsqueezing of your mask,
    do it yourself before passing the mask into this function.

    If `memory_efficient` is set to true, we will simply use a very large negative number for those
    masked positions so that the probabilities of those positions would be approximately 0.
    This is not accurate in math, but works for most cases and consumes less memory.

    In the case that the input vector is completely masked and `memory_efficient` is false, this function
    returns an array of `0.0`. This behavior may cause `NaN` if this is used as the last layer of
    a model that uses categorical cross-entropy loss. Instead, if `memory_efficient` is true, this function
    will treat every element as equal, and do softmax over equal numbers.
    """
    if mask is None:
        result = torch.nn.functional.softmax(vector, dim=dim)
    else:
        while mask.dim() < vector.dim():
            mask = mask.unsqueeze(1)
        if not memory_efficient:
            result = torch.nn.functional.softmax(vector * mask, dim=dim)
            result = result * mask
            result = result / (result.sum(dim=dim, keepdim=True) + tiny_value_of_dtype(result.dtype))
        else:
            masked_vector = vector.masked_fill(~mask, min_value_of_dtype(vector.dtype))
            result = torch.nn.functional.softmax(masked_vector, dim=dim)
    return result


def block_orthogonal(tensor: torch.Tensor, split_sizes: List[int], gain: float=1.0) ->None:
    """
    An initializer which allows initializing model parameters in "blocks". This is helpful
    in the case of recurrent models which use multiple gates applied to linear projections,
    which can be computed efficiently if they are concatenated together. However, they are
    separate parameters which should be initialized independently.

    # Parameters

    tensor : `torch.Tensor`, required.
        A tensor to initialize.
    split_sizes : `List[int]`, required.
        A list of length `tensor.ndim()` specifying the size of the
        blocks along that particular dimension. E.g. `[10, 20]` would
        result in the tensor being split into chunks of size 10 along the
        first dimension and 20 along the second.
    gain : `float`, optional (default = `1.0`)
        The gain (scaling) applied to the orthogonal initialization.
    """
    data = tensor.data
    sizes = list(tensor.size())
    if any(a % b != 0 for a, b in zip(sizes, split_sizes)):
        raise ConfigurationError('tensor dimensions must be divisible by their respective split_sizes. Found size: {} and split_sizes: {}'.format(sizes, split_sizes))
    indexes = [list(range(0, max_size, split)) for max_size, split in zip(sizes, split_sizes)]
    for block_start_indices in itertools.product(*indexes):
        index_and_step_tuples = zip(block_start_indices, split_sizes)
        block_slice = tuple(slice(start_index, start_index + step) for start_index, step in index_and_step_tuples)
        data[block_slice] = torch.nn.init.orthogonal_(tensor[block_slice].contiguous(), gain=gain)


class AugmentedLSTMCell(torch.nn.Module):
    """
    `AugmentedLSTMCell` implements a AugmentedLSTM cell.

    # Parameters

    embed_dim : `int`
        The number of expected features in the input.
    lstm_dim : `int`
        Number of features in the hidden state of the LSTM.
    use_highway : `bool`, optional (default = `True`)
        If `True` we append a highway network to the outputs of the LSTM.
    use_bias : `bool`, optional (default = `True`)
        If `True` we use a bias in our LSTM calculations, otherwise we don't.

    # Attributes

    input_linearity : `nn.Module`
        Fused weight matrix which computes a linear function over the input.
    state_linearity : `nn.Module`
        Fused weight matrix which computes a linear function over the states.
    """

    def __init__(self, embed_dim: int, lstm_dim: int, use_highway: bool=True, use_bias: bool=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.lstm_dim = lstm_dim
        self.use_highway = use_highway
        self.use_bias = use_bias
        if use_highway:
            self._highway_inp_proj_start = 5 * self.lstm_dim
            self._highway_inp_proj_end = 6 * self.lstm_dim
            self.input_linearity = torch.nn.Linear(self.embed_dim, self._highway_inp_proj_end, bias=self.use_bias)
            self.state_linearity = torch.nn.Linear(self.lstm_dim, self._highway_inp_proj_start, bias=True)
        else:
            self.input_linearity = torch.nn.Linear(self.embed_dim, 4 * self.lstm_dim, bias=self.use_bias)
            self.state_linearity = torch.nn.Linear(self.lstm_dim, 4 * self.lstm_dim, bias=True)
        self.reset_parameters()

    def reset_parameters(self):
        block_orthogonal(self.input_linearity.weight.data, [self.lstm_dim, self.embed_dim])
        block_orthogonal(self.state_linearity.weight.data, [self.lstm_dim, self.lstm_dim])
        self.state_linearity.bias.data.fill_(0.0)
        self.state_linearity.bias.data[self.lstm_dim:2 * self.lstm_dim].fill_(1.0)

    def forward(self, x: torch.Tensor, states=Tuple[torch.Tensor, torch.Tensor], variational_dropout_mask: Optional[torch.BoolTensor]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        !!! Warning
            DO NOT USE THIS LAYER DIRECTLY, instead use the AugmentedLSTM class

        # Parameters

        x : `torch.Tensor`
            Input tensor of shape (bsize x input_dim).
        states : `Tuple[torch.Tensor, torch.Tensor]`
            Tuple of tensors containing
            the hidden state and the cell state of each element in
            the batch. Each of these tensors have a dimension of
            (bsize x nhid). Defaults to `None`.

        # Returns

        `Tuple[torch.Tensor, torch.Tensor]`
            Returned states. Shape of each state is (bsize x nhid).

        """
        hidden_state, memory_state = states
        if variational_dropout_mask is not None and self.training:
            hidden_state = hidden_state * variational_dropout_mask
        projected_input = self.input_linearity(x)
        projected_state = self.state_linearity(hidden_state)
        input_gate = forget_gate = memory_init = output_gate = highway_gate = None
        if self.use_highway:
            fused_op = projected_input[:, :5 * self.lstm_dim] + projected_state
            fused_chunked = torch.chunk(fused_op, 5, 1)
            input_gate, forget_gate, memory_init, output_gate, highway_gate = fused_chunked
            highway_gate = torch.sigmoid(highway_gate)
        else:
            fused_op = projected_input + projected_state
            input_gate, forget_gate, memory_init, output_gate = torch.chunk(fused_op, 4, 1)
        input_gate = torch.sigmoid(input_gate)
        forget_gate = torch.sigmoid(forget_gate)
        memory_init = torch.tanh(memory_init)
        output_gate = torch.sigmoid(output_gate)
        memory = input_gate * memory_init + forget_gate * memory_state
        timestep_output: torch.Tensor = output_gate * torch.tanh(memory)
        if self.use_highway:
            highway_input_projection = projected_input[:, self._highway_inp_proj_start:self._highway_inp_proj_end]
            timestep_output = highway_gate * timestep_output + (1 - highway_gate) * highway_input_projection
        return timestep_output, memory


def get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.Tensor):
    """
    Computes and returns an element-wise dropout mask for a given tensor, where
    each element in the mask is dropped out with probability dropout_probability.
    Note that the mask is NOT applied to the tensor - the tensor is passed to retain
    the correct CUDA tensor type for the mask.

    # Parameters

    dropout_probability : `float`, required.
        Probability of dropping a dimension of the input.
    tensor_for_masking : `torch.Tensor`, required.


    # Returns

    `torch.FloatTensor`
        A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability).
        This scaling ensures expected values and variances of the output of applying this mask
        and the original tensor are the same.
    """
    binary_mask = torch.rand(tensor_for_masking.size()) > dropout_probability
    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)
    return dropout_mask


class AugmentedLstm(torch.nn.Module):
    """
    `AugmentedLstm` implements a one-layer single directional
    AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally
    appends an optional highway network to the output layer. Furthermore the
    dropout controls the level of variational dropout done.

    # Parameters

    input_size : `int`
        The number of expected features in the input.
    hidden_size : `int`
        Number of features in the hidden state of the LSTM.
        Defaults to 32.
    go_forward : `bool`
        Whether to compute features left to right (forward)
        or right to left (backward).
    recurrent_dropout_probability : `float`
        Variational dropout probability to use. Defaults to 0.0.
    use_highway : `bool`
        If `True` we append a highway network to the outputs of the LSTM.
    use_input_projection_bias : `bool`
        If `True` we use a bias in our LSTM calculations, otherwise we don't.

    # Attributes

    cell : `AugmentedLSTMCell`
        `AugmentedLSTMCell` that is applied at every timestep.

    """

    def __init__(self, input_size: int, hidden_size: int, go_forward: bool=True, recurrent_dropout_probability: float=0.0, use_highway: bool=True, use_input_projection_bias: bool=True):
        super().__init__()
        self.embed_dim = input_size
        self.lstm_dim = hidden_size
        self.go_forward = go_forward
        self.use_highway = use_highway
        self.recurrent_dropout_probability = recurrent_dropout_probability
        self.cell = AugmentedLSTMCell(self.embed_dim, self.lstm_dim, self.use_highway, use_input_projection_bias)

    def forward(self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None) ->Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Warning: Would be better to use the BiAugmentedLstm class in a regular model

        Given an input batch of sequential data such as word embeddings, produces a single layer unidirectional
        AugmentedLSTM representation of the sequential input and new state tensors.

        # Parameters

        inputs : `PackedSequence`
            `bsize` sequences of shape `(len, input_dim)` each, in PackedSequence format
        states : `Tuple[torch.Tensor, torch.Tensor]`
            Tuple of tensors containing the initial hidden state and
            the cell state of each element in the batch. Each of these tensors have a dimension of
            (1 x bsize x nhid). Defaults to `None`.

        # Returns

        `Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]`
            AugmentedLSTM representation of input and the state of the LSTM `t = seq_len`.
            Shape of representation is (bsize x seq_len x representation_dim).
            Shape of each state is (1 x bsize x nhid).

        """
        if not isinstance(inputs, PackedSequence):
            raise ConfigurationError('inputs must be PackedSequence but got %s' % type(inputs))
        sequence_tensor, batch_lengths = pad_packed_sequence(inputs, batch_first=True)
        batch_size = sequence_tensor.size()[0]
        total_timesteps = sequence_tensor.size()[1]
        output_accumulator = sequence_tensor.new_zeros(batch_size, total_timesteps, self.lstm_dim)
        if states is None:
            full_batch_previous_memory = sequence_tensor.new_zeros(batch_size, self.lstm_dim)
            full_batch_previous_state = sequence_tensor.data.new_zeros(batch_size, self.lstm_dim)
        else:
            full_batch_previous_state = states[0].squeeze(0)
            full_batch_previous_memory = states[1].squeeze(0)
        current_length_index = batch_size - 1 if self.go_forward else 0
        if self.recurrent_dropout_probability > 0.0:
            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability, full_batch_previous_memory)
        else:
            dropout_mask = None
        for timestep in range(total_timesteps):
            index = timestep if self.go_forward else total_timesteps - timestep - 1
            if self.go_forward:
                while batch_lengths[current_length_index] <= index:
                    current_length_index -= 1
            else:
                while current_length_index < len(batch_lengths) - 1 and batch_lengths[current_length_index + 1] > index:
                    current_length_index += 1
            previous_memory = full_batch_previous_memory[0:current_length_index + 1].clone()
            previous_state = full_batch_previous_state[0:current_length_index + 1].clone()
            timestep_input = sequence_tensor[0:current_length_index + 1, index]
            timestep_output, memory = self.cell(timestep_input, (previous_state, previous_memory), dropout_mask[0:current_length_index + 1] if dropout_mask is not None else None)
            full_batch_previous_memory = full_batch_previous_memory.data.clone()
            full_batch_previous_state = full_batch_previous_state.data.clone()
            full_batch_previous_memory[0:current_length_index + 1] = memory
            full_batch_previous_state[0:current_length_index + 1] = timestep_output
            output_accumulator[0:current_length_index + 1, index, :] = timestep_output
        output_accumulator = pack_padded_sequence(output_accumulator, batch_lengths, batch_first=True)
        final_state = full_batch_previous_state.unsqueeze(0), full_batch_previous_memory.unsqueeze(0)
        return output_accumulator, final_state


class BiAugmentedLstm(torch.nn.Module):
    """
    `BiAugmentedLstm` implements a generic AugmentedLSTM representation layer.
    BiAugmentedLstm is an LSTM which optionally appends an optional highway network to the output layer.
    Furthermore the dropout controls the level of variational dropout done.

    # Parameters

    input_size : `int`, required
        The dimension of the inputs to the LSTM.
    hidden_size : `int`, required.
        The dimension of the outputs of the LSTM.
    num_layers : `int`
        Number of recurrent layers. Eg. setting `num_layers=2`
        would mean stacking two LSTMs together to form a stacked LSTM,
        with the second LSTM taking in the outputs of the first LSTM and
        computing the final result. Defaults to 1.
    bias : `bool`
        If `True` we use a bias in our LSTM calculations, otherwise we don't.
    recurrent_dropout_probability : `float`, optional (default = `0.0`)
        Variational dropout probability to use.
    bidirectional : `bool`
        If `True`, becomes a bidirectional LSTM. Defaults to `True`.
    padding_value : `float`, optional (default = `0.0`)
        Value for the padded elements. Defaults to 0.0.
    use_highway : `bool`, optional (default = `True`)
        Whether or not to use highway connections between layers. This effectively involves
        reparameterising the normal output of an LSTM as::

            gate = sigmoid(W_x1 * x_t + W_h * h_t)
            output = gate * h_t  + (1 - gate) * (W_x2 * x_t)

    # Returns

    output_accumulator : `PackedSequence`
        The outputs of the LSTM for each timestep. A tensor of shape (batch_size, max_timesteps, hidden_size) where
        for a given batch element, all outputs past the sequence length for that batch are zero tensors.
    """

    def __init__(self, input_size: int, hidden_size: int, num_layers: int=1, bias: bool=True, recurrent_dropout_probability: float=0.0, bidirectional: bool=False, padding_value: float=0.0, use_highway: bool=True) ->None:
        super().__init__()
        self.input_size = input_size
        self.padding_value = padding_value
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.recurrent_dropout_probability = recurrent_dropout_probability
        self.use_highway = use_highway
        self.use_bias = bias
        num_directions = int(self.bidirectional) + 1
        self.forward_layers = torch.nn.ModuleList()
        if self.bidirectional:
            self.backward_layers = torch.nn.ModuleList()
        lstm_embed_dim = self.input_size
        for _ in range(self.num_layers):
            self.forward_layers.append(AugmentedLstm(lstm_embed_dim, self.hidden_size, go_forward=True, recurrent_dropout_probability=self.recurrent_dropout_probability, use_highway=self.use_highway, use_input_projection_bias=self.use_bias))
            if self.bidirectional:
                self.backward_layers.append(AugmentedLstm(lstm_embed_dim, self.hidden_size, go_forward=False, recurrent_dropout_probability=self.recurrent_dropout_probability, use_highway=self.use_highway, use_input_projection_bias=self.use_bias))
            lstm_embed_dim = self.hidden_size * num_directions
        self.representation_dim = lstm_embed_dim

    def forward(self, inputs: torch.Tensor, states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Given an input batch of sequential data such as word embeddings, produces
        a AugmentedLSTM representation of the sequential input and new state
        tensors.

        # Parameters

        inputs : `PackedSequence`, required.
            A tensor of shape (batch_size, num_timesteps, input_size)
            to apply the LSTM over.
        states : `Tuple[torch.Tensor, torch.Tensor]`
            Tuple of tensors containing
            the initial hidden state and the cell state of each element in
            the batch. Each of these tensors have a dimension of
            (bsize x num_layers x num_directions * nhid). Defaults to `None`.

        # Returns

        `Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]`
            AgumentedLSTM representation of input and
            the state of the LSTM `t = seq_len`.
            Shape of representation is (bsize x seq_len x representation_dim).
            Shape of each state is (bsize x num_layers * num_directions x nhid).

        """
        if not isinstance(inputs, PackedSequence):
            raise ConfigurationError('inputs must be PackedSequence but got %s' % type(inputs))
        if self.bidirectional:
            return self._forward_bidirectional(inputs, states)
        return self._forward_unidirectional(inputs, states)

    def _forward_bidirectional(self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]):
        output_sequence = inputs
        final_h = []
        final_c = []
        if not states:
            hidden_states = [None] * self.num_layers
        elif states[0].size()[0] != self.num_layers:
            raise RuntimeError('Initial states were passed to forward() but the number of initial states does not match the number of layers.')
        else:
            hidden_states = list(zip(states[0].chunk(self.num_layers, 0), states[1].chunk(self.num_layers, 0)))
        for i, state in enumerate(hidden_states):
            if state:
                forward_state = state[0].chunk(2, -1)
                backward_state = state[1].chunk(2, -1)
            else:
                forward_state = backward_state = None
            forward_layer = self.forward_layers[i]
            backward_layer = self.backward_layers[i]
            forward_output, final_forward_state = forward_layer(output_sequence, forward_state)
            backward_output, final_backward_state = backward_layer(output_sequence, backward_state)
            forward_output, lengths = pad_packed_sequence(forward_output, batch_first=True)
            backward_output, _ = pad_packed_sequence(backward_output, batch_first=True)
            output_sequence = torch.cat([forward_output, backward_output], -1)
            output_sequence = pack_padded_sequence(output_sequence, lengths, batch_first=True)
            final_h.extend([final_forward_state[0], final_backward_state[0]])
            final_c.extend([final_forward_state[1], final_backward_state[1]])
        final_h = torch.cat(final_h, dim=0)
        final_c = torch.cat(final_c, dim=0)
        final_state_tuple = final_h, final_c
        output_sequence, batch_lengths = pad_packed_sequence(output_sequence, padding_value=self.padding_value, batch_first=True)
        output_sequence = pack_padded_sequence(output_sequence, batch_lengths, batch_first=True)
        return output_sequence, final_state_tuple

    def _forward_unidirectional(self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]):
        output_sequence = inputs
        final_h = []
        final_c = []
        if not states:
            hidden_states = [None] * self.num_layers
        elif states[0].size()[0] != self.num_layers:
            raise RuntimeError('Initial states were passed to forward() but the number of initial states does not match the number of layers.')
        else:
            hidden_states = list(zip(states[0].chunk(self.num_layers, 0), states[1].chunk(self.num_layers, 0)))
        for i, state in enumerate(hidden_states):
            forward_layer = self.forward_layers[i]
            forward_output, final_forward_state = forward_layer(output_sequence, state)
            output_sequence = forward_output
            final_h.append(final_forward_state[0])
            final_c.append(final_forward_state[1])
        final_h = torch.cat(final_h, dim=0)
        final_c = torch.cat(final_c, dim=0)
        final_state_tuple = final_h, final_c
        output_sequence, batch_lengths = pad_packed_sequence(output_sequence, padding_value=self.padding_value, batch_first=True)
        output_sequence = pack_padded_sequence(output_sequence, batch_lengths, batch_first=True)
        return output_sequence, final_state_tuple


def sanitize_wordpiece(wordpiece: str) ->str:
    """
    Sanitizes wordpieces from BERT, RoBERTa or ALBERT tokenizers.
    """
    if wordpiece.startswith('##'):
        return wordpiece[2:]
    elif wordpiece.startswith('Ġ'):
        return wordpiece[1:]
    elif wordpiece.startswith('▁'):
        return wordpiece[1:]
    else:
        return wordpiece


class ScalarMix(torch.nn.Module):
    """
    Computes a parameterised scalar mixture of N tensors, `mixture = gamma * sum(s_k * tensor_k)`
    where `s = softmax(w)`, with `w` and `gamma` scalar parameters.

    In addition, if `do_layer_norm=True` then apply layer normalization to each tensor
    before weighting.
    """

    def __init__(self, mixture_size: int, do_layer_norm: bool=False, initial_scalar_parameters: List[float]=None, trainable: bool=True) ->None:
        super().__init__()
        self.mixture_size = mixture_size
        self.do_layer_norm = do_layer_norm
        if initial_scalar_parameters is None:
            initial_scalar_parameters = [0.0] * mixture_size
        elif len(initial_scalar_parameters) != mixture_size:
            raise ConfigurationError('Length of initial_scalar_parameters {} differs from mixture_size {}'.format(initial_scalar_parameters, mixture_size))
        self.scalar_parameters = ParameterList([Parameter(torch.FloatTensor([initial_scalar_parameters[i]]), requires_grad=trainable) for i in range(mixture_size)])
        self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)

    def forward(self, tensors: List[torch.Tensor], mask: torch.BoolTensor=None) ->torch.Tensor:
        """
        Compute a weighted average of the `tensors`.  The input tensors an be any shape
        with at least two dimensions, but must all be the same shape.

        When `do_layer_norm=True`, the `mask` is required input.  If the `tensors` are
        dimensioned  `(dim_0, ..., dim_{n-1}, dim_n)`, then the `mask` is dimensioned
        `(dim_0, ..., dim_{n-1})`, as in the typical case with `tensors` of shape
        `(batch_size, timesteps, dim)` and `mask` of shape `(batch_size, timesteps)`.

        When `do_layer_norm=False` the `mask` is ignored.
        """
        if len(tensors) != self.mixture_size:
            raise ConfigurationError('{} tensors were passed, but the module was initialized to mix {} tensors.'.format(len(tensors), self.mixture_size))

        def _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked):
            tensor_masked = tensor * broadcast_mask
            mean = torch.sum(tensor_masked) / num_elements_not_masked
            variance = torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2) / num_elements_not_masked
            return (tensor - mean) / torch.sqrt(variance + util.tiny_value_of_dtype(variance.dtype))
        normed_weights = torch.nn.functional.softmax(torch.cat([parameter for parameter in self.scalar_parameters]), dim=0)
        normed_weights = torch.split(normed_weights, split_size_or_sections=1)
        if not self.do_layer_norm:
            pieces = []
            for weight, tensor in zip(normed_weights, tensors):
                pieces.append(weight * tensor)
            return self.gamma * sum(pieces)
        else:
            assert mask is not None
            broadcast_mask = mask.unsqueeze(-1)
            input_dim = tensors[0].size(-1)
            num_elements_not_masked = torch.sum(mask) * input_dim
            pieces = []
            for weight, tensor in zip(normed_weights, tensors):
                pieces.append(weight * _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked))
            return self.gamma * sum(pieces)


def get_device_of(tensor: torch.Tensor) ->int:
    """
    Returns the device of the tensor.
    """
    if not tensor.is_cuda:
        return -1
    else:
        return tensor.get_device()


def get_range_vector(size: int, device: int) ->torch.Tensor:
    """
    Returns a range vector with the desired size, starting at 0. The CUDA implementation
    is meant to avoid copy data from CPU to GPU.
    """
    if device > -1:
        return torch.LongTensor(size, device=device).fill_(1).cumsum(0) - 1
    else:
        return torch.arange(0, size, dtype=torch.long)


def flatten_and_batch_shift_indices(indices: torch.Tensor, sequence_length: int) ->torch.Tensor:
    """
    This is a subroutine for [`batched_index_select`](./util.md#batched_index_select).
    The given `indices` of size `(batch_size, d_1, ..., d_n)` indexes into dimension 2 of a
    target tensor, which has size `(batch_size, sequence_length, embedding_size)`. This
    function returns a vector that correctly indexes into the flattened target. The sequence
    length of the target must be provided to compute the appropriate offsets.

    ```python
        indices = torch.ones([2,3], dtype=torch.long)
        # Sequence length of the target tensor.
        sequence_length = 10
        shifted_indices = flatten_and_batch_shift_indices(indices, sequence_length)
        # Indices into the second element in the batch are correctly shifted
        # to take into account that the target tensor will be flattened before
        # the indices are applied.
        assert shifted_indices == [1, 1, 1, 11, 11, 11]
    ```

    # Parameters

    indices : `torch.LongTensor`, required.
    sequence_length : `int`, required.
        The length of the sequence the indices index into.
        This must be the second dimension of the tensor.

    # Returns

    offset_indices : `torch.LongTensor`
    """
    if torch.max(indices) >= sequence_length or torch.min(indices) < 0:
        raise ConfigurationError(f'All elements in indices should be in range (0, {sequence_length - 1})')
    offsets = get_range_vector(indices.size(0), get_device_of(indices)) * sequence_length
    for _ in range(len(indices.size()) - 1):
        offsets = offsets.unsqueeze(1)
    offset_indices = indices + offsets
    offset_indices = offset_indices.view(-1)
    return offset_indices


def batched_index_select(target: torch.Tensor, indices: torch.LongTensor, flattened_indices: Optional[torch.LongTensor]=None) ->torch.Tensor:
    """
    The given `indices` of size `(batch_size, d_1, ..., d_n)` indexes into the sequence
    dimension (dimension 2) of the target, which has size `(batch_size, sequence_length,
    embedding_size)`.

    This function returns selected values in the target with respect to the provided indices, which
    have size `(batch_size, d_1, ..., d_n, embedding_size)`. This can use the optionally
    precomputed `flattened_indices` with size `(batch_size * d_1 * ... * d_n)` if given.

    An example use case of this function is looking up the start and end indices of spans in a
    sequence tensor. This is used in the
    [CoreferenceResolver](https://docs.allennlp.org/models/main/models/coref/models/coref/)
    model to select contextual word representations corresponding to the start and end indices of
    mentions.

    The key reason this can't be done with basic torch functions is that we want to be able to use look-up
    tensors with an arbitrary number of dimensions (for example, in the coref model, we don't know
    a-priori how many spans we are looking up).

    # Parameters

    target : `torch.Tensor`, required.
        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).
        This is the tensor to be indexed.
    indices : `torch.LongTensor`
        A tensor of shape (batch_size, ...), where each element is an index into the
        `sequence_length` dimension of the `target` tensor.
    flattened_indices : `Optional[torch.Tensor]`, optional (default = `None`)
        An optional tensor representing the result of calling `flatten_and_batch_shift_indices`
        on `indices`. This is helpful in the case that the indices can be flattened once and
        cached for many batch lookups.

    # Returns

    selected_targets : `torch.Tensor`
        A tensor with shape [indices.size(), target.size(-1)] representing the embedded indices
        extracted from the batch flattened target tensor.
    """
    if flattened_indices is None:
        flattened_indices = flatten_and_batch_shift_indices(indices, target.size(1))
    flattened_target = target.view(-1, target.size(-1))
    flattened_selected = flattened_target.index_select(0, flattened_indices)
    selected_shape = list(indices.size()) + [target.size(-1)]
    selected_targets = flattened_selected.view(*selected_shape)
    return selected_targets


def _check_incompatible_keys(module, missing_keys: List[str], unexpected_keys: List[str], strict: bool):
    error_msgs: List[str] = []
    if missing_keys:
        error_msgs.append('Missing key(s) in state_dict: {}'.format(', '.join(f'"{k}"' for k in missing_keys)))
    if unexpected_keys:
        error_msgs.append('Unexpected key(s) in state_dict: {}'.format(', '.join(f'"{k}"' for k in unexpected_keys)))
    if error_msgs and strict:
        raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(module.__class__.__name__, '\n\t'.join(error_msgs)))


def is_global_primary() ->bool:
    """
    Checks if the distributed process group is the global primary (rank = 0).
    If the distributed process group is not available or has not been initialized,
    this trivially returns `True`.
    """
    if not is_distributed():
        return True
    else:
        return dist.get_rank() == 0


def device_mapping(cuda_device: int):
    """
    In order to `torch.load()` a GPU-trained model onto a CPU (or specific GPU),
    you have to supply a `map_location` function. Call this with
    the desired `cuda_device` to get the function that `torch.load()` needs.
    """

    def inner_device_mapping(storage: torch.Storage, location) ->torch.Storage:
        if cuda_device >= 0:
            return storage
        else:
            return storage
    return inner_device_mapping


def apply_mask(values: torch.FloatTensor, mask: Union[torch.BoolTensor, torch.IntTensor, torch.FloatTensor]) ->torch.FloatTensor:
    """
    # Parameters

    values : `torch.FloatTensor`
        Shape `batch_size x num_attention_heads x source_seq_len x target_seq_len`
    mask : `torch.BoolTensor`
        Shape `batch_size x target_seq_len` OR `batch_size x 1 x 1 x target_seq_len`
    """
    if mask.dim() == 2:
        mask = mask[:, None, None, :]
    elif mask.dim() == 3:
        mask = mask[:, None, :, :]
    mask = mask
    mask = (1.0 - mask) * min_value_of_dtype(values.dtype)
    return values + mask


FloatT = Union[torch.FloatTensor]


IntT = Union[torch.IntTensor]


def replicate_layers(layer: torch.nn.Module, num_copies: int):
    """
    # Parameters
            layer (torch.nn.Module) - The torch layer that needs to be replicated.
            num_copies (int) - Number of copies to create.

    # Returns
            A ModuleList that contains `num_copies` of the `layer`.
    """
    return torch.nn.ModuleList([deepcopy(layer) for _ in range(num_copies)])


def masked_max(vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool=False) ->torch.Tensor:
    """
    To calculate max along certain dimensions on masked values

    # Parameters

    vector : `torch.Tensor`
        The vector to calculate max, assume unmasked parts are already zeros
    mask : `torch.BoolTensor`
        The mask of the vector. It must be broadcastable with vector.
    dim : `int`
        The dimension to calculate max
    keepdim : `bool`
        Whether to keep dimension

    # Returns

    `torch.Tensor`
        A `torch.Tensor` of including the maximum values.
    """
    replaced_vector = vector.masked_fill(~mask, min_value_of_dtype(vector.dtype))
    max_value, _ = replaced_vector.max(dim=dim, keepdim=keepdim)
    return max_value


def masked_mean(vector: torch.Tensor, mask: torch.BoolTensor, dim: int, keepdim: bool=False) ->torch.Tensor:
    """
    To calculate mean along certain dimensions on masked values

    # Parameters

    vector : `torch.Tensor`
        The vector to calculate mean.
    mask : `torch.BoolTensor`
        The mask of the vector. It must be broadcastable with vector.
    dim : `int`
        The dimension to calculate mean
    keepdim : `bool`
        Whether to keep dimension

    # Returns

    `torch.Tensor`
        A `torch.Tensor` of including the mean values.
    """
    replaced_vector = vector.masked_fill(~mask, 0.0)
    value_sum = torch.sum(replaced_vector, dim=dim, keepdim=keepdim)
    value_count = torch.sum(mask, dim=dim, keepdim=keepdim)
    return value_sum / value_count.float().clamp(min=tiny_value_of_dtype(torch.float))


def multi_perspective_match(vector1: torch.Tensor, vector2: torch.Tensor, weight: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Calculate multi-perspective cosine matching between time-steps of vectors
    of the same length.

    # Parameters

    vector1 : `torch.Tensor`
        A tensor of shape `(batch, seq_len, hidden_size)`
    vector2 : `torch.Tensor`
        A tensor of shape `(batch, seq_len or 1, hidden_size)`
    weight : `torch.Tensor`
        A tensor of shape `(num_perspectives, hidden_size)`

    # Returns

    `torch.Tensor` :
        Shape `(batch, seq_len, 1)`.
    `torch.Tensor` :
        Shape `(batch, seq_len, num_perspectives)`.
    """
    assert vector1.size(0) == vector2.size(0)
    assert weight.size(1) == vector1.size(2) == vector1.size(2)
    similarity_single = F.cosine_similarity(vector1, vector2, 2).unsqueeze(2)
    weight = weight.unsqueeze(0).unsqueeze(0)
    vector1 = weight * vector1.unsqueeze(2)
    vector2 = weight * vector2.unsqueeze(2)
    similarity_multi = F.cosine_similarity(vector1, vector2, dim=3)
    return similarity_single, similarity_multi


def multi_perspective_match_pairwise(vector1: torch.Tensor, vector2: torch.Tensor, weight: torch.Tensor) ->torch.Tensor:
    """
    Calculate multi-perspective cosine matching between each time step of
    one vector and each time step of another vector.

    # Parameters

    vector1 : `torch.Tensor`
        A tensor of shape `(batch, seq_len1, hidden_size)`
    vector2 : `torch.Tensor`
        A tensor of shape `(batch, seq_len2, hidden_size)`
    weight : `torch.Tensor`
        A tensor of shape `(num_perspectives, hidden_size)`

    # Returns

    `torch.Tensor` :
        A tensor of shape `(batch, seq_len1, seq_len2, num_perspectives)` consisting
        multi-perspective matching results
    """
    num_perspectives = weight.size(0)
    weight = weight.unsqueeze(0).unsqueeze(2)
    vector1 = weight * vector1.unsqueeze(1).expand(-1, num_perspectives, -1, -1)
    vector2 = weight * vector2.unsqueeze(1).expand(-1, num_perspectives, -1, -1)
    vector1_norm = vector1.norm(p=2, dim=3, keepdim=True)
    vector2_norm = vector2.norm(p=2, dim=3, keepdim=True)
    mul_result = torch.matmul(vector1, vector2.transpose(2, 3))
    norm_value = vector1_norm * vector2_norm.transpose(2, 3)
    return (mul_result / norm_value.clamp(min=tiny_value_of_dtype(norm_value.dtype))).permute(0, 2, 3, 1)


VITERBI_DECODING = Tuple[List[int], float]


class ConditionalRandomField(torch.nn.Module):
    """
    This module uses the "forward-backward" algorithm to compute
    the log-likelihood of its inputs assuming a conditional random field model.

    See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf

    # Parameters

    num_tags : `int`, required
        The number of tags.
    constraints : `List[Tuple[int, int]]`, optional (default = `None`)
        An optional list of allowed transitions (from_tag_id, to_tag_id).
        These are applied to `viterbi_tags()` but do not affect `forward()`.
        These should be derived from `allowed_transitions` so that the
        start and end transitions are handled correctly for your tag type.
    include_start_end_transitions : `bool`, optional (default = `True`)
        Whether to include the start and end transition parameters.
    """

    def __init__(self, num_tags: int, constraints: List[Tuple[int, int]]=None, include_start_end_transitions: bool=True) ->None:
        super().__init__()
        self.num_tags = num_tags
        self.transitions = torch.nn.Parameter(torch.empty(num_tags, num_tags))
        if constraints is None:
            constraint_mask = torch.full((num_tags + 2, num_tags + 2), 1.0)
        else:
            constraint_mask = torch.full((num_tags + 2, num_tags + 2), 0.0)
            for i, j in constraints:
                constraint_mask[i, j] = 1.0
        self._constraint_mask = torch.nn.Parameter(constraint_mask, requires_grad=False)
        self.include_start_end_transitions = include_start_end_transitions
        if include_start_end_transitions:
            self.start_transitions = torch.nn.Parameter(torch.Tensor(num_tags))
            self.end_transitions = torch.nn.Parameter(torch.Tensor(num_tags))
        self.reset_parameters()

    def reset_parameters(self):
        torch.nn.init.xavier_normal_(self.transitions)
        if self.include_start_end_transitions:
            torch.nn.init.normal_(self.start_transitions)
            torch.nn.init.normal_(self.end_transitions)

    def _input_likelihood(self, logits: torch.Tensor, transitions: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        """Computes the (batch_size,) denominator term $Z(x)$, per example, for the log-likelihood

        This is the sum of the likelihoods across all possible state sequences.

        Args:
            logits (torch.Tensor): a (batch_size, sequence_length num_tags) tensor of
                unnormalized log-probabilities
            transitions (torch.Tensor): a (batch_size, num_tags, num_tags) tensor of transition scores
            mask (torch.BoolTensor): a (batch_size, sequence_length) tensor of masking flags

        Returns:
            torch.Tensor: (batch_size,) denominator term $Z(x)$, per example, for the log-likelihood
        """
        batch_size, sequence_length, num_tags = logits.size()
        mask = mask.transpose(0, 1).contiguous()
        logits = logits.transpose(0, 1).contiguous()
        if self.include_start_end_transitions:
            alpha = self.start_transitions.view(1, num_tags) + logits[0]
        else:
            alpha = logits[0]
        for i in range(1, sequence_length):
            emit_scores = logits[i].view(batch_size, 1, num_tags)
            transition_scores = transitions.view(1, num_tags, num_tags)
            broadcast_alpha = alpha.view(batch_size, num_tags, 1)
            inner = broadcast_alpha + emit_scores + transition_scores
            alpha = util.logsumexp(inner, 1) * mask[i].view(batch_size, 1) + alpha * (~mask[i]).view(batch_size, 1)
        if self.include_start_end_transitions:
            stops = alpha + self.end_transitions.view(1, num_tags)
        else:
            stops = alpha
        return util.logsumexp(stops)

    def _joint_likelihood(self, logits: torch.Tensor, transitions: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        """Computes the numerator term for the log-likelihood, which is just score(inputs, tags)

        Args:
            logits (torch.Tensor): a (batch_size, sequence_length num_tags) tensor of unnormalized
                log-probabilities
            transitions (torch.Tensor): a (batch_size, num_tags, num_tags) tensor of transition scores
            tags (torch.Tensor): output tag sequences (batch_size, sequence_length) $y$ for each input sequence
            mask (torch.BoolTensor): a (batch_size, sequence_length) tensor of masking flags

        Returns:
            torch.Tensor: numerator term for the log-likelihood, which is just score(inputs, tags)
        """
        batch_size, sequence_length, _ = logits.data.shape
        logits = logits.transpose(0, 1).contiguous()
        mask = mask.transpose(0, 1).contiguous()
        tags = tags.transpose(0, 1).contiguous()
        if self.include_start_end_transitions:
            score = self.start_transitions.index_select(0, tags[0])
        else:
            score = 0.0
        for i in range(sequence_length - 1):
            current_tag, next_tag = tags[i], tags[i + 1]
            transition_score = transitions[current_tag.view(-1), next_tag.view(-1)]
            emit_score = logits[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)
            score = score + transition_score * mask[i + 1] + emit_score * mask[i]
        last_tag_index = mask.sum(0).long() - 1
        last_tags = tags.gather(0, last_tag_index.view(1, batch_size)).squeeze(0)
        if self.include_start_end_transitions:
            last_transition_score = self.end_transitions.index_select(0, last_tags)
        else:
            last_transition_score = 0.0
        last_inputs = logits[-1]
        last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))
        last_input_score = last_input_score.squeeze()
        score = score + last_transition_score + last_input_score * mask[-1]
        return score

    def forward(self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor=None) ->torch.Tensor:
        """Computes the log likelihood for the given batch of input sequences $(x,y)$

        Args:
            inputs (torch.Tensor): (batch_size, sequence_length, num_tags) tensor of logits for the inputs $x$
            tags (torch.Tensor): (batch_size, sequence_length) tensor of tags $y$
            mask (torch.BoolTensor, optional): (batch_size, sequence_length) tensor of masking flags.
                Defaults to None.

        Returns:
            torch.Tensor: (batch_size,) log likelihoods $log P(y|x)$ for each input
        """
        if mask is None:
            mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
        else:
            mask = mask
        log_denominator = self._input_likelihood(inputs, self.transitions, mask)
        log_numerator = self._joint_likelihood(inputs, self.transitions, tags, mask)
        return torch.sum(log_numerator - log_denominator)

    def viterbi_tags(self, logits: torch.Tensor, mask: torch.BoolTensor=None, top_k: int=None) ->Union[List[VITERBI_DECODING], List[List[VITERBI_DECODING]]]:
        """
        Uses viterbi algorithm to find most likely tags for the given inputs.
        If constraints are applied, disallows all other transitions.

        Returns a list of results, of the same size as the batch (one result per batch member)
        Each result is a List of length top_k, containing the top K viterbi decodings
        Each decoding is a tuple  (tag_sequence, viterbi_score)

        For backwards compatibility, if top_k is None, then instead returns a flat list of
        tag sequences (the top tag sequence for each batch item).
        """
        if mask is None:
            mask = torch.ones(*logits.shape[:2], dtype=torch.bool, device=logits.device)
        if top_k is None:
            top_k = 1
            flatten_output = True
        else:
            flatten_output = False
        _, max_seq_length, num_tags = logits.size()
        logits, mask = logits.data, mask.data
        start_tag = num_tags
        end_tag = num_tags + 1
        transitions = torch.full((num_tags + 2, num_tags + 2), -10000.0, device=logits.device)
        constrained_transitions = self.transitions * self._constraint_mask[:num_tags, :num_tags] + -10000.0 * (1 - self._constraint_mask[:num_tags, :num_tags])
        transitions[:num_tags, :num_tags] = constrained_transitions.data
        if self.include_start_end_transitions:
            transitions[start_tag, :num_tags] = self.start_transitions.detach() * self._constraint_mask[start_tag, :num_tags].data + -10000.0 * (1 - self._constraint_mask[start_tag, :num_tags].detach())
            transitions[:num_tags, end_tag] = self.end_transitions.detach() * self._constraint_mask[:num_tags, end_tag].data + -10000.0 * (1 - self._constraint_mask[:num_tags, end_tag].detach())
        else:
            transitions[start_tag, :num_tags] = -10000.0 * (1 - self._constraint_mask[start_tag, :num_tags].detach())
            transitions[:num_tags, end_tag] = -10000.0 * (1 - self._constraint_mask[:num_tags, end_tag].detach())
        best_paths = []
        tag_sequence = torch.empty(max_seq_length + 2, num_tags + 2, device=logits.device)
        for prediction, prediction_mask in zip(logits, mask):
            mask_indices = prediction_mask.nonzero(as_tuple=False).squeeze()
            masked_prediction = torch.index_select(prediction, 0, mask_indices)
            sequence_length = masked_prediction.shape[0]
            tag_sequence.fill_(-10000.0)
            tag_sequence[0, start_tag] = 0.0
            tag_sequence[1:sequence_length + 1, :num_tags] = masked_prediction
            tag_sequence[sequence_length + 1, end_tag] = 0.0
            viterbi_paths, viterbi_scores = util.viterbi_decode(tag_sequence=tag_sequence[:sequence_length + 2], transition_matrix=transitions, top_k=top_k)
            top_k_paths = []
            for viterbi_path, viterbi_score in zip(viterbi_paths, viterbi_scores):
                viterbi_path = viterbi_path[1:-1]
                top_k_paths.append((viterbi_path, viterbi_score.item()))
            best_paths.append(top_k_paths)
        if flatten_output:
            return [top_k_paths[0] for top_k_paths in best_paths]
        return best_paths


class ConditionalRandomFieldWeightEmission(ConditionalRandomField):
    """
    This module uses the "forward-backward" algorithm to compute
    the log-likelihood of its inputs assuming a conditional random field model.

    See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf

    This is a weighted version of `ConditionalRandomField` which accepts a `label_weights`
    parameter to be used in the loss function in order to give different weights for each
    token depending on its label. The method implemented here is based on the simple idea
    of weighting emission scores using the weight given for the corresponding tag.

    There are two other sample weighting methods implemented. You can find more details
    about them in: https://eraldoluis.github.io/2022/05/10/weighted-crf.html

    # Parameters

    num_tags : `int`, required
        The number of tags.
    label_weights : `List[float]`, required
        A list of weights to be used in the loss function in order to
        give different weights for each token depending on its label.
        `len(label_weights)` must be equal to `num_tags`. This is useful to
        deal with highly unbalanced datasets. The method implemented here is
        based on the simple idea of weighting emission scores using the weight
        given for the corresponding tag.
    constraints : `List[Tuple[int, int]]`, optional (default = `None`)
        An optional list of allowed transitions (from_tag_id, to_tag_id).
        These are applied to `viterbi_tags()` but do not affect `forward()`.
        These should be derived from `allowed_transitions` so that the
        start and end transitions are handled correctly for your tag type.
    include_start_end_transitions : `bool`, optional (default = `True`)
        Whether to include the start and end transition parameters.
    """

    def __init__(self, num_tags: int, label_weights: List[float], constraints: List[Tuple[int, int]]=None, include_start_end_transitions: bool=True) ->None:
        super().__init__(num_tags, constraints, include_start_end_transitions)
        if label_weights is None:
            raise ConfigurationError('label_weights must be given')
        self.register_buffer('label_weights', torch.Tensor(label_weights))

    def forward(self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor=None) ->torch.Tensor:
        """Computes the log likelihood for the given batch of input sequences $(x,y)$

        Args:
            inputs (torch.Tensor): (batch_size, sequence_length, num_tags) tensor of logits for the inputs $x$
            tags (torch.Tensor): (batch_size, sequence_length) tensor of tags $y$
            mask (torch.BoolTensor, optional): (batch_size, sequence_length) tensor of masking flags.
                Defaults to None.

        Returns:
            torch.Tensor: (batch_size,) log likelihoods $log P(y|x)$ for each input
        """
        if mask is None:
            mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
        else:
            mask = mask
        label_weights = self.label_weights
        inputs = inputs * label_weights.view(1, 1, -1)
        log_denominator = self._input_likelihood(inputs, self.transitions, mask)
        log_numerator = self._joint_likelihood(inputs, self.transitions, tags, mask)
        return torch.sum(log_numerator - log_denominator)


class ConditionalRandomFieldWeightLannoy(ConditionalRandomField):
    """
    This module uses the "forward-backward" algorithm to compute
    the log-likelihood of its inputs assuming a conditional random field model.

    See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf

    This is a weighted version of `ConditionalRandomField` which accepts a `label_weights`
    parameter to be used in the loss function in order to give different weights for each
    token depending on its label. The method implemented here is based on the paper
    *Weighted conditional random fields for supervised interpatient heartbeat
    classification* proposed by De Lannoy et. al (2019).
    See https://perso.uclouvain.be/michel.verleysen/papers/ieeetbe12gdl.pdf for more details.

    There are two other sample weighting methods implemented. You can find more details
    about them in: https://eraldoluis.github.io/2022/05/10/weighted-crf.html

    # Parameters

    num_tags : `int`, required
        The number of tags.
    label_weights : `List[float]`, required
        A list of weights to be used in the loss function in order to
        give different weights for each token depending on its label.
        `len(label_weights)` must be equal to `num_tags`. This is useful to
        deal with highly unbalanced datasets. The method implemented here was based on
        the paper *Weighted conditional random fields for supervised interpatient heartbeat
        classification* proposed by De Lannoy et. al (2019).
        See https://perso.uclouvain.be/michel.verleysen/papers/ieeetbe12gdl.pdf
    constraints : `List[Tuple[int, int]]`, optional (default = `None`)
        An optional list of allowed transitions (from_tag_id, to_tag_id).
        These are applied to `viterbi_tags()` but do not affect `forward()`.
        These should be derived from `allowed_transitions` so that the
        start and end transitions are handled correctly for your tag type.
    include_start_end_transitions : `bool`, optional (default = `True`)
        Whether to include the start and end transition parameters.
    """

    def __init__(self, num_tags: int, label_weights: List[float], constraints: List[Tuple[int, int]]=None, include_start_end_transitions: bool=True) ->None:
        super().__init__(num_tags, constraints, include_start_end_transitions)
        if label_weights is None:
            raise ConfigurationError('label_weights must be given')
        self.register_buffer('label_weights', torch.Tensor(label_weights))

    def forward(self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor=None) ->torch.Tensor:
        """Computes the log likelihood for the given batch of input sequences $(x,y)$

        Args:
            inputs (torch.Tensor): (batch_size, sequence_length, num_tags) tensor of logits for the inputs $x$
            tags (torch.Tensor): (batch_size, sequence_length) tensor of tags $y$
            mask (torch.BoolTensor, optional): (batch_size, sequence_length) tensor of masking flags.
                Defaults to None.

        Returns:
            torch.Tensor: (batch_size,) log likelihoods $log P(y|x)$ for each input
        """
        if mask is None:
            mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
        else:
            mask = mask
        log_denominator = self._input_likelihood_lannoy(inputs, tags, mask)
        log_numerator = self._joint_likelihood_lannoy(inputs, tags, mask)
        return torch.sum(log_numerator - log_denominator)

    def _input_likelihood_lannoy(self, logits: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        """
        Computes the (batch_size,) denominator term for the log-likelihood, which is the
        sum of the likelihoods across all possible state sequences.

        Compute this value using the scaling trick instead of the log domain trick, since
        this is necessary to implement the label-weighting method by Lannoy et al. (2012).
        """
        batch_size, sequence_length, num_tags = logits.size()
        mask = mask.transpose(0, 1).contiguous()
        logits = logits.transpose(0, 1).contiguous()
        tags = tags.transpose(0, 1).contiguous()
        label_weights = self.label_weights.view(num_tags, 1)
        emit_scores = logits[0]
        if self.include_start_end_transitions:
            alpha = torch.exp(self.start_transitions.view(1, num_tags) + emit_scores)
        else:
            alpha = torch.exp(emit_scores)
        z = alpha.sum(dim=1, keepdim=True)
        alpha = alpha / z
        sum_log_z = torch.log(z) * label_weights[tags[0]]
        for i in range(1, sequence_length):
            emit_scores = logits[i]
            emit_scores = emit_scores.view(batch_size, 1, num_tags)
            transition_scores = self.transitions.view(1, num_tags, num_tags)
            broadcast_alpha = alpha.view(batch_size, num_tags, 1)
            inner = broadcast_alpha * torch.exp(emit_scores + transition_scores)
            alpha = inner.sum(dim=1) * mask[i].view(batch_size, 1) + alpha * (~mask[i]).view(batch_size, 1)
            z = alpha.sum(dim=1, keepdim=True)
            alpha = alpha / z
            sum_log_z += torch.log(z) * label_weights[tags[i]]
        if self.include_start_end_transitions:
            alpha = alpha * torch.exp(self.end_transitions.view(1, num_tags))
            z = alpha.sum(dim=1, keepdim=True)
            sum_log_z += torch.log(z)
        return sum_log_z.squeeze(1)

    def _joint_likelihood_lannoy(self, logits: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        """
        Computes the numerator term for the log-likelihood, which is just score(inputs, tags)
        """
        batch_size, sequence_length, _ = logits.data.shape
        logits = logits.transpose(0, 1).contiguous()
        mask = mask.transpose(0, 1).contiguous()
        tags = tags.transpose(0, 1).contiguous()
        if self.include_start_end_transitions:
            score = self.start_transitions.index_select(0, tags[0])
        else:
            score = 0.0
        label_weights = self.label_weights
        transitions = self.transitions * label_weights.view(-1, 1)
        for i in range(sequence_length - 1):
            current_tag, next_tag = tags[i], tags[i + 1]
            transition_score = transitions[current_tag.view(-1), next_tag.view(-1)]
            emit_score = logits[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)
            emit_score *= label_weights[current_tag.view(-1)]
            score = score + transition_score * mask[i + 1] + emit_score * mask[i]
        last_tag_index = mask.sum(0).long() - 1
        last_tags = tags.gather(0, last_tag_index.view(1, batch_size)).squeeze(0)
        if self.include_start_end_transitions:
            last_transition_score = self.end_transitions.index_select(0, last_tags)
        else:
            last_transition_score = 0.0
        last_inputs = logits[-1]
        last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))
        last_input_score = last_input_score.squeeze()
        last_input_score = last_input_score * label_weights[last_tags.view(-1)]
        score = score + last_transition_score + last_input_score * mask[-1]
        return score


class ConditionalRandomFieldWeightTrans(ConditionalRandomField):
    """
    This module uses the "forward-backward" algorithm to compute
    the log-likelihood of its inputs assuming a conditional random field model.

    See, e.g. http://www.cs.columbia.edu/~mcollins/fb.pdf

    This is a weighted version of `ConditionalRandomField` which accepts a `label_weights`
    parameter to be used in the loss function in order to give different weights for each
    token depending on its label. The method implemented here is based on the simple idea
    of weighting emission and transition scores using the weight given for the
    corresponding tag.

    There are two other sample weighting methods implemented. You can find more details
    about them in: https://eraldoluis.github.io/2022/05/10/weighted-crf.html

    # Parameters

    num_tags : `int`, required
        The number of tags.
    label_weights : `List[float]`, required
        A list of weights to be used in the loss function in order to
        give different weights for each token depending on its label.
        `len(label_weights)` must be equal to `num_tags`. This is useful to
        deal with highly unbalanced datasets. The method implemented here is
        based on the simple idea of weighting emission and transition scores
        using the weight given for the corresponding tag.
    constraints : `List[Tuple[int, int]]`, optional (default = `None`)
        An optional list of allowed transitions (from_tag_id, to_tag_id).
        These are applied to `viterbi_tags()` but do not affect `forward()`.
        These should be derived from `allowed_transitions` so that the
        start and end transitions are handled correctly for your tag type.
    include_start_end_transitions : `bool`, optional (default = `True`)
        Whether to include the start and end transition parameters.
    """

    def __init__(self, num_tags: int, label_weights: List[float], constraints: List[Tuple[int, int]]=None, include_start_end_transitions: bool=True) ->None:
        super().__init__(num_tags, constraints, include_start_end_transitions)
        if label_weights is None:
            raise ConfigurationError('label_weights must be given')
        self.register_buffer('label_weights', torch.Tensor(label_weights))

    def forward(self, inputs: torch.Tensor, tags: torch.Tensor, mask: torch.BoolTensor=None) ->torch.Tensor:
        """Computes the log likelihood for the given batch of input sequences $(x,y)$

        Args:
            inputs (torch.Tensor): (batch_size, sequence_length, num_tags) tensor of logits for the inputs $x$
            tags (torch.Tensor): (batch_size, sequence_length) tensor of tags $y$
            mask (torch.BoolTensor, optional): (batch_size, sequence_length) tensor of masking flags.
                Defaults to None.

        Returns:
            torch.Tensor: (batch_size,) log likelihoods $log P(y|x)$ for each input
        """
        if mask is None:
            mask = torch.ones(*tags.size(), dtype=torch.bool, device=inputs.device)
        else:
            mask = mask
        label_weights = self.label_weights
        transitions = self.transitions * label_weights.view(-1, 1)
        inputs = inputs * label_weights.view(1, 1, -1)
        log_denominator = self._input_likelihood(inputs, transitions, mask)
        log_numerator = self._joint_likelihood(inputs, transitions, tags, mask)
        return torch.sum(log_numerator - log_denominator)


def _make_bos_eos(character: int, padding_character: int, beginning_of_word_character: int, end_of_word_character: int, max_word_length: int):
    char_ids = [padding_character] * max_word_length
    char_ids[0] = beginning_of_word_character
    char_ids[1] = character
    char_ids[2] = end_of_word_character
    return char_ids


class ELMoCharacterMapper:
    """
    Maps individual tokens to sequences of character ids, compatible with ELMo.
    To be consistent with previously trained models, we include it here as special of existing
    character indexers.

    We allow to add optional additional special tokens with designated
    character ids with `tokens_to_add`.
    """
    max_word_length = 50
    beginning_of_sentence_character = 256
    end_of_sentence_character = 257
    beginning_of_word_character = 258
    end_of_word_character = 259
    padding_character = 260
    beginning_of_sentence_characters = _make_bos_eos(beginning_of_sentence_character, padding_character, beginning_of_word_character, end_of_word_character, max_word_length)
    end_of_sentence_characters = _make_bos_eos(end_of_sentence_character, padding_character, beginning_of_word_character, end_of_word_character, max_word_length)
    bos_token = '<S>'
    eos_token = '</S>'

    def __init__(self, tokens_to_add: Dict[str, int]=None) ->None:
        self.tokens_to_add = tokens_to_add or {}

    def convert_word_to_char_ids(self, word: str) ->List[int]:
        if word in self.tokens_to_add:
            char_ids = [ELMoCharacterMapper.padding_character] * ELMoCharacterMapper.max_word_length
            char_ids[0] = ELMoCharacterMapper.beginning_of_word_character
            char_ids[1] = self.tokens_to_add[word]
            char_ids[2] = ELMoCharacterMapper.end_of_word_character
        elif word == ELMoCharacterMapper.bos_token:
            char_ids = ELMoCharacterMapper.beginning_of_sentence_characters
        elif word == ELMoCharacterMapper.eos_token:
            char_ids = ELMoCharacterMapper.end_of_sentence_characters
        else:
            word_encoded = word.encode('utf-8', 'ignore')[:ELMoCharacterMapper.max_word_length - 2]
            char_ids = [ELMoCharacterMapper.padding_character] * ELMoCharacterMapper.max_word_length
            char_ids[0] = ELMoCharacterMapper.beginning_of_word_character
            for k, chr_id in enumerate(word_encoded, start=1):
                char_ids[k] = chr_id
            char_ids[len(word_encoded) + 1] = ELMoCharacterMapper.end_of_word_character
        return [(c + 1) for c in char_ids]

    def __eq__(self, other) ->bool:
        if isinstance(self, other.__class__):
            return self.__dict__ == other.__dict__
        return NotImplemented


class LstmCellWithProjection(torch.nn.Module):
    """
    An LSTM with Recurrent Dropout and a projected and clipped hidden state and
    memory. Note: this implementation is slower than the native Pytorch LSTM because
    it cannot make use of CUDNN optimizations for stacked RNNs due to and
    variational dropout and the custom nature of the cell state.

    [0]: https://arxiv.org/abs/1512.05287

    # Parameters

    input_size : `int`, required.
        The dimension of the inputs to the LSTM.
    hidden_size : `int`, required.
        The dimension of the outputs of the LSTM.
    cell_size : `int`, required.
        The dimension of the memory cell used for the LSTM.
    go_forward : `bool`, optional (default = `True`)
        The direction in which the LSTM is applied to the sequence.
        Forwards by default, or backwards if False.
    recurrent_dropout_probability : `float`, optional (default = `0.0`)
        The dropout probability to be used in a dropout scheme as stated in
        [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks]
        [0]. Implementation wise, this simply
        applies a fixed dropout mask per sequence to the recurrent connection of the
        LSTM.
    state_projection_clip_value : `float`, optional, (default = `None`)
        The magnitude with which to clip the hidden_state after projecting it.
    memory_cell_clip_value : `float`, optional, (default = `None`)
        The magnitude with which to clip the memory cell.

    # Returns

    output_accumulator : `torch.FloatTensor`
        The outputs of the LSTM for each timestep. A tensor of shape
        (batch_size, max_timesteps, hidden_size) where for a given batch
        element, all outputs past the sequence length for that batch are
        zero tensors.
    final_state : `Tuple[torch.FloatTensor, torch.FloatTensor]`
        The final (state, memory) states of the LSTM, with shape
        (1, batch_size, hidden_size) and  (1, batch_size, cell_size)
        respectively. The first dimension is 1 in order to match the Pytorch
        API for returning stacked LSTM states.
    """

    def __init__(self, input_size: int, hidden_size: int, cell_size: int, go_forward: bool=True, recurrent_dropout_probability: float=0.0, memory_cell_clip_value: Optional[float]=None, state_projection_clip_value: Optional[float]=None) ->None:
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.cell_size = cell_size
        self.go_forward = go_forward
        self.state_projection_clip_value = state_projection_clip_value
        self.memory_cell_clip_value = memory_cell_clip_value
        self.recurrent_dropout_probability = recurrent_dropout_probability
        self.input_linearity = torch.nn.Linear(input_size, 4 * cell_size, bias=False)
        self.state_linearity = torch.nn.Linear(hidden_size, 4 * cell_size, bias=True)
        self.state_projection = torch.nn.Linear(cell_size, hidden_size, bias=False)
        self.reset_parameters()

    def reset_parameters(self):
        block_orthogonal(self.input_linearity.weight.data, [self.cell_size, self.input_size])
        block_orthogonal(self.state_linearity.weight.data, [self.cell_size, self.hidden_size])
        self.state_linearity.bias.data.fill_(0.0)
        self.state_linearity.bias.data[self.cell_size:2 * self.cell_size].fill_(1.0)

    def forward(self, inputs: torch.FloatTensor, batch_lengths: List[int], initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]]=None):
        """
        # Parameters

        inputs : `torch.FloatTensor`, required.
            A tensor of shape (batch_size, num_timesteps, input_size)
            to apply the LSTM over.
        batch_lengths : `List[int]`, required.
            A list of length batch_size containing the lengths of the sequences in batch.
        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)
            A tuple (state, memory) representing the initial hidden state and memory
            of the LSTM. The `state` has shape (1, batch_size, hidden_size) and the
            `memory` has shape (1, batch_size, cell_size).

        # Returns

        output_accumulator : `torch.FloatTensor`
            The outputs of the LSTM for each timestep. A tensor of shape
            (batch_size, max_timesteps, hidden_size) where for a given batch
            element, all outputs past the sequence length for that batch are
            zero tensors.
        final_state : `Tuple[torch.FloatTensor, torch.FloatTensor]`
            A tuple (state, memory) representing the initial hidden state and memory
            of the LSTM. The `state` has shape (1, batch_size, hidden_size) and the
            `memory` has shape (1, batch_size, cell_size).
        """
        batch_size = inputs.size()[0]
        total_timesteps = inputs.size()[1]
        output_accumulator = inputs.new_zeros(batch_size, total_timesteps, self.hidden_size)
        if initial_state is None:
            full_batch_previous_memory = inputs.new_zeros(batch_size, self.cell_size)
            full_batch_previous_state = inputs.new_zeros(batch_size, self.hidden_size)
        else:
            full_batch_previous_state = initial_state[0].squeeze(0)
            full_batch_previous_memory = initial_state[1].squeeze(0)
        current_length_index = batch_size - 1 if self.go_forward else 0
        if self.recurrent_dropout_probability > 0.0 and self.training:
            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability, full_batch_previous_state)
        else:
            dropout_mask = None
        for timestep in range(total_timesteps):
            index = timestep if self.go_forward else total_timesteps - timestep - 1
            if self.go_forward:
                while batch_lengths[current_length_index] <= index:
                    current_length_index -= 1
            else:
                while current_length_index < len(batch_lengths) - 1 and batch_lengths[current_length_index + 1] > index:
                    current_length_index += 1
            previous_memory = full_batch_previous_memory[0:current_length_index + 1].clone()
            previous_state = full_batch_previous_state[0:current_length_index + 1].clone()
            timestep_input = inputs[0:current_length_index + 1, index]
            projected_input = self.input_linearity(timestep_input)
            projected_state = self.state_linearity(previous_state)
            input_gate = torch.sigmoid(projected_input[:, 0 * self.cell_size:1 * self.cell_size] + projected_state[:, 0 * self.cell_size:1 * self.cell_size])
            forget_gate = torch.sigmoid(projected_input[:, 1 * self.cell_size:2 * self.cell_size] + projected_state[:, 1 * self.cell_size:2 * self.cell_size])
            memory_init = torch.tanh(projected_input[:, 2 * self.cell_size:3 * self.cell_size] + projected_state[:, 2 * self.cell_size:3 * self.cell_size])
            output_gate = torch.sigmoid(projected_input[:, 3 * self.cell_size:4 * self.cell_size] + projected_state[:, 3 * self.cell_size:4 * self.cell_size])
            memory = input_gate * memory_init + forget_gate * previous_memory
            if self.memory_cell_clip_value:
                memory = torch.clamp(memory, -self.memory_cell_clip_value, self.memory_cell_clip_value)
            pre_projection_timestep_output = output_gate * torch.tanh(memory)
            timestep_output = self.state_projection(pre_projection_timestep_output)
            if self.state_projection_clip_value:
                timestep_output = torch.clamp(timestep_output, -self.state_projection_clip_value, self.state_projection_clip_value)
            if dropout_mask is not None:
                timestep_output = timestep_output * dropout_mask[0:current_length_index + 1]
            full_batch_previous_memory = full_batch_previous_memory.clone()
            full_batch_previous_state = full_batch_previous_state.clone()
            full_batch_previous_memory[0:current_length_index + 1] = memory
            full_batch_previous_state[0:current_length_index + 1] = timestep_output
            output_accumulator[0:current_length_index + 1, index] = timestep_output
        final_state = full_batch_previous_state.unsqueeze(0), full_batch_previous_memory.unsqueeze(0)
        return output_accumulator, final_state


class ElmoLstm(_EncoderBase):
    """
    A stacked, bidirectional LSTM which uses
    [`LstmCellWithProjection`'s](./lstm_cell_with_projection.md)
    with highway layers between the inputs to layers.
    The inputs to the forward and backward directions are independent - forward and backward
    states are not concatenated between layers.

    Additionally, this LSTM maintains its `own` state, which is updated every time
    `forward` is called. It is dynamically resized for different batch sizes and is
    designed for use with non-continuous inputs (i.e inputs which aren't formatted as a stream,
    such as text used for a language modeling task, which is how stateful RNNs are typically used).
    This is non-standard, but can be thought of as having an "end of sentence" state, which is
    carried across different sentences.

    [0]: https://arxiv.org/abs/1512.05287

    # Parameters

    input_size : `int`, required
        The dimension of the inputs to the LSTM.
    hidden_size : `int`, required
        The dimension of the outputs of the LSTM.
    cell_size : `int`, required.
        The dimension of the memory cell of the `LstmCellWithProjection`.
    num_layers : `int`, required
        The number of bidirectional LSTMs to use.
    requires_grad : `bool`, optional
        If True, compute gradient of ELMo parameters for fine tuning.
    recurrent_dropout_probability : `float`, optional (default = `0.0`)
        The dropout probability to be used in a dropout scheme as stated in
        [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks][0].
    state_projection_clip_value : `float`, optional, (default = `None`)
        The magnitude with which to clip the hidden_state after projecting it.
    memory_cell_clip_value : `float`, optional, (default = `None`)
        The magnitude with which to clip the memory cell.
    """

    def __init__(self, input_size: int, hidden_size: int, cell_size: int, num_layers: int, requires_grad: bool=False, recurrent_dropout_probability: float=0.0, memory_cell_clip_value: Optional[float]=None, state_projection_clip_value: Optional[float]=None) ->None:
        super().__init__(stateful=True)
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.cell_size = cell_size
        self.requires_grad = requires_grad
        forward_layers = []
        backward_layers = []
        lstm_input_size = input_size
        go_forward = True
        for layer_index in range(num_layers):
            forward_layer = LstmCellWithProjection(lstm_input_size, hidden_size, cell_size, go_forward, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)
            backward_layer = LstmCellWithProjection(lstm_input_size, hidden_size, cell_size, not go_forward, recurrent_dropout_probability, memory_cell_clip_value, state_projection_clip_value)
            lstm_input_size = hidden_size
            self.add_module('forward_layer_{}'.format(layer_index), forward_layer)
            self.add_module('backward_layer_{}'.format(layer_index), backward_layer)
            forward_layers.append(forward_layer)
            backward_layers.append(backward_layer)
        self.forward_layers = forward_layers
        self.backward_layers = backward_layers

    def forward(self, inputs: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        """
        # Parameters

        inputs : `torch.Tensor`, required.
            A Tensor of shape `(batch_size, sequence_length, hidden_size)`.
        mask : `torch.BoolTensor`, required.
            A binary mask of shape `(batch_size, sequence_length)` representing the
            non-padded elements in each sequence in the batch.

        # Returns

        `torch.Tensor`
            A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),
            where the num_layers dimension represents the LSTM output from that layer.
        """
        batch_size, total_sequence_length = mask.size()
        stacked_sequence_output, final_states, restoration_indices = self.sort_and_run_forward(self._lstm_forward, inputs, mask)
        num_layers, num_valid, returned_timesteps, encoder_dim = stacked_sequence_output.size()
        if num_valid < batch_size:
            zeros = stacked_sequence_output.new_zeros(num_layers, batch_size - num_valid, returned_timesteps, encoder_dim)
            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 1)
            new_states = []
            for state in final_states:
                state_dim = state.size(-1)
                zeros = state.new_zeros(num_layers, batch_size - num_valid, state_dim)
                new_states.append(torch.cat([state, zeros], 1))
            final_states = new_states
        sequence_length_difference = total_sequence_length - returned_timesteps
        if sequence_length_difference > 0:
            zeros = stacked_sequence_output.new_zeros(num_layers, batch_size, sequence_length_difference, stacked_sequence_output[0].size(-1))
            stacked_sequence_output = torch.cat([stacked_sequence_output, zeros], 2)
        self._update_states(final_states, restoration_indices)
        return stacked_sequence_output.index_select(1, restoration_indices)

    def _lstm_forward(self, inputs: PackedSequence, initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]]=None) ->Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        # Parameters

        inputs : `PackedSequence`, required.
            A batch first `PackedSequence` to run the stacked LSTM over.
        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)
            A tuple (state, memory) representing the initial hidden state and memory
            of the LSTM, with shape (num_layers, batch_size, 2 * hidden_size) and
            (num_layers, batch_size, 2 * cell_size) respectively.

        # Returns

        output_sequence : `torch.FloatTensor`
            The encoded sequence of shape (num_layers, batch_size, sequence_length, hidden_size)
        final_states : `Tuple[torch.FloatTensor, torch.FloatTensor]`
            The per-layer final (state, memory) states of the LSTM, with shape
            (num_layers, batch_size, 2 * hidden_size) and  (num_layers, batch_size, 2 * cell_size)
            respectively. The last dimension is duplicated because it contains the state/memory
            for both the forward and backward layers.
        """
        if initial_state is None:
            hidden_states: List[Optional[Tuple[torch.Tensor, torch.Tensor]]] = [None] * len(self.forward_layers)
        elif initial_state[0].size()[0] != len(self.forward_layers):
            raise ConfigurationError('Initial states were passed to forward() but the number of initial states does not match the number of layers.')
        else:
            hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))
        inputs, batch_lengths = pad_packed_sequence(inputs, batch_first=True)
        forward_output_sequence = inputs
        backward_output_sequence = inputs
        final_states = []
        sequence_outputs = []
        for layer_index, state in enumerate(hidden_states):
            forward_layer = getattr(self, 'forward_layer_{}'.format(layer_index))
            backward_layer = getattr(self, 'backward_layer_{}'.format(layer_index))
            forward_cache = forward_output_sequence
            backward_cache = backward_output_sequence
            forward_state: Optional[Tuple[Any, Any]] = None
            backward_state: Optional[Tuple[Any, Any]] = None
            if state is not None:
                forward_hidden_state, backward_hidden_state = state[0].split(self.hidden_size, 2)
                forward_memory_state, backward_memory_state = state[1].split(self.cell_size, 2)
                forward_state = forward_hidden_state, forward_memory_state
                backward_state = backward_hidden_state, backward_memory_state
            forward_output_sequence, forward_state = forward_layer(forward_output_sequence, batch_lengths, forward_state)
            backward_output_sequence, backward_state = backward_layer(backward_output_sequence, batch_lengths, backward_state)
            if layer_index != 0:
                forward_output_sequence += forward_cache
                backward_output_sequence += backward_cache
            sequence_outputs.append(torch.cat([forward_output_sequence, backward_output_sequence], -1))
            final_states.append((torch.cat([forward_state[0], backward_state[0]], -1), torch.cat([forward_state[1], backward_state[1]], -1)))
        stacked_sequence_outputs: torch.FloatTensor = torch.stack(sequence_outputs)
        final_hidden_states, final_memory_states = zip(*final_states)
        final_state_tuple: Tuple[torch.FloatTensor, torch.FloatTensor] = (torch.cat(final_hidden_states, 0), torch.cat(final_memory_states, 0))
        return stacked_sequence_outputs, final_state_tuple

    def load_weights(self, weight_file: str) ->None:
        """
        Load the pre-trained weights from the file.
        """
        requires_grad = self.requires_grad
        with h5py.File(cached_path(weight_file), 'r') as fin:
            for i_layer, lstms in enumerate(zip(self.forward_layers, self.backward_layers)):
                for j_direction, lstm in enumerate(lstms):
                    cell_size = lstm.cell_size
                    dataset = fin['RNN_%s' % j_direction]['RNN']['MultiRNNCell']['Cell%s' % i_layer]['LSTMCell']
                    tf_weights = numpy.transpose(dataset['W_0'][...])
                    torch_weights = tf_weights.copy()
                    input_size = lstm.input_size
                    input_weights = torch_weights[:, :input_size]
                    recurrent_weights = torch_weights[:, input_size:]
                    tf_input_weights = tf_weights[:, :input_size]
                    tf_recurrent_weights = tf_weights[:, input_size:]
                    for torch_w, tf_w in [[input_weights, tf_input_weights], [recurrent_weights, tf_recurrent_weights]]:
                        torch_w[1 * cell_size:2 * cell_size, :] = tf_w[2 * cell_size:3 * cell_size, :]
                        torch_w[2 * cell_size:3 * cell_size, :] = tf_w[1 * cell_size:2 * cell_size, :]
                    lstm.input_linearity.weight.data.copy_(torch.FloatTensor(input_weights))
                    lstm.state_linearity.weight.data.copy_(torch.FloatTensor(recurrent_weights))
                    lstm.input_linearity.weight.requires_grad = requires_grad
                    lstm.state_linearity.weight.requires_grad = requires_grad
                    tf_bias = dataset['B'][...]
                    tf_bias[2 * cell_size:3 * cell_size] += 1
                    torch_bias = tf_bias.copy()
                    torch_bias[1 * cell_size:2 * cell_size] = tf_bias[2 * cell_size:3 * cell_size]
                    torch_bias[2 * cell_size:3 * cell_size] = tf_bias[1 * cell_size:2 * cell_size]
                    lstm.state_linearity.bias.data.copy_(torch.FloatTensor(torch_bias))
                    lstm.state_linearity.bias.requires_grad = requires_grad
                    proj_weights = numpy.transpose(dataset['W_P_0'][...])
                    lstm.state_projection.weight.data.copy_(torch.FloatTensor(proj_weights))
                    lstm.state_projection.weight.requires_grad = requires_grad


class Highway(torch.nn.Module):
    """
    A [Highway layer](https://arxiv.org/abs/1505.00387) does a gated combination of a linear
    transformation and a non-linear transformation of its input.  :math:`y = g * x + (1 - g) *
    f(A(x))`, where :math:`A` is a linear transformation, :math:`f` is an element-wise
    non-linearity, and :math:`g` is an element-wise gate, computed as :math:`sigmoid(B(x))`.

    This module will apply a fixed number of highway layers to its input, returning the final
    result.

    # Parameters

    input_dim : `int`, required
        The dimensionality of :math:`x`.  We assume the input has shape `(batch_size, ...,
        input_dim)`.
    num_layers : `int`, optional (default=`1`)
        The number of highway layers to apply to the input.
    activation : `Callable[[torch.Tensor], torch.Tensor]`, optional (default=`torch.nn.functional.relu`)
        The non-linearity to use in the highway layers.
    """

    def __init__(self, input_dim: int, num_layers: int=1, activation: Callable[[torch.Tensor], torch.Tensor]=torch.nn.functional.relu) ->None:
        super().__init__()
        self._input_dim = input_dim
        self._layers = torch.nn.ModuleList([torch.nn.Linear(input_dim, input_dim * 2) for _ in range(num_layers)])
        self._activation = activation
        for layer in self._layers:
            layer.bias[input_dim:].data.fill_(1)

    def forward(self, inputs: torch.Tensor) ->torch.Tensor:
        current_input = inputs
        for layer in self._layers:
            projected_input = layer(current_input)
            linear_part = current_input
            nonlinear_part, gate = projected_input.chunk(2, dim=-1)
            nonlinear_part = self._activation(nonlinear_part)
            gate = torch.sigmoid(gate)
            current_input = gate * linear_part + (1 - gate) * nonlinear_part
        return current_input


def add_sentence_boundary_token_ids(tensor: torch.Tensor, mask: torch.BoolTensor, sentence_begin_token: Any, sentence_end_token: Any) ->Tuple[torch.Tensor, torch.BoolTensor]:
    """
    Add begin/end of sentence tokens to the batch of sentences.
    Given a batch of sentences with size `(batch_size, timesteps)` or
    `(batch_size, timesteps, dim)` this returns a tensor of shape
    `(batch_size, timesteps + 2)` or `(batch_size, timesteps + 2, dim)` respectively.

    Returns both the new tensor and updated mask.

    # Parameters

    tensor : `torch.Tensor`
        A tensor of shape `(batch_size, timesteps)` or `(batch_size, timesteps, dim)`
    mask : `torch.BoolTensor`
         A tensor of shape `(batch_size, timesteps)`
    sentence_begin_token: `Any`
        Can be anything that can be broadcast in torch for assignment.
        For 2D input, a scalar with the `<S>` id. For 3D input, a tensor with length dim.
    sentence_end_token: `Any`
        Can be anything that can be broadcast in torch for assignment.
        For 2D input, a scalar with the `</S>` id. For 3D input, a tensor with length dim.

    # Returns

    tensor_with_boundary_tokens : `torch.Tensor`
        The tensor with the appended and prepended boundary tokens. If the input was 2D,
        it has shape (batch_size, timesteps + 2) and if the input was 3D, it has shape
        (batch_size, timesteps + 2, dim).
    new_mask : `torch.BoolTensor`
        The new mask for the tensor, taking into account the appended tokens
        marking the beginning and end of the sentence.
    """
    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()
    tensor_shape = list(tensor.data.shape)
    new_shape = list(tensor_shape)
    new_shape[1] = tensor_shape[1] + 2
    tensor_with_boundary_tokens = tensor.new_zeros(*new_shape, device=tensor.device)
    if len(tensor_shape) == 2:
        tensor_with_boundary_tokens[:, 1:-1] = tensor
        tensor_with_boundary_tokens[:, 0] = sentence_begin_token
        for i, j in enumerate(sequence_lengths):
            tensor_with_boundary_tokens[i, j + 1] = sentence_end_token
        new_mask = tensor_with_boundary_tokens != 0
    elif len(tensor_shape) == 3:
        tensor_with_boundary_tokens[:, 1:-1, :] = tensor
        sentence_begin_token = sentence_begin_token.detach()
        sentence_end_token = sentence_end_token.detach()
        for i, j in enumerate(sequence_lengths):
            tensor_with_boundary_tokens[i, 0, :] = sentence_begin_token
            tensor_with_boundary_tokens[i, j + 1, :] = sentence_end_token
        new_mask = (tensor_with_boundary_tokens > 0).sum(dim=-1) > 0
    else:
        raise ValueError('add_sentence_boundary_token_ids only accepts 2D and 3D input')
    return tensor_with_boundary_tokens, new_mask


class _ElmoCharacterEncoder(torch.nn.Module):
    """
    Compute context insensitive token representation using pretrained biLM.

    This embedder has input character ids of size (batch_size, sequence_length, 50)
    and returns (batch_size, sequence_length + 2, embedding_dim), where embedding_dim
    is specified in the options file (typically 512).

    We add special entries at the beginning and end of each sequence corresponding
    to <S> and </S>, the beginning and end of sentence tokens.

    Note: this is a lower level class useful for advanced usage.  Most users should
    use `ElmoTokenEmbedder` or `allennlp.modules.Elmo` instead.

    # Parameters

    options_file : `str`
        ELMo JSON options file
    weight_file : `str`
        ELMo hdf5 weight file
    requires_grad : `bool`, optional, (default = `False`).
        If True, compute gradient of ELMo parameters for fine tuning.


    The relevant section of the options file is something like:

    ```
    {'char_cnn': {
        'activation': 'relu',
        'embedding': {'dim': 4},
        'filters': [[1, 4], [2, 8], [3, 16], [4, 32], [5, 64]],
        'max_characters_per_token': 50,
        'n_characters': 262,
        'n_highway': 2
        }
    }
    ```
    """

    def __init__(self, options_file: str, weight_file: str, requires_grad: bool=False) ->None:
        super().__init__()
        with open(cached_path(options_file), 'r') as fin:
            self._options = json.load(fin)
        self._weight_file = weight_file
        self.output_dim = self._options['lstm']['projection_dim']
        self.requires_grad = requires_grad
        self._load_weights()
        self._beginning_of_sentence_characters = torch.from_numpy(numpy.array(ELMoCharacterMapper.beginning_of_sentence_characters) + 1)
        self._end_of_sentence_characters = torch.from_numpy(numpy.array(ELMoCharacterMapper.end_of_sentence_characters) + 1)

    def get_output_dim(self):
        return self.output_dim

    def forward(self, inputs: torch.Tensor) ->Dict[str, torch.Tensor]:
        """
        Compute context insensitive token embeddings for ELMo representations.

        # Parameters

        inputs : `torch.Tensor`
            Shape `(batch_size, sequence_length, 50)` of character ids representing the
            current batch.

        # Returns

        Dict with keys:
        `'token_embedding'` : `torch.Tensor`
            Shape `(batch_size, sequence_length + 2, embedding_dim)` tensor with context
            insensitive token representations.
        `'mask'`:  `torch.BoolTensor`
            Shape `(batch_size, sequence_length + 2)` long tensor with sequence mask.
        """
        mask = (inputs > 0).sum(dim=-1) > 0
        character_ids_with_bos_eos, mask_with_bos_eos = add_sentence_boundary_token_ids(inputs, mask, self._beginning_of_sentence_characters, self._end_of_sentence_characters)
        max_chars_per_token = self._options['char_cnn']['max_characters_per_token']
        character_embedding = torch.nn.functional.embedding(character_ids_with_bos_eos.view(-1, max_chars_per_token), self._char_embedding_weights)
        cnn_options = self._options['char_cnn']
        if cnn_options['activation'] == 'tanh':
            activation = torch.tanh
        elif cnn_options['activation'] == 'relu':
            activation = torch.nn.functional.relu
        else:
            raise ConfigurationError('Unknown activation')
        character_embedding = torch.transpose(character_embedding, 1, 2)
        convs = []
        for i in range(len(self._convolutions)):
            conv = getattr(self, 'char_conv_{}'.format(i))
            convolved = conv(character_embedding)
            convolved, _ = torch.max(convolved, dim=-1)
            convolved = activation(convolved)
            convs.append(convolved)
        token_embedding = torch.cat(convs, dim=-1)
        token_embedding = self._highways(token_embedding)
        token_embedding = self._projection(token_embedding)
        batch_size, sequence_length, _ = character_ids_with_bos_eos.size()
        return {'mask': mask_with_bos_eos, 'token_embedding': token_embedding.view(batch_size, sequence_length, -1)}

    def _load_weights(self):
        self._load_char_embedding()
        self._load_cnn_weights()
        self._load_highway()
        self._load_projection()

    def _load_char_embedding(self):
        with h5py.File(cached_path(self._weight_file), 'r') as fin:
            char_embed_weights = fin['char_embed'][...]
        weights = numpy.zeros((char_embed_weights.shape[0] + 1, char_embed_weights.shape[1]), dtype='float32')
        weights[1:, :] = char_embed_weights
        self._char_embedding_weights = torch.nn.Parameter(torch.FloatTensor(weights), requires_grad=self.requires_grad)

    def _load_cnn_weights(self):
        cnn_options = self._options['char_cnn']
        filters = cnn_options['filters']
        char_embed_dim = cnn_options['embedding']['dim']
        convolutions = []
        for i, (width, num) in enumerate(filters):
            conv = torch.nn.Conv1d(in_channels=char_embed_dim, out_channels=num, kernel_size=width, bias=True)
            with h5py.File(cached_path(self._weight_file), 'r') as fin:
                weight = fin['CNN']['W_cnn_{}'.format(i)][...]
                bias = fin['CNN']['b_cnn_{}'.format(i)][...]
            w_reshaped = numpy.transpose(weight.squeeze(axis=0), axes=(2, 1, 0))
            if w_reshaped.shape != tuple(conv.weight.data.shape):
                raise ValueError('Invalid weight file')
            conv.weight.data.copy_(torch.FloatTensor(w_reshaped))
            conv.bias.data.copy_(torch.FloatTensor(bias))
            conv.weight.requires_grad = self.requires_grad
            conv.bias.requires_grad = self.requires_grad
            convolutions.append(conv)
            self.add_module('char_conv_{}'.format(i), conv)
        self._convolutions = convolutions

    def _load_highway(self):
        cnn_options = self._options['char_cnn']
        filters = cnn_options['filters']
        n_filters = sum(f[1] for f in filters)
        n_highway = cnn_options['n_highway']
        self._highways = Highway(n_filters, n_highway, activation=torch.nn.functional.relu)
        for k in range(n_highway):
            with h5py.File(cached_path(self._weight_file), 'r') as fin:
                w_transform = numpy.transpose(fin['CNN_high_{}'.format(k)]['W_transform'][...])
                w_carry = -1.0 * numpy.transpose(fin['CNN_high_{}'.format(k)]['W_carry'][...])
                weight = numpy.concatenate([w_transform, w_carry], axis=0)
                self._highways._layers[k].weight.data.copy_(torch.FloatTensor(weight))
                self._highways._layers[k].weight.requires_grad = self.requires_grad
                b_transform = fin['CNN_high_{}'.format(k)]['b_transform'][...]
                b_carry = -1.0 * fin['CNN_high_{}'.format(k)]['b_carry'][...]
                bias = numpy.concatenate([b_transform, b_carry], axis=0)
                self._highways._layers[k].bias.data.copy_(torch.FloatTensor(bias))
                self._highways._layers[k].bias.requires_grad = self.requires_grad

    def _load_projection(self):
        cnn_options = self._options['char_cnn']
        filters = cnn_options['filters']
        n_filters = sum(f[1] for f in filters)
        self._projection = torch.nn.Linear(n_filters, self.output_dim, bias=True)
        with h5py.File(cached_path(self._weight_file), 'r') as fin:
            weight = fin['CNN_proj']['W_proj'][...]
            bias = fin['CNN_proj']['b_proj'][...]
            self._projection.weight.data.copy_(torch.FloatTensor(numpy.transpose(weight)))
            self._projection.bias.data.copy_(torch.FloatTensor(bias))
            self._projection.weight.requires_grad = self.requires_grad
            self._projection.bias.requires_grad = self.requires_grad


IndexedTokenList = Dict[str, List[Any]]


def pad_sequence_to_length(sequence: Sequence, desired_length: int, default_value: Callable[[], Any]=lambda : 0, padding_on_right: bool=True) ->List:
    """
    Take a list of objects and pads it to the desired length, returning the padded list.  The
    original list is not modified.

    # Parameters

    sequence : `List`
        A list of objects to be padded.

    desired_length : `int`
        Maximum length of each sequence. Longer sequences are truncated to this length, and
        shorter ones are padded to it.

    default_value: `Callable`, optional (default=`lambda: 0`)
        Callable that outputs a default value (of any type) to use as padding values.  This is
        a lambda to avoid using the same object when the default value is more complex, like a
        list.

    padding_on_right : `bool`, optional (default=`True`)
        When we add padding tokens (or truncate the sequence), should we do it on the right or
        the left?

    # Returns

    padded_sequence : `List`
    """
    sequence = list(sequence)
    if padding_on_right:
        padded_sequence = sequence[:desired_length]
    else:
        padded_sequence = sequence[-desired_length:]
    pad_length = desired_length - len(padded_sequence)
    values_to_pad = [default_value()] * pad_length
    if padding_on_right:
        padded_sequence = padded_sequence + values_to_pad
    else:
        padded_sequence = values_to_pad + padded_sequence
    return padded_sequence


def batch_to_ids(batch: List[List[str]]) ->torch.Tensor:
    """
    Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters
    (len(batch), max sentence length, max word length).

    # Parameters

    batch : `List[List[str]]`, required
        A list of tokenized sentences.

    # Returns

        A tensor of padded character ids.
    """
    instances = []
    indexer = ELMoTokenCharactersIndexer()
    for sentence in batch:
        tokens = [Token(token) for token in sentence]
        field = TextField(tokens, {'character_ids': indexer})
        instance = Instance({'elmo': field})
        instances.append(instance)
    dataset = Batch(instances)
    vocab = Vocabulary()
    dataset.index_instances(vocab)
    return dataset.as_tensor_dict()['elmo']['character_ids']['elmo_tokens']


def lazy_groups_of(iterable: Iterable[A], group_size: int) ->Iterator[List[A]]:
    """
    Takes an iterable and batches the individual instances into lists of the
    specified size. The last list may be smaller if there are instances left over.
    """
    iterator = iter(iterable)
    while True:
        s = list(islice(iterator, group_size))
        if len(s) > 0:
            yield s
        else:
            break


def remove_sentence_boundaries(tensor: torch.Tensor, mask: torch.BoolTensor) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Remove begin/end of sentence embeddings from the batch of sentences.
    Given a batch of sentences with size `(batch_size, timesteps, dim)`
    this returns a tensor of shape `(batch_size, timesteps - 2, dim)` after removing
    the beginning and end sentence markers.  The sentences are assumed to be padded on the right,
    with the beginning of each sentence assumed to occur at index 0 (i.e., `mask[:, 0]` is assumed
    to be 1).

    Returns both the new tensor and updated mask.

    This function is the inverse of `add_sentence_boundary_token_ids`.

    # Parameters

    tensor : `torch.Tensor`
        A tensor of shape `(batch_size, timesteps, dim)`
    mask : `torch.BoolTensor`
         A tensor of shape `(batch_size, timesteps)`

    # Returns

    tensor_without_boundary_tokens : `torch.Tensor`
        The tensor after removing the boundary tokens of shape `(batch_size, timesteps - 2, dim)`
    new_mask : `torch.BoolTensor`
        The new mask for the tensor of shape `(batch_size, timesteps - 2)`.
    """
    sequence_lengths = mask.sum(dim=1).detach().cpu().numpy()
    tensor_shape = list(tensor.data.shape)
    new_shape = list(tensor_shape)
    new_shape[1] = tensor_shape[1] - 2
    tensor_without_boundary_tokens = tensor.new_zeros(*new_shape)
    new_mask = tensor.new_zeros((new_shape[0], new_shape[1]), dtype=torch.bool)
    for i, j in enumerate(sequence_lengths):
        if j > 2:
            tensor_without_boundary_tokens[i, :j - 2, :] = tensor[i, 1:j - 1, :]
            new_mask[i, :j - 2] = True
    return tensor_without_boundary_tokens, new_mask


class _ElmoBiLm(torch.nn.Module):
    """
    Run a pre-trained bidirectional language model, outputting the activations at each
    layer for weighting together into an ELMo representation (with
    `allennlp.modules.seq2seq_encoders.Elmo`).  This is a lower level class, useful
    for advanced uses, but most users should use `allennlp.modules.Elmo` directly.

    # Parameters

    options_file : `str`
        ELMo JSON options file
    weight_file : `str`
        ELMo hdf5 weight file
    requires_grad : `bool`, optional, (default = `False`).
        If True, compute gradient of ELMo parameters for fine tuning.
    vocab_to_cache : `List[str]`, optional, (default = `None`).
        A list of words to pre-compute and cache character convolutions
        for. If you use this option, _ElmoBiLm expects that you pass word
        indices of shape (batch_size, timesteps) to forward, instead
        of character indices. If you use this option and pass a word which
        wasn't pre-cached, this will break.
    """

    def __init__(self, options_file: str, weight_file: str, requires_grad: bool=False, vocab_to_cache: List[str]=None) ->None:
        super().__init__()
        self._token_embedder = _ElmoCharacterEncoder(options_file, weight_file, requires_grad=requires_grad)
        self._requires_grad = requires_grad
        if requires_grad and vocab_to_cache:
            logging.warning('You are fine tuning ELMo and caching char CNN word vectors. This behaviour is not guaranteed to be well defined, particularly. if not all of your inputs will occur in the vocabulary cache.')
        self._word_embedding = None
        self._bos_embedding: torch.Tensor = None
        self._eos_embedding: torch.Tensor = None
        if vocab_to_cache:
            logging.info('Caching character cnn layers for words in vocabulary.')
            self.create_cached_cnn_embeddings(vocab_to_cache)
        with open(cached_path(options_file), 'r') as fin:
            options = json.load(fin)
        if not options['lstm'].get('use_skip_connections'):
            raise ConfigurationError('We only support pretrained biLMs with residual connections')
        self._elmo_lstm = ElmoLstm(input_size=options['lstm']['projection_dim'], hidden_size=options['lstm']['projection_dim'], cell_size=options['lstm']['dim'], num_layers=options['lstm']['n_layers'], memory_cell_clip_value=options['lstm']['cell_clip'], state_projection_clip_value=options['lstm']['proj_clip'], requires_grad=requires_grad)
        self._elmo_lstm.load_weights(weight_file)
        self.num_layers = options['lstm']['n_layers'] + 1

    def get_output_dim(self):
        return 2 * self._token_embedder.get_output_dim()

    def forward(self, inputs: torch.Tensor, word_inputs: torch.Tensor=None) ->Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:
        """
        # Parameters

        inputs : `torch.Tensor`, required.
            Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.
        word_inputs : `torch.Tensor`, required.
            If you passed a cached vocab, you can in addition pass a tensor of shape `(batch_size, timesteps)`,
            which represent word ids which have been pre-cached.

        # Returns

        Dict with keys:

        `'activations'` : `List[torch.Tensor]`
            A list of activations at each layer of the network, each of shape
            `(batch_size, timesteps + 2, embedding_dim)`
        `'mask'`:  `torch.BoolTensor`
            Shape `(batch_size, timesteps + 2)` long tensor with sequence mask.

        Note that the output tensors all include additional special begin and end of sequence
        markers.
        """
        if self._word_embedding is not None and word_inputs is not None:
            try:
                mask_without_bos_eos = word_inputs > 0
                embedded_inputs = self._word_embedding(word_inputs)
                type_representation, mask = add_sentence_boundary_token_ids(embedded_inputs, mask_without_bos_eos, self._bos_embedding, self._eos_embedding)
            except (RuntimeError, IndexError):
                token_embedding = self._token_embedder(inputs)
                mask = token_embedding['mask']
                type_representation = token_embedding['token_embedding']
        else:
            token_embedding = self._token_embedder(inputs)
            mask = token_embedding['mask']
            type_representation = token_embedding['token_embedding']
        lstm_outputs = self._elmo_lstm(type_representation, mask)
        output_tensors = [torch.cat([type_representation, type_representation], dim=-1) * mask.unsqueeze(-1)]
        for layer_activations in torch.chunk(lstm_outputs, lstm_outputs.size(0), dim=0):
            output_tensors.append(layer_activations.squeeze(0))
        return {'activations': output_tensors, 'mask': mask}

    def create_cached_cnn_embeddings(self, tokens: List[str]) ->None:
        """
        Given a list of tokens, this method precomputes word representations
        by running just the character convolutions and highway layers of elmo,
        essentially creating uncontextual word vectors. On subsequent forward passes,
        the word ids are looked up from an embedding, rather than being computed on
        the fly via the CNN encoder.

        This function sets 3 attributes:

        _word_embedding : `torch.Tensor`
            The word embedding for each word in the tokens passed to this method.
        _bos_embedding : `torch.Tensor`
            The embedding for the BOS token.
        _eos_embedding : `torch.Tensor`
            The embedding for the EOS token.

        # Parameters

        tokens : `List[str]`, required.
            A list of tokens to precompute character convolutions for.
        """
        tokens = [ELMoCharacterMapper.bos_token, ELMoCharacterMapper.eos_token] + tokens
        timesteps = 32
        batch_size = 32
        chunked_tokens = lazy_groups_of(iter(tokens), timesteps)
        all_embeddings = []
        device = get_device_of(next(self.parameters()))
        for batch in lazy_groups_of(chunked_tokens, batch_size):
            batched_tensor = batch_to_ids(batch)
            if device >= 0:
                batched_tensor = batched_tensor
            output = self._token_embedder(batched_tensor)
            token_embedding = output['token_embedding']
            mask = output['mask']
            token_embedding, _ = remove_sentence_boundaries(token_embedding, mask)
            all_embeddings.append(token_embedding.view(-1, token_embedding.size(-1)))
        full_embedding = torch.cat(all_embeddings, 0)
        full_embedding = full_embedding[:len(tokens), :]
        embedding = full_embedding[2:len(tokens), :]
        vocab_size, embedding_dim = list(embedding.size())
        self._bos_embedding = full_embedding[0, :]
        self._eos_embedding = full_embedding[1, :]
        self._word_embedding = Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, weight=embedding.data, trainable=self._requires_grad, padding_index=0)


class FeedForward(torch.nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.linear = torch.nn.Linear(4, 4)
        self.activation = torch.nn.ReLU()

    def forward(self, x):
        return self.activation(self.linear(x))


class InputVariationalDropout(torch.nn.Dropout):
    """
    Apply the dropout technique in Gal and Ghahramani, [Dropout as a Bayesian Approximation:
    Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142) to a
    3D tensor.

    This module accepts a 3D tensor of shape `(batch_size, num_timesteps, embedding_dim)`
    and samples a single dropout mask of shape `(batch_size, embedding_dim)` and applies
    it to every time step.
    """

    def forward(self, input_tensor):
        """
        Apply dropout to input tensor.

        # Parameters

        input_tensor : `torch.FloatTensor`
            A tensor of shape `(batch_size, num_timesteps, embedding_dim)`

        # Returns

        output : `torch.FloatTensor`
            A tensor of shape `(batch_size, num_timesteps, embedding_dim)` with dropout applied.
        """
        ones = input_tensor.data.new_ones(input_tensor.shape[0], input_tensor.shape[-1])
        dropout_mask = torch.nn.functional.dropout(ones, self.p, self.training, inplace=False)
        if self.inplace:
            input_tensor *= dropout_mask.unsqueeze(1)
            return None
        else:
            return dropout_mask.unsqueeze(1) * input_tensor


class MaskedLayerNorm(torch.nn.Module):
    """
    See LayerNorm for details.

    Note, however, that unlike LayerNorm this norm includes a batch component.
    """

    def __init__(self, size: int, gamma0: float=0.1) ->None:
        super().__init__()
        self.gamma = torch.nn.Parameter(torch.ones(1, 1, size) * gamma0)
        self.beta = torch.nn.Parameter(torch.zeros(1, 1, size))
        self.size = size

    def forward(self, tensor: torch.Tensor, mask: torch.BoolTensor) ->torch.Tensor:
        broadcast_mask = mask.unsqueeze(-1)
        num_elements = broadcast_mask.sum() * self.size
        mean = (tensor * broadcast_mask).sum() / num_elements
        masked_centered = (tensor - mean) * broadcast_mask
        std = torch.sqrt((masked_centered * masked_centered).sum() / num_elements + util.tiny_value_of_dtype(tensor.dtype))
        return self.gamma * (tensor - mean) / (std + util.tiny_value_of_dtype(tensor.dtype)) + self.beta


class ResidualWithLayerDropout(torch.nn.Module):
    """
    A residual connection with the layer dropout technique [Deep Networks with Stochastic
    Depth](https://arxiv.org/pdf/1603.09382.pdf).

    This module accepts the input and output of a layer, decides whether this layer should
    be stochastically dropped, returns either the input or output + input. During testing,
    it will re-calibrate the outputs of this layer by the expected number of times it
    participates in training.
    """

    def __init__(self, undecayed_dropout_prob: float=0.5) ->None:
        super().__init__()
        if undecayed_dropout_prob < 0 or undecayed_dropout_prob > 1:
            raise ValueError(f'undecayed dropout probability has to be between 0 and 1, but got {undecayed_dropout_prob}')
        self.undecayed_dropout_prob = undecayed_dropout_prob

    def forward(self, layer_input: torch.Tensor, layer_output: torch.Tensor, layer_index: int=None, total_layers: int=None) ->torch.Tensor:
        """
        Apply dropout to this layer, for this whole mini-batch.
        dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and
        total_layers is specified, else it will use the undecayed_dropout_prob directly.

        # Parameters

        layer_input `torch.FloatTensor` required
            The input tensor of this layer.
        layer_output `torch.FloatTensor` required
            The output tensor of this layer, with the same shape as the layer_input.
        layer_index `int`
            The layer index, starting from 1. This is used to calcuate the dropout prob
            together with the `total_layers` parameter.
        total_layers `int`
            The total number of layers.

        # Returns

        output : `torch.FloatTensor`
            A tensor with the same shape as `layer_input` and `layer_output`.
        """
        if layer_index is not None and total_layers is not None:
            dropout_prob = 1.0 * self.undecayed_dropout_prob * layer_index / total_layers
        else:
            dropout_prob = 1.0 * self.undecayed_dropout_prob
        if self.training:
            if torch.rand(1) < dropout_prob:
                return layer_input
            else:
                return layer_output + layer_input
        else:
            return (1 - dropout_prob) * layer_output + layer_input


def _choice(num_words: int, num_samples: int) ->Tuple[np.ndarray, int]:
    """
    Chooses `num_samples` samples without replacement from [0, ..., num_words).
    Returns a tuple (samples, num_tries).
    """
    num_tries = 0
    num_chosen = 0

    def get_buffer() ->np.ndarray:
        log_samples = np.random.rand(num_samples) * np.log(num_words + 1)
        samples = np.exp(log_samples).astype('int64') - 1
        return np.clip(samples, a_min=0, a_max=num_words - 1)
    sample_buffer = get_buffer()
    buffer_index = 0
    samples: Set[int] = set()
    while num_chosen < num_samples:
        num_tries += 1
        sample_id = sample_buffer[buffer_index]
        if sample_id not in samples:
            samples.add(sample_id)
            num_chosen += 1
        buffer_index += 1
        if buffer_index == num_samples:
            sample_buffer = get_buffer()
            buffer_index = 0
    return np.array(list(samples)), num_tries


class SampledSoftmaxLoss(torch.nn.Module):
    """
    Based on the default log_uniform_candidate_sampler in tensorflow.

    !!! NOTE
        num_words DOES NOT include padding id.

    !!! NOTE
        In all cases except (tie_embeddings=True and use_character_inputs=False)
        the weights are dimensioned as num_words and do not include an entry for the padding (0) id.
        For the (tie_embeddings=True and use_character_inputs=False) case,
        then the embeddings DO include the extra 0 padding, to be consistent with the word embedding layer.

    # Parameters

    num_words, `int`, required
        The number of words in the vocabulary
    embedding_dim, `int`, required
        The dimension to softmax over
    num_samples, `int`, required
        During training take this many samples. Must be less than num_words.
    sparse, `bool`, optional (default = `False`)
        If this is true, we use a sparse embedding matrix.
    unk_id, `int`, optional (default = `None`)
        If provided, the id that represents unknown characters.
    use_character_inputs, `bool`, optional (default = `True`)
        Whether to use character inputs
    use_fast_sampler, `bool`, optional (default = `False`)
        Whether to use the fast cython sampler.
    """

    def __init__(self, num_words: int, embedding_dim: int, num_samples: int, sparse: bool=False, unk_id: int=None, use_character_inputs: bool=True, use_fast_sampler: bool=False) ->None:
        super().__init__()
        self.tie_embeddings = False
        assert num_samples < num_words
        if use_fast_sampler:
            raise ConfigurationError('fast sampler is not implemented')
        else:
            self.choice_func = _choice
        if sparse:
            self.softmax_w = torch.nn.Embedding(num_embeddings=num_words, embedding_dim=embedding_dim, sparse=True)
            self.softmax_w.weight.data.normal_(mean=0.0, std=1.0 / np.sqrt(embedding_dim))
            self.softmax_b = torch.nn.Embedding(num_embeddings=num_words, embedding_dim=1, sparse=True)
            self.softmax_b.weight.data.fill_(0.0)
        else:
            self.softmax_w = torch.nn.Parameter(torch.randn(num_words, embedding_dim) / np.sqrt(embedding_dim))
            self.softmax_b = torch.nn.Parameter(torch.zeros(num_words))
        self.sparse = sparse
        self.use_character_inputs = use_character_inputs
        if use_character_inputs:
            self._unk_id = unk_id
        self._num_samples = num_samples
        self._embedding_dim = embedding_dim
        self._num_words = num_words
        self.initialize_num_words()

    def initialize_num_words(self):
        if self.sparse:
            num_words = self.softmax_w.weight.size(0)
        else:
            num_words = self.softmax_w.size(0)
        self._num_words = num_words
        self._log_num_words_p1 = np.log(num_words + 1)
        self._probs = (np.log(np.arange(num_words) + 2) - np.log(np.arange(num_words) + 1)) / self._log_num_words_p1

    def forward(self, embeddings: torch.Tensor, targets: torch.Tensor, target_token_embedding: torch.Tensor=None) ->torch.Tensor:
        if embeddings.shape[0] == 0:
            return torch.tensor(0.0, device=embeddings.device)
        if not self.training:
            return self._forward_eval(embeddings, targets)
        else:
            return self._forward_train(embeddings, targets, target_token_embedding)

    def _forward_train(self, embeddings: torch.Tensor, targets: torch.Tensor, target_token_embedding: torch.Tensor) ->torch.Tensor:
        sampled_ids, target_expected_count, sampled_expected_count = self.log_uniform_candidate_sampler(targets, choice_func=self.choice_func)
        long_targets = targets.long()
        long_targets.requires_grad_(False)
        all_ids = torch.cat([long_targets, sampled_ids], dim=0)
        if self.sparse:
            all_ids_1 = all_ids.unsqueeze(1)
            all_w = self.softmax_w(all_ids_1).squeeze(1)
            all_b = self.softmax_b(all_ids_1).squeeze(2).squeeze(1)
        else:
            all_w = torch.nn.functional.embedding(all_ids, self.softmax_w)
            all_b = torch.nn.functional.embedding(all_ids, self.softmax_b.unsqueeze(1)).squeeze(1)
        batch_size = long_targets.size(0)
        true_w = all_w[:batch_size, :]
        sampled_w = all_w[batch_size:, :]
        true_b = all_b[:batch_size]
        sampled_b = all_b[batch_size:]
        true_logits = (true_w * embeddings).sum(dim=1) + true_b - torch.log(target_expected_count + util.tiny_value_of_dtype(target_expected_count.dtype))
        sampled_logits = torch.matmul(embeddings, sampled_w.t()) + sampled_b - torch.log(sampled_expected_count + util.tiny_value_of_dtype(sampled_expected_count.dtype))
        true_in_sample_mask = sampled_ids == long_targets.unsqueeze(1)
        masked_sampled_logits = sampled_logits.masked_fill(true_in_sample_mask, -10000.0)
        logits = torch.cat([true_logits.unsqueeze(1), masked_sampled_logits], dim=1)
        log_softmax = torch.nn.functional.log_softmax(logits, dim=1)
        nll_loss = -1.0 * log_softmax[:, 0].sum()
        return nll_loss

    def _forward_eval(self, embeddings: torch.Tensor, targets: torch.Tensor) ->torch.Tensor:
        if self.sparse:
            w = self.softmax_w.weight
            b = self.softmax_b.weight.squeeze(1)
        else:
            w = self.softmax_w
            b = self.softmax_b
        log_softmax = torch.nn.functional.log_softmax(torch.matmul(embeddings, w.t()) + b, dim=-1)
        if self.tie_embeddings and not self.use_character_inputs:
            targets_ = targets + 1
        else:
            targets_ = targets
        return torch.nn.functional.nll_loss(log_softmax, targets_.long(), reduction='sum')

    def log_uniform_candidate_sampler(self, targets, choice_func=_choice):
        np_sampled_ids, num_tries = choice_func(self._num_words, self._num_samples)
        sampled_ids = torch.from_numpy(np_sampled_ids)
        target_probs = torch.log((targets.float() + 2.0) / (targets.float() + 1.0)) / self._log_num_words_p1
        target_expected_count = -1.0 * (torch.exp(num_tries * torch.log1p(-target_probs)) - 1.0)
        sampled_probs = torch.log((sampled_ids.float() + 2.0) / (sampled_ids.float() + 1.0)) / self._log_num_words_p1
        sampled_expected_count = -1.0 * (torch.exp(num_tries * torch.log1p(-sampled_probs)) - 1.0)
        sampled_ids.requires_grad_(False)
        target_expected_count.requires_grad_(False)
        sampled_expected_count.requires_grad_(False)
        return sampled_ids, target_expected_count, sampled_expected_count


class ResidualBlock(torch.nn.Module):

    def __init__(self, input_dim: int, layers: Sequence[Sequence[int]], direction: str, do_weight_norm: bool=True, dropout: float=0.0) ->None:
        super().__init__()
        self.dropout = dropout
        self._convolutions = torch.nn.ModuleList()
        last_dim = input_dim
        for k, layer in enumerate(layers):
            if len(layer) == 2:
                conv = torch.nn.Conv1d(last_dim, layer[1] * 2, layer[0], stride=1, padding=layer[0] - 1, bias=True)
            elif len(layer) == 3:
                assert layer[0] == 2, 'only support kernel = 2 for now'
                conv = torch.nn.Conv1d(last_dim, layer[1] * 2, layer[0], stride=1, padding=layer[2], dilation=layer[2], bias=True)
            else:
                raise ValueError('each layer must have length 2 or 3')
            if k == 0:
                conv_dropout = dropout
            else:
                conv_dropout = 0.0
            std = math.sqrt(4 * (1.0 - conv_dropout) / (layer[0] * last_dim))
            conv.weight.data.normal_(0, std=std)
            conv.bias.data.zero_()
            if do_weight_norm:
                conv = torch.nn.utils.weight_norm(conv, name='weight', dim=0)
            self._convolutions.append(conv)
            last_dim = layer[1]
        assert last_dim == input_dim
        if direction not in ('forward', 'backward'):
            raise ConfigurationError(f'invalid direction: {direction}')
        self._direction = direction

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        out = x
        timesteps = x.size(2)
        for k, convolution in enumerate(self._convolutions):
            if k == 0 and self.dropout > 0:
                out = torch.nn.functional.dropout(out, self.dropout, self.training)
            conv_out = convolution(out)
            dims_to_remove = conv_out.size(2) - timesteps
            if dims_to_remove > 0:
                if self._direction == 'forward':
                    conv_out = conv_out.narrow(2, 0, timesteps)
                else:
                    conv_out = conv_out.narrow(2, dims_to_remove, timesteps)
            out = torch.nn.functional.glu(conv_out, dim=1)
        return (out + x) * math.sqrt(0.5)


class SoftmaxLoss(torch.nn.Module):
    """
    Given some embeddings and some targets, applies a linear layer
    to create logits over possible words and then returns the
    negative log likelihood. Does not add a padding ID into the
    vocabulary, and input `targets` to `forward` should not include
    a padding ID.
    """

    def __init__(self, num_words: int, embedding_dim: int) ->None:
        super().__init__()
        self.tie_embeddings = False
        self.softmax_w = torch.nn.Parameter(torch.randn(embedding_dim, num_words) / np.sqrt(embedding_dim))
        self.softmax_b = torch.nn.Parameter(torch.zeros(num_words))

    def forward(self, embeddings: torch.Tensor, targets: torch.Tensor) ->torch.Tensor:
        """
        # Parameters

        embeddings : `torch.Tensor`
            A tensor of shape `(sequence_length, embedding_dim)`
        targets : `torch.Tensor`
            A tensor of shape `(batch_size, )`

        # Returns

        loss : `torch.FloatTensor`
            A scalar loss to be optimized.
        """
        probs = torch.nn.functional.log_softmax(torch.matmul(embeddings, self.softmax_w) + self.softmax_b, dim=-1)
        return torch.nn.functional.nll_loss(probs, targets.long(), reduction='sum')


TensorPair = Tuple[torch.Tensor, torch.Tensor]


class StackedAlternatingLstm(torch.nn.Module):
    """
    A stacked LSTM with LSTM layers which alternate between going forwards over
    the sequence and going backwards. This implementation is based on the
    description in [Deep Semantic Role Labeling - What works and what's next][0].

    [0]: https://www.aclweb.org/anthology/P17-1044.pdf
    [1]: https://arxiv.org/abs/1512.05287

    # Parameters

    input_size : `int`, required
        The dimension of the inputs to the LSTM.
    hidden_size : `int`, required
        The dimension of the outputs of the LSTM.
    num_layers : `int`, required
        The number of stacked LSTMs to use.
    recurrent_dropout_probability : `float`, optional (default = `0.0`)
        The dropout probability to be used in a dropout scheme as stated in
        [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks][1].
    use_input_projection_bias : `bool`, optional (default = `True`)
        Whether or not to use a bias on the input projection layer. This is mainly here
        for backwards compatibility reasons and will be removed (and set to False)
        in future releases.

    # Returns

    output_accumulator : `PackedSequence`
        The outputs of the interleaved LSTMs per timestep. A tensor of shape
        (batch_size, max_timesteps, hidden_size) where for a given batch
        element, all outputs past the sequence length for that batch are
        zero tensors.
    """

    def __init__(self, input_size: int, hidden_size: int, num_layers: int, recurrent_dropout_probability: float=0.0, use_highway: bool=True, use_input_projection_bias: bool=True) ->None:
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        layers = []
        lstm_input_size = input_size
        for layer_index in range(num_layers):
            go_forward = layer_index % 2 == 0
            layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward, recurrent_dropout_probability=recurrent_dropout_probability, use_highway=use_highway, use_input_projection_bias=use_input_projection_bias)
            lstm_input_size = hidden_size
            self.add_module('layer_{}'.format(layer_index), layer)
            layers.append(layer)
        self.lstm_layers = layers

    def forward(self, inputs: PackedSequence, initial_state: Optional[TensorPair]=None) ->Tuple[Union[torch.Tensor, PackedSequence], TensorPair]:
        """
        # Parameters

        inputs : `PackedSequence`, required.
            A batch first `PackedSequence` to run the stacked LSTM over.
        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)
            A tuple (state, memory) representing the initial hidden state and memory
            of the LSTM. Each tensor has shape (1, batch_size, output_dimension).

        # Returns

        output_sequence : `PackedSequence`
            The encoded sequence of shape (batch_size, sequence_length, hidden_size)
        final_states: `Tuple[torch.Tensor, torch.Tensor]`
            The per-layer final (state, memory) states of the LSTM, each with shape
            (num_layers, batch_size, hidden_size).
        """
        if not initial_state:
            hidden_states: List[Optional[TensorPair]] = [None] * len(self.lstm_layers)
        elif initial_state[0].size()[0] != len(self.lstm_layers):
            raise ConfigurationError('Initial states were passed to forward() but the number of initial states does not match the number of layers.')
        else:
            hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))
        output_sequence = inputs
        final_states = []
        for i, state in enumerate(hidden_states):
            layer = getattr(self, 'layer_{}'.format(i))
            output_sequence, final_state = layer(output_sequence, state)
            final_states.append(final_state)
        final_hidden_state, final_cell_state = tuple(torch.cat(state_list, 0) for state_list in zip(*final_states))
        return output_sequence, (final_hidden_state, final_cell_state)


class StackedBidirectionalLstm(torch.nn.Module):
    """
    A standard stacked Bidirectional LSTM where the LSTM layers
    are concatenated between each layer. The only difference between
    this and a regular bidirectional LSTM is the application of
    variational dropout to the hidden states and outputs of each layer apart
    from the last layer of the LSTM. Note that this will be slower, as it
    doesn't use CUDNN.

    [0]: https://arxiv.org/abs/1512.05287

    # Parameters

    input_size : `int`, required
        The dimension of the inputs to the LSTM.
    hidden_size : `int`, required
        The dimension of the outputs of the LSTM.
    num_layers : `int`, required
        The number of stacked Bidirectional LSTMs to use.
    recurrent_dropout_probability : `float`, optional (default = `0.0`)
        The recurrent dropout probability to be used in a dropout scheme as
        stated in [A Theoretically Grounded Application of Dropout in Recurrent
        Neural Networks][0].
    layer_dropout_probability : `float`, optional (default = `0.0`)
        The layer wise dropout probability to be used in a dropout scheme as
        stated in [A Theoretically Grounded Application of Dropout in Recurrent
        Neural Networks][0].
    use_highway : `bool`, optional (default = `True`)
        Whether or not to use highway connections between layers. This effectively involves
        reparameterising the normal output of an LSTM as::

            gate = sigmoid(W_x1 * x_t + W_h * h_t)
            output = gate * h_t  + (1 - gate) * (W_x2 * x_t)
    """

    def __init__(self, input_size: int, hidden_size: int, num_layers: int, recurrent_dropout_probability: float=0.0, layer_dropout_probability: float=0.0, use_highway: bool=True) ->None:
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = True
        layers = []
        lstm_input_size = input_size
        for layer_index in range(num_layers):
            forward_layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward=True, recurrent_dropout_probability=recurrent_dropout_probability, use_highway=use_highway, use_input_projection_bias=False)
            backward_layer = AugmentedLstm(lstm_input_size, hidden_size, go_forward=False, recurrent_dropout_probability=recurrent_dropout_probability, use_highway=use_highway, use_input_projection_bias=False)
            lstm_input_size = hidden_size * 2
            self.add_module('forward_layer_{}'.format(layer_index), forward_layer)
            self.add_module('backward_layer_{}'.format(layer_index), backward_layer)
            layers.append([forward_layer, backward_layer])
        self.lstm_layers = layers
        self.layer_dropout = InputVariationalDropout(layer_dropout_probability)

    def forward(self, inputs: PackedSequence, initial_state: Optional[TensorPair]=None) ->Tuple[PackedSequence, TensorPair]:
        """
        # Parameters

        inputs : `PackedSequence`, required.
            A batch first `PackedSequence` to run the stacked LSTM over.
        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = `None`)
            A tuple (state, memory) representing the initial hidden state and memory
            of the LSTM. Each tensor has shape (num_layers, batch_size, output_dimension * 2).

        # Returns

        output_sequence : `PackedSequence`
            The encoded sequence of shape (batch_size, sequence_length, hidden_size * 2)
        final_states: `torch.Tensor`
            The per-layer final (state, memory) states of the LSTM, each with shape
            (num_layers * 2, batch_size, hidden_size * 2).
        """
        if initial_state is None:
            hidden_states: List[Optional[TensorPair]] = [None] * len(self.lstm_layers)
        elif initial_state[0].size()[0] != len(self.lstm_layers):
            raise ConfigurationError('Initial states were passed to forward() but the number of initial states does not match the number of layers.')
        else:
            hidden_states = list(zip(initial_state[0].split(1, 0), initial_state[1].split(1, 0)))
        output_sequence = inputs
        final_h = []
        final_c = []
        for i, state in enumerate(hidden_states):
            forward_layer = getattr(self, 'forward_layer_{}'.format(i))
            backward_layer = getattr(self, 'backward_layer_{}'.format(i))
            forward_output, final_forward_state = forward_layer(output_sequence, state)
            backward_output, final_backward_state = backward_layer(output_sequence, state)
            forward_output, lengths = pad_packed_sequence(forward_output, batch_first=True)
            backward_output, _ = pad_packed_sequence(backward_output, batch_first=True)
            output_sequence = torch.cat([forward_output, backward_output], -1)
            if i < self.num_layers - 1:
                output_sequence = self.layer_dropout(output_sequence)
            output_sequence = pack_padded_sequence(output_sequence, lengths, batch_first=True)
            final_h.extend([final_forward_state[0], final_backward_state[0]])
            final_c.extend([final_forward_state[1], final_backward_state[1]])
        final_h = torch.cat(final_h, dim=0)
        final_c = torch.cat(final_c, dim=0)
        final_state_tuple = final_h, final_c
        return output_sequence, final_state_tuple


class RegionDetectorOutput(NamedTuple):
    """
    The output type from the forward pass of a `RegionDetector`.
    """
    features: List[Tensor]
    """
    A list of tensors, each with shape `(num_boxes, feature_dim)`.
    """
    boxes: List[Tensor]
    """
    A list of tensors containing the coordinates for each box. Each has shape `(num_boxes, 4)`.
    """
    class_probs: Optional[List[Tensor]] = None
    """
    An optional list of tensors. These tensors can have shape `(num_boxes,)` or
    `(num_boxes, *)` if probabilities for multiple classes are given.
    """
    class_labels: Optional[List[Tensor]] = None
    """
    An optional list of tensors that give the labels corresponding to the `class_probs`
    tensors. This should be non-`None` whenever `class_probs` is, and each tensor
    should have the same shape as the corresponding tensor from `class_probs`.
    """


class _IncompatibleKeys(NamedTuple):
    missing_keys: List[str]
    unexpected_keys: List[str]

    def __repr__(self):
        if not self.missing_keys and not self.unexpected_keys:
            return '<All keys matched successfully>'
        return f'(missing_keys = {self.missing_keys}, unexpected_keys = {self.unexpected_keys})'


class Module(torch.nn.Module):
    """
    This is just `torch.nn.Module` with some extra functionality.
    """

    def _post_load_state_dict(self, missing_keys: List[str], unexpected_keys: List[str]) ->Tuple[List[str], List[str]]:
        """
        Subclasses can override this and potentially modify `missing_keys` or `unexpected_keys`.
        """
        return missing_keys, unexpected_keys

    def load_state_dict(self, state_dict: StateDictType, strict: bool=True) ->_IncompatibleKeys:
        """
        Same as [`torch.nn.Module.load_state_dict()`]
        (https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict)
        except we also run the [`_post_load_state_dict`](#_post_load_state_dict) method before returning,
        which can be implemented by subclasses to customize the behavior.
        """
        missing_keys, unexpected_keys = super().load_state_dict(state_dict, strict=False)
        missing_keys, unexpected_keys = self._post_load_state_dict(missing_keys, unexpected_keys)
        _check_incompatible_keys(self, missing_keys, unexpected_keys, strict)
        return _IncompatibleKeys(missing_keys, unexpected_keys)

    def load_state_dict_distributed(self, state_dict: Optional[StateDictType], strict: bool=True) ->_IncompatibleKeys:
        missing_keys, unexpected_keys = load_state_dict_distributed(self, state_dict, strict=strict)
        missing_keys, unexpected_keys = self._post_load_state_dict(missing_keys, unexpected_keys)
        _check_incompatible_keys(self, missing_keys, unexpected_keys, strict)
        return _IncompatibleKeys(missing_keys, unexpected_keys)


class LinearForTesting(nn.Linear):

    def forward(self, *args, **kwargs):
        global forward_calls
        forward_calls += 1
        return super().forward(*args, **kwargs)


class SubmoduleWithKwargs(nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.linear1 = nn.Linear(3, 3)
        self.linear2 = nn.Linear(3, 3)

    def forward(self, x, y=None) ->torch.Tensor:
        out = self.linear1(x)
        if y is not None:
            out = x + self.linear2(y)
        return out.sum(-1)


class FeedForwardForTesting(nn.Module):

    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(3, 3)
        self.dropout = nn.Dropout(p=0.5)
        self.linear2 = nn.Linear(3, 3)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        global forward_calls
        forward_calls += 1
        return self.linear2(self.dropout(self.linear1(x)))


class Encoder(torch.nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.ff1 = FeedForward()
        self.ff2 = FeedForward()
        self.register_buffer('buffer', torch.randn(4, 4))

    def forward(self, x):
        return self.ff2(self.ff1(x))


class _Net1(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear_1 = torch.nn.Linear(5, 10)
        self.linear_2 = torch.nn.Linear(10, 5)
        self.scalar = torch.nn.Parameter(torch.rand(()))

    def forward(self, inputs):
        pass


class _Net2(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.linear_1 = torch.nn.Linear(5, 10)
        self.linear_3 = torch.nn.Linear(10, 5)
        self.scalar = torch.nn.Parameter(torch.rand(()))

    def forward(self, inputs):
        pass


class DistributedFixtureSubmodule(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.direct_param = torch.nn.Parameter(torch.randn(3, 5))
        self.register_buffer('direct_buffer', torch.randn(2, 2))
        self.linear_submodule = torch.nn.Linear(3, 5)

    def forward(self, x):
        pass


class ShardedDistributedFixtureSubmodule(DistributedFixtureSubmodule, ShardedModuleMixin):

    def get_original_module(self):
        return self


class DistributedFixtureModel(torch.nn.Module):
    """
    Fake model for testing `load_state_dict_distributed()`.
    """

    def __init__(self):
        super().__init__()
        self.direct_param = torch.nn.Parameter(torch.randn(3, 5))
        self.register_buffer('direct_buffer', torch.randn(2, 2))
        self.custom_submodule = DistributedFixtureSubmodule()
        self.custom_sharded_submodule = ShardedDistributedFixtureSubmodule()
        self.linear_submodule = torch.nn.Linear(3, 5)

    def forward(self, x):
        pass


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ConditionalRandomFieldWeightLannoy,
     lambda: ([], {'num_tags': 4, 'label_weights': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.ones([4, 4], dtype=torch.int64)], {}),
     False),
    (DistributedFixtureModel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DistributedFixtureSubmodule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ElmoLstm,
     lambda: ([], {'input_size': 4, 'hidden_size': 4, 'cell_size': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4])], {}),
     False),
    (Encoder,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FeedForward,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Highway,
     lambda: ([], {'input_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (InputVariationalDropout,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LstmCellWithProjection,
     lambda: ([], {'input_size': 4, 'hidden_size': 4, 'cell_size': 4}),
     lambda: ([torch.rand([4, 4, 4]), [4, 4, 4, 4]], {}),
     False),
    (MaskedLayerNorm,
     lambda: ([], {'size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (ResidualWithLayerDropout,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (ScalarMix,
     lambda: ([], {'mixture_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ShardedDistributedFixtureSubmodule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TimeDistributed,
     lambda: ([], {'module': _mock_layer()}),
     lambda: ([], {'input': torch.rand([4, 4, 4])}),
     False),
    (_Net1,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (_Net2,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_allenai_allennlp(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])


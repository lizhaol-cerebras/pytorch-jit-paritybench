import sys
_module = sys.modules[__name__]
del sys
conf = _module
mmf = _module
common = _module
batch_collator = _module
constants = _module
dataset_loader = _module
meter = _module
registry = _module
report = _module
sample = _module
test_reporter = _module
typings = _module
datasets = _module
base_dataset = _module
base_dataset_builder = _module
builders = _module
airstore = _module
builder = _module
dataset = _module
charades = _module
_utils = _module
dataset = _module
clevr = _module
dataset = _module
coco = _module
dataset = _module
detection_builder = _module
detection_dataset = _module
masked_builder = _module
masked_dataset = _module
coco2017 = _module
conceptual_captions = _module
dataset = _module
flickr30k = _module
glue = _module
builder = _module
gqa = _module
dataset = _module
hateful_memes = _module
dataset = _module
localized_narratives = _module
database = _module
mmimdb = _module
dataset = _module
nlvr2 = _module
dataset = _module
ocrvqa = _module
okvqa = _module
dataset = _module
retrieval = _module
dataset = _module
datasets = _module
sbu_captions = _module
stvqa = _module
textcaps = _module
textvqa = _module
dataset = _module
vinvl = _module
visual_dialog = _module
database = _module
dataset = _module
visual_entailment = _module
dataset = _module
visual_genome = _module
dataset = _module
vizwiz = _module
dataset = _module
vqa2 = _module
dataset = _module
masked_q_vqa2_builder = _module
masked_q_vqa2_dataset = _module
ocr_builder = _module
ocr_dataset = _module
vqacp_v2 = _module
dataset = _module
concat_dataset = _module
databases = _module
annotation_database = _module
features_database = _module
image_database = _module
readers = _module
feature_readers = _module
scene_graph_database = _module
iteration_strategies = _module
lightning_multi_datamodule = _module
lightning_multi_dataset_loader = _module
mmf_dataset = _module
mmf_dataset_builder = _module
multi_datamodule = _module
multi_dataset_loader = _module
processors = _module
bert_processors = _module
detection_transforms = _module
frcnn_processor = _module
functional = _module
image_processors = _module
prediction_processors = _module
processors = _module
video_processors = _module
subset_dataset = _module
models = _module
albef = _module
vit = _module
alignment = _module
ban = _module
base_model = _module
butd = _module
cnn_lstm = _module
frcnn = _module
fusions = _module
interfaces = _module
mmbt = _module
krisp = _module
lorra = _module
lxmert = _module
m4c = _module
m4c_captioner = _module
mmbt = _module
mmf_bert = _module
mmf_transformer = _module
movie_mcan = _module
pythia = _module
top_down_bottom_up = _module
transformers = _module
backends = _module
huggingface = _module
base = _module
heads = _module
contrastive = _module
itm = _module
mlm = _module
mlp = _module
mrc = _module
mrfr = _module
refiner = _module
refnet_classifier = _module
utils = _module
wra = _module
unimodal = _module
unit = _module
backbone = _module
matcher = _module
misc = _module
transformer = _module
unit = _module
unit_base_model = _module
uniter = _module
vilbert = _module
vilt = _module
vinvl = _module
visdial_multi_modal = _module
visual_bert = _module
modules = _module
attention = _module
bottleneck = _module
decoders = _module
embeddings = _module
encoders = _module
fusions = _module
hf_layers = _module
layers = _module
losses = _module
metrics = _module
optimizers = _module
ot = _module
poolers = _module
schedulers = _module
vit = _module
trainers = _module
base_trainer = _module
callbacks = _module
checkpoint = _module
early_stopping = _module
logistics = _module
lr_scheduler = _module
core = _module
callback_hook = _module
device = _module
evaluation_loop = _module
profiling = _module
training_loop = _module
lightning_core = _module
loop_callback = _module
loop_callback_with_torchmetrics = _module
torchmetric = _module
lightning_trainer = _module
mmf_trainer = _module
box_ops = _module
build = _module
checkpoint = _module
checkpoint_updater = _module
configuration = _module
dataset = _module
distributed = _module
download = _module
early_stopping = _module
env = _module
features = _module
visualizing_image = _module
file_io = _module
flags = _module
general = _module
inference = _module
logger = _module
m4c_evaluators = _module
modeling = _module
patch = _module
phoc = _module
build_phoc = _module
process_answers = _module
text = _module
timer = _module
torchscript = _module
transform = _module
visualize = _module
vocab = _module
xla = _module
version = _module
mmf_cli = _module
hm_convert = _module
interactive = _module
predict = _module
run = _module
torchx_entryscript = _module
graphnetwork_module = _module
scripts = _module
extract_ocr_frcn_feature = _module
coco_eval = _module
textcaps_eval = _module
setup = _module
tests = _module
test_batch_collator = _module
test_meter = _module
test_report = _module
test_sample = _module
configs = _module
test_configs_for_keys = _module
test_zoo_urls = _module
conftest = _module
user_dir = _module
always_one = _module
simple = _module
test_base_dataset = _module
test_bert_processors = _module
test_iteration_strategies = _module
test_mmf_dataset_builder = _module
test_multi_datamodule = _module
test_multi_dataset_loader = _module
test_prediction_processors = _module
test_processors = _module
test_interfaces = _module
test_albef = _module
test_cnn_lstm = _module
test_mmbt = _module
test_mmf_transformer = _module
test_uniter = _module
test_vilbert = _module
test_vilt = _module
test_vinvl = _module
test_visual_bert = _module
test_heads = _module
test_heads_dict = _module
test_encoders = _module
test_fusions = _module
test_hf_layers = _module
test_layers = _module
test_losses = _module
test_metrics = _module
test_optimizers = _module
test_poolers = _module
test_vit = _module
test_utils = _module
test_logistics = _module
test_lr_scheduler = _module
test_user_callback = _module
lightning = _module
lightning_trainer_mock = _module
test_checkpoint = _module
test_grad_accumulate = _module
test_grad_clipping = _module
test_logging = _module
test_loop_conditions = _module
test_loss = _module
test_lr_schedule = _module
test_validation = _module
test_device = _module
test_eval_loop = _module
test_fp16 = _module
test_sharded_ddp = _module
test_trainer_mocks = _module
test_training_loop = _module
test_utils = _module
test_checkpoint = _module
test_configuration = _module
test_distributed = _module
test_download = _module
test_env = _module
test_file_io = _module
test_general = _module
test_logger = _module
test_model = _module
test_patch = _module
test_quality_checks = _module
test_text = _module
test_timer = _module
test_visualize = _module
tools = _module
extract_bert_embeddings = _module
coco_caption_eval = _module
extract_features_vinvl = _module
extract_features_vmb = _module
extract_resnet152_feat = _module
extraction_utils = _module
extract_features_frcnn = _module
frcnn_utils = _module
modeling_frcnn = _module
processing_image = _module
lmdb_conversion = _module
convert_gqa_to_vqa = _module
extract_vocabulary = _module
generate_test_data = _module
build_imdb = _module
lib = _module
slurm = _module
sweep_visual_bert = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from collections import defaultdict


from collections import deque


import torch


import collections


import collections.abc


import copy


import warnings


from collections import OrderedDict


from typing import Any


from typing import Callable


from typing import Dict


from typing import List


from typing import Optional


from typing import Union


import logging


from torch.utils.data import Dataset


from torch.utils.data.dataset import Dataset


import uuid


from typing import Iterable


import pandas as pd


from torchvision.datasets.video_utils import VideoClips


from torchvision.io import read_video


import numpy as np


import torch.nn.functional as F


import torchvision


from torch import nn


from torch import Tensor


from torchvision import transforms


from typing import Type


import random


import functools


import types


from torch.utils.data import ConcatDataset


import torchvision.datasets.folder as tv_helpers


import math


from torch.utils.data import DataLoader


import typing


from copy import deepcopy


from typing import Iterator


from torch.utils.data.dataloader import DataLoader


from torch.utils.data.dataloader import Sampler


from typing import Tuple


import torchvision.transforms as T


import torchvision.transforms.functional as F


from torchvision.transforms import Compose


from torchvision.transforms import Normalize


from torchvision.transforms import Resize


from torchvision.transforms import ToTensor


import re


from collections import Counter


from torchvision import transforms as img_transforms


from torch.utils.data.dataset import Subset


from functools import partial


from itertools import repeat


import torch.nn as nn


from torch.nn import functional as F


from torch.nn import CrossEntropyLoss


from torch.nn import SmoothL1Loss


from abc import ABC


from abc import abstractmethod


from typing import NamedTuple


from torchvision.models._utils import IntermediateLayerGetter


from torchvision.ops.misc import FrozenBatchNorm2d


from scipy.optimize import linear_sum_assignment


from collections import namedtuple


from collections.abc import MutableMapping


from torchvision.models.resnet import Bottleneck


from torchvision.models.resnet import conv1x1


from torchvision.models.resnet import conv3x3


from torch.nn.utils.weight_norm import weight_norm


from functools import lru_cache


from enum import Enum


from torch.nn.utils.rnn import pack_padded_sequence


from sklearn.metrics import average_precision_score


from sklearn.metrics import f1_score


from sklearn.metrics import precision_recall_curve


from sklearn.metrics import precision_recall_fscore_support


from sklearn.metrics import roc_auc_score


from torch.optim.lr_scheduler import LambdaLR


from torchvision.ops.boxes import box_area


from itertools import chain


from torch import distributed as dist


import matplotlib as mpl


import matplotlib.colors as mplc


import matplotlib.figure as mplfigure


from matplotlib.backends.backend_agg import FigureCanvasAgg


import time


from functools import wraps


from torchtext import vocab


import itertools


import torchvision.models as models


import torchvision.transforms as transforms


from torch.autograd import Variable


from abc import ABCMeta


from torch.nn.modules.batchnorm import BatchNorm2d


from torchvision.ops import RoIPool


from torchvision.ops.boxes import batched_nms


from torchvision.ops.boxes import nms


class PostProcess(nn.Module):

    @torch.no_grad()
    def forward(self, outputs: Dict[str, Tensor], target_sizes: Tensor):
        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']
        assert len(out_logits) == len(target_sizes)
        assert target_sizes.shape[1] == 2
        prob = F.softmax(out_logits, -1)
        scores, labels = prob[..., :-1].max(-1)
        boxes = box_cxcywh_to_xyxy(out_bbox)
        img_h, img_w = target_sizes.unbind(1)
        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)
        boxes = boxes * scale_fct[:, None, :]
        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]
        if 'attr_logits' in outputs:
            assert len(outputs['attr_logits']) == len(results)
            attr_scores, attr_labels = outputs['attr_logits'].max(-1)
            for idx, r in enumerate(results):
                r['attr_scores'] = attr_scores[idx]
                r['attr_labels'] = attr_labels[idx]
        return results


def drop_block_2d(x, drop_prob: float=0.1, block_size: int=7, gamma_scale: float=1.0, with_noise: bool=False, inplace: bool=False, batchwise: bool=False):
    """DropBlock. See https://arxiv.org/pdf/1810.12890.pdf
    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training
    runs with success, but needs further validation and possibly optimization for lower runtime impact.
    """
    B, C, H, W = x.shape
    total_size = W * H
    clipped_block_size = min(block_size, min(W, H))
    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / ((W - block_size + 1) * (H - block_size + 1))
    w_i, h_i = torch.meshgrid(torch.arange(W), torch.arange(H))
    valid_block = (w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2) & ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))
    valid_block = torch.reshape(valid_block, (1, 1, H, W))
    if batchwise:
        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)
    else:
        uniform_noise = torch.rand_like(x)
    block_mask = 2 - gamma - valid_block + uniform_noise >= 1
    block_mask = -F.max_pool2d(-block_mask, kernel_size=clipped_block_size, stride=1, padding=clipped_block_size // 2)
    if with_noise:
        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)
        if inplace:
            x.mul_(block_mask).add_(normal_noise * (1 - block_mask))
        else:
            x = x * block_mask + normal_noise * (1 - block_mask)
    else:
        normalize_scale = block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-07)
        if inplace:
            x.mul_(block_mask * normalize_scale)
        else:
            x = x * block_mask * normalize_scale
    return x


def drop_block_fast_2d(x: torch.Tensor, drop_prob: float=0.1, block_size: int=7, gamma_scale: float=1.0, with_noise: bool=False, inplace: bool=False, batchwise: bool=False):
    """DropBlock. See https://arxiv.org/pdf/1810.12890.pdf
    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid
    block mask at edges.
    """
    B, C, H, W = x.shape
    total_size = W * H
    clipped_block_size = min(block_size, min(W, H))
    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / ((W - block_size + 1) * (H - block_size + 1))
    if batchwise:
        block_mask = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device) < gamma
    else:
        block_mask = torch.rand_like(x) < gamma
    block_mask = F.max_pool2d(block_mask, kernel_size=clipped_block_size, stride=1, padding=clipped_block_size // 2)
    if with_noise:
        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)
        if inplace:
            x.mul_(1.0 - block_mask).add_(normal_noise * block_mask)
        else:
            x = x * (1.0 - block_mask) + normal_noise * block_mask
    else:
        block_mask = 1 - block_mask
        normalize_scale = block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-07)
        if inplace:
            x.mul_(block_mask * normalize_scale)
        else:
            x = x * block_mask * normalize_scale
    return x


class DropBlock2d(nn.Module):
    """DropBlock. See https://arxiv.org/pdf/1810.12890.pdf"""

    def __init__(self, drop_prob=0.1, block_size=7, gamma_scale=1.0, with_noise=False, inplace=False, batchwise=False, fast=True):
        super().__init__()
        self.drop_prob = drop_prob
        self.gamma_scale = gamma_scale
        self.block_size = block_size
        self.with_noise = with_noise
        self.inplace = inplace
        self.batchwise = batchwise
        self.fast = fast

    def forward(self, x):
        if not self.training or not self.drop_prob:
            return x
        if self.fast:
            return drop_block_fast_2d(x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)
        else:
            return drop_block_2d(x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)


def drop_path(x, drop_prob: float=0.0, training: bool=False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


def _ntuple(n):

    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return x
        return tuple(repeat(x, n))
    return parse


to_2tuple = _ntuple(2)


class PatchEmbed(nn.Module):
    """2D Image to Patch Embedding"""

    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = img_size[0] // patch_size[0], img_size[1] // patch_size[1]
        self.num_patches = self.grid_size[0] * self.grid_size[1]
        self.flatten = flatten
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def forward(self, x):
        B, C, H, W = x.shape
        assert H == self.img_size[0] and W == self.img_size[1], f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)
        x = self.norm(x)
        return x


class Mlp(nn.Module):
    """MLP as used in Vision Transformer, MLP-Mixer and related networks"""

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    """Attention Layer as used in Vision Transformer."""

    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.attn_gradients = None
        self.attention_map = None

    def save_attn_gradients(self, attn_gradients):
        self.attn_gradients = attn_gradients

    def get_attn_gradients(self):
        return self.attn_gradients

    def save_attention_map(self, attention_map):
        self.attention_map = attention_map

    def get_attention_map(self):
        return self.attention_map

    def forward(self, x, register_hook=False):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        attn = q @ k.transpose(-2, -1) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        if register_hook:
            self.save_attention_map(attn)
            attn.register_hook(self.save_attn_gradients)
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


def get_chunks(x, sizes):
    out = []
    begin = 0
    for s in sizes:
        y = x.narrow(1, begin, s)
        out.append(y)
        begin += s
    return out


def get_sizes_list(dim, chunks):
    split_size = (dim + chunks - 1) // chunks
    sizes_list = [split_size] * chunks
    sizes_list[-1] = sizes_list[-1] - (sum(sizes_list) - dim)
    assert sum(sizes_list) == dim
    if sizes_list[-1] < 0:
        n_miss = sizes_list[-2] - sizes_list[-1]
        sizes_list[-1] = sizes_list[-2]
        for j in range(n_miss):
            sizes_list[-j - 1] -= 1
        assert sum(sizes_list) == dim
        assert min(sizes_list) > 0
    return sizes_list


def log_class_usage(component_type, klass):
    """This function is used to log the usage of different MMF components."""
    identifier = 'MMF'
    if klass and hasattr(klass, '__name__'):
        identifier += f'.{component_type}.{klass.__name__}'
    torch._C._log_api_usage_once(identifier)


class Registry:
    """Class for registry object which acts as central source of truth
    for MMF
    """
    mapping = {'builder_name_mapping': {}, 'trainer_name_mapping': {}, 'model_name_mapping': {}, 'metric_name_mapping': {}, 'torchmetric_name_mapping': {}, 'loss_name_mapping': {}, 'pool_name_mapping': {}, 'fusion_name_mapping': {}, 'optimizer_name_mapping': {}, 'scheduler_name_mapping': {}, 'processor_name_mapping': {}, 'encoder_name_mapping': {}, 'decoder_name_mapping': {}, 'transformer_backend_name_mapping': {}, 'transformer_head_name_mapping': {}, 'test_reporter_mapping': {}, 'iteration_strategy_name_mapping': {}, 'state': {}, 'callback_name_mapping': {}}

    @classmethod
    def register_trainer(cls, name):
        """Register a trainer to registry with key 'name'

        Args:
            name: Key with which the trainer will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.trainers.custom_trainer import CustomTrainer


            @registry.register_trainer("custom_trainer")
            class CustomTrainer():
                ...

        """

        def wrap(trainer_cls):
            cls.mapping['trainer_name_mapping'][name] = trainer_cls
            return trainer_cls
        return wrap

    @classmethod
    def register_builder(cls, name):
        """Register a dataset builder to registry with key 'name'

        Args:
            name: Key with which the metric will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.datasets.base_dataset_builder import BaseDatasetBuilder


            @registry.register_builder("vqa2")
            class VQA2Builder(BaseDatasetBuilder):
                ...

        """

        def wrap(builder_cls):
            assert issubclass(builder_cls, BaseDatasetBuilder), 'All builders must inherit BaseDatasetBuilder class'
            cls.mapping['builder_name_mapping'][name] = builder_cls
            return builder_cls
        return wrap

    @classmethod
    def register_callback(cls, name):
        """Register a callback to registry with key 'name'

        Args:
            name: Key with which the callback will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.trainers.callbacks.base import Callback


            @registry.register_callback("logistic")
            class LogisticCallback(Callback):
                ...

        """

        def wrap(func):
            assert issubclass(func, Callback), 'All callbacks must inherit Callback class'
            cls.mapping['callback_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_metric(cls, name):
        """Register a metric to registry with key 'name'

        Args:
            name: Key with which the metric will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.modules.metrics import BaseMetric


            @registry.register_metric("r@1")
            class RecallAt1(BaseMetric):
                ...

        """

        def wrap(func):
            assert issubclass(func, BaseMetric), 'All Metric must inherit BaseMetric class'
            cls.mapping['metric_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_torchmetric(cls, name):
        """Register a torchmetric to registry with key 'name'

        Args:
            name: Key with which the torchmetric will be registered.

        Usage::

            from mmf.common.registry import registry
            from torchmetrics.metric import Metric

            @registry.register_torchmetric("topk_accuracy")
            class TopKAccuracy(Metric):
                ...
        """

        def wrap(func):
            assert issubclass(func, Metric), 'All metric must inherit Metric class'
            cls.mapping['torchmetric_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_loss(cls, name):
        """Register a loss to registry with key 'name'

        Args:
            name: Key with which the loss will be registered.

        Usage::

            from mmf.common.registry import registry
            from torch import nn

            @registry.register_task("logit_bce")
            class LogitBCE(nn.Module):
                ...

        """

        def wrap(func):
            from torch import nn
            assert issubclass(func, nn.Module), 'All loss must inherit torch.nn.Module class'
            cls.mapping['loss_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_pooler(cls, name):
        """Register a modality pooling method to registry with key 'name'

        Args:
            name: Key with which the pooling method will be registered.

        Usage::

            from mmf.common.registry import registry
            from torch import nn

            @registry.register_pool("average_pool")
            class average_pool(nn.Module):
                ...

        """

        def wrap(func):
            from torch import nn
            assert issubclass(func, nn.Module), 'All pooling methods must inherit torch.nn.Module class'
            cls.mapping['pool_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_fusion(cls, name):
        """Register a fusion technique to registry with key 'name'

        Args:
            name: Key with which the fusion technique will be registered

        Usage::

            from mmf.common.registry import registry
            from torch import nn

            @registry.register_fusion("linear_sum")
            class LinearSum():
                ...
        """

        def wrap(func):
            from torch import nn
            assert issubclass(func, nn.Module), 'All Fusion must inherit torch.nn.Module class'
            cls.mapping['fusion_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_model(cls, name):
        """Register a model to registry with key 'name'

        Args:
            name: Key with which the model will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.models.base_model import BaseModel

            @registry.register_task("pythia")
            class Pythia(BaseModel):
                ...
        """

        def wrap(func):
            assert issubclass(func, BaseModel), 'All models must inherit BaseModel class'
            cls.mapping['model_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_processor(cls, name):
        """Register a processor to registry with key 'name'

        Args:
            name: Key with which the processor will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.datasets.processors import BaseProcessor

            @registry.register_task("glove")
            class GloVe(BaseProcessor):
                ...

        """

        def wrap(func):
            assert issubclass(func, BaseProcessor), 'All Processor classes must inherit BaseProcessor class'
            cls.mapping['processor_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_optimizer(cls, name):

        def wrap(func):
            cls.mapping['optimizer_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_scheduler(cls, name):

        def wrap(func):
            cls.mapping['scheduler_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_transformer_backend(cls, name):

        def wrap(func):
            cls.mapping['transformer_backend_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_transformer_head(cls, name):

        def wrap(func):
            cls.mapping['transformer_head_name_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_test_reporter(cls, name):

        def wrap(func):
            cls.mapping['test_reporter_mapping'][name] = func
            return func
        return wrap

    @classmethod
    def register_decoder(cls, name):
        """Register a decoder to registry with key 'name'

        Args:
            name: Key with which the decoder will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.utils.text import TextDecoder


            @registry.register_decoder("nucleus_sampling")
            class NucleusSampling(TextDecoder):
                ...

        """

        def wrap(decoder_cls):
            assert issubclass(decoder_cls, TextDecoder), 'All decoders must inherit TextDecoder class'
            cls.mapping['decoder_name_mapping'][name] = decoder_cls
            return decoder_cls
        return wrap

    @classmethod
    def register_encoder(cls, name):
        """Register a encoder to registry with key 'name'

        Args:
            name: Key with which the encoder will be registered.

        Usage::

            from mmf.common.registry import registry
            from mmf.modules.encoders import Encoder


            @registry.register_encoder("transformer")
            class TransformerEncoder(Encoder):
                ...

        """

        def wrap(encoder_cls):
            assert issubclass(encoder_cls, Encoder), 'All encoders must inherit Encoder class'
            cls.mapping['encoder_name_mapping'][name] = encoder_cls
            return encoder_cls
        return wrap

    @classmethod
    def register_datamodule(cls, name):
        """Register a datamodule to registry with key 'name'

        Args:
            name: Key with which the datamodule will be registered.

        Usage::

            from mmf.common.registry import registry
            import pytorch_lightning as pl


            @registry.register_datamodule("my_datamodule")
            class MyDataModule(pl.LightningDataModule):
                ...

        """

        def wrap(datamodule_cls):
            assert issubclass(datamodule_cls, pl.LightningDataModule), 'All datamodules must inherit PyTorch Lightning DataModule class'
            cls.mapping['builder_name_mapping'][name] = datamodule_cls
            return datamodule_cls
        return wrap

    @classmethod
    def register_iteration_strategy(cls, name):
        """Register an iteration_strategy to registry with key 'name'

        Args:
            name: Key with which the iteration_strategy will be registered.

        Usage::

            from dataclasses import dataclass
            from mmf.common.registry import registry
            from mmf.datasets.iterators import IterationStrategy


            @registry.register_iteration_strategy("my_iteration_strategy")
            class MyStrategy(IterationStrategy):
                @dataclass
                class Config:
                    name: str = "my_strategy"
                def __init__(self, config, dataloader):
                    ...
        """

        def wrap(iteration_strategy_cls):
            assert issubclass(iteration_strategy_cls, IterationStrategy), 'All datamodules must inherit IterationStrategy class'
            cls.mapping['iteration_strategy_name_mapping'][name] = iteration_strategy_cls
            return iteration_strategy_cls
        return wrap

    @classmethod
    def register(cls, name, obj):
        """Register an item to registry with key 'name'

        Args:
            name: Key with which the item will be registered.

        Usage::

            from mmf.common.registry import registry

            registry.register("config", {})
        """
        path = name.split('.')
        current = cls.mapping['state']
        for part in path[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]
        current[path[-1]] = obj

    @classmethod
    def get_trainer_class(cls, name):
        return cls.mapping['trainer_name_mapping'].get(name, None)

    @classmethod
    def get_builder_class(cls, name):
        return cls.mapping['builder_name_mapping'].get(name, None)

    @classmethod
    def get_callback_class(cls, name):
        return cls.mapping['callback_name_mapping'].get(name, None)

    @classmethod
    def get_model_class(cls, name):
        return cls.mapping['model_name_mapping'].get(name, None)

    @classmethod
    def get_processor_class(cls, name):
        return cls.mapping['processor_name_mapping'].get(name, None)

    @classmethod
    def get_metric_class(cls, name):
        return cls.mapping['metric_name_mapping'].get(name, None)

    @classmethod
    def get_torchmetric_class(cls, name):
        return cls.mapping['torchmetric_name_mapping'].get(name, None)

    @classmethod
    def get_loss_class(cls, name):
        return cls.mapping['loss_name_mapping'].get(name, None)

    @classmethod
    def get_pool_class(cls, name):
        return cls.mapping['pool_name_mapping'].get(name, None)

    @classmethod
    def get_optimizer_class(cls, name):
        return cls.mapping['optimizer_name_mapping'].get(name, None)

    @classmethod
    def get_scheduler_class(cls, name):
        return cls.mapping['scheduler_name_mapping'].get(name, None)

    @classmethod
    def get_decoder_class(cls, name):
        return cls.mapping['decoder_name_mapping'].get(name, None)

    @classmethod
    def get_encoder_class(cls, name):
        return cls.mapping['encoder_name_mapping'].get(name, None)

    @classmethod
    def get_iteration_strategy_class(cls, name):
        return cls.mapping['iteration_strategy_name_mapping'].get(name, None)

    @classmethod
    def get_transformer_backend_class(cls, name):
        return cls.mapping['transformer_backend_name_mapping'].get(name, None)

    @classmethod
    def get_transformer_head_class(cls, name):
        return cls.mapping['transformer_head_name_mapping'].get(name, None)

    @classmethod
    def get_test_rerporter_class(cls, name):
        return cls.mapping['test_reporter_mapping'].get(name, None)

    @classmethod
    def get(cls, name, default=None, no_warning=False):
        """Get an item from registry with key 'name'

        Args:
            name (string): Key whose value needs to be retrieved.
            default: If passed and key is not in registry, default value will
                     be returned with a warning. Default: None
            no_warning (bool): If passed as True, warning when key doesn't exist
                               will not be generated. Useful for MMF's
                               internal operations. Default: False
        Usage::

            from mmf.common.registry import registry

            config = registry.get("config")
        """
        original_name = name
        name = name.split('.')
        value = cls.mapping['state']
        for subname in name:
            value = value.get(subname, default)
            if value is default:
                break
        if 'writer' in cls.mapping['state'] and value == default and no_warning is False:
            cls.mapping['state']['writer'].warning('Key {} is not present in registry, returning default value of {}'.format(original_name, default))
        return value

    @classmethod
    def unregister(cls, name):
        """Remove an item from registry with key 'name'

        Args:
            name: Key which needs to be removed.
        Usage::

            from mmf.common.registry import registry

            config = registry.unregister("config")
        """
        return cls.mapping['state'].pop(name, None)


registry = Registry()


class Block(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1600, chunks=20, rank=15, shared=False, dropout_input=0.0, dropout_pre_lin=0.0, dropout_output=0.0, pos_norm='before_cat'):
        super().__init__()
        self.input_dims = input_dims
        self.output_dim = output_dim
        self.mm_dim = mm_dim
        self.chunks = chunks
        self.rank = rank
        self.shared = shared
        self.dropout_input = dropout_input
        self.dropout_pre_lin = dropout_pre_lin
        self.dropout_output = dropout_output
        assert pos_norm in ['before_cat', 'after_cat']
        self.pos_norm = pos_norm
        self.linear0 = nn.Linear(input_dims[0], mm_dim)
        if shared:
            self.linear1 = self.linear0
        else:
            self.linear1 = nn.Linear(input_dims[1], mm_dim)
        merge_linears0, merge_linears1 = [], []
        self.sizes_list = get_sizes_list(mm_dim, chunks)
        for size in self.sizes_list:
            ml0 = nn.Linear(size, size * rank)
            merge_linears0.append(ml0)
            if self.shared:
                ml1 = ml0
            else:
                ml1 = nn.Linear(size, size * rank)
            merge_linears1.append(ml1)
        self.merge_linears0 = nn.ModuleList(merge_linears0)
        self.merge_linears1 = nn.ModuleList(merge_linears1)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        bsize = x1.size(0)
        if self.dropout_input > 0:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        x0_chunks = get_chunks(x0, self.sizes_list)
        x1_chunks = get_chunks(x1, self.sizes_list)
        zs = []
        for chunk_id, m0, m1 in zip(range(len(self.sizes_list)), self.merge_linears0, self.merge_linears1):
            x0_c = x0_chunks[chunk_id]
            x1_c = x1_chunks[chunk_id]
            m = m0(x0_c) * m1(x1_c)
            m = m.view(bsize, self.rank, -1)
            z = torch.sum(m, 1)
            if self.pos_norm == 'before_cat':
                z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
                z = F.normalize(z, p=2)
            zs.append(z)
        z = torch.cat(zs, 1)
        if self.pos_norm == 'after_cat':
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        if self.dropout_pre_lin > 0:
            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)
        z = self.linear_out(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


def _no_grad_trunc_normal_(tensor, mean, std, a, b):

    def norm_cdf(x):
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0
    if mean < a - 2 * std or mean > b + 2 * std:
        warnings.warn('mean is more than 2 std from [a, b] in nn.init.trunc_normal_. The distribution of values may be incorrect.', stacklevel=2)
    with torch.no_grad():
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)
        tensor.uniform_(2 * l - 1, 2 * u - 1)
        tensor.erfinv_()
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    """
    # type: (Tensor, float, float, float, float) -> Tensor

    Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \\leq \\text{mean} \\leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class NestedTensor:
    """
    A data class to hold images of different sizes in a batch.

    It contains `tensors` to hold padded images to the maximum size and `mask` to
    indicate the actual image region of each padded image
    """

    def __init__(self, tensors: Tensor, mask: Tensor):
        self.tensors = tensors
        self.mask = mask

    def to(self, *args, **kwargs):
        cast_tensor = self.tensors
        cast_mask = self.mask if self.mask is not None else None
        return type(self)(cast_tensor, cast_mask)

    def decompose(self):
        return self.tensors, self.mask

    @classmethod
    def from_tensor_list(cls, tensor_list: List[Tensor]):
        """
        convert a list of images in CHW format in `tensor_list` to a NestedTensor by
        padding them to maximum image size.
        """
        if tensor_list[0].ndim == 3:
            max_size = tuple(max(s) for s in zip(*[img.shape for img in tensor_list]))
            batch_shape = (len(tensor_list),) + max_size
            b, c, h, w = batch_shape
            dtype = tensor_list[0].dtype
            device = tensor_list[0].device
            tensor = torch.zeros(batch_shape, dtype=dtype, device=device)
            mask = torch.ones((b, h, w), dtype=torch.bool, device=device)
            for img, pad_img, m in zip(tensor_list, tensor, mask):
                pad_img[:img.shape[0], :img.shape[1], :img.shape[2]].copy_(img)
                m[:img.shape[1], :img.shape[2]] = False
        else:
            raise Exception('tensor_list must contain images in CHW format')
        return cls(tensor, mask)


class PositionEmbeddingSine(nn.Module):
    """
    This is a more standard version of the position embedding, very similar to the one
    used by the Attention is all you need paper, generalized to work on images.
    """

    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):
        super().__init__()
        self.num_pos_feats = num_pos_feats
        self.temperature = temperature
        self.normalize = normalize
        if scale is not None and normalize is False:
            raise ValueError('normalize should be True if scale is passed')
        if scale is None:
            scale = 2 * math.pi
        self.scale = scale

    def forward(self, tensor_list: NestedTensor):
        x = tensor_list.tensors
        mask = tensor_list.mask
        not_mask = ~mask
        y_embed = not_mask.cumsum(1, dtype=torch.float32)
        x_embed = not_mask.cumsum(2, dtype=torch.float32)
        if self.normalize:
            eps = 1e-06
            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale
        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)
        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
        pos_x = x_embed[:, :, :, None] / dim_t
        pos_y = y_embed[:, :, :, None] / dim_t
        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)
        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
        return pos


class _NewEmptyTensorOp(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, new_shape):
        ctx.shape = x.shape
        return x.new_empty(new_shape)

    @staticmethod
    def backward(ctx, grad):
        shape = ctx.shape
        return _NewEmptyTensorOp.apply(grad, shape), None


class Conv2d(torch.nn.Conv2d):

    def __init__(self, *args, **kwargs):
        norm = kwargs.pop('norm', None)
        activation = kwargs.pop('activation', None)
        super().__init__(*args, **kwargs)
        self.norm = norm
        self.activation = activation

    def forward(self, x):
        if x.numel() == 0 and self.training:
            assert not isinstance(self.norm, torch.nn.SyncBatchNorm)
        if x.numel() == 0:
            assert not isinstance(self.norm, torch.nn.GroupNorm)
            output_shape = [((i + 2 * p - (di * (k - 1) + 1)) // s + 1) for i, p, di, k, s in zip(x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride)]
            output_shape = [x.shape[0], self.weight.shape[0]] + output_shape
            empty = _NewEmptyTensorOp.apply(x, output_shape)
            if self.training:
                _dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0
                return empty + _dummy
            else:
                return empty
        x = super().forward(x)
        if self.norm is not None:
            x = self.norm(x)
        if self.activation is not None:
            x = self.activation(x)
        return x


def get_norm(norm, out_channels):
    if isinstance(norm, str):
        if len(norm) == 0:
            return None
        norm = {'BN': BatchNorm2d, 'GN': lambda channels: nn.GroupNorm(32, channels), 'nnSyncBN': nn.SyncBatchNorm, '': lambda x: x}[norm]
    return norm(out_channels)


class BasicStem(nn.Module):

    def __init__(self, in_channels=3, out_channels=64, norm='BN', caffe_maxpool=False):
        super().__init__()
        self.conv1 = Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False, norm=get_norm(norm, out_channels))
        self.caffe_maxpool = caffe_maxpool

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu_(x)
        if self.caffe_maxpool:
            x = F.max_pool2d(x, kernel_size=3, stride=2, padding=0, ceil_mode=True)
        else:
            x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)
        return x

    @property
    def out_channels(self):
        return self.conv1.out_channels

    @property
    def stride(self):
        return 4


CONFIG_NAME = 'config.yaml'


def _decode_value(value):
    if not isinstance(value, str):
        return value
    if value == 'None':
        value = None
    try:
        value = literal_eval(value)
    except ValueError:
        pass
    except SyntaxError:
        pass
    return value


logger = logging.getLogger(__name__)


def get_default_config_path():
    directory = os.path.dirname(os.path.abspath(__file__))
    configs_dir = os.path.join(directory, '..', 'configs')
    fb_defaults = os.path.join(configs_dir, 'fb_defaults.yaml')
    if PathManager.exists(fb_defaults):
        return fb_defaults
    else:
        return os.path.join(configs_dir, 'defaults.yaml')


def import_user_module(user_dir: str):
    """Given a user dir, this function imports it as a module.

    This user_module is expected to have an __init__.py at its root.
    You can use import_files to import your python files easily in
    __init__.py

    Args:
        user_dir (str): directory which has to be imported
    """
    logger = logging.getLogger(__name__)
    if user_dir:
        if registry.get('__mmf_user_dir_imported__', no_warning=True):
            logger.info(f'User dir {user_dir} already imported. Skipping.')
            return
        if user_dir.endswith('.py'):
            user_dir = user_dir[:-3]
        dot_path = '.'.join(user_dir.split(os.path.sep))
        if os.path.isabs(user_dir):
            dot_path = dot_path[1:]
        try:
            dot_spec = importlib.util.find_spec(dot_path)
        except ModuleNotFoundError:
            dot_spec = None
        abs_user_dir = get_absolute_path(user_dir)
        module_parent, module_name = os.path.split(abs_user_dir)
        if dot_path in sys.modules or dot_spec is not None:
            module_name = dot_path
        else:
            user_dir = abs_user_dir
        logger.info(f'Importing from {user_dir}')
        if module_name != dot_path:
            sys.path.insert(0, module_parent)
        importlib.import_module(module_name)
        sys.modules['mmf_user_dir'] = sys.modules[module_name]
        config = registry.get('config')
        if config is None:
            registry.register('config', OmegaConf.create({'env': {'user_dir': user_dir}}))
        else:
            with open_dict(config):
                config.env.user_dir = user_dir
        registry.register('__mmf_user_dir_imported__', True)


def get_mmf_root():
    mmf_root = registry.get('mmf_root', no_warning=True)
    if mmf_root is None:
        mmf_root = os.path.dirname(os.path.abspath(__file__))
        mmf_root = os.path.abspath(os.path.join(mmf_root, '..'))
        registry.register('mmf_root', mmf_root)
    return mmf_root


def get_absolute_path(paths):
    if isinstance(paths, str):
        if os.path.isabs(paths):
            return paths
        possible_paths = [paths]
        mmf_root = get_mmf_root()
        user_dir = get_mmf_env(key='user_dir')
        if user_dir:
            possible_paths.append(os.path.join(user_dir, paths))
            possible_paths.append(os.path.join(mmf_root, '..', user_dir, paths))
        possible_paths.append(os.path.join(mmf_root, '..', paths))
        possible_paths.append(os.path.join(mmf_root, paths))
        for path in possible_paths:
            if PathManager.exists(path):
                if path.find('://') == -1:
                    return os.path.abspath(path)
                else:
                    return path
        return paths
    elif isinstance(paths, collections.abc.Iterable):
        return [get_absolute_path(path) for path in paths]
    else:
        raise TypeError('Paths passed to dataset should either be string or list')


def load_yaml(f):
    abs_f = get_absolute_path(f)
    try:
        mapping = OmegaConf.load(PathManager.get_local_path(abs_f))
        f = abs_f
    except FileNotFoundError as e:
        relative = os.path.abspath(os.path.join(get_mmf_root(), f))
        if not PathManager.isfile(relative):
            raise e
        else:
            f = relative
            mapping = OmegaConf.load(PathManager.get_local_path(f))
    if mapping is None:
        mapping = OmegaConf.create()
    includes = mapping.get('includes', [])
    if not isinstance(includes, collections.abc.Sequence):
        raise AttributeError('Includes must be a list, {} provided'.format(type(includes)))
    include_mapping = OmegaConf.create()
    mmf_root_dir = get_mmf_root()
    for include in includes:
        original_include_path = include
        include = os.path.join(mmf_root_dir, include)
        if not PathManager.exists(include):
            include = os.path.join(os.path.dirname(f), original_include_path)
        current_include_mapping = load_yaml(include)
        include_mapping = OmegaConf.merge(include_mapping, current_include_mapping)
    mapping.pop('includes', None)
    mapping = OmegaConf.merge(include_mapping, mapping)
    return mapping


def resolve_cache_dir(env_variable='MMF_CACHE_DIR', default='mmf'):
    try:
        from torch.hub import _get_torch_home
        torch_cache_home = _get_torch_home()
    except ImportError:
        torch_cache_home = os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))
    default_cache_path = os.path.join(torch_cache_home, default)
    cache_path = os.getenv(env_variable, default_cache_path)
    if not PathManager.exists(cache_path):
        try:
            PathManager.mkdirs(cache_path)
        except PermissionError:
            cache_path = os.path.join(get_mmf_root(), '.mmf_cache')
            PathManager.mkdirs(cache_path)
    return cache_path


def resolve_dir(env_variable, default='data'):
    default_dir = os.path.join(resolve_cache_dir(), default)
    dir_path = os.getenv(env_variable, default_dir)
    if not PathManager.exists(dir_path):
        PathManager.mkdirs(dir_path)
    return dir_path


def get_global_config(key=None):
    config = registry.get('config')
    if config is None:
        configuration = Configuration()
        config = configuration.get_config()
        registry.register('config', config)
    if key:
        config = OmegaConf.select(config, key)
    return config


def get_mmf_cache_dir():
    config = get_global_config()
    cache_dir = config.env.cache_dir
    if not os.path.exists(cache_dir):
        cache_dir = os.path.join(get_mmf_root(), cache_dir)
    return cache_dir


def check_header(url, from_google=False):
    """
    Performs a HEAD request to check if the URL / Google Drive ID is live.
    """
    session = requests.Session()
    if from_google:
        URL = 'https://docs.google.com/uc?export=download'
        response = session.head(URL, params={'id': url}, stream=True)
    else:
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) ' + 'AppleWebKit/537.36 (KHTML, like Gecko) ' + 'Chrome/77.0.3865.90 Safari/537.36'}
        response = session.head(url, allow_redirects=True, headers=headers)
    status = response.status_code
    session.close()
    assert status == 200, ('The url {} is broken. If this is not your own url,' + ' please open up an issue on GitHub').format(url)


def move(path1, path2):
    """
    Rename the given file.
    """
    shutil.move(path1, path2)


def download(url, path, fname, redownload=True, disable_tqdm=False):
    """
    Download file using `requests`.

    If ``redownload`` is set to false, then will not download tar file again if it is
    present (default ``True``).

    Returns whether download actually happened or not
    """
    outfile = os.path.join(path, fname)
    download = not PathManager.isfile(outfile) or redownload
    retry = 5
    exp_backoff = [(2 ** r) for r in reversed(range(retry))]
    pbar = None
    if download:
        check_header(url)
        if not disable_tqdm:
            None
        pbar = tqdm.tqdm(unit='B', unit_scale=True, desc=f'Downloading {fname}', disable=disable_tqdm)
    while download and retry >= 0:
        resume_file = outfile + '.part'
        resume = PathManager.isfile(resume_file)
        if resume:
            resume_pos = os.path.getsize(resume_file)
            mode = 'ab'
        else:
            resume_pos = 0
            mode = 'wb'
        response = None
        with requests.Session() as session:
            try:
                header = {'Range': 'bytes=%d-' % resume_pos, 'Accept-Encoding': 'identity'} if resume else {}
                response = session.get(url, stream=True, timeout=5, headers=header)
                if resume and response.headers.get('Accept-Ranges', 'none') == 'none':
                    resume_pos = 0
                    mode = 'wb'
                CHUNK_SIZE = 32768
                total_size = int(response.headers.get('Content-Length', -1))
                total_size += resume_pos
                pbar.total = total_size
                done = resume_pos
                with PathManager.open(resume_file, mode) as f:
                    for chunk in response.iter_content(CHUNK_SIZE):
                        if chunk:
                            f.write(chunk)
                        if total_size > 0:
                            done += len(chunk)
                            if total_size < done:
                                total_size = done
                                pbar.total = total_size
                            pbar.update(len(chunk))
                    break
            except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
                retry -= 1
                pbar.clear()
                if retry >= 0:
                    None
                    time.sleep(exp_backoff[retry])
                else:
                    None
            finally:
                if response:
                    response.close()
    if retry < 0:
        raise RuntimeWarning('Connection broken too many times. Stopped retrying.')
    if download and retry > 0:
        pbar.update(done - pbar.n)
        if done < total_size:
            raise RuntimeWarning('Received less data than specified in ' + 'Content-Length header for ' + url + '. There may be a download problem.')
        move(resume_file, outfile)
    if pbar:
        pbar.close()
    return download


def is_remote_url(url_or_filename):
    parsed = urlparse(url_or_filename)
    return parsed.scheme in ('http', 'https')


def url_to_filename(url, etag=None):
    url_bytes = url.encode('utf-8')
    url_hash = sha256(url_bytes)
    filename = url_hash.hexdigest()
    if etag:
        etag_bytes = etag.encode('utf-8')
        etag_hash = sha256(etag_bytes)
        filename += '.' + etag_hash.hexdigest()
    if url.endswith('.h5'):
        filename += '.h5'
    elif url.endswith('.yaml'):
        filename += '.yaml'
    elif url.endswith('.bin'):
        filename += '.bin'
    return filename


def cached_path(url_or_filename, cache_dir=None, force_download=False, proxies=None, resume_download=False, user_agent=None, extract_compressed_file=False, force_extract=False, local_files_only=False):
    if cache_dir is None:
        cache_dir = TRANSFORMERS_CACHE
    if isinstance(url_or_filename, Path):
        url_or_filename = str(url_or_filename)
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    if is_remote_url(url_or_filename):
        filename = url_to_filename(url_or_filename)
        output_path = os.path.join(cache_dir, filename)
        if not os.path.isfile(output_path):
            assert download(url_or_filename, cache_dir, filename), f'failed to download {url_or_filename}'
    elif os.path.exists(url_or_filename):
        output_path = url_or_filename
    elif urlparse(url_or_filename).scheme == '':
        raise OSError(f'file {url_or_filename} not found')
    else:
        raise ValueError(f'unable to parse {url_or_filename} as a URL or as a local path')
    if extract_compressed_file:
        if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):
            return output_path
        output_dir, output_file = os.path.split(output_path)
        output_extract_dir_name = output_file.replace('.', '-') + '-extracted'
        output_path_extracted = os.path.join(output_dir, output_extract_dir_name)
        if os.path.isdir(output_path_extracted) and os.listdir(output_path_extracted) and not force_extract:
            return output_path_extracted
        lock_path = output_path + '.lock'
        with FileLock(lock_path):
            shutil.rmtree(output_path_extracted, ignore_errors=True)
            os.makedirs(output_path_extracted)
            if is_zipfile(output_path):
                with ZipFile(output_path, 'r') as zip_file:
                    zip_file.extractall(output_path_extracted)
                    zip_file.close()
            elif tarfile.is_tarfile(output_path):
                tar_file = tarfile.open(output_path)
                tar_file.extractall(output_path_extracted)
                tar_file.close()
            else:
                raise OSError(f'Archive format of {output_path} could not be identified')
        return output_path_extracted
    return output_path


class Config:
    _pointer = {}

    def __init__(self, dictionary: dict, name: str='root', level=0):
        self._name = name
        self._level = level
        d = {}
        for k, v in dictionary.items():
            if v is None:
                raise ValueError()
            k = copy.deepcopy(k)
            v = copy.deepcopy(v)
            if isinstance(v, dict):
                v = Config(v, name=k, level=level + 1)
            d[k] = v
            setattr(self, k, v)
        self._pointer = d

    def __repr__(self):
        return str(list(self._pointer.keys()))

    def __setattr__(self, key, val):
        self.__dict__[key] = val
        self.__dict__[key.upper()] = val
        levels = key.split('.')
        last_level = len(levels) - 1
        pointer = self._pointer
        if len(levels) > 1:
            for i, level in enumerate(levels):
                if hasattr(self, level) and isinstance(getattr(self, level), Config):
                    setattr(getattr(self, level), '.'.join(levels[i:]), val)
                if level == last_level:
                    pointer[level] = val
                else:
                    pointer = pointer[level]

    def to_dict(self):
        return self._pointer

    def dump_yaml(self, data, file_name):
        with open(f'{file_name}', 'w') as stream:
            OmegaConf.save(data, stream)

    def dump_json(self, data, file_name):
        with open(f'{file_name}', 'w') as stream:
            json.dump(data, stream)

    @staticmethod
    def load_yaml(config):
        return dict(OmegaConf.load(config))

    def __str__(self):
        t = '    '
        if self._name != 'root':
            r = f'{t * (self._level - 1)}{self._name}:\n'
        else:
            r = ''
        level = self._level
        for i, (k, v) in enumerate(self._pointer.items()):
            if isinstance(v, Config):
                r += f'{t * self._level}{v}\n'
                self._level += 1
            else:
                r += f'{t * self._level}{k}: {v} ({type(v).__name__})\n'
            self._level = level
        return r[:-1]

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs):
        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
        return cls(config_dict)

    @classmethod
    def get_config_dict(cls, pretrained_model_name_or_path: str, **kwargs):
        cache_dir = kwargs.pop('cache_dir', None)
        force_download = kwargs.pop('force_download', False)
        resume_download = kwargs.pop('resume_download', False)
        proxies = kwargs.pop('proxies', None)
        local_files_only = kwargs.pop('local_files_only', False)
        if os.path.isdir(pretrained_model_name_or_path):
            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)
        else:
            config_file = pretrained_model_name_or_path
        try:
            resolved_config_file = cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only)
            if resolved_config_file is None:
                raise OSError
            config_file = Config.load_yaml(resolved_config_file)
        except OSError:
            msg = "Can't load config for"
            raise OSError(msg)
        if resolved_config_file == config_file:
            None
        else:
            None
        return Config.load_yaml(resolved_config_file), kwargs


class Box2BoxTransform:
    """
    This R-CNN transformation scales the box's width and height
    by exp(dw), exp(dh) and shifts a box's center by the offset
    (dx * width, dy * height).
    """

    def __init__(self, weights: Tuple[float, float, float, float], scale_clamp: float=None):
        """
        Args:
            weights (4-element tuple): Scaling factors that are applied to the
                (dx, dy, dw, dh) deltas. In Fast R-CNN, these were originally set
                such that the deltas have unit variance; now they are treated as
                hyperparameters of the system.
            scale_clamp (float): When predicting deltas, the predicted box scaling
                factors (dw and dh) are clamped such that they are <= scale_clamp.
        """
        self.weights = weights
        if scale_clamp is not None:
            self.scale_clamp = scale_clamp
        else:
            """
            Value for clamping large dw and dh predictions.
            The heuristic is that we clamp such that dw and dh are no larger
            than what would transform a 16px box into a 1000px box
            (based on a small anchor, 16px, and a typical image size, 1000px).
            """
            self.scale_clamp = math.log(1000.0 / 16)

    def get_deltas(self, src_boxes, target_boxes):
        """
        Get box regression transformation deltas (dx, dy, dw, dh) that can be used
        to transform the `src_boxes` into the `target_boxes`. That is, the relation
        ``target_boxes == self.apply_deltas(deltas, src_boxes)`` is true (unless
        any delta is too large and is clamped).
        Args:
            src_boxes (Tensor): source boxes, e.g., object proposals
            target_boxes (Tensor): target of the transformation, e.g., ground-truth
                boxes.
        """
        assert isinstance(src_boxes, torch.Tensor), type(src_boxes)
        assert isinstance(target_boxes, torch.Tensor), type(target_boxes)
        src_widths = src_boxes[:, 2] - src_boxes[:, 0]
        src_heights = src_boxes[:, 3] - src_boxes[:, 1]
        src_ctr_x = src_boxes[:, 0] + 0.5 * src_widths
        src_ctr_y = src_boxes[:, 1] + 0.5 * src_heights
        target_widths = target_boxes[:, 2] - target_boxes[:, 0]
        target_heights = target_boxes[:, 3] - target_boxes[:, 1]
        target_ctr_x = target_boxes[:, 0] + 0.5 * target_widths
        target_ctr_y = target_boxes[:, 1] + 0.5 * target_heights
        wx, wy, ww, wh = self.weights
        dx = wx * (target_ctr_x - src_ctr_x) / src_widths
        dy = wy * (target_ctr_y - src_ctr_y) / src_heights
        dw = ww * torch.log(target_widths / src_widths)
        dh = wh * torch.log(target_heights / src_heights)
        deltas = torch.stack((dx, dy, dw, dh), dim=1)
        assert (src_widths > 0).all().item(), 'Input boxes to Box2BoxTransform are not valid!'
        return deltas

    def apply_deltas(self, deltas, boxes):
        """
        Apply transformation `deltas` (dx, dy, dw, dh) to `boxes`.
        Args:
            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.
                deltas[i] represents k potentially different class-specific
                box transformations for the single box boxes[i].
            boxes (Tensor): boxes to transform, of shape (N, 4)
        """
        boxes = boxes
        widths = boxes[:, 2] - boxes[:, 0]
        heights = boxes[:, 3] - boxes[:, 1]
        ctr_x = boxes[:, 0] + 0.5 * widths
        ctr_y = boxes[:, 1] + 0.5 * heights
        wx, wy, ww, wh = self.weights
        dx = deltas[:, 0::4] / wx
        dy = deltas[:, 1::4] / wy
        dw = deltas[:, 2::4] / ww
        dh = deltas[:, 3::4] / wh
        dw = torch.clamp(dw, max=self.scale_clamp)
        dh = torch.clamp(dh, max=self.scale_clamp)
        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
        pred_w = torch.exp(dw) * widths[:, None]
        pred_h = torch.exp(dh) * heights[:, None]
        pred_boxes = torch.zeros_like(deltas)
        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w
        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h
        return pred_boxes


def _clip_box(tensor, box_size: Tuple[int, int]):
    assert torch.isfinite(tensor).all(), 'Box tensor contains infinite or NaN!'
    h, w = box_size
    tensor[:, 0].clamp_(min=0, max=w)
    tensor[:, 1].clamp_(min=0, max=h)
    tensor[:, 2].clamp_(min=0, max=w)
    tensor[:, 3].clamp_(min=0, max=h)


def do_nms(boxes, scores, image_shape, score_thresh, nms_thresh, mind, maxd):
    scores = scores[:, :-1]
    num_bbox_reg_classes = boxes.shape[1] // 4
    boxes = boxes.reshape(-1, 4)
    _clip_box(boxes, image_shape)
    boxes = boxes.view(-1, num_bbox_reg_classes, 4)
    max_scores, max_classes = scores.max(1)
    num_objs = boxes.size(0)
    boxes = boxes.view(-1, 4)
    idxs = torch.arange(num_objs) * num_bbox_reg_classes + max_classes
    max_boxes = boxes[idxs]
    keep = nms(max_boxes, max_scores, nms_thresh)
    keep = keep[:maxd]
    if keep.shape[-1] >= mind and keep.shape[-1] <= maxd:
        max_boxes, max_scores = max_boxes[keep], max_scores[keep]
        classes = max_classes[keep]
        return max_boxes, max_scores, classes, keep
    else:
        return None


class ROIOutputs:

    def __init__(self, cfg, training=False):
        self.smooth_l1_beta = cfg.roi_box_head.smooth_l1_beta
        self.box2box_transform = Box2BoxTransform(weights=cfg.roi_box_head.bbox_reg_weights)
        self.training = training
        self.score_thresh = cfg.roi_heads.score_thresh_test
        self.min_detections = cfg.min_detections
        self.max_detections = cfg.max_detections
        nms_thresh = list(cfg.roi_heads.nms_thresh_test)
        if not isinstance(nms_thresh, list):
            nms_thresh = [nms_thresh]
        self.nms_thresh = nms_thresh

    def _predict_boxes(self, proposals, box_deltas, preds_per_image):
        num_pred = box_deltas.size(0)
        B = proposals[0].size(-1)
        K = box_deltas.size(-1) // B
        box_deltas = box_deltas.view(num_pred * K, B)
        proposals = torch.cat(proposals, dim=0).unsqueeze(-2).expand(num_pred, K, B)
        proposals = proposals.reshape(-1, B)
        boxes = self.box2box_transform.apply_deltas(box_deltas, proposals)
        return boxes.view(num_pred, K * B).split(preds_per_image, dim=0)

    def _predict_objs(self, obj_logits, preds_per_image):
        probs = F.softmax(obj_logits, dim=-1)
        probs = probs.split(preds_per_image, dim=0)
        return probs

    def _predict_attrs(self, attr_logits, preds_per_image):
        attr_logits = attr_logits[..., :-1].softmax(-1)
        attr_probs, attrs = attr_logits.max(-1)
        return attr_probs.split(preds_per_image, dim=0), attrs.split(preds_per_image, dim=0)

    @torch.no_grad()
    def inference(self, obj_logits, attr_logits, box_deltas, pred_boxes, features, sizes, scales=None):
        preds_per_image = [p.size(0) for p in pred_boxes]
        boxes_all = self._predict_boxes(pred_boxes, box_deltas, preds_per_image)
        obj_scores_all = self._predict_objs(obj_logits, preds_per_image)
        attr_probs_all, attrs_all = self._predict_attrs(attr_logits, preds_per_image)
        features = features.split(preds_per_image, dim=0)
        final_results = []
        zipped = zip(boxes_all, obj_scores_all, attr_probs_all, attrs_all, sizes)
        for i, (boxes, obj_scores, attr_probs, attrs, size) in enumerate(zipped):
            for nms_t in self.nms_thresh:
                outputs = do_nms(boxes, obj_scores, size, self.score_thresh, nms_t, self.min_detections, self.max_detections)
                if outputs is not None:
                    max_boxes, max_scores, classes, ids = outputs
                    break
            if scales is not None:
                scale_yx = scales[i]
                max_boxes[:, 0::2] *= scale_yx[1]
                max_boxes[:, 1::2] *= scale_yx[0]
            final_results.append((max_boxes, classes, max_scores, attrs[ids], attr_probs[ids], features[i][ids]))
        boxes, classes, class_probs, attrs, attr_probs, roi_features = map(list, zip(*final_results))
        return boxes, classes, class_probs, attrs, attr_probs, roi_features

    def training(self, obj_logits, attr_logits, box_deltas, pred_boxes, features, sizes):
        pass

    def __call__(self, obj_logits, attr_logits, box_deltas, pred_boxes, features, sizes, scales=None):
        if self.training:
            raise NotImplementedError()
        return self.inference(obj_logits, attr_logits, box_deltas, pred_boxes, features, sizes, scales=scales)


class ShapeSpec(namedtuple('_ShapeSpec', ['channels', 'height', 'width', 'stride'])):

    def __new__(cls, *, channels=None, height=None, width=None, stride=None):
        return super().__new__(cls, channels, height, width, stride)


def _create_grid_offsets(size: List[int], stride: int, offset: float, device):
    grid_height, grid_width = size
    shifts_x = torch.arange(offset * stride, grid_width * stride, step=stride, dtype=torch.float32, device=device)
    shifts_y = torch.arange(offset * stride, grid_height * stride, step=stride, dtype=torch.float32, device=device)
    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
    shift_x = shift_x.reshape(-1)
    shift_y = shift_y.reshape(-1)
    return shift_x, shift_y


class AnchorGenerator(nn.Module):
    """
    For a set of image sizes and feature maps, computes a set of anchors.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        sizes = list(cfg.anchor_generator.sizes)
        aspect_ratios = list(cfg.anchor_generator.aspect_ratios)
        self.strides = [x.stride for x in input_shape]
        self.offset = cfg.anchor_generator.offset
        assert 0.0 <= self.offset < 1.0, self.offset
        """
        sizes (list[list[int]]): sizes[i] is the list of anchor sizes for feat map i
            1. given in absolute lengths in units of the input image;
            2. they do not dynamically scale if the input image size changes.
        aspect_ratios (list[list[float]])
        strides (list[int]): stride of each input feature.
        """
        self.num_features = len(self.strides)
        self.cell_anchors = nn.ParameterList(self._calculate_anchors(sizes, aspect_ratios))
        self._spacial_feat_dim = 4

    def _calculate_anchors(self, sizes, aspect_ratios):
        if len(sizes) == 1:
            sizes *= self.num_features
        if len(aspect_ratios) == 1:
            aspect_ratios *= self.num_features
        assert self.num_features == len(sizes)
        assert self.num_features == len(aspect_ratios)
        cell_anchors = [self.generate_cell_anchors(s, a).float() for s, a in zip(sizes, aspect_ratios)]
        return cell_anchors

    @property
    def box_dim(self):
        return self._spacial_feat_dim

    @property
    def num_cell_anchors(self):
        """
        Returns:
            list[int]: Each int is the number of anchors at every pixel
            location, on that feature map.
        """
        return [len(cell_anchors) for cell_anchors in self.cell_anchors]

    def grid_anchors(self, grid_sizes):
        anchors = []
        for size, stride, base_anchors in zip(grid_sizes, self.strides, self.cell_anchors):
            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, base_anchors.device)
            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
            anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))
        return anchors

    def generate_cell_anchors(self, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)):
        """
        anchors are continuous geometric rectangles
        centered on one feature map point sample.
        We can later build the set of anchors
        for the entire feature map by tiling these tensors
        """
        anchors = []
        for size in sizes:
            area = size ** 2.0
            for aspect_ratio in aspect_ratios:
                w = math.sqrt(area / aspect_ratio)
                h = aspect_ratio * w
                x0, y0, x1, y1 = -w / 2.0, -h / 2.0, w / 2.0, h / 2.0
                anchors.append([x0, y0, x1, y1])
        return nn.Parameter(torch.Tensor(anchors))

    def forward(self, features):
        """
        Args:
            features List[torch.Tensor]: list of feature maps on which to
            generate anchors.
        Returns:
            torch.Tensor: a list of #image elements.
        """
        num_images = features[0].size(0)
        grid_sizes = [feature_map.shape[-2:] for feature_map in features]
        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)
        anchors_over_all_feature_maps = torch.stack(anchors_over_all_feature_maps)
        return anchors_over_all_feature_maps.unsqueeze(0).repeat_interleave(num_images, dim=0)


class Matcher:
    """
    This class assigns to each predicted "element" (e.g., a box) a ground-truth
    element. Each predicted element will have exactly zero or one matches; each
    ground-truth element may be matched to zero or more predicted elements.
    The matching is determined by the MxN match_quality_matrix, that characterizes
    how well each (ground-truth, prediction)-pair match each other. For example,
    if the elements are boxes, this matrix may contain box intersection-over-union
    overlap values.
    The matcher returns (a) a vector of length N containing the index of the
    ground-truth element m in [0, M) that matches to prediction n in [0, N).
    (b) a vector of length N containing the labels for each prediction.
    """

    def __init__(self, thresholds: List[float], labels: List[int], allow_low_quality_matches: bool=False):
        """
        Args:
            thresholds (list): a list of thresholds used to stratify predictions
                into levels.
            labels (list): a list of values to label predictions belonging at
                each level. A label can be one of {-1, 0, 1} signifying
                {ignore, negative class, positive class}, respectively.
            allow_low_quality_matches (bool): if True, produce additional matches or
                predictions with maximum match quality lower than high_threshold.
                For example, thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions
                with iou < 0.3 will be marked with 0 and
                thus will be considered as false positives while training. All
                predictions with 0.3 <= iou < 0.5 will be marked with -1 and
                thus will be ignored. All predictions with 0.5 <= iou will be marked
                with 1 and thus will be considered as true positives.
        """
        thresholds = thresholds[:]
        assert thresholds[0] > 0
        thresholds.insert(0, -float('inf'))
        thresholds.append(float('inf'))
        assert all([(low <= high) for low, high in zip(thresholds[:-1], thresholds[1:])])
        assert all([(label_i in [-1, 0, 1]) for label_i in labels])
        assert len(labels) == len(thresholds) - 1
        self.thresholds = thresholds
        self.labels = labels
        self.allow_low_quality_matches = allow_low_quality_matches

    def __call__(self, match_quality_matrix):
        """
        Args:
            match_quality_matrix (Tensor[float]): an MxN tensor, containing the
            pairwise quality between M ground-truth elements and N predicted
                elements. All elements must be >= 0 (due to the us of `torch.nonzero`
                for selecting indices in :meth:`set_low_quality_matches_`).
        Returns:
            matches (Tensor[int64]): a vector of length N, where matches[i]
            is a matched ground-truth index in [0, M)
            match_labels (Tensor[int8]): a vector of length N, where pred_labels[i]
            indicates true or false positive or ignored
        """
        assert match_quality_matrix.dim() == 2
        if match_quality_matrix.numel() == 0:
            default_matches = match_quality_matrix.new_full((match_quality_matrix.size(1),), 0, dtype=torch.int64)
            default_match_labels = match_quality_matrix.new_full((match_quality_matrix.size(1),), self.labels[0], dtype=torch.int8)
            return default_matches, default_match_labels
        assert torch.all(match_quality_matrix >= 0)
        matched_vals, matches = match_quality_matrix.max(dim=0)
        match_labels = matches.new_full(matches.size(), 1, dtype=torch.int8)
        for l, low, high in zip(self.labels, self.thresholds[:-1], self.thresholds[1:]):
            low_high = (matched_vals >= low) & (matched_vals < high)
            match_labels[low_high] = l
        if self.allow_low_quality_matches:
            self.set_low_quality_matches_(match_labels, match_quality_matrix)
        return matches, match_labels

    def set_low_quality_matches_(self, match_labels, match_quality_matrix):
        """
        Produce additional matches for predictions that have only low-quality matches.
        Specifically, for each ground-truth G find the set of predictions that have
        maximum overlap with it (including ties); for each prediction in that set, if
        it is unmatched, then match it to the ground-truth G.
        This function implements the RPN assignment case (i)
        in Sec. 3.1.2 of Faster R-CNN.
        """
        highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)
        of_quality_inds = match_quality_matrix == highest_quality_foreach_gt[:, None]
        if of_quality_inds.dim() == 0:
            _, pred_inds_with_highest_quality = of_quality_inds.unsqueeze(0).nonzero().unbind(1)
        else:
            _, pred_inds_with_highest_quality = of_quality_inds.nonzero().unbind(1)
        match_labels[pred_inds_with_highest_quality] = 1


class RPNHead(nn.Module):
    """
    RPN classification and regression heads. Uses a 3x3 conv to produce a shared
    hidden state from which one 1x1 conv predicts objectness logits for each anchor
    and a second 1x1 conv predicts bounding-box deltas specifying how to deform
    each anchor into an object proposal.
    """

    def __init__(self, cfg, input_shape: List[ShapeSpec]):
        super().__init__()
        in_channels = [s.channels for s in input_shape]
        assert len(set(in_channels)) == 1, 'Each level must have the same channel!'
        in_channels = in_channels[0]
        anchor_generator = AnchorGenerator(cfg, input_shape)
        num_cell_anchors = anchor_generator.num_cell_anchors
        box_dim = anchor_generator.box_dim
        assert len(set(num_cell_anchors)) == 1, 'Each level must have the same number of cell anchors'
        num_cell_anchors = num_cell_anchors[0]
        if cfg.proposal_generator.hidden_channels == -1:
            hid_channels = in_channels
        else:
            hid_channels = cfg.proposal_generator.hidden_channels
        self.conv = nn.Conv2d(in_channels, hid_channels, kernel_size=3, stride=1, padding=1)
        self.objectness_logits = nn.Conv2d(hid_channels, num_cell_anchors, kernel_size=1, stride=1)
        self.anchor_deltas = nn.Conv2d(hid_channels, num_cell_anchors * box_dim, kernel_size=1, stride=1)
        for layer in [self.conv, self.objectness_logits, self.anchor_deltas]:
            nn.init.normal_(layer.weight, std=0.01)
            nn.init.constant_(layer.bias, 0)

    def forward(self, features):
        """
        Args:
            features (list[Tensor]): list of feature maps
        """
        pred_objectness_logits = []
        pred_anchor_deltas = []
        for x in features:
            t = F.relu(self.conv(x))
            pred_objectness_logits.append(self.objectness_logits(t))
            pred_anchor_deltas.append(self.anchor_deltas(t))
        return pred_objectness_logits, pred_anchor_deltas


class RPNOutputs:

    def __init__(self, box2box_transform, anchor_matcher, batch_size_per_image, positive_fraction, images, pred_objectness_logits, pred_anchor_deltas, anchors, boundary_threshold=0, gt_boxes=None, smooth_l1_beta=0.0):
        """
        Args:
            box2box_transform (Box2BoxTransform): :class:`Box2BoxTransform`
            instance for anchor-proposal transformations.
            anchor_matcher (Matcher): :class:`Matcher` instance for matching
            anchors to ground-truth boxes; used to determine training labels.
            batch_size_per_image (int): number of proposals to sample when training
            positive_fraction (float): target fraction of sampled proposals that
            should be positive
            images (ImageList): :class:`ImageList` instance representing N input images
            pred_objectness_logits (list[Tensor]): A list of L elements. Element i
            is a tensor of shape (N, A, Hi, W)
            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a
            tensor of shape (N, A*4, Hi, Wi)
            anchors (list[torch.Tensor]): nested list of boxes. anchors[i][j] at (n, l)
            stores anchor array for feature map l
            boundary_threshold (int): if >= 0, then anchors that extend beyond the image
            boundary by more than boundary_thresh are not used in training.
            gt_boxes (list[Boxes], optional): A list of N elements.
            smooth_l1_beta (float): The transition point between L1 and L2 lossn. When
            set to 0, the loss becomes L1. When +inf, it is ignored
        """
        self.box2box_transform = box2box_transform
        self.anchor_matcher = anchor_matcher
        self.batch_size_per_image = batch_size_per_image
        self.positive_fraction = positive_fraction
        self.pred_objectness_logits = pred_objectness_logits
        self.pred_anchor_deltas = pred_anchor_deltas
        self.anchors = anchors
        self.gt_boxes = gt_boxes
        self.num_feature_maps = len(pred_objectness_logits)
        self.num_images = len(images)
        self.boundary_threshold = boundary_threshold
        self.smooth_l1_beta = smooth_l1_beta

    def _get_ground_truth(self):
        raise NotImplementedError()

    def predict_proposals(self):
        proposals = []
        anchors = self.anchors.transpose(0, 1)
        for anchors_i, pred_anchor_deltas_i in zip(anchors, self.pred_anchor_deltas):
            B = anchors_i.size(-1)
            N, _, Hi, Wi = pred_anchor_deltas_i.shape
            anchors_i = anchors_i.flatten(start_dim=0, end_dim=1)
            pred_anchor_deltas_i = pred_anchor_deltas_i.view(N, -1, B, Hi, Wi).permute(0, 3, 4, 1, 2).reshape(-1, B)
            proposals_i = self.box2box_transform.apply_deltas(pred_anchor_deltas_i, anchors_i)
            proposals.append(proposals_i.view(N, -1, B))
        proposals = torch.stack(proposals)
        return proposals

    def predict_objectness_logits(self):
        """
        Returns:
            pred_objectness_logits (list[Tensor]) -> (N, Hi*Wi*A).
        """
        pred_objectness_logits = [score.permute(0, 2, 3, 1).reshape(self.num_images, -1) for score in self.pred_objectness_logits]
        return pred_objectness_logits


def _nonempty_boxes(box, threshold: float=0.0) ->torch.Tensor:
    widths = box[:, 2] - box[:, 0]
    heights = box[:, 3] - box[:, 1]
    keep = (widths > threshold) & (heights > threshold)
    return keep


def find_top_rpn_proposals(proposals, pred_objectness_logits, images, image_sizes, nms_thresh, pre_nms_topk, post_nms_topk, min_box_side_len, training):
    """Args:
        proposals (list[Tensor]): (L, N, Hi*Wi*A, 4).
        pred_objectness_logits: tensors of length L.
        nms_thresh (float): IoU threshold to use for NMS
        pre_nms_topk (int): before nms
        post_nms_topk (int): after nms
        min_box_side_len (float): minimum proposal box side
        training (bool): True if proposals are to be used in training,
    Returns:
        results (List[Dict]): stores post_nms_topk object proposals for image i.
    """
    num_images = len(images)
    device = proposals[0].device
    topk_scores = []
    topk_proposals = []
    level_ids = []
    batch_idx = torch.arange(num_images, device=device)
    for level_id, proposals_i, logits_i in zip(itertools.count(), proposals, pred_objectness_logits):
        Hi_Wi_A = logits_i.shape[1]
        num_proposals_i = min(pre_nms_topk, Hi_Wi_A)
        logits_i, idx = logits_i.sort(descending=True, dim=1)
        topk_scores_i = logits_i[batch_idx, :num_proposals_i]
        topk_idx = idx[batch_idx, :num_proposals_i]
        topk_proposals_i = proposals_i[batch_idx[:, None], topk_idx]
        topk_proposals.append(topk_proposals_i)
        topk_scores.append(topk_scores_i)
        level_ids.append(torch.full((num_proposals_i,), level_id, dtype=torch.int64, device=device))
    topk_scores = torch.cat(topk_scores, dim=1)
    topk_proposals = torch.cat(topk_proposals, dim=1)
    level_ids = torch.cat(level_ids, dim=0)
    results = []
    for n, image_size in enumerate(image_sizes):
        boxes = topk_proposals[n]
        scores_per_img = topk_scores[n]
        _clip_box(boxes, image_size)
        keep = _nonempty_boxes(boxes, threshold=min_box_side_len)
        lvl = level_ids
        if keep.sum().item() != len(boxes):
            boxes, scores_per_img, lvl = boxes[keep], scores_per_img[keep], level_ids[keep]
        keep = batched_nms(boxes, scores_per_img, lvl, nms_thresh)
        keep = keep[:post_nms_topk]
        res = boxes[keep], scores_per_img[keep]
        results.append(res)
    return results


class RPN(nn.Module):
    """
    Region Proposal Network, introduced by the Faster R-CNN paper.
    """

    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):
        super().__init__()
        self.min_box_side_len = cfg.proposal_generator.min_size
        self.in_features = cfg.rpn.in_features
        self.nms_thresh = cfg.rpn.nms_thresh
        self.batch_size_per_image = cfg.rpn.batch_size_per_image
        self.positive_fraction = cfg.rpn.positive_fraction
        self.smooth_l1_beta = cfg.rpn.smooth_l1_beta
        self.loss_weight = cfg.rpn.loss_weight
        self.pre_nms_topk = {(True): cfg.rpn.pre_nms_topk_train, (False): cfg.rpn.pre_nms_topk_test}
        self.post_nms_topk = {(True): cfg.rpn.post_nms_topk_train, (False): cfg.rpn.post_nms_topk_test}
        self.boundary_threshold = cfg.rpn.boundary_thresh
        self.anchor_generator = AnchorGenerator(cfg, [input_shape[f] for f in self.in_features])
        self.box2box_transform = Box2BoxTransform(weights=cfg.rpn.bbox_reg_weights)
        self.anchor_matcher = Matcher(cfg.rpn.iou_thresholds, cfg.rpn.iou_labels, allow_low_quality_matches=True)
        self.rpn_head = RPNHead(cfg, [input_shape[f] for f in self.in_features])

    def training(self, images, image_shapes, features, gt_boxes):
        pass

    def inference(self, outputs, images, image_shapes, features, gt_boxes=None):
        outputs = find_top_rpn_proposals(outputs.predict_proposals(), outputs.predict_objectness_logits(), images, image_shapes, self.nms_thresh, self.pre_nms_topk[self.training], self.post_nms_topk[self.training], self.min_box_side_len, self.training)
        results = []
        for img in outputs:
            im_boxes, img_box_logits = img
            img_box_logits, inds = img_box_logits.sort(descending=True)
            im_boxes = im_boxes[inds]
            results.append((im_boxes, img_box_logits))
        proposal_boxes, logits = tuple(map(list, zip(*results)))
        return proposal_boxes, logits

    def forward(self, images, image_shapes, features, gt_boxes=None):
        """
        Args:
            images (torch.Tensor): input images of length `N`
            features (dict[str: Tensor])
            gt_instances
        """
        features = [features[f] for f in self.in_features]
        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)
        anchors = self.anchor_generator(features)
        outputs = RPNOutputs(self.box2box_transform, self.anchor_matcher, self.batch_size_per_image, self.positive_fraction, images, pred_objectness_logits, pred_anchor_deltas, anchors, self.boundary_threshold, gt_boxes, self.smooth_l1_beta)
        if self.training:
            raise NotImplementedError()
            return self.training(outputs, images, image_shapes, features, gt_boxes)
        else:
            return self.inference(outputs, images, image_shapes, features, gt_boxes)


class ResNetBlockBase(nn.Module):

    def __init__(self, in_channels, out_channels, stride):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride

    def freeze(self):
        for p in self.parameters():
            p.requires_grad = False
        return self


class BottleneckBlock(ResNetBlockBase):

    def __init__(self, in_channels, out_channels, bottleneck_channels, stride=1, num_groups=1, norm='BN', stride_in_1x1=False, dilation=1):
        super().__init__(in_channels, out_channels, stride)
        if in_channels != out_channels:
            self.shortcut = Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False, norm=get_norm(norm, out_channels))
        else:
            self.shortcut = None
        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)
        self.conv1 = Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=stride_1x1, bias=False, norm=get_norm(norm, bottleneck_channels))
        self.conv2 = Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=stride_3x3, padding=1 * dilation, bias=False, groups=num_groups, dilation=dilation, norm=get_norm(norm, bottleneck_channels))
        self.conv3 = Conv2d(bottleneck_channels, out_channels, kernel_size=1, bias=False, norm=get_norm(norm, out_channels))

    def forward(self, x):
        out = self.conv1(x)
        out = F.relu_(out)
        out = self.conv2(out)
        out = F.relu_(out)
        out = self.conv3(out)
        if self.shortcut is not None:
            shortcut = self.shortcut(x)
        else:
            shortcut = x
        out += shortcut
        out = F.relu_(out)
        return out


class FastRCNNOutputLayers(nn.Module):
    """
    Two linear layers for predicting Fast R-CNN outputs:
      (1) proposal-to-detection box regression deltas
      (2) classification scores
    """

    def __init__(self, input_size, num_classes, cls_agnostic_bbox_reg, box_dim=4, use_attr=False, num_attrs=-1):
        """
        Args:
            input_size (int): channels, or (channels, height, width)
            num_classes (int)
            cls_agnostic_bbox_reg (bool)
            box_dim (int)
        """
        super().__init__()
        if not isinstance(input_size, int):
            input_size = np.prod(input_size)
        self.cls_score = nn.Linear(input_size, num_classes + 1)
        num_bbox_reg_classes = 1 if cls_agnostic_bbox_reg else num_classes
        self.bbox_pred = nn.Linear(input_size, num_bbox_reg_classes * box_dim)
        self.use_attr = use_attr
        if use_attr:
            """
            Modifications for VG in RoI heads
            Embedding: {num_classes + 1} --> {input_size // 8}
            Linear: {input_size + input_size // 8} --> {input_size // 4}
            Linear: {input_size // 4} --> {num_attrs + 1}
            """
            self.cls_embedding = nn.Embedding(num_classes + 1, input_size // 8)
            self.fc_attr = nn.Linear(input_size + input_size // 8, input_size // 4)
            self.attr_score = nn.Linear(input_size // 4, num_attrs + 1)
        nn.init.normal_(self.cls_score.weight, std=0.01)
        nn.init.normal_(self.bbox_pred.weight, std=0.001)
        for item in [self.cls_score, self.bbox_pred]:
            nn.init.constant_(item.bias, 0)

    def forward(self, roi_features):
        if roi_features.dim() > 2:
            roi_features = torch.flatten(roi_features, start_dim=1)
        scores = self.cls_score(roi_features)
        proposal_deltas = self.bbox_pred(roi_features)
        if self.use_attr:
            _, max_class = scores.max(-1)
            cls_emb = self.cls_embedding(max_class)
            roi_features = torch.cat([roi_features, cls_emb], -1)
            roi_features = self.fc_attr(roi_features)
            roi_features = F.relu(roi_features)
            attr_scores = self.attr_score(roi_features)
            return scores, attr_scores, proposal_deltas
        else:
            return scores, proposal_deltas


def assign_boxes_to_levels(box_lists: List[torch.Tensor], min_level: int, max_level: int, canonical_box_size: int, canonical_level: int):
    box_sizes = torch.sqrt(torch.cat([boxes.area() for boxes in box_lists]))
    level_assignments = torch.floor(canonical_level + torch.log2(box_sizes / canonical_box_size + 1e-08))
    level_assignments = torch.clamp(level_assignments, min=min_level, max=max_level)
    return level_assignments - min_level


def _fmt_box_list(box_tensor, batch_index: int):
    repeated_index = torch.full((len(box_tensor), 1), batch_index, dtype=box_tensor.dtype, device=box_tensor.device)
    return torch.cat((repeated_index, box_tensor), dim=1)


def convert_boxes_to_pooler_format(box_lists: List[torch.Tensor]):
    pooler_fmt_boxes = torch.cat([_fmt_box_list(box_list, i) for i, box_list in enumerate(box_lists)], dim=0)
    return pooler_fmt_boxes


class ROIPooler(nn.Module):
    """
    Region of interest feature map pooler that supports pooling from one or more
    feature maps.
    """

    def __init__(self, output_size, scales, sampling_ratio, canonical_box_size=224, canonical_level=4):
        super().__init__()
        min_level = -math.log2(scales[0])
        max_level = -math.log2(scales[-1])
        assert math.isclose(min_level, int(min_level)) and math.isclose(max_level, int(max_level))
        assert len(scales) == max_level - min_level + 1, 'not pyramid'
        assert 0 < min_level and min_level <= max_level
        if isinstance(output_size, int):
            output_size = output_size, output_size
        assert len(output_size) == 2 and isinstance(output_size[0], int) and isinstance(output_size[1], int)
        if len(scales) > 1:
            assert min_level <= canonical_level and canonical_level <= max_level
        assert canonical_box_size > 0
        self.output_size = output_size
        self.min_level = int(min_level)
        self.max_level = int(max_level)
        self.level_poolers = nn.ModuleList(RoIPool(output_size, spatial_scale=scale) for scale in scales)
        self.canonical_level = canonical_level
        self.canonical_box_size = canonical_box_size

    def forward(self, feature_maps, boxes):
        """
        Args:
            feature_maps: List[torch.Tensor(N,C,W,H)]
            box_lists: list[torch.Tensor])
        Returns:
            A tensor of shape(N*B, Channels, output_size, output_size)
        """
        x = [v for v in feature_maps.values()]
        num_level_assignments = len(self.level_poolers)
        assert len(x) == num_level_assignments and len(boxes) == x[0].size(0)
        pooler_fmt_boxes = convert_boxes_to_pooler_format(boxes)
        if num_level_assignments == 1:
            return self.level_poolers[0](x[0], pooler_fmt_boxes)
        level_assignments = assign_boxes_to_levels(boxes, self.min_level, self.max_level, self.canonical_box_size, self.canonical_level)
        num_boxes = len(pooler_fmt_boxes)
        num_channels = x[0].shape[1]
        output_size = self.output_size[0]
        dtype, device = x[0].dtype, x[0].device
        output = torch.zeros((num_boxes, num_channels, output_size, output_size), dtype=dtype, device=device)
        for level, (x_level, pooler) in enumerate(zip(x, self.level_poolers)):
            inds = torch.nonzero(level_assignments == level).squeeze(1)
            pooler_fmt_boxes_level = pooler_fmt_boxes[inds]
            output[inds] = pooler(x_level, pooler_fmt_boxes_level)
        return output


class Backbone(nn.Module, metaclass=ABCMeta):

    def __init__(self):
        super().__init__()

    @abstractmethod
    def forward(self):
        pass

    @property
    def size_divisibility(self):
        """
        Some backbones require the input height and width to be divisible
        by a specific integer. This is
        typically true for encoder / decoder type networks with lateral
        connection (e.g., FPN) for which feature maps need to match
        dimension in the "bottom up" and "top down" paths. Set to 0 if
        no specific input size divisibility is required.
        """
        return 0

    def output_shape(self):
        return {name: ShapeSpec(channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]) for name in self._out_features}

    @property
    def out_features(self):
        """deprecated"""
        return self._out_features

    @property
    def out_feature_strides(self):
        """deprecated"""
        return {f: self._out_feature_strides[f] for f in self._out_features}

    @property
    def out_feature_channels(self):
        """deprecated"""
        return {f: self._out_feature_channels[f] for f in self._out_features}


class ResNet(Backbone):

    def __init__(self, stem, stages, num_classes=None, out_features=None):
        """
        Args:
            stem (nn.Module): a stem module
            stages (list[list[ResNetBlock]]): several (typically 4) stages,
            each contains multiple :class:`ResNetBlockBase`.
            num_classes (None or int): if None, will not perform classification.
            out_features (list[str]): name of the layers whose outputs should
            be returned in forward. Can be anything in:
            "stem", "linear", or "res2" ... If None, will return the output
            of the last layer.
        """
        super().__init__()
        self.stem = stem
        self.num_classes = num_classes
        current_stride = self.stem.stride
        self._out_feature_strides = {'stem': current_stride}
        self._out_feature_channels = {'stem': self.stem.out_channels}
        self.stages_and_names = []
        for i, blocks in enumerate(stages):
            for block in blocks:
                assert isinstance(block, ResNetBlockBase), block
                curr_channels = block.out_channels
            stage = nn.Sequential(*blocks)
            name = 'res' + str(i + 2)
            self.add_module(name, stage)
            self.stages_and_names.append((stage, name))
            self._out_feature_strides[name] = current_stride = int(current_stride * np.prod([k.stride for k in blocks]))
            self._out_feature_channels[name] = blocks[-1].out_channels
        if num_classes is not None:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.linear = nn.Linear(curr_channels, num_classes)
            nn.init.normal_(self.linear.weight, stddev=0.01)
            name = 'linear'
        if out_features is None:
            out_features = [name]
        self._out_features = out_features
        assert len(self._out_features)
        children = [x[0] for x in self.named_children()]
        for out_feature in self._out_features:
            assert out_feature in children, 'Available children: {}'.format(', '.join(children))

    def forward(self, x):
        outputs = {}
        x = self.stem(x)
        if 'stem' in self._out_features:
            outputs['stem'] = x
        for stage, name in self.stages_and_names:
            x = stage(x)
            if name in self._out_features:
                outputs[name] = x
        if self.num_classes is not None:
            x = self.avgpool(x)
            x = self.linear(x)
            if 'linear' in self._out_features:
                outputs['linear'] = x
        return outputs

    def output_shape(self):
        return {name: ShapeSpec(channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]) for name in self._out_features}

    @staticmethod
    def make_stage(block_class, num_blocks, first_stride=None, *, in_channels, out_channels, **kwargs):
        """
        Usually, layers that produce the same feature map spatial size
        are defined as one "stage".
        Under such definition, stride_per_block[1:] should all be 1.
        """
        if first_stride is not None:
            assert 'stride' not in kwargs and 'stride_per_block' not in kwargs
            kwargs['stride_per_block'] = [first_stride] + [1] * (num_blocks - 1)
        blocks = []
        for i in range(num_blocks):
            curr_kwargs = {}
            for k, v in kwargs.items():
                if k.endswith('_per_block'):
                    assert len(v) == num_blocks, f"Argument '{k}' of make_stage should have the same length as num_blocks={num_blocks}."
                    newk = k[:-len('_per_block')]
                    assert newk not in kwargs, f'Cannot call make_stage with both {k} and {newk}!'
                    curr_kwargs[newk] = v[i]
                else:
                    curr_kwargs[k] = v
            blocks.append(block_class(in_channels=in_channels, out_channels=out_channels, **curr_kwargs))
            in_channels = out_channels
        return blocks


class Res5ROIHeads(nn.Module):
    """
    ROIHeads perform all per-region computation in an R-CNN.
    It contains logic of cropping the regions, extract per-region features
    (by the res-5 block in this case), and make per-region predictions.
    """

    def __init__(self, cfg, input_shape):
        super().__init__()
        self.batch_size_per_image = cfg.rpn.batch_size_per_image
        self.positive_sample_fraction = cfg.roi_heads.positive_fraction
        self.in_features = cfg.roi_heads.in_features
        self.num_classes = cfg.roi_heads.num_classes
        self.proposal_append_gt = cfg.roi_heads.proposal_append_gt
        self.feature_strides = {k: v.stride for k, v in input_shape.items()}
        self.feature_channels = {k: v.channels for k, v in input_shape.items()}
        self.cls_agnostic_bbox_reg = cfg.roi_box_head.cls_agnostic_bbox_reg
        self.stage_channel_factor = 2 ** 3
        self.out_channels = cfg.resnets.res2_out_channels * self.stage_channel_factor
        pooler_resolution = cfg.roi_box_head.pooler_resolution
        pooler_scales = 1.0 / self.feature_strides[self.in_features[0]],
        sampling_ratio = cfg.roi_box_head.pooler_sampling_ratio
        res5_halve = cfg.roi_box_head.res5halve
        use_attr = cfg.roi_box_head.attr
        num_attrs = cfg.roi_box_head.num_attrs
        self.pooler = ROIPooler(output_size=pooler_resolution, scales=pooler_scales, sampling_ratio=sampling_ratio)
        self.res5 = self._build_res5_block(cfg)
        if not res5_halve:
            """
            Modifications for VG in RoI heads:
            1. Change the stride of conv1 and shortcut in Res5.Block1 from 2 to 1
            2. Modifying all conv2 with (padding: 1 --> 2) and (dilation: 1 --> 2)
            """
            self.res5[0].conv1.stride = 1, 1
            self.res5[0].shortcut.stride = 1, 1
            for i in range(3):
                self.res5[i].conv2.padding = 2, 2
                self.res5[i].conv2.dilation = 2, 2
        self.box_predictor = FastRCNNOutputLayers(self.out_channels, self.num_classes, self.cls_agnostic_bbox_reg, use_attr=use_attr, num_attrs=num_attrs)

    def _build_res5_block(self, cfg):
        stage_channel_factor = self.stage_channel_factor
        num_groups = cfg.resnets.num_groups
        width_per_group = cfg.resnets.width_per_group
        bottleneck_channels = num_groups * width_per_group * stage_channel_factor
        out_channels = self.out_channels
        stride_in_1x1 = cfg.resnets.stride_in_1x1
        norm = cfg.resnets.norm
        blocks = ResNet.make_stage(BottleneckBlock, 3, first_stride=2, in_channels=out_channels // 2, bottleneck_channels=bottleneck_channels, out_channels=out_channels, num_groups=num_groups, norm=norm, stride_in_1x1=stride_in_1x1)
        return nn.Sequential(*blocks)

    def _shared_roi_transform(self, features, boxes):
        x = self.pooler(features, boxes)
        return self.res5(x)

    def forward(self, features, proposal_boxes, gt_boxes=None):
        if self.training:
            """
            see https://github.com/airsplay/py-bottom-up-attention/                    blob/master/detectron2/modeling/roi_heads/roi_heads.py
            """
            raise NotImplementedError()
        assert not proposal_boxes[0].requires_grad
        box_features = self._shared_roi_transform(features, proposal_boxes)
        feature_pooled = box_features.mean(dim=[2, 3])
        obj_logits, attr_logits, pred_proposal_deltas = self.box_predictor(feature_pooled)
        return obj_logits, attr_logits, pred_proposal_deltas, feature_pooled


WEIGHTS_NAME = 'pytorch_model.bin'


def build_backbone(cfg):
    input_shape = ShapeSpec(channels=len(cfg.model.pixel_mean))
    norm = cfg.resnets.norm
    stem = BasicStem(in_channels=input_shape.channels, out_channels=cfg.resnets.stem_out_channels, norm=norm, caffe_maxpool=cfg.model.max_pool)
    freeze_at = cfg.backbone.freeze_at
    if freeze_at >= 1:
        for p in stem.parameters():
            p.requires_grad = False
    out_features = cfg.resnets.out_features
    depth = cfg.resnets.depth
    num_groups = cfg.resnets.num_groups
    width_per_group = cfg.resnets.width_per_group
    bottleneck_channels = num_groups * width_per_group
    in_channels = cfg.resnets.stem_out_channels
    out_channels = cfg.resnets.res2_out_channels
    stride_in_1x1 = cfg.resnets.stride_in_1x1
    res5_dilation = cfg.resnets.res5_dilation
    assert res5_dilation in {1, 2}, f'res5_dilation cannot be {res5_dilation}.'
    num_blocks_per_stage = {(50): [3, 4, 6, 3], (101): [3, 4, 23, 3], (152): [3, 8, 36, 3]}[depth]
    stages = []
    out_stage_idx = [{'res2': 2, 'res3': 3, 'res4': 4, 'res5': 5}[f] for f in out_features]
    max_stage_idx = max(out_stage_idx)
    for idx, stage_idx in enumerate(range(2, max_stage_idx + 1)):
        dilation = res5_dilation if stage_idx == 5 else 1
        first_stride = 1 if idx == 0 or stage_idx == 5 and dilation == 2 else 2
        stage_kargs = {'num_blocks': num_blocks_per_stage[idx], 'first_stride': first_stride, 'in_channels': in_channels, 'bottleneck_channels': bottleneck_channels, 'out_channels': out_channels, 'num_groups': num_groups, 'norm': norm, 'stride_in_1x1': stride_in_1x1, 'dilation': dilation}
        stage_kargs['block_class'] = BottleneckBlock
        blocks = ResNet.make_stage(**stage_kargs)
        in_channels = out_channels
        out_channels *= 2
        bottleneck_channels *= 2
        if freeze_at >= stage_idx:
            for block in blocks:
                block.freeze()
        stages.append(blocks)
    return ResNet(stem, stages, out_features=out_features)


def load_checkpoint(ckp):
    r = OrderedDict()
    with open(ckp, 'rb') as f:
        ckp = pkl.load(f)['model']
    for k in copy.deepcopy(list(ckp.keys())):
        v = ckp.pop(k)
        if isinstance(v, np.ndarray):
            v = torch.tensor(v)
        else:
            assert isinstance(v, torch.tensor), type(v)
        r[k] = v
    return r


def norm_box(boxes, raw_sizes):
    if not isinstance(boxes, torch.Tensor):
        normalized_boxes = boxes.copy()
    else:
        normalized_boxes = boxes.clone()
    normalized_boxes[:, :, (0, 2)] /= raw_sizes[:, 1]
    normalized_boxes[:, :, (1, 3)] /= raw_sizes[:, 0]
    return normalized_boxes


def pad_list_tensors(list_tensors, preds_per_image, max_detections=None, return_tensors=None, padding=None, pad_value=0, location=None):
    """
    location will always be cpu for np tensors
    """
    if location is None:
        location = 'cpu'
    assert return_tensors in {'pt', 'np', None}
    assert padding in {'max_detections', 'max_batch', None}
    new = []
    if padding is None:
        if return_tensors is None:
            return list_tensors
        elif return_tensors == 'pt':
            if not isinstance(list_tensors, torch.Tensor):
                return torch.stack(list_tensors)
            else:
                return list_tensors
        elif not isinstance(list_tensors, list):
            return np.array(list_tensors)
        else:
            return list_tensors
    if padding == 'max_detections':
        assert max_detections is not None, 'specify max number of detections per batch'
    elif padding == 'max_batch':
        max_detections = max(preds_per_image)
    for i in range(len(list_tensors)):
        too_small = False
        tensor_i = list_tensors.pop(0)
        if tensor_i.ndim < 2:
            too_small = True
            tensor_i = tensor_i.unsqueeze(-1)
        assert isinstance(tensor_i, torch.Tensor)
        tensor_i = F.pad(input=tensor_i, pad=(0, 0, 0, max_detections - preds_per_image[i]), mode='constant', value=pad_value)
        if too_small:
            tensor_i = tensor_i.squeeze(-1)
        if return_tensors is None:
            if location == 'cpu':
                tensor_i = tensor_i.cpu()
            tensor_i = tensor_i.tolist()
        if return_tensors == 'np':
            if location == 'cpu':
                tensor_i = tensor_i.cpu()
            tensor_i = tensor_i.numpy()
        elif location == 'cpu':
            tensor_i = tensor_i.cpu()
        new.append(tensor_i)
    if return_tensors == 'np':
        return np.stack(new, axis=0)
    elif return_tensors == 'pt' and not isinstance(new, torch.Tensor):
        return torch.stack(new, dim=0)
    else:
        return list_tensors


class GeneralizedRCNN(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        self.device = torch.device(cfg.model.device)
        self.backbone = build_backbone(cfg)
        self.proposal_generator = RPN(cfg, self.backbone.output_shape())
        self.roi_heads = Res5ROIHeads(cfg, self.backbone.output_shape())
        self.roi_outputs = ROIOutputs(cfg)
        self

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        config = kwargs.pop('config', None)
        state_dict = kwargs.pop('state_dict', None)
        cache_dir = kwargs.pop('cache_dir', None)
        from_tf = kwargs.pop('from_tf', False)
        force_download = kwargs.pop('force_download', False)
        resume_download = kwargs.pop('resume_download', False)
        proxies = kwargs.pop('proxies', None)
        local_files_only = kwargs.pop('local_files_only', False)
        if not isinstance(config, Config):
            config_path = config if config is not None else pretrained_model_name_or_path
            config = Config.from_pretrained(config_path, cache_dir=cache_dir, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only)
        if pretrained_model_name_or_path is not None:
            if os.path.isdir(pretrained_model_name_or_path):
                if os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):
                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)
                else:
                    raise OSError('Error no file named {} found in directory {} '.format(WEIGHTS_NAME, pretrained_model_name_or_path))
            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):
                archive_file = pretrained_model_name_or_path
            elif os.path.isfile(pretrained_model_name_or_path + '.index'):
                assert from_tf, 'We found a TensorFlow checkpoint at {}, please set from_tf                 to True to load from this checkpoint'.format(pretrained_model_name_or_path + '.index')
                archive_file = pretrained_model_name_or_path + '.index'
            try:
                resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only)
                if resolved_archive_file is None:
                    raise OSError
            except OSError:
                msg = f"Can't load weights for '{pretrained_model_name_or_path}'."
                raise OSError(msg)
            if resolved_archive_file == archive_file:
                None
            else:
                None
        else:
            resolved_archive_file = None
        model = cls(config)
        if state_dict is None:
            try:
                try:
                    state_dict = torch.load(resolved_archive_file, map_location='cpu')
                except Exception:
                    state_dict = load_checkpoint(resolved_archive_file)
            except Exception:
                raise OSError('Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 ' + 'checkpoint, please set from_tf=True. ')
        missing_keys = []
        unexpected_keys = []
        error_msgs = []
        old_keys = []
        new_keys = []
        for key in state_dict.keys():
            new_key = None
            if 'gamma' in key:
                new_key = key.replace('gamma', 'weight')
            if 'beta' in key:
                new_key = key.replace('beta', 'bias')
            if new_key:
                old_keys.append(key)
                new_keys.append(new_key)
        for old_key, new_key in zip(old_keys, new_keys):
            state_dict[new_key] = state_dict.pop(old_key)
        metadata = getattr(state_dict, '_metadata', None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata
        model_to_load = model
        model_to_load.load_state_dict(state_dict)
        if model.__class__.__name__ != model_to_load.__class__.__name__:
            base_model_state_dict = model_to_load.state_dict().keys()
            head_model_state_dict_without_base_prefix = [key.split(cls.base_model_prefix + '.')[-1] for key in model.state_dict().keys()]
            missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)
        if len(unexpected_keys) > 0:
            None
        else:
            None
        if len(missing_keys) > 0:
            None
        else:
            None
        if len(error_msgs) > 0:
            raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(model.__class__.__name__, '\n\t'.join(error_msgs)))
        model.eval()
        return model

    def forward(self, images, image_shapes, gt_boxes=None, proposals=None, scales_yx=None, **kwargs):
        """
        kwargs:
            max_detections (int), return_tensors {"np", "pt", None}, padding {None,
            "max_detections"}, pad_value (int), location = {"cuda", "cpu"}
        """
        if self.training:
            raise NotImplementedError()
        return self.inference(images=images, image_shapes=image_shapes, gt_boxes=gt_boxes, proposals=proposals, scales_yx=scales_yx, **kwargs)

    @torch.no_grad()
    def inference(self, images, image_shapes, gt_boxes=None, proposals=None, scales_yx=None, **kwargs):
        original_sizes = image_shapes * scales_yx
        features = self.backbone(images)
        if proposals is None:
            proposal_boxes, _ = self.proposal_generator(images, image_shapes, features, gt_boxes)
        else:
            assert proposals is not None
        obj_logits, attr_logits, box_deltas, feature_pooled = self.roi_heads(features, proposal_boxes, gt_boxes)
        boxes, classes, class_probs, attrs, attr_probs, roi_features = self.roi_outputs(obj_logits=obj_logits, attr_logits=attr_logits, box_deltas=box_deltas, pred_boxes=proposal_boxes, features=feature_pooled, sizes=image_shapes, scales=scales_yx)
        subset_kwargs = {'max_detections': kwargs.get('max_detections', None), 'return_tensors': kwargs.get('return_tensors', None), 'pad_value': kwargs.get('pad_value', 0), 'padding': kwargs.get('padding', None)}
        preds_per_image = torch.tensor([p.size(0) for p in boxes])
        boxes = pad_list_tensors(boxes, preds_per_image, **subset_kwargs)
        classes = pad_list_tensors(classes, preds_per_image, **subset_kwargs)
        class_probs = pad_list_tensors(class_probs, preds_per_image, **subset_kwargs)
        attrs = pad_list_tensors(attrs, preds_per_image, **subset_kwargs)
        attr_probs = pad_list_tensors(attr_probs, preds_per_image, **subset_kwargs)
        roi_features = pad_list_tensors(roi_features, preds_per_image, **subset_kwargs)
        subset_kwargs['padding'] = None
        preds_per_image = pad_list_tensors(preds_per_image, None, **subset_kwargs)
        sizes = pad_list_tensors(image_shapes, None, **subset_kwargs)
        normalized_boxes = norm_box(boxes, original_sizes)
        return OrderedDict({'obj_ids': classes, 'obj_probs': class_probs, 'attr_ids': attrs, 'attr_probs': attr_probs, 'boxes': boxes, 'sizes': sizes, 'preds_per_image': preds_per_image, 'roi_features': roi_features, 'normalized_boxes': normalized_boxes})


class MMFLoss(nn.Module):
    """Internal MMF helper and wrapper class for all Loss classes.
    It makes sure that the value returned from a Loss class is a dict and
    contain proper dataset type in keys, so that it is easy to figure out
    which one is the val loss and which one is train loss.

    For example: it will return ``{"val/vqa2/logit_bce": 27.4}``, in case
    `logit_bce` is used and SampleList is from `val` set of dataset `vqa2`.

    Args:
        params (type): Description of parameter `params`.

    .. note::

        Since, ``MMFLoss`` is used by the ``Losses`` class, end user
        doesn't need to worry about it.
    """

    def __init__(self, params=None):
        super().__init__()
        if params is None:
            params = {}
        is_mapping = isinstance(params, collections.abc.MutableMapping)
        if is_mapping:
            if 'type' not in params:
                raise ValueError("Parameters to loss must have 'type' field tospecify type of loss to instantiate")
            else:
                loss_name = params['type']
        else:
            assert isinstance(params, str), "loss must be a string or dictionary with 'type' key"
            loss_name = params
        self.name = loss_name
        loss_class = registry.get_loss_class(loss_name)
        log_class_usage('Loss', loss_class)
        if loss_class is None:
            raise ValueError(f'No loss named {loss_name} is registered to registry')
        if loss_name.startswith('multi'):
            assert is_mapping
            self.loss_criterion = loss_class(params)
        else:
            if is_mapping:
                loss_params = params.get('params', {})
            else:
                loss_params = {}
            self.loss_criterion = loss_class(**loss_params)

    def forward(self, sample_list: Dict[str, Tensor], model_output: Dict[str, Tensor]):
        loss_dict = {}
        if hasattr(self.loss_criterion, 'datasets'):
            datasets = self.loss_criterion.datasets
            if isinstance(datasets, list) and sample_list['dataset_name'] not in datasets:
                return loss_dict
        loss_result = self.loss_criterion(sample_list, model_output)
        if not isinstance(loss_result, collections.abc.Mapping):
            loss_result = {'': loss_result}
        for child_loss_name, child_loss_result in loss_result.items():
            if not isinstance(child_loss_result, torch.Tensor):
                child_loss_result = torch.tensor(child_loss_result, dtype=torch.float)
            if child_loss_result.dim() == 0:
                child_loss_result = child_loss_result.view(1)
            if not torch.jit.is_scripting():
                key = '{}/{}/{}'.format(sample_list.dataset_type, sample_list.dataset_name, self.name)
            else:
                key = f'{self.name}'
            key = f'{key}/{child_loss_name}' if child_loss_name else key
            loss_dict[key] = child_loss_result
        return loss_dict


def _format_state_key(model: torch.nn.Module, attr: str):
    if hasattr(model, 'format_state_key'):
        formatted_attr = model.format_state_key(attr)
    else:
        formatted_attr = attr
    return formatted_attr


def _should_skip_if_mismatch(shape1: Tuple[str, torch.Size], shape2: Tuple[str, torch.Size], config: Dict[str, Any]) ->None:
    if shape1[1] != shape2[1]:
        message = f"""
            Modules {shape1[0]} and {shape2[0]} don't have the same shape:
            own_attr has shape {shape1[1]} while
            attr has shape {shape2[1]}. This can fail down the line.
            """
        if config.checkpoint.get('bypass_shape_mismatch', False):
            message += 'bypass_shape_mismatch in config.checkpoint '
            message += 'is set to be True, -- so skipping copy'
            logger.warning(message)
            return True
        else:
            logger.warning(message)
    return False


def get_pretrained_state_mapping_checkpoint(checkpoint: Dict[str, Any], model: torch.nn.Module, config: Dict[str, Any]) ->Dict[str, Any]:
    """
    This function gets the checkpoint keys that exists in pretrained state mapping
    that also exist in model's state, and returns a dictionary with the value from the
    `checkpoint` dict.
    """
    mapping = config.checkpoint.pretrained_state_mapping
    own_state = model.state_dict()
    tmp_checkpoint = dict(checkpoint)
    ckpt_update_dict = dict()
    for key, value in mapping.items():
        key += '.'
        value += '.'
        for attr in tmp_checkpoint:
            formatted_attr = _format_state_key(model, attr)
            for own_attr in own_state:
                if key in own_attr and value in formatted_attr and own_attr.replace(key, '') == formatted_attr.replace(value, ''):
                    if _should_skip_if_mismatch((own_attr, own_state[own_attr].shape), (attr, checkpoint[attr].shape), config):
                        continue
                    ckpt_update_dict[own_attr] = attr
    return ckpt_update_dict


def is_pl_trainer_checkpoint(checkpoint):
    return 'pytorch-lightning_version' in checkpoint


def is_model_only_checkpoint(checkpoint):
    if is_pl_trainer_checkpoint(checkpoint):
        return 'state_dict' not in checkpoint
    else:
        return 'model' not in checkpoint


def is_pl_model_checkpoint(checkpoint):
    return 'state_dict' in checkpoint


def remove_keys_inplace(ckpt: Dict[str, Any], keys_to_remove):
    tmp_keys = dict(ckpt)
    for key in tmp_keys:
        if key in keys_to_remove:
            ckpt.pop(key)


class MMFToPLCheckpointUpdater:

    def __init__(self):
        pass

    def update_checkpoint(self, checkpoint: Dict[str, Any], model: torch.nn.Module) ->None:
        """
        This function should only be called on lightning. It handles checkpoint
        update that is being called by LightningModule's `on_load_checkpoint`,
        which should update the checkpoint to the format desired. The logic
        contains two parts, when checkpoint is a model only checkpoint and
        when checkpoint is a trainer checkpoint. This function applies the checkpoint
        update in place.

        If the checkpoint is a model only checkpoint:
            1. If it is an mmf checkpoint, convert to lightning format
                putting it inside a "state_dict" key
            2. Apply the model's format state key to give the model a chance to update
            3. If config.checkpoint.pretrained_state_mapping is True, apply
                the mapping speicified in the config, and remove the keys that exist
                in the checkpoint that do not exist in the mapping.
        The updated checkpoint should be of the format: {"state_dict": ckpts}, where
        ckpts should be the model state_dict.

        If the checkpoint is a trainer only checkpoint:
            1. do the above steps for model checkpoint update
            2. do the checkpoint trainer state update from mmf to lightning
        The updated checkpoint should be of the format: {
            `epoch`: x,
            `global_step`: x,
            `pytorch-lightning_version`: x,
            `state_dict`: x,
            `callbacks`: x,
            `optimizer_states`: [x],
            `lr_schedulers`: [x],
        }
        """
        if is_model_only_checkpoint(checkpoint):
            self._update_model_checkpoint(checkpoint=checkpoint, model=model)
            return
        if not is_pl_trainer_checkpoint(checkpoint):
            self._update_trainer_checkpoint_from_mmf(checkpoint=checkpoint, model=model)

    def _update_trainer_checkpoint_from_mmf(self, checkpoint: Dict[str, Any], model: Any) ->None:
        """updates checkpoint from the mmf format to lightning format.
        mmf checkpoint is with keys:
        `model`, `optimizer`, `best_iteration`, `current_iteration`, `current_epoch`, ,
        `num_updates`, `best_update`, `best_metric_value`, `fp16_scaler`, `config`, ,
        `lr_scheduler`, `git/branch`, `git/commit_hash`, `git/commit_author`,
        `git/commit_message`, `git/diff`
        """
        remove_keys_inplace(checkpoint, {'best_iteration', 'current_iteration', 'best_update', 'best_metric_value', 'fp16_scaler', 'config', 'git/branch', 'git/commit_hash', 'git/commit_author', 'git/commit_message', 'git/diff'})
        if 'model' in checkpoint:
            model_checkpoint = checkpoint.pop('model')
            checkpoint['state_dict'] = model_checkpoint
            self._update_model_format_state_keys(checkpoint['state_dict'], model=model)
            config = registry.get('config')
            if config.checkpoint.get('resume_pretrained', False):
                self._update_pretrained_state_mapping(checkpoint=checkpoint['state_dict'], model=model, config=config)
        if 'optimizer' in checkpoint:
            optimizer = checkpoint.pop('optimizer')
            checkpoint['optimizer_states'] = [optimizer]
        if 'lr_scheduler' in checkpoint:
            lr_scheduler = checkpoint.pop('lr_scheduler')
            checkpoint['lr_schedulers'] = [lr_scheduler]
        else:
            checkpoint['lr_schedulers'] = []
        if 'num_updates' in checkpoint:
            global_step = checkpoint.pop('num_updates')
            checkpoint['global_step'] = global_step
        if 'current_epoch' in checkpoint:
            epoch = checkpoint.pop('current_epoch')
            checkpoint['epoch'] = epoch

    def _update_model_checkpoint(self, checkpoint: Dict[str, Any], model: torch.nn.Module) ->None:
        """
        This function assumes the checkpoint is just the model and does not include
        training params.
        """
        if not is_pl_model_checkpoint(checkpoint):
            self._update_model_checkpoint_from_mmf(checkpoint)
        self._update_model_format_state_keys(checkpoint['state_dict'], model=model)
        config = registry.get('config')
        if config.checkpoint.get('resume_pretrained', False):
            self._update_pretrained_state_mapping(checkpoint=checkpoint['state_dict'], model=model, config=config)

    def _update_pretrained_state_mapping(self, checkpoint: Dict[str, Any], model: torch.nn.Module, config: Dict[str, Any]) ->None:
        """
        This function removes all checkpoint keys that do not exist in
        the `pretrained_state_mapping`
        """
        ckpt_update_dict = get_pretrained_state_mapping_checkpoint(checkpoint=checkpoint, model=model, config=config)
        accepted_keys = set()
        for own_attr, attr in ckpt_update_dict.items():
            assert own_attr == attr, ('Since `_update_model_format_state_keys` was run ', 'before, this has to be held true')
            logger.info('Copying ' + own_attr + ' from ' + attr)
            accepted_keys.add(attr)
        tmp_checkpoint = dict(checkpoint)
        for key in tmp_checkpoint:
            if key not in accepted_keys:
                checkpoint.pop(key)

    def _update_model_format_state_keys(self, checkpoint: Dict[str, Any], model: torch.nn.Module) ->None:
        """
        Function to rewrite the checkpoint in place to give the model a chance
        to update state_dict keys. This assumes that checkpoint is the
        model's state_dict.
        """
        tmp_state_dict = dict(checkpoint)
        for attr in tmp_state_dict:
            new_attr = _format_state_key(model, attr)
            if attr != new_attr:
                logger.info(f'checkpoint: rewriting {attr} into {new_attr}')
                value = checkpoint.pop(attr)
                checkpoint[new_attr] = value

    def _update_model_checkpoint_from_mmf(self, checkpoint: Dict[str, Any]) ->None:
        tmp_checkpoint = dict(checkpoint)
        checkpoint.clear()
        checkpoint['state_dict'] = tmp_checkpoint


class Sample(OrderedDict):
    """Sample represent some arbitrary data. All datasets in MMF must
    return an object of type ``Sample``.

    Args:
        init_dict (Dict): Dictionary to init ``Sample`` class with.

    Usage::

        >>> sample = Sample({"text": torch.tensor(2)})
        >>> sample.text.zero_()
        # Custom attributes can be added to ``Sample`` after initialization
        >>> sample.context = torch.tensor(4)
    """

    def __init__(self, init_dict=None):
        if init_dict is None:
            init_dict = {}
        super().__init__(init_dict)

    def __setattr__(self, key, value):
        if isinstance(value, collections.abc.Mapping):
            value = Sample(value)
        self[key] = value

    def __setitem__(self, key, value):
        if isinstance(value, collections.abc.Mapping):
            value = Sample(value)
        super().__setitem__(key, value)

    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError:
            raise AttributeError(key)

    def fields(self):
        """Get current attributes/fields registered under the sample.

        Returns:
            List[str]: Attributes registered under the Sample.

        """
        return list(self.keys())


def detach_tensor(tensor: Any) ->Any:
    """Detaches any element passed which has a `.detach` function defined.
    Currently, in MMF can be SampleList, Report or a tensor.

    Args:
        tensor (Any): Item to be detached

    Returns:
        Any: Detached element
    """
    if hasattr(tensor, 'detach'):
        tensor = tensor.detach()
    return tensor


class SampleList(OrderedDict):
    """``SampleList`` is used to collate a list of ``Sample`` into a batch during batch
    preparation. It can be thought of as a merger of list of Dicts into a single Dict.

    If ``Sample`` contains an attribute 'text' of size (2) and there are 10 samples in
    list, the returned ``SampleList`` will have an attribute 'text' which is a tensor
    of size (10, 2).

    Args:
        samples (type): List of ``Sample`` from which the ``SampleList``
                        will be created.

    Usage::

        >>> sample_list = [
                Sample({"text": torch.tensor(2)}),
                Sample({"text": torch.tensor(2)})
            ]
        >>> sample_list.text
        torch.tensor([2, 2])
    """
    _TENSOR_FIELD_ = '_tensor_field'

    def __init__(self, samples=None):
        super().__init__(self)
        if samples is None:
            samples = []
        if len(samples) == 0:
            return
        if self._check_and_load_dict(samples):
            return
        if self._check_and_load_tuple(samples):
            return
        fields = samples[0].keys()
        for field in fields:
            if isinstance(samples[0][field], torch.Tensor):
                size = len(samples), *samples[0][field].size()
                self[field] = samples[0][field].new_empty(size)
                if self._get_tensor_field() is None:
                    self._set_tensor_field(field)
            else:
                self[field] = [None for _ in range(len(samples))]
            for idx, sample in enumerate(samples):
                if isinstance(sample[field], torch.Tensor) and len(sample[field].size()) != 0 and sample[field].size(0) != samples[0][field].size(0):
                    raise AssertionError('Fields for all samples must be equally sized. {} is of different sizes'.format(field))
                self[field][idx] = self._get_data_copy(sample[field])
            if isinstance(samples[0][field], collections.abc.Mapping):
                self[field] = SampleList(self[field])

    def _check_and_load_tuple(self, samples):
        if isinstance(samples[0], (tuple, list)) and isinstance(samples[0][0], str):
            for kv_pair in samples:
                self.add_field(kv_pair[0], kv_pair[1])
            return True
        else:
            return False

    def _check_and_load_dict(self, samples):
        if isinstance(samples, collections.abc.Mapping):
            for key, value in samples.items():
                self.add_field(key, value)
            return True
        else:
            return False

    def _fix_sample_type(self, samples):
        if not isinstance(samples[0], Sample):
            proper_samples = []
            for sample in samples:
                proper_samples.append(Sample(sample))
            samples = proper_samples
        return samples

    def __setattr__(self, key, value):
        self[key] = value

    def __getattr__(self, key):
        if key not in self:
            raise AttributeError('Key {} not found in the SampleList. Valid choices are {}'.format(key, self.fields()))
        fields = self.keys()
        if key in fields:
            return self[key]
        sample = Sample()
        for field in fields:
            sample[field] = self[field][key]
        return sample

    def get_device(self):
        field_tensor = self._get_tensor_field()
        assert field_tensor is not None, f'No tensor field in sample list, available keys: {self.fields()}'
        return self[field_tensor].device

    def get_item_list(self, key):
        """Get ``SampleList`` of only one particular attribute that is present
        in the ``SampleList``.

        Args:
            key (str): Attribute whose ``SampleList`` will be made.

        Returns:
            SampleList: SampleList containing only the attribute value of the key
            which was passed.

        """
        sample = self[key]
        return SampleList([sample])

    def copy(self):
        """Get a copy of the current SampleList

        Returns:
            SampleList: Copy of current SampleList.

        """
        sample_list = SampleList()
        fields = self.fields()
        for field in fields:
            sample_list.add_field(field, self[field])
        return sample_list

    def fields(self):
        """Get current attributes/fields registered under the SampleList.

        Returns:
            List[str]: list of attributes of the SampleList.

        """
        return list(self.keys())

    def get_fields(self, fields):
        """Get a new ``SampleList`` generated from the current ``SampleList``
        but contains only the attributes passed in `fields` argument

        Args:
            fields (List[str]): Attributes whose ``SampleList`` will be made.

        Returns:
            SampleList: SampleList containing only the attribute values of the fields
            which were passed.

        """
        current_fields = self.fields()
        return_list = SampleList()
        for field in fields:
            if field not in current_fields:
                raise AttributeError('{} not present in SampleList. Valid choices are {}'.format(field, current_fields))
            return_list.add_field(field, self[field])
        return return_list

    def get_field(self, field):
        """Get value of a particular attribute

        Args:
            field (str): Attribute whose value is to be returned.
        """
        return self[field]

    def _get_data_copy(self, data):
        return data

    def _get_tensor_field(self):
        return self.__dict__.get(SampleList._TENSOR_FIELD_, None)

    def _set_tensor_field(self, value):
        self.__dict__[SampleList._TENSOR_FIELD_] = value

    def get_batch_size(self):
        """Get batch size of the current ``SampleList``. There must be a tensor
        be a tensor present inside sample list to use this function.
        Returns:
            int: Size of the batch in ``SampleList``.

        """
        tensor_field = self._get_tensor_field()
        assert tensor_field is not None, 'There is no tensor yet in SampleList'
        return self[tensor_field].size(0)

    def add_field(self, field, data):
        """Add an attribute ``field`` with value ``data`` to the SampleList

        Args:
            field (str): Key under which the data will be added.
            data (object): Data to be added, can be a ``torch.Tensor``, ``list``
                         or ``Sample``
        """
        fields = self.fields()
        tensor_field = self._get_tensor_field()
        if len(fields) != 0 and isinstance(data, torch.Tensor) and len(data.size()) != 0 and tensor_field is not None and data.size(0) != self[tensor_field].size(0):
            raise AssertionError('A tensor field to be added must have same size as existing tensor fields in SampleList. Passed size: {}, Required size: {}'.format(len(data), len(self[tensor_field])))
        if isinstance(data, collections.abc.Mapping):
            self[field] = SampleList(data)
        else:
            self[field] = self._get_data_copy(data)
            if isinstance(self[field], torch.Tensor) and tensor_field is None:
                self._set_tensor_field(field)

    def to(self, device, non_blocking=True):
        """Similar to ``.to`` function on a `torch.Tensor`. Moves all of the
        tensors present inside the ``SampleList`` to a particular device. If an
        attribute's value is not a tensor, it is ignored and kept as it is.

        Args:
            device (str|torch.device): Device on which the ``SampleList`` should
                                       moved.
            non_blocking (bool): Whether the move should be non_blocking. Default: True

        Returns:
            SampleList: a SampleList moved to the ``device``.

        """
        fields = self.keys()
        sample_list = self.copy()
        if not isinstance(device, torch.device):
            if not isinstance(device, str):
                raise TypeError("device must be either 'str' or 'torch.device' type, {} found".format(type(device)))
            device = torch.device(device)
        for field in fields:
            if hasattr(sample_list[field], 'to'):
                sample_list[field] = sample_list[field]
        return sample_list

    def pin_memory(self):
        """In custom batch object, we need to define pin_memory function so that
        PyTorch can actually apply pinning. This function just individually pins
        all of the tensor fields
        """
        fields = self.keys()
        for field in fields:
            if hasattr(self[field], 'pin_memory'):
                self[field] = self[field].pin_memory()
        return self

    def detach(self):
        fields = self.keys()
        for field in fields:
            self[field] = detach_tensor(self[field])
        return self

    def to_dict(self) ->Dict[str, Any]:
        """Converts a sample list to dict, this is useful for TorchScript and for
        other internal API unification efforts.

        Returns:
            Dict[str, Any]: A dict representation of current sample list
        """
        sample_dict = {}
        fields = self.keys()
        for field in fields:
            if hasattr(self[field], 'to_dict'):
                sample_dict[field] = self[field].to_dict()
            else:
                sample_dict[field] = self[field]
        return sample_dict


class Report(OrderedDict):

    def __init__(self, batch: SampleList=None, model_output: Dict[str, Any]=None, *args):
        super().__init__(self)
        if batch is None:
            return
        if model_output is None:
            model_output = {}
        if self._check_and_load_tuple(batch):
            return
        all_args = [batch, model_output] + [*args]
        for idx, arg in enumerate(all_args):
            if not isinstance(arg, collections.abc.Mapping):
                raise TypeError('Argument {:d}, {} must be of instance of collections.abc.Mapping'.format(idx, arg))
        self.batch_size = batch.get_batch_size()
        self.warning_string = 'Updating forward report with key {}{}, but it already exists in {}. Please consider using a different key, as this can cause issues during loss and metric calculations.'
        for idx, arg in enumerate(all_args):
            for key, item in arg.items():
                if key in self and idx >= 2:
                    log = self.warning_string.format(key, '', 'in previous arguments to report')
                    warnings.warn(log)
                self[key] = item

    def get_batch_size(self) ->int:
        return self.batch_size

    @property
    def batch_size(self) ->int:
        return self._batch_size

    @batch_size.setter
    def batch_size(self, batch_size: int):
        self._batch_size = batch_size

    def _check_and_load_tuple(self, batch):
        if isinstance(batch, collections.abc.Mapping):
            return False
        if isinstance(batch[0], (tuple, list)) and isinstance(batch[0][0], str):
            for kv_pair in batch:
                self[kv_pair[0]] = kv_pair[1]
            return True
        else:
            return False

    def __setattr__(self, key: str, value: Any):
        self[key] = value

    def __getattr__(self, key: str):
        try:
            return self[key]
        except KeyError:
            raise AttributeError(key)

    def fields(self) ->List[str]:
        return list(self.keys())

    def apply_fn(self, fn: Callable, fields: Optional[List[str]]=None):
        """Applies a function `fn` on all items in a report. Can apply to specific
        fields if `fields` parameter is passed

        Args:
            fn (Callable): A callable to called on each item in report
            fields (List[str], optional): Use to apply on specific fields.
                Defaults to None.

        Returns:
            Report: Update report after apply fn
        """
        for key in self.keys():
            if fields is not None and isinstance(fields, (list, tuple)):
                if key not in fields:
                    continue
            self[key] = fn(self[key])
            if isinstance(self[key], collections.abc.MutableSequence):
                for idx, item in enumerate(self[key]):
                    self[key][idx] = fn(item)
            elif isinstance(self[key], dict):
                for subkey in self[key].keys():
                    self[key][subkey] = fn(self[key][subkey])
        return self

    def detach(self) ->'Report':
        """Similar to tensor.detach, detach all items in a report from their graphs.
        This is useful in clearing up memory sometimes.

        Returns:
            Report: Detached report is returned back.
        """
        return self.apply_fn(detach_tensor)

    def to(self, device: Union[torch.device, str], non_blocking: bool=True, fields: Optional[List[str]]=None):
        """Move report to a specific device defined 'device' parameter.
        This is similar to how one moves a tensor or sample_list to a device

        Args:
            device (torch.device): Device can be str defining device or torch.device
            non_blocking (bool, optional): Whether transfer should be non_blocking.
                Defaults to True.
            fields (List[str], optional): Use this is you only want to move some
                specific fields to the device instead of full report. Defaults to None.

        Raises:
            TypeError: If device type is not correct

        Returns:
            Report: Updated report is returned back
        """
        if not isinstance(device, torch.device):
            if not isinstance(device, str):
                raise TypeError("device must be either 'str' or 'torch.device' type, {} found".format(type(device)))
            device = torch.device(device)

        def fn(x):
            if hasattr(x, 'to'):
                x = x
            return x
        return self.apply_fn(fn, fields)

    def accumulate_tensor_fields_and_loss(self, report: 'Report', field_list: List[str]):
        for key in field_list:
            if key == '__prediction_report__':
                continue
            if key not in self.keys():
                warnings.warn(f'{key} not found in report. Metrics calculation ' + 'might not work as expected.')
                continue
            if isinstance(self[key], torch.Tensor):
                self[key] = torch.cat((self[key], report[key]), dim=0)
            elif isinstance(self[key], List):
                self[key].extend(report[key])
        self._accumulate_loss(report)

    def _accumulate_loss(self, report: 'Report'):
        for key, value in report.losses.items():
            if key not in self.losses.keys():
                warnings.warn(f'{key} not found in report. Loss calculation ' + 'might not work as expected.')
                self.losses[key] = value
            if isinstance(self.losses[key], torch.Tensor):
                self.losses[key] += value

    def copy(self) ->'Report':
        """Get a copy of the current Report

        Returns:
            Report: Copy of current Report.

        """
        report = Report()
        fields = self.fields()
        for field in fields:
            report[field] = copy.deepcopy(self[field])
        return report


def built(path, version_string=None):
    """
    Check if '.built' flag has been set for that task.

    If a version_string is provided, this has to match, or the version
    is regarded as not built.

    Version_string are generally the dataset version + the date the file was
    last updated. If this doesn't match, dataset will be mark not built. This makes
    sure that if we update our features or anything else features are updated
    for the end user.
    """
    if version_string:
        fname = os.path.join(path, '.built.json')
        if not PathManager.isfile(fname):
            return False
        else:
            with PathManager.open(fname, 'r') as read:
                text = json.load(read)
            return text.get('version', None) == version_string
    else:
        return PathManager.isfile(os.path.join(path, '.built.json'))


def decompress(path, fname, delete_original=True):
    """
    Unpack the given archive file to the same directory.

    Args:
        path(str): The folder containing the archive. Will contain the contents.
        fname (str): The filename of the archive file.
        delete_original (bool, optional): If true, the archive will be deleted
                                          after extraction. Default to True.
    """
    None
    fullpath = os.path.join(path, fname)
    shutil.unpack_archive(fullpath, path)
    if delete_original:
        os.remove(fullpath)


def _get_confirm_token(response):
    for key, value in response.cookies.items():
        if key.startswith('download_warning'):
            return value
    return None


def download_from_google_drive(gd_id, destination, redownload=True):
    """
    Use the requests package to download a file from Google Drive.
    """
    download = not PathManager.isfile(destination) or redownload
    URL = 'https://docs.google.com/uc?export=download'
    if not download:
        return download
    else:
        check_header(gd_id, from_google=True)
    with requests.Session() as session:
        response = session.get(URL, params={'id': gd_id}, stream=True)
        token = _get_confirm_token(response)
        if token:
            response.close()
            params = {'id': gd_id, 'confirm': token}
            response = session.get(URL, params=params, stream=True)
        CHUNK_SIZE = 32768
        with PathManager.open(destination, 'wb') as f:
            for chunk in response.iter_content(CHUNK_SIZE):
                if chunk:
                    f.write(chunk)
        response.close()
    return download


def make_dir(path):
    """
    Make the directory and any nonexistent parent directories (`mkdir -p`).
    """
    if path != '':
        PathManager.mkdirs(path)


class DownloadableFile:
    """
    A class used to abstract any file that has to be downloaded online.

    Originally taken from ParlAI, this file has been modified for MMF specific
    use cases.

    Any dataset/model that needs to download a file needs to have a list RESOURCES
    that have objects of this class as elements.

    The class automatically figures out if the file is from Google Drive.

    This class provides the following functionality:

    - Download a file from a URL / Google Drive
    - Decompress the file if compressed
    - Checksum for the downloaded file
    - Send HEAD request to validate URL or Google Drive link
    - If the file is present and checksum is same, it won't be redownloaded

    Raises:
        AssertionError: If while downloading checksum of the files fails.
    """
    GOOGLE_DRIVE_SUBSTR = 'drive.google'
    MMF_PREFIX = 'mmf://'
    MMF_PREFIX_REPLACEMENT = 'https://dl.fbaipublicfiles.com/mmf/data/'

    def __init__(self, url, file_name, hashcode=None, compressed=True, delete_original=False, dest_folder=None):
        """
        An object of this class needs to be created with:

        Args:
            url (string): URL or Google Drive id to download from
            file_name (string): File name that the file should be named
            hashcode (string, optional): SHA256 hashcode of the downloaded file.
                                         Defaults to None. Won't be checked if not
                                         passed.
            compressed (bool, optional): False if the file is not compressed.
                                         Defaults to True.
            delete_original (bool, optional): If compressed whether to delete original.
                                              Defaults to False.
            dest_folder (str, optional): Folder which will be appended to destination
                path provided when downloading. Defaults to None.
        """
        self._url = self._parse_url(url)
        self._file_name = file_name
        self._hashcode = hashcode
        self._compressed = compressed
        self._from_google = self._url.find(self.GOOGLE_DRIVE_SUBSTR) != -1
        if self._from_google:
            assert 'id=' in self._url, 'Google Drive URL should have Google Drive ID'
            self._url = self._url.split('=')[-1]
        self._delete_original = delete_original
        self._dest_folder = dest_folder

    def _parse_url(self, url):
        if url.find(self.MMF_PREFIX) == -1:
            return url
        else:
            return self.MMF_PREFIX_REPLACEMENT + url[len(self.MMF_PREFIX):]

    def checksum(self, download_path):
        """
        Checksum on a given file.

        Args:
            download_path (string): path to the downloaded file.
        """
        if self._hashcode is None:
            None
            return
        sha256_hash = hashlib.sha256()
        destination = os.path.join(download_path, self._file_name)
        if not PathManager.isfile(destination):
            return
        with PathManager.open(destination, 'rb') as f:
            None
            for byte_block in iter(lambda : f.read(65536), b''):
                sha256_hash.update(byte_block)
            if sha256_hash.hexdigest() != self._hashcode:
                raise AssertionError(f'[ Checksum for {self._file_name} from \n{self._url}\ndoes not match the expected checksum. Please try again. ]')
            else:
                None

    def download_file(self, download_path):
        downloaded = False
        redownload = False
        if self._dest_folder is not None:
            download_path = str(Path(f'{download_path}/{self._dest_folder}'))
            make_dir(download_path)
        try:
            self.checksum(download_path)
        except AssertionError:
            None
            redownload = True
        if self._from_google:
            downloaded = download_from_google_drive(self._url, os.path.join(download_path, self._file_name), redownload=redownload)
        else:
            downloaded = download(self._url, download_path, self._file_name, redownload=redownload)
        if downloaded:
            self.checksum(download_path)
            if self._compressed:
                decompress(download_path, self._file_name, self._delete_original)


def download_resource(resource, download_path):
    if isinstance(resource, collections.abc.Mapping):
        resource = DownloadableFile(**resource)
    assert isinstance(resource, DownloadableFile)
    resource.download_file(download_path)


def mark_done(path, version_string=None):
    """
    Mark this path as prebuilt.

    Marks the path as done by adding a '.built' file with the current timestamp
    plus a version description string if specified.

    Args:
        path (str): The file path to mark as built
        version_string (str): The version of this dataset
    """
    data = {}
    data['created_at'] = str(datetime.datetime.today())
    data['version'] = version_string
    with PathManager.open(os.path.join(path, '.built.json'), 'w') as f:
        json.dump(data, f)


def download_resources(resources, download_path, version):
    is_built = built(download_path, version_string=version)
    if not is_built:
        make_dir(download_path)
        if not isinstance(resources, collections.abc.Sequence):
            resources = [resources]
        if len(resources) == 0:
            return
        for resource in resources:
            download_resource(resource, download_path)
        mark_done(download_path, version_string=version)


def download_pretrained_model(model_name, *args, **kwargs):
    model_zoo = load_yaml(get_mmf_env(key='model_zoo'))
    OmegaConf.set_struct(model_zoo, True)
    OmegaConf.set_readonly(model_zoo, True)
    data_dir = get_absolute_path(get_mmf_env('data_dir'))
    model_data_dir = os.path.join(data_dir, 'models')
    download_path = os.path.join(model_data_dir, model_name)
    try:
        model_config = OmegaConf.select(model_zoo, model_name)
    except omegaconf.errors.OmegaConfBaseException as e:
        None
        raise e
    if 'version' not in model_config or 'resources' not in model_config:
        try:
            model_config = model_config.defaults
            download_path = os.path.join(model_data_dir, model_name + '.defaults')
        except omegaconf.errors.OmegaConfBaseException as e:
            None
            raise e
    if 'zoo_requirements' in model_config:
        requirements = model_config.zoo_requirements
        if isinstance(requirements, str):
            requirements = [requirements]
        for item in requirements:
            download_pretrained_model(item, *args, **kwargs)
    version = model_config.version
    resources = model_config.resources
    download_resources(resources, download_path, version)
    return download_path


def is_xla():
    return not not registry.get('is_xla', no_warning=True)


def get_current_device():
    if is_xla():
        return xm.xla_device()
    if torch.cuda.is_available() and torch.cuda.is_initialized():
        return f'cuda:{torch.cuda.current_device()}'
    else:
        return torch.device('cpu')


ALLOWED_CHECKPOINT_EXTS = ['.ckpt', '.pth', '.pt']


def _hack_imports():
    sys.modules['pythia'] = importlib.import_module('mmf')
    sys.modules['pythia.utils.configuration'] = importlib.import_module('mmf.utils.configuration')


def _load_pretrained_checkpoint(checkpoint_path, *args, **kwargs):
    assert os.path.splitext(checkpoint_path)[1] in ALLOWED_CHECKPOINT_EXTS, f'Checkpoint must have extensions: {ALLOWED_CHECKPOINT_EXTS}'
    _hack_imports()
    with PathManager.open(checkpoint_path, 'rb') as f:
        ckpt = torch.load(f, map_location=lambda storage, loc: storage)
    assert 'config' in ckpt, "No configs provided with pretrained model  while checkpoint also doesn't have configuration."
    config = ckpt.pop('config', None)
    model_config = config.get('model_config', config)
    ckpt = ckpt.get('model', ckpt)
    if 'model_name' in kwargs:
        model_name = kwargs['model_name']
    else:
        assert len(model_config.keys()) == 1, 'Only one model type should be specified.'
        model_name = list(model_config.keys())[0]
    model_config = model_config.get(model_name)
    return {'config': model_config, 'checkpoint': ckpt, 'full_config': config}


def get_ckpt_from_path(path) ->Dict[str, Any]:
    with PathManager.open(path, 'rb') as f:
        ckpt = torch.load(f, map_location=lambda storage, loc: storage)
        return ckpt


def get_ckpt_path_from_folder(folder) ->str:
    ckpts = []
    allowed_ckpt_types = [f'*{ext}' for ext in ALLOWED_CHECKPOINT_EXTS]
    for ckpt_type in allowed_ckpt_types:
        ckpts.extend(glob.glob(os.path.join(folder, ckpt_type)))
    assert len(ckpts) == 1, "None or multiple checkpoints files. MMF doesn't know what to do."
    return ckpts[0]


def get_config_from_folder_or_ckpt(folder: str, ckpt: Dict[str, Any]=None) ->Dict[str, Any]:
    """gets config from folder or checkpoint

    Args:
        folder (str): folder from which config will be searched first
        ckpt (Optional[Dict[str, Any]]): optional checkpoint from which config
            might be found.

    Returns:
        config (Dict[str, Any]): config object
    """
    configs = glob.glob(os.path.join(folder, '*.yaml'))
    if len(configs) > 0:
        assert len(configs) <= 1, 'Multiple yaml files with the pretrained model. ' + "MMF doesn't know what to do."
        config_file = configs[0]
        config = load_yaml(config_file)
    else:
        assert 'config' in ckpt, "No configs provided with pretrained model while checkpoint also doesn't have configuration."
        config = ckpt['config']
    return config


def _load_pretrained_model(model_name_or_path, *args, **kwargs):
    if PathManager.exists(model_name_or_path):
        download_path = model_name_or_path
        model_name = model_name_or_path
    else:
        download_path = download_pretrained_model(model_name_or_path, *args, **kwargs)
        model_name = model_name_or_path
    _hack_imports()
    ckpt_path = get_ckpt_path_from_folder(download_path)
    ckpt = get_ckpt_from_path(ckpt_path)
    config = get_config_from_folder_or_ckpt(download_path, ckpt)
    model_config = config.get('model_config', config)
    ckpt = ckpt.get('model', ckpt)
    if PathManager.exists(model_name):
        assert len(model_config.keys()) == 1, 'Checkpoint contains more than one model?'
        model_config = model_config[list(model_config.keys())[0]]
    else:
        model_config = model_config.get(model_name.split(os.path.sep)[-1].split('.')[0])
    return {'config': model_config, 'checkpoint': ckpt, 'full_config': config}


def load_pretrained_model(model_name_or_path_or_checkpoint, *args, **kwargs):
    if PathManager.isfile(model_name_or_path_or_checkpoint):
        return _load_pretrained_checkpoint(model_name_or_path_or_checkpoint, args, kwargs)
    else:
        return _load_pretrained_model(model_name_or_path_or_checkpoint, args, kwargs)


def convert_batch_to_sample_list(batch: Union[SampleList, Dict[str, Any]]) ->SampleList:
    sample_list = batch
    if isinstance(batch, list) and len(batch) == 1 and isinstance(batch[0], SampleList):
        sample_list = batch[0]
    elif not isinstance(batch, SampleList):
        sample_list = SampleList(batch)
    if sample_list._get_tensor_field() is None:
        sample_list = SampleList(sample_list.to_dict())
    return sample_list


device_type = Union[str, torch.device]


def to_device(sample_list: Union[SampleList, Dict[str, Any]], device: device_type='cuda') ->SampleList:
    if isinstance(sample_list, collections.abc.Mapping):
        sample_list = convert_batch_to_sample_list(sample_list)
    if not isinstance(sample_list, SampleList):
        warnings.warn('You are not returning SampleList/Sample from your dataset. MMF expects you to move your tensors to cuda yourself.')
        return sample_list
    if isinstance(device, str):
        device = torch.device(device)
    if device.type == 'cuda' and not torch.cuda.is_available():
        warnings.warn('Selected device is cuda, but it is NOT available!!! Falling back on cpu.')
        device = torch.device('cpu')
    if sample_list.get_device() != device:
        sample_list = sample_list
    return sample_list


class GraphPtrNet(nn.Module):

    def __init__(self, hidden_size, graph_hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.graph_hidden_size = graph_hidden_size
        self.bl_w = nn.Linear(hidden_size, hidden_size)
        self.graph_w = nn.Linear(graph_hidden_size, hidden_size)

    def forward(self, bl_hidden, graph_hidden):
        bl_hidden = self.bl_w(bl_hidden)
        assert bl_hidden.dim() == 2
        bl_hidden = bl_hidden.unsqueeze(1)
        graph_hidden = self.graph_w(graph_hidden)
        scores = torch.matmul(bl_hidden, graph_hidden.transpose(-1, -2))
        scores = scores / math.sqrt(self.hidden_size)
        scores = scores.squeeze(1)
        return scores


class BertBiAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        if config.bi_hidden_size % config.bi_num_attention_heads != 0:
            raise ValueError('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.bi_hidden_size, config.bi_num_attention_heads))
        self.visualization = config.visualization
        self.num_attention_heads = config.bi_num_attention_heads
        self.attention_head_size = int(config.bi_hidden_size / config.bi_num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.query1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.key1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.value1 = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.dropout1 = nn.Dropout(config.v_attention_probs_dropout_prob)
        self.query2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.key2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.value2 = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout2 = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, input_tensor1: Tensor, attention_mask1: Tensor, input_tensor2: Tensor, attention_mask2: Tensor, co_attention_mask: Optional[Tensor]=None, use_co_attention_mask: bool=False) ->Tuple[Tensor, Tensor, Dict[str, Tensor]]:
        mixed_query_layer1 = self.query1(input_tensor1)
        mixed_key_layer1 = self.key1(input_tensor1)
        mixed_value_layer1 = self.value1(input_tensor1)
        query_layer1 = self.transpose_for_scores(mixed_query_layer1)
        key_layer1 = self.transpose_for_scores(mixed_key_layer1)
        value_layer1 = self.transpose_for_scores(mixed_value_layer1)
        mixed_query_layer2 = self.query2(input_tensor2)
        mixed_key_layer2 = self.key2(input_tensor2)
        mixed_value_layer2 = self.value2(input_tensor2)
        query_layer2 = self.transpose_for_scores(mixed_query_layer2)
        key_layer2 = self.transpose_for_scores(mixed_key_layer2)
        value_layer2 = self.transpose_for_scores(mixed_value_layer2)
        attention_scores1 = torch.matmul(query_layer2, key_layer1.transpose(-1, -2))
        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)
        attention_scores1 = attention_scores1 + attention_mask1
        attention_probs1 = nn.functional.softmax(attention_scores1, dim=-1)
        attention_probs1 = self.dropout1(attention_probs1)
        context_layer1 = torch.matmul(attention_probs1, value_layer1)
        context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)
        context_layer1 = context_layer1.view(new_context_layer_shape1)
        attention_scores2 = torch.matmul(query_layer1, key_layer2.transpose(-1, -2))
        attention_scores2 = attention_scores2 / math.sqrt(self.attention_head_size)
        attention_scores2 = attention_scores2 + attention_mask2
        attention_probs2 = nn.functional.softmax(attention_scores2, dim=-1)
        attention_probs2 = self.dropout2(attention_probs2)
        context_layer2 = torch.matmul(attention_probs2, value_layer2)
        context_layer2 = context_layer2.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape2 = context_layer2.size()[:-2] + (self.all_head_size,)
        context_layer2 = context_layer2.view(new_context_layer_shape2)
        attn_data = {}
        if self.visualization:
            attn_data = {'attn1': attention_probs1, 'queries1': query_layer2, 'keys1': key_layer1, 'attn2': attention_probs2, 'querues2': query_layer1, 'keys2': key_layer2}
        return context_layer1, context_layer2, attn_data


class BertBiOutput(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense1 = nn.Linear(config.bi_hidden_size, config.v_hidden_size)
        self.LayerNorm1 = nn.LayerNorm(config.v_hidden_size, eps=1e-12)
        self.dropout1 = nn.Dropout(config.v_hidden_dropout_prob)
        self.q_dense1 = nn.Linear(config.bi_hidden_size, config.v_hidden_size)
        self.q_dropout1 = nn.Dropout(config.v_hidden_dropout_prob)
        self.dense2 = nn.Linear(config.bi_hidden_size, config.hidden_size)
        self.LayerNorm2 = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout2 = nn.Dropout(config.hidden_dropout_prob)
        self.q_dense2 = nn.Linear(config.bi_hidden_size, config.hidden_size)
        self.q_dropout2 = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states1: Tensor, input_tensor1: Tensor, hidden_states2: Tensor, input_tensor2: Tensor) ->Tuple[Tensor, Tensor]:
        context_state1 = self.dense1(hidden_states1)
        context_state1 = self.dropout1(context_state1)
        context_state2 = self.dense2(hidden_states2)
        context_state2 = self.dropout2(context_state2)
        hidden_states1 = self.LayerNorm1(context_state1 + input_tensor1)
        hidden_states2 = self.LayerNorm2(context_state2 + input_tensor2)
        return hidden_states1, hidden_states2


ACT2FN = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh, 'leaky_relu': nn.LeakyReLU}


class BertImageIntermediate(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.v_hidden_size, config.v_intermediate_size)
        if isinstance(config.v_hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.v_hidden_act]
        else:
            self.intermediate_act_fn = config.v_hidden_act

    def forward(self, hidden_states: Tensor) ->Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states


class BertImageOutput(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.v_intermediate_size, config.v_hidden_size)
        self.LayerNorm = nn.LayerNorm(config.v_hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.v_hidden_dropout_prob)

    def forward(self, hidden_states: Tensor, input_tensor: Tensor) ->Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertConnectionLayer(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.biattention = BertBiAttention(config)
        self.biOutput = BertBiOutput(config)
        self.v_intermediate = BertImageIntermediate(config)
        self.v_output = BertImageOutput(config)
        self.t_intermediate = BertIntermediate(config)
        self.t_output = BertOutput(config)

    def forward(self, input_tensor1: Tensor, attention_mask1: Tensor, input_tensor2: Tensor, attention_mask2: Tensor, co_attention_mask: Optional[Tensor]=None, use_co_attention_mask: bool=False) ->Tuple[Tensor, Tensor, Dict[str, Tensor]]:
        bi_output1, bi_output2, co_attention_probs = self.biattention(input_tensor1, attention_mask1, input_tensor2, attention_mask2, co_attention_mask, use_co_attention_mask)
        attention_output1, attention_output2 = self.biOutput(bi_output2, input_tensor1, bi_output1, input_tensor2)
        intermediate_output1 = self.v_intermediate(attention_output1)
        layer_output1 = self.v_output(intermediate_output1, attention_output1)
        intermediate_output2 = self.t_intermediate(attention_output2)
        layer_output2 = self.t_output(intermediate_output2, attention_output2)
        return layer_output1, layer_output2, co_attention_probs


class BertImageSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        if config.v_hidden_size % config.v_num_attention_heads != 0:
            raise ValueError('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.v_hidden_size, config.v_num_attention_heads))
        self.dynamic_attention = config.dynamic_attention
        self.num_attention_heads = config.v_num_attention_heads
        self.attention_head_size = int(config.v_hidden_size / config.v_num_attention_heads)
        self.visualization = config.visualization
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.query = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.key = nn.Linear(config.v_hidden_size, self.all_head_size)
        self.value = nn.Linear(config.v_hidden_size, self.all_head_size)
        if self.dynamic_attention:
            self.dyLinear_q = nn.Linear(config.hidden_size, self.all_head_size)
            self.dyLinear_k = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(config.v_attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states: Tensor, attention_mask: Tensor, txt_embedding: Tensor, txt_attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)
        if self.dynamic_attention and hasattr(self, 'dyLinear_q') and hasattr(self, 'dyLinear_k'):
            pool_embedding = (txt_embedding * txt_attention_mask).sum(1)
            pool_embedding = pool_embedding / txt_attention_mask.sum(1)
            gate_q = 1 + torch.sigmoid(self.dyLinear_q(pool_embedding))
            gate_k = 1 + torch.sigmoid(self.dyLinear_k(pool_embedding))
            mixed_query_layer = mixed_query_layer * gate_q.unsqueeze(1)
            mixed_key_layer = mixed_key_layer * gate_k.unsqueeze(1)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        attention_scores = attention_scores + attention_mask
        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(new_context_layer_shape)
        if self.visualization:
            attn_data = {'attn': attention_probs, 'queries': query_layer, 'keys': key_layer}
        else:
            attn_data = {}
        return context_layer, attn_data


class BertImageSelfOutput(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.v_hidden_size, config.v_hidden_size)
        self.LayerNorm = nn.LayerNorm(config.v_hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.v_hidden_dropout_prob)

    def forward(self, hidden_states: Tensor, input_tensor: Tensor) ->Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertImageAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.self = BertImageSelfAttention(config)
        self.output = BertImageSelfOutput(config)

    def forward(self, input_tensor: Tensor, attention_mask: Tensor, txt_embedding: Tensor, txt_attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        self_output, attention_probs = self.self(input_tensor, attention_mask, txt_embedding, txt_attention_mask)
        attention_output = self.output(self_output, input_tensor)
        return attention_output, attention_probs


class BertImageLayer(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attention = BertImageAttention(config)
        self.intermediate = BertImageIntermediate(config)
        self.output = BertImageOutput(config)

    def forward(self, hidden_states: Tensor, attention_mask: Tensor, txt_embedding: Tensor, txt_attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        attention_output, attention_probs = self.attention(hidden_states, attention_mask, txt_embedding, txt_attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output, attention_probs

    @torch.no_grad()
    def forward_no_grad(self, hidden_states: Tensor, attention_mask: Tensor, txt_embedding: Tensor, txt_attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        return self.forward(hidden_states, attention_mask, txt_embedding, txt_attention_mask)


class BertSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError('The hidden size (%d) is not a multiple of the number of attention heads (%d)' % (config.hidden_size, config.num_attention_heads))
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.visualization = config.visualization
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states: Tensor, attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(hidden_states)
        mixed_value_layer = self.value(hidden_states)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        attention_scores = attention_scores + attention_mask
        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(new_context_layer_shape)
        if self.visualization:
            attn_data = {'attn': attention_probs, 'queries': query_layer, 'keys': key_layer}
        else:
            attn_data = {}
        return context_layer, attn_data


class BertAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.self = BertSelfAttention(config)
        self.output = BertSelfOutput(config)

    def forward(self, input_tensor: Tensor, attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        self_output, attention_probs = self.self(input_tensor, attention_mask)
        attention_output = self.output(self_output, input_tensor)
        return attention_output, attention_probs


class BertLayer(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attention = BertAttention(config)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

    def forward(self, hidden_states: Tensor, attention_mask: Tensor) ->Tuple[Tensor, Dict[str, Tensor]]:
        attention_output, attention_probs = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output, attention_probs

    @torch.no_grad()
    def forward_no_grad(self, hidden_states, attention_mask):
        return self.forward(hidden_states, attention_mask)


class BertEncoder(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.FAST_MODE = config.fast_mode
        self.with_coattention = config.with_coattention
        self.v_biattention_id = config.v_biattention_id
        self.t_biattention_id = config.t_biattention_id
        self.in_batch_pairs = config.in_batch_pairs
        self.fixed_t_layer = config.fixed_t_layer
        self.fixed_v_layer = config.fixed_v_layer
        layer = BertLayer(config)
        v_layer = BertImageLayer(config)
        connect_layer = BertConnectionLayer(config)
        self.layer = nn.ModuleList([deepcopy(layer) for _ in range(config.num_hidden_layers)])
        self.v_layer = nn.ModuleList([deepcopy(v_layer) for _ in range(config.v_num_hidden_layers)])
        self.c_layer = nn.ModuleList([deepcopy(connect_layer) for _ in range(len(config.v_biattention_id))])

    def forward(self, txt_embedding: Tensor, image_embedding: Tensor, txt_attention_mask: Tensor, txt_attention_mask2: Tensor, image_attention_mask: Tensor, co_attention_mask: Tensor, output_all_encoded_layers: bool=True, output_all_attention_masks: bool=False) ->Tuple[List[Tensor], List[Tensor], Tuple[List[Tensor], List[Tensor], List[Tuple[Tensor, Tensor]]]]:
        v_start = 0
        t_start = 0
        count = 0
        all_encoder_layers_t: List[Tensor] = []
        all_encoder_layers_v: List[Tensor] = []
        all_attention_mask_t: List[Tensor] = []
        all_attnetion_mask_v: List[Tensor] = []
        all_attention_mask_c: List[Tuple[Tensor, Tensor]] = []
        batch_size, num_words, t_hidden_size = txt_embedding.size()
        _, num_regions, v_hidden_size = image_embedding.size()
        use_co_attention_mask = False
        for v_layer_id, t_layer_id in zip(self.v_biattention_id, self.t_biattention_id):
            v_end = v_layer_id
            t_end = t_layer_id
            assert self.fixed_t_layer <= t_end
            assert self.fixed_v_layer <= v_end
            cur_idx = 0
            for cur_layer in self.layer:
                if t_start <= cur_idx < self.fixed_t_layer:
                    txt_embedding, txt_attention_probs = cur_layer.forward_no_grad(txt_embedding, txt_attention_mask)
                    t_start = self.fixed_t_layer
                    if output_all_attention_masks and 'attn' in txt_attention_probs:
                        all_attention_mask_t.append(txt_attention_probs['attn'])
                cur_idx += 1
            cur_idx = 0
            for cur_layer in self.layer:
                if t_start <= cur_idx < t_end:
                    txt_embedding, txt_attention_probs = cur_layer(txt_embedding, txt_attention_mask)
                    if output_all_attention_masks and 'attn' in txt_attention_probs:
                        all_attention_mask_t.append(txt_attention_probs['attn'])
                cur_idx += 1
            cur_v_idx = 0
            for cur_v_layer in self.v_layer:
                if v_start <= cur_v_idx < self.fixed_v_layer:
                    image_embedding, image_attention_probs = cur_v_layer.forward_no_grad(image_embedding, image_attention_mask, txt_embedding, txt_attention_mask2)
                    v_start = self.fixed_v_layer
                    if output_all_attention_masks and 'attn' in image_attention_probs:
                        all_attnetion_mask_v.append(image_attention_probs['attn'])
                cur_v_idx += 1
            cur_v_idx = 0
            for cur_v_layer in self.v_layer:
                if v_start <= cur_v_idx < v_end:
                    image_embedding, image_attention_probs = cur_v_layer(image_embedding, image_attention_mask, txt_embedding, txt_attention_mask2)
                    if output_all_attention_masks and 'attn' in image_attention_probs:
                        all_attnetion_mask_v.append(image_attention_probs['attn'])
                cur_v_idx += 1
            if count == 0 and self.in_batch_pairs:
                image_embedding = image_embedding.unsqueeze(0).expand(batch_size, batch_size, num_regions, v_hidden_size).contiguous().view(batch_size * batch_size, num_regions, v_hidden_size)
                image_attention_mask = image_attention_mask.unsqueeze(0).expand(batch_size, batch_size, 1, 1, num_regions).contiguous().view(batch_size * batch_size, 1, 1, num_regions)
                txt_embedding = txt_embedding.unsqueeze(1).expand(batch_size, batch_size, num_words, t_hidden_size).contiguous().view(batch_size * batch_size, num_words, t_hidden_size)
                txt_attention_mask = txt_attention_mask.unsqueeze(1).expand(batch_size, batch_size, 1, 1, num_words).contiguous().view(batch_size * batch_size, 1, 1, num_words)
                co_attention_mask = co_attention_mask.unsqueeze(1).expand(batch_size, batch_size, 1, num_regions, num_words).contiguous().view(batch_size * batch_size, 1, num_regions, num_words)
            if count == 0 and self.FAST_MODE:
                txt_embedding = txt_embedding.expand(image_embedding.size(0), txt_embedding.size(1), txt_embedding.size(2))
                txt_attention_mask = txt_attention_mask.expand(image_embedding.size(0), txt_attention_mask.size(1), txt_attention_mask.size(2), txt_attention_mask.size(3))
            if self.with_coattention:
                cur_c_idx = 0
                for cur_c_layer in self.c_layer:
                    if cur_c_idx == count:
                        image_embedding, txt_embedding, co_attention_probs = cur_c_layer(image_embedding, image_attention_mask, txt_embedding, txt_attention_mask, co_attention_mask, use_co_attention_mask)
                        if output_all_attention_masks and 'attn1' in co_attention_probs and 'attn2' in co_attention_probs:
                            all_attention_mask_c.append((co_attention_probs['attn1'], co_attention_probs['attn2']))
                    cur_c_idx += 1
            v_start = v_end
            t_start = t_end
            count += 1
            if output_all_encoded_layers:
                all_encoder_layers_t.append(txt_embedding)
                all_encoder_layers_v.append(image_embedding)
        cur_v_idx = 0
        for cur_v_layer in self.v_layer:
            if cur_v_idx >= v_start:
                image_embedding, image_attention_probs = cur_v_layer(image_embedding, image_attention_mask, txt_embedding, txt_attention_mask2)
                if output_all_attention_masks and 'attn' in image_attention_probs:
                    all_attnetion_mask_v.append(image_attention_probs['attn'])
            cur_v_idx += 1
        cur_idx = 0
        for cur_layer in self.layer:
            if cur_idx >= t_start:
                txt_embedding, txt_attention_probs = cur_layer(txt_embedding, txt_attention_mask)
                if output_all_attention_masks and 'attn' in txt_attention_probs:
                    all_attention_mask_t.append(txt_attention_probs['attn'])
            cur_idx += 1
        if not output_all_encoded_layers:
            all_encoder_layers_t.append(txt_embedding)
            all_encoder_layers_v.append(image_embedding)
        return all_encoder_layers_t, all_encoder_layers_v, (all_attention_mask_t, all_attnetion_mask_v, all_attention_mask_c)


class BertSelfAttentionJit(BertSelfAttention):
    """
    Torchscriptable version of `BertSelfAttention` from Huggingface transformers v2.3.0
    https://github.com/huggingface/transformers/blob/v2.3.0/transformers/modeling_bert.py # noqa

    Modifies the `forward` function and `transpose_for_scores` function

    Changes to `transpose_for_scores` function ::
        Changes the `new_x_shape` unpacking as static size inference is not supported

    Changes to `forward` function ::
        Uses scriptable `nn.functional.softmax` and also removes several static size
        inference which is not supported.
    """

    def transpose_for_scores(self, x: Tensor) ->Tensor:
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states: Tensor, attention_mask: Optional[Tensor]=None, head_mask: Optional[Tensor]=None, encoder_hidden_states: Optional[Tensor]=None, encoder_attention_mask: Optional[Tensor]=None) ->Tuple[Tensor, Tensor]:
        mixed_query_layer = self.query(hidden_states)
        if encoder_hidden_states is not None:
            mixed_key_layer = self.key(encoder_hidden_states)
            mixed_value_layer = self.value(encoder_hidden_states)
            attention_mask = encoder_attention_mask
        else:
            mixed_key_layer = self.key(hidden_states)
            mixed_value_layer = self.value(hidden_states)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask
        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)
        if head_mask is not None:
            attention_probs = attention_probs * head_mask
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(new_context_layer_shape)
        outputs = context_layer, attention_probs
        return outputs


class BertAttentionJit(BertAttention):
    """
    Torchscriptable version of `BertAttention` from Huggingface transformers v2.3.0
    https://github.com/huggingface/transformers/blob/v2.3.0/transformers/modeling_bert.py # noqa

    Modifies the `forward` function as well as uses scriptable `BertSelfAttentionJit`

    Changes to `forward` function ::
        Typed inputs and modifies the output to be a List[Tensor]
    """

    def __init__(self, config):
        super().__init__(config)
        self.self = BertSelfAttentionJit(config)
        self.output = BertSelfOutput(config)
        self.pruned_heads = set()

    def forward(self, hidden_states: Tensor, attention_mask: Optional[Tensor]=None, head_mask: Optional[Tensor]=None, encoder_hidden_states: Optional[Tensor]=None, encoder_attention_mask: Optional[Tensor]=None) ->List[Tensor]:
        self_outputs = self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)
        attention_output = self.output(self_outputs[0], hidden_states)
        outputs = (attention_output,) + self_outputs[1:]
        return outputs


class BertLayerJit(BertLayer):
    """
    Torchscriptable version of `BertLayer` from Huggingface transformers v2.3.0
    https://github.com/huggingface/transformers/blob/v2.3.0/transformers/modeling_bert.py # noqa

    Modifies the `forward` function as well as uses scriptable `BertAttentionJit`

    Changes to `forward` function::
        Typed inputs and modifies the output to be a List[Tensor]
    """

    def __init__(self, config):
        super().__init__(config)
        self.attention = BertAttentionJit(config)
        self.is_decoder = config.is_decoder
        if self.is_decoder:
            self.crossattention = BertAttentionJit(config)

    def forward(self, hidden_states: Tensor, attention_mask: Optional[Tensor]=None, head_mask: Optional[Tensor]=None, encoder_hidden_states: Optional[Tensor]=None, encoder_attention_mask: Optional[Tensor]=None) ->List[Tensor]:
        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
        attention_output = self_attention_outputs[0]
        outputs = self_attention_outputs[1:]
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        outputs = (layer_output,) + outputs
        return outputs


class BertEncoderJit(BertEncoder):
    """
    Torchscriptable version of `BertEncoder` from Huggingface transformers v2.3.0
    https://github.com/huggingface/transformers/blob/v2.3.0/transformers/modeling_bert.py # noqa

    Modifies the `forward` function as well as uses scriptable `BertLayerJit`

    Changes to `forward` function::
        Typed inputs and modifies the output to be of Tuple[Tensor] type in scripting
        mode. Due to different possible types when `output_hidden_states` or
        `output_attentions` are enable, we do not support these in scripting mode
    """

    def __init__(self, config):
        super().__init__(config)
        self.output_attentions = config.output_attentions
        self.output_hidden_states = config.output_hidden_states
        self.layer = nn.ModuleList([BertLayerJit(config) for _ in range(config.num_hidden_layers)])

    def forward(self, hidden_states: Tensor, attention_mask: Optional[Tensor], encoder_hidden_states: Optional[Tensor]=None, encoder_attention_mask: Optional[Tensor]=None, output_attentions: bool=False, output_hidden_states: bool=False, return_dict: bool=False, head_mask: Optional[Tensor]=None) ->Tuple[Tensor]:
        all_hidden_states = ()
        all_attentions = ()
        for i, layer_module in enumerate(self.layer):
            if not torch.jit.is_scripting() and output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)
            layer_outputs = layer_module(hidden_states, attention_mask, None, encoder_hidden_states, encoder_attention_mask)
            hidden_states = layer_outputs[0]
            if not torch.jit.is_scripting() and output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)
        if not torch.jit.is_scripting() and output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        outputs = hidden_states,
        if not torch.jit.is_scripting():
            if output_hidden_states:
                outputs = outputs + (all_hidden_states,)
            if output_attentions:
                outputs = outputs + (all_attentions,)
        return outputs


class VisualBERTForClassification(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.output_attentions = self.config.output_attentions
        self.output_hidden_states = self.config.output_hidden_states
        self.pooler_strategy = self.config.get('pooler_strategy', 'default')
        self.bert_model_name = getattr(self.config, 'bert_model_name', None)
        self.bert_config = BertConfig.from_dict(OmegaConf.to_container(self.config, resolve=True))
        if self.bert_model_name is None:
            self.bert = VisualBERTBase(self.bert_config, visual_embedding_dim=self.config.visual_embedding_dim, embedding_strategy=self.config.embedding_strategy, bypass_transformer=self.config.bypass_transformer, output_attentions=self.config.output_attentions, output_hidden_states=self.config.output_hidden_states)
        else:
            self.bert = VisualBERTBase.from_pretrained(self.config.bert_model_name, config=self.bert_config, cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)), visual_embedding_dim=self.config.visual_embedding_dim, embedding_strategy=self.config.embedding_strategy, bypass_transformer=self.config.bypass_transformer, output_attentions=self.config.output_attentions, output_hidden_states=self.config.output_hidden_states)
        self.training_head_type = self.config.training_head_type
        self.num_labels = self.config.num_labels
        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)
        if self.config.training_head_type == 'nlvr2':
            self.bert.config.hidden_size *= 2
        self.classifier = nn.Sequential(BertPredictionHeadTransform(self.bert.config), nn.Linear(self.bert.config.hidden_size, self.config.num_labels))
        self.init_weights()

    def init_weights(self):
        if self.config.random_initialize is False:
            if self.bert_model_name is None:
                self.bert.init_weights()
            self.classifier.apply(self.bert._init_weights)
        if 'losses' in self.config and self.config.zerobias:
            for loss in self.config.losses:
                if 'bce' in loss['type']:
                    self.classifier[1].bias.data.fill_(self.config.biasfill)

    def forward(self, input_ids: Tensor, input_mask: Tensor, attention_mask: Optional[Tensor]=None, token_type_ids: Optional[Tensor]=None, visual_embeddings: Optional[Tensor]=None, visual_embeddings_type: Optional[Tensor]=None, image_text_alignment: Optional[Tensor]=None, masked_lm_labels: Optional[Tensor]=None) ->Dict[str, Tensor]:
        sequence_output, pooled_output, attention_weights = self.bert(input_ids, attention_mask, token_type_ids, visual_embeddings, visual_embeddings_type, image_text_alignment)
        if self.training_head_type == 'nlvr2':
            b, h = pooled_output.size()
            pooled_output = torch.cat([pooled_output[:b // 2], pooled_output[b // 2:]], dim=1)
        output_dict: Dict[str, Tensor] = {}
        if not torch.jit.is_scripting():
            if self.output_attentions:
                output_dict['attention_weights'] = attention_weights
            if self.output_hidden_states:
                output_dict['sequence_output'] = sequence_output
                output_dict['pooled_output'] = pooled_output
        else:
            assert not (self.output_attentions or self.output_hidden_states), 'output_attentions or output_hidden_states not supported in script mode'
        if self.pooler_strategy == 'vqa':
            index_to_gather = input_mask.sum(1) - 2
            pooled_output = torch.gather(sequence_output, 1, index_to_gather.unsqueeze(-1).unsqueeze(-1).expand(index_to_gather.size(0), 1, sequence_output.size(-1)))
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        reshaped_logits = logits.contiguous().view(-1, self.num_labels)
        output_dict['scores'] = reshaped_logits
        return output_dict


def get_bert_configured_parameters(module, lr=None, weight_decay=0.01):
    if isinstance(module, nn.Module):
        param_optimizer = list(module.named_parameters())
    elif isinstance(module, list):
        param_optimizer = module
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay}, {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]
    if lr is not None:
        for p in optimizer_grouped_parameters:
            p['lr'] = lr
    return optimizer_grouped_parameters


def get_optimizer_parameters_for_bert(module, config):
    lr = config.optimizer.params.lr
    model_config = config.model_config.get(config.model, {})
    finetune_lr_multiplier = model_config.get('finetune_lr_multiplier', 1)
    if module.config.training_head_type == 'pretraining' or finetune_lr_multiplier == 1:
        return get_bert_configured_parameters(module)
    parameters = []
    for name, submodule in module.named_children():
        if name == 'classifier':
            continue
        parameters += get_bert_configured_parameters(submodule, lr * finetune_lr_multiplier)
        logger.info(f"Overriding {name} module's LR to {lr * finetune_lr_multiplier}")
    parameters += get_bert_configured_parameters(module.classifier)
    return parameters


def transform_to_batch_sequence(tensor: Tensor) ->Tensor:
    if len(tensor.size()) == 2:
        return tensor
    else:
        assert len(tensor.size()) == 3
        return tensor.contiguous().view(-1, tensor.size(-1))


def transform_to_batch_sequence_dim(tensor: Tensor) ->Tensor:
    if len(tensor.size()) == 3:
        return tensor
    else:
        assert len(tensor.size()) == 4
        return tensor.contiguous().view(-1, tensor.size(-2), tensor.size(-1))


class VisualBERTModule(nn.Module):

    def __init__(self, config, extra_config=None):
        super().__init__()
        self.config = config
        if extra_config is None:
            self.extra_config = {}
        else:
            self.extra_config = extra_config
        self.build()

    def build(self):
        assert self.config.training_head_type != 'pretraining'
        self.model = VisualBERTForClassification(self.config, self.extra_config)
        if self.config.special_visual_initialize:
            self.model.bert.embeddings.initialize_visual_from_pretrained()
        if self.config.load_from_pretrained:
            pretrained_file = self.config.pretrained_file
            with PathManager.open(pretrained_file, 'rb') as f:
                ckpt = torch.load(f, map_location=lambda storage, loc: storage)
            model_ckpt = ckpt['model']
            model_ckpt_new = {}
            for key in model_ckpt:
                if 'bert' not in key:
                    continue
                model_ckpt_new[key.split('model.')[1]] = model_ckpt[key]
            model_ckpt = model_ckpt_new
            incompatible_keys = self.model.load_state_dict(model_ckpt, strict=False)
            if len(incompatible_keys.missing_keys) != 0:
                logger.warning(f'Missing keys {incompatible_keys.missing_keys} in the' + ' checkpoint.\n' + 'If this is not your checkpoint, please open up an ' + 'issue on MMF GitHub. \n' + f'Unexpected keys if any: {incompatible_keys.unexpected_keys}')
            if len(incompatible_keys.unexpected_keys) != 0:
                logger.warning('Unexpected keys in state dict: ' + f'{incompatible_keys.unexpected_keys} \n' + 'This is usually not a problem with pretrained models, but ' + 'if this is your own model, please double check. \n' + 'If you think this is an issue, please open up a ' + 'bug at MMF GitHub.')
        if getattr(self.config, 'freeze_base', False):
            for p in self.model.bert.parameters():
                p.requires_grad = False
        self.feed_graph_to_vb = self.extra_config['feed_graph_to_vb']
        self.graph_node_hid_dim = self.extra_config['node_hid_dim']
        self.graph_feed_mode = self.extra_config['feed_mode']
        if self.feed_graph_to_vb and self.extra_config['compress_crossmodel']:
            assert False

    def flatten(self, sample_list, to_be_flattened=None, to_be_flattened_dim=None):
        if to_be_flattened is None:
            to_be_flattened = {}
        if to_be_flattened_dim is None:
            to_be_flattened_dim = {}
        for key in to_be_flattened:
            sample_list[key] = getattr(sample_list, key, None)
            sample_list[key] = transform_to_batch_sequence(sample_list[key])
        for key in to_be_flattened_dim:
            sample_list[key] = getattr(sample_list, key, None)
            sample_list[key] = transform_to_batch_sequence_dim(sample_list[key])
        if sample_list.visual_embeddings_type is None:
            if sample_list.image_mask is not None:
                sample_list.visual_embeddings_type = torch.zeros_like(sample_list.image_mask)
        if sample_list.image_mask is not None:
            attention_mask = torch.cat((sample_list.input_mask, sample_list.image_mask), dim=-1)
            if sample_list.masked_lm_labels is not None:
                assert sample_list.masked_lm_labels.size(-1) == sample_list.input_mask.size(-1)
                new_lm_labels = torch.ones_like(attention_mask) * -1
                size_masked_lm_labels = sample_list.masked_lm_labels.size()
                assert len(size_masked_lm_labels) == 2
                new_lm_labels[:size_masked_lm_labels[0], :size_masked_lm_labels[1]] = sample_list.masked_lm_labels
                sample_list.masked_lm_labels = new_lm_labels
        else:
            attention_mask = sample_list.input_mask
        sample_list.attention_mask = attention_mask
        return sample_list

    def get_optimizer_parameters(self, config):
        return get_optimizer_parameters_for_bert(self.model, config)

    def flatten_for_bert(self, sample_list, **kwargs):
        to_be_flattened = ['input_ids', 'token_type_ids', 'input_mask', 'image_mask', 'masked_lm_labels']
        to_be_flattened_dim = ['visual_embeddings']
        flattened = self.flatten(sample_list, to_be_flattened, to_be_flattened_dim)
        return flattened

    def update_sample_list_based_on_head(self, sample_list):
        bert_input_ids = sample_list.input_ids
        bert_input_mask = sample_list.input_mask
        bert_input_type_ids = sample_list.segment_ids
        if self.config.training_head_type == 'nlvr2':
            bert_input_ids = torch.cat([bert_input_ids, bert_input_ids])
            bert_input_mask = torch.cat([bert_input_mask, bert_input_mask])
            bert_input_type_ids = torch.cat([bert_input_type_ids, bert_input_type_ids])
            img0 = getattr(sample_list, 'img0', {})
            image_info = getattr(img0, 'image_info_0', {})
            image_dim_variable_0 = getattr(image_info, 'max_features', None)
            image_feat_variable_0 = getattr(img0, 'image_feature_0', None)
            img1 = getattr(sample_list, 'img1', {})
            image_info = getattr(img1, 'image_info_0', {})
            image_dim_variable_1 = getattr(image_info, 'max_features', None)
            image_feat_variable_1 = getattr(img1, 'image_feature_0', None)
            image_feat_variable = torch.cat([image_feat_variable_0, image_feat_variable_1])
            image_dim_variable = torch.cat([image_dim_variable_0, image_dim_variable_1])
        else:
            image_info = getattr(sample_list, 'image_info_0', {})
            image_dim_variable = getattr(image_info, 'max_features', None)
            image_feat_variable = getattr(sample_list, 'image_feature_0', None)
        sample_list.visual_embeddings = image_feat_variable
        sample_list.image_dim = image_dim_variable
        sample_list.input_ids = bert_input_ids
        sample_list.input_mask = bert_input_mask
        sample_list.token_type_ids = bert_input_type_ids
        return sample_list

    def add_custom_params(self, sample_list):
        visual_embeddings = getattr(sample_list, 'visual_embeddings', None)
        image_dim = getattr(sample_list, 'image_dim', None)
        sample_list.masked_lm_labels = getattr(sample_list, 'lm_label_ids', None)
        if visual_embeddings is not None and image_dim is not None:
            image_mask = torch.arange(visual_embeddings.size(-2), device=visual_embeddings.device).expand(*visual_embeddings.size()[:-1])
            if len(image_dim.size()) < len(image_mask.size()):
                image_dim = image_dim.unsqueeze(-1)
                assert len(image_dim.size()) == len(image_mask.size())
            image_mask = image_mask < image_dim
            sample_list.image_mask = image_mask.long()
        else:
            sample_list.image_mask = None
        sample_list.position_embeddings_visual = None
        sample_list.visual_embeddings_type = None
        sample_list.image_text_alignment = None
        return sample_list

    @classmethod
    def format_state_key(cls, key):
        return key.replace('bert.bert', 'model.bert').replace('bert.cls', 'model.cls').replace('bert.classifier', 'model.classifier')

    def forward(self, sample_list):
        sample_list = self.update_sample_list_based_on_head(sample_list)
        sample_list = self.add_custom_params(sample_list)
        sample_list = self.flatten_for_bert(sample_list)
        if self.feed_graph_to_vb:
            if self.graph_feed_mode == 'feed_graph_hid_to_vb':
                assert 'graph_special_node_out' in sample_list
                graph_input = sample_list['graph_special_node_out']
            else:
                assert False
        else:
            graph_input = None
        output = self.model(sample_list.input_ids, sample_list.input_mask, sample_list.attention_mask, sample_list.token_type_ids, sample_list.visual_embeddings, sample_list.position_embeddings_visual, sample_list.visual_embeddings_type, sample_list.image_text_alignment, sample_list.masked_lm_labels, graph_input)
        return output


class GeLU(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x):
        return ACT2FN['gelu'](x)


class BertCrossattLayer(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.att = BertSelfAttention(config)
        self.output = BertSelfOutput(config)

    def forward(self, input_tensor, ctx_tensor, ctx_att_mask=None):
        output = self.att(input_tensor, encoder_hidden_states=ctx_tensor, encoder_attention_mask=ctx_att_mask)[0]
        attention_output = self.output(output, input_tensor)
        return attention_output


class BertClassificationHead(nn.Module):

    def __init__(self, num_labels, hid_dim, training_head_type):
        super().__init__()
        if training_head_type == 'nlvr2':
            in_dim = hid_dim * 2
            out_dim = 2
        else:
            in_dim = hid_dim
            out_dim = num_labels
        self.logit_fc = nn.Sequential(nn.Linear(in_dim, hid_dim * 2), GeLU(), nn.LayerNorm(hid_dim * 2, eps=1e-12), nn.Linear(hid_dim * 2, out_dim))

    def forward(self, x):
        logit = self.logit_fc(x)
        return logit


class BertLMPredictionHead(nn.Module):

    def __init__(self, config, bert_model_embedding_weights):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.decoder = nn.Linear(bert_model_embedding_weights.size(1), bert_model_embedding_weights.size(0), bias=False)
        self.decoder.weight = bert_model_embedding_weights
        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))

    def forward(self, hidden_states):
        hidden_states = self.transform(hidden_states)
        hidden_states = self.decoder(hidden_states) + self.bias
        return hidden_states


class BertVisualAnswerHead(nn.Module):

    def __init__(self, config, num_labels):
        super().__init__()
        hid_dim = config.hidden_size
        if config.training_head_type == 'nlvr2':
            in_dim = hid_dim * 2
            out_dim = 2
        else:
            in_dim = hid_dim
            out_dim = config.num_labels
        add_gqa = isinstance(num_labels, list)
        if add_gqa:
            self.logit_gqa = nn.Sequential(nn.Linear(in_dim, hid_dim * 2), GeLU(), nn.LayerNorm(hid_dim * 2, eps=1e-12), nn.Linear(hid_dim * 2, num_labels[1]))
            out_dim = num_labels[0]
        self.logit_fc = nn.Sequential(nn.Linear(in_dim, hid_dim * 2), GeLU(), nn.LayerNorm(hid_dim * 2, eps=1e-12), nn.Linear(hid_dim * 2, out_dim))

    def forward(self, hidden_states, name=None):
        if name is None or 'gqa' not in name:
            return self.logit_fc(hidden_states)
        else:
            return self.logit_gqa(hidden_states)


class BertVisualObjHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.visual_losses = config.visual_losses
        self.decoder_dict = nn.ModuleDict({key: nn.Linear(config.hidden_size, config.visual_loss_config[key][0]) for key in self.visual_losses})

    def forward(self, hidden_states):
        hidden_states = self.transform(hidden_states)
        output = {}
        for key in self.visual_losses:
            output[key] = self.decoder_dict[key](hidden_states)
        return output


class BertImgPredictionHeadTransform(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.v_hidden_size, config.v_hidden_size)
        if isinstance(config.hidden_act, str):
            self.transform_act_fn = ACT2FN[config.hidden_act]
        else:
            self.transform_act_fn = config.v_hidden_act
        self.LayerNorm = nn.LayerNorm(config.v_hidden_size, eps=1e-12)

    def forward(self, hidden_states: Tensor) ->Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.transform_act_fn(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        return hidden_states


class BertImagePredictionHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.transform = BertImgPredictionHeadTransform(config)
        self.decoder = nn.Linear(config.v_hidden_size, config.v_target_size)

    def forward(self, hidden_states: Tensor) ->Tensor:
        hidden_states = self.transform(hidden_states)
        hidden_states = self.decoder(hidden_states)
        return hidden_states


class BertPreTrainingHeads(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.bi_seq_relationship = nn.Linear(config.bi_hidden_size, 2)
        self.imagePredictions = BertImagePredictionHead(config)
        self.fusion_method = config.fusion_method
        self.dropout = nn.Dropout(0.1)

    def forward(self, sequence_output_t: Tensor, sequence_output_v: Tensor, pooled_output_t: Tensor, pooled_output_v: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
        if self.fusion_method == 'sum':
            pooled_output = self.dropout(pooled_output_t + pooled_output_v)
        elif self.fusion_method == 'mul':
            pooled_output = self.dropout(pooled_output_t * pooled_output_v)
        else:
            raise AssertionError
        prediction_scores_t = self.predictions(sequence_output_t)
        seq_relationship_score = self.bi_seq_relationship(pooled_output)
        prediction_scores_v = self.imagePredictions(sequence_output_v)
        return prediction_scores_t, prediction_scores_v, seq_relationship_score


class VisualFeatEncoder(nn.Module):

    def __init__(self, config):
        super().__init__()
        feat_dim = config.visual_feat_dim
        pos_dim = config.visual_pos_dim
        self.visn_fc = nn.Linear(feat_dim, config.hidden_size)
        self.visn_layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.box_fc = nn.Linear(pos_dim, config.hidden_size)
        self.box_layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, visn_input):
        feats, boxes = visn_input
        x = self.visn_fc(feats)
        x = self.visn_layer_norm(x)
        if boxes is not None:
            y = self.box_fc(boxes)
            y = self.box_layer_norm(y)
            output = (x + y) / 2
        else:
            output = x
        output = self.dropout(output)
        return output


class LXMERTXLayer(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.visual_attention = BertCrossattLayer(config)
        self.lang_self_att = BertAttention(config)
        self.visn_self_att = BertAttention(config)
        self.lang_inter = BertIntermediate(config)
        self.lang_output = BertOutput(config)
        self.visn_inter = BertIntermediate(config)
        self.visn_output = BertOutput(config)

    def cross_att(self, lang_input, lang_attention_mask, visn_input, visn_attention_mask):
        lang_att_output = self.visual_attention(lang_input, visn_input, ctx_att_mask=visn_attention_mask)
        visn_att_output = self.visual_attention(visn_input, lang_input, ctx_att_mask=lang_attention_mask)
        return lang_att_output, visn_att_output

    def self_att(self, lang_input, lang_attention_mask, visn_input, visn_attention_mask):
        lang_att_output = self.lang_self_att(lang_input, lang_attention_mask)[0]
        visn_att_output = self.visn_self_att(visn_input, visn_attention_mask)[0]
        return lang_att_output, visn_att_output

    def output_fc(self, lang_input, visn_input):
        lang_inter_output = self.lang_inter(lang_input)
        visn_inter_output = self.visn_inter(visn_input)
        lang_output = self.lang_output(lang_inter_output, lang_input)
        visn_output = self.visn_output(visn_inter_output, visn_input)
        return lang_output, visn_output

    def forward(self, lang_feats, lang_attention_mask, visn_feats, visn_attention_mask):
        lang_att_output = lang_feats
        visn_att_output = visn_feats
        lang_att_output, visn_att_output = self.cross_att(lang_att_output, lang_attention_mask, visn_att_output, visn_attention_mask)
        lang_att_output, visn_att_output = self.self_att(lang_att_output, lang_attention_mask, visn_att_output, visn_attention_mask)
        lang_output, visn_output = self.output_fc(lang_att_output, visn_att_output)
        return lang_output, visn_output


class LXMERTEncoder(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.visn_fc = VisualFeatEncoder(config)
        self.num_l_layers = config.l_layers
        self.num_x_layers = config.x_layers
        self.num_r_layers = config.r_layers
        self.layer = nn.ModuleList([BertLayer(config) for _ in range(self.num_l_layers)])
        self.x_layers = nn.ModuleList([LXMERTXLayer(config) for _ in range(self.num_x_layers)])
        self.r_layers = nn.ModuleList([BertLayer(config) for _ in range(self.num_r_layers)])

    def forward(self, lang_feats, lang_attention_mask, visn_feats, visn_attention_mask=None):
        visn_feats = self.visn_fc(visn_feats)
        for layer_module in self.layer:
            lang_feats = layer_module(lang_feats, lang_attention_mask)[0]
        for layer_module in self.r_layers:
            visn_feats = layer_module(visn_feats, visn_attention_mask)[0]
        for layer_module in self.x_layers:
            lang_feats, visn_feats = layer_module(lang_feats, lang_attention_mask, visn_feats, visn_attention_mask)
        return lang_feats, visn_feats


class LXMERTForPretraining(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.bert = LXMERTBase.from_pretrained(self.config.bert_model_name, config=BertConfig.from_dict(OmegaConf.to_container(self.config, resolve=True)), cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)))
        self.num_labels = config.num_labels
        self.gqa_labels = config.gqa_labels
        self.task_mask_lm = config.task_mask_lm
        self.task_obj_predict = config.task_obj_predict
        self.task_matched = config.task_matched
        self.task_qa = config.task_qa
        self.visual_losses = config.visual_losses
        self.visual_loss_config = config.visual_loss_config
        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)
        if self.task_obj_predict:
            self.obj_predict_head = BertVisualObjHead(config)
        if self.task_qa:
            self.answer_head = BertVisualAnswerHead(config, [self.num_labels, self.gqa_labels])
        self.loss_fcts = {'l2': SmoothL1Loss(reduction='none'), 'ce': CrossEntropyLoss(ignore_index=-1, reduction='none'), 'ce_lang': CrossEntropyLoss(ignore_index=-1)}

    def init_weights(self):
        if self.config.random_initialize is False:
            if self.config.bert_model_name is None:
                self.bert.init_weights()
                self.cls.apply(self.bert._init_weights)
            self.tie_weights()

    def tie_weights(self):
        """Make sure we are sharing the input and output embeddings.
        Export to TorchScript can't handle parameter sharing so we are cloning
        them instead.
        """
        self._tie_or_clone_weights(self.cls.predictions.decoder, self.bert.embeddings.word_embeddings)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, visual_feats=None, visual_pos=None, visual_attention_mask=None, masked_lm_labels=None, masked_image_labels=None, obj_labels=None, matched_label=None, ans=None, num_features=None, name=None, output_all_attention_masks=False, output_all_encoded_layers=False):
        (lang_output, visn_output), pooled_output = self.bert(input_ids, token_type_ids, attention_mask, visual_feats, visual_pos, visual_attention_mask, output_all_attention_masks, output_all_encoded_layers)
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        output = {}
        if output_all_attention_masks:
            raise NotImplementedError
        if ans is not None and self.task_qa:
            answer_score = self.answer_head(pooled_output, name)
            if name is None or 'gqa' not in name:
                num_labels = self.config.num_labels
            else:
                num_labels = self.config.gqa_labels
            answer_loss = self.loss_fcts['ce_lang'](answer_score.view(-1, num_labels), ans.argmax(-1))
            output['answer_loss'] = answer_loss
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts['ce_lang'](lang_prediction_scores.view(-1, lang_prediction_scores.size(-1)), masked_lm_labels.view(-1))
            output['masked_lm_loss'] = masked_lm_loss
        if matched_label is not None and self.task_matched:
            matched_label = matched_label.long()
            matched_loss = self.loss_fcts['ce_lang'](cross_relationship_score.view(-1, 2), matched_label)
            output['matched_loss'] = matched_loss
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visn_output)
            for key in self.visual_losses:
                visn_prediction_scores = visn_prediction_scores_dict[key]
                output_dim, loss_fct_name, label_shape, weight = self.visual_loss_config[key]
                if key == 'attr':
                    continue
                elif key == 'obj':
                    temp_obj_labels_dict = obj_labels.max(-1)
                    mask_conf = temp_obj_labels_dict.values
                    visn_loss = self.loss_fcts[loss_fct_name](visn_prediction_scores.view(-1, output_dim), temp_obj_labels_dict.indices.view(-1))
                elif key == 'feat':
                    if type(masked_image_labels) is None:
                        continue
                    mask_conf = (masked_image_labels == 1).float()
                    visn_loss = self.loss_fcts[loss_fct_name](visn_prediction_scores.view(-1, output_dim), visual_feats.view(-1, output_dim))
                if visn_loss.dim() > 1:
                    visn_loss = visn_loss.mean(1)
                visn_loss = (visn_loss * mask_conf.view(-1)).mean() * weight
                total_visn_loss += visn_loss
            output['visn_loss'] = total_visn_loss
        return output


class LXMERTForClassification(nn.Module):

    def __init__(self, config, mode='lxr'):
        super().__init__()
        self.config = config
        self.num_labels = config.num_labels
        self.gqa_labels = config.gqa_labels
        self.mode = config.mode
        self.bert = LXMERTBase.from_pretrained(self.config.bert_model_name, config=BertConfig.from_dict(OmegaConf.to_container(self.config, resolve=True)), cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)))
        self.classifier = BertVisualAnswerHead(config, [self.num_labels, self.gqa_labels])
        self.init_weights()

    def init_weights(self):
        if self.config.random_initialize is False:
            if self.config.bert_model_name is None:
                self.bert.init_weights()
            self.classifier.apply(self.bert._init_weights)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, visual_feats=None, visual_pos=None, visual_attention_mask=None, masked_lm_labels=None, obj_labels=None, matched_label=None, ans=None, max_features=None, output_all_attention_masks=False, output_all_encoded_layers=False):
        (lang_output, visn_output), pooled_output = self.bert(input_ids, token_type_ids, attention_mask, visual_feats, visual_pos, visual_attention_mask, output_all_encoded_layers, output_all_attention_masks)
        output = {}
        if output_all_attention_masks:
            raise NotImplementedError
        if self.config.training_head_type == 'nlvr2':
            pooled_output = pooled_output.view(-1, pooled_output.size(1) * 2)
        logits = self.classifier(pooled_output)
        reshaped_logits = logits.contiguous().view(-1, self.config.num_labels)
        output['scores'] = reshaped_logits
        return output


class OcrPtrNet(nn.Module):

    def __init__(self, hidden_size, query_key_size=None):
        super().__init__()
        if query_key_size is None:
            query_key_size = hidden_size
        self.hidden_size = hidden_size
        self.query_key_size = query_key_size
        self.query = nn.Linear(hidden_size, query_key_size)
        self.key = nn.Linear(hidden_size, query_key_size)

    def forward(self, query_inputs, key_inputs, attention_mask):
        extended_attention_mask = (1.0 - attention_mask) * -10000.0
        assert extended_attention_mask.dim() == 2
        extended_attention_mask = extended_attention_mask.unsqueeze(1)
        query_layer = self.query(query_inputs)
        if query_layer.dim() == 2:
            query_layer = query_layer.unsqueeze(1)
            squeeze_result = True
        else:
            squeeze_result = False
        key_layer = self.key(key_inputs)
        scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        scores = scores / math.sqrt(self.query_key_size)
        scores = scores + extended_attention_mask
        if squeeze_result:
            scores = scores.squeeze(1)
        return scores


def _batch_gather(x, inds):
    assert x.dim() == 3
    batch_size = x.size(0)
    length = x.size(1)
    dim = x.size(2)
    x_flat = x.view(batch_size * length, dim)
    batch_offsets = torch.arange(batch_size, device=inds.device) * length
    batch_offsets = batch_offsets.unsqueeze(-1)
    assert batch_offsets.dim() == inds.dim()
    inds_flat = batch_offsets + inds
    results = F.embedding(inds_flat, x_flat)
    return results


class PrevPredEmbeddings(nn.Module):

    def __init__(self, config):
        super().__init__()
        MAX_DEC_LENGTH = 100
        MAX_TYPE_NUM = 5
        hidden_size = config.hidden_size
        ln_eps = config.layer_norm_eps
        self.position_embeddings = nn.Embedding(MAX_DEC_LENGTH, hidden_size)
        self.token_type_embeddings = nn.Embedding(MAX_TYPE_NUM, hidden_size)
        self.ans_layer_norm = nn.LayerNorm(hidden_size, eps=ln_eps)
        self.ocr_layer_norm = nn.LayerNorm(hidden_size, eps=ln_eps)
        self.emb_layer_norm = nn.LayerNorm(hidden_size, eps=ln_eps)
        self.emb_dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, ans_emb, ocr_emb, prev_inds):
        assert prev_inds.dim() == 2 and prev_inds.dtype == torch.long
        assert ans_emb.dim() == 2
        batch_size = prev_inds.size(0)
        seq_length = prev_inds.size(1)
        ans_num = ans_emb.size(0)
        ans_emb = self.ans_layer_norm(ans_emb)
        ocr_emb = self.ocr_layer_norm(ocr_emb)
        assert ans_emb.size(-1) == ocr_emb.size(-1)
        ans_emb = ans_emb.unsqueeze(0).expand(batch_size, -1, -1)
        ans_ocr_emb_cat = torch.cat([ans_emb, ocr_emb], dim=1)
        raw_dec_emb = _batch_gather(ans_ocr_emb_cat, prev_inds)
        position_ids = torch.arange(seq_length, dtype=torch.long, device=ocr_emb.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_ids = prev_inds.ge(ans_num).long()
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        embeddings = position_embeddings + token_type_embeddings
        embeddings = self.emb_layer_norm(embeddings)
        embeddings = self.emb_dropout(embeddings)
        dec_emb = raw_dec_emb + embeddings
        return dec_emb


class ModalEmbeddings(nn.Module):
    """
    Generic Modal Embeddings which takes in an encoder,
    and a transformer embedding.
    """

    def __init__(self, config, encoder, embeddings):
        super().__init__()
        self.config = config
        self.encoder = encoder
        self.proj_embeddings = nn.Linear(config.modal_hidden_size, config.hidden_size)
        self.position_embeddings = embeddings.position_embeddings
        self.token_type_embeddings = embeddings.token_type_embeddings
        self.word_embeddings = embeddings.word_embeddings
        self.LayerNorm = embeddings.LayerNorm
        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)

    def forward(self, input_modal: Tensor, start_token: Optional[Tensor]=None, end_token: Optional[Tensor]=None, position_ids: Optional[Tensor]=None, token_type_ids: Optional[Tensor]=None):
        token_embeddings = self.proj_embeddings(self.encoder(input_modal))
        seq_length = token_embeddings.size(1)
        if start_token is not None:
            start_token_embeds = self.word_embeddings(start_token)
            seq_length += 1
            token_embeddings = torch.cat([start_token_embeds.unsqueeze(1), token_embeddings], dim=1)
        if end_token is not None:
            end_token_embeds = self.word_embeddings(end_token)
            seq_length += 1
            token_embeddings = torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1)
        if position_ids is None:
            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_modal.device)
            position_ids = position_ids.unsqueeze(0).expand(input_modal.size(0), seq_length)
        if token_type_ids is None:
            token_type_ids = torch.zeros((input_modal.size(0), seq_length), dtype=torch.long, device=input_modal.device)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        embeddings = token_embeddings + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class MMBTModel(nn.Module):
    """
    Outputs: `Tuple` comprising various elements depending on the configuration
        (config) and inputs:
        **last_hidden_state**: ``torch.FloatTensor`` of shape
            ``(batch_size, sequence_length, hidden_size)``. Sequence of
            hidden-states at the output of the last layer of the model.
        **pooler_output**: ``torch.FloatTensor`` of shape
            ``(batch_size, hidden_size)``. Last layer hidden-state of the
            first token of the sequence (classification token) further processed
            by a Linear layer and a Tanh activation function. The Linear
            layer weights are trained from the next sentence prediction
            (classification) objective during Bert pretraining. This output
            is usually *not* a good summary of the semantic content of the
            input, you're often better with averaging or pooling
            the sequence of hidden-states for the whole input sequence.
        **hidden_states**: (`optional`, returned when
            ``config.output_hidden_states=True``)
            list of ``torch.FloatTensor`` (one for the output of each layer +
            the output of the embeddings)
            of shape ``(batch_size, sequence_length, hidden_size)``:
            Hidden-states of the model at the output of each layer plus the
            initial embedding outputs.
        **attentions**: (`optional`, returned when
            ``config.output_attentions=True``) list of ``torch.FloatTensor``
            (one for each layer) of shape ``(batch_size, num_heads,
            sequence_length, sequence_length)``: Attentions weights after
            the attention softmax, used to compute the weighted average in the
            self-attention heads.
    Examples::
        # For example purposes. Not runnable.
        transformer = BertModel.from_pretrained('bert-base-uncased')
        encoder = ImageEncoder(args)
        mmbt = MMBTModel(config, transformer, encoder)
    """

    def __init__(self, config, transformer, encoder):
        super().__init__()
        self.is_decoder = config.is_decoder
        self.num_hidden_layers = config.num_hidden_layers
        self.transformer = transformer
        self.modal_encoder = ModalEmbeddings(config, encoder, transformer.embeddings)

    def forward(self, input_modal: Tensor, input_ids: Tensor, modal_start_tokens: Optional[Tensor]=None, modal_end_tokens: Optional[Tensor]=None, attention_mask: Optional[Tensor]=None, token_type_ids: Optional[Tensor]=None, modal_token_type_ids: Optional[Tensor]=None, position_ids: Optional[Tensor]=None, modal_position_ids: Optional[Tensor]=None, head_mask: Optional[Tensor]=None, inputs_embeds: Optional[Tensor]=None, encoder_hidden_states: Optional[Tensor]=None, encoder_attention_mask: Optional[Tensor]=None):
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError('You cannot specify both input_ids and inputs_embeds at the same time')
        elif input_ids is not None:
            input_txt_shape = input_ids.size()
        elif inputs_embeds is not None:
            input_txt_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError('You have to specify either input_ids or inputs_embeds')
        device = inputs_embeds.device if inputs_embeds is not None else input_ids.device
        modal_embeddings = self.modal_encoder(input_modal, start_token=modal_start_tokens, end_token=modal_end_tokens, position_ids=modal_position_ids, token_type_ids=modal_token_type_ids)
        input_modal_shape = modal_embeddings.size()[:-1]
        if token_type_ids is None:
            token_type_ids = torch.ones(input_txt_shape, dtype=torch.long, device=device)
        txt_embeddings = self.transformer.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
        embedding_output = torch.cat([modal_embeddings, txt_embeddings], 1)
        input_shape = embedding_output.size()[:-1]
        if attention_mask is None:
            attention_mask = torch.ones(input_shape, device=device)
        else:
            attention_mask = torch.cat([torch.ones(input_modal_shape, device=device, dtype=torch.long), attention_mask], dim=1)
        if encoder_attention_mask is None:
            encoder_attention_mask = torch.ones(input_shape, device=device)
        else:
            encoder_attention_mask = torch.cat([torch.ones(input_modal_shape, device=device), encoder_attention_mask], dim=1)
        if attention_mask.dim() == 3:
            attention_mask = attention_mask[:, None, :, :]
        if attention_mask.dim() == 2:
            if self.is_decoder:
                batch_size, seq_length = input_shape
                seq_ids = torch.arange(seq_length, device=device)
                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]
                attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]
            else:
                attention_mask = attention_mask[:, None, None, :]
        if not torch.jit.is_scripting():
            attention_mask = attention_mask
        attention_mask = (1.0 - attention_mask) * -10000.0
        if encoder_attention_mask.dim() == 3:
            encoder_attention_mask = encoder_attention_mask[:, None, :, :]
        if encoder_attention_mask.dim() == 2:
            encoder_attention_mask = encoder_attention_mask[:, None, None, :]
        if not torch.jit.is_scripting():
            encoder_attention_mask = encoder_attention_mask
        encoder_attention_mask = (1.0 - encoder_attention_mask) * -10000.0
        encoder_outputs = self.transformer.encoder(embedding_output, attention_mask=attention_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask)
        sequence_output = encoder_outputs[0]
        pooled_output = self.transformer.pooler(sequence_output)
        outputs = sequence_output, pooled_output, encoder_outputs[1:]
        return outputs

    def get_input_embeddings(self):
        return self.embeddings.word_embeddings

    def set_input_embeddings(self, value):
        self.embeddings.word_embeddings = value


class MMBTConfig:
    """Configuration class to store the configuration of a `MMBT Model`.
    Args:
        config (:obj:`~transformers.PreTrainedConfig`):
            Config of the underlying Transformer models. Its values are
            copied over to use a single config.
        num_labels (:obj:`int` or :obj:`None`, optional, defaults to `None`):
            Size of final Linear layer for classification.
        modal_hidden_size (:obj:`int`, optional, defautls to 2048):
            Embedding dimension of the non-text modality encoder.
    """

    def __init__(self, config, num_labels=None, modal_hidden_size=2048):
        self.__dict__ = config.__dict__
        self.modal_hidden_size = modal_hidden_size
        if num_labels:
            self.num_labels = num_labels


class ImageEncoderTypes(Enum):
    default = 'default'
    identity = 'identity'
    torchvision_resnet = 'torchvision_resnet'
    resnet152 = 'resnet152'
    detectron2_resnet = 'detectron2_resnet'


class AttentionTextEmbedding(nn.Module):

    def __init__(self, hidden_dim, embedding_dim, num_layers, dropout, **kwargs):
        super().__init__()
        self.text_out_dim = hidden_dim * kwargs['conv2_out']
        bidirectional = kwargs.get('bidirectional', False)
        self.recurrent_unit = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim // 2 if bidirectional else hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)
        self.dropout = nn.Dropout(p=dropout)
        conv1_out = kwargs['conv1_out']
        conv2_out = kwargs['conv2_out']
        kernel_size = kwargs['kernel_size']
        padding = kwargs['padding']
        self.conv1 = nn.Conv1d(in_channels=hidden_dim, out_channels=conv1_out, kernel_size=kernel_size, padding=padding)
        self.conv2 = nn.Conv1d(in_channels=conv1_out, out_channels=conv2_out, kernel_size=kernel_size, padding=padding)
        self.relu = nn.ReLU()

    def forward(self, x):
        batch_size = x.size(0)
        self.recurrent_unit.flatten_parameters()
        lstm_out, _ = self.recurrent_unit(x)
        lstm_drop = self.dropout(lstm_out)
        lstm_reshape = lstm_drop.permute(0, 2, 1)
        qatt_conv1 = self.conv1(lstm_reshape)
        qatt_relu = self.relu(qatt_conv1)
        qatt_conv2 = self.conv2(qatt_relu)
        qtt_softmax = nn.functional.softmax(qatt_conv2, dim=2)
        qtt_feature = torch.bmm(qtt_softmax, lstm_drop)
        qtt_feature_concat = qtt_feature.view(batch_size, -1)
        return qtt_feature_concat


class BiLSTMTextEmbedding(nn.Module):

    def __init__(self, hidden_dim, embedding_dim, num_layers, dropout, bidirectional=False, rnn_type='GRU'):
        super().__init__()
        self.text_out_dim = hidden_dim
        self.bidirectional = bidirectional
        if rnn_type == 'LSTM':
            rnn_cls = nn.LSTM
        elif rnn_type == 'GRU':
            rnn_cls = nn.GRU
        self.recurrent_encoder = rnn_cls(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)

    def forward(self, x):
        out, _ = self.recurrent_encoder(x)
        if self.bidirectional:
            return out[:, -1]
        forward_ = out[:, -1, :self.num_hid]
        backward = out[:, 0, self.num_hid:]
        return torch.cat((forward_, backward), dim=1)

    def forward_all(self, x):
        output, _ = self.recurrent_encoder(x)
        return output


class Identity(nn.Module):

    def __init__(self, **kwargs):
        super().__init__()

    def forward(self, x):
        return x


class PreExtractedEmbedding(nn.Module):

    def __init__(self, out_dim, base_path):
        super().__init__()
        self.text_out_dim = out_dim
        self.base_path = base_path
        self.cache = {}

    def forward(self, qids):
        embeddings = []
        for qid in qids:
            embeddings.append(self.get_item(qid))
        return torch.stack(embeddings, dim=0)

    @lru_cache(maxsize=5000)
    def get_item(self, qid):
        return np.load(os.path.join(self.base_path, str(qid.item()) + '.npy'))


class ProjectionEmbedding(nn.Module):

    def __init__(self, module, in_dim, out_dim, **kwargs):
        super().__init__()
        if module == 'linear':
            self.layers = nn.Linear(in_dim, out_dim)
            self.out_dim = out_dim
        elif module == 'conv':
            last_out_channels = in_dim
            layers = []
            for conv in kwargs['convs']:
                layers.append(nn.Conv1d(in_channels=last_out_channels, **conv))
                last_out_channels = conv['out_channels']
            self.layers = nn.ModuleList(*layers)
            self.out_dim = last_out_channels
        else:
            raise TypeError("Unknown module type for 'ProjectionEmbedding',use either 'linear' or 'conv'")

    def forward(self, x):
        return self.layers(x)


class AttnPool1d(nn.Module):
    """An attention pooling layer that learns weights using an mlp"""

    def __init__(self, num_features: int, num_attn: int=1, dropout: float=0.1):
        super().__init__()
        self.linear = nn.Sequential(nn.Linear(num_features, num_features // 2), nn.ReLU(), nn.Dropout(p=dropout), nn.Linear(num_features // 2, num_attn))
        self.p_attn = torch.tensor(float('nan'))
        self.num_attn = num_attn

    def forward(self, query: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor]=None) ->torch.Tensor:
        b = query.size(0)
        score = self.linear(query).transpose(-2, -1)
        if mask is not None:
            score.data.masked_fill_(mask.unsqueeze(1), -10000.0)
        p_attn = nn.functional.softmax(score, dim=-1)
        if self.training:
            self.p_attn = p_attn
        return torch.matmul(p_attn, value).view(b, self.num_attn, -1)


class MovieMcanMultiHeadAttention(nn.Module):
    """
    Multi-Head Attention implementation from https://arxiv.org/abs/1706.03762
    used for Movie+MCAN
    """

    def __init__(self, dim: int, num_attn: int, dropout: float=0.1):
        super().__init__()
        self.p_attn = None
        self.h = num_attn
        self.d_k = dim // num_attn
        self.linears = nn.ModuleList([nn.Linear(dim, dim) for _ in range(4)])
        self.dropout = nn.Dropout(p=dropout)

    def qkv_attention(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor]=None, dropout: Type[nn.Dropout]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        d_k = query.size(-1)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            scores.data.masked_fill_(mask.unsqueeze(1).unsqueeze(2), -1000000000.0)
        p_attn = nn.functional.softmax(scores, dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        return torch.matmul(p_attn, value), p_attn

    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor) ->torch.Tensor:
        b = q.size(0)
        q = self.linears[0](q).view(b, -1, self.h, self.d_k).transpose(1, 2)
        k = self.linears[1](k).view(b, -1, self.h, self.d_k).transpose(1, 2)
        v = self.linears[2](v).view(b, -1, self.h, self.d_k).transpose(1, 2)
        x, self.p_attn = self.qkv_attention(q, k, v, mask=mask, dropout=self.dropout)
        x = x.transpose(1, 2).contiguous().view(b, -1, self.h * self.d_k)
        return self.linears[-1](x)


class SelfAttention(nn.Module):

    def __init__(self, dim: int, num_attn: int, dropout: float):
        super().__init__()
        self.multi_head_attn = MovieMcanMultiHeadAttention(dim, num_attn, dropout=0.1)
        self.fcn = nn.Sequential(nn.Linear(dim, 4 * dim), nn.ReLU(inplace=True), nn.Dropout(p=dropout), nn.Linear(4 * dim, dim))
        self.drop_mha = nn.Dropout(p=dropout)
        self.ln_mha = nn.LayerNorm(dim)
        self.drop_fcn = nn.Dropout(p=dropout)
        self.ln_fcn = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor, x_mask: torch.Tensor) ->torch.Tensor:
        x = self.ln_mha(x + self.drop_mha(self.multi_head_attn(x, x, x, x_mask)))
        x = self.ln_fcn(x + self.drop_fcn(self.fcn(x)))
        return x


class SAEmbedding(nn.Module):
    """Encoder block implementation in MCAN https://arxiv.org/abs/1906.10770"""

    def __init__(self, hidden_dim: int, embedding_dim: int, **kwargs):
        super().__init__()
        num_attn = kwargs['num_attn']
        num_layers = kwargs['num_layers']
        dropout = kwargs.get('dropout', 0.1)
        num_attn_pool = kwargs.get('num_attn_pool', 1)
        num_feat = kwargs.get('num_feat', -1)
        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)
        self.self_attns = nn.ModuleList([SelfAttention(hidden_dim, num_attn, dropout) for _ in range(num_layers)])
        self.attn_pool = None
        self.num_feat = num_feat
        self.text_out_dim = hidden_dim
        if num_attn_pool > 0:
            self.attn_pool = AttnPool1d(hidden_dim, num_feat * num_attn_pool)
            self.text_out_dim = hidden_dim * num_attn_pool

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        b = x.size(0)
        out, (h, c) = self.lstm(x)
        for self_attn in self.self_attns:
            out = self_attn(out, mask)
        vec = h.transpose(0, 1).contiguous().view(b, 1, -1)
        if self.attn_pool:
            vec = self.attn_pool(out, out, mask).view(b, self.num_feat, -1)
        return out, vec


class BaseVocab:
    PAD_TOKEN = '<pad>'
    SOS_TOKEN = '<s>'
    EOS_TOKEN = '</s>'
    UNK_TOKEN = '<unk>'
    PAD_INDEX = 0
    SOS_INDEX = 1
    EOS_INDEX = 2
    UNK_INDEX = 3

    def __init__(self, vocab_file=None, embedding_dim=300, data_dir=None, *args, **kwargs):
        """Vocab class to be used when you want to train word embeddings from
        scratch based on a custom vocab. This will initialize the random
        vectors for the vocabulary you pass. Get the vectors using
        `get_vectors` function. This will also create random embeddings for
        some predefined words like PAD - <pad>, SOS - <s>, EOS - </s>,
        UNK - <unk>.

        Parameters
        ----------
        vocab_file : str
            Path of the vocabulary file containing one word per line
        embedding_dim : int
            Size of the embedding

        """
        self.type = 'base'
        self.word_dict = {}
        self.itos = {}
        self.itos[self.PAD_INDEX] = self.PAD_TOKEN
        self.itos[self.SOS_INDEX] = self.SOS_TOKEN
        self.itos[self.EOS_INDEX] = self.EOS_TOKEN
        self.itos[self.UNK_INDEX] = self.UNK_TOKEN
        self.word_dict[self.SOS_TOKEN] = self.SOS_INDEX
        self.word_dict[self.EOS_TOKEN] = self.EOS_INDEX
        self.word_dict[self.PAD_TOKEN] = self.PAD_INDEX
        self.word_dict[self.UNK_TOKEN] = self.UNK_INDEX
        index = len(self.itos.keys())
        self.total_predefined = len(self.itos.keys())
        if vocab_file is not None:
            if not os.path.isabs(vocab_file) and data_dir is not None:
                vocab_file = os.path.join(data_dir, vocab_file)
                vocab_file = get_absolute_path(vocab_file)
            if not PathManager.exists(vocab_file):
                raise RuntimeError('Vocab not found at ' + vocab_file)
            with PathManager.open(vocab_file, 'r') as f:
                for line in f:
                    self.itos[index] = line.strip()
                    self.word_dict[line.strip()] = index
                    index += 1
        self.word_dict[self.SOS_TOKEN] = self.SOS_INDEX
        self.word_dict[self.EOS_TOKEN] = self.EOS_INDEX
        self.word_dict[self.PAD_TOKEN] = self.PAD_INDEX
        self.word_dict[self.UNK_TOKEN] = self.UNK_INDEX
        self.stoi = defaultdict(self.get_unk_index)
        self.stoi.update(self.word_dict)
        self.vectors = torch.FloatTensor(self.get_size(), embedding_dim)

    def get_itos(self):
        return self.itos

    def get_stoi(self):
        return self.stoi

    def get_size(self):
        return len(self.itos)

    def get_pad_index(self):
        return self.PAD_INDEX

    def get_pad_token(self):
        return self.PAD_TOKEN

    def get_start_index(self):
        return self.SOS_INDEX

    def get_start_token(self):
        return self.SOS_TOKEN

    def get_end_index(self):
        return self.EOS_INDEX

    def get_end_token(self):
        return self.EOS_TOKEN

    def get_unk_index(self):
        return self.UNK_INDEX

    def get_unk_token(self):
        return self.UNK_TOKEN

    def get_vectors(self):
        return getattr(self, 'vectors', None)

    def get_embedding(self, cls, **embedding_kwargs):
        vector_dim = len(self.vectors[0])
        embedding_kwargs['vocab_size'] = self.get_size()
        embedding_dim = embedding_kwargs['embedding_dim']
        embedding_kwargs['embedding_dim'] = vector_dim
        embedding = None
        if cls == torch.nn.Embedding:
            embedding = torch.nn.Embedding(self.get_size(), vector_dim)
        else:
            embedding = cls(**embedding_kwargs)
        if hasattr(embedding, 'embedding'):
            embedding.embedding = torch.nn.Embedding.from_pretrained(self.vectors, freeze=False)
        else:
            embedding = torch.nn.Embedding.from_pretrained(self.vectors, freeze=False)
        if vector_dim == embedding_dim:
            return embedding
        else:
            return torch.nn.Sequential([embedding, torch.nn.Linear(vector_dim, embedding_dim)])


class CustomVocab(BaseVocab):

    def __init__(self, vocab_file, embedding_file, data_dir=None, *args, **kwargs):
        """Use this vocab class when you have a custom vocab as well as a
        custom embeddings file.

        This will inherit vocab class, so you will get predefined tokens with
        this one.

        IMPORTANT: To init your embedding, get your vectors from this class's
        object by calling `get_vectors` function

        Parameters
        ----------
        vocab_file : str
            Path of custom vocabulary
        embedding_file : str
            Path to custom embedding inititalization file
        data_dir : str
            Path to data directory if embedding file is not an absolute path.
            Default: None
        """
        super().__init__(vocab_file)
        self.type = 'custom'
        if not os.path.isabs(embedding_file) and data_dir is not None:
            embedding_file = os.path.join(data_dir, embedding_file)
            embedding_file = get_absolute_path(embedding_file)
        if not PathManager.exists(embedding_file):
            raise RuntimeError(f"Embedding file path {embedding_file} doesn't exist")
        embedding_vectors = torch.from_numpy(np.load(embedding_file))
        self.vectors = torch.FloatTensor(self.get_size(), len(embedding_vectors[0]))
        for i in range(0, 4):
            self.vectors[i] = torch.ones_like(self.vectors[i]) * 0.1 * i
        for i in range(4, self.get_size()):
            self.vectors[i] = embedding_vectors[i - 4]


class ExtractedVocab(BaseVocab):

    def __init__(self, base_path, emb_dim, *args, **kwargs):
        """Special vocab which is not really vocabulary but instead a class
        which returns embedding pre-extracted from files. Can be used load
        word embeddings from popular models like ELMo and BERT


        Parameters
        ----------
        base_path: str
            path containing saved files with embeddings one file per txt item
        """
        super().__init__(*args, **kwargs)
        self.type = 'extracted'
        self.emb_dim = emb_dim
        self.base_path = base_path

    def get_dim(self):
        return self.emb_dim


EMBEDDING_NAME_CLASS_MAPPING = {'glove': 'GloVe', 'fasttext': 'FastText'}


def get_rank():
    if is_xla():
        return xm.get_ordinal()
    if not dist.is_available():
        return 0
    if not dist.is_nccl_available():
        return 0
    if not dist.is_initialized():
        return 0
    return dist.get_rank()


def is_master():
    return get_rank() == 0


def is_main():
    return is_master()


def synchronize(message='sync-workers'):
    if is_xla():
        xm.rendezvous(message)
    elif not dist.is_available():
        return
    if not dist.is_nccl_available():
        return
    if not dist.is_initialized():
        return
    world_size = dist.get_world_size()
    if world_size == 1:
        return
    dist.barrier()


class IntersectedVocab(BaseVocab):

    def __init__(self, vocab_file, embedding_name, *args, **kwargs):
        """Use this vocab class when you have a custom vocabulary class but you
        want to use pretrained embedding vectos for it. This will only load
        the vectors which intersect with your vocabulary. Use the
        embedding_name specified in torchtext's pretrained aliases:
        ['charngram.100d', 'fasttext.en.300d', 'fasttext.simple.300d',
         'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B.25d',
         'glove.twitter.27B.50d', 'glove.twitter.27B.100d',
         'glove.twitter.27B.200d', 'glove.6B.50d', 'glove.6B.100d',
         'glove.6B.200d', 'glove.6B.300d']

        Parameters
        ----------
        vocab_file : str
            Vocabulary file containing list of words with one word per line
            which will be used to collect vectors
        embedding_name : str
            Embedding name picked up from the list of the pretrained aliases
            mentioned above
        """
        super().__init__(vocab_file, *args, **kwargs)
        self.type = 'intersected'
        name = embedding_name.split('.')[0]
        dim = embedding_name.split('.')[2][:-1]
        middle = embedding_name.split('.')[1]
        class_name = EMBEDDING_NAME_CLASS_MAPPING[name]
        if not hasattr(vocab, class_name):
            raise RuntimeError(f'Unknown embedding type: {name}')
        params = [middle]
        if name == 'glove':
            params.append(int(dim))
        vector_cache = get_mmf_cache_dir()
        if is_main():
            vocab.pretrained_aliases[embedding_name](cache=vector_cache)
        synchronize()
        embedding = getattr(vocab, class_name)(*params, cache=vector_cache)
        self.vectors = torch.empty((self.get_size(), len(embedding.vectors[0])), dtype=torch.float)
        self.embedding_dim = len(embedding.vectors[0])
        for i in range(0, 4):
            self.vectors[i] = torch.ones_like(self.vectors[i]) * 0.1 * i
        for i in range(4, self.get_size()):
            word = self.itos[i]
            embedding_index = embedding.stoi.get(word, None)
            if embedding_index is None:
                self.vectors[i] = self.vectors[self.UNK_INDEX]
            else:
                self.vectors[i] = embedding.vectors[embedding_index]

    def get_embedding_dim(self):
        return self.embedding_dim


class WordToVectorDict:

    def __init__(self, model):
        self.model = model

    def __getitem__(self, word):
        return np.mean([self.model.get_word_vector(w) for w in word.split(' ')], axis=0)


class ModelVocab(BaseVocab):

    def __init__(self, name, model_file, *args, **kwargs):
        """Special vocab which is not really vocabulary but instead a model
        which returns embedding directly instead of vocabulary. This is just
        an abstraction over a model which generates embeddings directly.
        For e.g. for fasttext model we encapsulate it inside this and provide
        it as a vocab so that the API of the vocab remains same.

        NOTE: stoi's functionality will remain same but it is actually calling
        a function to get word vectors. Currently, only fasttext is supported.

        Parameters
        ----------
        name : str
            Name of the embedding model which this vocab currently is loading
        model_file : str
            File from which model will be loaded. This API might need to be
            changed in future.
        """
        super().__init__(*args, **kwargs)
        self.type = 'model'
        if name != 'fasttext':
            raise ValueError('Model vocab only supports fasttext as of now')
        else:
            self._load_fasttext_model(model_file)

    def _load_fasttext_model(self, model_file):
        model_file = os.path.join(get_mmf_cache_dir(), model_file)
        logger.info(f'Loading fasttext model now from {model_file}')
        self.model = load_model(model_file)
        self.stoi = WordToVectorDict(self.model)

    def get_embedding_dim(self):
        return self.model.get_dimension()


class PretrainedVocab(BaseVocab):

    def __init__(self, embedding_name, *args, **kwargs):
        """Use this if you want to use pretrained embedding. See description
        of IntersectedVocab to get a list of the embedding available from
        torchtext

        Parameters
        ----------
        embedding_name : str
            Name of the pretrained alias for the embedding to used
        """
        self.type = 'pretrained'
        if embedding_name not in vocab.pretrained_aliases:
            raise RuntimeError(f'Unknown embedding type: {embedding_name}')
        vector_cache = get_mmf_cache_dir()
        if is_main():
            vocab.pretrained_aliases[embedding_name](cache=vector_cache)
        synchronize()
        embedding = vocab.pretrained_aliases[embedding_name](cache=vector_cache)
        self.UNK_INDEX = 3
        self.stoi = defaultdict(lambda : self.UNK_INDEX)
        self.itos = {}
        self.itos[self.PAD_INDEX] = self.PAD_TOKEN
        self.itos[self.SOS_INDEX] = self.SOS_TOKEN
        self.itos[self.EOS_INDEX] = self.EOS_TOKEN
        self.itos[self.UNK_INDEX] = self.UNK_TOKEN
        self.stoi[self.SOS_TOKEN] = self.SOS_INDEX
        self.stoi[self.EOS_TOKEN] = self.EOS_INDEX
        self.stoi[self.PAD_TOKEN] = self.PAD_INDEX
        self.stoi[self.UNK_TOKEN] = self.UNK_INDEX
        self.vectors = torch.FloatTensor(len(self.itos.keys()) + len(embedding.itos), len(embedding.vectors[0]))
        for i in range(4):
            self.vectors[i] = torch.ones_like(self.vectors[i]) * 0.1 * i
        index = 4
        for word in embedding.stoi:
            self.itos[index] = word
            self.stoi[word] = index
            actual_index = embedding.stoi[word]
            self.vectors[index] = embedding.vectors[actual_index]
            index += 1


class Vocab:

    def __init__(self, *args, **params):
        vocab_type = params.get('type', 'pretrained')
        if vocab_type == 'random':
            if params['vocab_file'] is None:
                raise ValueError('No vocab path passed for vocab')
            self.vocab = BaseVocab(*args, **params)
        elif vocab_type == 'custom':
            if params['vocab_file'] is None or params['embedding_file'] is None:
                raise ValueError('No vocab path or embedding_file passed for vocab')
            self.vocab = CustomVocab(*args, **params)
        elif vocab_type == 'pretrained':
            self.vocab = PretrainedVocab(*args, **params)
        elif vocab_type == 'intersected':
            if params['vocab_file'] is None or params['embedding_name'] is None:
                raise ValueError('No vocab path or embedding_name passed for vocab')
            self.vocab = IntersectedVocab(*args, **params)
        elif vocab_type == 'extracted':
            if params['base_path'] is None or params['embedding_dim'] is None:
                raise ValueError('No base_path or embedding_dim passed for vocab')
            self.vocab = ExtractedVocab(*args, **params)
        elif vocab_type == 'model':
            if params['name'] is None or params['model_file'] is None:
                raise ValueError('No name or model_file passed for vocab')
            if params['name'] == 'fasttext':
                self.vocab = ModelVocab(*args, **params)
        else:
            raise ValueError('Unknown vocab type: %s' % vocab_type)
        self._dir_representation = dir(self)

    def __call__(self, *args, **kwargs):
        return self.vocab(*args, **kwargs)

    def __getattr__(self, name):
        if '_dir_representation' in self.__dict__ and name in self._dir_representation:
            return getattr(self, name)
        elif 'vocab' in self.__dict__ and hasattr(self.vocab, name):
            return getattr(self.vocab, name)
        else:
            type_vocab = 'Vocab'
            if 'vocab' in self.__dict__:
                type_vocab = type(self.vocab)
            raise AttributeError(f'{type_vocab} vocab type has no attribute {name}.')


class VocabEmbedding(nn.Module):

    def __init__(self, embedding_dim, **vocab_params):
        super().__init__()
        self.vocab = Vocab(**vocab_params)
        self.module = self.vocab.get_embedding(nn.Embedding, embedding_dim=embedding_dim)

    def forward(self, x):
        return self.module(x)


class TextEmbedding(nn.Module):

    def __init__(self, emb_type, **kwargs):
        super().__init__()
        self.model_data_dir = kwargs.get('model_data_dir', None)
        self.embedding_dim = kwargs.get('embedding_dim', None)
        if emb_type == 'identity':
            self.module = Identity()
            self.module.text_out_dim = self.embedding_dim
        elif emb_type == 'vocab':
            self.module = VocabEmbedding(**kwargs)
            self.module.text_out_dim = self.embedding_dim
        elif emb_type == 'projection':
            self.module = ProjectionEmbedding(**kwargs)
            self.module.text_out_dim = self.module.out_dim
        elif emb_type == 'preextracted':
            self.module = PreExtractedEmbedding(**kwargs)
        elif emb_type == 'bilstm':
            self.module = BiLSTMTextEmbedding(**kwargs)
        elif emb_type == 'attention':
            self.module = AttentionTextEmbedding(**kwargs)
        elif emb_type == 'mcan':
            self.module = SAEmbedding(**kwargs)
        elif emb_type == 'torch':
            vocab_size = kwargs['vocab_size']
            embedding_dim = kwargs['embedding_dim']
            self.module = nn.Embedding(vocab_size, embedding_dim)
            self.module.text_out_dim = self.embedding_dim
        else:
            raise NotImplementedError("Unknown question embedding '%s'" % emb_type)
        self.text_out_dim = self.module.text_out_dim

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class TextEncoderTypes(Enum):
    identity = 'identity'
    transformer = 'transformer'
    embedding = 'embedding'


def build_image_encoder(config, direct_features=False, **kwargs):
    """Deprecated, please do not use"""
    if direct_features:
        module = ImageFeatureEncoderFactory(config)
    else:
        module = ImageEncoderFactory(config)
    return module.module


def _get_modules_dict(modules_list):
    """
    Expects a list of str module names.
    Returns a dict of module_name: module obj,
    a subset of globals().
    """
    global_table = globals()
    return {module_name: global_table[module_name] for module_name in modules_list}


patch_functions = ['BertEmbeddings.forward', 'BertEncoder.forward', 'BertLayer.forward', 'BertAttention.forward', 'BertSelfAttention.forward', 'BertSelfAttention.transpose_for_scores', 'BertModel.forward', 'RobertaEmbeddings.forward', 'RobertaEncoder.forward', 'RobertaLayer.forward', 'RobertaAttention.forward', 'RobertaSelfAttention.forward', 'RobertaSelfAttention.transpose_for_scores', 'RobertaModel.forward']


patch_modules = [p_fun.split('.')[0] for p_fun in patch_functions]


ORIGINAL_PATCH_FUNCTIONS_KEY = 'original_patch_functions'


def safecopy_modules(module_function_names, caller_modules):
    """
    Saves a reference to each module.function in list of strings module_function_names.
    References are made from dict caller_modules, from module name str to
    caller module obj.
    module.functions can be reassigned, replacing the current functions using
    restore_saved_modules(caller_modules)

    Example:
        from transformers.modeling_bert import BertSelfAttention

        caller_modules = {'BertSelfAttention': BertSelfAttention}
        original_forward = BertSelfAttention.forward
        safecopy_modules(['BertSelfAttention.forward'], caller_modules)
        BertSelfAttention.forward = None
        restore_saved_modules(caller_modules)
        assert( original_forward is BertSelfAttention.forward )
    """
    original_functions = registry.get(ORIGINAL_PATCH_FUNCTIONS_KEY)
    for module_function_name in module_function_names:
        module_name, function_name = module_function_name.split('.')
        module = caller_modules[module_name]
        function = getattr(module, function_name)
        original_functions[module_function_name] = original_functions.get(module_function_name, function)


def replace_with_jit():
    """
    Monkey patch some transformer functions to replace with scriptable ones.
    """
    safecopy_modules(patch_functions, _get_modules_dict(patch_modules))
    BertEmbeddings.forward = BertEmbeddingsJit.forward
    BertEncoder.forward = BertEncoderJit.forward
    BertLayer.forward = BertLayerJit.forward
    BertAttention.forward = BertAttentionJit.forward
    BertSelfAttention.forward = BertSelfAttentionJit.forward
    BertSelfAttention.transpose_for_scores = BertSelfAttentionJit.transpose_for_scores
    BertModel.forward = BertModelJit.forward
    PreTrainedModel.__jit_unused_properties__ = ['base_model', 'dummy_inputs', 'device', 'dtype']
    RobertaEmbeddings.forward = RobertaEmbeddingsJit.forward
    RobertaEncoder.forward = BertEncoderJit.forward
    RobertaLayer.forward = BertLayerJit.forward
    RobertaAttention.forward = BertAttentionJit.forward
    RobertaSelfAttention.forward = BertSelfAttentionJit.forward
    RobertaSelfAttention.transpose_for_scores = BertSelfAttentionJit.transpose_for_scores
    RobertaModel.forward = BertModelJit.forward


class MMBTForPreTraining(nn.Module):

    def __init__(self, config, *args, **kwargs):
        super().__init__()
        self.config = config
        self.bert = MMBTBase(config, *args, **kwargs)
        self.encoder_config = self.bert.encoder_config
        pretraining_module = BertForPreTraining.from_pretrained(self.config.bert_model_name, config=self.encoder_config, cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)))
        self.cls = deepcopy(pretraining_module.cls)
        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-1)
        self.tie_weights()

    def tie_weights(self):
        """Make sure we are sharing the input and output embeddings.
        Export to TorchScript can't handle parameter sharing so we
        are cloning them instead.
        """
        if hasattr(self, 'cls'):
            self.bert.mmbt.transformer._tie_or_clone_weights(self.cls.predictions.decoder, self.bert.mmbt.transformer.embeddings.word_embeddings)

    def forward(self, sample_list):
        module_output = self.bert(sample_list)
        sequence_output, pooled_output = module_output[0], module_output[1]
        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)
        output = {}
        if self.encoder_config.output_hidden_states or self.encoder_config.output_attentions:
            output['extras'] = module_output[2:]
        loss_key = f'{sample_list.dataset_name}/{sample_list.dataset_type}'
        if 'lm_label_ids' in sample_list and sample_list.lm_label_ids is not None:
            output['logits'] = prediction_scores
            lm_label_ids = sample_list.lm_label_ids
            text_scores = prediction_scores[:, -lm_label_ids.size(1):].contiguous().view(-1, self.encoder_config.vocab_size)
            masked_lm_loss = self.loss_fct(text_scores, sample_list.lm_label_ids.contiguous().view(-1))
            output['losses'] = {}
            output['losses'][f'{loss_key}/masked_lm_loss'] = masked_lm_loss
        if 'image_text_alignment' in sample_list and sample_list.image_text_alignment is not None:
            output['seq_relationship_logits'] = seq_relationship_score
            alignment_loss = self.loss_fct(seq_relationship_score.contiguous().view(-1), sample_list.image_text_alignment.contiguous().view(-1))
            output['losses'][f'{loss_key}/alignment_loss'] = alignment_loss
        return output


class MMBTForClassification(nn.Module):

    def __init__(self, config, *args, **kwargs):
        super().__init__()
        self.config = config
        self.bert = MMBTBase(config, *args, **kwargs)
        self.encoder_config = self.bert.encoder_config
        self.num_labels = self.config.num_labels
        self.output_hidden_states = self.encoder_config.output_hidden_states
        self.output_attentions = self.encoder_config.output_attentions
        self.fused_feature_only = self.config.get('fused_feature_only', False)
        self.dropout = nn.Dropout(self.encoder_config.hidden_dropout_prob)
        self.classifier = nn.Sequential(BertPredictionHeadTransform(self.encoder_config), nn.Linear(self.encoder_config.hidden_size, self.config.num_labels))

    def forward(self, sample_list: Dict[str, Tensor]):
        module_output = self.bert(sample_list)
        pooled_output = module_output[1]
        output = {}
        if not torch.jit.is_scripting():
            if self.output_hidden_states or self.output_attentions:
                output['extras'] = module_output[2:]
        else:
            assert not (self.output_hidden_states or self.output_attentions), 'output_attentions or output_hidden_states not supported in script mode'
        pooled_output = self.dropout(pooled_output)
        if self.fused_feature_only:
            output['fused_feature'] = self.classifier[0](pooled_output)
            return output
        logits = self.classifier(pooled_output)
        reshaped_logits = logits.contiguous().view(-1, self.num_labels)
        output['scores'] = reshaped_logits
        return output


LABEL_KEY = 'mlm_labels'


class MLP(nn.Module):

    def __init__(self, input_dim, dimensions, activation='relu', dropout=0.0):
        super().__init__()
        self.input_dim = input_dim
        self.dimensions = dimensions
        self.activation = activation
        self.dropout = dropout
        self.linears = nn.ModuleList([nn.Linear(input_dim, dimensions[0])])
        for din, dout in zip(dimensions[:-1], dimensions[1:]):
            self.linears.append(nn.Linear(din, dout))

    def forward(self, x):
        for i, lin in enumerate(self.linears):
            x = lin(x)
            if i < len(self.linears) - 1:
                x = F.__dict__[self.activation](x)
                if self.dropout > 0:
                    x = F.dropout(x, self.dropout, training=self.training)
        return x


COMBINED_LABEL_KEY = 'combined_labels'


def compute_masked_hidden(hidden: Tensor, mask: Tensor) ->Tensor:
    """Get only the masked region.

    hidden: tensor, dim (bs, num_feat, feat_dim)
    mask: bool tensor, dim (bs, num_feat)
    Returns a tensor of dim (bs * num_feat_unmasked, feat_dim),
    containing the features in hidden that are True in the mask tensor.
    """
    mask = mask.unsqueeze(-1).expand_as(hidden)
    hidden_masked = hidden[mask].contiguous().view(-1, hidden.size(-1))
    return hidden_masked


class MRC(nn.Module):

    def __init__(self, hidden_size: int=768, loss_name: str='mrc_loss', ignore_index: int=-1, mrc_label_key: str='region_class', mrc_mask_key: str='image_region_mask', label_dim: int=1601, eps: float=1e-12, use_kl: bool=True, *args, **kwargs):
        super().__init__()
        self.loss_name = loss_name
        self.ignore_index = ignore_index
        self.mrc_label_key = mrc_label_key
        self.mrc_mask_key = mrc_mask_key
        self.use_kl = use_kl
        self.region_classifier = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.GELU(), nn.LayerNorm(hidden_size, eps=eps), nn.Linear(hidden_size, label_dim))

    def forward(self, sequence_output: Tensor, processed_sample_list: Dict[str, Dict[str, Tensor]]) ->Dict[str, Dict[str, Tensor]]:
        output_dict = {}
        assert self.mrc_label_key in processed_sample_list and processed_sample_list[self.mrc_label_key] is not None, f'MRC pretraining requires {self.mrc_label_key} to be in sample ' + 'list with value not None.'
        region_labels = processed_sample_list[self.mrc_label_key]
        assert self.mrc_mask_key in processed_sample_list and processed_sample_list[self.mrc_mask_key] is not None, f'MRC pretraining requires {self.mrc_mask_key} to be in sample ' + 'list with value not None.'
        image_region_masks = processed_sample_list[self.mrc_mask_key]
        masked_output = compute_masked_hidden(sequence_output, image_region_masks)
        prediction_soft_label = self.region_classifier(masked_output)
        if self.use_kl:
            prediction_soft_label = F.log_softmax(prediction_soft_label, dim=-1)
            mrc_loss = F.kl_div(prediction_soft_label, region_labels, reduction='batchmean')
        else:
            label_targets = torch.max(region_labels[:, 1:], dim=-1)[1] + 1
            mrc_loss = F.cross_entropy(prediction_soft_label, label_targets, ignore_index=self.ignore_index, reduction='mean')
        output_dict['losses'] = {}
        output_dict['losses'][self.loss_name] = mrc_loss
        return output_dict


class MRFR(nn.Module):
    """
    Masked Region Feature Regression transformer head,
    From uniter paper https://arxiv.org/pdf/1909.11740.pdf
    For an example usage take a look at the unit test.
    """

    def __init__(self, img_embedding_weight: nn.Parameter, hidden_size: int=768, loss_name: str='mrfr_loss', mrfr_target_key: str='mrfr_region_target', mrfr_mask_key: str='mrfr_region_mask', img_dim: int=2048, eps: float=1e-12, *args, **kwargs):
        super().__init__()
        self.loss_name = loss_name
        self.mrfr_target_key = mrfr_target_key
        self.mrfr_mask_key = mrfr_mask_key
        assert img_embedding_weight is not None and tuple(img_embedding_weight.shape) == (hidden_size, img_dim), "MRFR head requires 'img_embedding_weight' with shape " + f'({hidden_size}, {img_dim}).'
        self.linear_proj_weight = img_embedding_weight
        self.linear_proj_bias = nn.Parameter(torch.zeros(img_dim))
        self.feat_regress = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.GELU(), nn.LayerNorm(hidden_size, eps=eps))

    def forward(self, sequence_output: Tensor, processed_sample_list: Dict[str, Dict[str, Tensor]]) ->Dict[str, Dict[str, Tensor]]:
        output_dict = {}
        assert self.mrfr_target_key in processed_sample_list and processed_sample_list[self.mrfr_target_key] is not None, f'MRFR pretraining requires {self.mrfr_target_key} to be in sample ' + 'list with value not None.'
        feat_targets = processed_sample_list[self.mrfr_target_key]
        assert self.mrfr_mask_key in processed_sample_list and processed_sample_list[self.mrfr_mask_key] is not None, f'MRFR pretraining requires {self.mrfr_mask_key} to be in sample ' + 'list with value not None.'
        image_region_masks = processed_sample_list[self.mrfr_mask_key]
        masked_output = compute_masked_hidden(sequence_output, image_region_masks)
        hidden_states = self.feat_regress(masked_output)
        prediction_feat = F.linear(hidden_states, self.linear_proj_weight.t(), self.linear_proj_bias)
        mrfr_loss = F.mse_loss(prediction_feat, feat_targets, reduction='mean')
        output_dict['losses'] = {}
        output_dict['losses'][self.loss_name] = mrfr_loss
        return output_dict


class RefinerContrastiveLoss(nn.Module):
    """
    A contrastive loss between the decoder outputs of a given embedding size
    and its targets

    This loss can be used in lieu of a reconstruction loss, wherein the goal
    is to get a decoded signal closer to its target than other targets. As long
    as the reconstructed signal of a given input is closer to its target than
    any other target, the loss will remain zero.

    Reference:

    Sankaran, S., Yang, D. and Lim, S.N., "Multimodal Fusion Refiner Networks"

    Parameters:

        sim_thresh: similarity threshold used to consider only samples beyond
        # this threshold

    """

    def __init__(self, sim_thresh=0.1, epsilon=1e-16):
        super().__init__()
        self.similarity_threshold = sim_thresh
        self.epsilon = epsilon

    def forward(self, sample_list, model_output):
        targets = sample_list['targets']
        inputs = model_output['scores']
        batch_size = inputs.size(0)
        inputs = F.normalize(inputs)
        targets = F.normalize(targets)
        sim_mat = torch.matmul(inputs, targets.t())
        loss = []
        for i in range(batch_size):
            sim_ij = sim_mat[i]
            pos_similarity = sim_ij[i]
            neg_pair_ = torch.masked_select(sim_ij, sim_ij > pos_similarity - self.similarity_threshold)
            neg_pair_ = torch.masked_select(neg_pair_, abs(neg_pair_ - pos_similarity) > self.epsilon)
            if neg_pair_.shape[0] > 0:
                neg_loss = torch.mean(self.similarity_threshold + neg_pair_ - pos_similarity)
                loss.append(neg_loss)
        if len(loss) == 0:
            loss = inputs.new_zeros(1, requires_grad=True)
        else:
            loss = sum(loss) / batch_size
        return loss


def calc_ms_loss(pair, base, param, multiplier):
    return 1.0 / param * torch.log(1 + torch.sum(torch.exp(multiplier * param * (pair - base))))


class RefinerMSLoss(nn.Module):
    """
    A Multi-Similarity loss between the decoder outputs of a given embedding size
    and its targets

    This loss pulls the decoded signal of a sample closer to its target,
    while simultaneously pushing it away from other targets

    References:

    1) Wang et al., Multi-Similarity Loss With General Pair Weighting
    for Deep Metric Learning, CVPR 2019
    2) Sankaran, S., Yang, D. and Lim, S.N., "Multimodal Fusion Refiner Networks"

    Parameters:

        same as ms_loss (see below)

    """

    def __init__(self, alpha: float=50, beta: float=2, base: float=0.5, margin: float=0.1, epsilon: float=1e-16):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.margin = margin
        self.base = base
        self.epsilon = epsilon

    def forward(self, sample_list, model_output):
        targets = sample_list['targets']
        inputs = model_output['scores']
        n = inputs.size(0)
        sim_mat = torch.matmul(inputs, targets.t())
        loss = []
        for i in range(n):
            pos_pair = sim_mat[i][i]
            neg_pairs_all = sim_mat[i]
            neg_pairs_all = neg_pairs_all[abs(neg_pairs_all - pos_pair) > self.epsilon]
            neg_pairs = neg_pairs_all[neg_pairs_all + self.margin > pos_pair]
            if len(neg_pairs) < 1:
                continue
            pos_loss = calc_ms_loss(pos_pair, self.base, self.beta, -1)
            neg_loss = calc_ms_loss(neg_pairs, self.base, self.alpha, 1)
            loss.append(pos_loss + neg_loss)
        if n > 0:
            loss = sum(loss) / n
        else:
            loss = inputs.new_zeros(1, requires_grad=True)
        return loss


class LogitBinaryCrossEntropy(nn.Module):
    """Returns Binary Cross Entropy for logits.

    Attention:
        `Key`: logit_bce
    """

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        """Calculates and returns the binary cross entropy for logits

        Args:
            sample_list (SampleList): SampleList containing `targets` attribute.
            model_output (Dict): Model output containing `scores` attribute.

        Returns:
            torch.FloatTensor: Float value for loss.

        """
        scores = model_output['scores']
        targets = sample_list['targets']
        loss = F.binary_cross_entropy_with_logits(scores, targets, reduction='mean')
        return loss * targets.size(1)


class MSLoss(nn.Module):
    """
    A Multi-Similarity loss between embeddings of similar and dissimilar
    labels is implemented here.

    Reference:

    "Multi-similarity loss with general pair weighting for deep metric learning"

    Args:

        alpha, beta, margin: parameters used in loss function calculation
        hard_mining: if true, select only the hardest examples (defined based on margin)
        is_multilabel: True if there are more than two labels, false otherwise

    """

    def __init__(self, alpha=50, beta=2, margin=0.5, hard_mining=True, is_multilabel=False):
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.hard_mining = hard_mining
        self.margin = margin
        self.is_multilabel = is_multilabel

    def get_positive_and_negative_pairs(self, sim_vec, targets, curr_target):
        if self.is_multilabel:
            pos_pair_ = torch.masked_select(sim_vec, torch.matmul(targets, targets[0]) > 0)
        else:
            pos_pair_ = torch.masked_select(sim_vec, targets == curr_target)
        pos_pair_ = torch.masked_select(pos_pair_, pos_pair_ < 1 - 1e-05)
        pos_pair_ = torch.sort(pos_pair_)[0]
        if self.is_multilabel:
            neg_pair_ = torch.masked_select(sim_vec, torch.matmul(targets, targets[0]) < 1e-05)
        else:
            neg_pair_ = torch.masked_select(sim_vec, targets != curr_target)
        neg_pair_ = torch.sort(neg_pair_)[0]
        if len(pos_pair_) == 0 or len(neg_pair_) == 0:
            return pos_pair_, neg_pair_
        if self.hard_mining is not None:
            neg_pair = torch.masked_select(neg_pair_, neg_pair_ + 0.1 > pos_pair_[0])
            pos_pair = torch.masked_select(pos_pair_, pos_pair_ - 0.1 < neg_pair_[-1])
            neg_pair_ = neg_pair
            pos_pair_ = pos_pair
        return pos_pair_, neg_pair_

    def forward(self, sample_list, model_output):
        fusion_features = model_output['fused_embedding']
        inputs = F.normalize(fusion_features)
        targets = sample_list['targets']
        batch_size = inputs.size(0)
        sim_mat = torch.matmul(inputs, inputs.t())
        base = self.margin
        loss = []
        for i in range(batch_size):
            pos_pair_, neg_pair_ = self.get_positive_and_negative_pairs(sim_mat[i], targets, targets[i])
            if len(pos_pair_) == 0 or len(neg_pair_) == 0:
                continue
            pos_loss = calc_ms_loss(pos_pair_, base, self.beta, -1)
            neg_loss = calc_ms_loss(neg_pair_, base, self.alpha, 1)
            loss.append(pos_loss + neg_loss)
        if len(loss) == 0:
            loss = inputs.new_zeros(1, requires_grad=True)
        else:
            loss = sum(loss) / batch_size
        return loss


class HeadsDict(nn.Module):
    """
    HeadsDict class manages the construction and forward pass for
    multiple possible heads for multi-task learning.
    Construction from list or dict configs is supported,
    take a look at `build_heads_dict(head_configs, tasks, losses)`.
    """

    def __init__(self, heads: Union[nn.ModuleDict, nn.ModuleList], head_names: Union[Dict, List], losses: Dict, head_loss_names: Union[Dict, List]):
        super().__init__()
        self.heads = heads
        self.head_names = head_names
        self.losses = losses
        self.head_loss_names = head_loss_names

    def forward(self, task: Optional[str], sequence: Tensor, sample_list: Dict[str, Tensor]) ->Dict[str, Tensor]:
        """
        For a given task, compute the forward for each head
        associated with the task, compute the losses for
        each head, and sum the losses and scores
        """
        if isinstance(self.heads, nn.ModuleList):
            heads_modules_list = self.heads
            head_losses = self.head_loss_names
            head_names = self.head_names
        else:
            heads_modules_list = self.heads[task]
            head_losses = self.head_loss_names[task]
            head_names = self.head_names[task]
        outputs_list = [head(sequence, processed_sample_list=sample_list) for head in heads_modules_list]
        assert len(head_losses) == len(outputs_list)
        processed_outputs_list = [self._process_head_output(outputs, loss_name, head_name, sample_list) for outputs, loss_name, head_name in zip(outputs_list, head_losses, head_names)]

        def reduce_losses(accum_result, loss_dict):
            for loss_key, loss_val in loss_dict.items():
                if loss_key in accum_result:
                    accum_result[loss_key] += loss_val
                else:
                    accum_result[loss_key] = loss_val
        loss_result = {}
        for output in processed_outputs_list:
            reduce_losses(loss_result, output['losses'])
        results = {'losses': loss_result, 'scores': sum([output.get('scores', 0) for output in processed_outputs_list])}
        return results

    def _process_head_output(self, outputs: Union[Dict, Tensor], loss_name: str, head_name: str, sample_list: Dict[str, Tensor]) ->Dict[str, Tensor]:
        if isinstance(outputs, collections.abc.MutableMapping) and 'losses' in outputs:
            return outputs
        if isinstance(outputs, collections.abc.MutableMapping) and 'scores' in outputs:
            logits = outputs['scores']
        else:
            logits = outputs
        logits = logits.contiguous().view(-1, logits.size(-1))
        if loss_name is None:
            raise ValueError(f"Transformer head {head_name} must either                                 define a 'loss' in its config or return                                 a dict that contains key 'losses'.")
        output = self.losses[loss_name](sample_list, {'scores': logits})
        return {'losses': output, 'scores': logits}


def cost_matrix_cosine(x: Tensor, y: Tensor, eps: float=1e-05) ->Tensor:
    """Compute cosine distance across every pairs of x, y (batched)
    [B, L_x, D] [B, L_y, D] -> [B, Lx, Ly]"""
    assert x.dim() == y.dim()
    assert x.size(0) == y.size(0)
    assert x.size(2) == y.size(2)
    x_norm = F.normalize(x, p=2, dim=-1, eps=eps)
    y_norm = F.normalize(y, p=2, dim=-1, eps=eps)
    cosine_sim = x_norm.matmul(y_norm.transpose(1, 2))
    cosine_dist = 1 - cosine_sim
    return cosine_dist


@torch.no_grad()
def ipot(C: Tensor, x_len: int, x_pad: Tensor, y_len: int, y_pad: Tensor, joint_pad: Tensor, beta: float, iteration: int, k: int) ->Tensor:
    """[B, M, N], [B], [B, M], [B], [B, N], [B, M, N]"""
    b, m, n = C.size()
    sigma = torch.ones(b, m, dtype=C.dtype, device=C.device) / x_len.unsqueeze(1)
    T = torch.ones(b, n, m, dtype=C.dtype, device=C.device)
    A = torch.exp(-C.transpose(1, 2) / beta)
    sigma.masked_fill_(x_pad, 0)
    joint_pad = joint_pad.transpose(1, 2)
    T.masked_fill_(joint_pad, 0)
    A.masked_fill_(joint_pad, 0)
    x_len = x_len.unsqueeze(1).unsqueeze(2)
    y_len = y_len.unsqueeze(1).unsqueeze(2)
    x_mask = (x_pad * 10000.0).unsqueeze(1)
    y_mask = (y_pad * 10000.0).unsqueeze(1)
    for _ in range(iteration):
        Q = A * T
        sigma = sigma.view(b, m, 1)
        for _ in range(k):
            delta = 1 / (y_len * Q.matmul(sigma).view(b, 1, n) + y_mask)
            sigma = 1 / (x_len * delta.matmul(Q) + x_mask)
        T = delta.view(b, n, 1) * Q * sigma
    T.masked_fill_(joint_pad, 0)
    return T


def trace(x: Tensor) ->Tensor:
    """Compute trace of input tensor (batched)"""
    b, m, n = x.size()
    assert m == n
    mask = torch.eye(n, dtype=torch.bool, device=x.device).unsqueeze(0).expand_as(x)
    trace = x.masked_select(mask).contiguous().view(b, n).sum(dim=-1, keepdim=False)
    return trace


def optimal_transport_dist(txt_emb: Tensor, img_emb: Tensor, txt_pad: Tensor, img_pad: Tensor, beta: float=0.5, iteration: int=50, k: int=1) ->Tensor:
    """[B, M, D], [B, N, D], [B, M], [B, N]"""
    cost = cost_matrix_cosine(txt_emb, img_emb)
    joint_pad = txt_pad.unsqueeze(-1) | img_pad.unsqueeze(-2)
    cost.masked_fill_(joint_pad, 0)
    txt_len = txt_pad.size(1) - txt_pad.sum(dim=1, keepdim=False)
    img_len = img_pad.size(1) - img_pad.sum(dim=1, keepdim=False)
    T = ipot(cost.detach(), txt_len, txt_pad, img_len, img_pad, joint_pad, beta, iteration, k)
    distance = trace(cost.matmul(T.detach()))
    return distance


class WRA(nn.Module):
    """
    Word Region Alignment from UNITER.
    Optimal Transport (OT) distance between text and image
    features is used to optimize for WRA.
    OT transport plan (T) is approximated through IPOT.
    """

    def __init__(self, loss_name: str='wra_loss', ot_inputs_key: str='wra_info', wra_label_key: str='is_correct', *args, **kwargs):
        super().__init__()
        self.loss_name = loss_name
        self.ot_inputs_key = ot_inputs_key
        self.wra_label_key = wra_label_key

    def forward(self, sequence_output: Tensor, processed_sample_list: Dict[str, Dict[str, Tensor]]) ->Dict[str, Dict[str, Tensor]]:
        output_dict = {}
        assert self.ot_inputs_key in processed_sample_list and processed_sample_list[self.ot_inputs_key] is not None, f'WRA pretraining requires {self.ot_inputs_key} to be in sample ' + 'list with value not None.'
        ot_inputs = processed_sample_list[self.ot_inputs_key]
        assert ot_inputs.get('txt_pad') is not None and ot_inputs.get('img_pad') is not None, "WRA pretraining requires 'txt_pad', and 'img_pad' to be in " + f"'processed_sample_list[{self.ot_inputs_key}]' with" + ' values not None.'
        assert processed_sample_list.get(self.wra_label_key) is not None, f'WRA pretraining requires {self.wra_label_key} to be in sample ' + 'list with value not None.'
        ctx_emb = sequence_output
        tl = processed_sample_list['input_ids'].size(1)
        il = processed_sample_list['image_feat'].size(1)
        txt_emb = ctx_emb[:, :tl, :]
        img_emb = ctx_emb[:, tl:tl + il, :]
        txt_pad = ot_inputs['txt_pad'].bool()
        img_pad = ot_inputs['img_pad'].bool()
        itm_labels = processed_sample_list[self.wra_label_key]
        ot_dist = optimal_transport_dist(txt_emb.float(), img_emb.float(), txt_pad, img_pad)
        ot_pos = ot_dist.masked_select(itm_labels == 1)
        ot_neg = ot_dist.masked_select(itm_labels == 0)
        ot_loss = (ot_pos.sum() - ot_neg.sum()) / (ot_pos.size(0) + ot_neg.size(0))
        output_dict['losses'] = {}
        output_dict['losses'][self.loss_name] = ot_loss
        return output_dict


class BackboneBase(nn.Module):

    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):
        super().__init__()
        for name, parameter in backbone.named_parameters():
            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:
                parameter.requires_grad_(False)
        if return_interm_layers:
            return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}
        else:
            return_layers = {'layer4': 0}
        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)
        self.num_channels = num_channels

    def forward(self, tensor_list: NestedTensor):
        xs = self.body(tensor_list.tensors)
        out = OrderedDict()
        for name, x in xs.items():
            mask = F.interpolate(tensor_list.mask[None].float(), size=x.shape[-2:]).bool()[0]
            out[name] = NestedTensor(x, mask)
        return out


class Joiner(nn.Sequential):

    def __init__(self, backbone: nn.Module, position_embedding: nn.Module):
        super().__init__(backbone, position_embedding)

    def forward(self, tensor_list: NestedTensor):
        xs = self[0](tensor_list)
        out = []
        pos = []
        for _, x in xs.items():
            out.append(x)
            pos.append(self[1](x))
        return out, pos


def box_cxcywh_to_xyxy(x: Tensor):
    x_c, y_c, w, h = x.unbind(-1)
    b = [x_c - 0.5 * w, y_c - 0.5 * h, x_c + 0.5 * w, y_c + 0.5 * h]
    return torch.stack(b, dim=-1)


def box_iou(boxes1: Tensor, boxes2: Tensor):
    area1 = box_area(boxes1)
    area2 = box_area(boxes2)
    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
    wh = (rb - lt).clamp(min=0)
    inter = wh[:, :, 0] * wh[:, :, 1]
    union = area1[:, None] + area2 - inter
    iou = inter / union
    return iou, union


def generalized_box_iou(boxes1: Tensor, boxes2: Tensor):
    """
    Generalized IoU from https://giou.stanford.edu/
    The boxes should be in [x0, y0, x1, y1] format
    Returns a [N, M] pairwise matrix, where N = len(boxes1)
    and M = len(boxes2)
    """
    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()
    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()
    iou, union = box_iou(boxes1, boxes2)
    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])
    wh = (rb - lt).clamp(min=0)
    area = wh[:, :, 0] * wh[:, :, 1]
    return iou - (area - union) / area


class HungarianMatcher(nn.Module):
    """
    This class computes an assignment between the targets and the predictions of the
    network

    For efficiency reasons, the targets don't include the no_object. Because of this,
    in general, there are more predictions than targets. In this case, we do a 1-to-1
    matching of the best predictions, while the others are un-matched (and thus treated
    as non-objects).
    """

    def __init__(self, cost_class: float=1, cost_bbox: float=1, cost_giou: float=1, logsoftmax: bool=False):
        """Creates the matcher

        Params:
            cost_class: This is the relative weight of the classification error in the
                matching cost
            cost_bbox: This is the relative weight of the L1 error of the bounding box
                coordinates in the matching cost
            cost_giou: This is the relative weight of the giou loss of the bounding box
                in the matching cost
        """
        super().__init__()
        self.cost_class = cost_class
        self.cost_bbox = cost_bbox
        self.cost_giou = cost_giou
        self.norm = nn.LogSoftmax(-1) if logsoftmax else nn.Softmax(-1)
        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, 'all costs cant be 0'

    @torch.no_grad()
    def forward(self, outputs: Dict[str, Tensor], targets: List[Dict[str, Tensor]]):
        """Performs the matching

        Params:
            outputs: This is a dict that contains at least these entries:
                 "pred_logits": Tensor of dim [batch_size, num_queries, num_classes]
                    with the classification logits
                 "pred_boxes": Tensor of dim [batch_size, num_queries, 4] with the
                    predicted box coordinates

            targets: This is a list of targets (len(targets) = batch_size), where each
                    target is a dict containing:
                 "labels": Tensor of dim [num_target_boxes] (where num_target_boxes is
                    the number of ground-truth objects in the target) containing the
                    class labels
                 "boxes": Tensor of dim [num_target_boxes, 4] containing the target box
                    coordinates

        Returns:
            A list of size batch_size, containing tuples of (index_i, index_j) where:
                - index_i is the indices of the selected predictions (in order)
                - index_j is the indices of the corresponding selected targets (in
                  order)
            For each batch element, it holds:
                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)
        """
        bs, num_queries = outputs['pred_logits'].shape[:2]
        out_prob = self.norm(outputs['pred_logits'].flatten(0, 1))
        out_bbox = outputs['pred_boxes'].flatten(0, 1)
        tgt_ids = torch.cat([v['labels'] for v in targets])
        tgt_bbox = torch.cat([v['boxes'] for v in targets])
        cost_class = -out_prob[:, tgt_ids]
        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)
        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox).float(), box_cxcywh_to_xyxy(tgt_bbox))
        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou
        C = C.view(bs, num_queries, -1).cpu()
        sizes = [len(v['boxes']) for v in targets]
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]


def _get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])


class TransformerDecoder(nn.Module):

    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):
        super().__init__()
        self.layers = _get_clones(decoder_layer, num_layers)
        self.num_layers = num_layers
        self.norm = norm
        self.return_intermediate = return_intermediate

    def forward(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        output = tgt
        intermediate = []
        for layer in self.layers:
            output = layer(output, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, pos=pos, query_pos=query_pos)
            if self.return_intermediate:
                intermediate.append(self.norm(output))
        if self.norm is not None:
            output = self.norm(output)
            if self.return_intermediate:
                intermediate.pop()
                intermediate.append(output)
        if self.return_intermediate:
            return torch.stack(intermediate)
        return output


def _get_activation_fn(activation):
    """Return an activation function given a string"""
    if activation == 'relu':
        return F.relu
    if activation == 'gelu':
        return F.gelu
    if activation == 'glu':
        return F.glu
    raise RuntimeError(f'activation should be relu/gelu, not {activation}.')


class TransformerDecoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu', normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)
        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        q = k = self.with_pos_embed(tgt, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos), key=self.with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt

    def forward_pre(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        tgt2 = self.norm1(tgt)
        q = k = self.with_pos_embed(tgt2, query_pos)
        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)[0]
        tgt = tgt + self.dropout1(tgt2)
        tgt2 = self.norm2(tgt)
        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos), key=self.with_pos_embed(memory, pos), value=memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]
        tgt = tgt + self.dropout2(tgt2)
        tgt2 = self.norm3(tgt)
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))
        tgt = tgt + self.dropout3(tgt2)
        return tgt

    def forward(self, tgt, memory, tgt_mask: Optional[Tensor]=None, memory_mask: Optional[Tensor]=None, tgt_key_padding_mask: Optional[Tensor]=None, memory_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None, query_pos: Optional[Tensor]=None):
        if self.normalize_before:
            return self.forward_pre(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)
        return self.forward_post(tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)


class TransformerEncoderLayer(nn.Module):

    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu', normalize_before=False):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.activation = _get_activation_fn(activation)
        self.normalize_before = normalize_before

    def with_pos_embed(self, tensor, pos: Optional[Tensor]):
        return tensor if pos is None else tensor + pos

    def forward_post(self, src, src_mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        q = k = self.with_pos_embed(src, pos)
        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

    def forward_pre(self, src, src_mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        src2 = self.norm1(src)
        q = k = self.with_pos_embed(src2, pos)
        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src2 = self.norm2(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        src = src + self.dropout2(src2)
        return src

    def forward(self, src, src_mask: Optional[Tensor]=None, src_key_padding_mask: Optional[Tensor]=None, pos: Optional[Tensor]=None):
        if self.normalize_before:
            return self.forward_pre(src, src_mask, src_key_padding_mask, pos)
        return self.forward_post(src, src_mask, src_key_padding_mask, pos)


class Transformer(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.args = args
        self.d_model_enc = args.encoder_hidden_dim
        self.d_model_dec = args.decoder_hidden_dim
        self.dropout = args.dropout
        self.nhead = args.nheads
        self.dim_feedforward = args.dim_feedforward
        self.num_encoder_layers = args.enc_layers
        self.num_decoder_layers = args.dec_layers
        self.normalize_before = args.pre_norm
        self.return_intermediate_dec = True
        self.pass_pos_and_query = args.pass_pos_and_query
        self.share_decoders = args.share_decoders
        self.activation = 'relu'
        self.pass_pos_and_query = self.pass_pos_and_query
        encoder_layer = TransformerEncoderLayer(self.d_model_enc, self.nhead, self.dim_feedforward, self.dropout, self.activation, self.normalize_before)
        encoder_norm = nn.LayerNorm(self.d_model_enc) if self.normalize_before else None
        self.encoder = TransformerEncoder(encoder_layer, self.num_encoder_layers, encoder_norm)
        if self.d_model_dec != self.d_model_enc:
            self.enc2dec_proj = nn.Linear(self.d_model_enc, self.d_model_dec)
            self.pos_embed_proj = nn.Linear(self.d_model_enc, self.d_model_dec)
        else:
            self.enc2dec_proj = nn.Identity()
            self.pos_embed_proj = nn.Identity()
        if self.share_decoders:
            decoder_layer = TransformerDecoderLayer(self.d_model_dec, self.nhead, self.dim_feedforward, self.dropout, self.activation, self.normalize_before)
            decoder_norm = nn.LayerNorm(self.d_model_dec)
            self.decoder = TransformerDecoder(decoder_layer, self.num_decoder_layers, decoder_norm, return_intermediate=self.return_intermediate_dec)
        self._reset_parameters()

    def _reset_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def forward(self, *args, **kwargs):
        raise NotImplementedError()


class UniTTransformer(Transformer):

    def __init__(self, args):
        super().__init__(args=args)
        num_queries = self.args.num_queries
        self.decoders = nn.ModuleDict()
        for task in num_queries:
            task_dict = nn.ModuleDict()
            for dataset in num_queries[task]:
                if self.share_decoders:
                    task_dict[dataset] = self.decoder
                else:
                    task_dict[dataset] = self.build_decoder_layer(d_model_dec=self.d_model_dec, nhead=self.nhead, dim_feedforward=self.dim_feedforward, dropout=self.dropout, activation=self.activation, normalize_before=self.normalize_before, num_decoder_layers=self.num_decoder_layers, return_intermediate_dec=self.return_intermediate_dec)
            self.decoders[task] = task_dict
        MAX_TASK_NUM = 256
        if args.use_task_embedding_in_img_encoder:
            self.task_embeddings_enc = nn.Embedding(MAX_TASK_NUM, self.d_model_enc)
        self.mem_out_begin_idx = 1 if args.use_task_embedding_in_img_encoder else 0

    def build_decoder_layer(self, d_model_dec=512, nhead=8, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu', normalize_before=False, return_intermediate_dec=False):
        decoder_layer = TransformerDecoderLayer(d_model_dec, nhead, dim_feedforward, dropout, activation, normalize_before)
        decoder_norm = nn.LayerNorm(d_model_dec)
        return TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm, return_intermediate=return_intermediate_dec)

    def forward(self, img_src: Optional[Tensor]=None, img_mask: Optional[Tensor]=None, img_pos: Optional[Tensor]=None, text_src: Optional[Tensor]=None, text_mask: Optional[Tensor]=None, text_pos: Optional[Tensor]=None, query_embed: Optional[Tensor]=None, task_type: Optional[str]=None, dataset_name: Optional[str]=None, task_idx: Optional[int]=None):
        memories = []
        pos_embeds = []
        masks = []
        if img_src is not None:
            bs, c, h, w = img_src.shape
            img_src = img_src.flatten(2).permute(2, 0, 1)
            img_pos = img_pos.flatten(2).permute(2, 0, 1)
            img_mask = img_mask.flatten(1)
            if text_src is None:
                query_embed = query_embed.unsqueeze(1).repeat(1, bs, 1)
                if self.pass_pos_and_query:
                    tgt = torch.zeros_like(query_embed)
                else:
                    img_src, tgt, query_embed, img_pos = img_src + 0.1 * img_pos, query_embed, None, None
            img_src, img_mask, img_pos = self._prefix_task_embedding_to_encoder_inputs(img_src, img_mask, img_pos, task_idx)
            memory = self.encoder(img_src, src_key_padding_mask=img_mask, pos=img_pos)
            if self.mem_out_begin_idx != 0:
                img_src = img_src[self.mem_out_begin_idx:]
                img_pos = img_pos[self.mem_out_begin_idx:]
                img_mask = img_mask[:, self.mem_out_begin_idx:]
                memory = memory[self.mem_out_begin_idx:]
            if self.args.residual_in_encoder:
                memory = img_src + memory
            memory = self.enc2dec_proj(memory)
            img_pos = self.pos_embed_proj(img_pos)
            memories.append(memory)
            pos_embeds.append(img_pos)
            masks.append(img_mask)
        if text_src is not None:
            text_src = text_src.permute(1, 0, 2)
            memories.append(text_src)
            text_pos = text_pos.unsqueeze(1).repeat(1, text_src.size(1), 1)
            pos_embeds.append(text_pos)
            masks.append(text_mask != 1)
            query_embed = query_embed.unsqueeze(1).repeat(1, text_src.size(1), 1)
            if self.pass_pos_and_query:
                tgt = torch.zeros_like(query_embed)
            else:
                raise NotImplementedError()
        decoder = self.decoders[task_type][dataset_name]
        memories = torch.cat(memories)
        masks = torch.cat(masks, dim=-1)
        pos_embeds = torch.cat(pos_embeds)
        hs = decoder(tgt, memories, memory_key_padding_mask=masks, pos=pos_embeds, query_pos=query_embed)
        hs = hs.transpose(1, 2)
        return hs, memories.permute(1, 2, 0)

    def _prefix_task_embedding_to_encoder_inputs(self, img_src, img_mask, img_pos, task_idx):
        if not self.args.use_task_embedding_in_img_encoder:
            return img_src, img_mask, img_pos
        bs = img_src.size(1)
        task_embed = self.task_embeddings_enc.weight[task_idx]
        task_embed = task_embed.unsqueeze(0).unsqueeze(0).repeat(1, bs, 1)
        img_src = torch.cat([task_embed, img_src], dim=0)
        img_mask_pad = torch.zeros_like(img_mask[:, :1])
        img_mask = torch.cat([img_mask_pad, img_mask], dim=1)
        img_pos_pad = torch.zeros_like(img_pos[:1])
        img_pos = torch.cat([img_pos_pad, img_pos], dim=0)
        return img_src, img_mask, img_pos


def build_unit_convnet_backbone(args):
    position_embedding = PositionEmbeddingSine(args.encoder_hidden_dim // 2, normalize=True)
    train_backbone = args.lr_backbone > 0
    return_interm_layers = False
    backbone = Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)
    model = Joiner(backbone, position_embedding)
    model.num_channels = backbone.num_channels
    return model


class UniTBaseModel(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.num_queries = args.num_queries
        self.backbone = build_unit_convnet_backbone(args)
        self.transformer = UniTTransformer(args)
        encoder_hidden_dim = self.transformer.d_model_enc
        decoder_hidden_dim = self.transformer.d_model_dec
        self.query_embeds = nn.ModuleDict()
        for task_type in self.num_queries:
            task_dict = nn.ModuleDict()
            for dataset in self.num_queries[task_type]:
                task_dict[dataset] = nn.Embedding(self.num_queries[task_type][dataset], decoder_hidden_dim)
            self.query_embeds[task_type] = task_dict
        self.input_proj = nn.Conv2d(self.backbone.num_channels, encoder_hidden_dim, kernel_size=1)

    def forward(self, img_src: Tensor, text_src: Optional[Tensor]=None, text_mask: Optional[Tensor]=None, text_pos: Optional[Tensor]=None, output_hidden_states_only: bool=False, task_type: str='detection', dataset_name: str='detection_coco', task_idx: Optional[int]=None):
        img_mask = None
        img_pos = [None]
        if img_src is not None:
            if not isinstance(img_src, NestedTensor):
                img_src = NestedTensor.from_tensor_list(img_src)
            features, img_pos = self.backbone(img_src)
            img_src, img_mask = features[-1].decompose()
            img_src = self.input_proj(img_src)
        query_embed = self.query_embeds[task_type][dataset_name]
        hs, _ = self.transformer(img_src=img_src, img_mask=img_mask, img_pos=img_pos[-1], text_src=text_src, text_mask=text_mask, text_pos=text_pos, query_embed=query_embed.weight, task_type=task_type, dataset_name=dataset_name, task_idx=task_idx)
        if hs is not None:
            assert hs.size(2) == self.num_queries[task_type][dataset_name]
        return {'hidden_states': hs}


class AttributeHead(nn.Module):

    def __init__(self, object_class_num, attribute_class_num, representation_size):
        super().__init__()
        self.cls_embed = nn.Embedding(object_class_num + 1, 256)
        self.attr_linear1 = nn.Linear(representation_size + 256, 512)
        self.attr_linear2 = nn.Linear(512, attribute_class_num)
        nn.init.normal_(self.cls_embed.weight, std=0.01)
        nn.init.normal_(self.attr_linear1.weight, std=0.01)
        nn.init.normal_(self.attr_linear2.weight, std=0.01)
        nn.init.constant_(self.attr_linear1.bias, 0)
        nn.init.constant_(self.attr_linear2.bias, 0)

    def forward(self, hidden_states: Tensor, labels: Tensor):
        cls_embed_out = self.cls_embed(labels)
        concat_attr = torch.cat([hidden_states, cls_embed_out], dim=-1)
        fc_attr = self.attr_linear1(concat_attr)
        attr_score = F.relu(self.attr_linear2(fc_attr))
        return attr_score


def get_world_size():
    if is_xla():
        return xm.xrt_world_size()
    if not dist.is_available():
        return 1
    if not dist.is_nccl_available():
        return 1
    if not dist.is_initialized():
        return 1
    return dist.get_world_size()


def is_dist_initialized():
    return dist.is_available() and dist.is_initialized()


class SetCriterion(nn.Module):
    """This class computes the loss for DETR.
    The process happens in two steps:
        1) we compute hungarian assignment between ground truth boxes and the outputs
           of the model
        2) we supervise each pair of matched ground-truth / prediction (supervise class
           and box)
    """

    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses, attribute_head=None, attribute_class_num=None, max_attribute_num=None):
        """Create the criterion.
        Parameters:
            num_classes: number of object categories, omitting the special no-object
                category
            matcher: module able to compute a matching between targets and proposals
            weight_dict: dict containing as key the names of the losses and as values
                their relative weight.
            eos_coef: relative classification weight applied to the no-object category
            losses: list of all the losses to be applied. See get_loss for list of
                available losses.
        """
        super().__init__()
        self.num_classes = num_classes
        self.matcher = matcher
        self.weight_dict = weight_dict
        self.eos_coef = eos_coef
        self.losses = losses
        empty_weight = torch.ones(self.num_classes + 1)
        empty_weight[-1] = self.eos_coef
        self.register_buffer('empty_weight', empty_weight)
        self.attribute_head = attribute_head
        self.attribute_class_num = attribute_class_num
        self.max_attribute_num = max_attribute_num

    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):
        """Classification loss (NLL)
        targets dicts must contain the key "labels" containing a tensor of dim
            [nb_target_boxes]
        """
        assert 'pred_logits' in outputs
        src_logits = outputs['pred_logits']
        idx = self._get_src_permutation_idx(indices)
        target_classes_o = torch.cat([t['labels'][J] for t, (_, J) in zip(targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device)
        target_classes[idx] = target_classes_o
        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)
        losses = {'loss_ce': loss_ce}
        if self.attribute_head is not None and 'attributes' in targets[0]:
            attribute_logits = self.attribute_head(outputs['hs_for_attr'], target_classes)
            target_attributes_o = torch.cat([t['attributes'][J] for t, (_, J) in zip(targets, indices)])
            target_attributes = -torch.ones(*src_logits.shape[:2], 16, dtype=torch.int64, device=src_logits.device)
            target_attributes[idx] = target_attributes_o
            losses['loss_attr'] = self._attribute_loss(attribute_logits, target_attributes)
        return losses

    def loss_labels_balanced(self, outputs, targets, indices, num_boxes, log=True):
        """Classification loss (NLL)
        targets dicts must contain the key "labels" containing a tensor of dim
            [nb_target_boxes]
        """
        assert 'pred_logits' in outputs
        src_logits = outputs['pred_logits']
        idx = self._get_src_permutation_idx(indices)
        target_classes_o = torch.cat([t['labels'][J] for t, (_, J) in zip(targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device)
        target_classes[idx] = target_classes_o
        sl = src_logits.flatten(0, 1)
        tc = target_classes.flatten(0, 1)
        pos = tc != self.num_classes
        loss_pos = F.cross_entropy(sl[pos], tc[pos], reduction='none').sum() / num_boxes
        loss_neg = F.cross_entropy(sl[~pos], tc[~pos], reduction='none').sum() / (sl.shape[0] - num_boxes)
        loss_ce = (1 - self.eos_coef) * loss_pos + self.eos_coef * loss_neg
        losses = {'loss_ce': loss_ce}
        if self.attribute_head is not None:
            raise NotImplementedError()
        return losses

    @torch.no_grad()
    def loss_cardinality(self, outputs, targets, indices, num_boxes):
        """Compute the cardinality error, ie the absolute error in the number of
        predicted non-empty boxes
        This is not really a loss, it is intended for logging purposes only. It doesn't
        propagate gradients
        """
        pred_logits = outputs['pred_logits']
        device = pred_logits.device
        tgt_lengths = torch.as_tensor([len(v['labels']) for v in targets], device=device)
        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)
        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())
        losses = {'cardinality_error': card_err}
        return losses

    def loss_boxes(self, outputs, targets, indices, num_boxes):
        """Compute the losses related to the bounding boxes, the L1 regression loss and
             the GIoU loss
        targets dicts must contain the key "boxes" containing a tensor of dim
             [nb_target_boxes, 4]
        The target boxes are expected in format (center_x, center_y, h, w),
             normalized by the image size.
        """
        assert 'pred_boxes' in outputs
        idx = self._get_src_permutation_idx(indices)
        src_boxes = outputs['pred_boxes'][idx]
        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)
        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')
        losses = {}
        losses['loss_bbox'] = loss_bbox.sum() / num_boxes
        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(box_ops.box_cxcywh_to_xyxy(src_boxes).float(), box_ops.box_cxcywh_to_xyxy(target_boxes)))
        losses['loss_giou'] = loss_giou.sum() / num_boxes
        return losses

    def _get_src_permutation_idx(self, indices):
        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])
        src_idx = torch.cat([src for src, _ in indices])
        return batch_idx, src_idx

    def _get_tgt_permutation_idx(self, indices):
        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])
        tgt_idx = torch.cat([tgt for _, tgt in indices])
        return batch_idx, tgt_idx

    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):
        loss_map = {'labels': self.loss_labels, 'labels_balanced': self.loss_labels_balanced, 'cardinality': self.loss_cardinality, 'boxes': self.loss_boxes}
        assert loss in loss_map, f'do you really want to compute {loss} loss?'
        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)

    def forward(self, outputs, targets):
        """This performs the loss computation.
        Parameters:
             outputs: dict of tensors, see the output specification of the model for
                      the format
             targets: list of dicts, such that len(targets) == batch_size.
                      The expected keys in each dict depends on the losses applied, see
                      each loss' doc
        """
        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}
        indices = self.matcher(outputs_without_aux, targets)
        num_boxes = sum(len(t['labels']) for t in targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)
        if is_dist_initialized():
            torch.distributed.all_reduce(num_boxes)
        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()
        losses = {}
        for loss in self.losses:
            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))
        if 'aux_outputs' in outputs:
            for i, aux_outputs in enumerate(outputs['aux_outputs']):
                indices = self.matcher(aux_outputs, targets)
                for loss in self.losses:
                    kwargs = {}
                    if loss in ('labels', 'labels_balanced'):
                        kwargs = {'log': False}
                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)
                    l_dict = {(k + f'_{i}'): v for k, v in l_dict.items()}
                    losses.update(l_dict)
        return losses

    def _attribute_loss(self, attribute_logits, attributes):
        _N, _B, _C = attribute_logits.size()
        assert _C == self.attribute_class_num
        attribute_logits = attribute_logits.view(_N * _B, _C)
        assert attributes.size(0) == _N
        assert attributes.size(1) == _B
        assert attributes.size(2) == self.max_attribute_num
        attributes = attributes.view(_N * _B, self.max_attribute_num)
        n_boxes = attribute_logits.shape[0]
        attribute_logits = attribute_logits.unsqueeze(1)
        attribute_logits = attribute_logits.expand(n_boxes, 16, self.attribute_class_num).contiguous().view(-1, self.attribute_class_num)
        inv_per_box_weights = (attributes >= 0).sum(dim=1).repeat(16, 1).transpose(0, 1).flatten()
        per_box_weights = inv_per_box_weights.float().reciprocal()
        per_box_weights[per_box_weights > 1] = 0.0
        attributes = attributes.view(-1)
        attribute_loss = 0.5 * F.cross_entropy(attribute_logits, attributes, reduction='none', ignore_index=-1)
        attribute_loss = (attribute_loss * per_box_weights).view(n_boxes, -1).sum(dim=1)
        n_valid_boxes = len(attribute_loss.nonzero())
        if n_valid_boxes > 0:
            attribute_loss = (attribute_loss / n_valid_boxes).sum()
        else:
            attribute_loss = (attribute_loss * 0.0).sum()
        return attribute_loss


class UNITERImageEmbeddings(nn.Module):
    """
    Image Embeddings used by UNITER.
    Code modified from https://github.com/ChenRocks/UNITER/blob/master/model/model.py
    Performs a linear projection then normalization over image and position features.
    """

    def __init__(self, img_dim: int=2048, hidden_size: int=768, eps: float=1e-12, hidden_dropout_prob: float=0, pos_dim: int=7):
        super().__init__()
        self.img_linear = nn.Linear(img_dim, hidden_size)
        self.img_layer_norm = nn.LayerNorm(hidden_size, eps=eps)
        self.pos_layer_norm = nn.LayerNorm(hidden_size, eps=eps)
        self.pos_linear = nn.Linear(pos_dim, hidden_size)
        self.mask_embedding = nn.Embedding(2, img_dim, padding_idx=0)
        self.final_layer_norm = nn.LayerNorm(hidden_size, eps=eps)
        self.dropout = nn.Dropout(hidden_dropout_prob)

    def forward(self, img_feat: Tensor, img_pos_feat: Tensor, type_embeddings: Tensor, img_masks: Optional[Tensor]=None) ->Tensor:
        if img_masks is not None:
            self.mask_embedding.weight.data[0, :].fill_(0)
            mask = self.mask_embedding(img_masks.long())
            img_feat = img_feat + mask
        transformed_im = self.img_layer_norm(self.img_linear(img_feat))
        transformed_pos = self.pos_layer_norm(self.pos_linear(img_pos_feat))
        embeddings = transformed_im + transformed_pos + type_embeddings
        embeddings = self.final_layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


NUM_RETRIES = 6


def retry_n(n: int, fn: Callable, *args, log_tries=False, **kwargs) ->Any:
    """Retries a function n times with increasing exponentionally
    increasing sleep intervals in between. First argument is number of tries
    if n==1, means function will be called at least twice, first is try, second
    is retry. Second argument is the function itself, rest of the arguments and
    keyword arguments are passed to the function directly. Returns the output
    of the function directly. if failed after n retries, the exception will be
    raised.

    Args:
        n (int): Number of tries to be made
        fn (Callable): Function to be called
        log_tries (bool): If the function should log the try iteration. Default: False

    Returns:
        Any: Output from fn
    """
    completed = False
    count = 0
    output = None
    while not completed:
        try:
            output = fn(*args, **kwargs)
            completed = True
        except Exception:
            if count < n:
                if log_tries:
                    logger.info(f'Try {count + 1}/{n} failed for {fn.__name__}. Will retry after {2 ** count} second(s).')
                time.sleep(2 ** count)
                count += 1
            else:
                raise
    return output


def _infer_with_heads(processed_sample_list: Dict[str, Tensor], uniter_model: Any, heads: Dict[str, Any], losses: Dict[str, Any]) ->Dict[str, Tensor]:
    sequence_output = uniter_model(processed_sample_list['input_ids'], processed_sample_list['position_ids'], processed_sample_list['image_feat'], processed_sample_list['img_pos_feat'], processed_sample_list['attention_mask'], img_masks=processed_sample_list['image_mask']).final_layer
    dataset_name = processed_sample_list['dataset_name']
    task = processed_sample_list.get('task', dataset_name)
    outputs = heads[task](sequence_output, processed_sample_list=processed_sample_list)
    if isinstance(outputs, MutableMapping) and 'losses' in outputs:
        return outputs
    logits = outputs
    if isinstance(outputs, MutableMapping) and 'scores' in outputs:
        logits = outputs['scores']
    logits = logits.contiguous().view(-1, logits.size(-1))
    output = losses[dataset_name](processed_sample_list, {'scores': logits})
    return {'losses': output, 'scores': logits}


DEFAULT_PRETRAINING_HEAD_CONFIGS = {'mlm': {'type': 'mlm'}, 'itm': {'type': 'itm'}, 'mrc': {'type': 'mrc'}, 'mrfr': {'type': 'mrfr'}, 'wra': {'type': 'wra'}}


DEFAULT_PRETRAINING_TASKS = 'mlm,itm,mrc,mrfr,wra'


class BertTextPooler(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.bi_hidden_size)
        self.activation = nn.ReLU()

    def forward(self, hidden_states: Tensor) ->Tensor:
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output


class BertImagePooler(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.v_hidden_size, config.bi_hidden_size)
        self.activation = nn.ReLU()

    def forward(self, hidden_states: Tensor) ->Tensor:
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output


class BertImageFeatureEmbeddings(nn.Module):
    """Construct the embeddings from image, spatial location (omit now) and
    token_type embeddings.
    """

    def __init__(self, config):
        super().__init__()
        self.image_embeddings = nn.Linear(config.v_feature_size, config.v_hidden_size)
        self.image_location_embeddings = nn.Linear(5, config.v_hidden_size)
        self.LayerNorm = nn.LayerNorm(config.v_hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, image_feature: Tensor, image_location: Tensor) ->Tensor:
        img_embeddings = self.image_embeddings(image_feature)
        loc_embeddings = self.image_location_embeddings(image_location)
        embeddings = self.LayerNorm(img_embeddings + loc_embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings


class ViLBERTForPretraining(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.bert = ViLBERTBase.from_pretrained(self.config.bert_model_name, config=BertConfig.from_dict(OmegaConf.to_container(self.config, resolve=True)), cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)))
        self.cls = BertPreTrainingHeads(config)
        self.vocab_size = self.config.vocab_size
        self.visual_target = config.visual_target
        self.num_negative = config.num_negative
        self.loss_fct = CrossEntropyLoss(ignore_index=-1)
        if self.visual_target == 0:
            self.vis_criterion = nn.KLDivLoss(reduction='none')
        elif self.visual_target == 1:
            self.vis_criterion = nn.MSELoss(reduction='none')
        elif self.visual_target == 2:
            self.vis_criterion = CrossEntropyLoss()

    def init_weights(self):
        if self.config.random_initialize is False:
            if self.config.bert_model_name is None:
                self.bert.init_weights()
                self.cls.apply(self.bert._init_weights)
            self.tie_weights()

    def tie_weights(self):
        """Make sure we are sharing the input and output embeddings.
        Export to TorchScript can't handle parameter sharing so we are cloning
        them instead.
        """
        self._tie_or_clone_weights(self.cls.predictions.decoder, self.bert.embeddings.word_embeddings)

    def forward(self, input_ids: Tensor, image_feature: Tensor, image_location: Tensor, token_type_ids: Tensor, attention_mask: Tensor, image_attention_mask: Tensor, masked_lm_labels: Optional[Tensor]=None, image_label: Optional[Tensor]=None, image_target: Optional[Tensor]=None, output_all_attention_masks: bool=False) ->Dict[str, Tensor]:
        masked_img_loss: Optional[Tensor] = None
        sequence_output_t, sequence_output_v, pooled_output_t, pooled_output_v, attention_weights, _encoded_layers_t_output, _encoded_layers_v_output = self.bert(input_ids, image_feature, image_location, token_type_ids, attention_mask, image_attention_mask, output_all_encoded_layers=False, output_all_attention_masks=output_all_attention_masks)
        prediction_scores_t, prediction_scores_v, seq_relationship_score = self.cls(sequence_output_t, sequence_output_v, pooled_output_t, pooled_output_v)
        output = {}
        if not torch.jit.is_scripting() and output_all_attention_masks:
            output['attention_weights'] = attention_weights
        if image_label is not None and image_target is not None:
            if self.visual_target == 1:
                img_loss = self.vis_criterion(prediction_scores_v, image_target)
                masked_img_loss = torch.sum(img_loss * torch.eq(image_label, 1).unsqueeze(2).float()) / max(torch.sum(torch.eq(image_label, 1).unsqueeze(2).expand_as(img_loss)), 1)
            elif self.visual_target == 0:
                img_loss = self.vis_criterion(F.log_softmax(prediction_scores_v, dim=2), image_target)
                masked_img_loss = torch.sum(img_loss * torch.eq(image_label, 1).unsqueeze(2).float()) / max(torch.sum(torch.eq(image_label, 1)), 0)
            elif self.visual_target == 2:
                num_across_batch = int(self.num_negative * 0.7)
                num_inside_batch = int(self.num_negative * 0.3)
                batch_size, num_regions, _ = prediction_scores_v.size()
                assert batch_size != 0
                row_across_index = torch.ones(batch_size, num_regions, num_across_batch, dtype=input_ids.dtype, device=input_ids.device).random_(0, batch_size - 1)
                col_across_index = torch.ones(batch_size, num_regions, num_across_batch, dtype=input_ids.dtype, device=input_ids.device).random_(0, num_regions)
                for i in range(batch_size - 1):
                    row_across_index[i][row_across_index[i] == i] = batch_size - 1
                final_across_index = row_across_index * num_regions + col_across_index
                row_inside_index = torch.zeros(batch_size, num_regions, num_inside_batch, dtype=input_ids.dtype, device=input_ids.device)
                col_inside_index = torch.ones(batch_size, num_regions, num_inside_batch, dtype=input_ids.dtype, device=input_ids.device).random_(0, num_regions - 1)
                for i in range(batch_size):
                    row_inside_index[i] = i
                for i in range(num_regions - 1):
                    col_inside_index[:, i, :][col_inside_index[:, i, :] == i] = num_regions - 1
                final_inside_index = row_inside_index * num_regions + col_inside_index
                final_index = torch.cat((final_across_index, final_inside_index), dim=2)
                predict_v = prediction_scores_v[image_label == 1]
                neg_index_v = final_index[image_label == 1]
                flat_image_target = image_target.view(batch_size * num_regions, -1)
                negative_v = flat_image_target[neg_index_v]
                positive_v = image_target[image_label == 1]
                sample_v = torch.cat((positive_v.unsqueeze(1), negative_v), dim=1)
                score = torch.bmm(sample_v, predict_v.unsqueeze(2)).squeeze(2)
                masked_img_loss = self.vis_criterion(score, torch.zeros(score.size(0), dtype=input_ids.dtype, device=input_ids.device))
            if masked_img_loss is not None:
                output['masked_img_loss'] = masked_img_loss.unsqueeze(0)
        if masked_lm_labels is not None:
            masked_lm_loss = self.loss_fct(prediction_scores_t.view(-1, self.vocab_size), masked_lm_labels.view(-1))
            output['masked_lm_loss'] = masked_lm_loss.unsqueeze(0)
        return output


class ViLBERTForClassification(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.bert = ViLBERTBase.from_pretrained(self.config.bert_model_name, config=BertConfig.from_dict(OmegaConf.to_container(self.config, resolve=True)), cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)))
        self.training_head_type = self.config.training_head_type
        self.num_labels = self.config.num_labels
        self.fusion_method = config.fusion_method
        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)
        classifier_config = deepcopy(config)
        classifier_config.hidden_size = config.bi_hidden_size
        if self.config.training_head_type == 'nlvr2':
            classifier_config.hidden_size *= 2
        self.classifier = nn.Sequential(BertPredictionHeadTransform(classifier_config), nn.Linear(classifier_config.hidden_size, self.num_labels))
        self.init_weights()

    def init_weights(self):
        if self.config.random_initialize is False:
            if self.config.bert_model_name is None:
                self.bert.init_weights()
            self.classifier.apply(self.bert._init_weights)

    def forward(self, input_ids: Tensor, image_feature: Tensor, image_location: Tensor, token_type_ids: Optional[Tensor]=None, attention_mask: Optional[Tensor]=None, image_attention_mask: Optional[Tensor]=None, masked_lm_labels: Optional[Tensor]=None, image_label: Optional[Tensor]=None, image_target: Optional[Tensor]=None, next_sentence_label: Optional[Tensor]=None, output_all_attention_masks: bool=False) ->Dict[str, Tensor]:
        sequence_output_t, sequence_output_v, pooled_output_t, pooled_output_v, attention_weights, _encoded_layers_t_output, _encoded_layers_v_output = self.bert(input_ids, image_feature, image_location, token_type_ids, attention_mask, image_attention_mask, output_all_encoded_layers=False, output_all_attention_masks=output_all_attention_masks)
        output = {}
        if not torch.jit.is_scripting() and output_all_attention_masks:
            output['attention_weights'] = attention_weights
        if self.fusion_method == 'sum':
            pooled_output = self.dropout(pooled_output_t + pooled_output_v)
        elif self.fusion_method == 'mul':
            pooled_output = self.dropout(pooled_output_t * pooled_output_v)
        else:
            raise AssertionError
        if self.training_head_type == 'nlvr2':
            pooled_output = pooled_output.view(-1, pooled_output.size(1) * 2)
        logits = self.classifier(pooled_output)
        reshaped_logits = logits.contiguous().view(-1, self.num_labels)
        output['scores'] = reshaped_logits
        return output


def check_vit_in_transformers():
    if not has_VIT:
        raise ImportError('transformers version >= 4.5.0 required for using modeling_vit')


class ViTLayer(nn.Module):
    """This corresponds to the Block class in the timm implementation."""

    def __init__(self, config):
        super().__init__()
        self.chunk_size_feed_forward = config.chunk_size_feed_forward
        self.seq_len_dim = 1
        self.attention = ViTAttention(config)
        self.intermediate = vit.ViTIntermediate(config)
        self.output = vit.ViTOutput(config)
        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):
        self_attention_outputs = self.attention(self.layernorm_before(hidden_states), attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions)
        attention_output = self_attention_outputs[0]
        outputs = self_attention_outputs[1:]
        hidden_states = attention_output + hidden_states
        layer_output = self.layernorm_after(hidden_states)
        layer_output = self.intermediate(layer_output)
        layer_output = self.output(layer_output, hidden_states)
        outputs = (layer_output,) + outputs
        return outputs


class ViTEncoder(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])

    def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False, output_hidden_states=False, return_dict=True):
        all_hidden_states = () if output_hidden_states else None
        all_self_attentions = () if output_attentions else None
        for i, layer_module in enumerate(self.layer):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)
            layer_head_mask = head_mask[i] if head_mask is not None else None
            if getattr(self.config, 'gradient_checkpointing', False) and self.training:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs, output_attentions)
                    return custom_forward
                layer_outputs = torch.utils.checkpoint.checkpoint(create_custom_forward(layer_module), hidden_states, attention_mask, layer_head_mask)
            else:
                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)
            hidden_states = layer_outputs[0]
            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)
        return vit.BaseModelOutput(last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_self_attentions)


class ViLTImageEmbedding(nn.Module):
    """
    Patch embedding used for ViLT.
    https://arxiv.org/pdf/2102.03334.pdf
    Implementation based off
    https://github.com/dandelin/ViLT/blob/master/vilt/modules/vilt_module.py
    Using huggingface ViT modules.
    Can be built with random init or the embeddings weights from an exisiting
    ViT model from huggingface. Model list: availible at
    https://huggingface.co/models?other=vit&sort=downloads
    """

    def __init__(self, random_init: bool=True, pretrained_model_name: str='google/vit-base-patch16-224', image_size: Optional[List]=None, hidden_dropout_prob: Optional[float]=None, hidden_size: Optional[int]=None, patch_size: Optional[int]=None, num_channels: Optional[int]=None, *args, **kwargs):
        super().__init__()
        config = OmegaConf.create({'random_init': random_init, 'pretrained_model_name': pretrained_model_name})
        if image_size is not None:
            config.image_size = image_size
        if hidden_dropout_prob is not None:
            config.hidden_dropout_prob = hidden_dropout_prob
        if hidden_size is not None:
            config.hidden_size = hidden_size
        if patch_size is not None:
            config.patch_size = patch_size
        if num_channels is not None:
            config.num_channels = num_channels
        encoder = ViTEncoder(config)
        self.embedding = encoder.embeddings
        hidden_size = encoder.hf_config.hidden_size
        self.token_type_embeddings = nn.Embedding(2, hidden_size)

    def forward(self, image: Tensor) ->Tensor:
        if image.dim() == 5:
            image = image.permute(1, 0, 2, 3, 4).flatten(start_dim=0, end_dim=1)
        img_embeddings = self.embedding(image)
        img_segment_ids = torch.ones(img_embeddings.size()[:-1], dtype=img_embeddings.dtype, device=img_embeddings.device).long()
        img_type_embed = self.token_type_embeddings(img_segment_ids)
        img_embeddings = img_embeddings + img_type_embed
        return img_embeddings


class ViLTTextEmbedding(nn.Module):

    def __init__(self, random_init: bool=True, bert_model_name: str='bert-base-uncased', hidden_size: Optional[int]=None, max_position_embeddings: Optional[int]=None, *args, **kwargs):
        super().__init__()
        config = OmegaConf.create({'bert_model_name': bert_model_name, 'random_init': random_init})
        if hidden_size is not None:
            config.hidden_size = hidden_size
        if max_position_embeddings is not None:
            config.max_position_embeddings = max_position_embeddings
        text_encoder = TransformerEncoder(config)
        self.text_embeddings = text_encoder.embeddings
        hidden_size = text_encoder.config.hidden_size
        self.token_type_embeddings = nn.Embedding(2, hidden_size)

    def forward(self, input_ids: Tensor, segment_ids: Tensor) ->Tensor:
        text_embedding = self.text_embeddings(input_ids, token_type_ids=segment_ids)
        text_type_embed = self.token_type_embeddings(segment_ids)
        return text_embedding + text_type_embed


class VinVLForClassification(nn.Module):
    """VINVL wrapper for classification"""

    def __init__(self, mlp_config: Optional[Dict]=None, loss_config: Optional[Dict]=None, random_init: bool=False, bert_model_name: str='bert-base-uncased', img_feature_dim: int=2054, use_img_layernorm: bool=True, img_layer_norm_eps: float=1e-12, *args, **kwargs):
        """VinVL model constructor for classification.
        MLP head is configurable through Dict type.
        Consult the MLP head class for the config options.

        Args:
            mlp_config (Optional[Dict], optional):
                Classifier MLP head config.
                Defaults to {"num_layers": 0}.
            loss_config (Optional[Dict], optional):
                nn.CrossEntropyLoss params dict.
                Defaults to {}.
            random_init (bool, optional):
                Flag to load VinVL bert weights from random_init.
                Defaults to False.
            bert_model_name (str, optional):
                Name for base bert model.
                Used for VinVL base configs and weights.
                Defaults to "bert-base-uncased".
            img_feature_dim (int, optional):
                The size of the VinVL image feature inputs.
                Defaults to 2054.
            use_img_layernorm (bool, optional):
                Flag to use layernorm on image encoding.
                Defaults to True.
            img_layer_norm_eps (float, optional):
                Image layernorm epsilon. Defaults to 1e-12.
        """
        super().__init__()
        if mlp_config is None:
            mlp_config = {'num_layers': 0}
        if loss_config is None:
            loss_config = {}
        self.bert = build_vinvl_base(bert_model_name=bert_model_name, img_feature_dim=img_feature_dim, use_img_layernorm=use_img_layernorm, img_layer_norm_eps=img_layer_norm_eps, random_init=random_init)
        self.classifier = MLP(config=mlp_config)
        self.ce_loss = nn.CrossEntropyLoss(**loss_config)

    def forward(self, input_ids: Tensor, token_type_ids: Tensor, attention_mask: Tensor, img_feats: Tensor, position_ids: Optional[Tensor]=None, labels: Optional[Tensor]=None) ->Dict[str, Tensor]:
        sequence_output = self.bert(input_ids, img_feats=img_feats, position_ids=position_ids, token_type_ids=token_type_ids, attention_mask=attention_mask).last_hidden_state
        logits = self.classifier(sequence_output)['scores']
        result = {'scores': logits}
        if labels is not None:
            ce_loss = self.ce_loss(logits.view(-1, logits.size(1)), labels.view(-1))
            result['losses'] = {'ce': ce_loss}
        return result


class VisualBERTForPretraining(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.output_attentions = self.config.output_attentions
        self.output_hidden_states = self.config.output_hidden_states
        self.bert_model_name = self.config.get('bert_model_name', None)
        self.bert_config = BertConfig.from_dict(OmegaConf.to_container(self.config, resolve=True))
        if self.bert_model_name is None:
            self.bert = VisualBERTBase(self.bert_config, visual_embedding_dim=self.config.visual_embedding_dim, embedding_strategy=self.config.embedding_strategy, bypass_transformer=self.config.bypass_transformer, output_attentions=self.config.output_attentions, output_hidden_states=self.config.output_hidden_states)
        else:
            self.bert = VisualBERTBase.from_pretrained(self.config.bert_model_name, config=self.bert_config, cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)), visual_embedding_dim=self.config.visual_embedding_dim, embedding_strategy=self.config.embedding_strategy, bypass_transformer=self.config.bypass_transformer, output_attentions=self.config.output_attentions, output_hidden_states=self.config.output_hidden_states)
        self.vocab_size = self.bert.config.vocab_size
        if self.bert_model_name is None:
            bert_masked_lm = BertForPreTraining(self.bert.config)
        else:
            bert_masked_lm = BertForPreTraining.from_pretrained(self.config.bert_model_name, config=self.bert.config, cache_dir=os.path.join(get_mmf_cache_dir(), 'distributed_{}'.format(-1)))
        self.cls = deepcopy(bert_masked_lm.cls)
        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-1)
        self.init_weights()

    def init_weights(self):
        if self.config.random_initialize is False:
            if self.bert_model_name is None:
                self.bert.init_weights()
                self.cls.apply(self.bert._init_weights)
            self.tie_weights()

    def tie_weights(self):
        """Make sure we are sharing the input and output embeddings.
        Export to TorchScript can't handle parameter sharing so we are cloning them
        instead.
        """
        self.bert._tie_or_clone_weights(self.cls.predictions.decoder, self.bert.embeddings.word_embeddings)

    def forward(self, input_ids: Tensor, input_mask: Tensor, attention_mask: Optional[Tensor]=None, token_type_ids: Optional[Tensor]=None, visual_embeddings: Optional[Tensor]=None, visual_embeddings_type: Optional[Tensor]=None, image_text_alignment: Optional[Tensor]=None, masked_lm_labels: Optional[Tensor]=None) ->Dict[str, Tensor]:
        sequence_output, pooled_output, attention_weights = self.bert(input_ids, attention_mask, token_type_ids, visual_embeddings, visual_embeddings_type, image_text_alignment)
        output_dict: Dict[str, Tensor] = {}
        if not torch.jit.is_scripting():
            if self.output_attentions:
                output_dict['attention_weights'] = attention_weights
            if self.output_hidden_states:
                output_dict['sequence_output'] = sequence_output
                output_dict['pooled_output'] = pooled_output
        else:
            assert not (self.output_attentions or self.output_hidden_states), 'output_attentions or output_hidden_states not supported in script mode'
        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)
        if masked_lm_labels is not None:
            output_dict['logits'] = prediction_scores
            masked_lm_loss = self.loss_fct(prediction_scores.contiguous().view(-1, self.vocab_size), masked_lm_labels.contiguous().view(-1))
            output_dict['masked_lm_loss'] = masked_lm_loss
            output_dict['loss'] = masked_lm_loss
        return output_dict


class MfbExpand(nn.Module):

    def __init__(self, img_feat_dim, txt_emb_dim, hidden_dim, dropout):
        super().__init__()
        self.lc_image = nn.Linear(in_features=img_feat_dim, out_features=hidden_dim)
        self.lc_ques = nn.Linear(in_features=txt_emb_dim, out_features=hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, image_feat, question_embed):
        image1 = self.lc_image(image_feat)
        ques1 = self.lc_ques(question_embed)
        if len(image_feat.data.shape) == 3:
            num_location = image_feat.data.size(1)
            ques1_expand = torch.unsqueeze(ques1, 1).expand(-1, num_location, -1)
        else:
            ques1_expand = ques1
        joint_feature = image1 * ques1_expand
        joint_feature = self.dropout(joint_feature)
        return joint_feature


class MFH(nn.Module):

    def __init__(self, image_feat_dim, ques_emb_dim, **kwargs):
        super().__init__()
        self.mfb_expand_list = nn.ModuleList()
        self.mfb_sqz_list = nn.ModuleList()
        self.relu = nn.ReLU()
        hidden_sizes = kwargs['hidden_sizes']
        self.out_dim = int(sum(hidden_sizes) / kwargs['pool_size'])
        self.order = kwargs['order']
        self.pool_size = kwargs['pool_size']
        for i in range(self.order):
            mfb_exp_i = MfbExpand(img_feat_dim=image_feat_dim, txt_emb_dim=ques_emb_dim, hidden_dim=hidden_sizes[i], dropout=kwargs['dropout'])
            self.mfb_expand_list.append(mfb_exp_i)
            self.mfb_sqz_list.append(self.mfb_squeeze)

    def forward(self, image_feat, question_embedding):
        feature_list = []
        prev_mfb_exp = 1
        for i in range(self.order):
            mfb_exp = self.mfb_expand_list[i]
            mfb_sqz = self.mfb_sqz_list[i]
            z_exp_i = mfb_exp(image_feat, question_embedding)
            if i > 0:
                z_exp_i = prev_mfb_exp * z_exp_i
            prev_mfb_exp = z_exp_i
            z = mfb_sqz(z_exp_i)
            feature_list.append(z)
        cat_dim = len(feature_list[0].size()) - 1
        feature = torch.cat(feature_list, dim=cat_dim)
        return feature

    def mfb_squeeze(self, joint_feature):
        orig_feature_size = len(joint_feature.size())
        if orig_feature_size == 2:
            joint_feature = torch.unsqueeze(joint_feature, dim=1)
        batch_size, num_loc, dim = joint_feature.size()
        if dim % self.pool_size != 0:
            exit('the dim %d is not multiply of              pool_size %d' % (dim, self.pool_size))
        joint_feature_reshape = joint_feature.view(batch_size, num_loc, int(dim / self.pool_size), self.pool_size)
        iatt_iq_sumpool = torch.sum(joint_feature_reshape, 3)
        iatt_iq_sqrt = torch.sqrt(self.relu(iatt_iq_sumpool)) - torch.sqrt(self.relu(-iatt_iq_sumpool))
        iatt_iq_sqrt = iatt_iq_sqrt.view(batch_size, -1)
        iatt_iq_l2 = nn.functional.normalize(iatt_iq_sqrt)
        iatt_iq_l2 = iatt_iq_l2.view(batch_size, num_loc, int(dim / self.pool_size))
        if orig_feature_size == 2:
            iatt_iq_l2 = torch.squeeze(iatt_iq_l2, dim=1)
        return iatt_iq_l2


class ReLUWithWeightNormFC(nn.Module):

    def __init__(self, in_dim, out_dim):
        super().__init__()
        layers = []
        layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))
        layers.append(nn.ReLU())
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)


class NonLinearElementMultiply(nn.Module):

    def __init__(self, image_feat_dim, ques_emb_dim, **kwargs):
        super().__init__()
        self.fa_image = ReLUWithWeightNormFC(image_feat_dim, kwargs['hidden_dim'])
        self.fa_txt = ReLUWithWeightNormFC(ques_emb_dim, kwargs['hidden_dim'])
        context_dim = kwargs.get('context_dim', None)
        if context_dim is not None:
            self.fa_context = ReLUWithWeightNormFC(context_dim, kwargs['hidden_dim'])
        self.dropout = nn.Dropout(kwargs['dropout'])
        self.out_dim = kwargs['hidden_dim']

    def forward(self, image_feat, question_embedding, context_embedding=None):
        image_fa = self.fa_image(image_feat)
        question_fa = self.fa_txt(question_embedding)
        if len(image_feat.size()) == 3 and len(question_fa.size()) != 3:
            question_fa_expand = question_fa.unsqueeze(1)
        else:
            question_fa_expand = question_fa
        joint_feature = image_fa * question_fa_expand
        if context_embedding is not None:
            context_fa = self.fa_context(context_embedding)
            context_text_joint_feaure = context_fa * question_fa_expand
            joint_feature = torch.cat([joint_feature, context_text_joint_feaure], dim=1)
        joint_feature = self.dropout(joint_feature)
        return joint_feature


class TopDownAttentionLSTM(nn.Module):

    def __init__(self, image_feat_dim, embed_dim, **kwargs):
        super().__init__()
        self.fa_image = weight_norm(nn.Linear(image_feat_dim, kwargs['attention_dim']))
        self.fa_hidden = weight_norm(nn.Linear(kwargs['hidden_dim'], kwargs['attention_dim']))
        self.top_down_lstm = nn.LSTMCell(embed_dim + image_feat_dim + kwargs['hidden_dim'], kwargs['hidden_dim'], bias=True)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(kwargs['dropout'])
        self.out_dim = kwargs['attention_dim']

    def forward(self, image_feat, embedding):
        image_feat_mean = image_feat.mean(1)
        state = registry.get(f'{image_feat.device}_lstm_state')
        h1, c1 = state['td_hidden']
        h2, c2 = state['lm_hidden']
        h1, c1 = self.top_down_lstm(torch.cat([h2, image_feat_mean, embedding], dim=1), (h1, c1))
        state['td_hidden'] = h1, c1
        image_fa = self.fa_image(image_feat)
        hidden_fa = self.fa_hidden(h1)
        joint_feature = self.relu(image_fa + hidden_fa.unsqueeze(1))
        joint_feature = self.dropout(joint_feature)
        return joint_feature


class TwoLayerElementMultiply(nn.Module):

    def __init__(self, image_feat_dim, ques_emb_dim, **kwargs):
        super().__init__()
        self.fa_image1 = ReLUWithWeightNormFC(image_feat_dim, kwargs['hidden_dim'])
        self.fa_image2 = ReLUWithWeightNormFC(kwargs['hidden_dim'], kwargs['hidden_dim'])
        self.fa_txt1 = ReLUWithWeightNormFC(ques_emb_dim, kwargs['hidden_dim'])
        self.fa_txt2 = ReLUWithWeightNormFC(kwargs['hidden_dim'], kwargs['hidden_dim'])
        self.dropout = nn.Dropout(kwargs['dropout'])
        self.out_dim = kwargs['hidden_dim']

    def forward(self, image_feat, question_embedding):
        image_fa = self.fa_image2(self.fa_image1(image_feat))
        question_fa = self.fa_txt2(self.fa_txt1(question_embedding))
        if len(image_feat.size()) == 3:
            num_location = image_feat.size(1)
            question_fa_expand = torch.unsqueeze(question_fa, 1).expand(-1, num_location, -1)
        else:
            question_fa_expand = question_fa
        joint_feature = image_fa * question_fa_expand
        joint_feature = self.dropout(joint_feature)
        return joint_feature


class ModalCombineLayer(nn.Module):

    def __init__(self, combine_type, img_feat_dim, txt_emb_dim, **kwargs):
        super().__init__()
        if combine_type == 'MFH':
            self.module = MFH(img_feat_dim, txt_emb_dim, **kwargs)
        elif combine_type == 'non_linear_element_multiply':
            self.module = NonLinearElementMultiply(img_feat_dim, txt_emb_dim, **kwargs)
        elif combine_type == 'two_layer_element_multiply':
            self.module = TwoLayerElementMultiply(img_feat_dim, txt_emb_dim, **kwargs)
        elif combine_type == 'top_down_attention_lstm':
            self.module = TopDownAttentionLSTM(img_feat_dim, txt_emb_dim, **kwargs)
        else:
            raise NotImplementedError('Not implemented combine type: %s' % combine_type)
        self.out_dim = self.module.out_dim

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class TopDownAttention(nn.Module):
    EPS = 1e-08

    def __init__(self, combination_layer, transform_module, normalization):
        super().__init__()
        self.combination_layer = combination_layer
        self.normalization = normalization
        self.transform = transform_module
        self.out_dim = self.transform.out_dim

    @staticmethod
    def _mask_attentions(attention, image_locs):
        batch_size, num_loc, n_att = attention.size()
        tmp1 = attention.new_zeros(num_loc)
        tmp1[:num_loc] = torch.arange(0, num_loc, dtype=attention.dtype).unsqueeze(dim=0)
        tmp1 = tmp1.expand(batch_size, num_loc)
        tmp2 = image_locs.type(tmp1.type())
        tmp2 = tmp2.unsqueeze(dim=1).expand(batch_size, num_loc)
        mask = torch.ge(tmp1, tmp2)
        mask = mask.unsqueeze(dim=2).expand_as(attention)
        attention = attention.masked_fill(mask, 0)
        return attention

    def forward(self, image_feat, question_embedding, image_locs=None):
        joint_feature = self.combination_layer(image_feat, question_embedding)
        raw_attn = self.transform(joint_feature)
        if self.normalization.lower() == 'softmax':
            attention = nn.functional.softmax(raw_attn, dim=1)
            if image_locs is not None:
                masked_attention = self._mask_attentions(attention, image_locs)
                masked_attention_sum = torch.sum(masked_attention, dim=1, keepdim=True)
                masked_attention_sum += masked_attention_sum.eq(0).float() + self.EPS
                masked_attention = masked_attention / masked_attention_sum
            else:
                masked_attention = attention
        elif self.normalization.lower() == 'sigmoid':
            attention = torch.sigmoid(raw_attn)
            masked_attention = attention
            if image_locs is not None:
                masked_attention = self._mask_attentions(attention, image_locs)
        return masked_attention


class ConvTransform(nn.Module):

    def __init__(self, in_dim, out_dim, hidden_dim):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=in_dim, out_channels=hidden_dim, kernel_size=1)
        self.conv2 = nn.Conv2d(in_channels=hidden_dim, out_channels=out_dim, kernel_size=1)
        self.out_dim = out_dim

    def forward(self, x):
        if len(x.size()) == 3:
            x_reshape = torch.unsqueeze(x.permute(0, 2, 1), 3)
        elif len(x.size()) == 2:
            x_reshape = torch.unsqueeze(torch.unsqueeze(x, 2), 3)
        iatt_conv1 = self.conv1(x_reshape)
        iatt_relu = nn.functional.relu(iatt_conv1)
        iatt_conv2 = self.conv2(iatt_relu)
        if len(x.size()) == 3:
            iatt_conv3 = torch.squeeze(iatt_conv2, 3).permute(0, 2, 1)
        elif len(x.size()) == 2:
            iatt_conv3 = torch.squeeze(torch.squeeze(iatt_conv2, 3), 2)
        return iatt_conv3


class LinearTransform(nn.Module):

    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.lc = weight_norm(nn.Linear(in_features=in_dim, out_features=out_dim), dim=None)
        self.out_dim = out_dim

    def forward(self, x):
        return self.lc(x)


class TransformLayer(nn.Module):

    def __init__(self, transform_type, in_dim, out_dim, hidden_dim=None):
        super().__init__()
        if transform_type == 'linear':
            self.module = LinearTransform(in_dim, out_dim)
        elif transform_type == 'conv':
            self.module = ConvTransform(in_dim, out_dim, hidden_dim)
        else:
            raise NotImplementedError('Unknown post combine transform type: %s' % transform_type)
        self.out_dim = self.module.out_dim

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class AttentionLayer(nn.Module):

    def __init__(self, image_dim, question_dim, **kwargs):
        super().__init__()
        combine_type = kwargs['modal_combine']['type']
        combine_params = kwargs['modal_combine']['params']
        modal_combine_layer = ModalCombineLayer(combine_type, image_dim, question_dim, **combine_params)
        transform_type = kwargs['transform']['type']
        transform_params = kwargs['transform']['params']
        transform_layer = TransformLayer(transform_type, modal_combine_layer.out_dim, **transform_params)
        normalization = kwargs['normalization']
        self.module = TopDownAttention(modal_combine_layer, transform_layer, normalization)
        if hasattr(self.module, 'out_dim'):
            self.out_dim = self.module.out_dim

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class GatedTanh(nn.Module):
    """
    From: https://arxiv.org/pdf/1707.07998.pdf
    nonlinear_layer (f_a) : x\\in R^m => y \\in R^n
    	ilda{y} = tanh(Wx + b)
    g = sigmoid(W'x + b')
    y = 	ilda(y) \\circ g
    input: (N, *, in_dim)
    output: (N, *, out_dim)
    """

    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.fc = nn.Linear(in_dim, out_dim)
        self.gate_fc = nn.Linear(in_dim, out_dim)

    def forward(self, x):
        y_tilda = torch.tanh(self.fc(x))
        gated = torch.sigmoid(self.gate_fc(x))
        y = y_tilda * gated
        return y


class ConcatenationAttention(nn.Module):

    def __init__(self, image_feat_dim, txt_rnn_embeding_dim, hidden_size):
        super().__init__()
        self.image_feat_dim = image_feat_dim
        self.txt_embeding_dim = txt_rnn_embeding_dim
        self.fa = GatedTanh(image_feat_dim + txt_rnn_embeding_dim, hidden_size)
        self.lc = nn.Linear(hidden_size, 1)

    def forward(self, image_feat, question_embedding):
        _, num_location, _ = image_feat.shape
        question_embedding_expand = torch.unsqueeze(question_embedding, 1).expand(-1, num_location, -1)
        concat_feature = torch.cat((image_feat, question_embedding_expand), dim=2)
        raw_attention = self.lc(self.fa(concat_feature))
        attention_weights = nn.functional.softmax(raw_attention, dim=1)
        attention_weights = attention_weights.expand_as(image_feat)
        return attention_weights


class ProjectAttention(nn.Module):

    def __init__(self, image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout=0.2):
        super().__init__()
        self.image_feat_dim = image_feat_dim
        self.txt_embeding_dim = txt_rnn_embeding_dim
        self.fa_image = GatedTanh(image_feat_dim, hidden_size)
        self.fa_txt = GatedTanh(txt_rnn_embeding_dim, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.lc = nn.Linear(hidden_size, 1)

    def compute_raw_att(self, image_feat, question_embedding):
        num_location = image_feat.shape[1]
        image_fa = self.fa_image(image_feat)
        question_fa = self.fa_txt(question_embedding)
        question_fa_expand = torch.unsqueeze(question_fa, 1).expand(-1, num_location, -1)
        joint_feature = image_fa * question_fa_expand
        joint_feature = self.dropout(joint_feature)
        raw_attention = self.lc(joint_feature)
        return raw_attention

    def forward(self, image_feat, question_embedding):
        raw_attention = self.compute_raw_att(image_feat, question_embedding)
        attention_weights = nn.functional.softmax(raw_attention, dim=1)
        attention_weights = attention_weights.expand_as(image_feat)
        return attention_weights


class DoubleProjectAttention(nn.Module):

    def __init__(self, image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout=0.2):
        super().__init__()
        self.att1 = ProjectAttention(image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout)
        self.att2 = ProjectAttention(image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout)
        self.image_feat_dim = image_feat_dim
        self.txt_embeding_dim = txt_rnn_embeding_dim

    def forward(self, image_feat, question_embedding):
        att1 = self.att1.compute_raw_att(image_feat, question_embedding)
        att2 = self.att2.compute_raw_att(image_feat, question_embedding)
        raw_attn_weights = att1 + att2
        attention_weights = nn.functional.softmax(raw_attn_weights, dim=1)
        attention_weights = attention_weights.expand_as(image_feat)
        return attention_weights


class SelfGuidedAttention(nn.Module):

    def __init__(self, dim: int, num_attn: int, dropout: float):
        super().__init__()
        self.multi_head_attn = nn.ModuleList([MovieMcanMultiHeadAttention(dim, num_attn, dropout=0.1) for _ in range(2)])
        self.fcn = nn.Sequential(nn.Linear(dim, 4 * dim), nn.ReLU(inplace=True), nn.Dropout(p=dropout), nn.Linear(4 * dim, dim))
        self.drop_mha = nn.ModuleList([nn.Dropout(p=dropout) for _ in range(2)])
        self.ln_mha = nn.ModuleList([nn.LayerNorm(dim) for _ in range(3)])
        self.drop_fcn = nn.Dropout(p=dropout)
        self.ln_fcn = nn.LayerNorm(dim)

    def forward(self, x: torch.Tensor, y: torch.Tensor, x_mask: torch.Tensor, y_mask: torch.Tensor) ->torch.Tensor:
        x = self.ln_mha[0](x + self.drop_mha[0](self.multi_head_attn[0](x, x, x, x_mask)))
        x = self.ln_mha[1](x + self.drop_mha[1](self.multi_head_attn[1](x, y, y, y_mask)))
        x = self.ln_fcn(x + self.drop_fcn(self.fcn(x)))
        return x


class ChannelPool(nn.Module):
    """Average pooling in the channel dimension"""

    def __init__(self):
        super().__init__()

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        return x.mean(dim=1, keepdim=True)


class SEModule(nn.Module):
    """Squeeze-and-Excitation module from https://arxiv.org/pdf/1709.01507.pdf

    Args:
        dim: the original hidden dim.
        sqrate: the squeeze rate in hidden dim.
    Returns:
        New features map that channels are gated
        by sigmoid weights from SE module.
    """

    def __init__(self, dim: int, sqrate: float):
        super().__init__()
        self.se = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(dim, dim // sqrate, kernel_size=1, bias=False), nn.ReLU(inplace=True), nn.Conv2d(dim // sqrate, dim, kernel_size=1, bias=False), nn.Sigmoid())
        self.attn = nn.Sequential(ChannelPool(), nn.Conv2d(1, 1, kernel_size=7, padding=3, bias=False), nn.Sigmoid())

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x = x * self.se(x)
        return x * self.attn(x)


class Modulation(nn.Module):

    def __init__(self, num_features: int, num_cond_features: int, compressed: bool=True):
        super().__init__()
        self.linear = nn.Linear(num_cond_features, num_features)
        self.conv = nn.Conv2d(num_features, 256, kernel_size=1) if compressed else nn.Conv2d(num_features, num_features, kernel_size=1)

    def forward(self, x: torch.Tensor, cond: torch.Tensor) ->torch.Tensor:
        cond = self.linear(cond).unsqueeze(2).unsqueeze(3)
        return self.conv(x * cond)


class MovieBottleneck(nn.Module):
    """
    Standard ResNet bottleneck with MoVie modulation in
    https://arxiv.org/abs/2004.11883
    The code is inspired from
    https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html
    """
    expansion = 4

    def __init__(self, inplanes: int, planes: int, cond_planes: int=None, stride: int=1, downsample: Optional[Type[nn.Module]]=None, groups: int=1, base_width: int=64, dilation: int=1, norm_layer: Optional[Type[nn.Module]]=None, stride_in_1x1: bool=False, compressed: bool=True, use_se: bool=True):
        super().__init__()
        if norm_layer is None:
            self.norm_layer = FrozenBatchNorm2d
        else:
            self.norm_layer = norm_layer
        self.cond_planes = cond_planes
        self.planes = planes
        self.inplanes = inplanes
        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)
        self.width = int(planes * (base_width / 64.0)) * groups
        self.conv1 = conv1x1(inplanes, self.width, stride_1x1)
        self.bn1 = self.norm_layer(self.width)
        self.conv2 = conv3x3(self.width, self.width, stride_3x3, groups, dilation)
        self.bn2 = self.norm_layer(self.width)
        self.conv3 = conv1x1(self.width, planes * self.expansion)
        self.bn3 = self.norm_layer(self.planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.se = None
        self.compressed = compressed
        self.use_se = use_se

    def init_layers(self):
        if self.cond_planes:
            self.cond = Modulation(self.inplanes, self.cond_planes, compressed=self.compressed)
            self.se = SEModule(self.planes * self.expansion, 4) if self.use_se else None

    def forward(self, x: torch.Tensor, cond: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, Optional[torch.Tensor]]:
        identity = x
        if self.cond_planes and self.compressed:
            x = self.conv1(x) + self.cond(x, cond)
        elif self.cond_planes and not self.compressed:
            x += self.cond(x, cond)
            x = self.conv1(x)
        else:
            x = self.conv1(x)
        out = self.bn1(x)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample:
            shortcut = self.downsample(identity)
        else:
            shortcut = identity
        if self.se:
            out = self.se(out)
        out += shortcut
        out = self.relu(out)
        return out, cond


class VisDialDiscriminator(nn.Module):

    def __init__(self, config, embedding):
        super().__init__()
        self.config = config
        self.embedding = embedding
        self.emb_out_dim = embedding.text_out_dim
        self.hidden_dim = self.config.hidden_dim
        self.projection_layer = nn.Linear(self.emb_out_dim, self.hidden_dim)

    def forward(self, encoder_output, batch):
        answer_options_len = batch['answer_options_len']
        answer_options = batch['answer_options']
        max_seq_len = answer_options.size(-1)
        batch_size, ndialogues, noptions, seq_len = answer_options.size()
        answer_options = answer_options.view(-1, max_seq_len)
        answer_options_len = answer_options_len.view(-1)
        answer_options = self.embedding(answer_options)
        answer_options = self.projection_layer(answer_options)
        answer_options = answer_options.view(batch_size * ndialogues, noptions, self.hidden_dim)
        encoder_output = encoder_output.unsqueeze(1).expand(-1, noptions, -1)
        scores = torch.sum(answer_options * encoder_output, dim=2)
        return scores


class LanguageDecoder(nn.Module):

    def __init__(self, in_dim, out_dim, **kwargs):
        super().__init__()
        self.language_lstm = nn.LSTMCell(in_dim + kwargs['hidden_dim'], kwargs['hidden_dim'], bias=True)
        self.fc = weight_norm(nn.Linear(kwargs['hidden_dim'], out_dim))
        self.dropout = nn.Dropout(p=kwargs['dropout'])
        self.init_weights(kwargs['fc_bias_init'])

    def init_weights(self, fc_bias_init):
        self.fc.bias.data.fill_(fc_bias_init)
        self.fc.weight.data.uniform_(-0.1, 0.1)

    def forward(self, weighted_attn):
        state = registry.get(f'{weighted_attn.device}_lstm_state')
        h1, c1 = state['td_hidden']
        h2, c2 = state['lm_hidden']
        h2, c2 = self.language_lstm(torch.cat([weighted_attn, h1], dim=1), (h2, c2))
        predictions = self.fc(self.dropout(h2))
        state['lm_hidden'] = h2, c2
        return predictions


class ImageFeatureEmbedding(nn.Module):
    """
    parameters:

    input:
    image_feat_variable: [batch_size, num_location, image_feat_dim]
    or a list of [num_location, image_feat_dim]
    when using adaptive number of objects
    question_embedding:[batch_size, txt_embeding_dim]

    output:
    image_embedding:[batch_size, image_feat_dim]


    """

    def __init__(self, img_dim, question_dim, **kwargs):
        super().__init__()
        self.image_attention_model = AttentionLayer(img_dim, question_dim, **kwargs)
        self.out_dim = self.image_attention_model.out_dim

    def forward(self, image_feat_variable, question_embedding, image_dims, extra=None):
        if extra is None:
            extra = {}
        attention = self.image_attention_model(image_feat_variable, question_embedding, image_dims)
        att_reshape = attention.permute(0, 2, 1)
        order_vectors = getattr(extra, 'order_vectors', None)
        if order_vectors is not None:
            image_feat_variable = torch.cat([image_feat_variable, order_vectors], dim=-1)
        tmp_embedding = torch.bmm(att_reshape, image_feat_variable)
        batch_size = att_reshape.size(0)
        image_embedding = tmp_embedding.view(batch_size, -1)
        return image_embedding, attention


class MultiHeadImageFeatureEmbedding(nn.Module):

    def __init__(self, img_dim, question_dim, **kwargs):
        super().__init__()
        self.module = nn.MultiheadAttention(embed_dim=question_dim, kdim=img_dim, vdim=img_dim, **kwargs)
        self.out_dim = question_dim

    def forward(self, image_feat_variable, question_embedding, image_dims, extra=None):
        if extra is None:
            extra = {}
        image_feat_variable = image_feat_variable.transpose(0, 1)
        question_embedding = question_embedding.unsqueeze(1).transpose(0, 1)
        output, weights = self.module(question_embedding, image_feat_variable, image_feat_variable)
        output = output.transpose(0, 1)
        return output.squeeze(), weights


class ImageFinetune(nn.Module):

    def __init__(self, in_dim, weights_file, bias_file):
        super().__init__()
        with PathManager.open(weights_file, 'rb') as w:
            weights = pickle.load(w)
        with PathManager.open(bias_file, 'rb') as b:
            bias = pickle.load(b)
        out_dim = bias.shape[0]
        self.lc = nn.Linear(in_dim, out_dim)
        self.lc.weight.data.copy_(torch.from_numpy(weights))
        self.lc.bias.data.copy_(torch.from_numpy(bias))
        self.out_dim = out_dim

    def forward(self, image):
        i2 = self.lc(image)
        i3 = nn.functional.relu(i2)
        return i3


class SGAEmbedding(nn.Module):
    """Decoder block implementation in MCAN https://arxiv.org/abs/1906.10770"""

    def __init__(self, embedding_dim: int, **kwargs):
        super().__init__()
        num_attn = kwargs['num_attn']
        num_layers = kwargs['num_layers']
        dropout = kwargs.get('dropout', 0.1)
        hidden_dim = kwargs.get('hidden_dim', 512)
        self.linear = nn.Linear(embedding_dim, hidden_dim)
        self.self_guided_attns = nn.ModuleList([SelfGuidedAttention(hidden_dim, num_attn, dropout) for _ in range(num_layers)])
        self.out_dim = hidden_dim

    def forward(self, x: torch.Tensor, y: torch.Tensor, x_mask: torch.Tensor, y_mask: torch.Tensor) ->torch.Tensor:
        if x.dim() == 4:
            b, c, h, w = x.shape
            x = x.view(b, c, -1).transpose(1, 2).contiguous()
        x = self.linear(x)
        for self_guided_attn in self.self_guided_attns:
            x = self_guided_attn(x, y, x_mask, y_mask)
        return x


class CBNEmbedding(nn.Module):
    """MoVie bottleneck layers from https://arxiv.org/abs/2004.11883"""

    def __init__(self, embedding_dim: int, **kwargs):
        super().__init__()
        cond_dim = kwargs['cond_dim']
        num_layers = kwargs['cbn_num_layers']
        compressed = kwargs.get('compressed', True)
        use_se = kwargs.get('use_se', True)
        self.out_dim = 1024
        self.layer_norm = nn.LayerNorm(self.out_dim)
        cbns = []
        for i in range(num_layers):
            if embedding_dim != self.out_dim:
                downsample = nn.Conv2d(embedding_dim, self.out_dim, kernel_size=1, stride=1, bias=False)
                cbns.append(MovieBottleneck(embedding_dim, self.out_dim // 4, cond_dim, downsample=downsample, compressed=compressed, use_se=use_se))
            else:
                cbns.append(MovieBottleneck(embedding_dim, self.out_dim // 4, cond_dim, compressed=compressed, use_se=use_se))
            embedding_dim = self.out_dim
        self.cbns = nn.ModuleList(cbns)
        self._init_layers()

    def _init_layers(self) ->None:
        for cbn in self.cbns:
            cbn.init_layers()

    def forward(self, x: torch.Tensor, v: torch.Tensor) ->torch.Tensor:
        for cbn in self.cbns:
            x, _ = cbn(x, v)
        x = self.layer_norm(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(3).squeeze(2))
        return x


class TwoBranchEmbedding(nn.Module):
    """Attach MoVie into MCAN model as a counting module in
    https://arxiv.org/abs/2004.11883
    """

    def __init__(self, embedding_dim: int, **kwargs):
        super().__init__()
        hidden_dim = kwargs.get('hidden_dim', 512)
        self.sga = SGAEmbedding(embedding_dim, **kwargs)
        self.sga_pool = AttnPool1d(hidden_dim, 1)
        self.cbn = CBNEmbedding(embedding_dim, **kwargs)
        self.out_dim = hidden_dim

    def forward(self, x: torch.Tensor, y: torch.Tensor, v: torch.Tensor, x_mask: torch.Tensor, y_mask: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
        x_sga = self.sga(x, y, x_mask, y_mask)
        x_sga = self.sga_pool(x_sga, x_sga, x_mask).squeeze(1)
        x_cbn = self.cbn(x, v)
        return x_sga, x_cbn


class ImageFeatureEncoderTypes(Enum):
    default = 'default'
    identity = 'identity'
    projection = 'projection'
    frcnn_fc7 = 'finetune_faster_rcnn_fpn_fc7'


def check_fft_version():
    if version.parse(torch.__version__) >= version.parse('1.7'):
        if 'torch.fft' not in sys.modules:
            raise RuntimeError('torch.fft module available but not imported')


def irfft(input_tensor, s=None, signal_ndim=1, dim=None, norm=None) ->torch.Tensor:
    check_fft_version()
    if 'torch.fft' not in sys.modules:
        return torch.irfft(input_tensor, signal_ndim=signal_ndim, signal_sizes=s)
    else:
        return torch.fft.irfftn(input_tensor, s, dim, norm)


def rfft(input_tensor, signal_ndim=1, n=None, dim=-1, norm=None) ->torch.Tensor:
    check_fft_version()
    if 'torch.fft' not in sys.modules:
        return torch.rfft(input_tensor, signal_ndim=signal_ndim)
    else:
        return torch.fft.rfft(input_tensor, n, dim, norm)


class CompactBilinearPooling(nn.Module):

    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool=True):
        super().__init__()
        self.output_dim = output_dim
        self.sum_pool = sum_pool
        self.sketch1 = nn.Parameter(self.generate_sketch_matrix(torch.randint(output_dim, size=(input_dim1,)), 2 * torch.randint(2, size=(input_dim1,)) - 1, input_dim1, output_dim), requires_grad=False)
        self.sketch2 = nn.Parameter(self.generate_sketch_matrix(torch.randint(output_dim, size=(input_dim2,)), 2 * torch.randint(2, size=(input_dim2,)) - 1, input_dim2, output_dim), requires_grad=False)

    def generate_sketch_matrix(self, rand_h, rand_s, input_dim, output_dim):
        return torch.sparse.FloatTensor(torch.stack([torch.arange(input_dim, out=torch.LongTensor()), rand_h.long()]), rand_s.float(), [input_dim, output_dim]).to_dense()

    def forward(self, x1, x2):
        assert len(x1.shape) == len(x2.shape)
        if len(x1.shape) == 4 and len(x2.shape) == 4:
            fft1 = rfft(x1.permute(0, 2, 3, 1).matmul(self.sketch1), signal_ndim=1)
            fft2 = rfft(x2.permute(0, 2, 3, 1).matmul(self.sketch2), signal_ndim=1)
        else:
            fft1 = rfft(x1.matmul(self.sketch1), signal_ndim=1)
            fft2 = rfft(x2.matmul(self.sketch2), signal_ndim=1)
        fft_product = torch.stack([fft1[..., 0] * fft2[..., 0] - fft1[..., 1] * fft2[..., 1], fft1[..., 0] * fft2[..., 1] + fft1[..., 1] * fft2[..., 0]], dim=-1)
        cbp = irfft(fft_product, signal_ndim=1, dim=-1, s=(self.output_dim,)) * self.output_dim
        if len(x1.shape) == 4 and len(x2.shape) == 4:
            cbp = cbp.sum(dim=[1, 2]) if self.sum_pool else cbp.permute(0, 3, 1, 2)
        return cbp


class BlockTucker(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1600, chunks=20, shared=False, dropout_input=0.0, dropout_pre_lin=0.0, dropout_output=0.0, pos_norm='before_cat'):
        super().__init__()
        self.input_dims = input_dims
        self.output_dim = output_dim
        self.mm_dim = mm_dim
        self.chunks = chunks
        self.shared = shared
        self.dropout_input = dropout_input
        self.dropout_pre_lin = dropout_pre_lin
        self.dropout_output = dropout_output
        assert pos_norm in ['before_cat', 'after_cat']
        self.pos_norm = pos_norm
        self.linear0 = nn.Linear(input_dims[0], mm_dim)
        if self.shared:
            self.linear1 = self.linear0
        else:
            self.linear1 = nn.Linear(input_dims[1], mm_dim)
        self.sizes_list = get_sizes_list(mm_dim, chunks)
        bilinears = []
        for size in self.sizes_list:
            bilinears.append(nn.Bilinear(size, size, size))
        self.bilinears = nn.ModuleList(bilinears)
        self.linear_out = nn.Linear(self.mm_dim, self.output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        if self.dropout_input:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        x0_chunks = get_chunks(x0, self.sizes_list)
        x1_chunks = get_chunks(x1, self.sizes_list)
        zs = []
        for chunk_id, bilinear in enumerate(self.bilinears):
            x0_c = x0_chunks[chunk_id]
            x1_c = x1_chunks[chunk_id]
            z = bilinear(x0_c, x1_c)
            if self.pos_norm == 'before_cat':
                z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
                z = F.normalize(z, p=2)
            zs.append(z)
        z = torch.cat(zs, 1)
        if self.pos_norm == 'after_cat':
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        if self.dropout_pre_lin > 0:
            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)
        z = self.linear_out(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class Mutan(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1600, rank=15, shared=False, normalize=False, dropout_input=0.0, dropout_pre_lin=0.0, dropout_output=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.shared = shared
        self.mm_dim = mm_dim
        self.rank = rank
        self.output_dim = output_dim
        self.dropout_input = dropout_input
        self.dropout_pre_lin = dropout_pre_lin
        self.dropout_output = dropout_output
        self.normalize = normalize
        self.linear0 = nn.Linear(input_dims[0], mm_dim)
        self.merge_linear0 = nn.Linear(mm_dim, mm_dim * rank)
        if self.shared:
            self.linear1 = self.linear0
            self.merge_linear1 = self.merge_linear0
        else:
            self.linear1 = nn.Linear(input_dims[1], mm_dim)
            self.merge_linear1 = nn.Linear(mm_dim, mm_dim * rank)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        if self.dropout_input > 0:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        m0 = self.merge_linear0(x0)
        m1 = self.merge_linear1(x1)
        m = m0 * m1
        m = m.view(-1, self.rank, self.mm_dim)
        z = torch.sum(m, 1)
        if self.normalize:
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        if self.dropout_pre_lin > 0:
            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)
        z = self.linear_out(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class Tucker(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1600, shared=False, normalize=False, dropout_input=0.0, dropout_pre_lin=0.0, dropout_output=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.shared = shared
        self.mm_dim = mm_dim
        self.output_dim = output_dim
        self.normalize = normalize
        self.dropout_input = dropout_input
        self.dropout_pre_lin = dropout_pre_lin
        self.dropout_output = dropout_output
        self.linear0 = nn.Linear(input_dims[0], mm_dim)
        if shared:
            self.linear1 = self.linear0
        else:
            self.linear1 = nn.Linear(input_dims[1], mm_dim)
        self.linear1 = nn.Linear(input_dims[1], mm_dim)
        self.bilinear = nn.Bilinear(mm_dim, mm_dim, mm_dim)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        if self.dropout_input > 0:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        z = self.bilinear(x0, x1)
        if self.normalize:
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        if self.dropout_pre_lin > 0:
            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)
        z = self.linear_out(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class MLB(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1200, activ_input='relu', activ_output='relu', normalize=False, dropout_input=0.0, dropout_pre_lin=0.0, dropout_output=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.mm_dim = mm_dim
        self.output_dim = output_dim
        self.activ_input = activ_input
        self.activ_output = activ_output
        self.normalize = normalize
        self.dropout_input = dropout_input
        self.dropout_pre_lin = dropout_pre_lin
        self.dropout_output = dropout_output
        self.linear0 = nn.Linear(input_dims[0], mm_dim)
        self.linear1 = nn.Linear(input_dims[1], mm_dim)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        if self.activ_input:
            x0 = getattr(F, self.activ_input)(x0)
            x1 = getattr(F, self.activ_input)(x1)
        if self.dropout_input > 0:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        z = x0 * x1
        if self.normalize:
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        if self.dropout_pre_lin > 0:
            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)
        z = self.linear_out(z)
        if self.activ_output:
            z = getattr(F, self.activ_output)(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class MFB(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1200, factor=2, activ_input='relu', activ_output='relu', normalize=False, dropout_input=0.0, dropout_pre_norm=0.0, dropout_output=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.mm_dim = mm_dim
        self.factor = factor
        self.output_dim = output_dim
        self.activ_input = activ_input
        self.activ_output = activ_output
        self.normalize = normalize
        self.dropout_input = dropout_input
        self.dropout_pre_norm = dropout_pre_norm
        self.dropout_output = dropout_output
        self.linear0 = nn.Linear(input_dims[0], mm_dim * factor)
        self.linear1 = nn.Linear(input_dims[1], mm_dim * factor)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        if self.activ_input:
            x0 = getattr(F, self.activ_input)(x0)
            x1 = getattr(F, self.activ_input)(x1)
        if self.dropout_input > 0:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        z = x0 * x1
        if self.dropout_pre_norm > 0:
            z = F.dropout(z, p=self.dropout_pre_norm, training=self.training)
        z = z.view(z.size(0), self.mm_dim, self.factor)
        z = z.sum(2)
        if self.normalize:
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        z = self.linear_out(z)
        if self.activ_output:
            z = getattr(F, self.activ_output)(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class MCB(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=16000, activ_output='relu', dropout_output=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.output_dim = output_dim
        self.mm_dim = mm_dim
        self.activ_output = activ_output
        self.dropout_output = dropout_output
        self.mcb = CompactBilinearPooling(input_dims[0], input_dims[1], mm_dim)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        z = self.mcb(x[0], x[1])
        z = self.linear_out(z)
        if self.activ_output:
            z = getattr(F, self.activ_output)(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class LinearSum(nn.Module):

    def __init__(self, input_dims, output_dim, mm_dim=1200, activ_input='relu', activ_output='relu', normalize=False, dropout_input=0.0, dropout_pre_lin=0.0, dropout_output=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.output_dim = output_dim
        self.mm_dim = mm_dim
        self.activ_input = activ_input
        self.activ_output = activ_output
        self.normalize = normalize
        self.dropout_input = dropout_input
        self.dropout_pre_lin = dropout_pre_lin
        self.dropout_output = dropout_output
        self.linear0 = nn.Linear(input_dims[0], mm_dim)
        self.linear1 = nn.Linear(input_dims[1], mm_dim)
        self.linear_out = nn.Linear(mm_dim, output_dim)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        x0 = self.linear0(x[0])
        x1 = self.linear1(x[1])
        if self.activ_input:
            x0 = getattr(F, self.activ_input)(x0)
            x1 = getattr(F, self.activ_input)(x1)
        if self.dropout_input > 0:
            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)
            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)
        z = x0 + x1
        if self.normalize:
            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))
            z = F.normalize(z, p=2)
        if self.dropout_pre_lin > 0:
            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)
        z = self.linear_out(z)
        if self.activ_output:
            z = getattr(F, self.activ_output)(z)
        if self.dropout_output > 0:
            z = F.dropout(z, p=self.dropout_output, training=self.training)
        return z


class ConcatMLP(nn.Module):

    def __init__(self, input_dims, output_dim, dimensions=None, activation='relu', dropout=0.0):
        super().__init__()
        self.input_dims = input_dims
        self.output_dim = output_dim
        self.input_dim = sum(input_dims)
        if dimensions is None:
            dimensions = [500, 500]
        self.dimensions = dimensions + [output_dim]
        self.activation = activation
        self.dropout = dropout
        self.mlp = MLP(self.input_dim, self.dimensions, self.activation, self.dropout)
        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        log_class_usage('Fusion', self.__class__)

    def forward(self, x):
        if x[0].dim() == 3 and x[1].dim() == 2:
            x[1] = x[1].unsqueeze(1).reshape_as(x[0])
        if x[1].dim() == 3 and x[0].dim() == 2:
            x[0] = x[0].unsqueeze(1).reshape_as(x[1])
        z = torch.cat(x, dim=x[0].dim() - 1)
        z = self.mlp(z)
        return z


class ConvNet(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, padding_size='same', pool_stride=2, batch_norm=True):
        super().__init__()
        if padding_size == 'same':
            padding_size = kernel_size // 2
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding_size)
        self.max_pool2d = nn.MaxPool2d(pool_stride, stride=pool_stride)
        self.batch_norm = batch_norm
        if self.batch_norm:
            self.batch_norm_2d = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = self.max_pool2d(nn.functional.leaky_relu(self.conv(x)))
        if self.batch_norm:
            x = self.batch_norm_2d(x)
        return x


class Flatten(nn.Module):

    def forward(self, input):
        if input.dim() > 1:
            input = input.view(input.size(0), -1)
        return input


class UnFlatten(nn.Module):

    def forward(self, input, sizes=None):
        if sizes is None:
            sizes = []
        return input.view(input.size(0), *sizes)


class BertClassifierHead(nn.Module):

    def __init__(self, in_dim=768, out_dim=2, config=None, *args, **kwargs):
        super().__init__()
        if config is None:
            config = BertConfig.from_pretrained('bert-base-uncased')
        assert config.hidden_size == in_dim
        self.module = nn.Sequential(nn.Dropout(config.hidden_dropout_prob), BertPredictionHeadTransform(config), nn.Linear(in_dim, out_dim))

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class LogitClassifier(nn.Module):

    def __init__(self, in_dim, out_dim, **kwargs):
        super().__init__()
        input_dim = in_dim
        num_ans_candidates = out_dim
        text_non_linear_dim = kwargs['text_hidden_dim']
        image_non_linear_dim = kwargs['img_hidden_dim']
        self.f_o_text = ReLUWithWeightNormFC(input_dim, text_non_linear_dim)
        self.f_o_image = ReLUWithWeightNormFC(input_dim, image_non_linear_dim)
        self.linear_text = nn.Linear(text_non_linear_dim, num_ans_candidates)
        self.linear_image = nn.Linear(image_non_linear_dim, num_ans_candidates)
        if 'pretrained_image' in kwargs and kwargs['pretrained_text'] is not None:
            self.linear_text.weight.data.copy_(torch.from_numpy(kwargs['pretrained_text']))
        if 'pretrained_image' in kwargs and kwargs['pretrained_image'] is not None:
            self.linear_image.weight.data.copy_(torch.from_numpy(kwargs['pretrained_image']))

    def forward(self, joint_embedding):
        text_val = self.linear_text(self.f_o_text(joint_embedding))
        image_val = self.linear_image(self.f_o_image(joint_embedding))
        logit_value = text_val + image_val
        return logit_value


class MLPClassifer(nn.Module):

    def __init__(self, in_dim, out_dim, hidden_dim=None, num_layers=0, dropout=0.5, hidden_act='relu', batch_norm=True, **kwargs):
        super().__init__()
        activation = ACT2FN[hidden_act]
        self.layers = nn.ModuleList()
        if hidden_dim is None:
            hidden_dim = in_dim
        for _ in range(num_layers):
            self.layers.append(nn.Linear(in_dim, hidden_dim))
            if batch_norm:
                self.layers.append(nn.BatchNorm1d(hidden_dim))
            self.layers.append(activation())
            self.layers.append(nn.Dropout(dropout))
            in_dim = hidden_dim
        self.layers.append(nn.Linear(in_dim, out_dim))

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


class TripleLinear(nn.Module):
    """
    The three-branch classifier in https://arxiv.org/abs/2004.11883:
    During training, all three branches will produce the prediction on its own.
    During inference, only the fused branch is used to predict the answers.
    """

    def __init__(self, in_dim: int, out_dim: int):
        super().__init__()
        self.linears = nn.ModuleList([nn.Linear(in_dim, out_dim) for _ in range(3)])

    def forward(self, joint_embedding: torch.Tensor) ->torch.Tensor:
        if self.training:
            feat = [self.linears[i](joint_embedding[:, i]) for i in range(3)]
            return torch.stack(feat, dim=1)
        return self.linears[0](joint_embedding)


class WeightNormClassifier(nn.Module):

    def __init__(self, in_dim, out_dim, hidden_dim, dropout):
        super().__init__()
        layers = [weight_norm(nn.Linear(in_dim, hidden_dim), dim=None), nn.ReLU(), nn.Dropout(dropout, inplace=True), weight_norm(nn.Linear(hidden_dim, out_dim), dim=None)]
        self.main = nn.Sequential(*layers)

    def forward(self, x):
        logits = self.main(x)
        return logits


class ClassifierLayer(nn.Module):

    def __init__(self, classifier_type, in_dim, out_dim, **kwargs):
        super().__init__()
        if classifier_type == 'weight_norm':
            self.module = WeightNormClassifier(in_dim, out_dim, **kwargs)
        elif classifier_type == 'logit':
            self.module = LogitClassifier(in_dim, out_dim, **kwargs)
        elif classifier_type == 'language_decoder':
            self.module = LanguageDecoder(in_dim, out_dim, **kwargs)
        elif classifier_type == 'bert':
            self.module = BertClassifierHead(in_dim, out_dim, kwargs.get('config', None)).module
        elif classifier_type == 'mlp':
            self.module = MLPClassifer(in_dim, out_dim, **kwargs)
        elif classifier_type == 'triple_linear':
            self.module = TripleLinear(in_dim, out_dim)
        elif classifier_type == 'linear':
            self.module = nn.Linear(in_dim, out_dim)
        else:
            raise NotImplementedError('Unknown classifier type: %s' % classifier_type)

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class FCNet(nn.Module):
    """
    Simple class for non-linear fully connect network
    """

    def __init__(self, dims, act='ReLU', dropout=0):
        super().__init__()
        layers = []
        for i in range(len(dims) - 2):
            in_dim = dims[i]
            out_dim = dims[i + 1]
            if dropout > 0:
                layers.append(nn.Dropout(dropout))
            layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))
            if act is not None:
                layers.append(getattr(nn, act)())
        if dropout > 0:
            layers.append(nn.Dropout(dropout))
        layers.append(weight_norm(nn.Linear(dims[-2], dims[-1]), dim=None))
        if act is not None:
            layers.append(getattr(nn, act)())
        self.main = nn.Sequential(*layers)

    def forward(self, x):
        return self.main(x)


class BCNet(nn.Module):
    """
    Simple class for non-linear bilinear connect network
    """

    def __init__(self, v_dim, q_dim, h_dim, h_out, act='ReLU', dropout=None, k=3):
        super().__init__()
        self.c = 32
        self.k = k
        self.v_dim = v_dim
        self.q_dim = q_dim
        self.h_dim = h_dim
        self.h_out = h_out
        if dropout is None:
            dropout = [0.2, 0.5]
        self.v_net = FCNet([v_dim, h_dim * self.k], act=act, dropout=dropout[0])
        self.q_net = FCNet([q_dim, h_dim * self.k], act=act, dropout=dropout[0])
        self.dropout = nn.Dropout(dropout[1])
        if k > 1:
            self.p_net = nn.AvgPool1d(self.k, stride=self.k)
        if h_out is None:
            pass
        elif h_out <= self.c:
            self.h_mat = nn.Parameter(torch.Tensor(1, h_out, 1, h_dim * self.k).normal_())
            self.h_bias = nn.Parameter(torch.Tensor(1, h_out, 1, 1).normal_())
        else:
            self.h_net = weight_norm(nn.Linear(h_dim * self.k, h_out), dim=None)

    def forward(self, v, q):
        if self.h_out is None:
            v_ = self.v_net(v).transpose(1, 2).unsqueeze(3)
            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)
            d_ = torch.matmul(v_, q_)
            logits = d_.transpose(1, 2).transpose(2, 3)
            return logits
        elif self.h_out <= self.c:
            v_ = self.dropout(self.v_net(v)).unsqueeze(1)
            q_ = self.q_net(q)
            h_ = v_ * self.h_mat
            logits = torch.matmul(h_, q_.unsqueeze(1).transpose(2, 3))
            logits = logits + self.h_bias
            return logits
        else:
            v_ = self.dropout(self.v_net(v)).transpose(1, 2).unsqueeze(3)
            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)
            d_ = torch.matmul(v_, q_)
            logits = self.h_net(d_.transpose(1, 2).transpose(2, 3))
            return logits.transpose(2, 3).transpose(1, 2)

    def forward_with_weights(self, v, q, w):
        v_ = self.v_net(v).transpose(1, 2).unsqueeze(2)
        q_ = self.q_net(q).transpose(1, 2).unsqueeze(3)
        logits = torch.matmul(torch.matmul(v_, w.unsqueeze(1)), q_)
        logits = logits.squeeze(3).squeeze(2)
        if self.k > 1:
            logits = logits.unsqueeze(1)
            logits = self.p_net(logits).squeeze(1) * self.k
        return logits


class BiAttention(nn.Module):

    def __init__(self, x_dim, y_dim, z_dim, glimpse, dropout=None):
        super().__init__()
        if dropout is None:
            dropout = [0.2, 0.5]
        self.glimpse = glimpse
        self.logits = weight_norm(BCNet(x_dim, y_dim, z_dim, glimpse, dropout=dropout, k=3), name='h_mat', dim=None)

    def forward(self, v, q, v_mask=True):
        p, logits = self.forward_all(v, q, v_mask)
        return p, logits

    def forward_all(self, v, q, v_mask=True):
        v_num = v.size(1)
        q_num = q.size(1)
        logits = self.logits(v, q)
        if v_mask:
            v_abs_sum = v.abs().sum(2)
            mask = (v_abs_sum == 0).unsqueeze(1).unsqueeze(3)
            mask = mask.expand(logits.size())
            logits.masked_fill_(mask, -float('inf'))
        expanded_logits = logits.view(-1, self.glimpse, v_num * q_num)
        p = nn.functional.softmax(expanded_logits, 2)
        return p.view(-1, self.glimpse, v_num, q_num), logits


class BranchCombineLayer(nn.Module):
    """Three-branch fusion module used for fusing MoVie and MCAN in
    https://arxiv.org/abs/2004.11883
    """

    def __init__(self, img_dim: int, ques_dim: int):
        super().__init__()
        self.out_dim = img_dim * 2
        self.linear_cga = nn.ModuleList([nn.Linear(img_dim, self.out_dim) for _ in range(2)])
        self.linear_cbn = nn.ModuleList([nn.Linear(img_dim, self.out_dim) for _ in range(2)])
        self.linear_ques = nn.ModuleList([nn.Linear(ques_dim, self.out_dim) for _ in range(2)])
        self.layer_norm = nn.ModuleList([nn.LayerNorm(self.out_dim) for _ in range(3)])

    def forward(self, v_cga: torch.Tensor, v_cbn: torch.Tensor, q: torch.Tensor) ->torch.Tensor:
        feat = [self.layer_norm[0](self.linear_ques[0](q) + self.linear_cbn[0](v_cbn) + self.linear_cga[0](v_cga)), self.layer_norm[1](self.linear_cbn[1](v_cbn)), self.layer_norm[2](self.linear_ques[1](q) + self.linear_cga[1](v_cga))]
        if self.training:
            return torch.stack(feat, dim=1)
        return feat[0]


class AttnPool2d(nn.Module):
    """An attention pooling layer in 2D with multiheaded attention"""

    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int=None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)
        x = x + self.positional_embedding[:, None, :]
        x, _ = nn.functional.multi_head_attention_forward(query=x, key=x, value=x, embed_dim_to_check=x.shape[-1], num_heads=self.num_heads, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight, in_proj_weight=None, in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]), bias_k=None, bias_v=None, add_zero_attn=False, dropout_p=0, out_proj_weight=self.c_proj.weight, out_proj_bias=self.c_proj.bias, use_separate_proj_weight=True, training=self.training, need_weights=False)
        return x[0]


class TripleLogitBinaryCrossEntropy(nn.Module):
    """
    This is used for Three-branch fusion only. We predict scores and compute
    cross entropy loss for each of branches.
    """

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        """Calculates and returns the binary cross entropy for logits
        Args:
            sample_list (SampleList): SampleList containing `targets` attribute.
            model_output (Dict): Model output containing `scores` attribute.
        Returns:
            torch.FloatTensor: Float value for loss.
        """
        scores = model_output['scores']
        targets = sample_list['targets']
        if scores.dim() == 3:
            loss = F.binary_cross_entropy_with_logits(scores[:, 0], targets, reduction='mean') + F.binary_cross_entropy_with_logits(scores[:, 1], targets, reduction='mean') + F.binary_cross_entropy_with_logits(scores[:, 2], targets, reduction='mean')
        else:
            loss = F.binary_cross_entropy_with_logits(scores, targets, reduction='mean')
        return loss * targets.size(-1)


class BinaryCrossEntropyLoss(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        """Calculates and returns the binary cross entropy.

        Args:
            sample_list (SampleList): SampleList containing `targets` attribute.
            model_output (Dict): Model output containing `scores` attribute.

        Returns:
            torch.FloatTensor: Float value for loss.

        """
        scores = model_output['scores']
        targets = sample_list['targets']
        loss = F.binary_cross_entropy(scores, targets, reduction='mean')
        return loss * targets.size(1)


class CaptionCrossEntropyLoss(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        """Calculates and returns the cross entropy loss for captions.

        Args:
            sample_list (SampleList): SampleList containing `targets` attribute.
            model_output (Dict): Model output containing `scores` attribute.

        Returns:
            torch.FloatTensor: Float value for loss.

        """
        scores = model_output['scores']
        targets = sample_list['targets']
        if hasattr(sample_list, 'caption_len'):
            caption_lengths, _ = sample_list.caption_len.sort(dim=0, descending=True)
            decode_lengths = (caption_lengths - 1).tolist()
        else:
            decode_lengths = [targets.size(1)] * targets.size(0)
        if version.parse(torch.__version__) >= version.parse('1.1'):
            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data
            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data
        else:
            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)
            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)
        loss = F.cross_entropy(scores, targets)
        return loss


class NLLLoss(nn.Module):
    """Negative log likelikehood loss."""

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        """Calculates and returns the negative log likelihood.

        Args:
            sample_list (SampleList): SampleList containing `targets` attribute.
            model_output (Dict): Model output containing `scores` attribute.

        Returns:
            torch.FloatTensor: Float value for loss.

        """
        scores = model_output['scores']
        targets = sample_list['targets']
        _, idx = targets.max(dim=1)
        loss = F.nll_loss(scores, idx, reduction='mean')
        return loss * targets.size(1)


class MultiLoss(nn.Module):
    """A loss for combining multiple losses with weights.

    Args:
        params (List(Dict)): A list containing parameters for each different loss
                             and their weights.

    Example::

        # MultiLoss works with config like below where each loss's params and
        # weights are defined
        losses:
        - type: multi
          params:
          - type: logit_bce
            weight: 0.3
            params: {}
          - type: attention_supervision
            weight: 0.7
            params: {}

    """

    def __init__(self, params):
        super().__init__()
        self.losses = []
        self.losses_weights = []
        self.loss_names = []
        for loss_params in params['params']:
            self.loss_names.append(loss_params['type'])
            loss_fn = MMFLoss(loss_params)
            loss_weight = loss_params.get('weight', {})
            self.losses.append(loss_fn)
            self.losses_weights.append(loss_weight)

    def forward(self, sample_list, model_output, *args, **kwargs):
        """Calculates and returns the multi loss.

        Args:
            sample_list (SampleList): SampleList containing `attentions` attribute.
            model_output (Dict): Model output containing `attention_supervision`
                                 attribute.

        Returns:
            torch.FloatTensor: Float value for loss.

        """
        loss = 0
        for idx, loss_fn in enumerate(self.losses):
            value = loss_fn(sample_list, model_output, *args, **kwargs)
            loss += self.losses_weights[idx] * list(value.values())[0]
        return loss


class AttentionSupervisionLoss(nn.Module):
    """Loss for attention supervision. Used in case you want to make attentions
    similar to some particular values.
    """

    def __init__(self):
        super().__init__()
        self.loss_fn = lambda *args, **kwargs: nn.functional.binary_cross_entropy(*args, **kwargs)

    def forward(self, sample_list, model_output):
        """Calculates and returns the multi loss.

        Args:
            sample_list (SampleList): SampleList containing `targets` attribute.
            model_output (Dict): Model output containing `scores` attribute.

        Returns:
            torch.FloatTensor: Float value for loss.

        """
        context_attentions = model_output['attentions']
        attention_supervision = sample_list['info']['attention_supervision']
        loss = self.loss_fn(context_attentions[0], attention_supervision.float(), weight=attention_supervision.float())
        return loss * attention_supervision.size(1)


def kl_div(log_x, y):
    y_is_0 = torch.eq(y.data, 0)
    y.data.masked_fill_(y_is_0, 1)
    log_y = torch.log(y)
    y.data.masked_fill_(y_is_0, 0)
    res = y * (log_y - log_x)
    return torch.sum(res, dim=1, keepdim=True)


class WeightedSoftmaxLoss(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        pred_score = model_output['scores']
        target_score = sample_list['targets']
        tar_sum = torch.sum(target_score, dim=1, keepdim=True)
        tar_sum_is_0 = torch.eq(tar_sum, 0)
        tar_sum.masked_fill_(tar_sum_is_0, 1e-06)
        tar = target_score / tar_sum
        res = F.log_softmax(pred_score, dim=1)
        loss = kl_div(res, tar)
        loss = loss * tar_sum
        loss = torch.sum(loss) / loss.size(0)
        return loss


class SoftmaxKlDivLoss(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        pred_score = model_output['scores']
        target_score = sample_list['targets']
        tar_sum = torch.sum(target_score, dim=1, keepdim=True)
        tar_sum_is_0 = torch.eq(tar_sum, 0)
        tar_sum.masked_fill_(tar_sum_is_0, 1e-06)
        tar = target_score / tar_sum
        res = F.log_softmax(pred_score, dim=1)
        loss = kl_div(res, tar)
        loss = torch.sum(loss) / loss.size(0)
        return loss


class WrongLoss(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        pred_score = model_output['scores']
        target_score = sample_list['targets']
        tar_sum = torch.sum(target_score, dim=1, keepdim=True)
        tar_sum_is_0 = torch.eq(tar_sum, 0)
        tar_sum.masked_fill_(tar_sum_is_0, 1e-06)
        tar = target_score / tar_sum
        res = F.log_softmax(pred_score, dim=1)
        loss = F.kl_div(res, tar, reduction='mean')
        loss *= target_score.size(1)
        return loss


class CombinedLoss(nn.Module):

    def __init__(self, weight_softmax):
        super().__init__()
        self.weight_softmax = weight_softmax

    def forward(self, sample_list, model_output):
        pred_score = model_output['scores']
        target_score = sample_list['targets']
        tar_sum = torch.sum(target_score, dim=1, keepdim=True)
        tar_sum_is_0 = torch.eq(tar_sum, 0)
        tar_sum.masked_fill_(tar_sum_is_0, 1e-06)
        tar = target_score / tar_sum
        res = F.log_softmax(pred_score, dim=1)
        loss1 = kl_div(res, tar)
        loss1 = torch.sum(loss1) / loss1.size(0)
        loss2 = F.binary_cross_entropy_with_logits(pred_score, target_score, reduction='mean')
        loss2 *= target_score.size(1)
        loss = self.weight_softmax * loss1 + loss2
        return loss


class M4CDecodingBCEWithMaskLoss(nn.Module):

    def __init__(self):
        super().__init__()
        self.one = torch.Tensor([1.0])

    def forward(self, sample_list, model_output):
        scores = model_output['scores']
        targets = sample_list['targets']
        loss_mask = sample_list['train_loss_mask']
        assert scores.dim() == 3 and loss_mask.dim() == 2
        losses = F.binary_cross_entropy_with_logits(scores, targets, reduction='none')
        losses *= loss_mask.unsqueeze(-1)
        count = torch.max(torch.sum(loss_mask), self.one)
        loss = torch.sum(losses) / count
        return loss


class CrossEntropyLoss(nn.Module):

    def __init__(self, **params):
        super().__init__()
        self.loss_fn = nn.CrossEntropyLoss(**params)

    def forward(self, sample_list, model_output):
        return self.loss_fn(model_output['scores'], sample_list['targets'])


class SoftLabelCrossEntropyLoss(nn.Module):

    def __init__(self, ignore_index=-100, reduction='mean', normalize_targets=True):
        assert reduction in ('mean', 'sum'), 'Argument `reduction` only supports `mean` and `sum`'
        super().__init__()
        self.ignore_index = ignore_index
        self.reduction = reduction
        self.normalize_targets = normalize_targets
        self.eps = torch.finfo(torch.float32).eps

    @staticmethod
    def convert_to_one_hot(targets, n_classes):
        one_hot_targets = torch.zeros((targets.size(0), n_classes), dtype=torch.long, device=targets.device)
        one_hot_targets.scatter_(1, targets.long().view(-1, 1), 1)
        return one_hot_targets

    def compute_loss(self, targets, scores):
        """for N examples and C classes
        - scores: N x C these are raw outputs (without softmax/sigmoid)
        - targets: N x C or N corresponding targets

        Target elements set to ignore_index contribute 0 loss.

        Samples where all entries are ignore_index do not contribute to the loss
        reduction.
        """
        assert targets.size(0) == scores.size(0), '`targets` and `scores` should have the same batch size'
        if targets.dim() == 1:
            targets = targets.unsqueeze(1)
            mask = targets.ne(self.ignore_index).float()
        else:
            mask = targets.sum(-1, keepdim=True).ne(0).float()
        if targets.size(1) == 1:
            targets = self.convert_to_one_hot(targets, scores.size(1))
        targets = targets.float() * mask
        if self.normalize_targets:
            targets /= self.eps + targets.sum(dim=1, keepdim=True)
        per_sample_per_target_loss = -targets * F.log_softmax(scores, dim=-1)
        per_sample_loss = torch.sum(per_sample_per_target_loss, -1)
        loss = per_sample_loss.sum()
        if self.reduction == 'mean':
            loss /= torch.sum(torch.sum(mask, -1) > 0).clamp(min=1)
        return loss

    def forward(self, sample_list, model_output):
        return self.compute_loss(sample_list['targets'], model_output['scores'])


class LabelSmoothingCrossEntropyLoss(SoftLabelCrossEntropyLoss):
    """Cross-entropy loss with label smoothing. If `label_smoothing` = 0, then
    it's canonical cross entropy.
    The smoothed one-hot encoding is 1 - label_smoothing for true label and
    label_smoothing / (num_classes - 1) for the rest.

    Reference: https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch
    """

    def __init__(self, label_smoothing=0.1, reduction='mean', ignore_index=-100):
        assert 0 <= label_smoothing < 1, 'value of argument `label_smoothing` must be in range [0, 1).'
        super().__init__(ignore_index, reduction, False)
        self.label_smoothing = label_smoothing

    def smooth_targets(self, targets, n_classes):
        if targets.dim() == 1:
            targets = targets.unsqueeze(1)
        mask = targets.ne(self.ignore_index)
        smoothing_value = self.label_smoothing / (n_classes - 1)
        one_hot = torch.full((n_classes,), smoothing_value, device=targets.device).repeat(targets.size(0), 1)
        one_hot.scatter_(1, targets * mask.long(), 1 - self.label_smoothing)
        return one_hot * mask.float()

    def forward(self, sample_list, model_output):
        scores = model_output['scores']
        one_hot = self.smooth_targets(sample_list['targets'], scores.size(1))
        loss = self.compute_loss(one_hot, scores)
        return loss


class InBatchHinge(nn.Module):
    """
    Based on the code from https://github.com/fartashf/vsepp/blob/master/model.py
    """

    def __init__(self, margin: float=0.0, hard: bool=False):
        super().__init__()
        self.margin = margin
        self.hard = hard

    def _compute_loss(self, correlations: Tensor):
        diagonal = correlations.diag()[:, None]
        d1 = diagonal.expand_as(correlations)
        d2 = diagonal.t().expand_as(correlations)
        cost_s = (self.margin + correlations - d1).clamp(min=0)
        cost_im = (self.margin + correlations - d2).clamp(min=0)
        mask = 1 - torch.eye(correlations.size(0), device=correlations.device)
        cost_s = cost_s * mask
        cost_im = cost_im * mask
        if self.hard:
            cost_s = cost_s.max(1)[0]
            cost_im = cost_im.max(0)[0]
        return cost_s.sum() + cost_im.sum()

    def forward(self, sample_list: Dict[str, Tensor], model_output: Dict[str, Tensor]):
        image_embeddings = model_output['scores']
        text_embeddings = model_output['targets']
        if image_embeddings.shape[0] == text_embeddings.shape[0]:
            correlations = image_embeddings @ text_embeddings.t()
            loss = self._compute_loss(correlations)
        else:
            assert text_embeddings.shape[0] % image_embeddings.shape[0] == 0
            batch_size, dim_size = image_embeddings.shape
            factor = text_embeddings.shape[0] // image_embeddings.shape[0]
            text_embeddings = text_embeddings.reshape(batch_size, factor, dim_size)
            correlations = image_embeddings @ text_embeddings.permute(1, 2, 0)
            loss = 0
            for corr in correlations:
                loss += self._compute_loss(corr)
        return loss


class GatherLayer(torch.autograd.Function):
    """
    Gather tensors from all workers with support for backward propagation:
    This implementation does not cut the gradients as torch.distributed.all_gather does.
    """

    @staticmethod
    def forward(ctx, x):
        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]
        dist.all_gather(output, x)
        return tuple(output)

    @staticmethod
    def backward(ctx, *grads):
        all_gradients = torch.stack(grads)
        dist.all_reduce(all_gradients)
        return all_gradients[dist.get_rank()]


class XLAGatherLayer(torch.autograd.Function):
    """
    Gather tensors from all TPU workers with support for backward propagation.
    """

    @staticmethod
    def forward(ctx, x, dim):
        ctx.dim = dim
        tensor_list = xm.all_gather(x.unsqueeze(dim), dim=dim)
        return tensor_list

    @staticmethod
    def backward(ctx, grad_output):
        dim = ctx.dim
        all_grad_output = xm.all_reduce(xm.REDUCE_SUM, grad_output)
        return all_grad_output.select(dim, xm.get_ordinal()), None


def gather_tensor_along_batch_with_backward(tensor, dim=0):
    world_size = get_world_size()
    if world_size < 2:
        return tensor
    if is_xla():
        tensor_list = XLAGatherLayer.apply(tensor, dim)
        tensor_list = tensor_list.flatten(start_dim=dim, end_dim=dim + 1)
    else:
        tensor_list = GatherLayer.apply(tensor)
        tensor_list = torch.cat(tensor_list, dim=dim)
    return tensor_list


class ContrastiveLoss(nn.Module):
    """
    This is a generic contrastive loss typically used for pretraining. No modality
    assumptions are made here.
    """

    def __init__(self):
        super().__init__()

    def forward(self, sample_list: Dict[str, Tensor], model_output: Dict[str, Tensor]):
        assert 'embedding_1' in model_output and 'embedding_2' in model_output, 'Embedding names must be available before loss calculation'
        embedding_1 = model_output['embedding_1']
        embedding_2 = model_output['embedding_2']
        assert embedding_1.size(0) == embedding_2.size(0), 'batch size must match'
        per_gpu_batch_size = embedding_1.size(0)
        embedding_1_all_gpus = gather_tensor_along_batch_with_backward(embedding_1)
        embedding_2_all_gpus = gather_tensor_along_batch_with_backward(embedding_2)
        temperature = model_output['temperature']
        logits_1 = torch.matmul(embedding_1, embedding_2_all_gpus.transpose(0, 1)) / temperature
        logits_2 = torch.matmul(embedding_2, embedding_1_all_gpus.transpose(0, 1)) / temperature
        labels = per_gpu_batch_size * get_rank() + torch.arange(per_gpu_batch_size, device=temperature.device)
        loss_1 = F.cross_entropy(logits_1, labels)
        loss_2 = F.cross_entropy(logits_2, labels)
        return (loss_1 + loss_2) / 2


class MSELoss(nn.Module):
    """Mean Squared Error loss"""

    def __init__(self):
        super().__init__()
        self.loss_fn = nn.MSELoss()

    def forward(self, sample_list, model_output):
        targets = sample_list['targets']
        scores = model_output['scores']
        loss = self.loss_fn(scores, targets)
        return loss


class CosineEmbeddingLoss(nn.Module):
    """Cosine embedding loss"""

    def __init__(self):
        super().__init__()
        self.loss_fn = nn.CosineEmbeddingLoss()

    def forward(self, sample_list, model_output):
        targets = sample_list['targets']
        scores = model_output['scores']
        y = torch.ones(targets.size(0))
        loss = self.loss_fn(scores, targets, y)
        return loss


class BCEAndKLLoss(nn.Module):
    """binary_cross_entropy_with_logits and kl divergence loss.
    Calculates both losses and returns a dict with string keys.
    Similar to bce_kl_combined, but returns both losses.
    """

    def __init__(self, weight_softmax):
        super().__init__()
        self.weight_softmax = weight_softmax

    def forward(self, sample_list, model_output):
        pred_score = model_output['scores']
        target_score = sample_list['targets']
        tar_sum = torch.sum(target_score, dim=1, keepdim=True)
        tar_sum_is_0 = torch.eq(tar_sum, 0)
        tar_sum.masked_fill_(tar_sum_is_0, 1e-06)
        tar = target_score / tar_sum
        res = F.log_softmax(pred_score, dim=1)
        loss1 = kl_div(res, tar)
        loss1 = torch.sum(loss1) / loss1.size(0)
        loss2 = F.binary_cross_entropy_with_logits(pred_score, target_score, reduction='mean')
        loss2 *= target_score.size(1)
        loss = {'kl': self.weight_softmax * loss1, 'bce': loss2}
        return loss


class AverageConcatLastN(nn.Module):

    def __init__(self, k=4, tol=1e-06):
        super().__init__()
        self.num_layers = k
        self.tol = tol

    def forward(self, encoded_layers: List[torch.Tensor], pad_mask: torch.Tensor):
        assert self.num_layers <= len(encoded_layers), 'k should be less than the number of encoder layers'
        encoder_avg = torch.cat(encoded_layers[-self.num_layers:], 2)
        pad_mask = pad_mask.unsqueeze(2)
        encoder_avg = encoder_avg * pad_mask.float()
        pooled_output = torch.sum(encoder_avg, 1) / (torch.sum(pad_mask, 1).float() + self.tol)
        return pooled_output


class AverageKFromLast(nn.Module):

    def __init__(self, k=2, tol=1e-06):
        super().__init__()
        self.k = k
        self.tol = tol

    def forward(self, encoded_layers: List[torch.Tensor], pad_mask: torch.Tensor):
        assert self.k <= len(encoded_layers), 'k should be less than the number of encoder layers'
        encoder_avg = encoded_layers[-self.k]
        pad_mask = pad_mask.unsqueeze(2)
        encoder_avg = encoder_avg * pad_mask.float()
        pooled_output = torch.sum(encoder_avg, 1) / (torch.sum(pad_mask, 1).float() + self.tol)
        return pooled_output


class AverageSumLastK(nn.Module):

    def __init__(self, k=4, tol=1e-06):
        super().__init__()
        self.k = k
        self.tol = tol

    def forward(self, encoded_layers: List[torch.Tensor], pad_mask: torch.Tensor):
        assert self.k <= len(encoded_layers), 'k should be less than the number of encoder layers'
        encoder_avg = torch.stack(encoded_layers[-self.k:]).sum(0)
        pad_mask = pad_mask.unsqueeze(2)
        encoder_avg = encoder_avg * pad_mask.float()
        pooled_output = torch.sum(encoder_avg, 1) / (torch.sum(pad_mask, 1).float() + self.tol)
        return pooled_output


class IdentityPooler(nn.Module):

    def forward(self, x: Any):
        return x


class ClsPooler(nn.Module):

    def __init__(self, dim=1, cls_index=0):
        super().__init__()
        self.dim = dim
        self.cls_index = cls_index

    def forward(self, last_hidden_state: torch.Tensor):
        """Returns the last layer hidden-state of the first token of of the
        sequence, the classification (cls) token.

        Args:
            last_hidden_state (torch.Tensor): Sequence of hidden-state of
            at the output of the last layer of the model (bs, seq length, hidden size)

        Returns:
            [torch.Tensor]: First token of the last hidden-state. (bs, hidden size)
        """
        return last_hidden_state.select(dim=self.dim, index=self.cls_index)


class MeanPooler(nn.Module):

    def __init__(self, dim=1):
        super().__init__()
        self.dim = dim

    def forward(self, last_hidden_state: torch.Tensor):
        """Returns the averaged feature of last layer hidden-state sequence,

        Args:
            last_hidden_state (torch.Tensor): Sequence of hidden-state of
            at the output of the last layer of the model (bs, seq length, hidden size)

        Returns:
            [torch.Tensor]: First token of the last hidden-state. (bs, hidden size)
        """
        return torch.mean(last_hidden_state, dim=self.dim)


class GraphNetwork(nn.Module):

    def __init__(self, config, in_node_dim, num_relations, num_nodes, special_input_node=False, special_input_sz=None):
        super().__init__()
        self.num_relations = num_relations
        self.num_nodes = num_nodes
        self.in_node_dim = in_node_dim
        self.node_hid_dim = config.node_hid_dim
        self.num_gcn_conv = config.num_gcn_conv
        self.use_bn = config.use_batch_norm
        self.use_drop = config.use_dropout
        self.output_type = config.output_type
        self.gcn_type = config.gcn_type
        if self.use_drop:
            self.drop_p = config.dropout_p
        if 'output_dim' in config:
            self.output_dim = config.output_dim
        else:
            self.output_dim = self.node_hid_dim
        self.special_input_node = special_input_node
        self.special_input_sz = special_input_sz
        self.output_special_node = config.output_special_node
        if self.num_gcn_conv >= 1:
            if self.gcn_type == 'RGCN':
                self.conv1 = RGCNConv(self.in_node_dim, self.node_hid_dim, self.num_relations, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv1 = GCNConv(self.in_node_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv1 = SAGEConv(self.in_node_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
        if self.num_gcn_conv >= 2:
            if self.use_bn:
                self.bn1 = BatchNorm(self.node_hid_dim)
            if self.gcn_type == 'RGCN':
                self.conv2 = RGCNConv(self.node_hid_dim, self.node_hid_dim, self.num_relations, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv2 = GCNConv(self.node_hid_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv2 = SAGEConv(self.node_hid_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
        if self.num_gcn_conv >= 3:
            if self.use_bn:
                self.bn2 = BatchNorm(self.node_hid_dim)
            if self.gcn_type == 'RGCN':
                self.conv3 = RGCNConv(self.node_hid_dim, self.node_hid_dim, self.num_relations, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv3 = GCNConv(self.node_hid_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv3 = SAGEConv(self.node_hid_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
        if self.num_gcn_conv >= 4:
            if self.use_bn:
                self.bn3 = BatchNorm(self.node_hid_dim)
            if self.gcn_type == 'RGCN':
                self.conv4 = RGCNConv(self.node_hid_dim, self.node_hid_dim, self.num_relations, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv4 = GCNConv(self.node_hid_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv4 = SAGEConv(self.node_hid_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
        if self.num_gcn_conv >= 5:
            if self.use_bn:
                self.bn4 = BatchNorm(self.node_hid_dim)
            if self.gcn_type == 'RGCN':
                self.conv5 = RGCNConv(self.node_hid_dim, self.node_hid_dim, self.num_relations, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv5 = GCNConv(self.node_hid_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv5 = SAGEConv(self.node_hid_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
        if self.num_gcn_conv >= 6:
            if self.use_bn:
                self.bn5 = BatchNorm(self.node_hid_dim)
            if self.gcn_type == 'RGCN':
                self.conv6 = RGCNConv(self.node_hid_dim, self.node_hid_dim, self.num_relations, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv6 = GCNConv(self.node_hid_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv6 = SAGEConv(self.node_hid_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
        if self.num_gcn_conv >= 7:
            raise Exception('Did not implement %d gcn layers yet' % self.num_gcn_conv)
        if self.output_special_node or self.special_input_node:
            if self.special_input_node:
                self.spec_input_fc = nn.Linear(self.special_input_sz, self.node_hid_dim)
            if self.use_bn:
                self.bn_spec = BatchNorm(self.node_hid_dim)
            if self.gcn_type == 'RGCN':
                self.conv_spec = RGCNConv(self.node_hid_dim, self.node_hid_dim, self.num_relations + 1, num_bases=None)
            elif self.gcn_type == 'GCN':
                self.conv_spec = GCNConv(self.node_hid_dim, self.node_hid_dim)
            elif self.gcn_type == 'SAGE':
                self.conv_spec = SAGEConv(self.node_hid_dim, self.node_hid_dim)
            else:
                raise Exception('GCN type %s not implemented' % self.gcn_type)
            self.edge_index_special = None
            self.edge_type_special = None
            self.special_bs = None
        if self.output_type in ['hidden', 'hidden_subindex', 'hidden_ans']:
            pass
        elif self.output_type in ['graph_level', 'graph_level_ansonly', 'graph_level_inputonly']:
            self.logit_pred = nn.Linear(self.node_hid_dim, 1)
            self.feat_layer = nn.Linear(self.node_hid_dim, self.output_dim)
        elif self.output_type in ['graph_prediction']:
            self.logit_pred = nn.Linear(self.node_hid_dim, 1)
        else:
            raise Exception('Output type %s is not implemented right now' % self.output_type)

    def forward(self, x, edge_index, edge_type=None, batch_size=1, output_nodes=None, special_node_input=None):
        if self.num_nodes is not None:
            assert x.size(0) == self.num_nodes * batch_size
        spec_out = None
        if self.gcn_type == 'RGCN':
            assert edge_type is not None
        elif self.gcn_type in ['GCN', 'SAGE']:
            assert edge_type is None
        else:
            raise Exception('GCN type %s not implemented' % self.gcn_type)
        if edge_type is not None:
            x = self.conv1(x, edge_index, edge_type)
        else:
            x = self.conv1(x, edge_index)
        if self.num_gcn_conv > 1:
            if self.use_bn:
                x = self.bn1(x)
            x = F.relu(x)
            if self.use_drop:
                x = F.dropout(x, p=self.drop_p, training=self.training)
            if edge_type is not None:
                x = self.conv2(x, edge_index, edge_type)
            else:
                x = self.conv2(x, edge_index)
        if self.num_gcn_conv > 2:
            if self.use_bn:
                x = self.bn2(x)
            x = F.relu(x)
            if self.use_drop:
                x = F.dropout(x, p=self.drop_p, training=self.training)
            if edge_type is not None:
                x = self.conv3(x, edge_index, edge_type)
            else:
                x = self.conv3(x, edge_index)
        if self.num_gcn_conv > 3:
            if self.use_bn:
                x = self.bn3(x)
            x = F.relu(x)
            if self.use_drop:
                x = F.dropout(x, p=self.drop_p, training=self.training)
            if edge_type is not None:
                x = self.conv4(x, edge_index, edge_type)
            else:
                x = self.conv4(x, edge_index)
        if self.num_gcn_conv > 4:
            if self.use_bn:
                x = self.bn4(x)
            x = F.relu(x)
            if self.use_drop:
                x = F.dropout(x, p=self.drop_p, training=self.training)
            if edge_type is not None:
                x = self.conv5(x, edge_index, edge_type)
            else:
                x = self.conv5(x, edge_index)
        if self.num_gcn_conv > 5:
            if self.use_bn:
                x = self.bn5(x)
            x = F.relu(x)
            if self.use_drop:
                x = F.dropout(x, p=self.drop_p, training=self.training)
            if edge_type is not None:
                x = self.conv6(x, edge_index, edge_type)
            else:
                x = self.conv6(x, edge_index)
        assert self.num_gcn_conv <= 6
        if self.output_special_node or self.special_input_node:
            if self.special_input_node:
                assert special_node_input is not None
                special_node_input = self.spec_input_fc(special_node_input)
            else:
                special_node_input = torch.zeros(batch_size, self.node_hid_dim)
            if self.edge_index_special is None or self.special_bs != batch_size:
                self.special_bs = batch_size
                spec_edges = []
                for batch_ind in range(batch_size):
                    spec_node_idx = self.num_nodes * batch_size + batch_ind
                    spec_edges += [[node_idx, spec_node_idx] for node_idx in range(self.num_nodes * batch_ind, self.num_nodes * (batch_ind + 1))]
                    spec_edges += [[spec_node_idx, node_idx] for node_idx in range(self.num_nodes * batch_ind, self.num_nodes * (batch_ind + 1))]
                assert len(spec_edges) == self.num_nodes * batch_size * 2
                self.edge_index_special = torch.LongTensor(spec_edges).transpose(0, 1)
                if self.gcn_type == 'RGCN':
                    self.edge_type_special = torch.LongTensor(len(spec_edges)).fill_(self.num_relations)
            if self.use_bn:
                x = self.bn_spec(x)
            x = F.relu(x)
            if self.use_drop:
                x = F.dropout(x, p=self.drop_p, training=self.training)
            edge_index_tmp = torch.cat([edge_index, self.edge_index_special], dim=1)
            x = torch.cat([x, special_node_input], dim=0)
            if edge_type is not None:
                edge_type_tmp = torch.cat([edge_type, self.edge_type_special], dim=0)
                x = self.conv_spec(x, edge_index_tmp, edge_type_tmp)
            else:
                x = self.conv_spec(x, edge_index_tmp)
            if self.num_nodes is not None:
                assert x.size(0) == self.num_nodes * batch_size + batch_size
            if self.output_special_node:
                spec_out = x[self.num_nodes * batch_size:]
                assert spec_out.size(0) == batch_size
            x = x[:self.num_nodes * batch_size]
            assert x.size(0) == self.num_nodes * batch_size
        if self.num_nodes is not None:
            x = x.reshape(batch_size, self.num_nodes, self.node_hid_dim)
        if self.output_type in ['hidden', 'hidden_ans', 'hidden_subindex']:
            pass
        elif self.output_type in ['graph_level', 'graph_level_ansonly', 'graph_level_inputonly']:
            x = F.relu(x)
            assert x.shape[2] == self.node_hid_dim
            if self.output_type in ['graph_level_ansonly', 'graph_level_inputonly']:
                assert output_nodes is not None
                x = x[:, output_nodes, :]
            bs, num_node, _ = x.shape
            x = x.reshape(bs * num_node, self.node_hid_dim)
            feat = self.feat_layer(x)
            feat = feat.reshape(bs, num_node, self.output_dim)
            logit = self.logit_pred(x)
            logit = logit.reshape(bs, num_node)
            logit = F.softmax(logit)
            x = torch.bmm(logit.unsqueeze(1), feat).squeeze()
        elif self.output_type in ['graph_prediction']:
            x = F.relu(x)
            x = self.logit_pred(x)
            x = x.squeeze()
        else:
            raise Exception('output type not known %s' % self.output_type)
        return x, spec_out


def load_str_list(fname):
    with PathManager.open(fname) as f:
        lines = f.readlines()
    lines = [line.strip() for line in lines]
    return lines


SENTENCE_SPLIT_REGEX = re.compile('(\\W+)')


def tokenize(sentence, regex=SENTENCE_SPLIT_REGEX, keep=None, remove=None):
    if keep is None:
        keep = ["'s"]
    if remove is None:
        remove = [',', '?']
    sentence = sentence.lower()
    for token in keep:
        sentence = sentence.replace(token, ' ' + token)
    for token in remove:
        sentence = sentence.replace(token, '')
    tokens = regex.split(sentence)
    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]
    return tokens


class VocabDict:
    UNK_TOKEN = '<unk>'
    PAD_TOKEN = '<pad>'
    START_TOKEN = '<s>'
    END_TOKEN = '</s>'
    PAD_INDEX = 0
    SOS_INDEX = 1
    EOS_INDEX = 2
    UNK_INDEX = 3

    def __init__(self, vocab_file, data_dir=None):
        if not PathManager.exists(vocab_file) and data_dir is not None:
            vocab_file = get_absolute_path(os.path.join(data_dir, vocab_file))
        if not PathManager.exists(vocab_file):
            raise RuntimeError(f"Vocab file {vocab_file} for vocab dict doesn't exist")
        self.word_list = load_str_list(vocab_file)
        self._build()

    def _build(self):
        if self.UNK_TOKEN not in self.word_list:
            self.word_list = [self.UNK_TOKEN] + self.word_list
        self.word2idx_dict = {w: n_w for n_w, w in enumerate(self.word_list)}
        self.stoi = self.word2idx_dict
        self.itos = self.word_list
        self.num_vocab = len(self.word_list)
        self.UNK_INDEX = self.word2idx_dict[self.UNK_TOKEN] if self.UNK_TOKEN in self.word2idx_dict else None
        self.PAD_INDEX = self.word2idx_dict[self.PAD_TOKEN] if self.PAD_TOKEN in self.word2idx_dict else None

    def idx2word(self, n_w):
        return self.word_list[n_w]

    def __len__(self):
        return len(self.word_list)

    def get_size(self):
        return len(self.word_list)

    def get_unk_index(self):
        return self.UNK_INDEX

    def get_unk_token(self):
        return self.UNK_TOKEN

    def word2idx(self, w):
        if w in self.word2idx_dict:
            return self.word2idx_dict[w]
        elif self.UNK_INDEX is not None:
            return self.UNK_INDEX
        else:
            raise ValueError('word %s not in dictionary                              (while dictionary does not contain <unk>)' % w)

    def tokenize_and_index(self, sentence):
        inds = [self.word2idx(w) for w in tokenize(sentence)]
        return inds


def get_edge_idx_type(graph, graph_idx, rel2idx, include_reverse_relations=False):
    edge_index = np.array(list(graph_idx.edges)).T
    edge_type = [graph.edges[e]['relation'] for e in graph.edges]
    edge_type = np.array([rel2idx[rel] for rel in edge_type])
    if include_reverse_relations:
        edge_src = np.expand_dims(edge_index[0, :], 0)
        edge_dest = np.expand_dims(edge_index[1, :], 0)
        edge_reverse = np.concatenate([edge_dest, edge_src], axis=0)
        edge_index = np.concatenate([edge_index, edge_reverse], axis=1)
    return edge_index, edge_type


def make_graph(raw_graph, prune_culdesacs=False, prune_unconnected=True, q_vocab=None, i_vocab=None, ans_vocab=None, include_reverse_relations=False):
    if q_vocab is None:
        q_vocab = []
    q_vocab = set(q_vocab)
    if i_vocab is None:
        i_vocab = []
    i_vocab = set(i_vocab)
    if ans_vocab is None:
        ans_vocab = []
    ans_vocab = set(ans_vocab)
    graph = nx.DiGraph()
    for concept in raw_graph['concepts2idx']:
        q_node = concept in q_vocab
        i_node = concept in i_vocab
        ans_node = concept in ans_vocab
        graph.add_node(concept, q_node=q_node, i_node=i_node, ans_node=ans_node)
    for triplet in raw_graph['triplets']:
        head_idx = triplet[0]
        rel_idx = triplet[1]
        tail_idx = triplet[2]
        head = raw_graph['concepts'][head_idx]
        tail = raw_graph['concepts'][tail_idx]
        rel = raw_graph['relations'][rel_idx]
        assert head in graph.nodes and tail in graph.nodes
        graph.add_edge(head, tail, relation=rel)
    if prune_unconnected:
        for concept in raw_graph['concepts2idx']:
            assert concept in graph.nodes
            connecting_edges = list(graph.in_edges(concept)) + list(graph.out_edges(concept))
            if len(connecting_edges) == 0:
                graph.remove_node(concept)
    to_remove = ['']
    for concept in to_remove:
        if concept in graph.nodes:
            graph.remove_node(concept)
    graph_idx = convert_node_labels_to_integers(graph)
    edge_index, edge_type = get_edge_idx_type(graph, graph_idx, raw_graph['relations2idx'], include_reverse_relations)
    return graph, graph_idx, edge_index, edge_type


def mmf_indirect(path):
    if os.path.exists(path):
        return path
    else:
        path = os.path.join(os.getenv('MMF_DATA_DIR'), 'datasets', path)
        return path


def prepare_embeddings(node_names, embedding_file, add_split):
    """
    This function is used to prepare embeddings for the graph
    :param embedding_file: location of the raw embedding file
    :return:
    """
    None
    embedding_model = ''
    if 'glove' in embedding_file:
        embedding_model = 'glove'
    elif 'GoogleNews' in embedding_file:
        embedding_model = 'word2vec'
    elif 'subword' in embedding_file:
        embedding_model = 'fasttext'
    elif 'numberbatch' in embedding_file:
        embedding_model = 'numberbatch'

    def transform(compound_word):
        return [compound_word, '_'.join([w.lower() for w in compound_word.split(' ')]), '_'.join([w.capitalize() for w in compound_word.split(' ')]), '-'.join([w for w in compound_word.split(' ')]), '-'.join([w for w in compound_word.split(' ')])]
    node2vec = {}
    model = None
    if embedding_model == 'glove':
        tmp_file = '.'.join(embedding_file.split('.')[:-1]) + '_tmp.txt'
        glove2word2vec(embedding_file, tmp_file)
        embedding_file = tmp_file
    None
    model = KeyedVectors.load_word2vec_format(embedding_file, binary=embedding_model == 'word2vec')
    no_match_nodes = []
    match_positions = []
    for node_name in tqdm(node_names, desc='Prepare node embeddings'):
        try_words = []
        try_words.extend(transform(node_name))
        found_mapping = False
        for i, try_word in enumerate(try_words):
            try:
                node2vec[node_name] = model.get_vector(try_word)
                match_positions.append(i + 1)
                found_mapping = True
            except KeyError:
                pass
            if found_mapping:
                break
        if add_split:
            if not found_mapping and len(node_name.split(' ')) > 1:
                sub_word_vecs = []
                for subword in node_name.split(' '):
                    try_words = []
                    try_words.extend(transform(subword))
                    mp = []
                    found_submap = False
                    for i, try_word in enumerate(try_words):
                        try:
                            sub_word_vecs.append(model.get_vector(try_word))
                            mp.append(i + 1)
                            found_submap = True
                        except KeyError:
                            pass
                        if found_submap:
                            break
                if len(sub_word_vecs) == len(node_name.split(' ')):
                    node2vec[node_name] = np.mean(sub_word_vecs, 0)
                    match_positions.append(np.mean(mp))
                    found_mapping = True
        elif not found_mapping and len(node_name.split('_')) > 1:
            sub_word_vecs = []
            for subword in node_name.split('_'):
                try_words = []
                try_words.extend(transform(subword))
                mp = []
                found_submap = False
                for i, try_word in enumerate(try_words):
                    try:
                        sub_word_vecs.append(model.get_vector(try_word))
                        mp.append(i + 1)
                        found_submap = True
                    except KeyError:
                        pass
                    if found_submap:
                        break
            if len(sub_word_vecs) == len(node_name.split('_')):
                node2vec[node_name] = np.mean(sub_word_vecs, 0)
                match_positions.append(np.mean(mp))
                found_mapping = True
        if not found_mapping:
            no_match_nodes.append([node_name, try_words])


class GraphNetworkModule(nn.Module):
    """The generic class for graph networks
    Can be generically added to any other kind of network
    """

    def __init__(self, config, config_extra=None):
        super().__init__()
        self.config = config
        if config_extra is None:
            self.config_extra = {}
        else:
            self.config_extra = config_extra
        raw_graph = torch.load(mmf_indirect(config.kg_path))
        self.graph, self.graph_idx, self.edge_index, self.edge_type = make_graph(raw_graph, config.prune_culdesacs)
        self.num_nodes = len(self.graph.nodes)
        assert len(self.graph_idx.nodes) == self.num_nodes
        self.num_edges = len(self.graph.edges)
        assert len(self.graph_idx.edges) == self.num_edges
        assert self.edge_index.shape[1] == self.num_edges
        assert self.edge_type.shape[0] == self.num_edges
        self.num_relations = len(raw_graph['relations2idx'])
        self.name2node_idx, self.qid2nodeact, self.img_class_sz = self.get_dataset_info(config)
        self.index_in_ans, self.index_in_node, self.graph_answers, self.graph_ans_node_idx = self.get_answer_info(config)
        torch.save(self.graph_answers, mmf_indirect(config.graph_vocab_file))
        node2vec_filename = mmf_indirect(config.node2vec_filename)
        node_names = list(self.name2node_idx.keys())
        valid_node2vec = False
        if os.path.exists(node2vec_filename):
            with open(node2vec_filename, 'rb') as f:
                node2vec, node_names_saved, no_match_nodes = pickle.load(f)
            if set(node_names) == set(node_names_saved):
                valid_node2vec = True
        if not valid_node2vec:
            node2vec, node_names_dbg, no_match_nodes = prepare_embeddings(node_names, mmf_indirect(config.embedding_file), config.add_w2v_multiword)
            None
            pickle.dump((node2vec, node_names_dbg, no_match_nodes), open(node2vec_filename, 'wb'))
        self.w2v_sz = node2vec[list(node2vec.keys())[0]].shape[0]
        self.in_node_dim = 0
        self.q_offest = 0
        self.img_offset = 0
        self.vb_offset = 0
        self.q_enc_offset = 0
        self.w2v_offset = 0
        if 'question' in config.node_inputs:
            self.q_offset = self.in_node_dim
            self.in_node_dim += 1
        if 'classifiers' in config.node_inputs:
            self.img_offset = self.in_node_dim
            self.in_node_dim += self.img_class_sz
        if 'w2v' in config.node_inputs:
            self.w2v_offset = self.in_node_dim
            self.in_node_dim += self.w2v_sz
        self.use_w2v = config.use_w2v
        if self.use_w2v:
            self.base_node_features = torch.zeros(self.num_nodes, self.in_node_dim)
            for node_name in node2vec:
                w2v = torch.from_numpy(node2vec[node_name])
                node_idx = self.name2node_idx[node_name]
                self.base_node_features[node_idx, self.w2v_offset:self.w2v_offset + self.w2v_sz].copy_(w2v)
        else:
            self.in_node_dim -= self.w2v_sz
            self.base_node_features = torch.zeros(self.num_nodes, self.in_node_dim)
        full_node_dim = self.in_node_dim
        special_input_node = False
        special_input_sz = None
        if 'feed_special_node' in self.config_extra and self.config_extra['feed_special_node']:
            assert not self.config_extra['compress_crossmodel']
            special_input_node = True
            special_input_sz = 0
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_logit_to_graph':
                special_input_sz += self.config.num_labels
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_hid_to_graph':
                special_input_sz += self.config_extra['vb_hid_sz']
            if 'feed_q_to_graph' in self.config_extra and self.config_extra['feed_q_to_graph']:
                special_input_sz += self.config_extra['q_hid_sz']
        else:
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_logit_to_graph':
                assert not self.config_extra['compress_crossmodel']
                self.vb_offset = self.in_node_dim
                full_node_dim += 1
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_hid_to_graph':
                self.vb_offset = self.in_node_dim
                if self.config_extra['compress_crossmodel']:
                    full_node_dim += self.config_extra['crossmodel_compress_dim']
                    self.compress_linear = nn.Linear(self.config_extra['vb_hid_sz'], self.config_extra['crossmodel_compress_dim'])
                else:
                    full_node_dim += self.config_extra['vb_hid_sz']
            if 'feed_q_to_graph' in self.config_extra and self.config_extra['feed_q_to_graph']:
                assert not self.config_extra['compress_crossmodel']
                self.q_enc_offset = self.in_node_dim
                full_node_dim += self.config_extra['q_hid_sz']
        self.noback_vb = self.config_extra['noback_vb']
        self.edge_index = torch.from_numpy(self.edge_index)
        self.edge_type = torch.from_numpy(self.edge_type)
        self.node_features_forward = None
        self.edge_index_forward = None
        self.edge_type_forward = None
        self.gn = GraphNetwork(config, full_node_dim, self.num_relations, self.num_nodes, special_input_node=special_input_node, special_input_sz=special_input_sz)
        self.graph_hidden_debug = None

    def get_dataset_info(self, config):
        dataset_data = torch.load(mmf_indirect(config.dataset_info_path))
        qid2qnode = {}
        qid2imginfo = {}
        for dat in dataset_data:
            qid = dat['id']
            q_words = list(dat['symbols_q'])
            qid2qnode[qid] = q_words
            in_data = dat['in_names_confs']
            in_data = [(name, conf, 0) for name, conf in in_data]
            places_data = dat['places_names_confs']
            places_data = [(name, conf, 1) for name, conf in places_data]
            lvis_data = dat['lvis_names_confs']
            lvis_data = [(name, conf, 2) for name, conf in lvis_data]
            vg_data = dat['vg_names_confs']
            vg_data = [(name, conf, 3) for name, conf in vg_data]
            all_image_tuples = in_data + places_data + lvis_data + vg_data
            img_data = {}
            for name, conf, datasetind in all_image_tuples:
                if name in img_data:
                    if conf > img_data[name][datasetind].item():
                        img_data[name][datasetind] = conf
                else:
                    conf_data = torch.zeros(4)
                    conf_data[datasetind] = conf
                    img_data[name] = conf_data
            img_data = [(name, img_data[name]) for name in img_data]
            qid2imginfo[qid] = img_data
        name2node_idx = {}
        idx = 0
        for nodename in self.graph.nodes:
            name2node_idx[nodename] = idx
            idx += 1
        qid2nodeact = {}
        img_class_sz = None
        for qid in qid2qnode:
            q_words = qid2qnode[qid]
            img_info = qid2imginfo[qid]
            img_words = [x[0] for x in img_info]
            img_confs = [x[1] for x in img_info]
            if img_class_sz is None:
                assert type(img_confs[0]) is torch.Tensor
                img_class_sz = img_confs[0].size(0)
            node_info = {}
            for word in q_words:
                if word not in name2node_idx:
                    continue
                node_idx = name2node_idx[word]
                val = torch.zeros(img_class_sz + 1)
                val[0] = 1
                node_info[node_idx] = val
            for word, img_confs_w in zip(img_words, img_confs):
                if word not in name2node_idx:
                    continue
                node_idx = name2node_idx[word]
                if node_idx in node_info:
                    node_info[node_idx][1:].copy_(img_confs_w)
                else:
                    val = torch.zeros(img_class_sz + 1)
                    val[1:].copy_(img_confs_w)
                    node_info[node_idx] = val
            qid2nodeact[qid] = node_info
        num_acts_per_qid = np.mean([len(qid2nodeact[qid].keys()) for qid in qid2nodeact])
        None
        return name2node_idx, qid2nodeact, img_class_sz

    def get_answer_info(self, config):
        answer_vocab = VocabDict(mmf_indirect(config.vocab_file))
        assert len(answer_vocab) == config.num_labels
        if config.okvqa_v_mode in ['v1.0', 'v1.0-121', 'v1.0-121-mc']:
            tx_data = torch.load(mmf_indirect(config.ans_translation_file))
            if config.okvqa_v_mode in ['v1.0-121', 'v1.0-121-mc']:
                old_graph_vocab = torch.load(mmf_indirect(config.old_graph_vocab_file))
            index_in_ans = []
            index_in_node = []
            graph_answers = []
            nomatch = []
            for ans_str in answer_vocab.word2idx_dict:
                if config.okvqa_v_mode == 'v1.0':
                    if ans_str not in tx_data['v10_2_v11_mc']:
                        nomatch.append(ans_str)
                        continue
                    if tx_data['v10_2_v11_mc'][ans_str] in self.name2node_idx:
                        raw_ans = tx_data['v10_2_v11_mc'][ans_str]
                    else:
                        v11_counts = tx_data['v10_2_v11_count'][ans_str]
                        sorted_counts = sorted(v11_counts.items(), key=lambda x: x[1], reverse=True)
                        raw_ans = None
                        for k, _ in sorted_counts:
                            if k in self.name2node_idx:
                                raw_ans = k
                                break
                        if raw_ans is None:
                            nomatch.append(ans_str)
                            continue
                    graph_answers.append(ans_str)
                    node_idx = self.name2node_idx[raw_ans]
                    index_in_node.append(node_idx)
                    ans_idx = answer_vocab.word2idx(ans_str)
                    index_in_ans.append(ans_idx)
                else:
                    if ans_str not in tx_data['v10_2_v11_mc']:
                        nomatch.append(ans_str)
                        continue
                    if config.okvqa_v_mode == 'v1.0-121-mc':
                        if tx_data['v10_2_raw_mc'][ans_str] in self.name2node_idx:
                            raw_ans = tx_data['v10_2_raw_mc'][ans_str]
                        else:
                            v11_counts = tx_data['v10_2_raw_count'][ans_str]
                            sorted_counts = sorted(v11_counts.items(), key=lambda x: x[1], reverse=True)
                            raw_ans = None
                            for k, _ in sorted_counts:
                                if k in self.name2node_idx:
                                    raw_ans = k
                                    break
                            if raw_ans is None:
                                nomatch.append(ans_str)
                                continue
                    elif tx_data['v10_2_v11_mc'][ans_str] in self.name2node_idx and tx_data['v10_2_v11_mc'][ans_str] in old_graph_vocab:
                        raw_ans = tx_data['v10_2_v11_mc'][ans_str]
                    else:
                        v11_counts = tx_data['v10_2_v11_count'][ans_str]
                        sorted_counts = sorted(v11_counts.items(), key=lambda x: x[1], reverse=True)
                        raw_ans = None
                        for k, _ in sorted_counts:
                            if k in self.name2node_idx and k in old_graph_vocab:
                                raw_ans = k
                                break
                        if raw_ans is None:
                            nomatch.append(ans_str)
                            continue
                    if self.name2node_idx[raw_ans] in index_in_node:
                        if config.okvqa_v_mode == 'v1.0-121-mc':
                            assert len(index_in_node) == len(graph_answers)
                            assert len(index_in_ans) == len(graph_answers)
                            idx = index_in_node.index(self.name2node_idx[raw_ans])
                            node_idx = index_in_node[idx]
                            old_ans_str = graph_answers[idx]
                            raw_counts = tx_data['v11_2_raw_count'][raw_ans]
                            assert ans_str in raw_counts and old_ans_str in raw_counts
                            assert ans_str != old_ans_str
                            if raw_counts[ans_str] > raw_counts[old_ans_str]:
                                assert node_idx == self.name2node_idx[raw_ans]
                                graph_answers[idx] = ans_str
                                ans_idx = answer_vocab.word2idx(ans_str)
                                index_in_ans[idx] = ans_idx
                            else:
                                continue
                        else:
                            nomatch.append(ans_str)
                            continue
                    else:
                        graph_answers.append(ans_str)
                        node_idx = self.name2node_idx[raw_ans]
                        index_in_node.append(node_idx)
                        ans_idx = answer_vocab.word2idx(ans_str)
                        index_in_ans.append(ans_idx)
            None
            graph_answers = sorted(graph_answers)
            graph_ans_node_idx = []
            for ans_str in graph_answers:
                node_idx = self.name2node_idx[raw_ans]
                graph_ans_node_idx.append(node_idx)
        else:
            assert config.okvqa_v_mode == 'v1.1'
            index_in_ans = []
            index_in_node = []
            graph_answers = []
            for ans_str in answer_vocab.word2idx_dict:
                if ans_str not in self.name2node_idx:
                    continue
                graph_answers.append(ans_str)
                node_idx = self.name2node_idx[ans_str]
                index_in_node.append(node_idx)
                ans_idx = answer_vocab.word2idx(ans_str)
                index_in_ans.append(ans_idx)
            graph_answers = sorted(graph_answers)
            graph_ans_node_idx = []
            for ans_str in graph_answers:
                node_idx = self.name2node_idx[ans_str]
                graph_ans_node_idx.append(node_idx)
        assert len(index_in_ans) == len(index_in_node)
        assert len(index_in_ans) == len(set(index_in_ans))
        if config.okvqa_v_mode != 'v1.0':
            assert len(index_in_node) == len(set(index_in_node))
        assert len(graph_answers) == len(graph_ans_node_idx)
        num_ans_in_graph = len(index_in_ans)
        None
        index_in_ans = torch.LongTensor(index_in_ans)
        index_in_node = torch.LongTensor(index_in_node)
        graph_ans_node_idx = torch.LongTensor(graph_ans_node_idx)
        return index_in_ans, index_in_node, graph_answers, graph_ans_node_idx

    def forward(self, sample_list):
        qids = sample_list['id']
        batch_size = qids.size(0)
        device = qids.device
        if self.node_features_forward is None or batch_size * self.num_nodes != self.node_features_forward.size(0):
            self.node_features_forward = torch.zeros(self.num_nodes * batch_size, self.in_node_dim)
            _, num_edges = self.edge_index.size()
            self.edge_index_forward = torch.LongTensor(2, num_edges * batch_size).fill_(0)
            if self.gn.gcn_type == 'RGCN':
                self.edge_type_forward = torch.LongTensor(num_edges * batch_size).fill_(0)
            for batch_ind in range(batch_size):
                self.node_features_forward[self.num_nodes * batch_ind:self.num_nodes * (batch_ind + 1), :].copy_(self.base_node_features)
                self.edge_index_forward[:, batch_ind * num_edges:(batch_ind + 1) * num_edges].copy_(self.edge_index)
                self.edge_index_forward[:, batch_ind * num_edges:(batch_ind + 1) * num_edges].add_(batch_ind * self.num_nodes)
                if self.gn.gcn_type == 'RGCN':
                    self.edge_type_forward[batch_ind * num_edges:(batch_ind + 1) * num_edges].copy_(self.edge_type)
        assert self.w2v_offset is not None and self.q_offset is not None and self.img_offset is not None
        assert self.w2v_offset > 0
        self.node_features_forward[:, :self.w2v_offset].zero_()
        if not self.config.use_conf:
            pass
        elif not self.config.use_q:
            assert self.config.use_img
            all_node_idx = []
            for batch_ind, qid in enumerate(qids):
                node_info = self.qid2nodeact[qid.item()]
                for node_idx in node_info:
                    node_val = node_info[node_idx]
                    node_val[0] = 0
                    self.node_features_forward[self.num_nodes * batch_ind + node_idx, :self.img_offset + self.img_class_sz].copy_(node_val)
                    all_node_idx.append(node_idx)
        elif not self.config.use_img:
            all_node_idx = []
            for batch_ind, qid in enumerate(qids):
                node_info = self.qid2nodeact[qid.item()]
                for node_idx in node_info:
                    node_val = node_info[node_idx]
                    node_val[1] = 0
                    node_val[2] = 0
                    node_val[3] = 0
                    node_val[4] = 0
                    self.node_features_forward[self.num_nodes * batch_ind + node_idx, :self.img_offset + self.img_class_sz].copy_(node_val)
                    all_node_idx.append(node_idx)
        elif self.config.use_partial_img:
            assert self.config.partial_img_idx in [0, 1, 2, 3]
            all_node_idx = []
            for batch_ind, qid in enumerate(qids):
                node_info = self.qid2nodeact[qid.item()]
                for node_idx in node_info:
                    node_val = node_info[node_idx]
                    db_count = 0
                    if self.config.partial_img_idx != 0:
                        node_val[1] = 0
                        db_count += 1
                    if self.config.partial_img_idx != 1:
                        node_val[2] = 0
                        db_count += 1
                    if self.config.partial_img_idx != 2:
                        node_val[3] = 0
                        db_count += 1
                    if self.config.partial_img_idx != 3:
                        node_val[4] = 0
                        db_count += 1
                    assert db_count == 3
                    self.node_features_forward[self.num_nodes * batch_ind + node_idx, :self.img_offset + self.img_class_sz].copy_(node_val)
                    all_node_idx.append(node_idx)
        else:
            all_node_idx = []
            for batch_ind, qid in enumerate(qids):
                node_info = self.qid2nodeact[qid.item()]
                for node_idx in node_info:
                    node_val = node_info[node_idx]
                    self.node_features_forward[self.num_nodes * batch_ind + node_idx, :self.img_offset + self.img_class_sz].copy_(node_val)
                    all_node_idx.append(node_idx)
        if self.gn.output_type == 'graph_level_ansonly':
            output_nodes = self.index_in_node
        elif self.gn.output_type == 'graph_level_inputonly':
            output_nodes = torch.LongTensor(all_node_idx)
        else:
            output_nodes = None
        if 'feed_special_node' in self.config_extra and self.config_extra['feed_special_node']:
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_logit_to_graph':
                if self.noback_vb:
                    vb_logits = sample_list['vb_logits'].detach()
                else:
                    vb_logits = sample_list['vb_logits']
                special_node_input = torch.sigmoid(vb_logits)
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_hid_to_graph':
                if self.noback_vb:
                    special_node_input = sample_list['vb_hidden'].detach()
                else:
                    special_node_input = sample_list['vb_hidden']
            if 'feed_q_to_graph' in self.config_extra and self.config_extra['feed_q_to_graph']:
                special_node_input = sample_list['q_encoded']
            if self.gn.gcn_type == 'RGCN':
                output, spec_out = self.gn(self.node_features_forward, self.edge_index_forward, self.edge_type_forward, batch_size=batch_size, output_nodes=output_nodes, special_node_input=special_node_input)
            elif self.gn.gcn_type in ['GCN', 'SAGE']:
                output, spec_out = self.gn(self.node_features_forward, self.edge_index_forward, batch_size=batch_size, output_nodes=output_nodes, special_node_input=special_node_input)
        else:
            node_feats_tmp = self.node_features_forward
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_logit_to_graph':
                assert not self.config_extra['compress_crossmodel']
                node_feats_tmp = node_feats_tmp.reshape((batch_size, self.num_nodes, -1))
                if self.noback_vb:
                    vb_logits = sample_list['vb_logits'].detach()
                else:
                    vb_logits = sample_list['vb_logits']
                vb_confs = torch.sigmoid(vb_logits)
                vb_confs_graphindexed = torch.zeros(batch_size, self.num_nodes)
                vb_confs_graphindexed[:, self.index_in_node] = vb_confs[:, self.index_in_ans]
                node_feats_tmp = torch.cat([node_feats_tmp, vb_confs_graphindexed.unsqueeze(2)], dim=2)
                node_feats_tmp = node_feats_tmp.reshape((batch_size * self.num_nodes, -1))
            if 'feed_vb_to_graph' in self.config_extra and self.config_extra['feed_vb_to_graph'] and self.config_extra['feed_mode'] == 'feed_vb_hid_to_graph':
                node_feats_tmp = node_feats_tmp.reshape((batch_size, self.num_nodes, -1))
                if self.noback_vb:
                    vb_hid = sample_list['vb_hidden'].detach()
                else:
                    vb_hid = sample_list['vb_hidden']
                if self.config_extra['compress_crossmodel']:
                    vb_hid = F.relu(self.compress_linear(vb_hid))
                node_feats_tmp = torch.cat([node_feats_tmp, vb_hid.unsqueeze(1).repeat((1, self.num_nodes, 1))], dim=2)
                node_feats_tmp = node_feats_tmp.reshape((batch_size * self.num_nodes, -1))
            if 'feed_q_to_graph' in self.config_extra and self.config_extra['feed_q_to_graph']:
                assert not self.config_extra['compress_crossmodel']
                node_feats_tmp = node_feats_tmp.reshape((batch_size, self.num_nodes, -1))
                node_feats_tmp = torch.cat([node_feats_tmp, sample_list['q_encoded'].unsqueeze(1).repeat((1, self.num_nodes, 1))], dim=2)
                node_feats_tmp = node_feats_tmp.reshape((batch_size * self.num_nodes, -1))
            if self.gn.gcn_type == 'RGCN':
                output, spec_out = self.gn(node_feats_tmp, self.edge_index_forward, self.edge_type_forward, batch_size=batch_size, output_nodes=output_nodes)
            elif self.gn.gcn_type in ['GCN', 'SAGE']:
                output, spec_out = self.gn(node_feats_tmp, self.edge_index_forward, batch_size=batch_size, output_nodes=output_nodes)
        if self.config.output_type == 'hidden_ans':
            assert output.size(1) == self.num_nodes
            assert output.size(2) == self.config.node_hid_dim
            assert output.dim() == 3
            if self.config_extra['analysis_mode']:
                self.graph_hidden_debug = output
            if self.config.output_order == 'alpha':
                output = output[:, self.graph_ans_node_idx, :]
                assert output.size(1) == len(self.graph_answers)
            else:
                assert self.config.output_order == 'ans'
                outputs_tmp = torch.zeros(batch_size, self.config.num_labels, self.config.node_hid_dim)
                outputs_tmp[:, self.index_in_ans, :] = output[:, self.index_in_node, :]
                output = outputs_tmp
        elif self.config.output_type in ['graph_level', 'graph_level_ansonly', 'graph_level_inputonly']:
            pass
        else:
            assert self.config.output_type == 'graph_prediction'
            assert output.size(1) == self.num_nodes
            assert output.dim() == 2
            if self.config.output_order == 'alpha':
                output = output[:, self.graph_ans_node_idx]
                assert output.size(1) == len(self.graph_answers)
            else:
                assert self.config.output_order == 'ans'
                logits = torch.zeros(batch_size, self.config.num_labels).fill_(-1000.0)
                logits[:, self.index_in_ans] = output[:, self.index_in_node]
                output = logits
        if spec_out is not None:
            sample_list['graph_special_node_out'] = spec_out
        return output

    def add_analysis_to_output(self, output):
        output['graph'] = self.graph
        output['graph_idx'] = self.graph_idx
        output['name2node_idx'] = self.name2node_idx
        output['node_acts'] = self.qid2nodeact
        output['index_in_ans'] = self.index_in_ans
        output['index_in_node'] = self.index_in_node
        output['graph_answers'] = self.graph_answers
        output['graph_ans_node_idx'] = self.graph_ans_node_idx
        output['graph_hidden_act'] = self.graph_hidden_debug.cpu()
        return output


class TestMSEAndMAELoss(nn.Module):
    """Mean squared, absolute error loss.
    Calculates both losses and returns a dict with string keys.
    """

    def __init__(self):
        super().__init__()

    def forward(self, sample_list, model_output):
        targets = sample_list['targets']
        scores = model_output['scores']
        loss = {'mse': F.mse_loss(scores, targets), 'mae': F.l1_loss(scores, targets)}
        return loss


class OnlyBase(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.base_test = torch.nn.Sequential(torch.nn.Linear(5, 4), torch.nn.Tanh(), torch.nn.Linear(4, 5))

    def format_state_key(self, key):
        return key


class TestDecoderModel(nn.Module):

    def __init__(self, config, vocab):
        super().__init__()
        self.config = config
        self.vocab = vocab

    def build(self):
        return

    def init_hidden_state(self, features):
        h = features.new_zeros((features.size(0), self.config.classifier.params.hidden_dim), dtype=torch.float)
        c = features.new_zeros((features.size(0), self.config.classifier.params.hidden_dim), dtype=torch.float)
        return h, c

    def get_data_t(self, data, batch_size_t):
        data['texts'] = data['texts'][:batch_size_t]
        if 'state' in data:
            h1 = data['state']['td_hidden'][0][:batch_size_t]
            c1 = data['state']['td_hidden'][1][:batch_size_t]
            h2 = data['state']['lm_hidden'][0][:batch_size_t]
            c2 = data['state']['lm_hidden'][1][:batch_size_t]
        else:
            h1, c1 = self.init_hidden_state(data['texts'])
            h2, c2 = self.init_hidden_state(data['texts'])
        data['state'] = {'td_hidden': (h1, c1), 'lm_hidden': (h2, c2)}
        registry.register(f'{h1.device}_lstm_state', data['state'])
        return data, batch_size_t

    def forward(self, sample_list):
        scores = torch.rand(sample_list.get_batch_size(), 3127)
        decoder = registry.get_decoder_class(self.config.inference.type)(self.vocab, self.config)
        sample_list.add_field('targets', sample_list.answers[:, 0, 1:])
        sample_list = decoder.init_batch(sample_list)
        batch_size = sample_list.image_feature_0.size(0)
        data = {}
        data['texts'] = sample_list.answers.new_full((batch_size, 1), self.vocab.SOS_INDEX, dtype=torch.long)
        timesteps = 10
        output = None
        batch_size_t = batch_size
        for t in range(timesteps):
            data, batch_size_t = self.get_data_t(data, batch_size_t)
            output = torch.randn(batch_size_t, self.vocab.get_size())
            if t == timesteps - 1:
                output = torch.ones(batch_size_t, self.vocab.get_size()) * -30.0
                output[0, self.vocab.EOS_INDEX] = 10
            finish, data, batch_size_t = decoder.decode(t, data, output)
            if finish:
                break
        model_output = {'scores': scores}
        model_output['captions'] = decoder.get_result()
        return model_output


RESNET152_MODEL = models.resnet152(pretrained=True)


class ResNet152FeatModule(nn.Module):

    def __init__(self):
        super().__init__()
        modules = list(RESNET152_MODEL.children())[:-2]
        self.feature_module = nn.Sequential(*modules)

    def forward(self, x):
        return self.feature_module(x)


class LastLevelMaxPool(nn.Module):
    """
    This module is used in the original FPN to generate a downsampled P6
    feature from P5.
    """

    def __init__(self):
        super().__init__()
        self.num_levels = 1
        self.in_feature = 'p5'

    def forward(self, x):
        return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]


class LastLevelP6P7(nn.Module):
    """
    This module is used in RetinaNet to generate extra layers, P6 and P7
    from C5 feature.
    """

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.num_levels = 2
        self.in_feature = 'res5'
        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)
        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)

    def forward(self, c5):
        p6 = self.p6(c5)
        p7 = self.p7(F.relu(p6))
        return [p6, p7]


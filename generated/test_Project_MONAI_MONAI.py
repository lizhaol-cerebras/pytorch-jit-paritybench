import sys
_module = sys.modules[__name__]
del sys
conf = _module
monai = _module
_extensions = _module
loader = _module
_version = _module
apps = _module
auto3dseg = _module
auto_runner = _module
bundle_gen = _module
data_analyzer = _module
ensemble_builder = _module
hpo_gen = _module
transforms = _module
utils = _module
datasets = _module
deepedit = _module
interaction = _module
transforms = _module
deepgrow = _module
dataset = _module
interaction = _module
transforms = _module
detection = _module
metrics = _module
coco = _module
matching = _module
networks = _module
retinanet_detector = _module
retinanet_network = _module
array = _module
box_ops = _module
dictionary = _module
ATSS_matcher = _module
anchor_utils = _module
box_coder = _module
box_selector = _module
detector_utils = _module
hard_negative_sampler = _module
predict_utils = _module
mmars = _module
mmars = _module
model_desc = _module
nuclick = _module
transforms = _module
pathology = _module
data = _module
engines = _module
utils = _module
handlers = _module
prob_map_producer = _module
inferers = _module
inferer = _module
losses = _module
hovernet_loss = _module
lesion_froc = _module
post = _module
array = _module
spatial = _module
array = _module
dictionary = _module
stain = _module
utils = _module
reconstruction = _module
complex_utils = _module
fastmri_reader = _module
mri_utils = _module
blocks = _module
varnetblock = _module
nets = _module
coil_sensitivity_model = _module
complex_unet = _module
utils = _module
varnet = _module
array = _module
dictionary = _module
tcia = _module
label_desc = _module
algo_gen = _module
analyzer = _module
operations = _module
seg_summarizer = _module
utils = _module
bundle = _module
config_item = _module
config_parser = _module
reference_resolver = _module
scripts = _module
utils = _module
config = _module
deviceconfig = _module
type_definitions = _module
box_utils = _module
csv_saver = _module
dataloader = _module
dataset = _module
dataset_summary = _module
decathlon_datalist = _module
fft_utils = _module
folder_layout = _module
grid_dataset = _module
image_dataset = _module
image_reader = _module
image_writer = _module
iterable_dataset = _module
meta_obj = _module
meta_tensor = _module
nifti_saver = _module
nifti_writer = _module
png_saver = _module
png_writer = _module
samplers = _module
synthetic = _module
test_time_augmentation = _module
thread_buffer = _module
torchscript_utils = _module
utils = _module
video_dataset = _module
wsi_datasets = _module
wsi_reader = _module
evaluator = _module
multi_gpu_supervised_trainer = _module
trainer = _module
utils = _module
workflow = _module
fl = _module
client = _module
client_algo = _module
monai_algo = _module
constants = _module
exchange_object = _module
filters = _module
checkpoint_loader = _module
checkpoint_saver = _module
classification_saver = _module
confusion_matrix = _module
decollate_batch = _module
earlystop_handler = _module
garbage_collector = _module
hausdorff_distance = _module
ignite_metric = _module
logfile_handler = _module
lr_schedule_handler = _module
mean_dice = _module
mean_iou = _module
metric_logger = _module
metrics_saver = _module
mlflow_handler = _module
nvtx_handlers = _module
panoptic_quality = _module
parameter_scheduler = _module
postprocessing = _module
probability_maps = _module
regression_metrics = _module
roc_auc = _module
smartcache_handler = _module
stats_handler = _module
surface_distance = _module
tensorboard_handlers = _module
utils = _module
validation_handler = _module
inferer = _module
utils = _module
contrastive = _module
deform = _module
dice = _module
ds_loss = _module
focal_loss = _module
giou_loss = _module
image_dissimilarity = _module
multi_scale = _module
spatial_mask = _module
ssim_loss = _module
tversky = _module
unified_focal_loss = _module
active_learning_metrics = _module
confusion_matrix = _module
cumulative_average = _module
f_beta_score = _module
froc = _module
generalized_dice = _module
hausdorff_distance = _module
loss_metric = _module
meandice = _module
meaniou = _module
metric = _module
panoptic_quality = _module
regression = _module
rocauc = _module
surface_dice = _module
surface_distance = _module
utils = _module
acti_norm = _module
activation = _module
aspp = _module
backbone_fpn_utils = _module
convolutions = _module
crf = _module
denseblock = _module
dints_block = _module
downsample = _module
dynunet_block = _module
encoder = _module
fcn = _module
feature_pyramid_network = _module
fft_utils_t = _module
localnet_block = _module
mlp = _module
patchembedding = _module
regunet_block = _module
segresnet_block = _module
selfattention = _module
squeeze_and_excitation = _module
transformerblock = _module
unetr_block = _module
upsample = _module
warp = _module
layers = _module
convutils = _module
drop_path = _module
factories = _module
filtering = _module
gmm = _module
simplelayers = _module
spatial_transforms = _module
utils = _module
weight_init = _module
ahnet = _module
attentionunet = _module
autoencoder = _module
basic_unet = _module
basic_unetplusplus = _module
classifier = _module
densenet = _module
dints = _module
dynunet = _module
efficientnet = _module
flexible_unet = _module
fullyconnectednet = _module
generator = _module
highresnet = _module
hovernet = _module
milmodel = _module
netadapter = _module
regressor = _module
regunet = _module
resnet = _module
segresnet = _module
segresnet_ds = _module
senet = _module
swin_unetr = _module
torchvision_fc = _module
transchex = _module
unet = _module
unetr = _module
varautoencoder = _module
vit = _module
vitautoenc = _module
vnet = _module
utils = _module
optimizers = _module
lr_finder = _module
lr_scheduler = _module
novograd = _module
utils = _module
adaptors = _module
compose = _module
croppad = _module
array = _module
batch = _module
dictionary = _module
intensity = _module
array = _module
dictionary = _module
inverse = _module
inverse_batch_transform = _module
io = _module
array = _module
lazy = _module
functional = _module
utils = _module
meta_utility = _module
dictionary = _module
nvtx = _module
array = _module
dictionary = _module
signal = _module
array = _module
smooth_field = _module
array = _module
dictionary = _module
array = _module
dictionary = _module
transform = _module
utility = _module
array = _module
dictionary = _module
utils = _module
utils_create_transform_ims = _module
utils_pytorch_numpy_unification = _module
aliases = _module
decorators = _module
deprecate_utils = _module
dist = _module
enums = _module
jupyter_utils = _module
misc = _module
module = _module
nvtx = _module
profiling = _module
state_cacher = _module
type_conversion = _module
visualize = _module
class_activation_maps = _module
gradient_based = _module
img2tensorboard = _module
occlusion_sensitivity = _module
utils = _module
visualizer = _module
setup = _module
tests = _module
clang_format_utils = _module
croppers = _module
hvd_evenly_divisible_all_gather = _module
min_tests = _module
ngc_bundle_download = _module
padders = _module
cprofile_profiling = _module
min_classes = _module
profiling = _module
pyspy_profiling = _module
runner = _module
test_acn_block = _module
test_activations = _module
test_activationsd = _module
test_adaptors = _module
test_add_channeld = _module
test_add_coordinate_channels = _module
test_add_coordinate_channelsd = _module
test_add_extreme_points_channel = _module
test_add_extreme_points_channeld = _module
test_adjust_contrast = _module
test_adjust_contrastd = _module
test_adn = _module
test_affine = _module
test_affine_grid = _module
test_affine_transform = _module
test_affined = _module
test_ahnet = _module
test_alias = _module
test_anchor_box = _module
test_apply = _module
test_apply_filter = _module
test_arraydataset = _module
test_as_channel_first = _module
test_as_channel_firstd = _module
test_as_channel_last = _module
test_as_channel_lastd = _module
test_as_discrete = _module
test_as_discreted = _module
test_atss_box_matcher = _module
test_attentionunet = _module
test_auto3dseg = _module
test_auto3dseg_ensemble = _module
test_auto3dseg_hpo = _module
test_autoencoder = _module
test_basic_unet = _module
test_basic_unetplusplus = _module
test_bending_energy = _module
test_bilateral_approx_cpu = _module
test_bilateral_approx_cuda = _module
test_bilateral_precise = _module
test_blend_images = _module
test_border_pad = _module
test_border_padd = _module
test_bounding_rect = _module
test_bounding_rectd = _module
test_box_coder = _module
test_box_transform = _module
test_box_utils = _module
test_bundle_ckpt_export = _module
test_bundle_download = _module
test_bundle_get_data = _module
test_bundle_init_bundle = _module
test_bundle_utils = _module
test_bundle_verify_metadata = _module
test_bundle_verify_net = _module
test_cachedataset = _module
test_cachedataset_parallel = _module
test_cachedataset_persistent_workers = _module
test_cachentransdataset = _module
test_call_dist = _module
test_cast_to_type = _module
test_cast_to_typed = _module
test_center_scale_crop = _module
test_center_scale_cropd = _module
test_center_spatial_crop = _module
test_center_spatial_cropd = _module
test_channel_pad = _module
test_check_hash = _module
test_check_missing_files = _module
test_classes_to_indices = _module
test_classes_to_indicesd = _module
test_complex_utils = _module
test_component_locator = _module
test_compose = _module
test_compose_get_number_conversions = _module
test_compute_confusion_matrix = _module
test_compute_f_beta = _module
test_compute_froc = _module
test_compute_generalized_dice = _module
test_compute_ho_ver_maps = _module
test_compute_ho_ver_maps_d = _module
test_compute_meandice = _module
test_compute_meaniou = _module
test_compute_panoptic_quality = _module
test_compute_regression_metrics = _module
test_compute_roc_auc = _module
test_compute_variance = _module
test_concat_itemsd = _module
test_config_item = _module
test_config_parser = _module
test_contrastive_loss = _module
test_convert_data_type = _module
test_convert_to_multi_channel = _module
test_convert_to_multi_channeld = _module
test_convert_to_torchscript = _module
test_convolutions = _module
test_copy_itemsd = _module
test_copy_model_state = _module
test_correct_crop_centers = _module
test_create_cross_validation_datalist = _module
test_create_grid_and_affine = _module
test_crf_cpu = _module
test_crf_cuda = _module
test_crop_foreground = _module
test_crop_foregroundd = _module
test_cross_validation = _module
test_csv_dataset = _module
test_csv_iterable_dataset = _module
test_csv_saver = _module
test_cucim_dict_transform = _module
test_cucim_transform = _module
test_cumulative = _module
test_cumulative_average = _module
test_cumulative_average_dist = _module
test_cv2_dist = _module
test_data_stats = _module
test_data_statsd = _module
test_dataloader = _module
test_dataset = _module
test_dataset_func = _module
test_dataset_summary = _module
test_decathlondataset = _module
test_decollate = _module
test_deepedit_interaction = _module
test_deepedit_transforms = _module
test_deepgrow_dataset = _module
test_deepgrow_interaction = _module
test_deepgrow_transforms = _module
test_delete_itemsd = _module
test_denseblock = _module
test_densenet = _module
test_deprecated = _module
test_detect_envelope = _module
test_detection_coco_metrics = _module
test_detector_boxselector = _module
test_detector_utils = _module
test_dev_collate = _module
test_dice_ce_loss = _module
test_dice_focal_loss = _module
test_dice_loss = _module
test_dints_cell = _module
test_dints_mixop = _module
test_dints_network = _module
test_discriminator = _module
test_divisible_pad = _module
test_divisible_padd = _module
test_download_and_extract = _module
test_downsample_block = _module
test_drop_path = _module
test_ds_loss = _module
test_dvf2ddf = _module
test_dynunet = _module
test_dynunet_block = _module
test_efficientnet = _module
test_ensemble_evaluator = _module
test_ensure_channel_first = _module
test_ensure_channel_firstd = _module
test_ensure_tuple = _module
test_ensure_type = _module
test_ensure_typed = _module
test_enum_bound_interp = _module
test_eval_mode = _module
test_evenly_divisible_all_gather_dist = _module
test_factorized_increase = _module
test_factorized_reduce = _module
test_fastmri_reader = _module
test_fft_utils = _module
test_fg_bg_to_indices = _module
test_fg_bg_to_indicesd = _module
test_file_basename = _module
test_fill_holes = _module
test_fill_holesd = _module
test_fl_exchange_object = _module
test_fl_monai_algo = _module
test_fl_monai_algo_dist = _module
test_fl_monai_algo_stats = _module
test_flatten_sub_keysd = _module
test_flexible_unet = _module
test_flip = _module
test_flipd = _module
test_focal_loss = _module
test_folder_layout = _module
test_foreground_mask = _module
test_foreground_maskd = _module
test_fourier = _module
test_fpn_block = _module
test_from_engine_hovernet = _module
test_fullyconnectednet = _module
test_gaussian = _module
test_gaussian_filter = _module
test_gaussian_sharpen = _module
test_gaussian_sharpend = _module
test_gaussian_smooth = _module
test_gaussian_smoothd = _module
test_generalized_dice_focal_loss = _module
test_generalized_dice_loss = _module
test_generalized_wasserstein_dice_loss = _module
test_generate_distance_map = _module
test_generate_distance_mapd = _module
test_generate_instance_border = _module
test_generate_instance_borderd = _module
test_generate_instance_centroid = _module
test_generate_instance_centroidd = _module
test_generate_instance_contour = _module
test_generate_instance_contourd = _module
test_generate_instance_type = _module
test_generate_instance_typed = _module
test_generate_label_classes_crop_centers = _module
test_generate_param_groups = _module
test_generate_pos_neg_label_crop_centers = _module
test_generate_spatial_bounding_box = _module
test_generate_succinct_contour = _module
test_generate_succinct_contourd = _module
test_generate_watershed_markers = _module
test_generate_watershed_markersd = _module
test_generate_watershed_mask = _module
test_generate_watershed_maskd = _module
test_generator = _module
test_get_equivalent_dtype = _module
test_get_extreme_points = _module
test_get_layers = _module
test_get_package_version = _module
test_get_unique_labels = _module
test_gibbs_noise = _module
test_gibbs_noised = _module
test_giou_loss = _module
test_global_mutual_information_loss = _module
test_globalnet = _module
test_gmm = _module
test_grid_dataset = _module
test_grid_distortion = _module
test_grid_distortiond = _module
test_grid_patch = _module
test_grid_patchd = _module
test_grid_pull = _module
test_grid_split = _module
test_grid_splitd = _module
test_handler_checkpoint_loader = _module
test_handler_checkpoint_saver = _module
test_handler_classification_saver = _module
test_handler_classification_saver_dist = _module
test_handler_confusion_matrix = _module
test_handler_confusion_matrix_dist = _module
test_handler_decollate_batch = _module
test_handler_early_stop = _module
test_handler_garbage_collector = _module
test_handler_hausdorff_distance = _module
test_handler_logfile = _module
test_handler_lr_scheduler = _module
test_handler_mean_dice = _module
test_handler_mean_iou = _module
test_handler_metric_logger = _module
test_handler_metrics_saver = _module
test_handler_metrics_saver_dist = _module
test_handler_mlflow = _module
test_handler_nvtx = _module
test_handler_panoptic_quality = _module
test_handler_parameter_scheduler = _module
test_handler_post_processing = _module
test_handler_prob_map_producer = _module
test_handler_regression_metrics = _module
test_handler_regression_metrics_dist = _module
test_handler_rocauc = _module
test_handler_rocauc_dist = _module
test_handler_smartcache = _module
test_handler_stats = _module
test_handler_surface_distance = _module
test_handler_tb_image = _module
test_handler_tb_stats = _module
test_handler_validation = _module
test_hardnegsampler = _module
test_hashing = _module
test_hausdorff_distance = _module
test_header_correct = _module
test_highresnet = _module
test_hilbert_transform = _module
test_histogram_normalize = _module
test_histogram_normalized = _module
test_hovernet = _module
test_hovernet_instance_map_post_processing = _module
test_hovernet_instance_map_post_processingd = _module
test_hovernet_loss = _module
test_hovernet_nuclear_type_post_processing = _module
test_hovernet_nuclear_type_post_processingd = _module
test_identity = _module
test_identityd = _module
test_image_dataset = _module
test_image_rw = _module
test_img2tensorboard = _module
test_init_reader = _module
test_integration_autorunner = _module
test_integration_bundle_run = _module
test_integration_classification_2d = _module
test_integration_determinism = _module
test_integration_fast_train = _module
test_integration_gpu_customization = _module
test_integration_segmentation_3d = _module
test_integration_sliding_window = _module
test_integration_stn = _module
test_integration_unet_2d = _module
test_integration_workers = _module
test_integration_workflows = _module
test_integration_workflows_gan = _module
test_intensity_stats = _module
test_intensity_statsd = _module
test_inverse = _module
test_inverse_array = _module
test_inverse_collation = _module
test_invert = _module
test_invertd = _module
test_is_supported_format = _module
test_iterable_dataset = _module
test_itk_writer = _module
test_k_space_spike_noise = _module
test_k_space_spike_noised = _module
test_keep_largest_connected_component = _module
test_keep_largest_connected_componentd = _module
test_kspace_mask = _module
test_label_filter = _module
test_label_filterd = _module
test_label_quality_score = _module
test_label_to_contour = _module
test_label_to_contourd = _module
test_label_to_mask = _module
test_label_to_maskd = _module
test_lambda = _module
test_lambdad = _module
test_lesion_froc = _module
test_list_data_collate = _module
test_list_to_dict = _module
test_lltm = _module
test_lmdbdataset = _module
test_lmdbdataset_dist = _module
test_load_decathlon_datalist = _module
test_load_image = _module
test_load_imaged = _module
test_load_spacing_orientation = _module
test_loader_semaphore = _module
test_local_normalized_cross_correlation_loss = _module
test_localnet = _module
test_localnet_block = _module
test_look_up_option = _module
test_loss_metric = _module
test_lr_finder = _module
test_lr_scheduler = _module
test_make_nifti = _module
test_map_binary_to_indices = _module
test_map_classes_to_indices = _module
test_map_label_value = _module
test_map_label_valued = _module
test_map_transform = _module
test_mask_intensity = _module
test_mask_intensityd = _module
test_masked_dice_loss = _module
test_masked_inference_wsi_dataset = _module
test_masked_loss = _module
test_masked_patch_wsi_dataset = _module
test_matshow3d = _module
test_mean_ensemble = _module
test_mean_ensembled = _module
test_median_filter = _module
test_median_smooth = _module
test_median_smoothd = _module
test_mednistdataset = _module
test_meta_affine = _module
test_meta_tensor = _module
test_metatensor_integration = _module
test_milmodel = _module
test_mlp = _module
test_mmar_download = _module
test_module_list = _module
test_monai_env_vars = _module
test_mri_utils = _module
test_multi_scale = _module
test_net_adapter = _module
test_network_consistency = _module
test_nifti_endianness = _module
test_nifti_header_revise = _module
test_nifti_rw = _module
test_nifti_saver = _module
test_normalize_intensity = _module
test_normalize_intensityd = _module
test_npzdictitemdataset = _module
test_nrrd_reader = _module
test_nuclick_transforms = _module
test_numpy_reader = _module
test_nvtx_decorator = _module
test_nvtx_transform = _module
test_occlusion_sensitivity = _module
test_one_of = _module
test_optim_novograd = _module
test_optional_import = _module
test_ori_ras_lps = _module
test_orientation = _module
test_orientationd = _module
test_p3d_block = _module
test_pad_collation = _module
test_pad_mode = _module
test_parallel_execution = _module
test_parallel_execution_dist = _module
test_partition_dataset = _module
test_partition_dataset_classes = _module
test_patch_dataset = _module
test_patch_wsi_dataset = _module
test_patchembedding = _module
test_pathology_he_stain = _module
test_pathology_he_stain_dict = _module
test_pathology_prob_nms = _module
test_persistentdataset = _module
test_persistentdataset_dist = _module
test_phl_cpu = _module
test_phl_cuda = _module
test_pil_reader = _module
test_plot_2d_or_3d_image = _module
test_png_rw = _module
test_png_saver = _module
test_polyval = _module
test_prepare_batch_default = _module
test_prepare_batch_default_dist = _module
test_prepare_batch_extra_input = _module
test_prepare_batch_hovernet = _module
test_print_info = _module
test_print_transform_backends = _module
test_probnms = _module
test_probnmsd = _module
test_profiling = _module
test_pytorch_version_after = _module
test_query_memory = _module
test_rand_adjust_contrast = _module
test_rand_adjust_contrastd = _module
test_rand_affine = _module
test_rand_affine_grid = _module
test_rand_affined = _module
test_rand_axis_flip = _module
test_rand_axis_flipd = _module
test_rand_bias_field = _module
test_rand_bias_fieldd = _module
test_rand_coarse_dropout = _module
test_rand_coarse_dropoutd = _module
test_rand_coarse_shuffle = _module
test_rand_coarse_shuffled = _module
test_rand_crop_by_label_classes = _module
test_rand_crop_by_label_classesd = _module
test_rand_crop_by_pos_neg_label = _module
test_rand_crop_by_pos_neg_labeld = _module
test_rand_cucim_dict_transform = _module
test_rand_cucim_transform = _module
test_rand_deform_grid = _module
test_rand_elastic_2d = _module
test_rand_elastic_3d = _module
test_rand_elasticd_2d = _module
test_rand_elasticd_3d = _module
test_rand_flip = _module
test_rand_flipd = _module
test_rand_gaussian_noise = _module
test_rand_gaussian_noised = _module
test_rand_gaussian_sharpen = _module
test_rand_gaussian_sharpend = _module
test_rand_gaussian_smooth = _module
test_rand_gaussian_smoothd = _module
test_rand_gibbs_noise = _module
test_rand_gibbs_noised = _module
test_rand_grid_distortion = _module
test_rand_grid_distortiond = _module
test_rand_grid_patch = _module
test_rand_grid_patchd = _module
test_rand_histogram_shift = _module
test_rand_histogram_shiftd = _module
test_rand_k_space_spike_noise = _module
test_rand_k_space_spike_noised = _module
test_rand_lambda = _module
test_rand_lambdad = _module
test_rand_rician_noise = _module
test_rand_rician_noised = _module
test_rand_rotate = _module
test_rand_rotate90 = _module
test_rand_rotate90d = _module
test_rand_rotated = _module
test_rand_scale_crop = _module
test_rand_scale_cropd = _module
test_rand_scale_intensity = _module
test_rand_scale_intensityd = _module
test_rand_shift_intensity = _module
test_rand_shift_intensityd = _module
test_rand_spatial_crop = _module
test_rand_spatial_crop_samples = _module
test_rand_spatial_crop_samplesd = _module
test_rand_spatial_cropd = _module
test_rand_std_shift_intensity = _module
test_rand_std_shift_intensityd = _module
test_rand_weighted_crop = _module
test_rand_weighted_cropd = _module
test_rand_zoom = _module
test_rand_zoomd = _module
test_random_order = _module
test_randomizable = _module
test_randomizable_transform_type = _module
test_randtorchvisiond = _module
test_recon_net_utils = _module
test_reference_based_normalize_intensity = _module
test_reference_based_spatial_cropd = _module
test_reference_resolver = _module
test_reg_loss_integration = _module
test_regunet = _module
test_regunet_block = _module
test_remove_repeated_channel = _module
test_remove_repeated_channeld = _module
test_remove_small_objects = _module
test_repeat_channel = _module
test_repeat_channeld = _module
test_replace_module = _module
test_require_pkg = _module
test_resample = _module
test_resample_backends = _module
test_resample_datalist = _module
test_resample_to_match = _module
test_resample_to_matchd = _module
test_resampler = _module
test_resize = _module
test_resize_with_pad_or_crop = _module
test_resize_with_pad_or_cropd = _module
test_resized = _module
test_resnet = _module
test_retinanet = _module
test_retinanet_detector = _module
test_retinanet_predict_utils = _module
test_rotate = _module
test_rotate90 = _module
test_rotate90d = _module
test_rotated = _module
test_safe_dtype_range = _module
test_saliency_inferer = _module
test_sample_slices = _module
test_sampler_dist = _module
test_save_classificationd = _module
test_save_image = _module
test_save_imaged = _module
test_save_state = _module
test_savitzky_golay_filter = _module
test_savitzky_golay_smooth = _module
test_savitzky_golay_smoothd = _module
test_scale_intensity = _module
test_scale_intensity_range = _module
test_scale_intensity_range_percentiles = _module
test_scale_intensity_range_percentilesd = _module
test_scale_intensity_ranged = _module
test_scale_intensityd = _module
test_se_block = _module
test_se_blocks = _module
test_seg_loss_integration = _module
test_segresnet = _module
test_segresnet_block = _module
test_segresnet_ds = _module
test_select_cross_validation_folds = _module
test_select_itemsd = _module
test_selfattention = _module
test_senet = _module
test_separable_filter = _module
test_set_determinism = _module
test_set_visible_devices = _module
test_shift_intensity = _module
test_shift_intensityd = _module
test_shuffle_buffer = _module
test_signal_continuouswavelet = _module
test_signal_fillempty = _module
test_signal_rand_add_gaussiannoise = _module
test_signal_rand_add_sine = _module
test_signal_rand_add_sine_partial = _module
test_signal_rand_add_squarepulse = _module
test_signal_rand_add_squarepulse_partial = _module
test_signal_rand_drop = _module
test_signal_rand_scale = _module
test_signal_rand_shift = _module
test_signal_remove_frequency = _module
test_simple_aspp = _module
test_simulatedelay = _module
test_simulatedelayd = _module
test_skip_connection = _module
test_slice_inferer = _module
test_sliding_patch_wsi_dataset = _module
test_sliding_window_hovernet_inference = _module
test_sliding_window_inference = _module
test_smartcache_patch_wsi_dataset = _module
test_smartcachedataset = _module
test_smooth_field = _module
test_sobel_gradient = _module
test_sobel_gradientd = _module
test_spacing = _module
test_spacingd = _module
test_spatial_crop = _module
test_spatial_cropd = _module
test_spatial_pad = _module
test_spatial_padd = _module
test_spatial_resample = _module
test_spatial_resampled = _module
test_split_channel = _module
test_split_channeld = _module
test_split_on_grid = _module
test_split_on_grid_dict = _module
test_splitdim = _module
test_splitdimd = _module
test_squeezedim = _module
test_squeezedimd = _module
test_ssim_loss = _module
test_ssim_metric = _module
test_state_cacher = _module
test_std_shift_intensity = _module
test_std_shift_intensityd = _module
test_str2bool = _module
test_str2list = _module
test_subpixel_upsample = _module
test_surface_dice = _module
test_surface_distance = _module
test_swin_unetr = _module
test_synthetic = _module
test_tciadataset = _module
test_testtimeaugmentation = _module
test_thread_buffer = _module
test_threadcontainer = _module
test_threshold_intensity = _module
test_threshold_intensityd = _module
test_tile_on_grid = _module
test_tile_on_grid_dict = _module
test_timedcall_dist = _module
test_to_contiguous = _module
test_to_cupy = _module
test_to_cupyd = _module
test_to_device = _module
test_to_deviced = _module
test_to_from_meta_tensord = _module
test_to_numpy = _module
test_to_numpyd = _module
test_to_onehot = _module
test_to_pil = _module
test_to_pild = _module
test_to_tensor = _module
test_to_tensord = _module
test_torchscript_utils = _module
test_torchvision = _module
test_torchvision_fc_model = _module
test_torchvisiond = _module
test_traceable_transform = _module
test_train_mode = _module
test_transchex = _module
test_transform = _module
test_transformerblock = _module
test_transpose = _module
test_transposed = _module
test_tversky_loss = _module
test_unet = _module
test_unetr = _module
test_unetr_block = _module
test_unified_focal_loss = _module
test_upsample_block = _module
test_utils_pytorch_numpy_unification = _module
test_varautoencoder = _module
test_varnet = _module
test_version_leq = _module
test_video_datasets = _module
test_vis_cam = _module
test_vis_gradbased = _module
test_vis_gradcam = _module
test_vit = _module
test_vitautoenc = _module
test_vnet = _module
test_vote_ensemble = _module
test_vote_ensembled = _module
test_warp = _module
test_watershed = _module
test_watershedd = _module
test_weight_init = _module
test_weighted_random_sampler_dist = _module
test_with_allow_missing_keys = _module
test_write_metrics_reports = _module
test_wsireader = _module
test_zipdataset = _module
test_zoom = _module
test_zoom_affine = _module
test_zoomd = _module
cpp_resample_answers = _module
integration_answers = _module
utils = _module
versioneer = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import Optional


import torch


from copy import deepcopy


from time import sleep


from typing import Any


from typing import Dict


from typing import List


from typing import Union


from typing import cast


import numpy as np


import time


import warnings


from typing import Mapping


from typing import Hashable


from typing import Callable


from typing import Sequence


import logging


import random


import logging as logger


from typing import Tuple


from torch import Tensor


from torch import nn


import math


from typing import Type


from abc import ABC


from abc import abstractmethod


from typing import TypeVar


import torch.nn.functional as F


from torch.nn import CrossEntropyLoss


from torch.nn import functional as F


from torch.nn.modules.loss import _Loss


from numpy.lib.stride_tricks import as_strided


import copy


import re


import torch.nn as nn


from numpy import ndarray


from numbers import Number


from typing import Iterable


from logging.config import fileConfig


from torch.cuda import is_available


from collections import OrderedDict


from typing import Collection


import inspect


from torch.utils.data import DataLoader as _TorchDataLoader


from torch.utils.data import Dataset


import collections.abc


from copy import copy


from typing import IO


from typing import TYPE_CHECKING


from torch.multiprocessing import Manager


from torch.serialization import DEFAULT_PROTOCOL


from torch.utils.data import Dataset as _TorchDataset


from torch.utils.data import Subset


from itertools import chain


from typing import Iterator


from torch.utils.data._utils.collate import np_str_obj_array_pattern


from torch.utils.data import IterableDataset as _TorchIterableDataset


from torch.utils.data import get_worker_info


import itertools


import functools


from torch.utils.data import DistributedSampler as _TorchDistributedSampler


from queue import Empty


from queue import Full


from queue import Queue


from collections import abc


from collections import defaultdict


from functools import reduce


from itertools import product


from itertools import starmap


from itertools import zip_longest


from typing import Generator


from typing import Sized


from torch.utils.data._utils.collate import default_collate


from torch.utils.data import IterableDataset


from torch.utils.data import DataLoader


import torch.nn


from torch.nn.parallel import DataParallel


from torch.nn.parallel import DistributedDataParallel


from torch.optim.optimizer import Optimizer


import torch.distributed as dist


from torch.utils.data.distributed import DistributedSampler


from typing import MutableMapping


from torch.optim.lr_scheduler import ReduceLROnPlateau


from torch.optim.lr_scheduler import _LRScheduler


from warnings import warn


from functools import partial


from torch.nn.functional import softmax


from torch.nn import LayerNorm


from torch.autograd import Function


from torch.hub import load_state_dict_from_url


from torch.nn.functional import interpolate


from typing import NamedTuple


from torch.utils import model_zoo


import torch.utils.checkpoint as checkpoint


from torch.optim import Optimizer


from torch.optim.lr_scheduler import LambdaLR


from math import ceil


from torch.nn.functional import pad as pad_pt


from collections.abc import Iterable


from torch.utils.data.dataloader import DataLoader as TorchDataLoader


from torch.nn.functional import grid_sample


from enum import Enum


from functools import wraps


from inspect import getmembers


from inspect import isclass


from typing import Set


import types


import enum


from inspect import isfunction


from inspect import ismethod


from re import match


from types import FunctionType


from torch.nn import Module


from collections import namedtuple


from inspect import getframeinfo


from inspect import stack


from time import perf_counter


from time import perf_counter_ns


from collections.abc import Sequence


from torch.autograd import gradcheck


import numpy


from torch.cuda.amp import autocast


from torch.optim import SGD


import torch.optim as optim


import torch.multiprocessing as mp


from numpy.fft import fftn


from numpy.fft import fftshift


import string


import torch.multiprocessing


from torch.autograd import Variable


import scipy.ndimage


from scipy.ndimage import zoom as zoom_scipy


import queue


COMPUTE_DTYPE = torch.float32


INF = float('inf')


NdarrayOrTensor = Union[np.ndarray, torch.Tensor]


TO_REMOVE = 0.0


DtypeLike = Union[np.dtype, type, str, None]


NdarrayTensor = TypeVar('NdarrayTensor', bound=NdarrayOrTensor)


OPTIONAL_IMPORT_MSG_FMT = '{}'


class OptionalImportError(ImportError):
    """
    Could not import APIs from an optional dependency.
    """


def min_version(the_module, min_version_str: str='', *_args) ->bool:
    """
    Convert version strings into tuples of int and compare them.

    Returns True if the module's version is greater or equal to the 'min_version'.
    When min_version_str is not provided, it always returns True.
    """
    if not min_version_str or not hasattr(the_module, '__version__'):
        return True
    mod_version = tuple(int(x) for x in the_module.__version__.split('.')[:2])
    required = tuple(int(x) for x in min_version_str.split('.')[:2])
    return mod_version >= required


def optional_import(module: str, version: str='', version_checker: Callable[..., bool]=min_version, name: str='', descriptor: str=OPTIONAL_IMPORT_MSG_FMT, version_args=None, allow_namespace_pkg: bool=False, as_type: str='default') ->Tuple[Any, bool]:
    """
    Imports an optional module specified by `module` string.
    Any importing related exceptions will be stored, and exceptions raise lazily
    when attempting to use the failed-to-import module.

    Args:
        module: name of the module to be imported.
        version: version string used by the version_checker.
        version_checker: a callable to check the module version, Defaults to monai.utils.min_version.
        name: a non-module attribute (such as method/class) to import from the imported module.
        descriptor: a format string for the final error message when using a not imported module.
        version_args: additional parameters to the version checker.
        allow_namespace_pkg: whether importing a namespace package is allowed. Defaults to False.
        as_type: there are cases where the optionally imported object is used as
            a base class, or a decorator, the exceptions should raise accordingly. The current supported values
            are "default" (call once to raise), "decorator" (call the constructor and the second call to raise),
            and anything else will return a lazy class that can be used as a base class (call the constructor to raise).

    Returns:
        The imported module and a boolean flag indicating whether the import is successful.

    Examples::

        >>> torch, flag = optional_import('torch', '1.1')
        >>> print(torch, flag)
        <module 'torch' from 'python/lib/python3.6/site-packages/torch/__init__.py'> True

        >>> the_module, flag = optional_import('unknown_module')
        >>> print(flag)
        False
        >>> the_module.method  # trying to access a module which is not imported
        OptionalImportError: import unknown_module (No module named 'unknown_module').

        >>> torch, flag = optional_import('torch', '42', exact_version)
        >>> torch.nn  # trying to access a module for which there isn't a proper version imported
        OptionalImportError: import torch (requires version '42' by 'exact_version').

        >>> conv, flag = optional_import('torch.nn.functional', '1.0', name='conv1d')
        >>> print(conv)
        <built-in method conv1d of type object at 0x11a49eac0>

        >>> conv, flag = optional_import('torch.nn.functional', '42', name='conv1d')
        >>> conv()  # trying to use a function from the not successfully imported module (due to unmatched version)
        OptionalImportError: from torch.nn.functional import conv1d (requires version '42' by 'min_version').
    """
    tb = None
    exception_str = ''
    if name:
        actual_cmd = f'from {module} import {name}'
    else:
        actual_cmd = f'import {module}'
    try:
        pkg = __import__(module)
        the_module = import_module(module)
        if not allow_namespace_pkg:
            is_namespace = getattr(the_module, '__file__', None) is None and hasattr(the_module, '__path__')
            if is_namespace:
                raise AssertionError
        if name:
            the_module = getattr(the_module, name)
    except Exception as import_exception:
        tb = import_exception.__traceback__
        exception_str = f'{import_exception}'
    else:
        if version_args and version_checker(pkg, f'{version}', version_args):
            return the_module, True
        if not version_args and version_checker(pkg, f'{version}'):
            return the_module, True
    msg = descriptor.format(actual_cmd)
    if version and tb is None:
        msg += f" (requires '{module} {version}' by '{version_checker.__name__}')"
    if exception_str:
        msg += f' ({exception_str})'


    class _LazyRaise:

        def __init__(self, *_args, **_kwargs):
            _default_msg = f'{msg}.' + '\n\nFor details about installing the optional dependencies, please visit:' + '\n    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies'
            if tb is None:
                self._exception = OptionalImportError(_default_msg)
            else:
                self._exception = OptionalImportError(_default_msg).with_traceback(tb)

        def __getattr__(self, name):
            """
            Raises:
                OptionalImportError: When you call this method.
            """
            raise self._exception

        def __call__(self, *_args, **_kwargs):
            """
            Raises:
                OptionalImportError: When you call this method.
            """
            raise self._exception

        def __getitem__(self, item):
            raise self._exception

        def __iter__(self):
            raise self._exception
    if as_type == 'default':
        return _LazyRaise(), False


    class _LazyCls(_LazyRaise):

        def __init__(self, *_args, **kwargs):
            super().__init__()
            if not as_type.startswith('decorator'):
                raise self._exception
    return _LazyCls, False


cp, _ = optional_import('cupy')


cp_ndarray, _ = optional_import('cupy', name='ndarray')


UNSUPPORTED_TYPES = {np.dtype('uint16'): torch.int32, np.dtype('uint32'): torch.int64, np.dtype('uint64'): torch.int64}


def dtype_numpy_to_torch(dtype: np.dtype) ->torch.dtype:
    """Convert a numpy dtype to its torch equivalent."""
    return torch.from_numpy(np.empty([], dtype=dtype)).dtype


def dtype_torch_to_numpy(dtype: torch.dtype) ->np.dtype:
    """Convert a torch dtype to its numpy equivalent."""
    return torch.empty([], dtype=dtype).numpy().dtype


def get_equivalent_dtype(dtype, data_type):
    """Convert to the `dtype` that corresponds to `data_type`.

    The input dtype can also be a string. e.g., `"float32"` becomes `torch.float32` or
    `np.float32` as necessary.

    Example::

        im = torch.tensor(1)
        dtype = get_equivalent_dtype(np.float32, type(im))

    """
    if dtype is None:
        return None
    if data_type is torch.Tensor or data_type.__name__ == 'MetaTensor':
        if isinstance(dtype, torch.dtype):
            return dtype
        return dtype_numpy_to_torch(dtype)
    if not isinstance(dtype, torch.dtype):
        return dtype
    return dtype_torch_to_numpy(dtype)


def get_dtype_bound_value(dtype: Union[DtypeLike, torch.dtype]):
    """
    Get dtype bound value
    Args:
        dtype: dtype to get bound value
    Returns:
        (bound_min_value, bound_max_value)
    """
    if dtype in UNSUPPORTED_TYPES:
        is_floating_point = False
    else:
        is_floating_point = get_equivalent_dtype(dtype, torch.Tensor).is_floating_point
    dtype = get_equivalent_dtype(dtype, np.array)
    if is_floating_point:
        return np.finfo(dtype).min, np.finfo(dtype).max
    else:
        return np.iinfo(dtype).min, np.iinfo(dtype).max


cp, has_cp = optional_import('cupy')


def safe_dtype_range(data: Any, dtype: Union[DtypeLike, torch.dtype]=None):
    """
    Utility to safely convert the input data to target dtype.

    Args:
        data: input data can be PyTorch Tensor, numpy array, list, dictionary, int, float, bool, str, etc.
            will convert to target dtype and keep the original type.
            for dictionary, list or tuple, convert every item.
        dtype: target data type to convert.
    """

    def _safe_dtype_range(data, dtype):
        output_dtype = dtype if dtype is not None else data.dtype
        dtype_bound_value = get_dtype_bound_value(output_dtype)
        if data.ndim == 0:
            data_bound = data, data
        elif isinstance(data, torch.Tensor):
            data_bound = torch.min(data), torch.max(data)
        else:
            data_bound = np.min(data), np.max(data)
        if data_bound[1] > dtype_bound_value[1] or data_bound[0] < dtype_bound_value[0]:
            if isinstance(data, torch.Tensor):
                return torch.clamp(data, dtype_bound_value[0], dtype_bound_value[1])
            elif isinstance(data, np.ndarray):
                return np.clip(data, dtype_bound_value[0], dtype_bound_value[1])
            elif has_cp and isinstance(data, cp_ndarray):
                return cp.clip(data, dtype_bound_value[0], dtype_bound_value[1])
        else:
            return data
    if has_cp and isinstance(data, cp_ndarray):
        return cp.asarray(_safe_dtype_range(data, dtype))
    elif isinstance(data, np.ndarray):
        return np.asarray(_safe_dtype_range(data, dtype))
    elif isinstance(data, torch.Tensor):
        return _safe_dtype_range(data, dtype)
    elif isinstance(data, (float, int, bool)) and dtype is None:
        return data
    elif isinstance(data, (float, int, bool)) and dtype is not None:
        output_dtype = dtype
        dtype_bound_value = get_dtype_bound_value(output_dtype)
        data = dtype_bound_value[1] if data > dtype_bound_value[1] else data
        data = dtype_bound_value[0] if data < dtype_bound_value[0] else data
        return data
    elif isinstance(data, list):
        return [safe_dtype_range(i, dtype=dtype) for i in data]
    elif isinstance(data, tuple):
        return tuple(safe_dtype_range(i, dtype=dtype) for i in data)
    elif isinstance(data, dict):
        return {k: safe_dtype_range(v, dtype=dtype) for k, v in data.items()}
    return data


def convert_to_cupy(data, dtype: Optional[np.dtype]=None, wrap_sequence: bool=False, safe: bool=False):
    """
    Utility to convert the input data to a cupy array. If passing a dictionary, list or tuple,
    recursively check every item and convert it to cupy array.

    Args:
        data: input data can be PyTorch Tensor, numpy array, cupy array, list, dictionary, int, float, bool, str, etc.
            Tensor, numpy array, cupy array, float, int, bool are converted to cupy arrays,
            for dictionary, list or tuple, convert every item to a numpy array if applicable.
        dtype: target data type when converting to Cupy array, tt must be an argument of `numpy.dtype`,
            for more details: https://docs.cupy.dev/en/stable/reference/generated/cupy.array.html.
        wrap_sequence: if `False`, then lists will recursively call this function.
            E.g., `[1, 2]` -> `[array(1), array(2)]`. If `True`, then `[1, 2]` -> `array([1, 2])`.
        safe: if `True`, then do safe dtype convert when intensity overflow. default to `False`.
            E.g., `[256, -12]` -> `[array(0), array(244)]`. If `True`, then `[256, -12]` -> `[array(255), array(0)]`.
    """
    if safe:
        data = safe_dtype_range(data, dtype)
    if isinstance(data, (cp_ndarray, np.ndarray, torch.Tensor, float, int, bool)):
        data = cp.asarray(data, dtype)
    elif isinstance(data, list):
        list_ret = [convert_to_cupy(i, dtype) for i in data]
        return cp.asarray(list_ret) if wrap_sequence else list_ret
    elif isinstance(data, tuple):
        tuple_ret = tuple(convert_to_cupy(i, dtype) for i in data)
        return cp.asarray(tuple_ret) if wrap_sequence else tuple_ret
    elif isinstance(data, dict):
        return {k: convert_to_cupy(v, dtype) for k, v in data.items()}
    if not isinstance(data, cp.ndarray):
        raise ValueError(f'The input data type [{type(data)}] cannot be converted into cupy arrays!')
    if data.ndim > 0:
        data = cp.ascontiguousarray(data)
    return data


def convert_to_numpy(data, dtype: DtypeLike=None, wrap_sequence: bool=False, safe: bool=False):
    """
    Utility to convert the input data to a numpy array. If passing a dictionary, list or tuple,
    recursively check every item and convert it to numpy array.

    Args:
        data: input data can be PyTorch Tensor, numpy array, list, dictionary, int, float, bool, str, etc.
            will convert Tensor, Numpy array, float, int, bool to numpy arrays, strings and objects keep the original.
            for dictionary, list or tuple, convert every item to a numpy array if applicable.
        dtype: target data type when converting to numpy array.
        wrap_sequence: if `False`, then lists will recursively call this function.
            E.g., `[1, 2]` -> `[array(1), array(2)]`. If `True`, then `[1, 2]` -> `array([1, 2])`.
        safe: if `True`, then do safe dtype convert when intensity overflow. default to `False`.
            E.g., `[256, -12]` -> `[array(0), array(244)]`. If `True`, then `[256, -12]` -> `[array(255), array(0)]`.
    """
    if safe:
        data = safe_dtype_range(data, dtype)
    if isinstance(data, torch.Tensor):
        data = np.asarray(data.detach().numpy(), dtype=get_equivalent_dtype(dtype, np.ndarray))
    elif has_cp and isinstance(data, cp_ndarray):
        data = cp.asnumpy(data).astype(dtype, copy=False)
    elif isinstance(data, (np.ndarray, float, int, bool)):
        if isinstance(data, np.ndarray) and data.ndim > 0 and data.dtype.itemsize < np.dtype(dtype).itemsize:
            data = np.ascontiguousarray(data)
        data = np.asarray(data, dtype=dtype)
    elif isinstance(data, list):
        list_ret = [convert_to_numpy(i, dtype=dtype) for i in data]
        return np.asarray(list_ret) if wrap_sequence else list_ret
    elif isinstance(data, tuple):
        tuple_ret = tuple(convert_to_numpy(i, dtype=dtype) for i in data)
        return np.asarray(tuple_ret) if wrap_sequence else tuple_ret
    elif isinstance(data, dict):
        return {k: convert_to_numpy(v, dtype=dtype) for k, v in data.items()}
    if isinstance(data, np.ndarray) and data.ndim > 0:
        data = np.ascontiguousarray(data)
    return data


def convert_to_tensor(data, dtype: Union[DtypeLike, torch.dtype]=None, device: Union[None, str, torch.device]=None, wrap_sequence: bool=False, track_meta: bool=False, safe: bool=False):
    """
    Utility to convert the input data to a PyTorch Tensor, if `track_meta` is True, the output will be a `MetaTensor`,
    otherwise, the output will be a regular torch Tensor.
    If passing a dictionary, list or tuple, recursively check every item and convert it to PyTorch Tensor.

    Args:
        data: input data can be PyTorch Tensor, numpy array, list, dictionary, int, float, bool, str, etc.
            will convert Tensor, Numpy array, float, int, bool to Tensor, strings and objects keep the original.
            for dictionary, list or tuple, convert every item to a Tensor if applicable.
        dtype: target data type to when converting to Tensor.
        device: target device to put the converted Tensor data.
        wrap_sequence: if `False`, then lists will recursively call this function.
            E.g., `[1, 2]` -> `[tensor(1), tensor(2)]`. If `True`, then `[1, 2]` -> `tensor([1, 2])`.
        track_meta: whether to track the meta information, if `True`, will convert to `MetaTensor`.
            default to `False`.
        safe: if `True`, then do safe dtype convert when intensity overflow. default to `False`.
            E.g., `[256, -12]` -> `[tensor(0), tensor(244)]`.
            If `True`, then `[256, -12]` -> `[tensor(255), tensor(0)]`.

    """

    def _convert_tensor(tensor, **kwargs):
        if not isinstance(tensor, torch.Tensor):
            if isinstance(tensor, np.ndarray) and tensor.dtype in UNSUPPORTED_TYPES:
                tensor = tensor.astype(UNSUPPORTED_TYPES[tensor.dtype])
            tensor = torch.as_tensor(tensor, **kwargs)
        if track_meta and not isinstance(tensor, monai.data.MetaTensor):
            return monai.data.MetaTensor(tensor)
        if not track_meta and isinstance(tensor, monai.data.MetaTensor):
            return tensor.as_tensor()
        return tensor
    if safe:
        data = safe_dtype_range(data, dtype)
    dtype = get_equivalent_dtype(dtype, torch.Tensor)
    if isinstance(data, torch.Tensor):
        return _convert_tensor(data)
    if isinstance(data, np.ndarray):
        if re.search('[SaUO]', data.dtype.str) is None:
            if data.ndim > 0:
                data = np.ascontiguousarray(data)
            return _convert_tensor(data, dtype=dtype, device=device)
    elif has_cp and isinstance(data, cp_ndarray) or isinstance(data, (float, int, bool)):
        return _convert_tensor(data, dtype=dtype, device=device)
    elif isinstance(data, list):
        list_ret = [convert_to_tensor(i, dtype=dtype, device=device, track_meta=track_meta) for i in data]
        return _convert_tensor(list_ret, dtype=dtype, device=device) if wrap_sequence else list_ret
    elif isinstance(data, tuple):
        tuple_ret = tuple(convert_to_tensor(i, dtype=dtype, device=device, track_meta=track_meta) for i in data)
        return _convert_tensor(tuple_ret, dtype=dtype, device=device) if wrap_sequence else tuple_ret
    elif isinstance(data, dict):
        return {k: convert_to_tensor(v, dtype=dtype, device=device, track_meta=track_meta) for k, v in data.items()}
    return data


def convert_data_type(data: Any, output_type: Optional[Type[NdarrayTensor]]=None, device: Union[None, str, torch.device]=None, dtype: Union[DtypeLike, torch.dtype]=None, wrap_sequence: bool=False, safe: bool=False) ->Tuple[NdarrayTensor, type, Optional[torch.device]]:
    """
    Convert to `MetaTensor`, `torch.Tensor` or `np.ndarray` from `MetaTensor`, `torch.Tensor`,
    `np.ndarray`, `float`, `int`, etc.

    Args:
        data: data to be converted
        output_type: `monai.data.MetaTensor`, `torch.Tensor`, or `np.ndarray` (if `None`, unchanged)
        device: if output is `MetaTensor` or `torch.Tensor`, select device (if `None`, unchanged)
        dtype: dtype of output data. Converted to correct library type (e.g.,
            `np.float32` is converted to `torch.float32` if output type is `torch.Tensor`).
            If left blank, it remains unchanged.
        wrap_sequence: if `False`, then lists will recursively call this function.
            E.g., `[1, 2]` -> `[array(1), array(2)]`. If `True`, then `[1, 2]` -> `array([1, 2])`.
        safe: if `True`, then do safe dtype convert when intensity overflow. default to `False`.
            E.g., `[256, -12]` -> `[array(0), array(244)]`. If `True`, then `[256, -12]` -> `[array(255), array(0)]`.

    Returns:
        modified data, orig_type, orig_device

    Note:
        When both `output_type` and `dtype` are specified with different backend
        (e.g., `torch.Tensor` and `np.float32`), the `output_type` will be used as the primary type,
        for example::

            >>> convert_data_type(1, torch.Tensor, dtype=np.float32)
            (1.0, <class 'torch.Tensor'>, None)

    """
    orig_type: type
    if isinstance(data, monai.data.MetaTensor):
        orig_type = monai.data.MetaTensor
    elif isinstance(data, torch.Tensor):
        orig_type = torch.Tensor
    elif isinstance(data, np.ndarray):
        orig_type = np.ndarray
    elif has_cp and isinstance(data, cp.ndarray):
        orig_type = cp.ndarray
    else:
        orig_type = type(data)
    orig_device = data.device if isinstance(data, torch.Tensor) else None
    output_type = output_type or orig_type
    dtype_ = get_equivalent_dtype(dtype, output_type)
    data_: NdarrayTensor
    if issubclass(output_type, torch.Tensor):
        track_meta = issubclass(output_type, monai.data.MetaTensor)
        data_ = convert_to_tensor(data, dtype=dtype_, device=device, wrap_sequence=wrap_sequence, track_meta=track_meta, safe=safe)
        return data_, orig_type, orig_device
    if issubclass(output_type, np.ndarray):
        data_ = convert_to_numpy(data, dtype=dtype_, wrap_sequence=wrap_sequence, safe=safe)
        return data_, orig_type, orig_device
    elif has_cp and issubclass(output_type, cp.ndarray):
        data_ = convert_to_cupy(data, dtype=dtype_, wrap_sequence=wrap_sequence, safe=safe)
        return data_, orig_type, orig_device
    raise ValueError(f'Unsupported output type: {output_type}')


SUPPORTED_SPATIAL_DIMS = [2, 3]


def min(x: NdarrayTensor, dim: Optional[Union[int, Tuple]]=None, **kwargs) ->NdarrayTensor:
    """`torch.min` with equivalent implementation for numpy

    Args:
        x: array/tensor.

    Returns:
        the minimum of x.
    """
    ret: NdarrayTensor
    if dim is None:
        ret = np.min(x, **kwargs) if isinstance(x, (np.ndarray, list)) else torch.min(x, **kwargs)
    elif isinstance(x, (np.ndarray, list)):
        ret = np.min(x, axis=dim, **kwargs)
    else:
        ret = torch.min(x, int(dim), **kwargs)
    return ret


def damerau_levenshtein_distance(s1: str, s2: str):
    """
    Calculates the Damerau–Levenshtein distance between two strings for spelling correction.
    https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance
    """
    if s1 == s2:
        return 0
    string_1_length = len(s1)
    string_2_length = len(s2)
    if not s1:
        return string_2_length
    if not s2:
        return string_1_length
    d = {(i, -1): (i + 1) for i in range(-1, string_1_length + 1)}
    for j in range(-1, string_2_length + 1):
        d[-1, j] = j + 1
    for i, s1i in enumerate(s1):
        for j, s2j in enumerate(s2):
            cost = 0 if s1i == s2j else 1
            d[i, j] = min(d[i - 1, j] + 1, d[i, j - 1] + 1, d[i - 1, j - 1] + cost)
            if i and j and s1i == s2[j - 1] and s1[i - 1] == s2j:
                d[i, j] = min(d[i, j], d[i - 2, j - 2] + cost)
    return d[string_1_length - 1, string_2_length - 1]


def look_up_option(opt_str, supported: Union[Collection, enum.EnumMeta], default='no_default', print_all_options=True):
    """
    Look up the option in the supported collection and return the matched item.
    Raise a value error possibly with a guess of the closest match.

    Args:
        opt_str: The option string or Enum to look up.
        supported: The collection of supported options, it can be list, tuple, set, dict, or Enum.
        default: If it is given, this method will return `default` when `opt_str` is not found,
            instead of raising a `ValueError`. Otherwise, it defaults to `"no_default"`,
            so that the method may raise a `ValueError`.
        print_all_options: whether to print all available options when `opt_str` is not found. Defaults to True

    Examples:

    .. code-block:: python

        from enum import Enum
        from monai.utils import look_up_option
        class Color(Enum):
            RED = "red"
            BLUE = "blue"
        look_up_option("red", Color)  # <Color.RED: 'red'>
        look_up_option(Color.RED, Color)  # <Color.RED: 'red'>
        look_up_option("read", Color)
        # ValueError: By 'read', did you mean 'red'?
        # 'read' is not a valid option.
        # Available options are {'blue', 'red'}.
        look_up_option("red", {"red", "blue"})  # "red"

    Adapted from https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/utilities/util_common.py#L249
    """
    if not isinstance(opt_str, Hashable):
        raise ValueError(f'Unrecognized option type: {type(opt_str)}:{opt_str}.')
    if isinstance(opt_str, str):
        opt_str = opt_str.strip()
    if isinstance(supported, enum.EnumMeta):
        if isinstance(opt_str, str) and opt_str in {item.value for item in cast(Iterable[enum.Enum], supported)}:
            return supported(opt_str)
        if isinstance(opt_str, enum.Enum) and opt_str in supported:
            return opt_str
    elif isinstance(supported, Mapping) and opt_str in supported:
        return supported[opt_str]
    elif isinstance(supported, Collection) and opt_str in supported:
        return opt_str
    if default != 'no_default':
        return default
    set_to_check: set
    if isinstance(supported, enum.EnumMeta):
        set_to_check = {item.value for item in cast(Iterable[enum.Enum], supported)}
    else:
        set_to_check = set(supported) if supported is not None else set()
    if not set_to_check:
        raise ValueError(f'No options available: {supported}.')
    edit_dists = {}
    opt_str = f'{opt_str}'
    for key in set_to_check:
        edit_dist = damerau_levenshtein_distance(f'{key}', opt_str)
        if edit_dist <= 3:
            edit_dists[key] = edit_dist
    supported_msg = f'Available options are {set_to_check}.\n' if print_all_options else ''
    if edit_dists:
        guess_at_spelling = min(edit_dists, key=edit_dists.get)
        raise ValueError(f"By '{opt_str}', did you mean '{guess_at_spelling}'?\n" + f"'{opt_str}' is not a valid value.\n" + supported_msg)
    raise ValueError(f"Unsupported option '{opt_str}', " + supported_msg)


def get_spatial_dims(boxes: Union[torch.Tensor, np.ndarray, None]=None, points: Union[torch.Tensor, np.ndarray, None]=None, corners: Union[Sequence, None]=None, spatial_size: Union[Sequence[int], torch.Tensor, np.ndarray, None]=None) ->int:
    """
    Get spatial dimension for the giving setting and check the validity of them.
    Missing input is allowed. But at least one of the input value should be given.
    It raises ValueError if the dimensions of multiple inputs do not match with each other.

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray
        points: point coordinates, [x, y] or [x, y, z], Nx2 or Nx3 torch tensor or ndarray
        corners: corners of boxes, 4-element or 6-element tuple, each element is a Nx1 torch tensor or ndarray
        spatial_size: The spatial size of the image where the boxes are attached.
                len(spatial_size) should be in [2, 3].

    Returns:
        ``int``: spatial_dims, number of spatial dimensions of the bounding boxes.

    Example:
        .. code-block:: python

            boxes = torch.ones(10,6)
            get_spatial_dims(boxes, spatial_size=[100,200,200]) # will return 3
            get_spatial_dims(boxes, spatial_size=[100,200]) # will raise ValueError
            get_spatial_dims(boxes) # will return 3
    """
    spatial_dims_set = set()
    if boxes is not None:
        if int(boxes.shape[1]) not in [4, 6]:
            raise ValueError(f'Currently we support only boxes with shape [N,4] or [N,6], got boxes with shape {boxes.shape}.')
        spatial_dims_set.add(int(boxes.shape[1] / 2))
    if points is not None:
        if int(points.shape[1]) not in SUPPORTED_SPATIAL_DIMS:
            raise ValueError(f'Currently we support only points with shape [N,2] or [N,3], got boxes with shape {points.shape}.')
        spatial_dims_set.add(int(points.shape[1]))
    if corners is not None:
        if len(corners) not in [4, 6]:
            raise ValueError(f'Currently we support only boxes with shape [N,4] or [N,6], got box corner tuple with length {len(corners)}.')
        spatial_dims_set.add(len(corners) // 2)
    if spatial_size is not None:
        if len(spatial_size) not in SUPPORTED_SPATIAL_DIMS:
            raise ValueError(f'Currently we support only boxes on 2-D and 3-D images, got image spatial_size {spatial_size}.')
        spatial_dims_set.add(len(spatial_size))
    spatial_dims_list = list(spatial_dims_set)
    if len(spatial_dims_list) == 0:
        raise ValueError('At least one of the inputs needs to be non-empty.')
    if len(spatial_dims_list) == 1:
        spatial_dims = int(spatial_dims_list[0])
        spatial_dims = look_up_option(spatial_dims, supported=[2, 3])
        return int(spatial_dims)
    raise ValueError('The dimensions of multiple inputs should match with each other.')


def is_valid_box_values(boxes: NdarrayOrTensor) ->bool:
    """
    This function checks whether the box size is non-negative.

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``

    Returns:
        whether ``boxes`` is valid
    """
    spatial_dims = get_spatial_dims(boxes=boxes)
    for axis in range(0, spatial_dims):
        if (boxes[:, spatial_dims + axis] < boxes[:, axis]).sum() > 0:
            return False
    return True


def box_area(boxes: NdarrayOrTensor) ->NdarrayOrTensor:
    """
    This function computes the area (2D) or volume (3D) of each box.
    Half precision is not recommended for this function as it may cause overflow, especially for 3D images.

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``

    Returns:
        area (2D) or volume (3D) of boxes, with size of (N,).

    Example:
        .. code-block:: python

            boxes = torch.ones(10,6)
            # we do computation with torch.float32 to avoid overflow
            compute_dtype = torch.float32
            area = box_area(boxes=boxes.to(dtype=compute_dtype))  # torch.float32, size of (10,)
    """
    if not is_valid_box_values(boxes):
        raise ValueError('Given boxes has invalid values. The box size must be non-negative.')
    spatial_dims = get_spatial_dims(boxes=boxes)
    area = boxes[:, spatial_dims] - boxes[:, 0] + TO_REMOVE
    for axis in range(1, spatial_dims):
        area = area * (boxes[:, axis + spatial_dims] - boxes[:, axis] + TO_REMOVE)
    area_t, *_ = convert_data_type(area, torch.Tensor)
    if area_t.isnan().any() or area_t.isinf().any():
        if area_t.dtype is torch.float16:
            raise ValueError('Box area is NaN or Inf. boxes is float16. Please change to float32 and test it again.')
        else:
            raise ValueError('Box area is NaN or Inf.')
    return area


def _box_inter_union(boxes1_t: torch.Tensor, boxes2_t: torch.Tensor, compute_dtype: torch.dtype=torch.float32) ->Tuple[torch.Tensor, torch.Tensor]:
    """
    This internal function computes the intersection and union area of two set of boxes.

    Args:
        boxes1: bounding boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
        boxes2: bounding boxes, Mx4 or Mx6 torch tensor. The box mode is assumed to be ``StandardMode``
        compute_dtype: default torch.float32, dtype with which the results will be computed

    Returns:
        inter, with size of (N,M) and dtype of ``compute_dtype``.
        union, with size of (N,M) and dtype of ``compute_dtype``.

    """
    spatial_dims = get_spatial_dims(boxes=boxes1_t)
    area1 = box_area(boxes=boxes1_t)
    area2 = box_area(boxes=boxes2_t)
    lt = torch.max(boxes1_t[:, None, :spatial_dims], boxes2_t[:, :spatial_dims])
    rb = torch.min(boxes1_t[:, None, spatial_dims:], boxes2_t[:, spatial_dims:])
    wh = (rb - lt + TO_REMOVE).clamp(min=0)
    inter = torch.prod(wh, dim=-1, keepdim=False)
    union = area1[:, None] + area2 - inter
    return inter, union


def convert_to_dst_type(src: Any, dst: NdarrayTensor, dtype: Union[DtypeLike, torch.dtype, None]=None, wrap_sequence: bool=False, device: Union[None, str, torch.device]=None, safe: bool=False) ->Tuple[NdarrayTensor, type, Optional[torch.device]]:
    """
    Convert source data to the same data type and device as the destination data.
    If `dst` is an instance of `torch.Tensor` or its subclass, convert `src` to `torch.Tensor` with the same data type as `dst`,
    if `dst` is an instance of `numpy.ndarray` or its subclass, convert to `numpy.ndarray` with the same data type as `dst`,
    otherwise, convert to the type of `dst` directly.

    Args:
        src: source data to convert type.
        dst: destination data that convert to the same data type as it.
        dtype: an optional argument if the target `dtype` is different from the original `dst`'s data type.
        wrap_sequence: if `False`, then lists will recursively call this function. E.g., `[1, 2]` -> `[array(1), array(2)]`.
            If `True`, then `[1, 2]` -> `array([1, 2])`.
        device: target device to put the converted Tensor data. If unspecified, `dst.device` will be used if possible.
        safe: if `True`, then do safe dtype convert when intensity overflow. default to `False`.
            E.g., `[256, -12]` -> `[array(0), array(244)]`. If `True`, then `[256, -12]` -> `[array(255), array(0)]`.

    See Also:
        :func:`convert_data_type`
    """
    device = dst.device if device is None and isinstance(dst, torch.Tensor) else device
    if dtype is None:
        dtype = dst.dtype
    copy_meta = False
    output_type: Any
    if isinstance(dst, monai.data.MetaTensor):
        output_type = monai.data.MetaTensor
        if not isinstance(src, monai.data.MetaTensor):
            copy_meta = True
    elif isinstance(dst, torch.Tensor):
        output_type = torch.Tensor
    elif isinstance(dst, np.ndarray):
        output_type = np.ndarray
    else:
        output_type = type(dst)
    output: NdarrayTensor
    output, _type, _device = convert_data_type(data=src, output_type=output_type, device=device, dtype=dtype, wrap_sequence=wrap_sequence, safe=safe)
    if copy_meta and isinstance(output, monai.data.MetaTensor):
        output.copy_meta_from(dst)
    return output, _type, _device


def box_iou(boxes1: NdarrayOrTensor, boxes2: NdarrayOrTensor) ->NdarrayOrTensor:
    """
    Compute the intersection over union (IoU) of two set of boxes.

    Args:
        boxes1: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        boxes2: bounding boxes, Mx4 or Mx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``

    Returns:
        IoU, with size of (N,M) and same data type as ``boxes1``

    """
    if not isinstance(boxes1, type(boxes2)):
        warnings.warn(f'boxes1 is {type(boxes1)}, while boxes2 is {type(boxes2)}. The result will be {type(boxes1)}.')
    boxes1_t, *_ = convert_data_type(boxes1, torch.Tensor)
    boxes2_t, *_ = convert_data_type(boxes2, torch.Tensor)
    box_dtype = boxes1_t.dtype
    inter, union = _box_inter_union(boxes1_t, boxes2_t, compute_dtype=COMPUTE_DTYPE)
    iou_t = inter / (union + torch.finfo(COMPUTE_DTYPE).eps)
    iou_t = iou_t
    if torch.isnan(iou_t).any() or torch.isinf(iou_t).any():
        raise ValueError('Box IoU is NaN or Inf.')
    iou, *_ = convert_to_dst_type(src=iou_t, dst=boxes1)
    return iou


class Matcher(ABC):
    """
    Base class of Matcher, which matches boxes and anchors to each other

    Args:
        similarity_fn: function for similarity computation between
            boxes and anchors
    """
    BELOW_LOW_THRESHOLD: int = -1
    BETWEEN_THRESHOLDS: int = -2

    def __init__(self, similarity_fn: Callable[[Tensor, Tensor], Tensor]=box_iou):
        self.similarity_fn = similarity_fn

    def __call__(self, boxes: torch.Tensor, anchors: torch.Tensor, num_anchors_per_level: Sequence[int], num_anchors_per_loc: int) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute matches for a single image

        Args:
            boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
            anchors: anchors to match Mx4 or Mx6, also assumed to be ``StandardMode``.
            num_anchors_per_level: number of anchors per feature pyramid level
            num_anchors_per_loc: number of anchors per position

        Returns:
            - matrix which contains the similarity from each boxes to each anchor [N, M]
            - vector which contains the matched box index for all
                anchors (if background `BELOW_LOW_THRESHOLD` is used
                and if it should be ignored `BETWEEN_THRESHOLDS` is used) [M]

        Note:
            ``StandardMode`` = :class:`~monai.data.box_utils.CornerCornerModeTypeA`,
            also represented as "xyxy" ([xmin, ymin, xmax, ymax]) for 2D
            and "xyzxyz" ([xmin, ymin, zmin, xmax, ymax, zmax]) for 3D.
        """
        if boxes.numel() == 0:
            num_anchors = anchors.shape[0]
            match_quality_matrix = torch.tensor([])
            matches = torch.empty(num_anchors, dtype=torch.int64).fill_(self.BELOW_LOW_THRESHOLD)
            return match_quality_matrix, matches
        return self.compute_matches(boxes=boxes, anchors=anchors, num_anchors_per_level=num_anchors_per_level, num_anchors_per_loc=num_anchors_per_loc)

    @abstractmethod
    def compute_matches(self, boxes: torch.Tensor, anchors: torch.Tensor, num_anchors_per_level: Sequence[int], num_anchors_per_loc: int) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute matches

        Args:
            boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
            anchors: anchors to match Mx4 or Mx6, also assumed to be ``StandardMode``.
            num_anchors_per_level: number of anchors per feature pyramid level
            num_anchors_per_loc: number of anchors per position

        Returns:
            - matrix which contains the similarity from each boxes to each anchor [N, M]
            - vector which contains the matched box index for all
              anchors (if background `BELOW_LOW_THRESHOLD` is used
              and if it should be ignored `BETWEEN_THRESHOLDS` is used) [M]
        """
        raise NotImplementedError


class StrEnum(str, Enum):
    """
    Enum subclass that converts its value to a string.

    .. code-block:: python

        from monai.utils import StrEnum

        class Example(StrEnum):
            MODE_A = "A"
            MODE_B = "B"

        assert (list(Example) == ["A", "B"])
        assert Example.MODE_A == "A"
        assert str(Example.MODE_A) == "A"
        assert monai.utils.look_up_option("A", Example) == "A"
    """

    def __str__(self):
        return self.value

    def __repr__(self):
        return self.value


class BoxModeName(StrEnum):
    """
    Box mode names.
    """
    XYXY = 'xyxy'
    XYZXYZ = 'xyzxyz'
    XXYY = 'xxyy'
    XXYYZZ = 'xxyyzz'
    XYXYZZ = 'xyxyzz'
    XYWH = 'xywh'
    XYZWHD = 'xyzwhd'
    CCWH = 'ccwh'
    CCCWHD = 'cccwhd'


class BoxMode(ABC):
    """
    An abstract class of a ``BoxMode``.

    A ``BoxMode`` is callable that converts box mode of ``boxes``, which are Nx4 (2D) or Nx6 (3D) torch tensor or ndarray.
    ``BoxMode`` has several subclasses that represents different box modes, including

    - :class:`~monai.data.box_utils.CornerCornerModeTypeA`:
      represents [xmin, ymin, xmax, ymax] for 2D and [xmin, ymin, zmin, xmax, ymax, zmax] for 3D
    - :class:`~monai.data.box_utils.CornerCornerModeTypeB`:
      represents [xmin, xmax, ymin, ymax] for 2D and [xmin, xmax, ymin, ymax, zmin, zmax] for 3D
    - :class:`~monai.data.box_utils.CornerCornerModeTypeC`:
      represents [xmin, ymin, xmax, ymax] for 2D and [xmin, ymin, xmax, ymax, zmin, zmax] for 3D
    - :class:`~monai.data.box_utils.CornerSizeMode`:
      represents [xmin, ymin, xsize, ysize] for 2D and [xmin, ymin, zmin, xsize, ysize, zsize] for 3D
    - :class:`~monai.data.box_utils.CenterSizeMode`:
      represents [xcenter, ycenter, xsize, ysize] for 2D and [xcenter, ycenter, zcenter, xsize, ysize, zsize] for 3D

    We currently define ``StandardMode`` = :class:`~monai.data.box_utils.CornerCornerModeTypeA`,
    and monai detection pipelines mainly assume ``boxes`` are in ``StandardMode``.

    The implementation should be aware of:

    - remember to define class variable ``name``,
      a dictionary that maps ``spatial_dims`` to :class:`~monai.utils.enums.BoxModeName`.
    - :func:`~monai.data.box_utils.BoxMode.boxes_to_corners` and :func:`~monai.data.box_utils.BoxMode.corners_to_boxes`
      should not modify inputs in place.
    """
    name: Dict[int, BoxModeName] = {}

    @classmethod
    def get_name(cls, spatial_dims: int) ->str:
        """
        Get the mode name for the given spatial dimension using class variable ``name``.

        Args:
            spatial_dims: number of spatial dimensions of the bounding boxes.

        Returns:
            ``str``: mode string name
        """
        return cls.name[spatial_dims].value

    @abstractmethod
    def boxes_to_corners(self, boxes: torch.Tensor) ->Tuple:
        """
        Convert the bounding boxes of the current mode to corners.

        Args:
            boxes: bounding boxes, Nx4 or Nx6 torch tensor

        Returns:
            ``Tuple``: corners of boxes, 4-element or 6-element tuple, each element is a Nx1 torch tensor.
            It represents (xmin, ymin, xmax, ymax) or (xmin, ymin, zmin, xmax, ymax, zmax)

        Example:
            .. code-block:: python

                boxes = torch.ones(10,6)
                boxmode = BoxMode()
                boxmode.boxes_to_corners(boxes) # will return a 6-element tuple, each element is a 10x1 tensor
        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')

    @abstractmethod
    def corners_to_boxes(self, corners: Sequence) ->torch.Tensor:
        """
        Convert the given box corners to the bounding boxes of the current mode.

        Args:
            corners: corners of boxes, 4-element or 6-element tuple, each element is a Nx1 torch tensor.
                It represents (xmin, ymin, xmax, ymax) or (xmin, ymin, zmin, xmax, ymax, zmax)

        Returns:
            ``Tensor``: bounding boxes, Nx4 or Nx6 torch tensor

        Example:
            .. code-block:: python

                corners = (torch.ones(10,1), torch.ones(10,1), torch.ones(10,1), torch.ones(10,1))
                boxmode = BoxMode()
                boxmode.corners_to_boxes(corners) # will return a 10x4 tensor
        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')


class CenterSizeMode(BoxMode):
    """
    A subclass of ``BoxMode``.

    Also represented as "ccwh" or "cccwhd", with format of
    [xmin, ymin, xsize, ysize] or [xmin, ymin, zmin, xsize, ysize, zsize].

    Example:
        .. code-block:: python

            CenterSizeMode.get_name(spatial_dims=2) # will return "ccwh"
            CenterSizeMode.get_name(spatial_dims=3) # will return "cccwhd"
    """
    name = {(2): BoxModeName.CCWH, (3): BoxModeName.CCCWHD}

    def boxes_to_corners(self, boxes: torch.Tensor) ->Tuple:
        corners: Tuple
        box_dtype = boxes.dtype
        spatial_dims = get_spatial_dims(boxes=boxes)
        if spatial_dims == 3:
            xc, yc, zc, w, h, d = boxes.split(1, dim=-1)
            xmin = xc - ((w - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            xmax = xc + ((w - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            ymin = yc - ((h - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            ymax = yc + ((h - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            zmin = zc - ((d - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            zmax = zc + ((d - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            corners = xmin, ymin, zmin, xmax, ymax, zmax
        elif spatial_dims == 2:
            xc, yc, w, h = boxes.split(1, dim=-1)
            xmin = xc - ((w - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            xmax = xc + ((w - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            ymin = yc - ((h - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            ymax = yc + ((h - TO_REMOVE) / 2.0).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            corners = xmin, ymin, xmax, ymax
        return corners

    def corners_to_boxes(self, corners: Sequence) ->torch.Tensor:
        boxes: torch.Tensor
        spatial_dims = get_spatial_dims(corners=corners)
        if spatial_dims == 3:
            xmin, ymin, zmin, xmax, ymax, zmax = corners[0], corners[1], corners[2], corners[3], corners[4], corners[5]
            boxes = torch.cat(((xmin + xmax + TO_REMOVE) / 2.0, (ymin + ymax + TO_REMOVE) / 2.0, (zmin + zmax + TO_REMOVE) / 2.0, xmax - xmin + TO_REMOVE, ymax - ymin + TO_REMOVE, zmax - zmin + TO_REMOVE), dim=-1)
        elif spatial_dims == 2:
            xmin, ymin, xmax, ymax = corners[0], corners[1], corners[2], corners[3]
            boxes = torch.cat(((xmin + xmax + TO_REMOVE) / 2.0, (ymin + ymax + TO_REMOVE) / 2.0, xmax - xmin + TO_REMOVE, ymax - ymin + TO_REMOVE), dim=-1)
        return boxes


class CornerCornerModeTypeA(BoxMode):
    """
    A subclass of ``BoxMode``.

    Also represented as "xyxy" or "xyzxyz", with format of
    [xmin, ymin, xmax, ymax] or [xmin, ymin, zmin, xmax, ymax, zmax].

    Example:
        .. code-block:: python

            CornerCornerModeTypeA.get_name(spatial_dims=2) # will return "xyxy"
            CornerCornerModeTypeA.get_name(spatial_dims=3) # will return "xyzxyz"
    """
    name = {(2): BoxModeName.XYXY, (3): BoxModeName.XYZXYZ}

    def boxes_to_corners(self, boxes: torch.Tensor) ->Tuple:
        corners: Tuple
        corners = boxes.split(1, dim=-1)
        return corners

    def corners_to_boxes(self, corners: Sequence) ->torch.Tensor:
        boxes: torch.Tensor
        boxes = torch.cat(tuple(corners), dim=-1)
        return boxes


StandardMode = CornerCornerModeTypeA


class CornerCornerModeTypeB(BoxMode):
    """
    A subclass of ``BoxMode``.

    Also represented as "xxyy" or "xxyyzz", with format of
    [xmin, xmax, ymin, ymax] or [xmin, xmax, ymin, ymax, zmin, zmax].

    Example:
        .. code-block:: python

            CornerCornerModeTypeB.get_name(spatial_dims=2) # will return "xxyy"
            CornerCornerModeTypeB.get_name(spatial_dims=3) # will return "xxyyzz"
    """
    name = {(2): BoxModeName.XXYY, (3): BoxModeName.XXYYZZ}

    def boxes_to_corners(self, boxes: torch.Tensor) ->Tuple:
        corners: Tuple
        spatial_dims = get_spatial_dims(boxes=boxes)
        if spatial_dims == 3:
            xmin, xmax, ymin, ymax, zmin, zmax = boxes.split(1, dim=-1)
            corners = xmin, ymin, zmin, xmax, ymax, zmax
        elif spatial_dims == 2:
            xmin, xmax, ymin, ymax = boxes.split(1, dim=-1)
            corners = xmin, ymin, xmax, ymax
        return corners

    def corners_to_boxes(self, corners: Sequence) ->torch.Tensor:
        boxes: torch.Tensor
        spatial_dims = get_spatial_dims(corners=corners)
        if spatial_dims == 3:
            boxes = torch.cat((corners[0], corners[3], corners[1], corners[4], corners[2], corners[5]), dim=-1)
        elif spatial_dims == 2:
            boxes = torch.cat((corners[0], corners[2], corners[1], corners[3]), dim=-1)
        return boxes


class CornerCornerModeTypeC(BoxMode):
    """
    A subclass of ``BoxMode``.

    Also represented as "xyxy" or "xyxyzz", with format of
    [xmin, ymin, xmax, ymax] or [xmin, ymin, xmax, ymax, zmin, zmax].

    Example:
        .. code-block:: python

            CornerCornerModeTypeC.get_name(spatial_dims=2) # will return "xyxy"
            CornerCornerModeTypeC.get_name(spatial_dims=3) # will return "xyxyzz"
    """
    name = {(2): BoxModeName.XYXY, (3): BoxModeName.XYXYZZ}

    def boxes_to_corners(self, boxes: torch.Tensor) ->Tuple:
        corners: Tuple
        spatial_dims = get_spatial_dims(boxes=boxes)
        if spatial_dims == 3:
            xmin, ymin, xmax, ymax, zmin, zmax = boxes.split(1, dim=-1)
            corners = xmin, ymin, zmin, xmax, ymax, zmax
        elif spatial_dims == 2:
            corners = boxes.split(1, dim=-1)
        return corners

    def corners_to_boxes(self, corners: Sequence) ->torch.Tensor:
        boxes: torch.Tensor
        spatial_dims = get_spatial_dims(corners=corners)
        if spatial_dims == 3:
            boxes = torch.cat((corners[0], corners[1], corners[3], corners[4], corners[2], corners[5]), dim=-1)
        elif spatial_dims == 2:
            boxes = torch.cat(tuple(corners), dim=-1)
        return boxes


class CornerSizeMode(BoxMode):
    """
    A subclass of ``BoxMode``.

    Also represented as "xywh" or "xyzwhd", with format of
    [xmin, ymin, xsize, ysize] or [xmin, ymin, zmin, xsize, ysize, zsize].

    Example:
        .. code-block:: python

            CornerSizeMode.get_name(spatial_dims=2) # will return "xywh"
            CornerSizeMode.get_name(spatial_dims=3) # will return "xyzwhd"
    """
    name = {(2): BoxModeName.XYWH, (3): BoxModeName.XYZWHD}

    def boxes_to_corners(self, boxes: torch.Tensor) ->Tuple:
        corners: Tuple
        box_dtype = boxes.dtype
        spatial_dims = get_spatial_dims(boxes=boxes)
        if spatial_dims == 3:
            xmin, ymin, zmin, w, h, d = boxes.split(1, dim=-1)
            xmax = xmin + (w - TO_REMOVE).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            ymax = ymin + (h - TO_REMOVE).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            zmax = zmin + (d - TO_REMOVE).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            corners = xmin, ymin, zmin, xmax, ymax, zmax
        elif spatial_dims == 2:
            xmin, ymin, w, h = boxes.split(1, dim=-1)
            xmax = xmin + (w - TO_REMOVE).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            ymax = ymin + (h - TO_REMOVE).to(dtype=COMPUTE_DTYPE).clamp(min=0)
            corners = xmin, ymin, xmax, ymax
        return corners

    def corners_to_boxes(self, corners: Sequence) ->torch.Tensor:
        boxes: torch.Tensor
        spatial_dims = get_spatial_dims(corners=corners)
        if spatial_dims == 3:
            xmin, ymin, zmin, xmax, ymax, zmax = corners[0], corners[1], corners[2], corners[3], corners[4], corners[5]
            boxes = torch.cat((xmin, ymin, zmin, xmax - xmin + TO_REMOVE, ymax - ymin + TO_REMOVE, zmax - zmin + TO_REMOVE), dim=-1)
        elif spatial_dims == 2:
            xmin, ymin, xmax, ymax = corners[0], corners[1], corners[2], corners[3]
            boxes = torch.cat((xmin, ymin, xmax - xmin + TO_REMOVE, ymax - ymin + TO_REMOVE), dim=-1)
        return boxes


SUPPORTED_MODES = [CornerCornerModeTypeA, CornerCornerModeTypeB, CornerCornerModeTypeC, CornerSizeMode, CenterSizeMode]


def get_boxmode(mode: Union[str, BoxMode, Type[BoxMode], None]=None, *args, **kwargs) ->BoxMode:
    """
    This function that return a :class:`~monai.data.box_utils.BoxMode` object giving a representation of box mode

    Args:
        mode: a representation of box mode. If it is not given, this func will assume it is ``StandardMode()``.

    Note:
        ``StandardMode`` = :class:`~monai.data.box_utils.CornerCornerModeTypeA`,
        also represented as "xyxy" for 2D and "xyzxyz" for 3D.

        mode can be:
            #. str: choose from :class:`~monai.utils.enums.BoxModeName`, for example,
                - "xyxy": boxes has format [xmin, ymin, xmax, ymax]
                - "xyzxyz": boxes has format [xmin, ymin, zmin, xmax, ymax, zmax]
                - "xxyy": boxes has format [xmin, xmax, ymin, ymax]
                - "xxyyzz": boxes has format [xmin, xmax, ymin, ymax, zmin, zmax]
                - "xyxyzz": boxes has format [xmin, ymin, xmax, ymax, zmin, zmax]
                - "xywh": boxes has format [xmin, ymin, xsize, ysize]
                - "xyzwhd": boxes has format [xmin, ymin, zmin, xsize, ysize, zsize]
                - "ccwh": boxes has format [xcenter, ycenter, xsize, ysize]
                - "cccwhd": boxes has format [xcenter, ycenter, zcenter, xsize, ysize, zsize]
            #. BoxMode class: choose from the subclasses of :class:`~monai.data.box_utils.BoxMode`, for example,
                - CornerCornerModeTypeA: equivalent to "xyxy" or "xyzxyz"
                - CornerCornerModeTypeB: equivalent to "xxyy" or "xxyyzz"
                - CornerCornerModeTypeC: equivalent to "xyxy" or "xyxyzz"
                - CornerSizeMode: equivalent to "xywh" or "xyzwhd"
                - CenterSizeMode: equivalent to "ccwh" or "cccwhd"
            #. BoxMode object: choose from the subclasses of :class:`~monai.data.box_utils.BoxMode`, for example,
                - CornerCornerModeTypeA(): equivalent to "xyxy" or "xyzxyz"
                - CornerCornerModeTypeB(): equivalent to "xxyy" or "xxyyzz"
                - CornerCornerModeTypeC(): equivalent to "xyxy" or "xyxyzz"
                - CornerSizeMode(): equivalent to "xywh" or "xyzwhd"
                - CenterSizeMode(): equivalent to "ccwh" or "cccwhd"
            #. None: will assume mode is ``StandardMode()``

    Returns:
        BoxMode object

    Example:
        .. code-block:: python

            mode = "xyzxyz"
            get_boxmode(mode) # will return CornerCornerModeTypeA()
    """
    if isinstance(mode, BoxMode):
        return mode
    if inspect.isclass(mode) and issubclass(mode, BoxMode):
        return mode(*args, **kwargs)
    if isinstance(mode, str):
        for m in SUPPORTED_MODES:
            for n in SUPPORTED_SPATIAL_DIMS:
                if inspect.isclass(m) and issubclass(m, BoxMode) and m.get_name(n) == mode:
                    return m(*args, **kwargs)
    if mode is not None:
        raise ValueError(f'Unsupported box mode: {mode}.')
    return StandardMode(*args, **kwargs)


def convert_box_mode(boxes: NdarrayOrTensor, src_mode: Union[str, BoxMode, Type[BoxMode], None]=None, dst_mode: Union[str, BoxMode, Type[BoxMode], None]=None) ->NdarrayOrTensor:
    """
    This function converts the boxes in src_mode to the dst_mode.

    Args:
        boxes: source bounding boxes, Nx4 or Nx6 torch tensor or ndarray.
        src_mode: source box mode. If it is not given, this func will assume it is ``StandardMode()``.
            It follows the same format with ``mode`` in :func:`~monai.data.box_utils.get_boxmode`.
        dst_mode: target box mode. If it is not given, this func will assume it is ``StandardMode()``.
            It follows the same format with ``mode`` in :func:`~monai.data.box_utils.get_boxmode`.

    Returns:
        bounding boxes with target mode, with same data type as ``boxes``, does not share memory with ``boxes``

    Example:
        .. code-block:: python

            boxes = torch.ones(10,4)
            # The following three lines are equivalent
            # They convert boxes with format [xmin, ymin, xmax, ymax] to [xcenter, ycenter, xsize, ysize].
            convert_box_mode(boxes=boxes, src_mode="xyxy", dst_mode="ccwh")
            convert_box_mode(boxes=boxes, src_mode="xyxy", dst_mode=monai.data.box_utils.CenterSizeMode)
            convert_box_mode(boxes=boxes, src_mode="xyxy", dst_mode=monai.data.box_utils.CenterSizeMode())
    """
    src_boxmode = get_boxmode(src_mode)
    dst_boxmode = get_boxmode(dst_mode)
    if isinstance(src_boxmode, type(dst_boxmode)):
        return deepcopy(boxes)
    boxes_t, *_ = convert_data_type(boxes, torch.Tensor)
    corners = src_boxmode.boxes_to_corners(boxes_t)
    spatial_dims = get_spatial_dims(boxes=boxes_t)
    for axis in range(0, spatial_dims):
        if (corners[spatial_dims + axis] < corners[axis]).sum() > 0:
            warnings.warn('Given boxes has invalid values. The box size must be non-negative.')
    boxes_t_dst = dst_boxmode.corners_to_boxes(corners)
    boxes_dst, *_ = convert_to_dst_type(src=boxes_t_dst, dst=boxes)
    return boxes_dst


def box_centers(boxes: NdarrayOrTensor) ->NdarrayOrTensor:
    """
    Compute center points of boxes

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``

    Returns:
        center points with size of (N, spatial_dims)

    """
    spatial_dims = get_spatial_dims(boxes=boxes)
    return convert_box_mode(boxes=boxes, src_mode=StandardMode, dst_mode=CenterSizeMode)[:, :spatial_dims]


def boxes_center_distance(boxes1: NdarrayOrTensor, boxes2: NdarrayOrTensor, euclidean: bool=True) ->Tuple[NdarrayOrTensor, NdarrayOrTensor, NdarrayOrTensor]:
    """
    Distance of center points between two sets of boxes

    Args:
        boxes1: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        boxes2: bounding boxes, Mx4 or Mx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        euclidean: computed the euclidean distance otherwise it uses the l1 distance

    Returns:
        - The pairwise distances for every element in boxes1 and boxes2,
          with size of (N,M) and same data type as ``boxes1``.
        - Center points of boxes1, with size of (N,spatial_dims) and same data type as ``boxes1``.
        - Center points of boxes2, with size of (M,spatial_dims) and same data type as ``boxes1``.

    Reference:
        https://github.com/MIC-DKFZ/nnDetection/blob/main/nndet/core/boxes/ops.py

    """
    if not isinstance(boxes1, type(boxes2)):
        warnings.warn(f'boxes1 is {type(boxes1)}, while boxes2 is {type(boxes2)}. The result will be {type(boxes1)}.')
    boxes1_t, *_ = convert_data_type(boxes1, torch.Tensor)
    boxes2_t, *_ = convert_data_type(boxes2, torch.Tensor)
    center1 = box_centers(boxes1_t)
    center2 = box_centers(boxes2_t)
    if euclidean:
        dists = (center1[:, None] - center2[None]).pow(2).sum(-1).sqrt()
    else:
        dists = (center1[:, None] - center2[None]).sum(-1)
    (dists, center1, center2), *_ = convert_to_dst_type(src=(dists, center1, center2), dst=boxes1)
    return dists, center1, center2


def centers_in_boxes(centers: NdarrayOrTensor, boxes: NdarrayOrTensor, eps: float=0.01) ->NdarrayOrTensor:
    """
    Checks which center points are within boxes

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``.
        centers: center points, Nx2 or Nx3 torch tensor or ndarray.
        eps: minimum distance to border of boxes.

    Returns:
        boolean array indicating which center points are within the boxes, sized (N,).

    Reference:
        https://github.com/MIC-DKFZ/nnDetection/blob/main/nndet/core/boxes/ops.py

    """
    spatial_dims = get_spatial_dims(boxes=boxes)
    center_to_border = [(centers[:, axis] - boxes[:, axis]) for axis in range(spatial_dims)] + [(boxes[:, axis + spatial_dims] - centers[:, axis]) for axis in range(spatial_dims)]
    if isinstance(boxes, np.ndarray):
        min_center_to_border: np.ndarray = np.stack(center_to_border, axis=1).min(axis=1)
        return min_center_to_border > eps
    return torch.stack(center_to_border, dim=1).min(dim=1)[0] > eps


class ATSSMatcher(Matcher):

    def __init__(self, num_candidates: int=4, similarity_fn: Callable[[Tensor, Tensor], Tensor]=box_iou, center_in_gt: bool=True, debug: bool=False):
        """
        Compute matching based on ATSS https://arxiv.org/abs/1912.02424
        `Bridging the Gap Between Anchor-based and Anchor-free Detection
        via Adaptive Training Sample Selection`

        Args:
            num_candidates: number of positions to select candidates from.
                Smaller value will result in a higher matcher threshold and less matched candidates.
            similarity_fn: function for similarity computation between boxes and anchors
            center_in_gt: If False (default), matched anchor center points do not need
                to lie withing the ground truth box. Recommend False for small objects.
                If True, will result in a strict matcher and less matched candidates.
            debug: if True, will print the matcher threshold in order to
                tune ``num_candidates`` and ``center_in_gt``.
        """
        super().__init__(similarity_fn=similarity_fn)
        self.num_candidates = num_candidates
        self.min_dist = 0.01
        self.center_in_gt = center_in_gt
        self.debug = debug
        logging.info(f'Running ATSS Matching with num_candidates={self.num_candidates} and center_in_gt {self.center_in_gt}.')

    def compute_matches(self, boxes: torch.Tensor, anchors: torch.Tensor, num_anchors_per_level: Sequence[int], num_anchors_per_loc: int) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Compute matches according to ATTS for a single image
        Adapted from
        (https://github.com/sfzhang15/ATSS/blob/79dfb28bd1/atss_core/modeling/rpn/atss/loss.py#L180-L184)

        Args:
            boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
            anchors: anchors to match Mx4 or Mx6, also assumed to be ``StandardMode``.
            num_anchors_per_level: number of anchors per feature pyramid level
            num_anchors_per_loc: number of anchors per position

        Returns:
            - matrix which contains the similarity from each boxes to each anchor [N, M]
            - vector which contains the matched box index for all
              anchors (if background `BELOW_LOW_THRESHOLD` is used
              and if it should be ignored `BETWEEN_THRESHOLDS` is used) [M]

        Note:
            ``StandardMode`` = :class:`~monai.data.box_utils.CornerCornerModeTypeA`,
            also represented as "xyxy" ([xmin, ymin, xmax, ymax]) for 2D
            and "xyzxyz" ([xmin, ymin, zmin, xmax, ymax, zmax]) for 3D.
        """
        num_gt = boxes.shape[0]
        num_anchors = anchors.shape[0]
        distances_, _, anchors_center = boxes_center_distance(boxes, anchors)
        distances = convert_to_tensor(distances_)
        candidate_idx_list = []
        start_idx = 0
        for _, apl in enumerate(num_anchors_per_level):
            end_idx = start_idx + apl * num_anchors_per_loc
            topk = min(self.num_candidates * num_anchors_per_loc, apl)
            _, idx = distances[:, start_idx:end_idx].topk(topk, dim=1, largest=False)
            candidate_idx_list.append(idx + start_idx)
            start_idx = end_idx
        candidate_idx = torch.cat(candidate_idx_list, dim=1)
        match_quality_matrix = self.similarity_fn(boxes, anchors)
        candidate_ious = match_quality_matrix.gather(1, candidate_idx)
        if candidate_idx.shape[1] <= 1:
            matches = -1 * torch.ones((num_anchors,), dtype=torch.long, device=boxes.device)
            matches[candidate_idx] = 0
            return match_quality_matrix, matches
        iou_mean_per_gt = candidate_ious.mean(dim=1)
        iou_std_per_gt = candidate_ious.std(dim=1)
        iou_thresh_per_gt = iou_mean_per_gt + iou_std_per_gt
        is_pos = candidate_ious >= iou_thresh_per_gt[:, None]
        if self.debug:
            None
        if self.center_in_gt:
            boxes_idx = torch.arange(num_gt, device=boxes.device, dtype=torch.long)[:, None].expand_as(candidate_idx).contiguous()
            is_in_gt_ = centers_in_boxes(anchors_center[candidate_idx.view(-1)], boxes[boxes_idx.view(-1)], eps=self.min_dist)
            is_in_gt = convert_to_tensor(is_in_gt_)
            is_pos = is_pos & is_in_gt.view_as(is_pos)
        for ng in range(num_gt):
            candidate_idx[ng, :] += ng * num_anchors
        ious_inf = torch.full_like(match_quality_matrix, -INF).view(-1)
        index = candidate_idx.view(-1)[is_pos.view(-1)]
        ious_inf[index] = match_quality_matrix.view(-1)[index]
        ious_inf = ious_inf.view_as(match_quality_matrix)
        matched_vals, matches = ious_inf.max(dim=0)
        matches[matched_vals == -INF] = self.BELOW_LOW_THRESHOLD
        return match_quality_matrix, matches


def issequenceiterable(obj: Any) ->bool:
    """
    Determine if the object is an iterable sequence and is not a string.
    """
    try:
        if hasattr(obj, 'ndim') and obj.ndim == 0:
            return False
    except Exception:
        return False
    return isinstance(obj, Iterable) and not isinstance(obj, (str, bytes))


def ensure_tuple(vals: Any, wrap_array: bool=False) ->Tuple[Any, ...]:
    """
    Returns a tuple of `vals`.

    Args:
        vals: input data to convert to a tuple.
        wrap_array: if `True`, treat the input numerical array (ndarray/tensor) as one item of the tuple.
            if `False`, try to convert the array with `tuple(vals)`, default to `False`.

    """
    if wrap_array and isinstance(vals, (np.ndarray, torch.Tensor)):
        return vals,
    return tuple(vals) if issequenceiterable(vals) else (vals,)


class AnchorGenerator(nn.Module):
    """
    This module is modified from torchvision to support both 2D and 3D images.

    Module that generates anchors for a set of feature maps and
    image sizes.

    The module support computing anchors at multiple sizes and aspect ratios
    per feature map.

    sizes and aspect_ratios should have the same number of elements, and it should
    correspond to the number of feature maps.

    sizes[i] and aspect_ratios[i] can have an arbitrary number of elements.
    For 2D images, anchor width and height w:h = 1:aspect_ratios[i,j]
    For 3D images, anchor width, height, and depth w:h:d = 1:aspect_ratios[i,j,0]:aspect_ratios[i,j,1]

    AnchorGenerator will output a set of sizes[i] * aspect_ratios[i] anchors
    per spatial location for feature map i.

    Args:
        sizes: base size of each anchor.
            len(sizes) is the number of feature maps, i.e., the number of output levels for
            the feature pyramid network (FPN).
            Each element of ``sizes`` is a Sequence which represents several anchor sizes for each feature map.
        aspect_ratios: the aspect ratios of anchors. ``len(aspect_ratios) = len(sizes)``.
            For 2D images, each element of ``aspect_ratios[i]`` is a Sequence of float.
            For 3D images, each element of ``aspect_ratios[i]`` is a Sequence of 2 value Sequence.
        indexing: choose from {``'ij'``, ``'xy'``}, optional,
            Matrix (``'ij'``, default and recommended) or Cartesian (``'xy'``) indexing of output.

            - Matrix (``'ij'``, default and recommended) indexing keeps the original axis not changed.
            - To use other monai detection components, please set ``indexing = 'ij'``.
            - Cartesian (``'xy'``) indexing swaps axis 0 and 1.
            - For 2D cases, monai ``AnchorGenerator(sizes, aspect_ratios, indexing='xy')`` and
              ``torchvision.models.detection.anchor_utils.AnchorGenerator(sizes, aspect_ratios)`` are equivalent.


    Reference:.
        https://github.com/pytorch/vision/blob/release/0.12/torchvision/models/detection/anchor_utils.py

    Example:
        .. code-block:: python

            # 2D example inputs for a 2-level feature maps
            sizes = ((10,12,14,16), (20,24,28,32))
            base_aspect_ratios = (1., 0.5,  2.)
            aspect_ratios = (base_aspect_ratios, base_aspect_ratios)
            anchor_generator = AnchorGenerator(sizes, aspect_ratios)

            # 3D example inputs for a 2-level feature maps
            sizes = ((10,12,14,16), (20,24,28,32))
            base_aspect_ratios = ((1., 1.), (1., 0.5), (0.5, 1.), (2., 2.))
            aspect_ratios = (base_aspect_ratios, base_aspect_ratios)
            anchor_generator = AnchorGenerator(sizes, aspect_ratios)
    """
    __annotations__ = {'cell_anchors': List[torch.Tensor]}

    def __init__(self, sizes: Sequence[Sequence[int]]=((20, 30, 40),), aspect_ratios: Sequence=(((0.5, 1), (1, 0.5)),), indexing: str='ij') ->None:
        super().__init__()
        if not issequenceiterable(sizes[0]):
            self.sizes = tuple((s,) for s in sizes)
        else:
            self.sizes = ensure_tuple(sizes)
        if not issequenceiterable(aspect_ratios[0]):
            aspect_ratios = (aspect_ratios,) * len(self.sizes)
        if len(self.sizes) != len(aspect_ratios):
            raise ValueError('len(sizes) and len(aspect_ratios) should be equal.                 It represents the number of feature maps.')
        spatial_dims = len(ensure_tuple(aspect_ratios[0][0])) + 1
        spatial_dims = look_up_option(spatial_dims, [2, 3])
        self.spatial_dims = spatial_dims
        self.indexing = look_up_option(indexing, ['ij', 'xy'])
        self.aspect_ratios = aspect_ratios
        self.cell_anchors = [self.generate_anchors(size, aspect_ratio) for size, aspect_ratio in zip(self.sizes, aspect_ratios)]

    def generate_anchors(self, scales: Sequence, aspect_ratios: Sequence, dtype: torch.dtype=torch.float32, device: Union[torch.device, None]=None) ->torch.Tensor:
        """
        Compute cell anchor shapes at multiple sizes and aspect ratios for the current feature map.

        Args:
            scales: a sequence which represents several anchor sizes for the current feature map.
            aspect_ratios: a sequence which represents several aspect_ratios for the current feature map.
                For 2D images, it is a Sequence of float aspect_ratios[j],
                anchor width and height w:h = 1:aspect_ratios[j].
                For 3D images, it is a Sequence of 2 value Sequence aspect_ratios[j,0] and aspect_ratios[j,1],
                anchor width, height, and depth w:h:d = 1:aspect_ratios[j,0]:aspect_ratios[j,1]
            dtype: target data type of the output Tensor.
            device: target device to put the output Tensor data.

            Returns:
                For each s in scales, returns [s, s*aspect_ratios[j]] for 2D images,
                and [s, s*aspect_ratios[j,0],s*aspect_ratios[j,1]] for 3D images.
        """
        scales_t = torch.as_tensor(scales, dtype=dtype, device=device)
        aspect_ratios_t = torch.as_tensor(aspect_ratios, dtype=dtype, device=device)
        if self.spatial_dims >= 3 and len(aspect_ratios_t.shape) != 2:
            raise ValueError(f'In {self.spatial_dims}-D image, aspect_ratios for each level should be                 {len(aspect_ratios_t.shape) - 1}-D. But got aspect_ratios with shape {aspect_ratios_t.shape}.')
        if self.spatial_dims >= 3 and aspect_ratios_t.shape[1] != self.spatial_dims - 1:
            raise ValueError(f'In {self.spatial_dims}-D image, aspect_ratios for each level should has                 shape (_,{self.spatial_dims - 1}). But got aspect_ratios with shape {aspect_ratios_t.shape}.')
        if self.spatial_dims == 2:
            area_scale = torch.sqrt(aspect_ratios_t)
            w_ratios = 1 / area_scale
            h_ratios = area_scale
        elif self.spatial_dims == 3:
            area_scale = torch.pow(aspect_ratios_t[:, 0] * aspect_ratios_t[:, 1], 1 / 3.0)
            w_ratios = 1 / area_scale
            h_ratios = aspect_ratios_t[:, 0] / area_scale
            d_ratios = aspect_ratios_t[:, 1] / area_scale
        ws = (w_ratios[:, None] * scales_t[None, :]).view(-1)
        hs = (h_ratios[:, None] * scales_t[None, :]).view(-1)
        if self.spatial_dims == 2:
            base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2.0
        elif self.spatial_dims == 3:
            ds = (d_ratios[:, None] * scales_t[None, :]).view(-1)
            base_anchors = torch.stack([-ws, -hs, -ds, ws, hs, ds], dim=1) / 2.0
        return base_anchors.round()

    def set_cell_anchors(self, dtype: torch.dtype, device: torch.device):
        """
        Convert each element in self.cell_anchors to ``dtype`` and send to ``device``.
        """
        self.cell_anchors = [cell_anchor for cell_anchor in self.cell_anchors]

    def num_anchors_per_location(self):
        """
        Return number of anchor shapes for each feature map.
        """
        return [c.shape[0] for c in self.cell_anchors]

    def grid_anchors(self, grid_sizes: List[List[int]], strides: List[List[Tensor]]) ->List[Tensor]:
        """
        Every combination of (a, (g, s), i) in (self.cell_anchors, zip(grid_sizes, strides), 0:spatial_dims)
        corresponds to a feature map.
        It outputs g[i] anchors that are s[i] distance apart in direction i, with the same dimensions as a.

        Args:
            grid_sizes: spatial size of the feature maps
            strides: strides of the feature maps regarding to the original image

        Example:
            .. code-block:: python

                grid_sizes = [[100,100],[50,50]]
                strides = [[torch.tensor(2),torch.tensor(2)], [torch.tensor(4),torch.tensor(4)]]
        """
        anchors = []
        cell_anchors = self.cell_anchors
        if cell_anchors is None:
            raise AssertionError
        if not len(grid_sizes) == len(strides) == len(cell_anchors):
            raise ValueError('Anchors should be Tuple[Tuple[int]] because each feature map could potentially have different sizes and aspect ratios. There needs to be a match between the number of feature maps passed and the number of sizes / aspect ratios specified.')
        for size, stride, base_anchors in zip(grid_sizes, strides, cell_anchors):
            device = base_anchors.device
            shifts_centers = [(torch.arange(0, size[axis], dtype=torch.int32, device=device) * stride[axis]) for axis in range(self.spatial_dims)]
            shifts_centers = list(torch.meshgrid(shifts_centers[:self.spatial_dims], indexing='ij'))
            for axis in range(self.spatial_dims):
                shifts_centers[axis] = shifts_centers[axis].reshape(-1)
            if self.indexing == 'xy':
                shifts_centers[1], shifts_centers[0] = shifts_centers[0], shifts_centers[1]
            shifts = torch.stack(shifts_centers * 2, dim=1)
            anchors.append((shifts.view(-1, 1, self.spatial_dims * 2) + base_anchors.view(1, -1, self.spatial_dims * 2)).reshape(-1, self.spatial_dims * 2))
        return anchors

    def forward(self, images: Tensor, feature_maps: List[Tensor]) ->List[Tensor]:
        """
        Generate anchor boxes for each image.

        Args:
            images: sized (B, C, W, H) or (B, C, W, H, D)
            feature_maps: for FPN level i, feature_maps[i] is sized (B, C_i, W_i, H_i) or (B, C_i, W_i, H_i, D_i).
                This input argument does not have to be the actual feature maps.
                Any list variable with the same (C_i, W_i, H_i) or (C_i, W_i, H_i, D_i) as feature maps works.

        Return:
            A list with length of B. Each element represents the anchors for this image.
            The B elements are identical.

        Example:
            .. code-block:: python

                images = torch.zeros((3,1,128,128,128))
                feature_maps = [torch.zeros((3,6,64,64,32)), torch.zeros((3,6,32,32,16))]
                anchor_generator(images, feature_maps)
        """
        grid_sizes = [list(feature_map.shape[-self.spatial_dims:]) for feature_map in feature_maps]
        image_size = images.shape[-self.spatial_dims:]
        batchsize = images.shape[0]
        dtype, device = feature_maps[0].dtype, feature_maps[0].device
        strides = [[torch.tensor(image_size[axis] // g[axis], dtype=torch.int64, device=device) for axis in range(self.spatial_dims)] for g in grid_sizes]
        self.set_cell_anchors(dtype, device)
        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes, strides)
        anchors_per_image = torch.cat(list(anchors_over_all_feature_maps))
        return [anchors_per_image] * batchsize


BalancedPositiveNegativeSampler, _ = optional_import('torchvision.models.detection._utils', name='BalancedPositiveNegativeSampler')


class BlendMode(StrEnum):
    """
    See also: :py:class:`monai.data.utils.compute_importance_map`
    """
    CONSTANT = 'constant'
    GAUSSIAN = 'gaussian'


def encode_boxes(gt_boxes: Tensor, proposals: Tensor, weights: Tensor) ->Tensor:
    """
    Encode a set of proposals with respect to some reference ground truth (gt) boxes.

    Args:
        gt_boxes: gt boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
        proposals: boxes to be encoded, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
        weights: the weights for ``(cx, cy, w, h) or (cx,cy,cz, w,h,d)``

    Return:
        encoded gt, target of box regression that is used to convert proposals into gt_boxes, Nx4 or Nx6 torch tensor.
    """
    if gt_boxes.shape[0] != proposals.shape[0]:
        raise ValueError('gt_boxes.shape[0] should be equal to proposals.shape[0].')
    spatial_dims = look_up_option(len(weights), [4, 6]) // 2
    if not is_valid_box_values(gt_boxes):
        raise ValueError('gt_boxes is not valid. Please check if it contains empty boxes.')
    if not is_valid_box_values(proposals):
        raise ValueError('proposals is not valid. Please check if it contains empty boxes.')
    ex_cccwhd: Tensor = convert_box_mode(proposals, src_mode=StandardMode, dst_mode=CenterSizeMode)
    gt_cccwhd: Tensor = convert_box_mode(gt_boxes, src_mode=StandardMode, dst_mode=CenterSizeMode)
    targets_dxyz = weights[:spatial_dims].unsqueeze(0) * (gt_cccwhd[:, :spatial_dims] - ex_cccwhd[:, :spatial_dims]) / ex_cccwhd[:, spatial_dims:]
    targets_dwhd = weights[spatial_dims:].unsqueeze(0) * torch.log(gt_cccwhd[:, spatial_dims:] / ex_cccwhd[:, spatial_dims:])
    targets = torch.cat((targets_dxyz, targets_dwhd), dim=1)
    if torch.isnan(targets).any() or torch.isinf(targets).any():
        raise ValueError('targets is NaN or Inf.')
    return targets


class BoxCoder:
    """
    This class encodes and decodes a set of bounding boxes into
    the representation used for training the regressors.

    Args:
        weights: 4-element tuple or 6-element tuple
        boxes_xform_clip: high threshold to prevent sending too large values into torch.exp()

    Example:
        .. code-block:: python

            box_coder = BoxCoder(weights=[1., 1., 1., 1., 1., 1.])
            gt_boxes = torch.tensor([[1,2,1,4,5,6],[1,3,2,7,8,9]])
            proposals = gt_boxes + torch.rand(gt_boxes.shape)
            rel_gt_boxes = box_coder.encode_single(gt_boxes, proposals)
            gt_back = box_coder.decode_single(rel_gt_boxes, proposals)
            # We expect gt_back to be equal to gt_boxes
    """

    def __init__(self, weights: Tuple[float], boxes_xform_clip: Union[float, None]=None) ->None:
        if boxes_xform_clip is None:
            boxes_xform_clip = math.log(1000.0 / 16)
        self.spatial_dims = look_up_option(len(weights), [4, 6]) // 2
        self.weights = weights
        self.boxes_xform_clip = boxes_xform_clip

    def encode(self, gt_boxes: Sequence[Tensor], proposals: Sequence[Tensor]) ->Tuple[Tensor]:
        """
        Encode a set of proposals with respect to some ground truth (gt) boxes.

        Args:
            gt_boxes: list of gt boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
            proposals: list of boxes to be encoded, each element is Mx4 or Mx6 torch tensor.
                The box mode is assumed to be ``StandardMode``

        Return:
            A tuple of encoded gt, target of box regression that is used to
                convert proposals into gt_boxes, Nx4 or Nx6 torch tensor.
        """
        boxes_per_image = [len(b) for b in gt_boxes]
        concat_gt_boxes = torch.cat(tuple(gt_boxes), dim=0)
        concat_proposals = torch.cat(tuple(proposals), dim=0)
        concat_targets = self.encode_single(concat_gt_boxes, concat_proposals)
        targets: Tuple[Tensor] = concat_targets.split(boxes_per_image, 0)
        return targets

    def encode_single(self, gt_boxes: Tensor, proposals: Tensor) ->Tensor:
        """
        Encode proposals with respect to ground truth (gt) boxes.

        Args:
            gt_boxes: gt boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
            proposals: boxes to be encoded, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``

        Return:
            encoded gt, target of box regression that is used to convert proposals into gt_boxes, Nx4 or Nx6 torch tensor.
        """
        dtype = gt_boxes.dtype
        device = gt_boxes.device
        weights = torch.as_tensor(self.weights, dtype=dtype, device=device)
        targets = encode_boxes(gt_boxes, proposals, weights)
        return targets

    def decode(self, rel_codes: Tensor, reference_boxes: Sequence[Tensor]) ->Tensor:
        """
        From a set of original reference_boxes and encoded relative box offsets,

        Args:
            rel_codes: encoded boxes, Nx4 or Nx6 torch tensor.
            reference_boxes: a list of reference boxes, each element is Mx4 or Mx6 torch tensor.
                The box mode is assumed to be ``StandardMode``

        Return:
            decoded boxes, Nx1x4 or Nx1x6 torch tensor. The box mode will be ``StandardMode``
        """
        if not isinstance(reference_boxes, Sequence) or not isinstance(rel_codes, torch.Tensor):
            raise ValueError('Input arguments wrong type.')
        boxes_per_image = [b.size(0) for b in reference_boxes]
        concat_boxes = torch.cat(tuple(reference_boxes), dim=0)
        box_sum = 0
        for val in boxes_per_image:
            box_sum += val
        if box_sum > 0:
            rel_codes = rel_codes.reshape(box_sum, -1)
        pred_boxes = self.decode_single(rel_codes, concat_boxes)
        if box_sum > 0:
            pred_boxes = pred_boxes.reshape(box_sum, -1, 2 * self.spatial_dims)
        return pred_boxes

    def decode_single(self, rel_codes: Tensor, reference_boxes: Tensor) ->Tensor:
        """
        From a set of original boxes and encoded relative box offsets,

        Args:
            rel_codes: encoded boxes, Nx(4*num_box_reg) or Nx(6*num_box_reg) torch tensor.
            reference_boxes: reference boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``

        Return:
            decoded boxes, Nx(4*num_box_reg) or Nx(6*num_box_reg) torch tensor. The box mode will to be ``StandardMode``
        """
        reference_boxes = reference_boxes
        offset = reference_boxes.shape[-1]
        pred_boxes = []
        boxes_cccwhd = convert_box_mode(reference_boxes, src_mode=StandardMode, dst_mode=CenterSizeMode)
        for axis in range(self.spatial_dims):
            whd_axis = boxes_cccwhd[:, axis + self.spatial_dims]
            ctr_xyz_axis = boxes_cccwhd[:, axis]
            dxyz_axis = rel_codes[:, axis::offset] / self.weights[axis]
            dwhd_axis = rel_codes[:, self.spatial_dims + axis::offset] / self.weights[axis + self.spatial_dims]
            dwhd_axis = torch.clamp(dwhd_axis, max=self.boxes_xform_clip)
            pred_ctr_xyx_axis = dxyz_axis * whd_axis[:, None] + ctr_xyz_axis[:, None]
            pred_whd_axis = torch.exp(dwhd_axis) * whd_axis[:, None]
            pred_whd_axis = pred_whd_axis
            if torch.isnan(pred_whd_axis).any() or torch.isinf(pred_whd_axis).any():
                raise ValueError('pred_whd_axis is NaN or Inf.')
            c_to_c_whd_axis = torch.tensor(0.5, dtype=pred_ctr_xyx_axis.dtype, device=pred_whd_axis.device) * pred_whd_axis
            pred_boxes.append(pred_ctr_xyx_axis - c_to_c_whd_axis)
            pred_boxes.append(pred_ctr_xyx_axis + c_to_c_whd_axis)
        pred_boxes = pred_boxes[::2] + pred_boxes[1::2]
        pred_boxes_final = torch.stack(pred_boxes, dim=2).flatten(1)
        return pred_boxes_final


def non_max_suppression(boxes: NdarrayOrTensor, scores: NdarrayOrTensor, nms_thresh: float, max_proposals: int=-1, box_overlap_metric: Callable=box_iou) ->NdarrayOrTensor:
    """
    Non-maximum suppression (NMS).

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        scores: prediction scores of the boxes, sized (N,). This function keeps boxes with higher scores.
        nms_thresh: threshold of NMS. Discards all overlapping boxes with box_overlap > nms_thresh.
        max_proposals: maximum number of boxes it keeps.
            If ``max_proposals`` = -1, there is no limit on the number of boxes that are kept.
        box_overlap_metric: the metric to compute overlap between boxes.

    Returns:
        Indexes of ``boxes`` that are kept after NMS.

    Example:
        .. code-block:: python

            boxes = torch.ones(10,6)
            scores = torch.ones(10)
            keep = non_max_suppression(boxes, scores, num_thresh=0.1)
            boxes_after_nms = boxes[keep]
    """
    if boxes.shape[0] == 0:
        return convert_to_dst_type(src=np.array([]), dst=boxes, dtype=torch.long)[0]
    if boxes.shape[0] != scores.shape[0]:
        raise ValueError(f'boxes and scores should have same length, got boxes shape {boxes.shape}, scores shape {scores.shape}')
    boxes_t, *_ = convert_data_type(boxes, torch.Tensor)
    scores_t, *_ = convert_to_dst_type(scores, boxes_t)
    sort_idxs = torch.argsort(scores_t, dim=0, descending=True)
    boxes_sort = deepcopy(boxes_t)[sort_idxs, :]
    pick = []
    idxs = torch.Tensor(list(range(0, boxes_sort.shape[0])))
    while len(idxs) > 0:
        i = int(idxs[0].item())
        pick.append(i)
        if len(pick) >= max_proposals >= 1:
            break
        box_overlap = box_overlap_metric(boxes_sort[idxs, :], boxes_sort[i:i + 1, :])
        to_keep_idx = (box_overlap <= nms_thresh).flatten()
        to_keep_idx[0] = False
        idxs = idxs[to_keep_idx]
    pick_idx = sort_idxs[pick]
    return convert_to_dst_type(src=pick_idx, dst=boxes, dtype=pick_idx.dtype)[0]


def batched_nms(boxes: NdarrayOrTensor, scores: NdarrayOrTensor, labels: NdarrayOrTensor, nms_thresh: float, max_proposals: int=-1, box_overlap_metric: Callable=box_iou) ->NdarrayOrTensor:
    """
    Performs non-maximum suppression in a batched fashion.
    Each labels value correspond to a category, and NMS will not be applied between elements of different categories.

    Adapted from https://github.com/MIC-DKFZ/nnDetection/blob/main/nndet/core/boxes/nms.py

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        scores: prediction scores of the boxes, sized (N,). This function keeps boxes with higher scores.
        labels: indices of the categories for each one of the boxes. sized(N,), value range is (0, num_classes)
        nms_thresh: threshold of NMS. Discards all overlapping boxes with box_overlap > nms_thresh.
        max_proposals: maximum number of boxes it keeps.
            If ``max_proposals`` = -1, there is no limit on the number of boxes that are kept.
        box_overlap_metric: the metric to compute overlap between boxes.

    Returns:
        Indexes of ``boxes`` that are kept after NMS.
    """
    if boxes.shape[0] == 0:
        return convert_to_dst_type(src=np.array([]), dst=boxes, dtype=torch.long)[0]
    boxes_t, *_ = convert_data_type(boxes, torch.Tensor, dtype=torch.float32)
    scores_t, *_ = convert_to_dst_type(scores, boxes_t)
    labels_t, *_ = convert_to_dst_type(labels, boxes_t, dtype=torch.long)
    max_coordinate = boxes_t.max()
    offsets = labels_t * (max_coordinate + 1)
    boxes_for_nms = boxes + offsets[:, None]
    keep = non_max_suppression(boxes_for_nms, scores_t, nms_thresh, max_proposals, box_overlap_metric)
    return convert_to_dst_type(src=keep, dst=boxes, dtype=keep.dtype)[0]


def spatial_crop_boxes(boxes: NdarrayOrTensor, roi_start: Union[Sequence[int], NdarrayOrTensor], roi_end: Union[Sequence[int], NdarrayOrTensor], remove_empty: bool=True) ->Tuple[NdarrayOrTensor, NdarrayOrTensor]:
    """
    This function generate the new boxes when the corresponding image is cropped to the given ROI.
    When ``remove_empty=True``, it makes sure the bounding boxes are within the new cropped image.

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        roi_start: voxel coordinates for start of the crop ROI, negative values allowed.
        roi_end: voxel coordinates for end of the crop ROI, negative values allowed.
        remove_empty: whether to remove the boxes that are actually empty

    Returns:
        - cropped boxes, boxes[keep], does not share memory with original boxes
        - ``keep``, it indicates whether each box in ``boxes`` are kept when ``remove_empty=True``.
    """
    boxes_t = convert_data_type(boxes, torch.Tensor)[0].clone()
    boxes_t = boxes_t
    roi_start_t = convert_to_dst_type(src=roi_start, dst=boxes_t, wrap_sequence=True)[0]
    roi_end_t = convert_to_dst_type(src=roi_end, dst=boxes_t, wrap_sequence=True)[0]
    roi_end_t = torch.maximum(roi_end_t, roi_start_t)
    spatial_dims = get_spatial_dims(boxes=boxes, spatial_size=roi_end)
    for axis in range(0, spatial_dims):
        boxes_t[:, axis] = boxes_t[:, axis].clamp(min=roi_start_t[axis], max=roi_end_t[axis] - TO_REMOVE)
        boxes_t[:, axis + spatial_dims] = boxes_t[:, axis + spatial_dims].clamp(min=roi_start_t[axis], max=roi_end_t[axis] - TO_REMOVE)
        boxes_t[:, axis] -= roi_start_t[axis]
        boxes_t[:, axis + spatial_dims] -= roi_start_t[axis]
    if remove_empty:
        keep_t = boxes_t[:, spatial_dims] >= boxes_t[:, 0] + 1 - TO_REMOVE
        for axis in range(1, spatial_dims):
            keep_t = keep_t & (boxes_t[:, axis + spatial_dims] >= boxes_t[:, axis] + 1 - TO_REMOVE)
        boxes_t = boxes_t[keep_t]
    else:
        keep_t = torch.full_like(boxes_t[:, 0], fill_value=True, dtype=torch.bool)
    boxes_keep, *_ = convert_to_dst_type(src=boxes_t, dst=boxes)
    keep, *_ = convert_to_dst_type(src=keep_t, dst=boxes, dtype=keep_t.dtype)
    return boxes_keep, keep


def clip_boxes_to_image(boxes: NdarrayOrTensor, spatial_size: Union[Sequence[int], NdarrayOrTensor], remove_empty: bool=True) ->Tuple[NdarrayOrTensor, NdarrayOrTensor]:
    """
    This function clips the ``boxes`` to makes sure the bounding boxes are within the image.

    Args:
        boxes: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        spatial_size: The spatial size of the image where the boxes are attached. len(spatial_size) should be in [2, 3].
        remove_empty: whether to remove the boxes that are actually empty

    Returns:
        - clipped boxes, boxes[keep], does not share memory with original boxes
        - ``keep``, it indicates whether each box in ``boxes`` are kept when ``remove_empty=True``.
    """
    spatial_dims = get_spatial_dims(boxes=boxes, spatial_size=spatial_size)
    return spatial_crop_boxes(boxes, roi_start=[0] * spatial_dims, roi_end=spatial_size, remove_empty=remove_empty)


def version_leq(lhs: str, rhs: str):
    """
    Returns True if version `lhs` is earlier or equal to `rhs`.

    Args:
        lhs: version name to compare with `rhs`, return True if earlier or equal to `rhs`.
        rhs: version name to compare with `lhs`, return True if later or equal to `lhs`.

    """
    lhs, rhs = str(lhs), str(rhs)
    pkging, has_ver = optional_import('pkg_resources', name='packaging')
    if has_ver:
        try:
            return pkging.version.Version(lhs) <= pkging.version.Version(rhs)
        except pkging.version.InvalidVersion:
            return True

    def _try_cast(val: str):
        val = val.strip()
        try:
            m = match('(\\d+)(.*)', val)
            if m is not None:
                val = m.groups()[0]
                return int(val)
            return val
        except ValueError:
            return val
    lhs = lhs.split('+', 1)[0]
    rhs = rhs.split('+', 1)[0]
    lhs_ = map(_try_cast, lhs.split('.'))
    rhs_ = map(_try_cast, rhs.split('.'))
    for l, r in zip(lhs_, rhs_):
        if l != r:
            if isinstance(l, int) and isinstance(r, int):
                return l < r
            return f'{l}' < f'{r}'
    return True


def is_module_ver_at_least(module, version):
    """Determine if a module's version is at least equal to the given value.

    Args:
        module: imported module's name, e.g., `np` or `torch`.
        version: required version, given as a tuple, e.g., `(1, 8, 0)`.
    Returns:
        `True` if module is the given version or newer.
    """
    test_ver = '.'.join(map(str, version))
    return module.__version__ != test_ver and version_leq(test_ver, module.__version__)


def floor_divide(a: NdarrayOrTensor, b) ->NdarrayOrTensor:
    """`np.floor_divide` with equivalent implementation for torch.

    As of pt1.8, use `torch.div(..., rounding_mode="floor")`, and
    before that, use `torch.floor_divide`.

    Args:
        a: first array/tensor
        b: scalar to divide by

    Returns:
        Element-wise floor division between two arrays/tensors.
    """
    if isinstance(a, torch.Tensor):
        if is_module_ver_at_least(torch, (1, 8, 0)):
            return torch.div(a, b, rounding_mode='floor')
        return torch.floor_divide(a, b)
    return np.floor_divide(a, b)


class BoxSelector:
    """
    Box selector which selects the predicted boxes.
    The box selection is performed with the following steps:

    #. For each level, discard boxes with scores less than self.score_thresh.
    #. For each level, keep boxes with top self.topk_candidates_per_level scores.
    #. For the whole image, perform non-maximum suppression (NMS) on boxes, with overlapping threshold nms_thresh.
    #. For the whole image, keep boxes with top self.detections_per_img scores.

    Args:
        apply_sigmoid: whether to apply sigmoid to get scores from classification logits
        score_thresh: no box with scores less than score_thresh will be kept
        topk_candidates_per_level: max number of boxes to keep for each level
        nms_thresh: box overlapping threshold for NMS
        detections_per_img: max number of boxes to keep for each image

    Example:

        .. code-block:: python

            input_param = {
                "apply_sigmoid": True,
                "score_thresh": 0.1,
                "topk_candidates_per_level": 2,
                "nms_thresh": 0.1,
                "detections_per_img": 5,
            }
            box_selector = BoxSelector(**input_param)
            boxes = [torch.randn([3,6]), torch.randn([7,6])]
            logits = [torch.randn([3,3]), torch.randn([7,3])]
            spatial_size = (8,8,8)
            selected_boxes, selected_scores, selected_labels = box_selector.select_boxes_per_image(
                boxes, logits, spatial_size
            )
    """

    def __init__(self, box_overlap_metric: Callable=box_iou, apply_sigmoid: bool=True, score_thresh: float=0.05, topk_candidates_per_level: int=1000, nms_thresh: float=0.5, detections_per_img: int=300):
        self.box_overlap_metric = box_overlap_metric
        self.apply_sigmoid = apply_sigmoid
        self.score_thresh = score_thresh
        self.topk_candidates_per_level = topk_candidates_per_level
        self.nms_thresh = nms_thresh
        self.detections_per_img = detections_per_img

    def select_top_score_idx_per_level(self, logits: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
        """
        Select indices with highest scores.

        The indices selection is performed with the following steps:

        #. If self.apply_sigmoid, get scores by applying sigmoid to logits. Otherwise, use logits as scores.
        #. Discard indices with scores less than self.score_thresh
        #. Keep indices with top self.topk_candidates_per_level scores

        Args:
            logits: predicted classification logits, Tensor sized (N, num_classes)

        Return:
            - topk_idxs: selected M indices, Tensor sized (M, )
            - selected_scores: selected M scores, Tensor sized (M, )
            - selected_labels: selected M labels, Tensor sized (M, )
        """
        num_classes = logits.shape[-1]
        if self.apply_sigmoid:
            scores = torch.sigmoid(logits).flatten()
        else:
            scores = logits.flatten()
        keep_idxs = scores > self.score_thresh
        scores = scores[keep_idxs]
        flatten_topk_idxs = torch.where(keep_idxs)[0]
        num_topk = min(self.topk_candidates_per_level, flatten_topk_idxs.size(0))
        selected_scores, idxs = scores.topk(num_topk)
        flatten_topk_idxs = flatten_topk_idxs[idxs]
        selected_labels = flatten_topk_idxs % num_classes
        topk_idxs = floor_divide(flatten_topk_idxs, num_classes)
        return topk_idxs, selected_scores, selected_labels

    def select_boxes_per_image(self, boxes_list: List[Tensor], logits_list: List[Tensor], spatial_size: Union[List[int], Tuple[int]]) ->Tuple[Tensor, Tensor, Tensor]:
        """
        Postprocessing to generate detection result from classification logits and boxes.

        The box selection is performed with the following steps:

        #. For each level, discard boxes with scores less than self.score_thresh.
        #. For each level, keep boxes with top self.topk_candidates_per_level scores.
        #. For the whole image, perform non-maximum suppression (NMS) on boxes, with overlapping threshold nms_thresh.
        #. For the whole image, keep boxes with top self.detections_per_img scores.

        Args:
            boxes_list: list of predicted boxes from a single image,
                each element i is a Tensor sized (N_i, 2*spatial_dims)
            logits_list: list of predicted classification logits from a single image,
                each element i is a Tensor sized (N_i, num_classes)
            spatial_size: spatial size of the image

        Return:
            - selected boxes, Tensor sized (P, 2*spatial_dims)
            - selected_scores, Tensor sized (P, )
            - selected_labels, Tensor sized (P, )
        """
        if len(boxes_list) != len(logits_list):
            raise ValueError(f'len(boxes_list) should equal to len(logits_list). Got len(boxes_list)={len(boxes_list)}, len(logits_list)={len(logits_list)}')
        image_boxes = []
        image_scores = []
        image_labels = []
        boxes_dtype = boxes_list[0].dtype
        logits_dtype = logits_list[0].dtype
        for boxes_per_level, logits_per_level in zip(boxes_list, logits_list):
            topk_idxs: Tensor
            topk_idxs, scores_per_level, labels_per_level = self.select_top_score_idx_per_level(logits_per_level)
            boxes_per_level = boxes_per_level[topk_idxs]
            keep: Tensor
            boxes_per_level, keep = clip_boxes_to_image(boxes_per_level, spatial_size, remove_empty=True)
            image_boxes.append(boxes_per_level)
            image_scores.append(scores_per_level[keep])
            image_labels.append(labels_per_level[keep])
        image_boxes_t: Tensor = torch.cat(image_boxes, dim=0)
        image_scores_t: Tensor = torch.cat(image_scores, dim=0)
        image_labels_t: Tensor = torch.cat(image_labels, dim=0)
        keep_t: Tensor = batched_nms(image_boxes_t, image_scores_t, image_labels_t, self.nms_thresh, box_overlap_metric=self.box_overlap_metric, max_proposals=self.detections_per_img)
        selected_boxes = image_boxes_t[keep_t]
        selected_scores = image_scores_t[keep_t]
        selected_labels = image_labels_t[keep_t]
        return selected_boxes, selected_scores, selected_labels


class HardNegativeSamplerBase:
    """
    Base class of hard negative sampler.

    Hard negative sampler is used to suppress false positive rate in classification tasks.
    During training, it select negative samples with high prediction scores.

    The training workflow is described as the follows:
    1) forward network and get prediction scores (classification prob/logits) for all the samples;
    2) use hard negative sampler to choose negative samples with high prediction scores and some positive samples;
    3) compute classification loss for the selected samples;
    4) do back propagation.

    Args:
        pool_size: when we need ``num_neg`` hard negative samples, they will be randomly selected from
            ``num_neg * pool_size`` negative samples with the highest prediction scores.
            Larger ``pool_size`` gives more randomness, yet selects negative samples that are less 'hard',
            i.e., negative samples with lower prediction scores.
    """

    def __init__(self, pool_size: float=10) ->None:
        self.pool_size = pool_size

    def select_negatives(self, negative: Tensor, num_neg: int, fg_probs: Tensor) ->Tensor:
        """
        Select hard negative samples.

        Args:
            negative: indices of all the negative samples, sized (P,),
                where P is the number of negative samples
            num_neg: number of negative samples to sample
            fg_probs: maximum foreground prediction scores (probability) across all the classes
                for each sample, sized (A,), where A is the number of samples.

        Returns:
            binary mask of negative samples to choose, sized (A,),
                where A is the number of samples in one image
        """
        if negative.numel() > fg_probs.numel():
            raise ValueError('The number of negative samples should not be larger than the number of all samples.')
        pool = int(num_neg * self.pool_size)
        pool = min(negative.numel(), pool)
        _, negative_idx_pool = fg_probs[negative].topk(pool, dim=0, sorted=True)
        hard_negative = negative[negative_idx_pool]
        perm2 = torch.randperm(hard_negative.numel(), device=hard_negative.device)[:num_neg]
        selected_neg_idx = hard_negative[perm2]
        neg_mask = torch.zeros_like(fg_probs, dtype=torch.uint8)
        neg_mask[selected_neg_idx] = 1
        return neg_mask


def max(x: NdarrayTensor, dim: Optional[Union[int, Tuple]]=None, **kwargs) ->NdarrayTensor:
    """`torch.max` with equivalent implementation for numpy

    Args:
        x: array/tensor.

    Returns:
        the maximum of x.

    """
    ret: NdarrayTensor
    if dim is None:
        ret = np.max(x, **kwargs) if isinstance(x, (np.ndarray, list)) else torch.max(x, **kwargs)
    elif isinstance(x, (np.ndarray, list)):
        ret = np.max(x, axis=dim, **kwargs)
    else:
        ret = torch.max(x, int(dim), **kwargs)
    return ret


class HardNegativeSampler(HardNegativeSamplerBase):
    """
    HardNegativeSampler is used to suppress false positive rate in classification tasks.
    During training, it selects negative samples with high prediction scores.

    The training workflow is described as the follows:
    1) forward network and get prediction scores (classification prob/logits) for all the samples;
    2) use hard negative sampler to choose negative samples with high prediction scores and some positive samples;
    3) compute classification loss for the selected samples;
    4) do back propagation.

    Args:
        batch_size_per_image: number of training samples to be randomly selected per image
        positive_fraction: percentage of positive elements in the selected samples
        min_neg: minimum number of negative samples to select if possible.
        pool_size: when we need ``num_neg`` hard negative samples, they will be randomly selected from
            ``num_neg * pool_size`` negative samples with the highest prediction scores.
            Larger ``pool_size`` gives more randomness, yet selects negative samples that are less 'hard',
            i.e., negative samples with lower prediction scores.
    """

    def __init__(self, batch_size_per_image: int, positive_fraction: float, min_neg: int=1, pool_size: float=10) ->None:
        super().__init__(pool_size=pool_size)
        self.min_neg = min_neg
        self.batch_size_per_image = batch_size_per_image
        self.positive_fraction = positive_fraction
        logging.info('Sampling hard negatives on a per batch basis')

    def __call__(self, target_labels: List[Tensor], concat_fg_probs: Tensor) ->Tuple[List[Tensor], List[Tensor]]:
        """
        Select positives and hard negatives from list samples per image.
        Hard negative sampler will be applied to each image independently.

        Args:
            target_labels: list of labels per image.
                For image i in the batch, target_labels[i] is a Tensor sized (A_i,),
                where A_i is the number of samples in image i.
                Positive samples have positive labels, negative samples have label 0.
            concat_fg_probs: concatenated maximum foreground probability for all the images, sized (R,),
                where R is the sum of all samples inside one batch, i.e., R = A_0 + A_1 + ...

        Returns:
            - list of binary mask for positive samples
            - list of binary mask for negative samples

        Example:
            .. code-block:: python

                sampler = HardNegativeSampler(
                    batch_size_per_image=6, positive_fraction=0.5, min_neg=1, pool_size=2
                )
                # two images with different number of samples
                target_labels = [ torch.tensor([0,1]), torch.tensor([1,0,2,1])]
                concat_fg_probs = torch.rand(6)
                pos_idx_list, neg_idx_list = sampler(target_labels, concat_fg_probs)
        """
        samples_per_image = [samples_in_image.shape[0] for samples_in_image in target_labels]
        fg_probs = concat_fg_probs.split(samples_per_image, 0)
        return self.select_samples_img_list(target_labels, fg_probs)

    def select_samples_img_list(self, target_labels: List[Tensor], fg_probs: List[Tensor]) ->Tuple[List[Tensor], List[Tensor]]:
        """
        Select positives and hard negatives from list samples per image.
        Hard negative sampler will be applied to each image independently.

        Args:
            target_labels: list of labels per image.
                For image i in the batch, target_labels[i] is a Tensor sized (A_i,),
                where A_i is the number of samples in image i.
                Positive samples have positive labels, negative samples have label 0.
            fg_probs: list of maximum foreground probability per images,
                For image i in the batch, target_labels[i] is a Tensor sized (A_i,),
                where A_i is the number of samples in image i.

        Returns:
            - list of binary mask for positive samples
            - list binary mask for negative samples

        Example:
            .. code-block:: python

                sampler = HardNegativeSampler(
                    batch_size_per_image=6, positive_fraction=0.5, min_neg=1, pool_size=2
                )
                # two images with different number of samples
                target_labels = [ torch.tensor([0,1]), torch.tensor([1,0,2,1])]
                fg_probs = [ torch.rand(2), torch.rand(4)]
                pos_idx_list, neg_idx_list = sampler.select_samples_img_list(target_labels, fg_probs)
        """
        pos_idx = []
        neg_idx = []
        if len(target_labels) != len(fg_probs):
            raise ValueError(f'Require len(target_labels) == len(fg_probs). Got len(target_labels)={len(target_labels)}, len(fg_probs)={len(fg_probs)}.')
        for labels_per_img, fg_probs_per_img in zip(target_labels, fg_probs):
            pos_idx_per_image_mask, neg_idx_per_image_mask = self.select_samples_per_img(labels_per_img, fg_probs_per_img)
            pos_idx.append(pos_idx_per_image_mask)
            neg_idx.append(neg_idx_per_image_mask)
        return pos_idx, neg_idx

    def select_samples_per_img(self, labels_per_img: Tensor, fg_probs_per_img: Tensor) ->Tuple[Tensor, Tensor]:
        """
        Select positives and hard negatives from samples.

        Args:
            labels_per_img: labels, sized (A,).
                Positive samples have positive labels, negative samples have label 0.
            fg_probs_per_img: maximum foreground probability, sized (A,)

        Returns:
            - binary mask for positive samples, sized (A,)
            - binary mask for negative samples, sized (A,)

        Example:
            .. code-block:: python

                sampler = HardNegativeSampler(
                    batch_size_per_image=6, positive_fraction=0.5, min_neg=1, pool_size=2
                )
                # two images with different number of samples
                target_labels = torch.tensor([1,0,2,1])
                fg_probs = torch.rand(4)
                pos_idx, neg_idx = sampler.select_samples_per_img(target_labels, fg_probs)
        """
        if labels_per_img.numel() != fg_probs_per_img.numel():
            raise ValueError('labels_per_img and fg_probs_per_img should have same number of elements.')
        positive = torch.where(labels_per_img >= 1)[0]
        negative = torch.where(labels_per_img == 0)[0]
        num_pos = self.get_num_pos(positive)
        pos_idx_per_image_mask = self.select_positives(positive, num_pos, labels_per_img)
        num_neg = self.get_num_neg(negative, num_pos)
        neg_idx_per_image_mask = self.select_negatives(negative, num_neg, fg_probs_per_img)
        return pos_idx_per_image_mask, neg_idx_per_image_mask

    def get_num_pos(self, positive: torch.Tensor) ->int:
        """
        Number of positive samples to draw

        Args:
            positive: indices of positive samples

        Returns:
            number of positive sample
        """
        num_pos = int(self.batch_size_per_image * self.positive_fraction)
        num_pos = min(positive.numel(), num_pos)
        return num_pos

    def get_num_neg(self, negative: torch.Tensor, num_pos: int) ->int:
        """
        Sample enough negatives to fill up ``self.batch_size_per_image``

        Args:
            negative: indices of positive samples
            num_pos: number of positive samples to draw

        Returns:
            number of negative samples
        """
        num_neg = int(max(1, num_pos) * abs(1 - 1.0 / float(self.positive_fraction)))
        num_neg = min(negative.numel(), max(num_neg, self.min_neg))
        return num_neg

    def select_positives(self, positive: Tensor, num_pos: int, labels: Tensor) ->Tensor:
        """
        Select positive samples

        Args:
            positive: indices of positive samples, sized (P,),
                where P is the number of positive samples
            num_pos: number of positive samples to sample
            labels: labels for all samples, sized (A,),
                where A is the number of samples.

        Returns:
            binary mask of positive samples to choose, sized (A,),
                where A is the number of samples in one image
        """
        if positive.numel() > labels.numel():
            raise ValueError('The number of positive samples should not be larger than the number of all samples.')
        perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]
        pos_idx_per_image = positive[perm1]
        pos_idx_per_image_mask = torch.zeros_like(labels, dtype=torch.uint8)
        pos_idx_per_image_mask[pos_idx_per_image] = 1
        return pos_idx_per_image_mask


class PytorchPadMode(StrEnum):
    """
    See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html
    """
    CONSTANT = 'constant'
    REFLECT = 'reflect'
    REPLICATE = 'replicate'
    CIRCULAR = 'circular'


class Inferer(ABC):
    """
    A base class for model inference.
    Extend this class to support operations during inference, e.g. a sliding window method.

    Example code::

        device = torch.device("cuda:0")
        transform = Compose([ToTensor(), LoadImage(image_only=True)])
        data = transform(img_path).to(device)
        model = UNet(...).to(device)
        inferer = SlidingWindowInferer(...)

        model.eval()
        with torch.no_grad():
            pred = inferer(inputs=data, network=model)
        ...

    """

    @abstractmethod
    def __call__(self, inputs: torch.Tensor, network: Callable[..., torch.Tensor], *args: Any, **kwargs: Any):
        """
        Run inference on `inputs` with the `network` model.

        Args:
            inputs: input of the model inference.
            network: model for inference.
            args: optional args to be passed to ``network``.
            kwargs: optional keyword args to be passed to ``network``.

        Raises:
            NotImplementedError: When the subclass does not override this method.

        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')


def ensure_tuple_rep(tup: Any, dim: int) ->Tuple[Any, ...]:
    """
    Returns a copy of `tup` with `dim` values by either shortened or duplicated input.

    Raises:
        ValueError: When ``tup`` is a sequence and ``tup`` length is not ``dim``.

    Examples::

        >>> ensure_tuple_rep(1, 3)
        (1, 1, 1)
        >>> ensure_tuple_rep(None, 3)
        (None, None, None)
        >>> ensure_tuple_rep('test', 3)
        ('test', 'test', 'test')
        >>> ensure_tuple_rep([1, 2, 3], 3)
        (1, 2, 3)
        >>> ensure_tuple_rep(range(3), 3)
        (0, 1, 2)
        >>> ensure_tuple_rep([1, 2], 3)
        ValueError: Sequence must have length 3, got length 2.

    """
    if isinstance(tup, torch.Tensor):
        tup = tup.detach().cpu().numpy()
    if isinstance(tup, np.ndarray):
        tup = tup.tolist()
    if not issequenceiterable(tup):
        return (tup,) * dim
    if len(tup) == dim:
        return tuple(tup)
    raise ValueError(f'Sequence must have length {dim}, got {len(tup)}.')


def compute_importance_map(patch_size: Tuple[int, ...], mode: Union[BlendMode, str]=BlendMode.CONSTANT, sigma_scale: Union[Sequence[float], float]=0.125, device: Union[torch.device, int, str]='cpu') ->torch.Tensor:
    """Get importance map for different weight modes.

    Args:
        patch_size: Size of the required importance map. This should be either H, W [,D].
        mode: {``"constant"``, ``"gaussian"``}
            How to blend output of overlapping windows. Defaults to ``"constant"``.

            - ``"constant``": gives equal weight to all predictions.
            - ``"gaussian``": gives less weight to predictions on edges of windows.

        sigma_scale: Sigma_scale to calculate sigma for each dimension
            (sigma = sigma_scale * dim_size). Used for gaussian mode only.
        device: Device to put importance map on.

    Raises:
        ValueError: When ``mode`` is not one of ["constant", "gaussian"].

    Returns:
        Tensor of size patch_size.

    """
    mode = look_up_option(mode, BlendMode)
    device = torch.device(device)
    if mode == BlendMode.CONSTANT:
        importance_map = torch.ones(patch_size, device=device, dtype=torch.float)
    elif mode == BlendMode.GAUSSIAN:
        sigma_scale = ensure_tuple_rep(sigma_scale, len(patch_size))
        sigmas = [(i * sigma_s) for i, sigma_s in zip(patch_size, sigma_scale)]
        for i in range(len(patch_size)):
            x = torch.arange(start=-(patch_size[i] - 1) / 2.0, end=(patch_size[i] - 1) / 2.0 + 1, dtype=torch.float, device=device)
            x = torch.exp(x ** 2 / (-2 * sigmas[i] ** 2))
            importance_map = importance_map.unsqueeze(-1) * x[(None,) * i] if i > 0 else x
    else:
        raise ValueError(f'Unsupported mode: {mode}, available options are [{BlendMode.CONSTANT}, {BlendMode.CONSTANT}].')
    return importance_map


class LazyAttr(StrEnum):
    """
    MetaTensor with pending operations requires some key attributes tracked especially when the primary array
    is not up-to-date due to lazy evaluation.
    This class specifies the set of key attributes to be tracked for each MetaTensor.
    """
    SHAPE = 'lazy_shape'
    AFFINE = 'lazy_affine'
    PADDING_MODE = 'lazy_padding_mode'
    INTERP_MODE = 'lazy_interpolation_mode'
    DTYPE = 'lazy_dtype'


class MetaKeys(StrEnum):
    """
    Typical keys for MetaObj.meta
    """
    AFFINE = 'affine'
    ORIGINAL_AFFINE = 'original_affine'
    SPATIAL_SHAPE = 'spatial_shape'
    SPACE = 'space'
    ORIGINAL_CHANNEL_DIM = 'original_channel_dim'


class TraceKeys(StrEnum):
    """Extra metadata keys used for traceable transforms."""
    CLASS_NAME: str = 'class'
    ID: str = 'id'
    ORIG_SIZE: str = 'orig_size'
    EXTRA_INFO: str = 'extra_info'
    DO_TRANSFORM: str = 'do_transforms'
    KEY_SUFFIX: str = '_transforms'
    NONE: str = 'none'


def first(iterable, default=None):
    """
    Returns the first item in the given iterable or `default` if empty, meaningful mostly with 'for' expressions.
    """
    for i in iterable:
        return i
    return default


class MetaObj:
    """
    Abstract base class that stores data as well as any extra metadata.

    This allows for subclassing `torch.Tensor` and `np.ndarray` through multiple inheritance.

    Metadata is stored in the form of a dictionary.

    Behavior should be the same as extended class (e.g., `torch.Tensor` or `np.ndarray`)
    aside from the extended meta functionality.

    Copying of information:

        * For `c = a + b`, then auxiliary data (e.g., metadata) will be copied from the
          first instance of `MetaObj` if `a.is_batch` is False
          (For batched data, the metadata will be shallow copied for efficiency purposes).

    """

    def __init__(self) ->None:
        self._meta: dict = MetaObj.get_default_meta()
        self._applied_operations: list = MetaObj.get_default_applied_operations()
        self._pending_operations: list = MetaObj.get_default_applied_operations()
        self._is_batch: bool = False

    @staticmethod
    def flatten_meta_objs(*args: Iterable):
        """
        Recursively flatten input and yield all instances of `MetaObj`.
        This means that for both `torch.add(a, b)`, `torch.stack([a, b])` (and
        their numpy equivalents), we return `[a, b]` if both `a` and `b` are of type
        `MetaObj`.

        Args:
            args: Iterables of inputs to be flattened.
        Returns:
            list of nested `MetaObj` from input.
        """
        for a in itertools.chain(*args):
            if isinstance(a, (list, tuple)):
                yield from MetaObj.flatten_meta_objs(a)
            elif isinstance(a, MetaObj):
                yield a

    @staticmethod
    def copy_items(data):
        """returns a copy of the data. list and dict are shallow copied for efficiency purposes."""
        if isinstance(data, (list, dict, np.ndarray)):
            return data.copy()
        if isinstance(data, torch.Tensor):
            return data.detach().clone()
        return deepcopy(data)

    def copy_meta_from(self, input_objs, copy_attr=True) ->None:
        """
        Copy metadata from a `MetaObj` or an iterable of `MetaObj` instances.

        Args:
            input_objs: list of `MetaObj` to copy data from.
            copy_attr: whether to copy each attribute with `MetaObj.copy_item`.
                note that if the attribute is a nested list or dict, only a shallow copy will be done.
        """
        first_meta = input_objs if isinstance(input_objs, MetaObj) else first(input_objs, default=self)
        first_meta = first_meta.__dict__
        if not copy_attr:
            self.__dict__ = first_meta.copy()
        else:
            self.__dict__.update({a: MetaObj.copy_items(first_meta[a]) for a in first_meta})

    @staticmethod
    def get_default_meta() ->dict:
        """Get the default meta.

        Returns:
            default metadata.
        """
        return {}

    @staticmethod
    def get_default_applied_operations() ->list:
        """Get the default applied operations.

        Returns:
            default applied operations.
        """
        return []

    def __repr__(self) ->str:
        """String representation of class."""
        out: str = '\nMetadata\n'
        if self.meta is not None:
            out += ''.join(f'\t{k}: {v}\n' for k, v in self.meta.items())
        else:
            out += 'None'
        out += '\nApplied operations\n'
        if self.applied_operations is not None:
            out += pprint.pformat(self.applied_operations, indent=2, compact=True, width=120)
        else:
            out += 'None'
        out += f'\nIs batch?: {self.is_batch}'
        return out

    @property
    def meta(self) ->dict:
        """Get the meta. Defaults to ``{}``."""
        return self._meta if hasattr(self, '_meta') else MetaObj.get_default_meta()

    @meta.setter
    def meta(self, d) ->None:
        """Set the meta."""
        if d == TraceKeys.NONE:
            self._meta = MetaObj.get_default_meta()
        else:
            self._meta = d

    @property
    def applied_operations(self) ->list[dict]:
        """Get the applied operations. Defaults to ``[]``."""
        if hasattr(self, '_applied_operations'):
            return self._applied_operations
        return MetaObj.get_default_applied_operations()

    @applied_operations.setter
    def applied_operations(self, t) ->None:
        """Set the applied operations."""
        if t == TraceKeys.NONE:
            self._applied_operations = MetaObj.get_default_applied_operations()
            return
        self._applied_operations = t

    def push_applied_operation(self, t: Any) ->None:
        self._applied_operations.append(t)

    def pop_applied_operation(self) ->Any:
        return self._applied_operations.pop()

    @property
    def pending_operations(self) ->list[dict]:
        """Get the pending operations. Defaults to ``[]``."""
        if hasattr(self, '_pending_operations'):
            return self._pending_operations
        return MetaObj.get_default_applied_operations()

    def push_pending_operation(self, t: Any) ->None:
        self._pending_operations.append(t)

    def pop_pending_operation(self) ->Any:
        return self._pending_operations.pop()

    def clear_pending_operations(self) ->Any:
        self._pending_operations = MetaObj.get_default_applied_operations()

    @property
    def is_batch(self) ->bool:
        """Return whether object is part of batch or not."""
        return self._is_batch if hasattr(self, '_is_batch') else False

    @is_batch.setter
    def is_batch(self, val: bool) ->None:
        """Set whether object is part of batch or not."""
        self._is_batch = val


class PostFix(StrEnum):
    """Post-fixes."""

    @staticmethod
    def _get_str(prefix, suffix):
        return suffix if prefix is None else f'{prefix}_{suffix}'

    @staticmethod
    def meta(key: Optional[str]=None):
        return PostFix._get_str(key, 'meta_dict')

    @staticmethod
    def orig_meta(key: Optional[str]=None):
        return PostFix._get_str(key, 'orig_meta_dict')

    @staticmethod
    def transforms(key: Optional[str]=None):
        return PostFix._get_str(key, TraceKeys.KEY_SUFFIX[1:])


class SpaceKeys(StrEnum):
    """
    The coordinate system keys, for example, Nifti1 uses Right-Anterior-Superior or "RAS",
    DICOM (0020,0032) uses Left-Posterior-Superior or "LPS". This type does not distinguish spatial 1/2/3D.
    """
    RAS = 'RAS'
    LPS = 'LPS'


@functools.lru_cache(None)
def _get_named_tuple_like_type(func):
    if hasattr(torch, 'return_types') and hasattr(func, '__name__') and hasattr(torch.return_types, func.__name__) and isinstance(getattr(torch.return_types, func.__name__), type):
        return getattr(torch.return_types, func.__name__)
    return None


def _not_requiring_metadata(ret):
    return isinstance(ret, (int, str, bytes, torch.Size, torch.dtype, torch.device, np.ndarray)) or not (isinstance(ret, MetaTensor) or isinstance(ret, Sequence) and any(isinstance(x, MetaTensor) for x in ret))


def affine_to_spacing(affine: NdarrayTensor, r: int=3, dtype=float, suppress_zeros: bool=True) ->NdarrayTensor:
    """
    Computing the current spacing from the affine matrix.

    Args:
        affine: a d x d affine matrix.
        r: indexing based on the spatial rank, spacing is computed from `affine[:r, :r]`.
        dtype: data type of the output.
        suppress_zeros: whether to suppress the zeros with ones.

    Returns:
        an `r` dimensional vector of spacing.
    """
    if len(affine.shape) != 2 or affine.shape[0] != affine.shape[1]:
        raise ValueError(f'affine must be a square matrix, got {affine.shape}.')
    _affine, *_ = convert_to_dst_type(affine[:r, :r], dst=affine, dtype=dtype)
    if isinstance(_affine, torch.Tensor):
        spacing = torch.sqrt(torch.sum(_affine * _affine, dim=0))
    else:
        spacing = np.sqrt(np.sum(_affine * _affine, axis=0))
    if suppress_zeros:
        spacing[spacing == 0] = 1.0
    spacing_, *_ = convert_to_dst_type(spacing, dst=affine, dtype=dtype)
    return spacing_


def _non_zipping_check(batch_data: Union[Mapping, Iterable], detach: bool, pad: bool, fill_value):
    """
    Utility function based on `decollate_batch`, to identify the largest batch size from the collated data.
    returns batch_size, the list of non-iterable items, and the dictionary or list with their items decollated.

    See `decollate_batch` for more details.
    """
    _deco: Union[Mapping, Sequence]
    if isinstance(batch_data, Mapping):
        _deco = {key: decollate_batch(batch_data[key], detach, pad=pad, fill_value=fill_value) for key in batch_data}
    elif isinstance(batch_data, Iterable):
        _deco = [decollate_batch(b, detach, pad=pad, fill_value=fill_value) for b in batch_data]
    else:
        raise NotImplementedError(f'Unable to de-collate: {batch_data}, type: {type(batch_data)}.')
    batch_size, non_iterable = 0, []
    for k, v in (_deco.items() if isinstance(_deco, Mapping) else enumerate(_deco)):
        if not isinstance(v, Iterable) or isinstance(v, (str, bytes)) or isinstance(v, torch.Tensor) and v.ndim == 0:
            non_iterable.append(k)
        elif isinstance(v, Sized):
            batch_size = max(batch_size, len(v))
    return batch_size, non_iterable, _deco


PICKLE_KEY_SUFFIX = TraceKeys.KEY_SUFFIX


def pickle_operations(data, key=PICKLE_KEY_SUFFIX, is_encode: bool=True):
    """
    Applied_operations are dictionaries with varying sizes, this method converts them to bytes so that we can (de-)collate.

    Args:
        data: a list or dictionary with substructures to be pickled/unpickled.
        key: the key suffix for the target substructures, defaults to "_transforms" (`data.utils.PICKLE_KEY_SUFFIX`).
        is_encode: whether it's encoding using pickle.dumps (True) or decoding using pickle.loads (False).
    """
    if isinstance(data, Mapping):
        data = dict(data)
        for k in data:
            if f'{k}'.endswith(key):
                if is_encode and not isinstance(data[k], bytes):
                    data[k] = pickle.dumps(data[k], 0)
                if not is_encode and isinstance(data[k], bytes):
                    data[k] = pickle.loads(data[k])
        return {k: pickle_operations(v, key=key, is_encode=is_encode) for k, v in data.items()}
    elif isinstance(data, (list, tuple)):
        return [pickle_operations(item, key=key, is_encode=is_encode) for item in data]
    return data


def decollate_batch(batch, detach: bool=True, pad=True, fill_value=None):
    """De-collate a batch of data (for example, as produced by a `DataLoader`).

    Returns a list of structures with the original tensor's 0-th dimension sliced into elements using `torch.unbind`.

    Images originally stored as (B,C,H,W,[D]) will be returned as (C,H,W,[D]). Other information,
    such as metadata, may have been stored in a list (or a list inside nested dictionaries). In
    this case we return the element of the list corresponding to the batch idx.

    Return types aren't guaranteed to be the same as the original, since numpy arrays will have been
    converted to torch.Tensor, sequences may be converted to lists of tensors,
    mappings may be converted into dictionaries.

    For example:

    .. code-block:: python

        batch_data = {
            "image": torch.rand((2,1,10,10)),
            DictPostFix.meta("image"): {"scl_slope": torch.Tensor([0.0, 0.0])}
        }
        out = decollate_batch(batch_data)
        print(len(out))
        >>> 2

        print(out[0])
        >>> {'image': tensor([[[4.3549e-01...43e-01]]]), DictPostFix.meta("image"): {'scl_slope': 0.0}}

        batch_data = [torch.rand((2,1,10,10)), torch.rand((2,3,5,5))]
        out = decollate_batch(batch_data)
        print(out[0])
        >>> [tensor([[[4.3549e-01...43e-01]]], tensor([[[5.3435e-01...45e-01]]])]

        batch_data = torch.rand((2,1,10,10))
        out = decollate_batch(batch_data)
        print(out[0])
        >>> tensor([[[4.3549e-01...43e-01]]])

        batch_data = {
            "image": [1, 2, 3], "meta": [4, 5],  # undetermined batch size
        }
        out = decollate_batch(batch_data, pad=True, fill_value=0)
        print(out)
        >>> [{'image': 1, 'meta': 4}, {'image': 2, 'meta': 5}, {'image': 3, 'meta': 0}]
        out = decollate_batch(batch_data, pad=False)
        print(out)
        >>> [{'image': 1, 'meta': 4}, {'image': 2, 'meta': 5}]

    Args:
        batch: data to be de-collated.
        detach: whether to detach the tensors. Scalars tensors will be detached into number types
            instead of torch tensors.
        pad: when the items in a batch indicate different batch size, whether to pad all the sequences to the longest.
            If False, the batch size will be the length of the shortest sequence.
        fill_value: when `pad` is True, the `fillvalue` to use when padding, defaults to `None`.
    """
    if batch is None:
        return batch
    if isinstance(batch, (float, int, str, bytes)) or type(batch).__module__ == 'numpy' and not isinstance(batch, Iterable):
        return batch
    if isinstance(batch, torch.Tensor):
        if detach:
            batch = batch.detach()
        if batch.ndim == 0:
            return batch.item() if detach else batch
        out_list = torch.unbind(batch, dim=0)
        if isinstance(batch, MetaObj):
            for t, m in zip(out_list, decollate_batch(batch.meta)):
                if isinstance(t, MetaObj):
                    t.meta = m
                    t.is_batch = False
            for t, m in zip(out_list, batch.applied_operations):
                if isinstance(t, MetaObj):
                    t.applied_operations = m
                    t.is_batch = False
        if out_list[0].ndim == 0 and detach:
            return [t.item() for t in out_list]
        return list(out_list)
    b, non_iterable, deco = _non_zipping_check(batch, detach, pad, fill_value)
    if b <= 0:
        return deco
    if pad:
        for k in non_iterable:
            deco[k] = [deepcopy(deco[k]) for _ in range(b)]
    if isinstance(deco, Mapping):
        _gen = zip_longest(*deco.values(), fillvalue=fill_value) if pad else zip(*deco.values())
        ret = [dict(zip(deco, item)) for item in _gen]
        if not config.USE_META_DICT:
            return ret
        return pickle_operations(ret, is_encode=False)
    if isinstance(deco, Iterable):
        _gen = zip_longest(*deco, fillvalue=fill_value) if pad else zip(*deco)
        ret_list = [list(item) for item in _gen]
        if not config.USE_META_DICT:
            return ret_list
        return pickle_operations(ret_list, is_encode=False)
    raise NotImplementedError(f'Unable to de-collate: {batch}, type: {type(batch)}.')


_TRACK_META = True


def get_track_meta() ->bool:
    """
    Return the boolean as to whether metadata is tracked. If `True`, metadata will be
    associated its data by using subclasses of `MetaObj`. If `False`, then data will be
    returned with empty metadata.

    If `set_track_meta` is `False`, then standard data objects will be returned (e.g.,
    `torch.Tensor` and `np.ndarray`) as opposed to MONAI's enhanced objects.

    By default, this is `True`, and most users will want to leave it this way. However,
    if you are experiencing any problems regarding metadata, and aren't interested in
    preserving metadata, then you can disable it.
    """
    return _TRACK_META


def collate_meta_tensor(batch):
    """collate a sequence of meta tensor sequences/dictionaries into
    a single batched metatensor or a dictionary of batched metatensor"""
    if not isinstance(batch, Sequence):
        raise NotImplementedError()
    elem_0 = first(batch)
    if isinstance(elem_0, MetaObj):
        collated = default_collate(batch)
        collated.meta = default_collate([(i.meta or TraceKeys.NONE) for i in batch])
        collated.applied_operations = [(i.applied_operations or TraceKeys.NONE) for i in batch]
        collated.is_batch = True
        return collated
    if isinstance(elem_0, Mapping):
        return {k: collate_meta_tensor([d[k] for d in batch]) for k in elem_0}
    if isinstance(elem_0, (tuple, list)):
        return [collate_meta_tensor([d[i] for d in batch]) for i in range(len(elem_0))]
    return default_collate(batch)


def dev_collate(batch, level: int=1, logger_name: str='dev_collate'):
    """
    Recursively run collate logic and provide detailed loggings for debugging purposes.
    It reports results at the 'critical' level, is therefore suitable in the context of exception handling.

    Args:
        batch: batch input to collate
        level: current level of recursion for logging purposes
        logger_name: name of logger to use for logging

    See also: https://pytorch.org/docs/stable/data.html#working-with-collate-fn
    """
    elem = batch[0]
    elem_type = type(elem)
    l_str = '>' * level
    batch_str = f"{batch[:10]}{' ... ' if len(batch) > 10 else ''}"
    if isinstance(elem, torch.Tensor):
        try:
            logging.getLogger(logger_name).critical(f'{l_str} collate/stack a list of tensors')
            return torch.stack(batch, 0)
        except TypeError as e:
            logging.getLogger(logger_name).critical(f'{l_str} E: {e}, type {[type(elem).__name__ for elem in batch]} in collate({batch_str})')
            return
        except RuntimeError as e:
            logging.getLogger(logger_name).critical(f'{l_str} E: {e}, shape {[elem.shape for elem in batch]} in collate({batch_str})')
            return
    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' and elem_type.__name__ != 'string_':
        if elem_type.__name__ in ['ndarray', 'memmap']:
            logging.getLogger(logger_name).critical(f'{l_str} collate/stack a list of numpy arrays')
            return dev_collate([torch.as_tensor(b) for b in batch], level=level, logger_name=logger_name)
        elif elem.shape == ():
            return batch
    elif isinstance(elem, (float, int, str, bytes)):
        return batch
    elif isinstance(elem, abc.Mapping):
        out = {}
        for key in elem:
            logging.getLogger(logger_name).critical(f'{l_str} collate dict key "{key}" out of {len(elem)} keys')
            out[key] = dev_collate([d[key] for d in batch], level=level + 1, logger_name=logger_name)
        return out
    elif isinstance(elem, abc.Sequence):
        it = iter(batch)
        els = list(it)
        try:
            sizes = [len(elem) for elem in els]
        except TypeError:
            types = [type(elem).__name__ for elem in els]
            logging.getLogger(logger_name).critical(f'{l_str} E: type {types} in collate({batch_str})')
            return
        logging.getLogger(logger_name).critical(f'{l_str} collate list of sizes: {sizes}.')
        if any(s != sizes[0] for s in sizes):
            logging.getLogger(logger_name).critical(f'{l_str} collate list inconsistent sizes, got size: {sizes}, in collate({batch_str})')
        transposed = zip(*batch)
        return [dev_collate(samples, level=level + 1, logger_name=logger_name) for samples in transposed]
    logging.getLogger(logger_name).critical(f'{l_str} E: unsupported type in collate {batch_str}.')
    return


def list_data_collate(batch: Sequence):
    """
    Enhancement for PyTorch DataLoader default collate.
    If dataset already returns a list of batch data that generated in transforms, need to merge all data to 1 list.
    Then it's same as the default collate behavior.

    Note:
        Need to use this collate if apply some transforms that can generate batch data.

    """
    elem = batch[0]
    data = [i for k in batch for i in k] if isinstance(elem, list) else batch
    key = None
    try:
        if config.USE_META_DICT:
            data = pickle_operations(data)
        if isinstance(elem, Mapping):
            ret = {}
            for k in elem:
                key = k
                data_for_batch = [d[key] for d in data]
                ret[key] = collate_meta_tensor(data_for_batch)
        else:
            ret = collate_meta_tensor(data)
        return ret
    except RuntimeError as re:
        re_str = str(re)
        if 'equal size' in re_str:
            if key is not None:
                re_str += f"\nCollate error on the key '{key}' of dictionary data."
            re_str += '\n\nMONAI hint: if your transforms intentionally create images of different shapes, creating your ' + '`DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem (check its ' + 'documentation).'
        _ = dev_collate(data)
        raise RuntimeError(re_str) from re
    except TypeError as re:
        re_str = str(re)
        if 'numpy' in re_str and 'Tensor' in re_str:
            if key is not None:
                re_str += f"\nCollate error on the key '{key}' of dictionary data."
            re_str += '\n\nMONAI hint: if your transforms intentionally create mixtures of torch Tensor and numpy ndarray, ' + 'creating your `DataLoader` with `collate_fn=pad_list_data_collate` might solve this problem ' + '(check its documentation).'
        _ = dev_collate(data)
        raise TypeError(re_str) from re


def get_extra_metadata_keys() ->List[str]:
    """
    Get a list of unnecessary keys for metadata that can be removed.

    Returns:
        List of keys to be removed.
    """
    keys = ['srow_x', 'srow_y', 'srow_z', 'quatern_b', 'quatern_c', 'quatern_d', 'qoffset_x', 'qoffset_y', 'qoffset_z', 'dim', 'pixdim', *[f'dim[{i}]' for i in range(8)], *[f'pixdim[{i}]' for i in range(8)]]
    return keys


def remove_keys(data: dict, keys: List[str]) ->None:
    """
    Remove keys from a dictionary. Operates in-place so nothing is returned.

    Args:
        data: dictionary to be modified.
        keys: keys to be deleted from dictionary.

    Returns:
        `None`
    """
    for k in keys:
        _ = data.pop(k, None)


def remove_extra_metadata(meta: dict) ->None:
    """
    Remove extra metadata from the dictionary. Operates in-place so nothing is returned.

    Args:
        meta: dictionary containing metadata to be modified.

    Returns:
        `None`
    """
    keys = get_extra_metadata_keys()
    remove_keys(data=meta, keys=keys)


def polyval(coef, x) ->torch.Tensor:
    """
    Evaluates the polynomial defined by `coef` at `x`.

    For a 1D sequence of coef (length n), evaluate::

        y = coef[n-1] + x * (coef[n-2] + ... + x * (coef[1] + x * coef[0]))

    Args:
        coef: a sequence of floats representing the coefficients of the polynomial
        x: float or a sequence of floats representing the variable of the polynomial

    Returns:
        1D torch tensor
    """
    device = x.device if isinstance(x, torch.Tensor) else None
    coef = torch.as_tensor(coef, dtype=torch.float, device=device)
    if coef.ndim == 0 or len(coef) < 1:
        return torch.zeros(x.shape)
    x = torch.as_tensor(x, dtype=torch.float, device=device)
    ans = coef[0]
    for c in coef[1:]:
        ans = ans * x + c
    return ans


def _modified_bessel_0(x: torch.Tensor) ->torch.Tensor:
    x = torch.as_tensor(x, dtype=torch.float, device=x.device if isinstance(x, torch.Tensor) else None)
    if torch.abs(x) < 3.75:
        y = x * x / 14.0625
        return polyval([0.0045813, 0.0360768, 0.2659732, 1.2067492, 3.0899424, 3.5156229, 1.0], y)
    ax = torch.abs(x)
    y = 3.75 / ax
    _coef = [0.00392377, -0.01647633, 0.02635537, -0.02057706, 0.00916281, -0.00157565, 0.00225319, 0.01328592, 0.39894228]
    return polyval(_coef, y) * torch.exp(ax) / torch.sqrt(ax)


def _modified_bessel_1(x: torch.Tensor) ->torch.Tensor:
    x = torch.as_tensor(x, dtype=torch.float, device=x.device if isinstance(x, torch.Tensor) else None)
    if torch.abs(x) < 3.75:
        y = x * x / 14.0625
        _coef = [0.00032411, 0.00301532, 0.02658733, 0.15084934, 0.51498869, 0.87890594, 0.5]
        return torch.abs(x) * polyval(_coef, y)
    ax = torch.abs(x)
    y = 3.75 / ax
    _coef = [-0.00420059, 0.01787654, -0.02895312, 0.02282967, -0.01031555, 0.00163801, -0.00362018, -0.03988024, 0.39894228]
    ans = polyval(_coef, y) * torch.exp(ax) / torch.sqrt(ax)
    return -ans if x < 0.0 else ans


def _modified_bessel_i(n: int, x: torch.Tensor) ->torch.Tensor:
    if n < 2:
        raise ValueError(f'n must be greater than 1, got n={n}.')
    x = torch.as_tensor(x, dtype=torch.float, device=x.device if isinstance(x, torch.Tensor) else None)
    if x == 0.0:
        return x
    device = x.device
    tox = 2.0 / torch.abs(x)
    ans, bip, bi = torch.tensor(0.0, device=device), torch.tensor(0.0, device=device), torch.tensor(1.0, device=device)
    m = int(2 * (n + np.floor(np.sqrt(40.0 * n))))
    for j in range(m, 0, -1):
        bim = bip + float(j) * tox * bi
        bip = bi
        bi = bim
        if abs(bi) > 10000000000.0:
            ans = ans * 1e-10
            bi = bi * 1e-10
            bip = bip * 1e-10
        if j == n:
            ans = bip
    ans = ans * _modified_bessel_0(x) / bi
    return -ans if x < 0.0 and n % 2 == 1 else ans


def gaussian_1d(sigma: torch.Tensor, truncated: float=4.0, approx: str='erf', normalize: bool=False) ->torch.Tensor:
    """
    one dimensional Gaussian kernel.

    Args:
        sigma: std of the kernel
        truncated: tail length
        approx: discrete Gaussian kernel type, available options are "erf", "sampled", and "scalespace".

            - ``erf`` approximation interpolates the error function;
            - ``sampled`` uses a sampled Gaussian kernel;
            - ``scalespace`` corresponds to
              https://en.wikipedia.org/wiki/Scale_space_implementation#The_discrete_Gaussian_kernel
              based on the modified Bessel functions.

        normalize: whether to normalize the kernel with `kernel.sum()`.

    Raises:
        ValueError: When ``truncated`` is non-positive.

    Returns:
        1D torch tensor

    """
    sigma = torch.as_tensor(sigma, dtype=torch.float, device=sigma.device if isinstance(sigma, torch.Tensor) else None)
    device = sigma.device
    if truncated <= 0.0:
        raise ValueError(f'truncated must be positive, got {truncated}.')
    tail = int(max(float(sigma) * truncated, 0.5) + 0.5)
    if approx.lower() == 'erf':
        x = torch.arange(-tail, tail + 1, dtype=torch.float, device=device)
        t = 0.70710678 / torch.abs(sigma)
        out = 0.5 * ((t * (x + 0.5)).erf() - (t * (x - 0.5)).erf())
        out = out.clamp(min=0)
    elif approx.lower() == 'sampled':
        x = torch.arange(-tail, tail + 1, dtype=torch.float, device=sigma.device)
        out = torch.exp(-0.5 / (sigma * sigma) * x ** 2)
        if not normalize:
            out = out / (2.5066282 * sigma)
    elif approx.lower() == 'scalespace':
        sigma2 = sigma * sigma
        out_pos: List[Optional[torch.Tensor]] = [None] * (tail + 1)
        out_pos[0] = _modified_bessel_0(sigma2)
        out_pos[1] = _modified_bessel_1(sigma2)
        for k in range(2, len(out_pos)):
            out_pos[k] = _modified_bessel_i(k, sigma2)
        out = out_pos[:0:-1]
        out.extend(out_pos)
        out = torch.stack(out) * torch.exp(-sigma2)
    else:
        raise NotImplementedError(f"Unsupported option: approx='{approx}'.")
    return out / out.sum() if normalize else out


def sum(x: NdarrayTensor, dim: Optional[Union[int, Tuple]]=None, **kwargs) ->NdarrayTensor:
    """`torch.sum` with equivalent implementation for numpy

    Args:
        x: array/tensor.

    Returns:
        the sum of x.
    """
    ret: NdarrayTensor
    if dim is None:
        ret = np.sum(x, **kwargs) if isinstance(x, (np.ndarray, list)) else torch.sum(x, **kwargs)
    elif isinstance(x, (np.ndarray, list)):
        ret = np.sum(x, axis=dim, **kwargs)
    else:
        ret = torch.sum(x, int(dim), **kwargs)
    return ret


def _separable_filtering_conv(input_: torch.Tensor, kernels: List[torch.Tensor], pad_mode: str, d: int, spatial_dims: int, paddings: List[int], num_channels: int) ->torch.Tensor:
    if d < 0:
        return input_
    s = [1] * len(input_.shape)
    s[d + 2] = -1
    _kernel = kernels[d].reshape(s)
    if _kernel.numel() == 1 and _kernel[0] == 1:
        return _separable_filtering_conv(input_, kernels, pad_mode, d - 1, spatial_dims, paddings, num_channels)
    _kernel = _kernel.repeat([num_channels, 1] + [1] * spatial_dims)
    _padding = [0] * spatial_dims
    _padding[d] = paddings[d]
    conv_type = [F.conv1d, F.conv2d, F.conv3d][spatial_dims - 1]
    _reversed_padding_repeated_twice: List[List[int]] = [[p, p] for p in reversed(_padding)]
    _sum_reversed_padding_repeated_twice: List[int] = sum(_reversed_padding_repeated_twice, [])
    padded_input = F.pad(input_, _sum_reversed_padding_repeated_twice, mode=pad_mode)
    return conv_type(input=_separable_filtering_conv(padded_input, kernels, pad_mode, d - 1, spatial_dims, paddings, num_channels), weight=_kernel, groups=num_channels)


def separable_filtering(x: torch.Tensor, kernels: List[torch.Tensor], mode: str='zeros') ->torch.Tensor:
    """
    Apply 1-D convolutions along each spatial dimension of `x`.

    Args:
        x: the input image. must have shape (batch, channels, H[, W, ...]).
        kernels: kernel along each spatial dimension.
            could be a single kernel (duplicated for all spatial dimensions), or
            a list of `spatial_dims` number of kernels.
        mode (string, optional): padding mode passed to convolution class. ``'zeros'``, ``'reflect'``, ``'replicate'``
            or ``'circular'``. Default: ``'zeros'``. See ``torch.nn.Conv1d()`` for more information.

    Raises:
        TypeError: When ``x`` is not a ``torch.Tensor``.

    Examples:

    .. code-block:: python

        >>> import torch
        >>> from monai.networks.layers import separable_filtering
        >>> img = torch.randn(2, 4, 32, 32)  # batch_size 2, channels 4, 32x32 2D images
        # applying a [-1, 0, 1] filter along each of the spatial dimensions.
        # the output shape is the same as the input shape.
        >>> out = separable_filtering(img, torch.tensor((-1., 0., 1.)))
        # applying `[-1, 0, 1]`, `[1, 0, -1]` filters along two spatial dimensions respectively.
        # the output shape is the same as the input shape.
        >>> out = separable_filtering(img, [torch.tensor((-1., 0., 1.)), torch.tensor((1., 0., -1.))])

    """
    if not isinstance(x, torch.Tensor):
        raise TypeError(f'x must be a torch.Tensor but is {type(x).__name__}.')
    spatial_dims = len(x.shape) - 2
    if isinstance(kernels, torch.Tensor):
        kernels = [kernels] * spatial_dims
    _kernels = [s for s in kernels]
    _paddings = [((k.shape[0] - 1) // 2) for k in _kernels]
    n_chs = x.shape[1]
    pad_mode = 'constant' if mode == 'zeros' else mode
    return _separable_filtering_conv(x, _kernels, pad_mode, spatial_dims - 1, spatial_dims, _paddings, n_chs)


class GaussianFilter(nn.Module):

    def __init__(self, spatial_dims: int, sigma: Union[Sequence[float], float, Sequence[torch.Tensor], torch.Tensor], truncated: float=4.0, approx: str='erf', requires_grad: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions of the input image.
                must have shape (Batch, channels, H[, W, ...]).
            sigma: std. could be a single value, or `spatial_dims` number of values.
            truncated: spreads how many stds.
            approx: discrete Gaussian kernel type, available options are "erf", "sampled", and "scalespace".

                - ``erf`` approximation interpolates the error function;
                - ``sampled`` uses a sampled Gaussian kernel;
                - ``scalespace`` corresponds to
                  https://en.wikipedia.org/wiki/Scale_space_implementation#The_discrete_Gaussian_kernel
                  based on the modified Bessel functions.

            requires_grad: whether to store the gradients for sigma.
                if True, `sigma` will be the initial value of the parameters of this module
                (for example `parameters()` iterator could be used to get the parameters);
                otherwise this module will fix the kernels using `sigma` as the std.
        """
        if issequenceiterable(sigma):
            if len(sigma) != spatial_dims:
                raise ValueError
        else:
            sigma = [deepcopy(sigma) for _ in range(spatial_dims)]
        super().__init__()
        self.sigma = [torch.nn.Parameter(torch.as_tensor(s, dtype=torch.float, device=s.device if isinstance(s, torch.Tensor) else None), requires_grad=requires_grad) for s in sigma]
        self.truncated = truncated
        self.approx = approx
        for idx, param in enumerate(self.sigma):
            self.register_parameter(f'kernel_sigma_{idx}', param)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: in shape [Batch, chns, H, W, D].
        """
        _kernel = [gaussian_1d(s, truncated=self.truncated, approx=self.approx) for s in self.sigma]
        return separable_filtering(x=x, kernels=_kernel)


class TransformBackends(StrEnum):
    """
    Transform backends. Most of `monai.transforms` components first converts the input data into ``torch.Tensor`` or
    ``monai.data.MetaTensor``. Internally, some transforms are made by converting the data into ``numpy.array`` or
    ``cupy.array`` and use the underlying transform backend API to achieve the actual output array and
    converting back to ``Tensor``/``MetaTensor``. Transforms with more than one backend indicate the that they may
    convert the input data types to accommodate the underlying API.
    """
    TORCH = 'torch'
    NUMPY = 'numpy'
    CUPY = 'cupy'


class Transform(ABC):
    """
    An abstract class of a ``Transform``.
    A transform is callable that processes ``data``.

    It could be stateful and may modify ``data`` in place,
    the implementation should be aware of:

        #. thread safety when mutating its own states.
           When used from a multi-process context, transform's instance variables are read-only.
           thread-unsafe transforms should inherit :py:class:`monai.transforms.ThreadUnsafe`.
        #. ``data`` content unused by this transform may still be used in the
           subsequent transforms in a composed transform.
        #. storing too much information in ``data`` may cause some memory issue or IPC sync issue,
           especially in the multi-processing environment of PyTorch DataLoader.

    See Also

        :py:class:`monai.transforms.Compose`
    """
    backend: List[TransformBackends] = []

    @abstractmethod
    def __call__(self, data: Any):
        """
        ``data`` is an element which often comes from an iteration over an
        iterable, such as :py:class:`torch.utils.data.Dataset`. This method should
        return an updated version of ``data``.
        To simplify the input validations, most of the transforms assume that

        - ``data`` is a Numpy ndarray, PyTorch Tensor or string,
        - the data shape can be:

          #. string data without shape, `LoadImage` transform expects file paths,
          #. most of the pre-/post-processing transforms expect: ``(num_channels, spatial_dim_1[, spatial_dim_2, ...])``,
             except for example: `AddChannel` expects (spatial_dim_1[, spatial_dim_2, ...]) and
             `AsChannelFirst` expects (spatial_dim_1[, spatial_dim_2, ...], num_channels),

        - the channel dimension is often not omitted even if number of channels is one.

        This method can optionally take additional arguments to help execute transformation operation.

        Raises:
            NotImplementedError: When the subclass does not override this method.

        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')


class GaussianSmooth(Transform):
    """
    Apply Gaussian smooth to the input data based on specified `sigma` parameter.
    A default value `sigma=1.0` is provided for reference.

    Args:
        sigma: if a list of values, must match the count of spatial dimensions of input data,
            and apply every value in the list to 1 spatial dimension. if only 1 value provided,
            use it for all spatial dimensions.
        approx: discrete Gaussian kernel type, available options are "erf", "sampled", and "scalespace".
            see also :py:meth:`monai.networks.layers.GaussianFilter`.

    """
    backend = [TransformBackends.TORCH]

    def __init__(self, sigma: Union[Sequence[float], float]=1.0, approx: str='erf') ->None:
        self.sigma = sigma
        self.approx = approx

    def __call__(self, img: NdarrayTensor) ->NdarrayTensor:
        img = convert_to_tensor(img, track_meta=get_track_meta())
        img_t, *_ = convert_data_type(img, torch.Tensor, dtype=torch.float)
        sigma: Union[Sequence[torch.Tensor], torch.Tensor]
        if isinstance(self.sigma, Sequence):
            sigma = [torch.as_tensor(s, device=img_t.device) for s in self.sigma]
        else:
            sigma = torch.as_tensor(self.sigma, device=img_t.device)
        gaussian_filter = GaussianFilter(img_t.ndim - 1, sigma, approx=self.approx)
        out_t: torch.Tensor = gaussian_filter(img_t.unsqueeze(0)).squeeze(0)
        out, *_ = convert_to_dst_type(out_t, dst=img, dtype=out_t.dtype)
        return out


class InterpolateMode(StrEnum):
    """
    See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html
    """
    NEAREST = 'nearest'
    NEAREST_EXACT = 'nearest-exact'
    LINEAR = 'linear'
    BILINEAR = 'bilinear'
    BICUBIC = 'bicubic'
    TRILINEAR = 'trilinear'
    AREA = 'area'


def ensure_tuple_size(tup: Any, dim: int, pad_val: Any=0) ->Tuple[Any, ...]:
    """
    Returns a copy of `tup` with `dim` values by either shortened or padded with `pad_val` as necessary.
    """
    new_tup = ensure_tuple(tup) + (pad_val,) * dim
    return new_tup[:dim]


def fall_back_tuple(user_provided: Any, default: Union[Sequence, NdarrayTensor], func: Callable=lambda x: x and x > 0) ->Tuple[Any, ...]:
    """
    Refine `user_provided` according to the `default`, and returns as a validated tuple.

    The validation is done for each element in `user_provided` using `func`.
    If `func(user_provided[idx])` returns False, the corresponding `default[idx]` will be used
    as the fallback.

    Typically used when `user_provided` is a tuple of window size provided by the user,
    `default` is defined by data, this function returns an updated `user_provided` with its non-positive
    components replaced by the corresponding components from `default`.

    Args:
        user_provided: item to be validated.
        default: a sequence used to provided the fallbacks.
        func: a Callable to validate every components of `user_provided`.

    Examples::

        >>> fall_back_tuple((1, 2), (32, 32))
        (1, 2)
        >>> fall_back_tuple(None, (32, 32))
        (32, 32)
        >>> fall_back_tuple((-1, 10), (32, 32))
        (32, 10)
        >>> fall_back_tuple((-1, None), (32, 32))
        (32, 32)
        >>> fall_back_tuple((1, None), (32, 32))
        (1, 32)
        >>> fall_back_tuple(0, (32, 32))
        (32, 32)
        >>> fall_back_tuple(range(3), (32, 64, 48))
        (32, 1, 2)
        >>> fall_back_tuple([0], (32, 32))
        ValueError: Sequence must have length 2, got length 1.

    """
    ndim = len(default)
    user = ensure_tuple_rep(user_provided, ndim)
    return tuple(user_c if func(user_c) else default_c for default_c, user_c in zip(default, user))


def _create_scale(spatial_dims: int, scaling_factor: Union[Sequence[float], float], array_func=np.diag) ->NdarrayOrTensor:
    scaling_factor = ensure_tuple_size(scaling_factor, dim=spatial_dims, pad_val=1.0)
    return array_func(scaling_factor[:spatial_dims] + (1.0,))


def create_scale(spatial_dims: int, scaling_factor: Union[Sequence[float], float], device: Optional[torch.device]=None, backend=TransformBackends.NUMPY) ->NdarrayOrTensor:
    """
    create a scaling matrix

    Args:
        spatial_dims: spatial rank
        scaling_factor: scaling factors for every spatial dim, defaults to 1.
        device: device to compute and store the output (when the backend is "torch").
        backend: APIs to use, ``numpy`` or ``torch``.
    """
    _backend = look_up_option(backend, TransformBackends)
    if _backend == TransformBackends.NUMPY:
        return _create_scale(spatial_dims=spatial_dims, scaling_factor=scaling_factor, array_func=np.diag)
    if _backend == TransformBackends.TORCH:
        return _create_scale(spatial_dims=spatial_dims, scaling_factor=scaling_factor, array_func=lambda x: torch.diag(torch.as_tensor(x, device=device)))
    raise ValueError(f'backend {backend} is not supported')


def scale_affine(affine, spatial_size, new_spatial_size, centered: bool=True):
    """
    Scale the affine matrix according to the new spatial size.

    Args:
        affine: affine matrix to scale.
        spatial_size: original spatial size.
        new_spatial_size: new spatial size.
        centered: whether the scaling is with respect to
            the image center (True, default) or corner (False).

    Returns:
        Scaled affine matrix.

    """
    if spatial_size == new_spatial_size:
        return affine
    r = len(affine) - 1
    s = np.array([(float(o) / float(max(n, 1))) for o, n in zip(spatial_size, new_spatial_size)])
    scale = create_scale(r, s.tolist())
    if centered:
        scale[:r, -1] = (np.diag(scale)[:r] - 1) / 2
    return affine @ convert_to_dst_type(scale, affine)[0]


def _get_scan_interval(image_size: Sequence[int], roi_size: Sequence[int], num_spatial_dims: int, overlap: float) ->Tuple[int, ...]:
    """
    Compute scan interval according to the image size, roi size and overlap.
    Scan interval will be `int((1 - overlap) * roi_size)`, if interval is 0,
    use 1 instead to make sure sliding window works.

    """
    if len(image_size) != num_spatial_dims:
        raise ValueError('image coord different from spatial dims.')
    if len(roi_size) != num_spatial_dims:
        raise ValueError('roi coord different from spatial dims.')
    scan_interval = []
    for i in range(num_spatial_dims):
        if roi_size[i] == image_size[i]:
            scan_interval.append(int(roi_size[i]))
        else:
            interval = int(roi_size[i] * (1 - overlap))
            scan_interval.append(interval if interval > 0 else 1)
    return tuple(scan_interval)


def get_valid_patch_size(image_size: Sequence[int], patch_size: Union[Sequence[int], int, np.ndarray]) ->Tuple[int, ...]:
    """
    Given an image of dimensions `image_size`, return a patch size tuple taking the dimension from `patch_size` if this is
    not 0/None. Otherwise, or if `patch_size` is shorter than `image_size`, the dimension from `image_size` is taken. This ensures
    the returned patch size is within the bounds of `image_size`. If `patch_size` is a single number this is interpreted as a
    patch of the same dimensionality of `image_size` with that size in each dimension.
    """
    ndim = len(image_size)
    patch_size_ = ensure_tuple_size(patch_size, ndim)
    return tuple(min(ms, ps or ms) for ms, ps in zip(image_size, patch_size_))


def dense_patch_slices(image_size: Sequence[int], patch_size: Sequence[int], scan_interval: Sequence[int]) ->List[Tuple[slice, ...]]:
    """
    Enumerate all slices defining ND patches of size `patch_size` from an `image_size` input image.

    Args:
        image_size: dimensions of image to iterate over
        patch_size: size of patches to generate slices
        scan_interval: dense patch sampling interval

    Returns:
        a list of slice objects defining each patch

    """
    num_spatial_dims = len(image_size)
    patch_size = get_valid_patch_size(image_size, patch_size)
    scan_interval = ensure_tuple_size(scan_interval, num_spatial_dims)
    scan_num = []
    for i in range(num_spatial_dims):
        if scan_interval[i] == 0:
            scan_num.append(1)
        else:
            num = int(math.ceil(float(image_size[i]) / scan_interval[i]))
            scan_dim = first(d for d in range(num) if d * scan_interval[i] + patch_size[i] >= image_size[i])
            scan_num.append(scan_dim + 1 if scan_dim is not None else 1)
    starts = []
    for dim in range(num_spatial_dims):
        dim_starts = []
        for idx in range(scan_num[dim]):
            start_idx = idx * scan_interval[dim]
            start_idx -= max(start_idx + patch_size[dim] - image_size[dim], 0)
            dim_starts.append(start_idx)
        starts.append(dim_starts)
    out = np.asarray([x.flatten() for x in np.meshgrid(*starts, indexing='ij')]).T
    return [tuple(slice(s, s + patch_size[d]) for d, s in enumerate(x)) for x in out]


tqdm, _ = optional_import('tqdm', name='tqdm')


def sliding_window_inference(inputs: torch.Tensor, roi_size: Union[Sequence[int], int], sw_batch_size: int, predictor: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor], Dict[Any, torch.Tensor]]], overlap: float=0.25, mode: Union[BlendMode, str]=BlendMode.CONSTANT, sigma_scale: Union[Sequence[float], float]=0.125, padding_mode: Union[PytorchPadMode, str]=PytorchPadMode.CONSTANT, cval: float=0.0, sw_device: Union[torch.device, str, None]=None, device: Union[torch.device, str, None]=None, progress: bool=False, roi_weight_map: Optional[torch.Tensor]=None, process_fn: Optional[Callable]=None, *args: Any, **kwargs: Any) ->Union[torch.Tensor, Tuple[torch.Tensor, ...], Dict[Any, torch.Tensor]]:
    """
    Sliding window inference on `inputs` with `predictor`.

    The outputs of `predictor` could be a tensor, a tuple, or a dictionary of tensors.
    Each output in the tuple or dict value is allowed to have different resolutions with respect to the input.
    e.g., the input patch spatial size is [128,128,128], the output (a tuple of two patches) patch sizes
    could be ([128,64,256], [64,32,128]).
    In this case, the parameter `overlap` and `roi_size` need to be carefully chosen to ensure the output ROI is still
    an integer. If the predictor's input and output spatial sizes are not equal, we recommend choosing the parameters
    so that `overlap*roi_size*output_size/input_size` is an integer (for each spatial dimension).

    When roi_size is larger than the inputs' spatial size, the input image are padded during inference.
    To maintain the same spatial sizes, the output image will be cropped to the original input size.

    Args:
        inputs: input image to be processed (assuming NCHW[D])
        roi_size: the spatial window size for inferences.
            When its components have None or non-positives, the corresponding inputs dimension will be used.
            if the components of the `roi_size` are non-positive values, the transform will use the
            corresponding components of img size. For example, `roi_size=(32, -1)` will be adapted
            to `(32, 64)` if the second spatial dimension size of img is `64`.
        sw_batch_size: the batch size to run window slices.
        predictor: given input tensor ``patch_data`` in shape NCHW[D],
            The outputs of the function call ``predictor(patch_data)`` should be a tensor, a tuple, or a dictionary
            with Tensor values. Each output in the tuple or dict value should have the same batch_size, i.e. NM'H'W'[D'];
            where H'W'[D'] represents the output patch's spatial size, M is the number of output channels,
            N is `sw_batch_size`, e.g., the input shape is (7, 1, 128,128,128),
            the output could be a tuple of two tensors, with shapes: ((7, 5, 128, 64, 256), (7, 4, 64, 32, 128)).
            In this case, the parameter `overlap` and `roi_size` need to be carefully chosen
            to ensure the scaled output ROI sizes are still integers.
            If the `predictor`'s input and output spatial sizes are different,
            we recommend choosing the parameters so that ``overlap*roi_size*zoom_scale`` is an integer for each dimension.
        overlap: Amount of overlap between scans.
        mode: {``"constant"``, ``"gaussian"``}
            How to blend output of overlapping windows. Defaults to ``"constant"``.

            - ``"constant``": gives equal weight to all predictions.
            - ``"gaussian``": gives less weight to predictions on edges of windows.

        sigma_scale: the standard deviation coefficient of the Gaussian window when `mode` is ``"gaussian"``.
            Default: 0.125. Actual window sigma is ``sigma_scale`` * ``dim_size``.
            When sigma_scale is a sequence of floats, the values denote sigma_scale at the corresponding
            spatial dimensions.
        padding_mode: {``"constant"``, ``"reflect"``, ``"replicate"``, ``"circular"``}
            Padding mode for ``inputs``, when ``roi_size`` is larger than inputs. Defaults to ``"constant"``
            See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html
        cval: fill value for 'constant' padding mode. Default: 0
        sw_device: device for the window data.
            By default the device (and accordingly the memory) of the `inputs` is used.
            Normally `sw_device` should be consistent with the device where `predictor` is defined.
        device: device for the stitched output prediction.
            By default the device (and accordingly the memory) of the `inputs` is used. If for example
            set to device=torch.device('cpu') the gpu memory consumption is less and independent of the
            `inputs` and `roi_size`. Output is on the `device`.
        progress: whether to print a `tqdm` progress bar.
        roi_weight_map: pre-computed (non-negative) weight map for each ROI.
            If not given, and ``mode`` is not `constant`, this map will be computed on the fly.
        process_fn: process inference output and adjust the importance map per window
        args: optional args to be passed to ``predictor``.
        kwargs: optional keyword args to be passed to ``predictor``.

    Note:
        - input must be channel-first and have a batch dim, supports N-D sliding window.

    """
    compute_dtype = inputs.dtype
    num_spatial_dims = len(inputs.shape) - 2
    if overlap < 0 or overlap >= 1:
        raise ValueError('overlap must be >= 0 and < 1.')
    batch_size, _, *image_size_ = inputs.shape
    if device is None:
        device = inputs.device
    if sw_device is None:
        sw_device = inputs.device
    roi_size = fall_back_tuple(roi_size, image_size_)
    image_size = tuple(max(image_size_[i], roi_size[i]) for i in range(num_spatial_dims))
    pad_size = []
    for k in range(len(inputs.shape) - 1, 1, -1):
        diff = max(roi_size[k - 2] - inputs.shape[k], 0)
        half = diff // 2
        pad_size.extend([half, diff - half])
    inputs = F.pad(inputs, pad=pad_size, mode=look_up_option(padding_mode, PytorchPadMode), value=cval)
    scan_interval = _get_scan_interval(image_size, roi_size, num_spatial_dims, overlap)
    slices = dense_patch_slices(image_size, roi_size, scan_interval)
    num_win = len(slices)
    total_slices = num_win * batch_size
    valid_patch_size = get_valid_patch_size(image_size, roi_size)
    if valid_patch_size == roi_size and roi_weight_map is not None:
        importance_map_ = roi_weight_map
    else:
        try:
            importance_map_ = compute_importance_map(valid_patch_size, mode=mode, sigma_scale=sigma_scale, device=device)
        except BaseException as e:
            raise RuntimeError("Seems to be OOM. Please try smaller patch size or mode='constant' instead of mode='gaussian'.") from e
    importance_map_ = convert_data_type(importance_map_, torch.Tensor, device, compute_dtype)[0]
    min_non_zero = max(importance_map_[importance_map_ != 0].min().item(), 0.001)
    importance_map_ = torch.clamp(importance_map_.to(torch.float32), min=min_non_zero)
    dict_key, output_image_list, count_map_list = None, [], []
    _initialized_ss = -1
    is_tensor_output = True
    for slice_g in (tqdm(range(0, total_slices, sw_batch_size)) if progress else range(0, total_slices, sw_batch_size)):
        slice_range = range(slice_g, min(slice_g + sw_batch_size, total_slices))
        unravel_slice = [([slice(int(idx / num_win), int(idx / num_win) + 1), slice(None)] + list(slices[idx % num_win])) for idx in slice_range]
        window_data = torch.cat([convert_data_type(inputs[win_slice], torch.Tensor)[0] for win_slice in unravel_slice])
        seg_prob_out = predictor(window_data, *args, **kwargs)
        seg_prob_tuple: Tuple[torch.Tensor, ...]
        if isinstance(seg_prob_out, torch.Tensor):
            seg_prob_tuple = seg_prob_out,
        elif isinstance(seg_prob_out, Mapping):
            if dict_key is None:
                dict_key = sorted(seg_prob_out.keys())
            seg_prob_tuple = tuple(seg_prob_out[k] for k in dict_key)
            is_tensor_output = False
        else:
            seg_prob_tuple = ensure_tuple(seg_prob_out)
            is_tensor_output = False
        if process_fn:
            seg_prob_tuple, importance_map = process_fn(seg_prob_tuple, window_data, importance_map_)
        else:
            importance_map = importance_map_
        for ss, seg_prob in enumerate(seg_prob_tuple):
            seg_prob = seg_prob
            zoom_scale = []
            for axis, (img_s_i, out_w_i, in_w_i) in enumerate(zip(image_size, seg_prob.shape[2:], window_data.shape[2:])):
                _scale = out_w_i / float(in_w_i)
                if not (img_s_i * _scale).is_integer():
                    warnings.warn(f'For spatial axis: {axis}, output[{ss}] will have non-integer shape. Spatial zoom_scale between output[{ss}] and input is {_scale}. Please pad inputs.')
                zoom_scale.append(_scale)
            if _initialized_ss < ss:
                output_classes = seg_prob.shape[1]
                output_shape = [batch_size, output_classes] + [int(image_size_d * zoom_scale_d) for image_size_d, zoom_scale_d in zip(image_size, zoom_scale)]
                output_image_list.append(torch.zeros(output_shape, dtype=compute_dtype, device=device))
                count_map_list.append(torch.zeros([1, 1] + output_shape[2:], dtype=compute_dtype, device=device))
                _initialized_ss += 1
            resizer = Resize(spatial_size=seg_prob.shape[2:], mode='nearest', anti_aliasing=False)
            for idx, original_idx in zip(slice_range, unravel_slice):
                original_idx_zoom = list(original_idx)
                for axis in range(2, len(original_idx_zoom)):
                    zoomed_start = original_idx[axis].start * zoom_scale[axis - 2]
                    zoomed_end = original_idx[axis].stop * zoom_scale[axis - 2]
                    if not zoomed_start.is_integer() or not zoomed_end.is_integer():
                        warnings.warn(f'For axis-{axis - 2} of output[{ss}], the output roi range is not int. Input roi range is ({original_idx[axis].start}, {original_idx[axis].stop}). Spatial zoom_scale between output[{ss}] and input is {zoom_scale[axis - 2]}. Corresponding output roi range is ({zoomed_start}, {zoomed_end}).\nPlease change overlap ({overlap}) or roi_size ({roi_size[axis - 2]}) for axis-{axis - 2}. Tips: if overlap*roi_size*zoom_scale is an integer, it usually works.')
                    original_idx_zoom[axis] = slice(int(zoomed_start), int(zoomed_end), None)
                importance_map_zoom = resizer(importance_map.unsqueeze(0))[0]
                output_image_list[ss][original_idx_zoom] += importance_map_zoom * seg_prob[idx - slice_g]
                count_map_list[ss][original_idx_zoom] += importance_map_zoom.unsqueeze(0).unsqueeze(0).expand(count_map_list[ss][original_idx_zoom].shape)
    for ss in range(len(output_image_list)):
        output_image_list[ss] = output_image_list[ss] / count_map_list.pop(0)
    for ss, output_i in enumerate(output_image_list):
        if torch.isnan(output_i).any() or torch.isinf(output_i).any():
            warnings.warn('Sliding window inference results contain NaN or Inf.')
        zoom_scale = [(seg_prob_map_shape_d / roi_size_d) for seg_prob_map_shape_d, roi_size_d in zip(output_i.shape[2:], roi_size)]
        final_slicing: List[slice] = []
        for sp in range(num_spatial_dims):
            slice_dim = slice(pad_size[sp * 2], image_size_[num_spatial_dims - sp - 1] + pad_size[sp * 2])
            slice_dim = slice(int(round(slice_dim.start * zoom_scale[num_spatial_dims - sp - 1])), int(round(slice_dim.stop * zoom_scale[num_spatial_dims - sp - 1])))
            final_slicing.insert(0, slice_dim)
        while len(final_slicing) < len(output_i.shape):
            final_slicing.insert(0, slice(None))
        output_image_list[ss] = output_i[final_slicing]
    if dict_key is not None:
        final_output = dict(zip(dict_key, output_image_list))
    else:
        final_output = tuple(output_image_list)
    final_output = final_output[0] if is_tensor_output else final_output
    if isinstance(inputs, MetaTensor):
        final_output = convert_to_dst_type(final_output, inputs, device=device)[0]
    return final_output


class SlidingWindowInferer(Inferer):
    """
    Sliding window method for model inference,
    with `sw_batch_size` windows for every model.forward().
    Usage example can be found in the :py:class:`monai.inferers.Inferer` base class.

    Args:
        roi_size: the window size to execute SlidingWindow evaluation.
            If it has non-positive components, the corresponding `inputs` size will be used.
            if the components of the `roi_size` are non-positive values, the transform will use the
            corresponding components of img size. For example, `roi_size=(32, -1)` will be adapted
            to `(32, 64)` if the second spatial dimension size of img is `64`.
        sw_batch_size: the batch size to run window slices.
        overlap: Amount of overlap between scans.
        mode: {``"constant"``, ``"gaussian"``}
            How to blend output of overlapping windows. Defaults to ``"constant"``.

            - ``"constant``": gives equal weight to all predictions.
            - ``"gaussian``": gives less weight to predictions on edges of windows.

        sigma_scale: the standard deviation coefficient of the Gaussian window when `mode` is ``"gaussian"``.
            Default: 0.125. Actual window sigma is ``sigma_scale`` * ``dim_size``.
            When sigma_scale is a sequence of floats, the values denote sigma_scale at the corresponding
            spatial dimensions.
        padding_mode: {``"constant"``, ``"reflect"``, ``"replicate"``, ``"circular"``}
            Padding mode when ``roi_size`` is larger than inputs. Defaults to ``"constant"``
            See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html
        cval: fill value for 'constant' padding mode. Default: 0
        sw_device: device for the window data.
            By default the device (and accordingly the memory) of the `inputs` is used.
            Normally `sw_device` should be consistent with the device where `predictor` is defined.
        device: device for the stitched output prediction.
            By default the device (and accordingly the memory) of the `inputs` is used. If for example
            set to device=torch.device('cpu') the gpu memory consumption is less and independent of the
            `inputs` and `roi_size`. Output is on the `device`.
        progress: whether to print a tqdm progress bar.
        cache_roi_weight_map: whether to precompute the ROI weight map.
        cpu_thresh: when provided, dynamically switch to stitching on cpu (to save gpu memory)
            when input image volume is larger than this threshold (in pixels/voxels).
            Otherwise use ``"device"``. Thus, the output may end-up on either cpu or gpu.

    Note:
        ``sw_batch_size`` denotes the max number of windows per network inference iteration,
        not the batch size of inputs.

    """

    def __init__(self, roi_size: Union[Sequence[int], int], sw_batch_size: int=1, overlap: float=0.25, mode: Union[BlendMode, str]=BlendMode.CONSTANT, sigma_scale: Union[Sequence[float], float]=0.125, padding_mode: Union[PytorchPadMode, str]=PytorchPadMode.CONSTANT, cval: float=0.0, sw_device: Union[torch.device, str, None]=None, device: Union[torch.device, str, None]=None, progress: bool=False, cache_roi_weight_map: bool=False, cpu_thresh: Optional[int]=None) ->None:
        super().__init__()
        self.roi_size = roi_size
        self.sw_batch_size = sw_batch_size
        self.overlap = overlap
        self.mode: BlendMode = BlendMode(mode)
        self.sigma_scale = sigma_scale
        self.padding_mode = padding_mode
        self.cval = cval
        self.sw_device = sw_device
        self.device = device
        self.progress = progress
        self.cpu_thresh = cpu_thresh
        self.roi_weight_map = None
        try:
            if cache_roi_weight_map and isinstance(roi_size, Sequence) and min(roi_size) > 0:
                if device is None:
                    device = 'cpu'
                self.roi_weight_map = compute_importance_map(ensure_tuple(self.roi_size), mode=mode, sigma_scale=sigma_scale, device=device)
            if cache_roi_weight_map and self.roi_weight_map is None:
                warnings.warn('cache_roi_weight_map=True, but cache is not created. (dynamic roi_size?)')
        except BaseException as e:
            raise RuntimeError("Seems to be OOM. Please try smaller roi_size, or use mode='constant' instead of mode='gaussian'. ") from e

    def __call__(self, inputs: torch.Tensor, network: Callable[..., Union[torch.Tensor, Sequence[torch.Tensor], Dict[Any, torch.Tensor]]], *args: Any, **kwargs: Any) ->Union[torch.Tensor, Tuple[torch.Tensor, ...], Dict[Any, torch.Tensor]]:
        """

        Args:
            inputs: model input data for inference.
            network: target model to execute inference.
                supports callables such as ``lambda x: my_torch_model(x, additional_config)``
            args: optional args to be passed to ``network``.
            kwargs: optional keyword args to be passed to ``network``.

        """
        device = self.device
        if device is None and self.cpu_thresh is not None and inputs.shape[2:].numel() > self.cpu_thresh:
            device = 'cpu'
        return sliding_window_inference(inputs, self.roi_size, self.sw_batch_size, network, self.overlap, self.mode, self.sigma_scale, self.padding_mode, self.cval, self.sw_device, device, self.progress, self.roi_weight_map, None, *args, **kwargs)


def check_training_targets(input_images: Union[List[Tensor], Tensor], targets: Union[List[Dict[str, Tensor]], None], spatial_dims: int, target_label_key: str, target_box_key: str) ->None:
    """
    Validate the input images/targets during training (raise a `ValueError` if invalid).

    Args:
        input_images: It can be 1) a tensor sized (B, C, H, W) or  (B, C, H, W, D),
            or 2) a list of image tensors, each image i may have different size (C, H_i, W_i) or  (C, H_i, W_i, D_i).
        targets: a list of dict. Each dict with two keys: target_box_key and target_label_key,
            ground-truth boxes present in the image.
        spatial_dims: number of spatial dimensions of the images, 2 or 3.
        target_label_key: the expected key of target labels.
        target_box_key: the expected key of target boxes.
    """
    if targets is None:
        raise ValueError('Please provide ground truth targets during training.')
    if len(input_images) != len(targets):
        raise ValueError(f'len(input_images) should equal to len(targets), got {len(input_images)}, {len(targets)}.')
    for target in targets:
        if target_label_key not in target.keys() or target_box_key not in target.keys():
            raise ValueError(f'{target_label_key} and {target_box_key} are expected keys in targets. Got {target.keys()}.')
        boxes = target[target_box_key]
        if not isinstance(boxes, torch.Tensor):
            raise ValueError(f'Expected target boxes to be of type Tensor, got {type(boxes)}.')
        if len(boxes.shape) != 2 or boxes.shape[-1] != 2 * spatial_dims:
            raise ValueError(f'Expected target boxes to be a tensor of shape [N, {2 * spatial_dims}], got {boxes.shape}.')
    return


def ensure_dict_value_to_list_(head_outputs: Dict[str, List[Tensor]], keys: Optional[List[str]]=None) ->None:
    """
    An in-place function. We expect ``head_outputs`` to be Dict[str, List[Tensor]].
    Yet if it is Dict[str, Tensor], this func converts it to Dict[str, List[Tensor]].
    It will be modified in-place.

    Args:
        head_outputs: a Dict[str, List[Tensor]] or Dict[str, Tensor], will be modifier in-place
        keys: the keys in head_output that need to have value type List[Tensor]. If not provided, will use head_outputs.keys().
    """
    if keys is None:
        keys = list(head_outputs.keys())
    for k in keys:
        value_k = head_outputs[k]
        if isinstance(value_k, Tensor):
            head_outputs[k] = [value_k]
        elif isinstance(value_k[0], Tensor):
            head_outputs[k] = list(value_k)
        else:
            raise ValueError('The output of network should be Dict[str, List[Tensor]] or Dict[str, Tensor].')


def check_dict_values_same_length(head_outputs: Dict[str, List[Tensor]], keys: Optional[List[str]]=None) ->None:
    """
    We expect the values in ``head_outputs``: Dict[str, List[Tensor]] to have the same length.
    Will raise ValueError if not.

    Args:
        head_outputs: a Dict[str, List[Tensor]] or Dict[str, Tensor]
        keys: the keys in head_output that need to have values (List) with same length.
            If not provided, will use head_outputs.keys().
    """
    if keys is None:
        keys = list(head_outputs.keys())
    num_output_levels_list: List[int] = [len(head_outputs[k]) for k in keys]
    num_output_levels = torch.unique(torch.tensor(num_output_levels_list))
    if len(num_output_levels) != 1:
        raise ValueError(f'The values in the input dict should have the same length, Got {num_output_levels_list}.')


def _network_sequence_output(images: Tensor, network, keys: Optional[List[str]]=None) ->List[Tensor]:
    """
    Decompose the output of network (a dict) into a list.

    Args:
        images: input of the network
        keys: the keys in the network output whose values will be output in this func.
            If not provided, will use all keys.

    Return:
        network output values concat to a single List[Tensor]
    """
    head_outputs = network(images)
    ensure_dict_value_to_list_(head_outputs, keys)
    if keys is None:
        keys = list(head_outputs.keys())
    check_dict_values_same_length(head_outputs, keys)
    head_outputs_sequence = []
    for k in keys:
        head_outputs_sequence += list(head_outputs[k])
    return head_outputs_sequence


def predict_with_inferer(images: Tensor, network, keys: List[str], inferer: Optional[SlidingWindowInferer]=None) ->Dict[str, List[Tensor]]:
    """
    Predict network dict output with an inferer. Compared with directly output network(images),
    it enables a sliding window inferer that can be used to handle large inputs.

    Args:
        images: input of the network, Tensor sized (B, C, H, W) or  (B, C, H, W, D)
        network: a network that takes an image Tensor sized (B, C, H, W) or (B, C, H, W, D) as input
            and outputs a dictionary Dict[str, List[Tensor]] or Dict[str, Tensor].
        keys: the keys in the output dict, should be network output keys or a subset of them.
        inferer: a SlidingWindowInferer to handle large inputs.

    Return:
        The predicted head_output from network, a Dict[str, List[Tensor]]

    Example:
        .. code-block:: python

            # define a naive network
            import torch
            import monai
            class NaiveNet(torch.nn.Module):
                def __init__(self, ):
                    super().__init__()

                def forward(self, images: torch.Tensor):
                    return {"cls": torch.randn(images.shape), "box_reg": [torch.randn(images.shape)]}

            # create a predictor
            network = NaiveNet()
            inferer = monai.inferers.SlidingWindowInferer(
                roi_size = (128, 128, 128),
                overlap = 0.25,
                cache_roi_weight_map = True,
            )
            network_output_keys=["cls", "box_reg"]
            images = torch.randn((2, 3, 512, 512, 512))  # a large input
            head_outputs = predict_with_inferer(images, network, network_output_keys, inferer)

    """
    if inferer is None:
        raise ValueError('Please set inferer as a monai.inferers.inferer.SlidingWindowInferer(*)')
    head_outputs_sequence = inferer(images, _network_sequence_output, network, keys=keys)
    num_output_levels: int = len(head_outputs_sequence) // len(keys)
    head_outputs = {}
    for i, k in enumerate(keys):
        head_outputs[k] = list(head_outputs_sequence[num_output_levels * i:num_output_levels * (i + 1)])
    return head_outputs


def check_input_images(input_images: Union[List[Tensor], Tensor], spatial_dims: int) ->None:
    """
    Validate the input dimensionality (raise a `ValueError` if invalid).

    Args:
        input_images: It can be 1) a tensor sized (B, C, H, W) or  (B, C, H, W, D),
            or 2) a list of image tensors, each image i may have different size (C, H_i, W_i) or  (C, H_i, W_i, D_i).
        spatial_dims: number of spatial dimensions of the images, 2 or 3.
    """
    if isinstance(input_images, Tensor):
        if len(input_images.shape) != spatial_dims + 2:
            raise ValueError(f'When input_images is a Tensor, its need to be (spatial_dims + 2)-D.In this case, it should be a {spatial_dims + 2}-D Tensor, got Tensor shape {input_images.shape}.')
    elif isinstance(input_images, List):
        for img in input_images:
            if len(img.shape) != spatial_dims + 1:
                raise ValueError(f'When input_images is a List[Tensor], each element should have be (spatial_dims + 1)-D.In this case, it should be a {spatial_dims + 1}-D Tensor, got Tensor shape {img.shape}.')
    else:
        raise ValueError('input_images needs to be a List[Tensor] or Tensor.')
    return


class Method(StrEnum):
    """
    See also: :py:class:`monai.transforms.croppad.array.SpatialPad`
    """
    SYMMETRIC = 'symmetric'
    END = 'end'


def _create_translate(spatial_dims: int, shift: Union[Sequence[float], float], eye_func=np.eye, array_func=np.asarray) ->NdarrayOrTensor:
    shift = ensure_tuple(shift)
    affine = eye_func(spatial_dims + 1)
    for i, a in enumerate(shift[:spatial_dims]):
        affine[i, spatial_dims] = a
    return array_func(affine)


def create_translate(spatial_dims: int, shift: Union[Sequence[float], float], device: Optional[torch.device]=None, backend=TransformBackends.NUMPY) ->NdarrayOrTensor:
    """
    create a translation matrix

    Args:
        spatial_dims: spatial rank
        shift: translate pixel/voxel for every spatial dim, defaults to 0.
        device: device to compute and store the output (when the backend is "torch").
        backend: APIs to use, ``numpy`` or ``torch``.
    """
    _backend = look_up_option(backend, TransformBackends)
    if _backend == TransformBackends.NUMPY:
        return _create_translate(spatial_dims=spatial_dims, shift=shift, eye_func=np.eye, array_func=np.asarray)
    if _backend == TransformBackends.TORCH:
        return _create_translate(spatial_dims=spatial_dims, shift=shift, eye_func=lambda x: torch.eye(torch.as_tensor(x), device=device), array_func=lambda x: torch.as_tensor(x, device=device))
    raise ValueError(f'backend {backend} is not supported')


def get_torch_version_tuple():
    """
    Returns:
        tuple of ints represents the pytorch major/minor version.
    """
    return tuple(int(x) for x in torch.__version__.split('.')[:2])


def pytorch_after(major, minor, patch=0, current_ver_string=None) ->bool:
    """
    Compute whether the current pytorch version is after or equal to the specified version.
    The current system pytorch version is determined by `torch.__version__` or
    via system environment variable `PYTORCH_VER`.

    Args:
        major: major version number to be compared with
        minor: minor version number to be compared with
        patch: patch version number to be compared with
        current_ver_string: if None, `torch.__version__` will be used.

    Returns:
        True if the current pytorch version is greater than or equal to the specified version.
    """
    try:
        if current_ver_string is None:
            _env_var = os.environ.get('PYTORCH_VER', '')
            current_ver_string = _env_var if _env_var else torch.__version__
        ver, has_ver = optional_import('pkg_resources', name='parse_version')
        if has_ver:
            return ver('.'.join((f'{major}', f'{minor}', f'{patch}'))) <= ver(f'{current_ver_string}')
        parts = f'{current_ver_string}'.split('+', 1)[0].split('.', 3)
        while len(parts) < 3:
            parts += ['0']
        c_major, c_minor, c_patch = parts[:3]
    except (AttributeError, ValueError, TypeError):
        c_major, c_minor = get_torch_version_tuple()
        c_patch = '0'
    c_mn = int(c_major), int(c_minor)
    mn = int(major), int(minor)
    if c_mn != mn:
        return c_mn > mn
    is_prerelease = 'a' in f'{c_patch}'.lower() or 'rc' in f'{c_patch}'.lower()
    c_p = 0
    try:
        p_reg = re.search('\\d+', f'{c_patch}')
        if p_reg:
            c_p = int(p_reg.group())
    except (AttributeError, TypeError, ValueError):
        is_prerelease = True
    patch = int(patch)
    if c_p != patch:
        return c_p > patch
    if is_prerelease:
        return False
    return True


class NumpyPadMode(StrEnum):
    """
    See also: https://numpy.org/doc/1.18/reference/generated/numpy.pad.html
    """
    CONSTANT = 'constant'
    EDGE = 'edge'
    LINEAR_RAMP = 'linear_ramp'
    MAXIMUM = 'maximum'
    MEAN = 'mean'
    MEDIAN = 'median'
    MINIMUM = 'minimum'
    REFLECT = 'reflect'
    SYMMETRIC = 'symmetric'
    WRAP = 'wrap'
    EMPTY = 'empty'


def convert_pad_mode(dst: NdarrayOrTensor, mode: Optional[str]):
    """
    Utility to convert padding mode between numpy array and PyTorch Tensor.

    Args:
        dst: target data to convert padding mode for, should be numpy array or PyTorch Tensor.
        mode: current padding mode.

    """
    if isinstance(dst, torch.Tensor):
        if mode == 'wrap':
            mode = 'circular'
        if mode == 'edge':
            mode = 'replicate'
        return look_up_option(mode, PytorchPadMode)
    if isinstance(dst, np.ndarray):
        if mode == 'circular':
            mode = 'wrap'
        if mode == 'replicate':
            mode = 'edge'
        return look_up_option(mode, NumpyPadMode)
    raise ValueError(f'unsupported data type: {type(dst)}.')


def compute_divisible_spatial_size(spatial_shape: Sequence[int], k: Union[Sequence[int], int]):
    """
    Compute the target spatial size which should be divisible by `k`.

    Args:
        spatial_shape: original spatial shape.
        k: the target k for each spatial dimension.
            if `k` is negative or 0, the original size is preserved.
            if `k` is an int, the same `k` be applied to all the input spatial dimensions.

    """
    k = fall_back_tuple(k, (1,) * len(spatial_shape))
    new_size = []
    for k_d, dim in zip(k, spatial_shape):
        new_dim = int(np.ceil(dim / k_d) * k_d) if k_d > 0 else dim
        new_size.append(new_dim)
    return new_size


def pad_images(input_images: Union[List[Tensor], Tensor], spatial_dims: int, size_divisible: Union[int, Sequence[int]], mode: Union[PytorchPadMode, str]=PytorchPadMode.CONSTANT, **kwargs) ->Tuple[Tensor, List[List[int]]]:
    """
    Pad the input images, so that the output spatial sizes are divisible by `size_divisible`.
    It pads them at the end to create a (B, C, H, W) or (B, C, H, W, D) Tensor.
    Padded size (H, W) or (H, W, D) is divisible by size_divisible.
    Default padding uses constant padding with value 0.0

    Args:
        input_images: It can be 1) a tensor sized (B, C, H, W) or  (B, C, H, W, D),
            or 2) a list of image tensors, each image i may have different size (C, H_i, W_i) or  (C, H_i, W_i, D_i).
        spatial_dims: number of spatial dimensions of the images, 2D or 3D.
        size_divisible: int or Sequence[int], is the expected pattern on the input image shape.
            If an int, the same `size_divisible` will be applied to all the input spatial dimensions.
        mode: available modes for PyTorch Tensor: {``"constant"``, ``"reflect"``, ``"replicate"``, ``"circular"``}.
            One of the listed string values or a user supplied function. Defaults to ``"constant"``.
            See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html
        kwargs: other arguments for `torch.pad` function.

    Return:
        - images, a (B, C, H, W) or (B, C, H, W, D) Tensor
        - image_sizes, the original spatial size of each image
    """
    size_divisible = ensure_tuple_rep(size_divisible, spatial_dims)
    if isinstance(input_images, Tensor):
        orig_size = list(input_images.shape[-spatial_dims:])
        new_size = compute_divisible_spatial_size(spatial_shape=orig_size, k=size_divisible)
        all_pad_width = [(0, max(sp_i - orig_size[i], 0)) for i, sp_i in enumerate(new_size)]
        pt_pad_width = [val for sublist in all_pad_width for val in sublist[::-1]][::-1]
        if max(pt_pad_width) == 0:
            return input_images, [orig_size] * input_images.shape[0]
        mode_: str = convert_pad_mode(dst=input_images, mode=mode)
        return F.pad(input_images, pt_pad_width, mode=mode_, **kwargs), [orig_size] * input_images.shape[0]
    image_sizes = [img.shape[-spatial_dims:] for img in input_images]
    in_channels = input_images[0].shape[0]
    dtype = input_images[0].dtype
    device = input_images[0].device
    image_sizes_t = torch.tensor(image_sizes)
    max_spatial_size_t, _ = torch.max(image_sizes_t, dim=0)
    if len(max_spatial_size_t) != spatial_dims or len(size_divisible) != spatial_dims:
        raise ValueError(' Require len(max_spatial_size_t) == spatial_dims ==len(size_divisible).')
    max_spatial_size = compute_divisible_spatial_size(spatial_shape=list(max_spatial_size_t), k=size_divisible)
    images = torch.zeros([len(image_sizes), in_channels] + max_spatial_size, dtype=dtype, device=device)
    padder = SpatialPad(spatial_size=max_spatial_size, method='end', mode=mode, **kwargs)
    for idx, img in enumerate(input_images):
        images[idx, ...] = padder(img)
    return images, [list(ss) for ss in image_sizes]


def preprocess_images(input_images: Union[List[Tensor], Tensor], spatial_dims: int, size_divisible: Union[int, Sequence[int]], mode: Union[PytorchPadMode, str]=PytorchPadMode.CONSTANT, **kwargs) ->Tuple[Tensor, List[List[int]]]:
    """
    Preprocess the input images, including

    - validate of the inputs
    - pad the inputs so that the output spatial sizes are divisible by `size_divisible`.
      It pads them at the end to create a (B, C, H, W) or (B, C, H, W, D) Tensor.
      Padded size (H, W) or (H, W, D) is divisible by size_divisible.
      Default padding uses constant padding with value 0.0

    Args:
        input_images: It can be 1) a tensor sized (B, C, H, W) or  (B, C, H, W, D),
            or 2) a list of image tensors, each image i may have different size (C, H_i, W_i) or  (C, H_i, W_i, D_i).
        spatial_dims: number of spatial dimensions of the images, 2 or 3.
        size_divisible: int or Sequence[int], is the expected pattern on the input image shape.
            If an int, the same `size_divisible` will be applied to all the input spatial dimensions.
        mode: available modes for PyTorch Tensor: {``"constant"``, ``"reflect"``, ``"replicate"``, ``"circular"``}.
            One of the listed string values or a user supplied function. Defaults to ``"constant"``.
            See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html
        kwargs: other arguments for `torch.pad` function.

    Return:
        - images, a (B, C, H, W) or (B, C, H, W, D) Tensor
        - image_sizes, the original spatial size of each image
    """
    check_input_images(input_images, spatial_dims)
    size_divisible = ensure_tuple_rep(size_divisible, spatial_dims)
    return pad_images(input_images, spatial_dims, size_divisible, mode, **kwargs)


class RetinaNetDetector(nn.Module):
    """
    Retinanet detector, expandable to other one stage anchor based box detectors in the future.
    An example of construction can found in the source code of
    :func:`~monai.apps.detection.networks.retinanet_detector.retinanet_resnet50_fpn_detector` .

    The input to the model is expected to be a list of tensors, each of shape (C, H, W) or  (C, H, W, D),
    one for each image, and should be in 0-1 range. Different images can have different sizes.
    Or it can also be a Tensor sized (B, C, H, W) or  (B, C, H, W, D). In this case, all images have same size.

    The behavior of the model changes depending if it is in training or evaluation mode.

    During training, the model expects both the input tensors, as well as a targets (list of dictionary),
    containing:

    - boxes (``FloatTensor[N, 4]`` or ``FloatTensor[N, 6]``): the ground-truth boxes in ``StandardMode``, i.e.,
      ``[xmin, ymin, xmax, ymax]`` or ``[xmin, ymin, zmin, xmax, ymax, zmax]`` format,
      with ``0 <= xmin < xmax <= H``, ``0 <= ymin < ymax <= W``, ``0 <= zmin < zmax <= D``.
    - labels: the class label for each ground-truth box

    The model returns a Dict[str, Tensor] during training, containing the classification and regression
    losses.
    When saving the model, only self.network contains trainable parameters and needs to be saved.

    During inference, the model requires only the input tensors, and returns the post-processed
    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as
    follows:

    - boxes (``FloatTensor[N, 4]`` or ``FloatTensor[N, 6]``): the predicted boxes in ``StandardMode``, i.e.,
      ``[xmin, ymin, xmax, ymax]`` or ``[xmin, ymin, zmin, xmax, ymax, zmax]`` format,
      with ``0 <= xmin < xmax <= H``, ``0 <= ymin < ymax <= W``, ``0 <= zmin < zmax <= D``.
    - labels (Int64Tensor[N]): the predicted labels for each image
    - labels_scores (Tensor[N]): the scores for each prediction

    Args:
        network: a network that takes an image Tensor sized (B, C, H, W) or (B, C, H, W, D) as input
            and outputs a dictionary Dict[str, List[Tensor]] or Dict[str, Tensor].
        anchor_generator: anchor generator.
        box_overlap_metric: func that compute overlap between two sets of boxes, default is Intersection over Union (IoU).
        debug: whether to print out internal parameters, used for debugging and parameter tuning.

    Notes:

        Input argument ``network`` can be a monai.apps.detection.networks.retinanet_network.RetinaNet(*) object,
        but any network that meets the following rules is a valid input ``network``.

        1. It should have attributes including spatial_dims, num_classes, cls_key, box_reg_key, num_anchors, size_divisible.

            - spatial_dims (int) is the spatial dimension of the network, we support both 2D and 3D.
            - num_classes (int) is the number of classes, excluding the background.
            - size_divisible (int or Sequence[int]) is the expectation on the input image shape.
              The network needs the input spatial_size to be divisible by size_divisible, length should be 2 or 3.
            - cls_key (str) is the key to represent classification in the output dict.
            - box_reg_key (str) is the key to represent box regression in the output dict.
            - num_anchors (int) is the number of anchor shapes at each location. it should equal to
              ``self.anchor_generator.num_anchors_per_location()[0]``.

        2. Its input should be an image Tensor sized (B, C, H, W) or (B, C, H, W, D).

        3. About its output ``head_outputs``:

            - It should be a dictionary with at least two keys:
              ``network.cls_key`` and ``network.box_reg_key``.
            - ``head_outputs[network.cls_key]`` should be List[Tensor] or Tensor. Each Tensor represents
              classification logits map at one resolution level,
              sized (B, num_classes*num_anchors, H_i, W_i) or (B, num_classes*num_anchors, H_i, W_i, D_i).
            - ``head_outputs[network.box_reg_key]`` should be List[Tensor] or Tensor. Each Tensor represents
              box regression map at one resolution level,
              sized (B, 2*spatial_dims*num_anchors, H_i, W_i)or (B, 2*spatial_dims*num_anchors, H_i, W_i, D_i).
            - ``len(head_outputs[network.cls_key]) == len(head_outputs[network.box_reg_key])``.

    Example:

        .. code-block:: python

            # define a naive network
            import torch
            class NaiveNet(torch.nn.Module):
                def __init__(self, spatial_dims: int, num_classes: int):
                    super().__init__()
                    self.spatial_dims = spatial_dims
                    self.num_classes = num_classes
                    self.size_divisible = 2
                    self.cls_key = "cls"
                    self.box_reg_key = "box_reg"
                    self.num_anchors = 1
                def forward(self, images: torch.Tensor):
                    spatial_size = images.shape[-self.spatial_dims:]
                    out_spatial_size = tuple(s//self.size_divisible for s in spatial_size)  # half size of input
                    out_cls_shape = (images.shape[0],self.num_classes*self.num_anchors) + out_spatial_size
                    out_box_reg_shape = (images.shape[0],2*self.spatial_dims*self.num_anchors) + out_spatial_size
                    return {self.cls_key: [torch.randn(out_cls_shape)], self.box_reg_key: [torch.randn(out_box_reg_shape)]}

            # create a RetinaNetDetector detector
            spatial_dims = 3
            num_classes = 5
            anchor_generator = monai.apps.detection.utils.anchor_utils.AnchorGeneratorWithAnchorShape(
                feature_map_scales=(1, ), base_anchor_shapes=((8,) * spatial_dims)
            )
            net = NaiveNet(spatial_dims, num_classes)
            detector = RetinaNetDetector(net, anchor_generator)

            # only detector.network may contain trainable parameters.
            optimizer = torch.optim.SGD(
                detector.network.parameters(),
                1e-3,
                momentum=0.9,
                weight_decay=3e-5,
                nesterov=True,
            )
            torch.save(detector.network.state_dict(), 'model.pt')  # save model
            detector.network.load_state_dict(torch.load('model.pt'))  # load model
    """

    def __init__(self, network, anchor_generator: AnchorGenerator, box_overlap_metric: Callable=box_iou, debug: bool=False):
        super().__init__()
        if not all(hasattr(network, attr) for attr in ['spatial_dims', 'num_classes', 'cls_key', 'box_reg_key', 'num_anchors', 'size_divisible']):
            raise AttributeError("network should have attributes, including: 'spatial_dims', 'num_classes', 'cls_key', 'box_reg_key', 'num_anchors', 'size_divisible'.")
        self.network = network
        self.spatial_dims = self.network.spatial_dims
        self.num_classes = self.network.num_classes
        self.size_divisible = ensure_tuple_rep(self.network.size_divisible, self.spatial_dims)
        self.cls_key = self.network.cls_key
        self.box_reg_key = self.network.box_reg_key
        self.anchor_generator = anchor_generator
        self.num_anchors_per_loc = self.anchor_generator.num_anchors_per_location()[0]
        if self.num_anchors_per_loc != self.network.num_anchors:
            raise ValueError(f'Number of feature map channels ({self.network.num_anchors}) should match with number of anchors at each location ({self.num_anchors_per_loc}).')
        self.anchors: Union[List[Tensor], None] = None
        self.previous_image_shape: Union[Any, None] = None
        self.box_overlap_metric = box_overlap_metric
        self.debug = debug
        self.fg_bg_sampler: Union[Any, None] = None
        self.set_cls_loss(torch.nn.BCEWithLogitsLoss(reduction='mean'))
        self.set_box_regression_loss(torch.nn.SmoothL1Loss(beta=1.0 / 9, reduction='mean'), encode_gt=True, decode_pred=False)
        self.box_coder = BoxCoder(weights=(1.0,) * 2 * self.spatial_dims)
        self.target_box_key = 'boxes'
        self.target_label_key = 'labels'
        self.pred_score_key = self.target_label_key + '_scores'
        self.inferer: Union[SlidingWindowInferer, None] = None
        self.box_selector = BoxSelector(box_overlap_metric=self.box_overlap_metric, score_thresh=0.05, topk_candidates_per_level=1000, nms_thresh=0.5, detections_per_img=300, apply_sigmoid=True)

    def set_box_coder_weights(self, weights: Tuple[float]):
        """
        Set the weights for box coder.

        Args:
            weights: a list/tuple with length of 2*self.spatial_dims

        """
        if len(weights) != 2 * self.spatial_dims:
            raise ValueError(f'len(weights) should be {2 * self.spatial_dims}, got weights={weights}.')
        self.box_coder = BoxCoder(weights=weights)

    def set_target_keys(self, box_key: str, label_key: str):
        """
        Set keys for the training targets and inference outputs.
        During training, both box_key and label_key should be keys in the targets
        when performing ``self.forward(input_images, targets)``.
        During inference, they will be the keys in the output dict of `self.forward(input_images)``.
        """
        self.target_box_key = box_key
        self.target_label_key = label_key
        self.pred_score_key = label_key + '_scores'

    def set_cls_loss(self, cls_loss: nn.Module) ->None:
        """
        Using for training. Set loss for classification that takes logits as inputs, make sure sigmoid/softmax is built in.

        Args:
            cls_loss: loss module for classification

        Example:
            .. code-block:: python

                detector.set_cls_loss(torch.nn.BCEWithLogitsLoss(reduction="mean"))
                detector.set_cls_loss(FocalLoss(reduction="mean", gamma=2.0))
        """
        self.cls_loss_func = cls_loss

    def set_box_regression_loss(self, box_loss: nn.Module, encode_gt: bool, decode_pred: bool) ->None:
        """
        Using for training. Set loss for box regression.

        Args:
            box_loss: loss module for box regression
            encode_gt: if True, will encode ground truth boxes to target box regression
                before computing the losses. Should be True for L1 loss and False for GIoU loss.
            decode_pred: if True, will decode predicted box regression into predicted boxes
                before computing losses. Should be False for L1 loss and True for GIoU loss.

        Example:
            .. code-block:: python

                detector.set_box_regression_loss(
                    torch.nn.SmoothL1Loss(beta=1.0 / 9, reduction="mean"),
                    encode_gt = True, decode_pred = False
                )
                detector.set_box_regression_loss(
                    monai.losses.giou_loss.BoxGIoULoss(reduction="mean"),
                    encode_gt = False, decode_pred = True
                )
        """
        self.box_loss_func = box_loss
        self.encode_gt = encode_gt
        self.decode_pred = decode_pred

    def set_regular_matcher(self, fg_iou_thresh: float, bg_iou_thresh: float, allow_low_quality_matches=True) ->None:
        """
        Using for training. Set torchvision matcher that matches anchors with ground truth boxes.

        Args:
            fg_iou_thresh: foreground IoU threshold for Matcher, considered as matched if IoU > fg_iou_thresh
            bg_iou_thresh: background IoU threshold for Matcher, considered as not matched if IoU < bg_iou_thresh
            allow_low_quality_matches: if True, produce additional matches
                for predictions that have only low-quality match candidates.
        """
        if fg_iou_thresh < bg_iou_thresh:
            raise ValueError(f'Require fg_iou_thresh >= bg_iou_thresh. Got fg_iou_thresh={fg_iou_thresh}, bg_iou_thresh={bg_iou_thresh}.')
        self.proposal_matcher = Matcher(fg_iou_thresh, bg_iou_thresh, allow_low_quality_matches=allow_low_quality_matches)

    def set_atss_matcher(self, num_candidates: int=4, center_in_gt: bool=False) ->None:
        """
        Using for training. Set ATSS matcher that matches anchors with ground truth boxes

        Args:
            num_candidates: number of positions to select candidates from.
                Smaller value will result in a higher matcher threshold and less matched candidates.
            center_in_gt: If False (default), matched anchor center points do not need
                to lie withing the ground truth box. Recommend False for small objects.
                If True, will result in a strict matcher and less matched candidates.
        """
        self.proposal_matcher = ATSSMatcher(num_candidates, self.box_overlap_metric, center_in_gt, debug=self.debug)

    def set_hard_negative_sampler(self, batch_size_per_image: int, positive_fraction: float, min_neg: int=1, pool_size: float=10):
        """
        Using for training. Set hard negative sampler that samples part of the anchors for training.

        HardNegativeSampler is used to suppress false positive rate in classification tasks.
        During training, it select negative samples with high prediction scores.

        Args:
            batch_size_per_image: number of elements to be selected per image
            positive_fraction: percentage of positive elements in the selected samples
            min_neg: minimum number of negative samples to select if possible.
            pool_size: when we need ``num_neg`` hard negative samples, they will be randomly selected from
                ``num_neg * pool_size`` negative samples with the highest prediction scores.
                Larger ``pool_size`` gives more randomness, yet selects negative samples that are less 'hard',
                i.e., negative samples with lower prediction scores.
        """
        self.fg_bg_sampler = HardNegativeSampler(batch_size_per_image=batch_size_per_image, positive_fraction=positive_fraction, min_neg=min_neg, pool_size=pool_size)

    def set_balanced_sampler(self, batch_size_per_image: int, positive_fraction: float):
        """
        Using for training. Set torchvision balanced sampler that samples part of the anchors for training.

        Args:
            batch_size_per_image: number of elements to be selected per image
            positive_fraction: percentage of positive elements per batch

        """
        self.fg_bg_sampler = BalancedPositiveNegativeSampler(batch_size_per_image=batch_size_per_image, positive_fraction=positive_fraction)

    def set_sliding_window_inferer(self, roi_size: Union[Sequence[int], int], sw_batch_size: int=1, overlap: float=0.5, mode: Union[BlendMode, str]=BlendMode.CONSTANT, sigma_scale: Union[Sequence[float], float]=0.125, padding_mode: Union[PytorchPadMode, str]=PytorchPadMode.CONSTANT, cval: float=0.0, sw_device: Union[torch.device, str, None]=None, device: Union[torch.device, str, None]=None, progress: bool=False, cache_roi_weight_map: bool=False):
        """
        Define sliding window inferer and store it to self.inferer.
        """
        self.inferer = SlidingWindowInferer(roi_size, sw_batch_size, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, cache_roi_weight_map)

    def set_box_selector_parameters(self, score_thresh: float=0.05, topk_candidates_per_level: int=1000, nms_thresh: float=0.5, detections_per_img: int=300, apply_sigmoid: bool=True):
        """
        Using for inference. Set the parameters that are used for box selection during inference.
        The box selection is performed with the following steps:

        #. For each level, discard boxes with scores less than self.score_thresh.
        #. For each level, keep boxes with top self.topk_candidates_per_level scores.
        #. For the whole image, perform non-maximum suppression (NMS) on boxes, with overlapping threshold nms_thresh.
        #. For the whole image, keep boxes with top self.detections_per_img scores.

        Args:
            score_thresh: no box with scores less than score_thresh will be kept
            topk_candidates_per_level: max number of boxes to keep for each level
            nms_thresh: box overlapping threshold for NMS
            detections_per_img: max number of boxes to keep for each image
        """
        self.box_selector = BoxSelector(box_overlap_metric=self.box_overlap_metric, apply_sigmoid=apply_sigmoid, score_thresh=score_thresh, topk_candidates_per_level=topk_candidates_per_level, nms_thresh=nms_thresh, detections_per_img=detections_per_img)

    def forward(self, input_images: Union[List[Tensor], Tensor], targets: Union[List[Dict[str, Tensor]], None]=None, use_inferer: bool=False) ->Union[Dict[str, Tensor], List[Dict[str, Tensor]]]:
        """
        Returns a dict of losses during training, or a list predicted dict of boxes and labels during inference.

        Args:
            input_images: The input to the model is expected to be a list of tensors, each of shape (C, H, W) or  (C, H, W, D),
                one for each image, and should be in 0-1 range. Different images can have different sizes.
                Or it can also be a Tensor sized (B, C, H, W) or  (B, C, H, W, D). In this case, all images have same size.
            targets: a list of dict. Each dict with two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image (optional).
            use_inferer: whether to use self.inferer, a sliding window inferer, to do the inference.
                If False, will simply forward the network.
                If True, will use self.inferer, and requires
                ``self.set_sliding_window_inferer(*args)`` to have been called before.

        Return:
            If training mode, will return a dict with at least two keys,
            including self.cls_key and self.box_reg_key, representing classification loss and box regression loss.

            If evaluation mode, will return a list of detection results.
            Each element corresponds to an images in ``input_images``, is a dict with at least three keys,
            including self.target_box_key, self.target_label_key, self.pred_score_key,
            representing predicted boxes, classification labels, and classification scores.

        """
        if self.training:
            check_training_targets(input_images, targets, self.spatial_dims, self.target_label_key, self.target_box_key)
            self._check_detector_training_components()
        images, image_sizes = preprocess_images(input_images, self.spatial_dims, self.size_divisible)
        if self.training or not use_inferer:
            head_outputs = self.network(images)
            ensure_dict_value_to_list_(head_outputs)
        else:
            if self.inferer is None:
                raise ValueError('`self.inferer` is not defined.Please refer to function self.set_sliding_window_inferer(*).')
            head_outputs = predict_with_inferer(images, self.network, keys=[self.cls_key, self.box_reg_key], inferer=self.inferer)
        self.generate_anchors(images, head_outputs)
        num_anchor_locs_per_level = [x.shape[2:].numel() for x in head_outputs[self.cls_key]]
        for key in [self.cls_key, self.box_reg_key]:
            head_outputs[key] = self._reshape_maps(head_outputs[key])
        if self.training:
            losses = self.compute_loss(head_outputs, targets, self.anchors, num_anchor_locs_per_level)
            return losses
        detections = self.postprocess_detections(head_outputs, self.anchors, image_sizes, num_anchor_locs_per_level)
        return detections

    def _check_detector_training_components(self):
        """
        Check if self.proposal_matcher and self.fg_bg_sampler have been set for training.
        """
        if not hasattr(self, 'proposal_matcher'):
            raise AttributeError('Matcher is not set. Please refer to self.set_regular_matcher(*) or self.set_atss_matcher(*).')
        if self.fg_bg_sampler is None and self.debug:
            warnings.warn('No balanced sampler is used. Negative samples are likely to be much more than positive samples. Please set balanced samplers with self.set_balanced_sampler(*) or self.set_hard_negative_sampler(*), or set classification loss function as Focal loss with self.set_cls_loss(*)')

    def generate_anchors(self, images: Tensor, head_outputs: Dict[str, List[Tensor]]):
        """
        Generate anchors and store it in self.anchors: List[Tensor].
        We generate anchors only when there is no stored anchors,
        or the new coming images has different shape with self.previous_image_shape

        Args:
            images: input images, a (B, C, H, W) or (B, C, H, W, D) Tensor.
            head_outputs: head_outputs. ``head_output_reshape[self.cls_key]`` is a Tensor
              sized (B, sum(HW(D)A), self.num_classes). ``head_output_reshape[self.box_reg_key]`` is a Tensor
              sized (B, sum(HW(D)A), 2*self.spatial_dims)
        """
        if self.anchors is None or self.previous_image_shape != images.shape:
            self.anchors = self.anchor_generator(images, head_outputs[self.cls_key])
            self.previous_image_shape = images.shape

    def _reshape_maps(self, result_maps: List[Tensor]) ->Tensor:
        """
        Concat network output map list to a single Tensor.
        This function is used in both training and inference.

        Args:
            result_maps: a list of Tensor, each Tensor is a (B, num_channel*A, H, W) or (B, num_channel*A, H, W, D) map.
                A = self.num_anchors_per_loc

        Return:
            reshaped and concatenated result, sized (B, sum(HWA), num_channel) or (B, sum(HWDA), num_channel)
        """
        all_reshaped_result_map = []
        for result_map in result_maps:
            batch_size = result_map.shape[0]
            num_channel = result_map.shape[1] // self.num_anchors_per_loc
            spatial_size = result_map.shape[-self.spatial_dims:]
            view_shape = (batch_size, -1, num_channel) + spatial_size
            reshaped_result_map = result_map.view(view_shape)
            if self.spatial_dims == 2:
                reshaped_result_map = reshaped_result_map.permute(0, 3, 4, 1, 2)
            elif self.spatial_dims == 3:
                reshaped_result_map = reshaped_result_map.permute(0, 3, 4, 5, 1, 2)
            else:
                ValueError('Images can only be 2D or 3D.')
            reshaped_result_map = reshaped_result_map.reshape(batch_size, -1, num_channel)
            if torch.isnan(reshaped_result_map).any() or torch.isinf(reshaped_result_map).any():
                raise ValueError('Concatenated result is NaN or Inf.')
            all_reshaped_result_map.append(reshaped_result_map)
        return torch.cat(all_reshaped_result_map, dim=1)

    def postprocess_detections(self, head_outputs_reshape: Dict[str, Tensor], anchors: List[Tensor], image_sizes: List[List[int]], num_anchor_locs_per_level: Sequence[int], need_sigmoid: bool=True) ->List[Dict[str, Tensor]]:
        """
        Postprocessing to generate detection result from classification logits and box regression.
        Use self.box_selector to select the final output boxes for each image.

        Args:
            head_outputs_reshape: reshaped head_outputs. ``head_output_reshape[self.cls_key]`` is a Tensor
              sized (B, sum(HW(D)A), self.num_classes). ``head_output_reshape[self.box_reg_key]`` is a Tensor
              sized (B, sum(HW(D)A), 2*self.spatial_dims)
            targets: a list of dict. Each dict with two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            anchors: a list of Tensor. Each Tensor represents anchors for each image,
                sized (sum(HWA), 2*spatial_dims) or (sum(HWDA), 2*spatial_dims).
                A = self.num_anchors_per_loc.

        Return:
            a list of dict, each dict corresponds to detection result on image.
        """
        num_anchors_per_level = [(num_anchor_locs * self.num_anchors_per_loc) for num_anchor_locs in num_anchor_locs_per_level]
        split_head_outputs: Dict[str, List[Tensor]] = {}
        for k in head_outputs_reshape:
            split_head_outputs[k] = list(head_outputs_reshape[k].split(num_anchors_per_level, dim=1))
        split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]
        class_logits = split_head_outputs[self.cls_key]
        box_regression = split_head_outputs[self.box_reg_key]
        compute_dtype = class_logits[0].dtype
        num_images = len(image_sizes)
        detections: List[Dict[str, Tensor]] = []
        for index in range(num_images):
            box_regression_per_image = [br[index] for br in box_regression]
            logits_per_image = [cl[index] for cl in class_logits]
            anchors_per_image, img_spatial_size = split_anchors[index], image_sizes[index]
            boxes_per_image = [self.box_coder.decode_single(b.to(torch.float32), a) for b, a in zip(box_regression_per_image, anchors_per_image)]
            selected_boxes, selected_scores, selected_labels = self.box_selector.select_boxes_per_image(boxes_per_image, logits_per_image, img_spatial_size)
            detections.append({self.target_box_key: selected_boxes, self.pred_score_key: selected_scores, self.target_label_key: selected_labels})
        return detections

    def compute_loss(self, head_outputs_reshape: Dict[str, Tensor], targets: List[Dict[str, Tensor]], anchors: List[Tensor], num_anchor_locs_per_level: Sequence[int]) ->Dict[str, Tensor]:
        """
        Compute losses.

        Args:
            head_outputs_reshape: reshaped head_outputs. ``head_output_reshape[self.cls_key]`` is a Tensor
              sized (B, sum(HW(D)A), self.num_classes). ``head_output_reshape[self.box_reg_key]`` is a Tensor
              sized (B, sum(HW(D)A), 2*self.spatial_dims)
            targets: a list of dict. Each dict with two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            anchors: a list of Tensor. Each Tensor represents anchors for each image,
                sized (sum(HWA), 2*spatial_dims) or (sum(HWDA), 2*spatial_dims).
                A = self.num_anchors_per_loc.

        Return:
            a dict of several kinds of losses.
        """
        matched_idxs = self.compute_anchor_matched_idxs(anchors, targets, num_anchor_locs_per_level)
        losses_cls = self.compute_cls_loss(head_outputs_reshape[self.cls_key], targets, matched_idxs)
        losses_box_regression = self.compute_box_loss(head_outputs_reshape[self.box_reg_key], targets, anchors, matched_idxs)
        return {self.cls_key: losses_cls, self.box_reg_key: losses_box_regression}

    def compute_anchor_matched_idxs(self, anchors: List[Tensor], targets: List[Dict[str, Tensor]], num_anchor_locs_per_level: Sequence[int]) ->List[Tensor]:
        """
        Compute the matched indices between anchors and ground truth (gt) boxes in targets.
        output[k][i] represents the matched gt index for anchor[i] in image k.
        Suppose there are M gt boxes for image k. The range of it output[k][i] value is [-2, -1, 0, ..., M-1].
        [0, M - 1] indicates this anchor is matched with a gt box,
        while a negative value indicating that it is not matched.

        Args:
            anchors: a list of Tensor. Each Tensor represents anchors for each image,
                sized (sum(HWA), 2*spatial_dims) or (sum(HWDA), 2*spatial_dims).
                A = self.num_anchors_per_loc.
            targets: a list of dict. Each dict with two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            num_anchor_locs_per_level: each element represents HW or HWD at this level.


        Return:
            a list of matched index `matched_idxs_per_image` (Tensor[int64]), Tensor sized (sum(HWA),) or (sum(HWDA),).
            Suppose there are M gt boxes. `matched_idxs_per_image[i]` is a matched gt index in [0, M - 1]
            or a negative value indicating that anchor i could not be matched.
            BELOW_LOW_THRESHOLD = -1, BETWEEN_THRESHOLDS = -2
        """
        matched_idxs = []
        for anchors_per_image, targets_per_image in zip(anchors, targets):
            if targets_per_image[self.target_box_key].numel() == 0:
                matched_idxs.append(torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64, device=anchors_per_image.device))
                continue
            if isinstance(self.proposal_matcher, Matcher):
                match_quality_matrix = self.box_overlap_metric(targets_per_image[self.target_box_key], anchors_per_image)
                matched_idxs_per_image = self.proposal_matcher(match_quality_matrix)
            elif isinstance(self.proposal_matcher, ATSSMatcher):
                match_quality_matrix, matched_idxs_per_image = self.proposal_matcher(targets_per_image[self.target_box_key], anchors_per_image, num_anchor_locs_per_level, self.num_anchors_per_loc)
            else:
                raise NotImplementedError('Currently support torchvision Matcher and monai ATSS matcher. Other types of matcher not supported. Please override self.compute_anchor_matched_idxs(*) for your own matcher.')
            if self.debug:
                None
            if torch.max(matched_idxs_per_image) < 0:
                warnings.warn(f'No anchor is matched with GT boxes. Please adjust matcher setting, anchor setting, or the network setting to change zoom scale between network output and input images.GT boxes are {targets_per_image[self.target_box_key]}.')
            matched_idxs.append(matched_idxs_per_image)
        return matched_idxs

    def compute_cls_loss(self, cls_logits: Tensor, targets: List[Dict[str, Tensor]], matched_idxs: List[Tensor]) ->Tensor:
        """
        Compute classification losses.

        Args:
            cls_logits: classification logits, sized (B, sum(HW(D)A), self.num_classes)
            targets: a list of dict. Each dict with two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            matched_idxs: a list of matched index. each element is sized (sum(HWA),) or  (sum(HWDA),)

        Return:
            classification losses.
        """
        total_cls_logits_list = []
        total_gt_classes_target_list = []
        for targets_per_image, cls_logits_per_image, matched_idxs_per_image in zip(targets, cls_logits, matched_idxs):
            sampled_cls_logits_per_image, sampled_gt_classes_target = self.get_cls_train_sample_per_image(cls_logits_per_image, targets_per_image, matched_idxs_per_image)
            total_cls_logits_list.append(sampled_cls_logits_per_image)
            total_gt_classes_target_list.append(sampled_gt_classes_target)
        total_cls_logits = torch.cat(total_cls_logits_list, dim=0)
        total_gt_classes_target = torch.cat(total_gt_classes_target_list, dim=0)
        losses: Tensor = self.cls_loss_func(total_cls_logits, total_gt_classes_target)
        return losses

    def compute_box_loss(self, box_regression: Tensor, targets: List[Dict[str, Tensor]], anchors: List[Tensor], matched_idxs: List[Tensor]) ->Tensor:
        """
        Compute box regression losses.

        Args:
            box_regression: box regression results, sized (B, sum(HWA), 2*self.spatial_dims)
            targets: a list of dict. Each dict with two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            anchors: a list of Tensor. Each Tensor represents anchors for each image,
                sized (sum(HWA), 2*spatial_dims) or (sum(HWDA), 2*spatial_dims).
                A = self.num_anchors_per_loc.
            matched_idxs: a list of matched index. each element is sized (sum(HWA),) or  (sum(HWDA),)

        Return:
            box regression losses.
        """
        total_box_regression_list = []
        total_target_regression_list = []
        for targets_per_image, box_regression_per_image, anchors_per_image, matched_idxs_per_image in zip(targets, box_regression, anchors, matched_idxs):
            decode_box_regression_per_image, matched_gt_boxes_per_image = self.get_box_train_sample_per_image(box_regression_per_image, targets_per_image, anchors_per_image, matched_idxs_per_image)
            total_box_regression_list.append(decode_box_regression_per_image)
            total_target_regression_list.append(matched_gt_boxes_per_image)
        total_box_regression = torch.cat(total_box_regression_list, dim=0)
        total_target_regression = torch.cat(total_target_regression_list, dim=0)
        if total_box_regression.shape[0] == 0:
            losses = torch.tensor(0.0)
            return losses
        losses = self.box_loss_func(total_box_regression, total_target_regression)
        return losses

    def get_cls_train_sample_per_image(self, cls_logits_per_image: Tensor, targets_per_image: Dict[str, Tensor], matched_idxs_per_image: Tensor) ->Tuple[Tensor, Tensor]:
        """
        Get samples from one image for classification losses computation.

        Args:
            cls_logits_per_image: classification logits for one image, (sum(HWA), self.num_classes)
            targets_per_image: a dict with at least two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            matched_idxs_per_image: matched index, Tensor sized (sum(HWA),) or (sum(HWDA),)
                Suppose there are M gt boxes. matched_idxs_per_image[i] is a matched gt index in [0, M - 1]
                or a negative value indicating that anchor i could not be matched.
                BELOW_LOW_THRESHOLD = -1, BETWEEN_THRESHOLDS = -2

        Return:
            paired predicted and GT samples from one image for classification losses computation
        """
        if torch.isnan(cls_logits_per_image).any() or torch.isinf(cls_logits_per_image).any():
            raise ValueError('NaN or Inf in predicted classification logits.')
        foreground_idxs_per_image = matched_idxs_per_image >= 0
        num_foreground = foreground_idxs_per_image.sum()
        num_gt_box = targets_per_image[self.target_box_key].shape[0]
        if self.debug:
            None
            if num_gt_box > 0 and num_foreground < 2 * num_gt_box:
                None
        gt_classes_target = torch.zeros_like(cls_logits_per_image)
        gt_classes_target[foreground_idxs_per_image, targets_per_image[self.target_label_key][matched_idxs_per_image[foreground_idxs_per_image]]] = 1.0
        if self.fg_bg_sampler is None:
            valid_idxs_per_image = matched_idxs_per_image != self.proposal_matcher.BETWEEN_THRESHOLDS
        else:
            if isinstance(self.fg_bg_sampler, HardNegativeSampler):
                max_cls_logits_per_image = torch.max(cls_logits_per_image, dim=1)[0]
                sampled_pos_inds_list, sampled_neg_inds_list = self.fg_bg_sampler([matched_idxs_per_image + 1], max_cls_logits_per_image)
            elif isinstance(self.fg_bg_sampler, BalancedPositiveNegativeSampler):
                sampled_pos_inds_list, sampled_neg_inds_list = self.fg_bg_sampler([matched_idxs_per_image + 1])
            else:
                raise NotImplementedError('Currently support torchvision BalancedPositiveNegativeSampler and monai HardNegativeSampler matcher. Other types of sampler not supported. Please override self.get_cls_train_sample_per_image(*) for your own sampler.')
            sampled_pos_inds = torch.where(torch.cat(sampled_pos_inds_list, dim=0))[0]
            sampled_neg_inds = torch.where(torch.cat(sampled_neg_inds_list, dim=0))[0]
            valid_idxs_per_image = torch.cat([sampled_pos_inds, sampled_neg_inds], dim=0)
        return cls_logits_per_image[valid_idxs_per_image, :], gt_classes_target[valid_idxs_per_image, :]

    def get_box_train_sample_per_image(self, box_regression_per_image: Tensor, targets_per_image: Dict[str, Tensor], anchors_per_image: Tensor, matched_idxs_per_image: Tensor) ->Tuple[Tensor, Tensor]:
        """
        Get samples from one image for box regression losses computation.

        Args:
            box_regression_per_image: box regression result for one image, (sum(HWA), 2*self.spatial_dims)
            targets_per_image: a dict with at least two keys: self.target_box_key and self.target_label_key,
                ground-truth boxes present in the image.
            anchors_per_image: anchors of one image,
                sized (sum(HWA), 2*spatial_dims) or (sum(HWDA), 2*spatial_dims).
                A = self.num_anchors_per_loc.
            matched_idxs_per_image: matched index, sized (sum(HWA),) or  (sum(HWDA),)

        Return:
            paired predicted and GT samples from one image for box regression losses computation
        """
        if torch.isnan(box_regression_per_image).any() or torch.isinf(box_regression_per_image).any():
            raise ValueError('NaN or Inf in predicted box regression.')
        foreground_idxs_per_image = torch.where(matched_idxs_per_image >= 0)[0]
        num_gt_box = targets_per_image[self.target_box_key].shape[0]
        if num_gt_box == 0:
            return box_regression_per_image[0:0, :], box_regression_per_image[0:0, :]
        matched_gt_boxes_per_image = targets_per_image[self.target_box_key][matched_idxs_per_image[foreground_idxs_per_image]]
        box_regression_per_image = box_regression_per_image[foreground_idxs_per_image, :]
        anchors_per_image = anchors_per_image[foreground_idxs_per_image, :]
        matched_gt_boxes_per_image_ = matched_gt_boxes_per_image
        box_regression_per_image_ = box_regression_per_image
        if self.encode_gt:
            matched_gt_boxes_per_image_ = self.box_coder.encode_single(matched_gt_boxes_per_image_, anchors_per_image)
        if self.decode_pred:
            box_regression_per_image_ = self.box_coder.decode_single(box_regression_per_image_, anchors_per_image)
        return box_regression_per_image_, matched_gt_boxes_per_image_


class LayerFactory:
    """
    Factory object for creating layers, this uses given factory functions to actually produce the types or constructing
    callables. These functions are referred to by name and can be added at any time.
    """

    def __init__(self) ->None:
        self.factories: Dict[str, Callable] = {}

    @property
    def names(self) ->Tuple[str, ...]:
        """
        Produces all factory names.
        """
        return tuple(self.factories)

    def add_factory_callable(self, name: str, func: Callable) ->None:
        """
        Add the factory function to this object under the given name.
        """
        self.factories[name.upper()] = func
        self.__doc__ = 'The supported member' + ('s are: ' if len(self.names) > 1 else ' is: ') + ', '.join(f'``{name}``' for name in self.names) + '.\nPlease see :py:class:`monai.networks.layers.split_args` for additional args parsing.'

    def factory_function(self, name: str) ->Callable:
        """
        Decorator for adding a factory function with the given name.
        """

        def _add(func: Callable) ->Callable:
            self.add_factory_callable(name, func)
            return func
        return _add

    def get_constructor(self, factory_name: str, *args) ->Any:
        """
        Get the constructor for the given factory name and arguments.

        Raises:
            TypeError: When ``factory_name`` is not a ``str``.

        """
        if not isinstance(factory_name, str):
            raise TypeError(f'factory_name must a str but is {type(factory_name).__name__}.')
        func = look_up_option(factory_name.upper(), self.factories)
        return func(*args)

    def __getitem__(self, args) ->Any:
        """
        Get the given name or name/arguments pair. If `args` is a callable it is assumed to be the constructor
        itself and is returned, otherwise it should be the factory name or a pair containing the name and arguments.
        """
        if callable(args):
            return args
        if isinstance(args, str):
            name_obj, args = args, ()
        else:
            name_obj, *args = args
        return self.get_constructor(name_obj, *args)

    def __getattr__(self, key):
        """
        If `key` is a factory name, return it, otherwise behave as inherited. This allows referring to factory names
        as if they were constants, eg. `Fact.FOO` for a factory Fact with factory function foo.
        """
        if key in self.factories:
            return key
        return super().__getattribute__(key)


Conv = LayerFactory()


class RetinaNetClassificationHead(nn.Module):
    """
    A classification head for use in RetinaNet.

    This head takes a list of feature maps as inputs, and outputs a list of classification maps.
    Each output map has same spatial size with the corresponding input feature map,
    and the number of output channel is num_anchors * num_classes.

    Args:
        in_channels: number of channels of the input feature
        num_anchors: number of anchors to be predicted
        num_classes: number of classes to be predicted
        spatial_dims: spatial dimension of the network, should be 2 or 3.
        prior_probability: prior probability to initialize classification convolutional layers.
    """

    def __init__(self, in_channels: int, num_anchors: int, num_classes: int, spatial_dims: int, prior_probability: float=0.01):
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        conv = []
        for _ in range(4):
            conv.append(conv_type(in_channels, in_channels, kernel_size=3, stride=1, padding=1))
            conv.append(nn.GroupNorm(num_groups=8, num_channels=in_channels))
            conv.append(nn.ReLU())
        self.conv = nn.Sequential(*conv)
        for layer in self.conv.children():
            if isinstance(layer, conv_type):
                torch.nn.init.normal_(layer.weight, std=0.01)
                torch.nn.init.constant_(layer.bias, 0)
        self.cls_logits = conv_type(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)
        torch.nn.init.normal_(self.cls_logits.weight, std=0.01)
        torch.nn.init.constant_(self.cls_logits.bias, -math.log((1 - prior_probability) / prior_probability))
        self.num_classes = num_classes
        self.num_anchors = num_anchors

    def forward(self, x: List[Tensor]) ->List[Tensor]:
        """
        It takes a list of feature maps as inputs, and outputs a list of classification maps.
        Each output classification map has same spatial size with the corresponding input feature map,
        and the number of output channel is num_anchors * num_classes.

        Args:
            x: list of feature map, x[i] is a (B, in_channels, H_i, W_i) or (B, in_channels, H_i, W_i, D_i) Tensor.

        Return:
            cls_logits_maps, list of classification map. cls_logits_maps[i] is a
            (B, num_anchors * num_classes, H_i, W_i) or (B, num_anchors * num_classes, H_i, W_i, D_i) Tensor.

        """
        cls_logits_maps = []
        if isinstance(x, Tensor):
            feature_maps = [x]
        else:
            feature_maps = x
        for features in feature_maps:
            cls_logits = self.conv(features)
            cls_logits = self.cls_logits(cls_logits)
            cls_logits_maps.append(cls_logits)
            if torch.isnan(cls_logits).any() or torch.isinf(cls_logits).any():
                raise ValueError('cls_logits is NaN or Inf.')
        return cls_logits_maps


class RetinaNetRegressionHead(nn.Module):
    """
    A regression head for use in RetinaNet.

    This head takes a list of feature maps as inputs, and outputs a list of box regression maps.
    Each output box regression map has same spatial size with the corresponding input feature map,
    and the number of output channel is num_anchors * 2 * spatial_dims.

    Args:
        in_channels: number of channels of the input feature
        num_anchors: number of anchors to be predicted
        spatial_dims: spatial dimension of the network, should be 2 or 3.
    """

    def __init__(self, in_channels: int, num_anchors: int, spatial_dims: int):
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        conv = []
        for _ in range(4):
            conv.append(conv_type(in_channels, in_channels, kernel_size=3, stride=1, padding=1))
            conv.append(nn.GroupNorm(num_groups=8, num_channels=in_channels))
            conv.append(nn.ReLU())
        self.conv = nn.Sequential(*conv)
        self.bbox_reg = conv_type(in_channels, num_anchors * 2 * spatial_dims, kernel_size=3, stride=1, padding=1)
        torch.nn.init.normal_(self.bbox_reg.weight, std=0.01)
        torch.nn.init.zeros_(self.bbox_reg.bias)
        for layer in self.conv.children():
            if isinstance(layer, conv_type):
                torch.nn.init.normal_(layer.weight, std=0.01)
                torch.nn.init.zeros_(layer.bias)

    def forward(self, x: List[Tensor]) ->List[Tensor]:
        """
        It takes a list of feature maps as inputs, and outputs a list of box regression maps.
        Each output box regression map has same spatial size with the corresponding input feature map,
        and the number of output channel is num_anchors * 2 * spatial_dims.

        Args:
            x: list of feature map, x[i] is a (B, in_channels, H_i, W_i) or (B, in_channels, H_i, W_i, D_i) Tensor.

        Return:
            box_regression_maps, list of box regression map. cls_logits_maps[i] is a
            (B, num_anchors * 2 * spatial_dims, H_i, W_i) or (B, num_anchors * 2 * spatial_dims, H_i, W_i, D_i) Tensor.

        """
        box_regression_maps = []
        if isinstance(x, Tensor):
            feature_maps = [x]
        else:
            feature_maps = x
        for features in feature_maps:
            box_regression = self.conv(features)
            box_regression = self.bbox_reg(box_regression)
            box_regression_maps.append(box_regression)
            if torch.isnan(box_regression).any() or torch.isinf(box_regression).any():
                raise ValueError('box_regression is NaN or Inf.')
        return box_regression_maps


class RetinaNet(nn.Module):
    """
    The network used in RetinaNet.

    It takes an image tensor as inputs, and outputs a dictionary ``head_outputs``.
    ``head_outputs[self.cls_key]`` is the predicted classification maps, a list of Tensor.
    ``head_outputs[self.box_reg_key]`` is the predicted box regression maps, a list of Tensor.

    Args:
        spatial_dims: number of spatial dimensions of the images. We support both 2D and 3D images.
        num_classes: number of output classes of the model (excluding the background).
        num_anchors: number of anchors at each location.
        feature_extractor: a network that outputs feature maps from the input images,
            each feature map corresponds to a different resolution.
            Its output can have a format of Tensor, Dict[Any, Tensor], or Sequence[Tensor].
            It can be the output of ``resnet_fpn_feature_extractor(*args, **kwargs)``.
        size_divisible: the spatial size of the network input should be divisible by size_divisible,
            decided by the feature_extractor.

    Example:

        .. code-block:: python

            from monai.networks.nets import resnet
            spatial_dims = 3  # 3D network
            conv1_t_stride = (2,2,1)  # stride of first convolutional layer in backbone
            backbone = resnet.ResNet(
                spatial_dims = spatial_dims,
                block = resnet.ResNetBottleneck,
                layers = [3, 4, 6, 3],
                block_inplanes = resnet.get_inplanes(),
                n_input_channels= 1,
                conv1_t_stride = conv1_t_stride,
                conv1_t_size = (7,7,7),
            )
            # This feature_extractor outputs 4-level feature maps.
            # number of output feature maps is len(returned_layers)+1
            returned_layers = [1,2,3]  # returned layer from feature pyramid network
            feature_extractor = resnet_fpn_feature_extractor(
                backbone = backbone,
                spatial_dims = spatial_dims,
                pretrained_backbone = False,
                trainable_backbone_layers = None,
                returned_layers = returned_layers,
            )
            # This feature_extractor requires input image spatial size
            # to be divisible by (32, 32, 16).
            size_divisible = tuple(2*s*2**max(returned_layers) for s in conv1_t_stride)
            model = RetinaNet(
                spatial_dims = spatial_dims,
                num_classes = 5,
                num_anchors = 6,
                feature_extractor=feature_extractor,
                size_divisible = size_divisible,
            ).to(device)
            result = model(torch.rand(2, 1, 128,128,128))
            cls_logits_maps = result["cls_logits"]  # a list of len(returned_layers)+1 Tensor
            box_regression_maps = result["box_regression"]  # a list of len(returned_layers)+1 Tensor
    """

    def __init__(self, spatial_dims: int, num_classes: int, num_anchors: int, feature_extractor, size_divisible: Union[Sequence[int], int]=1):
        super().__init__()
        self.spatial_dims = look_up_option(spatial_dims, supported=[1, 2, 3])
        self.num_classes = num_classes
        self.size_divisible = ensure_tuple_rep(size_divisible, self.spatial_dims)
        if not hasattr(feature_extractor, 'out_channels'):
            raise ValueError('feature_extractor should contain an attribute out_channels specifying the number of output channels (assumed to be the same for all the levels)')
        self.feature_extractor = feature_extractor
        self.feature_map_channels: int = self.feature_extractor.out_channels
        self.num_anchors = num_anchors
        self.classification_head = RetinaNetClassificationHead(self.feature_map_channels, self.num_anchors, self.num_classes, spatial_dims=self.spatial_dims)
        self.regression_head = RetinaNetRegressionHead(self.feature_map_channels, self.num_anchors, spatial_dims=self.spatial_dims)
        self.cls_key: str = 'classification'
        self.box_reg_key: str = 'box_regression'

    def forward(self, images: Tensor) ->Dict[str, List[Tensor]]:
        """
        It takes an image tensor as inputs, and outputs a dictionary ``head_outputs``.
        ``head_outputs[self.cls_key]`` is the predicted classification maps, a list of Tensor.
        ``head_outputs[self.box_reg_key]`` is the predicted box regression maps, a list of Tensor.

        Args:
            images: input images, sized (B, img_channels, H, W) or (B, img_channels, H, W, D).

        Return:
            a dictionary ``head_outputs`` with keys including self.cls_key and self.box_reg_key.
            ``head_outputs[self.cls_key]`` is the predicted classification maps, a list of Tensor.
            ``head_outputs[self.box_reg_key]`` is the predicted box regression maps, a list of Tensor.

        """
        features = self.feature_extractor(images)
        if isinstance(features, Tensor):
            feature_maps = [features]
        elif torch.jit.isinstance(features, Dict[str, Tensor]):
            feature_maps = list(features.values())
        else:
            feature_maps = list(features)
        if not isinstance(feature_maps[0], Tensor):
            raise ValueError('feature_extractor output format must be Tensor, Dict[str, Tensor], or Sequence[Tensor].')
        head_outputs: Dict[str, List[Tensor]] = {self.cls_key: self.classification_head(feature_maps)}
        head_outputs[self.box_reg_key] = self.regression_head(feature_maps)
        return head_outputs


class AnchorGeneratorWithAnchorShape(AnchorGenerator):
    """
    Module that generates anchors for a set of feature maps and
    image sizes, inherited from :py:class:`~monai.apps.detection.networks.utils.anchor_utils.AnchorGenerator`

    The module support computing anchors at multiple base anchor shapes
    per feature map.

    ``feature_map_scales`` should have the same number of elements with the number of feature maps.

    base_anchor_shapes can have an arbitrary number of elements.
    For 2D images, each element represents anchor width and height [w,h].
    For 2D images, each element represents anchor width, height, and depth [w,h,d].

    AnchorGenerator will output a set of ``len(base_anchor_shapes)`` anchors
    per spatial location for feature map ``i``.

    Args:
        feature_map_scales: scale of anchors for each feature map, i.e., each output level of
            the feature pyramid network (FPN). ``len(feature_map_scales)`` is the number of feature maps.
            ``scale[i]*base_anchor_shapes`` represents the anchor shapes for feature map ``i``.
        base_anchor_shapes: a sequence which represents several anchor shapes for one feature map.
            For N-D images, it is a Sequence of N value Sequence.
        indexing: choose from {'xy', 'ij'}, optional
            Cartesian ('xy') or matrix ('ij', default) indexing of output.
            Cartesian ('xy') indexing swaps axis 0 and 1, which is the setting inside torchvision.
            matrix ('ij', default) indexing keeps the original axis not changed.
            See also indexing in https://pytorch.org/docs/stable/generated/torch.meshgrid.html

    Example:
        .. code-block:: python

            # 2D example inputs for a 2-level feature maps
            feature_map_scales = (1, 2)
            base_anchor_shapes = ((10, 10), (6, 12), (12, 6))
            anchor_generator = AnchorGeneratorWithAnchorShape(feature_map_scales, base_anchor_shapes)

            # 3D example inputs for a 2-level feature maps
            feature_map_scales = (1, 2)
            base_anchor_shapes = ((10, 10, 10), (12, 12, 8), (10, 10, 6), (16, 16, 10))
            anchor_generator = AnchorGeneratorWithAnchorShape(feature_map_scales, base_anchor_shapes)
    """
    __annotations__ = {'cell_anchors': List[torch.Tensor]}

    def __init__(self, feature_map_scales: Union[Sequence[int], Sequence[float]]=(1, 2, 4, 8), base_anchor_shapes: Union[Sequence[Sequence[int]], Sequence[Sequence[float]]]=((32, 32, 32), (48, 20, 20), (20, 48, 20), (20, 20, 48)), indexing: str='ij') ->None:
        nn.Module.__init__(self)
        spatial_dims = len(base_anchor_shapes[0])
        spatial_dims = look_up_option(spatial_dims, [2, 3])
        self.spatial_dims = spatial_dims
        self.indexing = look_up_option(indexing, ['ij', 'xy'])
        base_anchor_shapes_t = torch.Tensor(base_anchor_shapes)
        self.cell_anchors = [self.generate_anchors_using_shape(s * base_anchor_shapes_t) for s in feature_map_scales]

    @staticmethod
    def generate_anchors_using_shape(anchor_shapes: torch.Tensor, dtype: torch.dtype=torch.float32, device: Union[torch.device, None]=None) ->torch.Tensor:
        """
        Compute cell anchor shapes at multiple sizes and aspect ratios for the current feature map.

        Args:
            anchor_shapes: [w, h] or [w, h, d], sized (N, spatial_dims),
                represents N anchor shapes for the current feature map.
            dtype: target data type of the output Tensor.
            device: target device to put the output Tensor data.

        Returns:
            For 2D images, returns [-w/2, -h/2, w/2, h/2];
            For 3D images, returns [-w/2, -h/2, -d/2, w/2, h/2, d/2]
        """
        half_anchor_shapes = anchor_shapes / 2.0
        base_anchors = torch.cat([-half_anchor_shapes, half_anchor_shapes], dim=1)
        return base_anchors.round()


class LossReduction(StrEnum):
    """
    See also:
        - :py:class:`monai.losses.dice.DiceLoss`
        - :py:class:`monai.losses.dice.GeneralizedDiceLoss`
        - :py:class:`monai.losses.focal_loss.FocalLoss`
        - :py:class:`monai.losses.tversky.TverskyLoss`
    """
    NONE = 'none'
    MEAN = 'mean'
    SUM = 'sum'


def one_hot(labels: torch.Tensor, num_classes: int, dtype: torch.dtype=torch.float, dim: int=1) ->torch.Tensor:
    """
    For every value v in `labels`, the value in the output will be either 1 or 0. Each vector along the `dim`-th
    dimension has the "one-hot" format, i.e., it has a total length of `num_classes`,
    with a one and `num_class-1` zeros.
    Note that this will include the background label, thus a binary mask should be treated as having two classes.

    Args:
        labels: input tensor of integers to be converted into the 'one-hot' format. Internally `labels` will be
            converted into integers `labels.long()`.
        num_classes: number of output channels, the corresponding length of `labels[dim]` will be converted to
            `num_classes` from `1`.
        dtype: the data type of the output one_hot label.
        dim: the dimension to be converted to `num_classes` channels from `1` channel, should be non-negative number.

    Example:

    For a tensor `labels` of dimensions [B]1[spatial_dims], return a tensor of dimensions `[B]N[spatial_dims]`
    when `num_classes=N` number of classes and `dim=1`.

    .. code-block:: python

        from monai.networks.utils import one_hot
        import torch

        a = torch.randint(0, 2, size=(1, 2, 2, 2))
        out = one_hot(a, num_classes=2, dim=0)
        print(out.shape)  # torch.Size([2, 2, 2, 2])

        a = torch.randint(0, 2, size=(2, 1, 2, 2, 2))
        out = one_hot(a, num_classes=2, dim=1)
        print(out.shape)  # torch.Size([2, 2, 2, 2, 2])

    """
    if labels.ndim < dim + 1:
        shape = list(labels.shape) + [1] * (dim + 1 - len(labels.shape))
        labels = torch.reshape(labels, shape)
    sh = list(labels.shape)
    if sh[dim] != 1:
        raise AssertionError('labels should have a channel with length equal to one.')
    sh[dim] = num_classes
    o = torch.zeros(size=sh, dtype=dtype, device=labels.device)
    labels = o.scatter_(dim=dim, index=labels.long(), value=1)
    return labels


class DiceLoss(_Loss):
    """
    Compute average Dice loss between two tensors. It can support both multi-classes and multi-labels tasks.
    The data `input` (BNHW[D] where N is number of classes) is compared with ground truth `target` (BNHW[D]).

    Note that axis N of `input` is expected to be logits or probabilities for each class, if passing logits as input,
    must set `sigmoid=True` or `softmax=True`, or specifying `other_act`. And the same axis of `target`
    can be 1 or N (one-hot format).

    The `smooth_nr` and `smooth_dr` parameters are values added to the intersection and union components of
    the inter-over-union calculation to smooth results respectively, these values should be small.

    The original paper: Milletari, F. et. al. (2016) V-Net: Fully Convolutional Neural Networks forVolumetric
    Medical Image Segmentation, 3DV, 2016.

    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Optional[Callable]=None, squared_pred: bool=False, jaccard: bool=False, reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False) ->None:
        """
        Args:
            include_background: if False, channel index 0 (background category) is excluded from the calculation.
                if the non-background segmentations are small compared to the total image size they can get overwhelmed
                by the signal from the background so excluding it in such cases helps convergence.
            to_onehot_y: whether to convert the ``target`` into the one-hot format,
                using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
            sigmoid: if True, apply a sigmoid function to the prediction.
            softmax: if True, apply a softmax function to the prediction.
            other_act: callable function to execute other activation layers, Defaults to ``None``. for example:
                ``other_act = torch.tanh``.
            squared_pred: use squared versions of targets and predictions in the denominator or not.
            jaccard: compute Jaccard Index (soft IoU) instead of dice or not.
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.

            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, a Dice loss value is computed independently from each item in the batch
                before any `reduction`.

        Raises:
            TypeError: When ``other_act`` is not an ``Optional[Callable]``.
            ValueError: When more than 1 of [``sigmoid=True``, ``softmax=True``, ``other_act is not None``].
                Incompatible values.

        """
        super().__init__(reduction=LossReduction(reduction).value)
        if other_act is not None and not callable(other_act):
            raise TypeError(f'other_act must be None or callable but is {type(other_act).__name__}.')
        if int(sigmoid) + int(softmax) + int(other_act is not None) > 1:
            raise ValueError('Incompatible values: more than 1 of [sigmoid=True, softmax=True, other_act is not None].')
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        self.sigmoid = sigmoid
        self.softmax = softmax
        self.other_act = other_act
        self.squared_pred = squared_pred
        self.jaccard = jaccard
        self.smooth_nr = float(smooth_nr)
        self.smooth_dr = float(smooth_dr)
        self.batch = batch

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD], where N is the number of classes.
            target: the shape should be BNH[WD] or B1H[WD], where N is the number of classes.

        Raises:
            AssertionError: When input and target (after one hot transform if set)
                have different shapes.
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].

        Example:
            >>> from monai.losses.dice import *  # NOQA
            >>> import torch
            >>> from monai.losses.dice import DiceLoss
            >>> B, C, H, W = 7, 5, 3, 2
            >>> input = torch.rand(B, C, H, W)
            >>> target_idx = torch.randint(low=0, high=C - 1, size=(B, H, W)).long()
            >>> target = one_hot(target_idx[:, None, ...], num_classes=C)
            >>> self = DiceLoss(reduction='none')
            >>> loss = self(input, target)
            >>> assert np.broadcast_shapes(loss.shape, input.shape) == input.shape
        """
        if self.sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if self.softmax:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `softmax=True` ignored.')
            else:
                input = torch.softmax(input, 1)
        if self.other_act is not None:
            input = self.other_act(input)
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        if not self.include_background:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `include_background=False` ignored.')
            else:
                target = target[:, 1:]
                input = input[:, 1:]
        if target.shape != input.shape:
            raise AssertionError(f'ground truth has different shape ({target.shape}) from input ({input.shape})')
        reduce_axis: List[int] = torch.arange(2, len(input.shape)).tolist()
        if self.batch:
            reduce_axis = [0] + reduce_axis
        intersection = torch.sum(target * input, dim=reduce_axis)
        if self.squared_pred:
            target = torch.pow(target, 2)
            input = torch.pow(input, 2)
        ground_o = torch.sum(target, dim=reduce_axis)
        pred_o = torch.sum(input, dim=reduce_axis)
        denominator = ground_o + pred_o
        if self.jaccard:
            denominator = 2.0 * (denominator - intersection)
        f: torch.Tensor = 1.0 - (2.0 * intersection + self.smooth_nr) / (denominator + self.smooth_dr)
        if self.reduction == LossReduction.MEAN.value:
            f = torch.mean(f)
        elif self.reduction == LossReduction.SUM.value:
            f = torch.sum(f)
        elif self.reduction == LossReduction.NONE.value:
            broadcast_shape = list(f.shape[0:2]) + [1] * (len(input.shape) - 2)
            f = f.view(broadcast_shape)
        else:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return f


class HoVerNetBranch(StrEnum):
    """
    Three branches of HoVerNet model, which results in three outputs:
    `HV` is horizontal and vertical gradient map of each nucleus (regression),
    `NP` is the pixel prediction of all nuclei (segmentation), and
    `NC` is the type of each nucleus (classification).
    """
    HV = 'horizontal_vertical'
    NP = 'nucleus_prediction'
    NC = 'type_prediction'


class SobelGradients(Transform):
    """Calculate Sobel gradients of a grayscale image with the shape of CxH[xWxDx...] or BxH[xWxDx...].

    Args:
        kernel_size: the size of the Sobel kernel. Defaults to 3.
        spatial_axes: the axes that define the direction of the gradient to be calculated. It calculate the gradient
            along each of the provide axis. By default it calculate the gradient for all spatial axes.
        normalize_kernels: if normalize the Sobel kernel to provide proper gradients. Defaults to True.
        normalize_gradients: if normalize the output gradient to 0 and 1. Defaults to False.
        padding_mode: the padding mode of the image when convolving with Sobel kernels. Defaults to `"reflect"`.
            Acceptable values are ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
            See ``torch.nn.Conv1d()`` for more information.
        dtype: kernel data type (torch.dtype). Defaults to `torch.float32`.

    """
    backend = [TransformBackends.TORCH]

    def __init__(self, kernel_size: int=3, spatial_axes: Optional[Union[Sequence[int], int]]=None, normalize_kernels: bool=True, normalize_gradients: bool=False, padding_mode: str='reflect', dtype: torch.dtype=torch.float32) ->None:
        super().__init__()
        self.padding = padding_mode
        self.spatial_axes = spatial_axes
        self.normalize_kernels = normalize_kernels
        self.normalize_gradients = normalize_gradients
        self.kernel_diff, self.kernel_smooth = self._get_kernel(kernel_size, dtype)

    def _get_kernel(self, size, dtype) ->Tuple[torch.Tensor, torch.Tensor]:
        if size < 3:
            raise ValueError(f'Sobel kernel size should be at least three. {size} was given.')
        if size % 2 == 0:
            raise ValueError(f'Sobel kernel size should be an odd number. {size} was given.')
        kernel_diff = torch.tensor([[[-1, 0, 1]]], dtype=dtype)
        kernel_smooth = torch.tensor([[[1, 2, 1]]], dtype=dtype)
        kernel_expansion = torch.tensor([[[1, 2, 1]]], dtype=dtype)
        if self.normalize_kernels:
            if not dtype.is_floating_point:
                raise ValueError(f'`dtype` for Sobel kernel should be floating point when `normalize_kernel==True`. {dtype} was given.')
            kernel_diff /= 2.0
            kernel_smooth /= 4.0
            kernel_expansion /= 4.0
        expand = (size - 3) // 2
        for _ in range(expand):
            kernel_diff = F.conv1d(kernel_diff, kernel_expansion, padding=2)
            kernel_smooth = F.conv1d(kernel_smooth, kernel_expansion, padding=2)
        return kernel_diff.squeeze(), kernel_smooth.squeeze()

    def __call__(self, image: NdarrayOrTensor) ->torch.Tensor:
        image_tensor = convert_to_tensor(image, track_meta=get_track_meta())
        n_spatial_dims = image_tensor.ndim - 1
        valid_spatial_axes = list(range(n_spatial_dims)) + list(range(-n_spatial_dims, 0))
        if self.spatial_axes is None:
            spatial_axes = list(range(n_spatial_dims))
        else:
            invalid_axis = set(ensure_tuple(self.spatial_axes)) - set(valid_spatial_axes)
            if invalid_axis:
                raise ValueError(f'The provide axes to calculate gradient is not valid: {invalid_axis}. The image has {n_spatial_dims} spatial dimensions so it should be: {valid_spatial_axes}.')
            spatial_axes = [(ax % n_spatial_dims if ax < 0 else ax) for ax in ensure_tuple(self.spatial_axes)]
        image_tensor = image_tensor.unsqueeze(0)
        kernel_diff = self.kernel_diff
        kernel_smooth = self.kernel_smooth
        grad_list = []
        for ax in spatial_axes:
            kernels = [kernel_smooth] * n_spatial_dims
            kernels[ax] = kernel_diff
            grad = separable_filtering(image_tensor, kernels, mode=self.padding)
            if self.normalize_gradients:
                grad_min = grad.min()
                if grad_min != grad.max():
                    grad -= grad_min
                grad_max = grad.max()
                if grad_max > 0:
                    grad /= grad_max
            grad_list.append(grad)
        grads = torch.cat(grad_list, dim=1)
        grads = convert_to_dst_type(grads.squeeze(0), image_tensor)[0]
        return grads


class HoVerNetLoss(_Loss):
    """
    Loss function for HoVerNet pipeline, which is combination of losses across the three branches.
    The NP (nucleus prediction) branch uses Dice + CrossEntropy.
    The HV (Horizontal and Vertical) distance from centroid branch uses MSE + MSE of the gradient.
    The NC (Nuclear Class prediction) branch uses Dice + CrossEntropy
    The result is a weighted sum of these losses.

    Args:
        lambda_hv_mse: Weight factor to apply to the HV regression MSE part of the overall loss
        lambda_hv_mse_grad: Weight factor to apply to the MSE of the HV gradient part of the overall loss
        lambda_np_ce: Weight factor to apply to the nuclei prediction CrossEntropyLoss part
            of the overall loss
        lambda_np_dice: Weight factor to apply to the nuclei prediction DiceLoss part of overall loss
        lambda_nc_ce: Weight factor to apply to the nuclei class prediction CrossEntropyLoss part
            of the overall loss
        lambda_nc_dice: Weight factor to apply to the nuclei class prediction DiceLoss part of the
            overall loss

    """

    def __init__(self, lambda_hv_mse: float=2.0, lambda_hv_mse_grad: float=1.0, lambda_np_ce: float=1.0, lambda_np_dice: float=1.0, lambda_nc_ce: float=1.0, lambda_nc_dice: float=1.0) ->None:
        self.lambda_hv_mse = lambda_hv_mse
        self.lambda_hv_mse_grad = lambda_hv_mse_grad
        self.lambda_np_ce = lambda_np_ce
        self.lambda_np_dice = lambda_np_dice
        self.lambda_nc_ce = lambda_nc_ce
        self.lambda_nc_dice = lambda_nc_dice
        super().__init__()
        self.dice = DiceLoss(softmax=True, smooth_dr=0.001, smooth_nr=0.001, reduction='sum', batch=True)
        self.ce = CrossEntropyLoss(reduction='mean')
        self.sobel_v = SobelGradients(kernel_size=5, spatial_axes=0)
        self.sobel_h = SobelGradients(kernel_size=5, spatial_axes=1)

    def _compute_sobel(self, image: torch.Tensor) ->torch.Tensor:
        """Compute the Sobel gradients of the horizontal vertical map (HoVerMap).
        More specifically, it will compute horizontal gradient of the input horizontal gradient map (channel=0) and
        vertical gradient of the input vertical gradient map (channel=1).

        Args:
            image: a tensor with the shape of BxCxHxW representing HoVerMap

        """
        result_h = self.sobel_h(image[:, 0])
        result_v = self.sobel_v(image[:, 1])
        return torch.stack([result_h, result_v], dim=1)

    def _mse_gradient_loss(self, prediction: torch.Tensor, target: torch.Tensor, focus: torch.Tensor) ->torch.Tensor:
        """Compute the MSE loss of the gradients of the horizontal and vertical centroid distance maps"""
        pred_grad = self._compute_sobel(prediction)
        true_grad = self._compute_sobel(target)
        loss = pred_grad - true_grad
        focus = focus[:, None, ...]
        focus = torch.cat((focus, focus), 1)
        loss = focus * (loss * loss)
        loss = loss.sum() / (focus.sum() + 1e-08)
        return loss

    def forward(self, prediction: Dict[str, torch.Tensor], target: Dict[str, torch.Tensor]) ->torch.Tensor:
        """
        Args:
            prediction: dictionary of predicted outputs for three branches,
                each of which should have the shape of BNHW.
            target: dictionary of ground truths for three branches,
                each of which should have the shape of BNHW.
        """
        if not (HoVerNetBranch.NP.value in prediction and HoVerNetBranch.HV.value in prediction):
            raise ValueError('nucleus prediction (NP) and horizontal_vertical (HV) branches must be present for prediction and target parameters')
        if not (HoVerNetBranch.NP.value in target and HoVerNetBranch.HV.value in target):
            raise ValueError('nucleus prediction (NP) and horizontal_vertical (HV) branches must be present for prediction and target parameters')
        if HoVerNetBranch.NC.value not in target and HoVerNetBranch.NC.value in target:
            raise ValueError('type_prediction (NC) must be present in both or neither of the prediction and target parameters')
        if HoVerNetBranch.NC.value in target and HoVerNetBranch.NC.value not in target:
            raise ValueError('type_prediction (NC) must be present in both or neither of the prediction and target parameters')
        dice_loss_np = self.dice(prediction[HoVerNetBranch.NP.value], target[HoVerNetBranch.NP.value]) * self.lambda_np_dice
        argmax_target = target[HoVerNetBranch.NP.value].argmax(dim=1)
        ce_loss_np = self.ce(prediction[HoVerNetBranch.NP.value], argmax_target) * self.lambda_np_ce
        loss_np = dice_loss_np + ce_loss_np
        loss_hv_mse = F.mse_loss(prediction[HoVerNetBranch.HV.value], target[HoVerNetBranch.HV.value]) * self.lambda_hv_mse
        loss_hv_mse_grad = self._mse_gradient_loss(prediction[HoVerNetBranch.HV.value], target[HoVerNetBranch.HV.value], target[HoVerNetBranch.NP.value][:, 1]) * self.lambda_hv_mse_grad
        loss_hv = loss_hv_mse_grad + loss_hv_mse
        loss_nc = 0
        if HoVerNetBranch.NC.value in prediction:
            dice_loss_nc = self.dice(prediction[HoVerNetBranch.NC.value], target[HoVerNetBranch.NC.value]) * self.lambda_nc_dice
            argmax_target = target[HoVerNetBranch.NC.value].argmax(dim=1)
            ce_loss_nc = self.ce(prediction[HoVerNetBranch.NC.value], argmax_target) * self.lambda_nc_ce
            loss_nc = dice_loss_nc + ce_loss_nc
        loss: torch.Tensor = loss_hv + loss_np + loss_nc
        return loss


def complex_mul_t(x: Tensor, y: Tensor) ->Tensor:
    """
    Compute complex-valued multiplication. Supports Ndim inputs with last dim equal to 2 (real/imaginary channels)

    Args:
        x: Input tensor with 2 channels in the last dimension representing real and imaginary parts.
        y: Input tensor with 2 channels in the last dimension representing real and imaginary parts.

    Returns:
        Complex multiplication of x and y
    """
    if x.shape[-1] != 2 or y.shape[-1] != 2:
        raise ValueError(f'last dim must be 2, but x.shape[-1] is {x.shape[-1]} and y.shape[-1] is {y.shape[-1]}.')
    real_part = x[..., 0] * y[..., 0] - x[..., 1] * y[..., 1]
    imag_part = x[..., 0] * y[..., 1] + x[..., 1] * y[..., 0]
    return torch.stack((real_part, imag_part), dim=-1)


def roll_1d(x: Tensor, shift: int, shift_dim: int) ->Tensor:
    """
    Similar to roll but for only one dim.

    Args:
        x: input data (k-space or image) that can be
            1) real-valued: the shape is (C,H,W) for 2D spatial inputs and (C,H,W,D) for 3D, or
            2) complex-valued: the shape is (C,H,W,2) for 2D spatial data and (C,H,W,D,2) for 3D. C is the number of channels.
        shift: the amount of shift along each of shift_dims dimension
        shift_dim: the dimension over which the shift is applied

    Returns:
        1d-shifted version of x

    Note:
        This function is called when fftshift and ifftshift are not available in the running pytorch version
    """
    shift = shift % x.size(shift_dim)
    if shift == 0:
        return x
    left = x.narrow(shift_dim, 0, x.size(shift_dim) - shift)
    right = x.narrow(shift_dim, x.size(shift_dim) - shift, shift)
    return torch.cat((right, left), dim=shift_dim)


def roll(x: Tensor, shift: List[int], shift_dims: List[int]) ->Tensor:
    """
    Similar to np.roll but applies to PyTorch Tensors

    Args:
        x: input data (k-space or image) that can be
            1) real-valued: the shape is (C,H,W) for 2D spatial inputs and (C,H,W,D) for 3D, or
            2) complex-valued: the shape is (C,H,W,2) for 2D spatial data and (C,H,W,D,2) for 3D. C is the number of channels.
        shift: the amount of shift along each of shift_dims dimensions
        shift_dims: dimensions over which the shift is applied

    Returns:
        shifted version of x

    Note:
        This function is called when fftshift and ifftshift are not available in the running pytorch version
    """
    if len(shift) != len(shift_dims):
        raise ValueError(f'len(shift) != len(shift_dims), got f{len(shift)} and f{len(shift_dims)}.')
    for s, d in zip(shift, shift_dims):
        x = roll_1d(x, s, d)
    return x


def ifftshift(x: Tensor, shift_dims: List[int]) ->Tensor:
    """
    Similar to np.fft.ifftshift but applies to PyTorch Tensors

    Args:
        x: input data (k-space or image) that can be
            1) real-valued: the shape is (C,H,W) for 2D spatial inputs and (C,H,W,D) for 3D, or
            2) complex-valued: the shape is (C,H,W,2) for 2D spatial data and (C,H,W,D,2) for 3D. C is the number of channels.
        shift_dims: dimensions over which the shift is applied

    Returns:
        ifft-shifted version of x

    Note:
        This function is called when ifftshift is not available in the running pytorch version
    """
    shift = [0] * len(shift_dims)
    for i, dim_num in enumerate(shift_dims):
        shift[i] = (x.shape[dim_num] + 1) // 2
    return roll(x, shift, shift_dims)


def fftn_centered_t(im: Tensor, spatial_dims: int, is_complex: bool=True) ->Tensor:
    """
    Pytorch-based fft for spatial_dims-dim signals. "centered" means this function automatically takes care
    of the required ifft and fft shifts.
    This is equivalent to do ifft in numpy based on numpy.fft.fftn, numpy.fft.fftshift, and numpy.fft.ifftshift

    Args:
        im: image that can be
            1) real-valued: the shape is (C,H,W) for 2D spatial inputs and (C,H,W,D) for 3D, or
            2) complex-valued: the shape is (C,H,W,2) for 2D spatial data and (C,H,W,D,2) for 3D. C is the number of channels.
        spatial_dims: number of spatial dimensions (e.g., is 2 for an image, and is 3 for a volume)
        is_complex: if True, then the last dimension of the input im is expected to be 2 (representing real and imaginary channels)

    Returns:
        "out" which is the output kspace (fourier of im)

    Example:

        .. code-block:: python

            import torch
            im = torch.ones(1,3,3,2) # the last dim belongs to real/imaginary parts
            # output1 and output2 will be identical
            output1 = torch.fft.fftn(torch.view_as_complex(torch.fft.ifftshift(im,dim=(-3,-2))), dim=(-2,-1), norm="ortho")
            output1 = torch.fft.fftshift( torch.view_as_real(output1), dim=(-3,-2) )

            output2 = fftn_centered(im, spatial_dims=2, is_complex=True)
    """
    shift = list(range(-spatial_dims, 0))
    if is_complex:
        if im.shape[-1] != 2:
            raise ValueError(f'img.shape[-1] is not 2 ({im.shape[-1]}).')
        shift = list(range(-spatial_dims - 1, -1))
    dims = list(range(-spatial_dims, 0))
    x = ifftshift(im, shift)
    if is_complex:
        x = torch.view_as_real(torch.fft.fftn(torch.view_as_complex(x), dim=dims, norm='ortho'))
    else:
        x = torch.view_as_real(torch.fft.fftn(x, dim=dims, norm='ortho'))
    out: Tensor = fftshift(x, shift)
    return out


def sensitivity_map_expand(img: Tensor, sens_maps: Tensor, spatial_dims: int=2) ->Tensor:
    """
    Expands an image to its corresponding coil images based on the given sens_maps. Let's say there
    are C coils. This function multiples image img with each coil sensitivity map in sens_maps and stacks
    the resulting C coil images along the channel dimension which is reserved for coils.

    Args:
        img: 2D image (B,1,H,W,2) with the last dimension being 2 (for real/imaginary parts). 3D data will have
            the shape (B,1,H,W,D,2).
        sens_maps: Sensitivity maps for combining coil images. The shape is (B,C,H,W,2) for 2D data
            or (B,C,H,W,D,2) for 3D data (C denotes the coil dimension).
        spatial_dims: is 2 for 2D data and is 3 for 3D data

    Returns:
        Expansion of x to (B,C,H,W,2) for 2D data and (B,C,H,W,D,2) for 3D data. The output is transferred
            to the frequency domain to yield coil measurements.
    """
    return fftn_centered_t(complex_mul_t(img, sens_maps), spatial_dims=spatial_dims, is_complex=True)


def complex_conj_t(x: Tensor) ->Tensor:
    """
    Compute complex conjugate of a tensor. Supports Ndim inputs with last dim equal to 2 (real/imaginary channels)

    Args:
        x: Input tensor with 2 channels in the last dimension representing real and imaginary parts.

    Returns:
        Complex conjugate of x
    """
    if x.shape[-1] != 2:
        raise ValueError(f'last dim must be 2, but x.shape[-1] is {x.shape[-1]}.')
    return torch.stack((x[..., 0], -x[..., 1]), dim=-1)


def ifftn_centered_t(ksp: Tensor, spatial_dims: int, is_complex: bool=True) ->Tensor:
    """
    Pytorch-based ifft for spatial_dims-dim signals. "centered" means this function automatically takes care
    of the required ifft and fft shifts.
    This is equivalent to do fft in numpy based on numpy.fft.ifftn, numpy.fft.fftshift, and numpy.fft.ifftshift

    Args:
        ksp: k-space data that can be
            1) real-valued: the shape is (C,H,W) for 2D spatial inputs and (C,H,W,D) for 3D, or
            2) complex-valued: the shape is (C,H,W,2) for 2D spatial data and (C,H,W,D,2) for 3D. C is the number of channels.
        spatial_dims: number of spatial dimensions (e.g., is 2 for an image, and is 3 for a volume)
        is_complex: if True, then the last dimension of the input ksp is expected to be 2 (representing real and imaginary channels)

    Returns:
        "out" which is the output image (inverse fourier of ksp)

    Example:

        .. code-block:: python

            import torch
            ksp = torch.ones(1,3,3,2) # the last dim belongs to real/imaginary parts
            # output1 and output2 will be identical
            output1 = torch.fft.ifftn(torch.view_as_complex(torch.fft.ifftshift(ksp,dim=(-3,-2))), dim=(-2,-1), norm="ortho")
            output1 = torch.fft.fftshift( torch.view_as_real(output1), dim=(-3,-2) )

            output2 = ifftn_centered(ksp, spatial_dims=2, is_complex=True)
    """
    shift = list(range(-spatial_dims, 0))
    if is_complex:
        if ksp.shape[-1] != 2:
            raise ValueError(f'ksp.shape[-1] is not 2 ({ksp.shape[-1]}).')
        shift = list(range(-spatial_dims - 1, -1))
    dims = list(range(-spatial_dims, 0))
    x = ifftshift(ksp, shift)
    if is_complex:
        x = torch.view_as_real(torch.fft.ifftn(torch.view_as_complex(x), dim=dims, norm='ortho'))
    else:
        x = torch.view_as_real(torch.fft.ifftn(x, dim=dims, norm='ortho'))
    out: Tensor = fftshift(x, shift)
    return out


def sensitivity_map_reduce(kspace: Tensor, sens_maps: Tensor, spatial_dims: int=2) ->Tensor:
    """
    Reduces coil measurements to a corresponding image based on the given sens_maps. Let's say there
    are C coil measurements inside kspace, then this function multiplies the conjugate of each coil sensitivity map with the
    corresponding coil image. The result of this process will be C images. Summing those images together gives the
    resulting "reduced image."

    Args:
        kspace: 2D kspace (B,C,H,W,2) with the last dimension being 2 (for real/imaginary parts) and C denoting the
            coil dimension. 3D data will have the shape (B,C,H,W,D,2).
        sens_maps: sensitivity maps of the same shape as input x.
        spatial_dims: is 2 for 2D data and is 3 for 3D data

    Returns:
        reduction of x to (B,1,H,W,2) for 2D data or (B,1,H,W,D,2) for 3D data.
    """
    img = ifftn_centered_t(kspace, spatial_dims=spatial_dims, is_complex=True)
    return complex_mul_t(img, complex_conj_t(sens_maps)).sum(dim=1, keepdim=True)


class VarNetBlock(nn.Module):
    """
    A variational block based on Sriram et. al., "End-to-end variational networks for accelerated MRI reconstruction".
    It applies data consistency and refinement to the intermediate kspace and combines those results.

    Modified and adopted from: https://github.com/facebookresearch/fastMRI

    Args:
        refinement_model: the model used for refinement (typically a U-Net but can be any deep learning model
            that performs well when the input and output are in image domain (e.g., a convolutional network).
        spatial_dims: is 2 for 2D data and is 3 for 3D data
    """

    def __init__(self, refinement_model: nn.Module, spatial_dims: int=2):
        super().__init__()
        self.model = refinement_model
        self.spatial_dims = spatial_dims
        self.dc_weight = nn.Parameter(torch.ones(1))
        buffer_shape = [(1) for _ in range(spatial_dims + 3)]
        self.register_buffer('zeros', torch.zeros(buffer_shape))

    def soft_dc(self, x: Tensor, ref_kspace: Tensor, mask: Tensor) ->Tensor:
        """
        Applies data consistency to input x. Suppose x is an intermediate estimate of the kspace and ref_kspace
        is the reference under-sampled measurement. This function returns mask * (x - ref_kspace). View this as the
        residual between the original under-sampled kspace and the estimate given by the network.

        Args:
            x: 2D kspace (B,C,H,W,2) with the last dimension being 2 (for real/imaginary parts) and C denoting the
                coil dimension. 3D data will have the shape (B,C,H,W,D,2).
            ref_kspace: original under-sampled kspace with the same shape as x.
            mask: the under-sampling mask with shape (1,1,1,W,1) for 2D data or (1,1,1,1,D,1) for 3D data.

        Returns:
            Output of DC block with the same shape as x
        """
        return torch.where(mask, x - ref_kspace, self.zeros) * self.dc_weight

    def forward(self, current_kspace: Tensor, ref_kspace: Tensor, mask: Tensor, sens_maps: Tensor) ->Tensor:
        """
        Args:
            current_kspace: Predicted kspace from the previous block. It's a 2D kspace (B,C,H,W,2)
                with the last dimension being 2 (for real/imaginary parts) and C denoting the
                coil dimension. 3D data will have the shape (B,C,H,W,D,2).
            ref_kspace: reference kspace for applying data consistency (is the under-sampled kspace in MRI reconstruction).
                Its shape is the same as current_kspace.
            mask: the under-sampling mask with shape (1,1,1,W,1) for 2D data or (1,1,1,1,D,1) for 3D data.
            sens_maps: coil sensitivity maps with the same shape as current_kspace

        Returns:
            Output of VarNetBlock with the same shape as current_kspace
        """
        dc_out = self.soft_dc(current_kspace, ref_kspace, mask)
        refinement_out = sensitivity_map_expand(self.model(sensitivity_map_reduce(current_kspace, sens_maps, spatial_dims=self.spatial_dims)), sens_maps, spatial_dims=self.spatial_dims)
        output = current_kspace - dc_out - refinement_out
        return output


Pool = LayerFactory()


Act = LayerFactory()


def split_args(args):
    """
    Split arguments in a way to be suitable for using with the factory types. If `args` is a string it's interpreted as
    the type name.

    Args:
        args (str or a tuple of object name and kwarg dict): input arguments to be parsed.

    Raises:
        TypeError: When ``args`` type is not in ``Union[str, Tuple[Union[str, Callable], dict]]``.

    Examples::

        >>> act_type, args = split_args("PRELU")
        >>> monai.networks.layers.Act[act_type]
        <class 'torch.nn.modules.activation.PReLU'>

        >>> act_type, args = split_args(("PRELU", {"num_parameters": 1, "init": 0.25}))
        >>> monai.networks.layers.Act[act_type](**args)
        PReLU(num_parameters=1)

    """
    if isinstance(args, str):
        return args, {}
    name_obj, name_args = args
    if not (isinstance(name_obj, str) or callable(name_obj)) or not isinstance(name_args, dict):
        msg = 'Layer specifiers must be single strings or pairs of the form (name/object-types, argument dict)'
        raise TypeError(msg)
    return name_obj, name_args


def get_act_layer(name: Union[Tuple, str]):
    """
    Create an activation layer instance.

    For example, to create activation layers:

    .. code-block:: python

        from monai.networks.layers import get_act_layer

        s_layer = get_act_layer(name="swish")
        p_layer = get_act_layer(name=("prelu", {"num_parameters": 1, "init": 0.25}))

    Args:
        name: an activation type string or a tuple of type string and parameters.
    """
    if name == '':
        return torch.nn.Identity()
    act_name, act_args = split_args(name)
    act_type = Act[act_name]
    return act_type(**act_args)


Dropout = LayerFactory()


def get_dropout_layer(name: Union[Tuple, str, float, int], dropout_dim: Optional[int]=1):
    """
    Create a dropout layer instance.

    For example, to create dropout layers:

    .. code-block:: python

        from monai.networks.layers import get_dropout_layer

        d_layer = get_dropout_layer(name="dropout")
        a_layer = get_dropout_layer(name=("alphadropout", {"p": 0.25}))

    Args:
        name: a dropout ratio or a tuple of dropout type and parameters.
        dropout_dim: the spatial dimension of the dropout operation.
    """
    if name == '':
        return torch.nn.Identity()
    if isinstance(name, (int, float)):
        drop_name = Dropout.DROPOUT
        drop_args = {'p': float(name)}
    else:
        drop_name, drop_args = split_args(name)
    drop_type = Dropout[drop_name, dropout_dim]
    return drop_type(**drop_args)


Norm = LayerFactory()


def has_option(obj, keywords: Union[str, Sequence[str]]) ->bool:
    """
    Return a boolean indicating whether the given callable `obj` has the `keywords` in its signature.
    """
    if not callable(obj):
        return False
    sig = inspect.signature(obj)
    return all(key in sig.parameters for key in ensure_tuple(keywords))


def get_norm_layer(name: Union[Tuple, str], spatial_dims: Optional[int]=1, channels: Optional[int]=1):
    """
    Create a normalization layer instance.

    For example, to create normalization layers:

    .. code-block:: python

        from monai.networks.layers import get_norm_layer

        g_layer = get_norm_layer(name=("group", {"num_groups": 1}))
        n_layer = get_norm_layer(name="instance", spatial_dims=2)

    Args:
        name: a normalization type string or a tuple of type string and parameters.
        spatial_dims: number of spatial dimensions of the input.
        channels: number of features/channels when the normalization layer requires this parameter
            but it is not specified in the norm parameters.
    """
    if name == '':
        return torch.nn.Identity()
    norm_name, norm_args = split_args(name)
    norm_type = Norm[norm_name, spatial_dims]
    kw_args = dict(norm_args)
    if has_option(norm_type, 'num_features') and 'num_features' not in kw_args:
        kw_args['num_features'] = channels
    if has_option(norm_type, 'num_channels') and 'num_channels' not in kw_args:
        kw_args['num_channels'] = channels
    return norm_type(**kw_args)


class ADN(nn.Sequential):
    """
    Constructs a sequential module of optional activation (A), dropout (D), and normalization (N) layers
    with an arbitrary order::

        -- (Norm) -- (Dropout) -- (Acti) --

    Args:
        ordering: a string representing the ordering of activation, dropout, and normalization. Defaults to "NDA".
        in_channels: `C` from an expected input of size (N, C, H[, W, D]).
        act: activation type and arguments. Defaults to PReLU.
        norm: feature normalization type and arguments. Defaults to instance norm.
        norm_dim: determine the spatial dimensions of the normalization layer.
            defaults to `dropout_dim` if unspecified.
        dropout: dropout ratio. Defaults to no dropout.
        dropout_dim: determine the spatial dimensions of dropout.
            defaults to `norm_dim` if unspecified.

            - When dropout_dim = 1, randomly zeroes some of the elements for each channel.
            - When dropout_dim = 2, Randomly zeroes out entire channels (a channel is a 2D feature map).
            - When dropout_dim = 3, Randomly zeroes out entire channels (a channel is a 3D feature map).

    Examples::

        # activation, group norm, dropout
        >>> norm_params = ("GROUP", {"num_groups": 1, "affine": False})
        >>> ADN(norm=norm_params, in_channels=1, dropout_dim=1, dropout=0.8, ordering="AND")
        ADN(
            (A): ReLU()
            (N): GroupNorm(1, 1, eps=1e-05, affine=False)
            (D): Dropout(p=0.8, inplace=False)
        )

        # LeakyReLU, dropout
        >>> act_params = ("leakyrelu", {"negative_slope": 0.1, "inplace": True})
        >>> ADN(act=act_params, in_channels=1, dropout_dim=1, dropout=0.8, ordering="AD")
        ADN(
            (A): LeakyReLU(negative_slope=0.1, inplace=True)
            (D): Dropout(p=0.8, inplace=False)
        )

    See also:

        :py:class:`monai.networks.layers.Dropout`
        :py:class:`monai.networks.layers.Act`
        :py:class:`monai.networks.layers.Norm`
        :py:class:`monai.networks.layers.split_args`

    """

    def __init__(self, ordering: str='NDA', in_channels: Optional[int]=None, act: Optional[Union[Tuple, str]]='RELU', norm: Optional[Union[Tuple, str]]=None, norm_dim: Optional[int]=None, dropout: Optional[Union[Tuple, str, float]]=None, dropout_dim: Optional[int]=None) ->None:
        super().__init__()
        op_dict = {'A': None, 'D': None, 'N': None}
        if norm is not None:
            if norm_dim is None and dropout_dim is None:
                raise ValueError('norm_dim or dropout_dim needs to be specified.')
            op_dict['N'] = get_norm_layer(name=norm, spatial_dims=norm_dim or dropout_dim, channels=in_channels)
        if act is not None:
            op_dict['A'] = get_act_layer(act)
        if dropout is not None:
            if norm_dim is None and dropout_dim is None:
                raise ValueError('norm_dim or dropout_dim needs to be specified.')
            op_dict['D'] = get_dropout_layer(name=dropout, dropout_dim=dropout_dim or norm_dim)
        for item in ordering.upper():
            if item not in op_dict:
                raise ValueError(f'ordering must be a string of {op_dict}, got {item} in it.')
            if op_dict[item] is not None:
                self.add_module(item, op_dict[item])


def same_padding(kernel_size: Union[Sequence[int], int], dilation: Union[Sequence[int], int]=1) ->Union[Tuple[int, ...], int]:
    """
    Return the padding value needed to ensure a convolution using the given kernel size produces an output of the same
    shape as the input for a stride of 1, otherwise ensure a shape of the input divided by the stride rounded down.

    Raises:
        NotImplementedError: When ``np.any((kernel_size - 1) * dilation % 2 == 1)``.

    """
    kernel_size_np = np.atleast_1d(kernel_size)
    dilation_np = np.atleast_1d(dilation)
    if np.any((kernel_size_np - 1) * dilation % 2 == 1):
        raise NotImplementedError(f'Same padding not available for kernel_size={kernel_size_np} and dilation={dilation_np}.')
    padding_np = (kernel_size_np - 1) / 2 * dilation_np
    padding = tuple(int(p) for p in padding_np)
    return padding if len(padding) > 1 else padding[0]


def stride_minus_kernel_padding(kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int]) ->Union[Tuple[int, ...], int]:
    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    out_padding_np = stride_np - kernel_size_np
    out_padding = tuple(int(p) for p in out_padding_np)
    return out_padding if len(out_padding) > 1 else out_padding[0]


class Convolution(nn.Sequential):
    """
    Constructs a convolution with normalization, optional dropout, and optional activation layers::

        -- (Conv|ConvTrans) -- (Norm -- Dropout -- Acti) --

    if ``conv_only`` set to ``True``::

        -- (Conv|ConvTrans) --

    For example:

    .. code-block:: python

        from monai.networks.blocks import Convolution

        conv = Convolution(
            spatial_dims=3,
            in_channels=1,
            out_channels=1,
            adn_ordering="ADN",
            act=("prelu", {"init": 0.2}),
            dropout=0.1,
            norm=("layer", {"normalized_shape": (10, 10, 10)}),
        )
        print(conv)

    output::

        Convolution(
          (conv): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (adn): ADN(
            (A): PReLU(num_parameters=1)
            (D): Dropout(p=0.1, inplace=False)
            (N): LayerNorm((10, 10, 10), eps=1e-05, elementwise_affine=True)
          )
        )

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        strides: convolution stride. Defaults to 1.
        kernel_size: convolution kernel size. Defaults to 3.
        adn_ordering: a string representing the ordering of activation, normalization, and dropout.
            Defaults to "NDA".
        act: activation type and arguments. Defaults to PReLU.
        norm: feature normalization type and arguments. Defaults to instance norm.
        dropout: dropout ratio. Defaults to no dropout.
        dropout_dim: determine the spatial dimensions of dropout. Defaults to 1.

            - When dropout_dim = 1, randomly zeroes some of the elements for each channel.
            - When dropout_dim = 2, Randomly zeroes out entire channels (a channel is a 2D feature map).
            - When dropout_dim = 3, Randomly zeroes out entire channels (a channel is a 3D feature map).

            The value of dropout_dim should be no larger than the value of `spatial_dims`.
        dilation: dilation rate. Defaults to 1.
        groups: controls the connections between inputs and outputs. Defaults to 1.
        bias: whether to have a bias term. Defaults to True.
        conv_only: whether to use the convolutional layer only. Defaults to False.
        is_transposed: if True uses ConvTrans instead of Conv. Defaults to False.
        padding: controls the amount of implicit zero-paddings on both sides for padding number of points
            for each dimension. Defaults to None.
        output_padding: controls the additional size added to one side of the output shape.
            Defaults to None.

    See also:

        :py:class:`monai.networks.layers.Conv`
        :py:class:`monai.networks.blocks.ADN`

    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, strides: Union[Sequence[int], int]=1, kernel_size: Union[Sequence[int], int]=3, adn_ordering: str='NDA', act: Optional[Union[Tuple, str]]='PRELU', norm: Optional[Union[Tuple, str]]='INSTANCE', dropout: Optional[Union[Tuple, str, float]]=None, dropout_dim: Optional[int]=1, dilation: Union[Sequence[int], int]=1, groups: int=1, bias: bool=True, conv_only: bool=False, is_transposed: bool=False, padding: Optional[Union[Sequence[int], int]]=None, output_padding: Optional[Union[Sequence[int], int]]=None) ->None:
        super().__init__()
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.is_transposed = is_transposed
        if padding is None:
            padding = same_padding(kernel_size, dilation)
        conv_type = Conv[Conv.CONVTRANS if is_transposed else Conv.CONV, self.spatial_dims]
        conv: nn.Module
        if is_transposed:
            if output_padding is None:
                output_padding = stride_minus_kernel_padding(1, strides)
            conv = conv_type(in_channels, out_channels, kernel_size=kernel_size, stride=strides, padding=padding, output_padding=output_padding, groups=groups, bias=bias, dilation=dilation)
        else:
            conv = conv_type(in_channels, out_channels, kernel_size=kernel_size, stride=strides, padding=padding, dilation=dilation, groups=groups, bias=bias)
        self.add_module('conv', conv)
        if conv_only:
            return
        if act is None and norm is None and dropout is None:
            return
        self.add_module('adn', ADN(ordering=adn_ordering, in_channels=out_channels, act=act, norm=norm, norm_dim=self.spatial_dims, dropout=dropout, dropout_dim=dropout_dim))


class TwoConv(nn.Sequential):
    """two convolutions."""

    def __init__(self, spatial_dims: int, in_chns: int, out_chns: int, act: Union[str, tuple], norm: Union[str, tuple], bias: bool, dropout: Union[float, tuple]=0.0):
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_chns: number of input channels.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            bias: whether to have a bias term in convolution blocks.
            dropout: dropout ratio. Defaults to no dropout.

        """
        super().__init__()
        conv_0 = Convolution(spatial_dims, in_chns, out_chns, act=act, norm=norm, dropout=dropout, bias=bias, padding=1)
        conv_1 = Convolution(spatial_dims, out_chns, out_chns, act=act, norm=norm, dropout=dropout, bias=bias, padding=1)
        self.add_module('conv_0', conv_0)
        self.add_module('conv_1', conv_1)


class Down(nn.Sequential):
    """maxpooling downsampling and two convolutions."""

    def __init__(self, spatial_dims: int, in_chns: int, out_chns: int, act: Union[str, tuple], norm: Union[str, tuple], bias: bool, dropout: Union[float, tuple]=0.0):
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_chns: number of input channels.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            bias: whether to have a bias term in convolution blocks.
            dropout: dropout ratio. Defaults to no dropout.

        """
        super().__init__()
        max_pooling = Pool['MAX', spatial_dims](kernel_size=2)
        convs = TwoConv(spatial_dims, in_chns, out_chns, act, norm, bias, dropout)
        self.add_module('max_pooling', max_pooling)
        self.add_module('convs', convs)


def icnr_init(conv, upsample_factor, init=nn.init.kaiming_normal_):
    """
    ICNR initialization for 2D/3D kernels adapted from Aitken et al.,2017 , "Checkerboard artifact free
    sub-pixel convolution".
    """
    out_channels, in_channels, *dims = conv.weight.shape
    scale_factor = upsample_factor ** len(dims)
    oc2 = int(out_channels / scale_factor)
    kernel = torch.zeros([oc2, in_channels] + dims)
    kernel = init(kernel)
    kernel = kernel.transpose(0, 1)
    kernel = kernel.reshape(oc2, in_channels, -1)
    kernel = kernel.repeat(1, 1, scale_factor)
    kernel = kernel.reshape([in_channels, out_channels] + dims)
    kernel = kernel.transpose(0, 1)
    conv.weight.data.copy_(kernel)


def pixelshuffle(x: torch.Tensor, spatial_dims: int, scale_factor: int) ->torch.Tensor:
    """
    Apply pixel shuffle to the tensor `x` with spatial dimensions `spatial_dims` and scaling factor `scale_factor`.

    See: Shi et al., 2016, "Real-Time Single Image and Video Super-Resolution
    Using a nEfficient Sub-Pixel Convolutional Neural Network."

    See: Aitken et al., 2017, "Checkerboard artifact free sub-pixel convolution".

    Args:
        x: Input tensor
        spatial_dims: number of spatial dimensions, typically 2 or 3 for 2D or 3D
        scale_factor: factor to rescale the spatial dimensions by, must be >=1

    Returns:
        Reshuffled version of `x`.

    Raises:
        ValueError: When input channels of `x` are not divisible by (scale_factor ** spatial_dims)
    """
    dim, factor = spatial_dims, scale_factor
    input_size = list(x.size())
    batch_size, channels = input_size[:2]
    scale_divisor = factor ** dim
    if channels % scale_divisor != 0:
        raise ValueError(f'Number of input channels ({channels}) must be evenly divisible by scale_factor ** dimensions ({factor}**{dim}={scale_divisor}).')
    org_channels = int(channels // scale_divisor)
    output_size = [batch_size, org_channels] + [(d * factor) for d in input_size[2:]]
    indices = list(range(2, 2 + 2 * dim))
    indices = indices[dim:] + indices[:dim]
    permute_indices = [0, 1]
    for idx in range(dim):
        permute_indices.extend(indices[idx::dim])
    x = x.reshape([batch_size, org_channels] + [factor] * dim + input_size[2:])
    x = x.permute(permute_indices).reshape(output_size)
    return x


class SubpixelUpsample(nn.Module):
    """
    Upsample via using a subpixel CNN. This module supports 1D, 2D and 3D input images.
    The module is consisted with two parts. First of all, a convolutional layer is employed
    to increase the number of channels into: ``in_channels * (scale_factor ** dimensions)``.
    Secondly, a pixel shuffle manipulation is utilized to aggregates the feature maps from
    low resolution space and build the super resolution space.
    The first part of the module is not fixed, a sequential layers can be used to replace the
    default single layer.

    See: Shi et al., 2016, "Real-Time Single Image and Video Super-Resolution
    Using a nEfficient Sub-Pixel Convolutional Neural Network."

    See: Aitken et al., 2017, "Checkerboard artifact free sub-pixel convolution".

    The idea comes from:
    https://arxiv.org/abs/1609.05158

    The pixel shuffle mechanism refers to:
    https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle.
    and:
    https://github.com/pytorch/pytorch/pull/6340.

    """

    def __init__(self, spatial_dims: int, in_channels: Optional[int], out_channels: Optional[int]=None, scale_factor: int=2, conv_block: Optional[Union[nn.Module, str]]='default', apply_pad_pool: bool=True, bias: bool=True) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions of the input image.
            in_channels: number of channels of the input image.
            out_channels: optional number of channels of the output image.
            scale_factor: multiplier for spatial size. Defaults to 2.
            conv_block: a conv block to extract feature maps before upsampling. Defaults to None.

                - When ``conv_block`` is ``"default"``, one reserved conv layer will be utilized.
                - When ``conv_block`` is an ``nn.module``,
                  please ensure the output number of channels is divisible ``(scale_factor ** dimensions)``.

            apply_pad_pool: if True the upsampled tensor is padded then average pooling is applied with a kernel the
                size of `scale_factor` with a stride of 1. This implements the nearest neighbour resize convolution
                component of subpixel convolutions described in Aitken et al.
            bias: whether to have a bias term in the default conv_block. Defaults to True.

        """
        super().__init__()
        if scale_factor <= 0:
            raise ValueError(f'The `scale_factor` multiplier must be an integer greater than 0, got {scale_factor}.')
        self.dimensions = spatial_dims
        self.scale_factor = scale_factor
        if conv_block == 'default':
            out_channels = out_channels or in_channels
            if not out_channels:
                raise ValueError('in_channels need to be specified.')
            conv_out_channels = out_channels * scale_factor ** self.dimensions
            self.conv_block = Conv[Conv.CONV, self.dimensions](in_channels=in_channels, out_channels=conv_out_channels, kernel_size=3, stride=1, padding=1, bias=bias)
            icnr_init(self.conv_block, self.scale_factor)
        elif conv_block is None:
            self.conv_block = nn.Identity()
        else:
            self.conv_block = conv_block
        self.pad_pool: nn.Module = nn.Identity()
        if apply_pad_pool:
            pool_type = Pool[Pool.AVG, self.dimensions]
            pad_type = Pad[Pad.CONSTANTPAD, self.dimensions]
            self.pad_pool = nn.Sequential(pad_type(padding=(self.scale_factor - 1, 0) * self.dimensions, value=0.0), pool_type(kernel_size=self.scale_factor, stride=1))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: Tensor in shape (batch, channel, spatial_1[, spatial_2, ...).
        """
        x = self.conv_block(x)
        if x.shape[1] % self.scale_factor ** self.dimensions != 0:
            raise ValueError(f'Number of channels after `conv_block` ({x.shape[1]}) must be evenly divisible by scale_factor ** dimensions ({self.scale_factor}^{self.dimensions}={self.scale_factor ** self.dimensions}).')
        x = pixelshuffle(x, self.dimensions, self.scale_factor)
        x = self.pad_pool(x)
        return x


class UpsampleMode(StrEnum):
    """
    See also: :py:class:`monai.networks.blocks.UpSample`
    """
    DECONV = 'deconv'
    DECONVGROUP = 'deconvgroup'
    NONTRAINABLE = 'nontrainable'
    PIXELSHUFFLE = 'pixelshuffle'


class UpSample(nn.Sequential):
    """
    Upsamples data by `scale_factor`.
    Supported modes are:

        - "deconv": uses a transposed convolution.
        - "deconvgroup": uses a transposed group convolution.
        - "nontrainable": uses :py:class:`torch.nn.Upsample`.
        - "pixelshuffle": uses :py:class:`monai.networks.blocks.SubpixelUpsample`.

    This module can optionally take a pre-convolution
    (often used to map the number of features from `in_channels` to `out_channels`).
    """

    def __init__(self, spatial_dims: int, in_channels: Optional[int]=None, out_channels: Optional[int]=None, scale_factor: Union[Sequence[float], float]=2, kernel_size: Optional[Union[Sequence[float], float]]=None, size: Optional[Union[Tuple[int], int]]=None, mode: Union[UpsampleMode, str]=UpsampleMode.DECONV, pre_conv: Optional[Union[nn.Module, str]]='default', interp_mode: str=InterpolateMode.LINEAR, align_corners: Optional[bool]=True, bias: bool=True, apply_pad_pool: bool=True) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions of the input image.
            in_channels: number of channels of the input image.
            out_channels: number of channels of the output image. Defaults to `in_channels`.
            scale_factor: multiplier for spatial size. Has to match input size if it is a tuple. Defaults to 2.
            kernel_size: kernel size used during transposed convolutions. Defaults to `scale_factor`.
            size: spatial size of the output image.
                Only used when ``mode`` is ``UpsampleMode.NONTRAINABLE``.
                In torch.nn.functional.interpolate, only one of `size` or `scale_factor` should be defined,
                thus if size is defined, `scale_factor` will not be used.
                Defaults to None.
            mode: {``"deconv"``, ``"deconvgroup"``, ``"nontrainable"``, ``"pixelshuffle"``}. Defaults to ``"deconv"``.
            pre_conv: a conv block applied before upsampling. Defaults to "default".
                When ``conv_block`` is ``"default"``, one reserved conv layer will be utilized when
                Only used in the "nontrainable" or "pixelshuffle" mode.
            interp_mode: {``"nearest"``, ``"linear"``, ``"bilinear"``, ``"bicubic"``, ``"trilinear"``}
                Only used in the "nontrainable" mode.
                If ends with ``"linear"`` will use ``spatial dims`` to determine the correct interpolation.
                This corresponds to linear, bilinear, trilinear for 1D, 2D, and 3D respectively.
                The interpolation mode. Defaults to ``"linear"``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html
            align_corners: set the align_corners parameter of `torch.nn.Upsample`. Defaults to True.
                Only used in the "nontrainable" mode.
            bias: whether to have a bias term in the default preconv and deconv layers. Defaults to True.
            apply_pad_pool: if True the upsampled tensor is padded then average pooling is applied with a kernel the
                size of `scale_factor` with a stride of 1. See also: :py:class:`monai.networks.blocks.SubpixelUpsample`.
                Only used in the "pixelshuffle" mode.

        """
        super().__init__()
        scale_factor_ = ensure_tuple_rep(scale_factor, spatial_dims)
        up_mode = look_up_option(mode, UpsampleMode)
        if not kernel_size:
            kernel_size_ = scale_factor_
            output_padding = padding = 0
        else:
            kernel_size_ = ensure_tuple_rep(kernel_size, spatial_dims)
            padding = tuple((k - 1) // 2 for k in kernel_size_)
            output_padding = tuple(s - 1 - (k - 1) % 2 for k, s in zip(kernel_size_, scale_factor_))
        if up_mode == UpsampleMode.DECONV:
            if not in_channels:
                raise ValueError(f"in_channels needs to be specified in the '{mode}' mode.")
            self.add_module('deconv', Conv[Conv.CONVTRANS, spatial_dims](in_channels=in_channels, out_channels=out_channels or in_channels, kernel_size=kernel_size_, stride=scale_factor_, padding=padding, output_padding=output_padding, bias=bias))
        elif up_mode == UpsampleMode.DECONVGROUP:
            if not in_channels:
                raise ValueError(f"in_channels needs to be specified in the '{mode}' mode.")
            if out_channels is None:
                out_channels = in_channels
            groups = out_channels if in_channels % out_channels == 0 else 1
            self.add_module('deconvgroup', Conv[Conv.CONVTRANS, spatial_dims](in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size_, stride=scale_factor_, padding=padding, output_padding=output_padding, groups=groups, bias=bias))
        elif up_mode == UpsampleMode.NONTRAINABLE:
            if pre_conv == 'default' and out_channels != in_channels:
                if not in_channels:
                    raise ValueError(f"in_channels needs to be specified in the '{mode}' mode.")
                self.add_module('preconv', Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=out_channels or in_channels, kernel_size=1, bias=bias))
            elif pre_conv is not None and pre_conv != 'default':
                self.add_module('preconv', pre_conv)
            elif pre_conv is None and out_channels != in_channels:
                raise ValueError('in the nontrainable mode, if not setting pre_conv, out_channels should equal to in_channels.')
            interp_mode = InterpolateMode(interp_mode)
            linear_mode = [InterpolateMode.LINEAR, InterpolateMode.BILINEAR, InterpolateMode.TRILINEAR]
            if interp_mode in linear_mode:
                interp_mode = linear_mode[spatial_dims - 1]
            self.add_module('upsample_non_trainable', nn.Upsample(size=size, scale_factor=None if size else scale_factor_, mode=interp_mode.value, align_corners=align_corners))
        elif up_mode == UpsampleMode.PIXELSHUFFLE:
            self.add_module('pixelshuffle', SubpixelUpsample(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, scale_factor=scale_factor_[0], conv_block=pre_conv, apply_pad_pool=apply_pad_pool, bias=bias))
        else:
            raise NotImplementedError(f'Unsupported upsampling mode {mode}.')


class UpCat(nn.Module):
    """upsampling, concatenation with the encoder feature map, two convolutions"""

    def __init__(self, spatial_dims: int, in_chns: int, cat_chns: int, out_chns: int, act: Union[str, tuple], norm: Union[str, tuple], bias: bool, dropout: Union[float, tuple]=0.0, upsample: str='deconv', pre_conv: Optional[Union[nn.Module, str]]='default', interp_mode: str='linear', align_corners: Optional[bool]=True, halves: bool=True, is_pad: bool=True):
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_chns: number of input channels to be upsampled.
            cat_chns: number of channels from the encoder.
            out_chns: number of output channels.
            act: activation type and arguments.
            norm: feature normalization type and arguments.
            bias: whether to have a bias term in convolution blocks.
            dropout: dropout ratio. Defaults to no dropout.
            upsample: upsampling mode, available options are
                ``"deconv"``, ``"pixelshuffle"``, ``"nontrainable"``.
            pre_conv: a conv block applied before upsampling.
                Only used in the "nontrainable" or "pixelshuffle" mode.
            interp_mode: {``"nearest"``, ``"linear"``, ``"bilinear"``, ``"bicubic"``, ``"trilinear"``}
                Only used in the "nontrainable" mode.
            align_corners: set the align_corners parameter for upsample. Defaults to True.
                Only used in the "nontrainable" mode.
            halves: whether to halve the number of channels during upsampling.
                This parameter does not work on ``nontrainable`` mode if ``pre_conv`` is `None`.
            is_pad: whether to pad upsampling features to fit features from encoder. Defaults to True.

        """
        super().__init__()
        if upsample == 'nontrainable' and pre_conv is None:
            up_chns = in_chns
        else:
            up_chns = in_chns // 2 if halves else in_chns
        self.upsample = UpSample(spatial_dims, in_chns, up_chns, 2, mode=upsample, pre_conv=pre_conv, interp_mode=interp_mode, align_corners=align_corners)
        self.convs = TwoConv(spatial_dims, cat_chns + up_chns, out_chns, act, norm, bias, dropout)
        self.is_pad = is_pad

    def forward(self, x: torch.Tensor, x_e: Optional[torch.Tensor]):
        """

        Args:
            x: features to be upsampled.
            x_e: features from the encoder.
        """
        x_0 = self.upsample(x)
        if x_e is not None:
            if self.is_pad:
                dimensions = len(x.shape) - 2
                sp = [0] * (dimensions * 2)
                for i in range(dimensions):
                    if x_e.shape[-i - 1] != x_0.shape[-i - 1]:
                        sp[i * 2 + 1] = 1
                x_0 = torch.nn.functional.pad(x_0, sp, 'replicate')
            x = self.convs(torch.cat([x_e, x_0], dim=1))
        else:
            x = self.convs(x_0)
        return x


class DeprecatedError(Exception):
    pass


def warn_deprecated(obj, msg, warning_category=FutureWarning):
    """
    Issue the warning message `msg`.
    """
    warnings.warn(f'{obj}: {msg}', category=warning_category, stacklevel=2)


def deprecated_arg(name, since: Optional[str]=None, removed: Optional[str]=None, msg_suffix: str='', version_val: str=__version__, new_name: Optional[str]=None, warning_category=FutureWarning):
    """
    Marks a particular named argument of a callable as deprecated. The same conditions for `since` and `removed` as
    described in the `deprecated` decorator.

    When the decorated definition is called, that is when the function is called or the class instantiated with args,
    a `warning_category` is issued if `since` is given and the current version is at or later than that given.
    a `DeprecatedError` exception is instead raised if `removed` is given and the current version is at or later
    than that, or if neither `since` nor `removed` is provided.

    The relevant docstring of the deprecating function should also be updated accordingly,
    using the Sphinx directives such as `.. versionchanged:: version` and `.. deprecated:: version`.
    https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded

    In the current implementation type annotations are not preserved.


    Args:
        name: name of position or keyword argument to mark as deprecated.
        since: version at which the argument was marked deprecated but not removed.
        removed: version at which the argument was/will be removed and no longer usable.
        msg_suffix: message appended to warning/exception detailing reasons for deprecation and what to use instead.
        version_val: (used for testing) version to compare since and removed against, default is MONAI version.
        new_name: name of position or keyword argument to replace the deprecated argument.
            if it is specified and the signature of the decorated function has a `kwargs`, the value to the
            deprecated argument `name` will be removed.
        warning_category: a warning category class, defaults to `FutureWarning`.

    Returns:
        Decorated callable which warns or raises exception when deprecated argument used.
    """
    if version_val.startswith('0+') or not f'{version_val}'.strip()[0].isdigit():
        version_val = f'{sys.maxsize}'
    if since is not None and removed is not None and not version_leq(since, removed):
        raise ValueError(f'since must be less or equal to removed, got since={since}, removed={removed}.')
    is_not_yet_deprecated = since is not None and version_val != since and version_leq(version_val, since)
    if is_not_yet_deprecated:
        return lambda obj: obj
    if since is None and removed is None:
        is_removed = True
        is_deprecated = True
    else:
        is_deprecated = since is not None and version_leq(since, version_val)
        is_removed = removed is not None and version_leq(removed, version_val)

    def _decorator(func):
        argname = f'{func.__module__} {func.__qualname__}:{name}'
        msg_prefix = f'Argument `{name}`'
        if is_removed:
            msg_infix = f'was removed in version {removed}.'
        elif is_deprecated:
            msg_infix = f'has been deprecated since version {since}.'
            if removed is not None:
                msg_infix += f' It will be removed in version {removed}.'
        else:
            msg_infix = 'has been deprecated.'
        msg = f'{msg_prefix} {msg_infix} {msg_suffix}'.strip()
        sig = inspect.signature(func)

        @wraps(func)
        def _wrapper(*args, **kwargs):
            if new_name is not None and name in kwargs and new_name not in kwargs:
                kwargs[new_name] = kwargs[name]
                try:
                    sig.bind(*args, **kwargs).arguments
                except TypeError:
                    kwargs.pop(new_name, None)
            binding = sig.bind(*args, **kwargs).arguments
            positional_found = name in binding
            kw_found = False
            for k, param in sig.parameters.items():
                if param.kind == inspect.Parameter.VAR_KEYWORD and k in binding and name in binding[k]:
                    kw_found = True
                    kwargs.pop(name, None)
            if positional_found or kw_found:
                if is_removed:
                    raise DeprecatedError(msg)
                if is_deprecated:
                    warn_deprecated(argname, msg, warning_category)
            return func(*args, **kwargs)
        return _wrapper
    return _decorator


class BasicUNet(nn.Module):

    @deprecated_arg(name='dimensions', new_name='spatial_dims', since='0.6', msg_suffix='Please use `spatial_dims` instead.')
    def __init__(self, spatial_dims: int=3, in_channels: int=1, out_channels: int=2, features: Sequence[int]=(32, 32, 64, 128, 256, 32), act: Union[str, tuple]=('LeakyReLU', {'negative_slope': 0.1, 'inplace': True}), norm: Union[str, tuple]=('instance', {'affine': True}), bias: bool=True, dropout: Union[float, tuple]=0.0, upsample: str='deconv', dimensions: Optional[int]=None):
        """
        A UNet implementation with 1D/2D/3D supports.

        Based on:

            Falk et al. "U-Net – Deep Learning for Cell Counting, Detection, and
            Morphometry". Nature Methods 16, 67–70 (2019), DOI:
            http://dx.doi.org/10.1038/s41592-018-0261-2

        Args:
            spatial_dims: number of spatial dimensions. Defaults to 3 for spatial 3D inputs.
            in_channels: number of input channels. Defaults to 1.
            out_channels: number of output channels. Defaults to 2.
            features: six integers as numbers of features.
                Defaults to ``(32, 32, 64, 128, 256, 32)``,

                - the first five values correspond to the five-level encoder feature sizes.
                - the last value corresponds to the feature size after the last upsampling.

            act: activation type and arguments. Defaults to LeakyReLU.
            norm: feature normalization type and arguments. Defaults to instance norm.
            bias: whether to have a bias term in convolution blocks. Defaults to True.
                According to `Performance Tuning Guide <https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html>`_,
                if a conv layer is directly followed by a batch norm layer, bias should be False.
            dropout: dropout ratio. Defaults to no dropout.
            upsample: upsampling mode, available options are
                ``"deconv"``, ``"pixelshuffle"``, ``"nontrainable"``.

        .. deprecated:: 0.6.0
            ``dimensions`` is deprecated, use ``spatial_dims`` instead.

        Examples::

            # for spatial 2D
            >>> net = BasicUNet(spatial_dims=2, features=(64, 128, 256, 512, 1024, 128))

            # for spatial 2D, with group norm
            >>> net = BasicUNet(spatial_dims=2, features=(64, 128, 256, 512, 1024, 128), norm=("group", {"num_groups": 4}))

            # for spatial 3D
            >>> net = BasicUNet(spatial_dims=3, features=(32, 32, 64, 128, 256, 32))

        See Also

            - :py:class:`monai.networks.nets.DynUNet`
            - :py:class:`monai.networks.nets.UNet`

        """
        super().__init__()
        if dimensions is not None:
            spatial_dims = dimensions
        fea = ensure_tuple_rep(features, 6)
        None
        self.conv_0 = TwoConv(spatial_dims, in_channels, features[0], act, norm, bias, dropout)
        self.down_1 = Down(spatial_dims, fea[0], fea[1], act, norm, bias, dropout)
        self.down_2 = Down(spatial_dims, fea[1], fea[2], act, norm, bias, dropout)
        self.down_3 = Down(spatial_dims, fea[2], fea[3], act, norm, bias, dropout)
        self.down_4 = Down(spatial_dims, fea[3], fea[4], act, norm, bias, dropout)
        self.upcat_4 = UpCat(spatial_dims, fea[4], fea[3], fea[3], act, norm, bias, dropout, upsample)
        self.upcat_3 = UpCat(spatial_dims, fea[3], fea[2], fea[2], act, norm, bias, dropout, upsample)
        self.upcat_2 = UpCat(spatial_dims, fea[2], fea[1], fea[1], act, norm, bias, dropout, upsample)
        self.upcat_1 = UpCat(spatial_dims, fea[1], fea[0], fea[5], act, norm, bias, dropout, upsample, halves=False)
        self.final_conv = Conv['conv', spatial_dims](fea[5], out_channels, kernel_size=1)

    def forward(self, x: torch.Tensor):
        """
        Args:
            x: input should have spatially N dimensions
                ``(Batch, in_channels, dim_0[, dim_1, ..., dim_N-1])``, N is defined by `spatial_dims`.
                It is recommended to have ``dim_n % 16 == 0`` to ensure all maxpooling inputs have
                even edge lengths.

        Returns:
            A torch Tensor of "raw" predictions in shape
            ``(Batch, out_channels, dim_0[, dim_1, ..., dim_N-1])``.
        """
        x0 = self.conv_0(x)
        x1 = self.down_1(x0)
        x2 = self.down_2(x1)
        x3 = self.down_3(x2)
        x4 = self.down_4(x3)
        u4 = self.upcat_4(x4, x3)
        u3 = self.upcat_3(u4, x2)
        u2 = self.upcat_2(u3, x1)
        u1 = self.upcat_1(u2, x0)
        logits = self.final_conv(u1)
        return logits


def complex_normalize(x: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
    """
    Performs layer mean-std normalization for complex data. Normalization is done for each batch member
    along each part (part refers to real and imaginary parts), separately.

    Args:
        x: input of shape (B,C,H,W) for 2D data or (B,C,H,W,D) for 3D data

    Returns:
        A tuple containing
            (1) normalized output of shape (B,C,H,W) for 2D data or (B,C,H,W,D) for 3D data
            (2) mean
            (3) std
    """
    if len(x.shape) == 4:
        b, c, h, w = x.shape
        x = x.contiguous().view(b, 2, c // 2 * h * w)
        mean = x.mean(dim=2).view(b, 2, 1, 1, 1).expand(b, 2, c // 2, 1, 1).contiguous().view(b, c, 1, 1)
        std = x.std(dim=2, unbiased=False).view(b, 2, 1, 1, 1).expand(b, 2, c // 2, 1, 1).contiguous().view(b, c, 1, 1)
        x = x.view(b, c, h, w)
        return (x - mean) / std, mean, std
    elif len(x.shape) == 5:
        b, c, h, w, d = x.shape
        x = x.contiguous().view(b, 2, c // 2 * h * w * d)
        mean = x.mean(dim=2).view(b, 2, 1, 1, 1, 1).expand(b, 2, c // 2, 1, 1, 1).contiguous().view(b, c, 1, 1, 1)
        std = x.std(dim=2, unbiased=False).view(b, 2, 1, 1, 1, 1).expand(b, 2, c // 2, 1, 1, 1).contiguous().view(b, c, 1, 1, 1)
        x = x.view(b, c, h, w, d)
        return (x - mean) / std, mean, std
    else:
        raise ValueError(f'only 2D (B,C,H,W) and 3D (B,C,H,W,D) data are supported but x has shape {x.shape}')


def floor_ceil(n: float) ->Tuple[int, int]:
    """
    Returns floor and ceil of the input

    Args:
        n: input number

    Returns:
        A tuple containing:
            (1) floor(n)
            (2) ceil(n)
    """
    return math.floor(n), math.ceil(n)


def divisible_pad_t(x: Tensor, k: int=16) ->Tuple[Tensor, Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int], int, int, int]]:
    """
    Pad input to feed into the network (torch script compatible)

    Args:
        x: input of shape (B,C,H,W) for 2D data or (B,C,H,W,D) for 3D data
        k: padding factor. each padded dimension will be divisible by k.

    Returns:
        A tuple containing
            (1) padded input
            (2) pad sizes (in order to reverse padding if needed)

    Example:
        .. code-block:: python

            import torch

            # 2D data
            x = torch.ones([3,2,50,70])
            x_pad,padding_sizes = divisible_pad_t(x, k=16)
            # the following line should print (3, 2, 64, 80)
            print(x_pad.shape)

            # 3D data
            x = torch.ones([3,2,50,70,80])
            x_pad,padding_sizes = divisible_pad_t(x, k=16)
            # the following line should print (3, 2, 64, 80, 80)
            print(x_pad.shape)

    """
    if len(x.shape) == 4:
        b, c, h, w = x.shape
        w_mult = (w - 1 | k - 1) + 1
        h_mult = (h - 1 | k - 1) + 1
        w_pad = floor_ceil((w_mult - w) / 2)
        h_pad = floor_ceil((h_mult - h) / 2)
        x = F.pad(x, w_pad + h_pad)
        d_mult = -1
        d_pad = -1, -1
        pad_sizes = h_pad, w_pad, d_pad, h_mult, w_mult, d_mult
    elif len(x.shape) == 5:
        b, c, h, w, d = x.shape
        w_mult = (w - 1 | k - 1) + 1
        h_mult = (h - 1 | k - 1) + 1
        d_mult = (d - 1 | k - 1) + 1
        w_pad = floor_ceil((w_mult - w) / 2)
        h_pad = floor_ceil((h_mult - h) / 2)
        d_pad = floor_ceil((d_mult - d) / 2)
        x = F.pad(x, d_pad + w_pad + h_pad)
        pad_sizes = h_pad, w_pad, d_pad, h_mult, w_mult, d_mult
    else:
        raise ValueError(f'only 2D (B,C,H,W) and 3D (B,C,H,W,D) data are supported but x has shape {x.shape}')
    return x, pad_sizes


def inverse_divisible_pad_t(x: Tensor, pad_sizes: Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int], int, int, int]) ->Tensor:
    """
    De-pad network output to match its original shape

    Args:
        x: input of shape (B,C,H,W) for 2D data or (B,C,H,W,D) for 3D data
        pad_sizes: padding values

    Returns:
        de-padded input
    """
    h_pad, w_pad, d_pad, h_mult, w_mult, d_mult = pad_sizes
    if len(x.shape) == 4:
        return x[..., h_pad[0]:h_mult - h_pad[1], w_pad[0]:w_mult - w_pad[1]]
    elif len(x.shape) == 5:
        return x[..., h_pad[0]:h_mult - h_pad[1], w_pad[0]:w_mult - w_pad[1], d_pad[0]:d_mult - d_pad[1]]
    else:
        raise ValueError(f'only 2D (B,C,H,W) and 3D (B,C,H,W,D) data are supported but x has shape {x.shape}')


def reshape_channel_complex_to_last_dim(x: Tensor) ->Tensor:
    """
    Swaps the complex dimension with the channel dimension so that the network output has 2 as its last dimension

    Args:
        x: input of shape (B,C*2,H,W) for 2D data or (B,C*2,H,W,D) for 3D data

    Returns:
        output of shape (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data
    """
    if x.shape[1] % 2 != 0:
        raise ValueError(f'channel dimension should be even but ({x.shape[1]}) is odd.')
    if len(x.shape) == 4:
        b, c2, h, w = x.shape
        c = c2 // 2
        return x.view(b, 2, c, h, w).permute(0, 2, 3, 4, 1)
    elif len(x.shape) == 5:
        b, c2, h, w, d = x.shape
        c = c2 // 2
        return x.view(b, 2, c, h, w, d).permute(0, 2, 3, 4, 5, 1)
    else:
        raise ValueError(f'only 2D (B,C*2,H,W) and 3D (B,C*2,H,W,D) data are supported but x has shape {x.shape}')


def reshape_complex_to_channel_dim(x: Tensor) ->Tensor:
    """
    Swaps the complex dimension with the channel dimension so that the network treats real/imaginary
    parts as two separate channels.

    Args:
        x: input of shape (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data

    Returns:
        output of shape (B,C*2,H,W) for 2D data or (B,C*2,H,W,D) for 3D data
    """
    if x.shape[-1] != 2:
        raise ValueError(f'last dim must be 2, but x.shape[-1] is {x.shape[-1]}.')
    if len(x.shape) == 5:
        b, c, h, w, two = x.shape
        return x.permute(0, 4, 1, 2, 3).contiguous().view(b, 2 * c, h, w)
    elif len(x.shape) == 6:
        b, c, h, w, d, two = x.shape
        return x.permute(0, 5, 1, 2, 3, 4).contiguous().view(b, 2 * c, h, w, d)
    else:
        raise ValueError(f'only 2D (B,C,H,W,2) and 3D (B,C,H,W,D,2) data are supported but x has shape {x.shape}')


class ComplexUnet(nn.Module):
    """
    This variant of U-Net handles complex-value input/output. It can be
    used as a model to learn sensitivity maps in multi-coil MRI data. It is
    built based on :py:class:`monai.networks.nets.BasicUNet` by default but the user
    can input their convolutional model as well.
    ComplexUnet also applies default normalization to the input which makes it more stable to train.

    The data being a (complex) 2-channel tensor is a requirement for using this model.

    Modified and adopted from: https://github.com/facebookresearch/fastMRI

    Args:
        spatial_dims: number of spatial dimensions.
        features: six integers as numbers of features. denotes number of channels in each layer.
        act: activation type and arguments. Defaults to LeakyReLU.
        norm: feature normalization type and arguments. Defaults to instance norm.
        bias: whether to have a bias term in convolution blocks. Defaults to True.
        dropout: dropout ratio. Defaults to 0.0.
        upsample: upsampling mode, available options are
            ``"deconv"``, ``"pixelshuffle"``, ``"nontrainable"``.
        pad_factor: an integer denoting the number which each padded dimension will be divisible to.
            For example, 16 means each dimension will be divisible by 16 after padding
        conv_net: the learning model used inside the ComplexUnet. The default
            is :py:class:`monai.networks.nets.basic_unet`. The only requirement on the model is to
            have 2 as input and output number of channels.
    """

    def __init__(self, spatial_dims: int=2, features: Sequence[int]=(32, 32, 64, 128, 256, 32), act: Union[str, tuple]=('LeakyReLU', {'negative_slope': 0.1, 'inplace': True}), norm: Union[str, tuple]=('instance', {'affine': True}), bias: bool=True, dropout: Union[float, tuple]=0.0, upsample: str='deconv', pad_factor: int=16, conv_net: Optional[nn.Module]=None):
        super().__init__()
        if conv_net is None:
            self.unet = BasicUNet(spatial_dims=spatial_dims, in_channels=2, out_channels=2, features=features, act=act, norm=norm, bias=bias, dropout=dropout, upsample=upsample)
        else:
            params = [p.shape for p in conv_net.parameters()]
            if params[0][1] != 2:
                raise ValueError(f"in_channels should be 2 but it's {params[0][1]}.")
            self.unet = conv_net
        self.pad_factor = pad_factor

    def forward(self, x: Tensor) ->Tensor:
        """
        Args:
            x: input of shape (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data

        Returns:
            output of shape (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data
        """
        x = reshape_complex_to_channel_dim(x)
        x, mean, std = complex_normalize(x)
        x, padding_sizes = divisible_pad_t(x, k=self.pad_factor)
        x = self.unet(x)
        x = inverse_divisible_pad_t(x, padding_sizes)
        x = x * std + mean
        x = reshape_channel_complex_to_last_dim(x)
        return x


def reshape_batch_channel_to_channel_dim(x: Tensor, batch_size: int) ->Tensor:
    """
    Detaches batch and channel dimensions.

    Args:
        x: input of shape (B*C,1,H,W,2) for 2D data or (B*C,1,H,W,D,2) for 3D data
        batch_size: batch size

    Returns:
        output of shape (B,C,...)
    """
    if len(x.shape) == 5:
        bc, one, h, w, two = x.shape
        c = bc // batch_size
        return x.view(batch_size, c, h, w, two)
    elif len(x.shape) == 6:
        bc, one, h, w, d, two = x.shape
        c = bc // batch_size
        return x.view(batch_size, c, h, w, d, two)
    else:
        raise ValueError(f'only 2D (B*C,1,H,W,2) and 3D (B*C,1,H,W,D,2) data are supported but x has shape {x.shape}')


def reshape_channel_to_batch_dim(x: Tensor) ->Tuple[Tensor, int]:
    """
    Combines batch and channel dimensions.

    Args:
        x: input of shape (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data

    Returns:
        A tuple containing:
            (1) output of shape (B*C,1,...)
            (2) batch size
    """
    if len(x.shape) == 5:
        b, c, h, w, two = x.shape
        return x.contiguous().view(b * c, 1, h, w, two), b
    elif len(x.shape) == 6:
        b, c, h, w, d, two = x.shape
        return x.contiguous().view(b * c, 1, h, w, d, two), b
    else:
        raise ValueError(f'only 2D (B,C,H,W,2) and 3D (B,C,H,W,D,2) data are supported but x has shape {x.shape}')


def root_sum_of_squares_t(x: Tensor, spatial_dim: int) ->Tensor:
    """
    Compute the root sum of squares (rss) of the data (typically done for multi-coil MRI samples)

    Args:
        x: Input tensor
        spatial_dim: dimension along which rss is applied

    Returns:
        rss of x along spatial_dim

    Example:
        .. code-block:: python

            import numpy as np
            x = torch.ones([2,3])
            # the following line prints Tensor([1.41421356, 1.41421356, 1.41421356])
            print(rss(x,spatial_dim=0))
    """
    rss_x: Tensor = (x ** 2).sum(spatial_dim) ** 0.5
    return rss_x


class CoilSensitivityModel(nn.Module):
    """
    This class uses a convolutional model to learn coil sensitivity maps for multi-coil MRI reconstruction.
    The convolutional model is :py:class:`monai.apps.reconstruction.networks.nets.complex_unet` by default
    but can be specified by the user as well. Learning is done on the center of the under-sampled
    kspace (that region is fully sampled).

    The data being a (complex) 2-channel tensor is a requirement for using this model.

    Modified and adopted from: https://github.com/facebookresearch/fastMRI

    Args:
        spatial_dims: number of spatial dimensions.
        features: six integers as numbers of features. denotes number of channels in each layer.
        act: activation type and arguments. Defaults to LeakyReLU.
        norm: feature normalization type and arguments. Defaults to instance norm.
        bias: whether to have a bias term in convolution blocks. Defaults to True.
        dropout: dropout ratio. Defaults to 0.0.
        upsample: upsampling mode, available options are
            ``"deconv"``, ``"pixelshuffle"``, ``"nontrainable"``.
        coil_dim: coil dimension in the data
        conv_net: the learning model used to estimate the coil sensitivity maps. default
            is :py:class:`monai.apps.reconstruction.networks.nets.complex_unet`. The only
            requirement on the model is to have 2 as input and output number of channels.
    """

    def __init__(self, spatial_dims: int=2, features: Sequence[int]=(32, 32, 64, 128, 256, 32), act: Union[str, tuple]=('LeakyReLU', {'negative_slope': 0.1, 'inplace': True}), norm: Union[str, tuple]=('instance', {'affine': True}), bias: bool=True, dropout: Union[float, tuple]=0.0, upsample: str='deconv', coil_dim: int=1, conv_net: Optional[nn.Module]=None):
        super().__init__()
        if conv_net is None:
            self.conv_net = ComplexUnet(spatial_dims=spatial_dims, features=features, act=act, norm=norm, bias=bias, dropout=dropout, upsample=upsample)
        else:
            params = [p.shape for p in conv_net.parameters()]
            if params[0][1] != 2:
                raise ValueError(f"in_channels should be 2 but it's {params[0][1]}.")
            self.conv_net = conv_net
        self.spatial_dims = spatial_dims
        self.coil_dim = coil_dim

    def get_fully_sampled_region(self, mask: Tensor) ->Tuple[int, int]:
        """
        Extracts the size of the fully-sampled part of the kspace. Note that when a kspace
        is under-sampled, a part of its center is fully sampled. This part is called the Auto
        Calibration Region (ACR). ACR is used for sensitivity map computation.

        Args:
            mask: the under-sampling mask of shape (..., S, 1) where S denotes the sampling dimension

        Returns:
            A tuple containing
                (1) left index of the region
                (2) right index of the region

        Note:
            Suppose the mask is of shape (1,1,20,1). If this function returns 8,12 as left and right
                indices, then it means that the fully-sampled center region has size 4 starting from 8 to 12.
        """
        left = right = mask.shape[-2] // 2
        while mask[..., right, :]:
            right += 1
        while mask[..., left, :]:
            left -= 1
        return left + 1, right

    def forward(self, masked_kspace: Tensor, mask: Tensor) ->Tensor:
        """
        Args:
            masked_kspace: the under-sampled kspace (which is the input measurement). Its shape
                is (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data.
            mask: the under-sampling mask with shape (1,1,1,W,1) for 2D data or (1,1,1,1,D,1) for 3D data.

        Returns:
            predicted coil sensitivity maps with shape (B,C,H,W,2) for 2D data or (B,C,H,W,D,2) for 3D data.
        """
        left, right = self.get_fully_sampled_region(mask)
        num_low_freqs = right - left
        x = torch.zeros_like(masked_kspace)
        start = (mask.shape[-2] - num_low_freqs + 1) // 2
        x[..., start:start + num_low_freqs, :] = masked_kspace[..., start:start + num_low_freqs, :]
        x = ifftn_centered_t(x, spatial_dims=self.spatial_dims, is_complex=True)
        x, b = reshape_channel_to_batch_dim(x)
        x = self.conv_net(x)
        x = reshape_batch_channel_to_channel_dim(x, b)
        x = x / root_sum_of_squares_t(x, spatial_dim=self.coil_dim).unsqueeze(self.coil_dim)
        return x


def complex_abs_t(x: Tensor) ->Tensor:
    """
    Compute the absolute value of a complex tensor.

    Args:
        x: Input tensor with 2 channels in the last dimension representing real and imaginary parts.

    Returns:
        Absolute value along the last dimension
    """
    if x.shape[-1] != 2:
        raise ValueError(f'x.shape[-1] is not 2 ({x.shape[-1]}).')
    return (x[..., 0] ** 2 + x[..., 1] ** 2) ** 0.5


class VariationalNetworkModel(nn.Module):
    """
    The end-to-end variational network (or simply e2e-VarNet) based on Sriram et. al., "End-to-end variational
    networks for accelerated MRI reconstruction".
    It comprises several cascades each consisting of refinement and data consistency steps. The network takes in
    the under-sampled kspace and estimates the ground-truth reconstruction.

    Modified and adopted from: https://github.com/facebookresearch/fastMRI

    Args:
        coil_sensitivity_model: A convolutional model for learning coil sensitivity maps. An example is
            :py:class:`monai.apps.reconstruction.networks.nets.coil_sensitivity_model.CoilSensitivityModel`.
        refinement_model: A convolutional network used in the refinement step of e2e-VarNet. An example
            is :py:class:`monai.apps.reconstruction.networks.nets.complex_unet.ComplexUnet`.
        num_cascades: Number of cascades. Each cascade is a
            :py:class:`monai.apps.reconstruction.networks.blocks.varnetblock.VarNetBlock` which consists of
            refinement and data consistency steps.
        spatial_dims: number of spatial dimensions.
    """

    def __init__(self, coil_sensitivity_model: nn.Module, refinement_model: nn.Module, num_cascades: int=12, spatial_dims: int=2):
        super().__init__()
        self.coil_sensitivity_model = coil_sensitivity_model
        self.cascades = nn.ModuleList([VarNetBlock(copy.deepcopy(refinement_model)) for i in range(num_cascades)])
        self.spatial_dims = spatial_dims

    def forward(self, masked_kspace: Tensor, mask: Tensor) ->Tensor:
        """
        Args:
            masked_kspace: The under-sampled kspace. It's a 2D kspace (B,C,H,W,2)
                with the last dimension being 2 (for real/imaginary parts) and C denoting the
                coil dimension. 3D data will have the shape (B,C,H,W,D,2).
            mask: The under-sampling mask with shape (1,1,1,W,1) for 2D data or (1,1,1,1,D,1) for 3D data.

        Returns:
            The reconstructed image which is the root sum of squares (rss) of the absolute value
                of the inverse fourier of the predicted kspace (note that rss combines coil images into one image).
        """
        sensitivity_maps = self.coil_sensitivity_model(masked_kspace, mask)
        kspace_pred = masked_kspace.clone()
        for cascade in self.cascades:
            kspace_pred = cascade(kspace_pred, masked_kspace, mask, sensitivity_maps)
        output_image = root_sum_of_squares_t(complex_abs_t(ifftn_centered_t(kspace_pred, spatial_dims=self.spatial_dims)), spatial_dim=1)
        return output_image


class ContrastiveLoss(_Loss):
    """
    Compute the Contrastive loss defined in:

        Chen, Ting, et al. "A simple framework for contrastive learning of visual representations." International
        conference on machine learning. PMLR, 2020. (http://proceedings.mlr.press/v119/chen20j.html)

    Adapted from:
        https://github.com/Sara-Ahmed/SiT/blob/1aacd6adcd39b71efc903d16b4e9095b97dda76f/losses.py#L5

    """

    @deprecated_arg(name='reduction', since='0.8', msg_suffix='`reduction` is no longer supported.')
    def __init__(self, temperature: float=0.5, batch_size: int=-1, reduction='sum') ->None:
        """
        Args:
            temperature: Can be scaled between 0 and 1 for learning from negative samples, ideally set to 0.5.

        Raises:
            ValueError: When an input of dimension length > 2 is passed
            ValueError: When input and target are of different shapes

        .. deprecated:: 0.8.0

            `reduction` is no longer supported.

        """
        super().__init__()
        self.temperature = temperature
        if batch_size != -1:
            warn('batch_size is no longer required to be set. It will be estimated dynamically in the forward call')

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be B[F].
            target: the shape should be B[F].
        """
        if len(target.shape) > 2 or len(input.shape) > 2:
            raise ValueError(f'Either target or input has dimensions greater than 2 where target shape is ({target.shape}) and input shape is ({input.shape})')
        if target.shape != input.shape:
            raise ValueError(f'ground truth has differing shape ({target.shape}) from input ({input.shape})')
        temperature_tensor = torch.as_tensor(self.temperature)
        batch_size = input.shape[0]
        norm_i = F.normalize(input, dim=1)
        norm_j = F.normalize(target, dim=1)
        negatives_mask = ~torch.eye(batch_size * 2, batch_size * 2, dtype=torch.bool)
        negatives_mask = torch.clone(negatives_mask.type(torch.float))
        repr = torch.cat([norm_i, norm_j], dim=0)
        sim_matrix = F.cosine_similarity(repr.unsqueeze(1), repr.unsqueeze(0), dim=2)
        sim_ij = torch.diag(sim_matrix, batch_size)
        sim_ji = torch.diag(sim_matrix, -batch_size)
        positives = torch.cat([sim_ij, sim_ji], dim=0)
        nominator = torch.exp(positives / temperature_tensor)
        denominator = negatives_mask * torch.exp(sim_matrix / temperature_tensor)
        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))
        return torch.sum(loss_partial) / (2 * batch_size)


def spatial_gradient(x: torch.Tensor, dim: int) ->torch.Tensor:
    """
    Calculate gradients on single dimension of a tensor using central finite difference.
    It moves the tensor along the dimension to calculate the approximate gradient
    dx[i] = (x[i+1] - x[i-1]) / 2.
    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)

    Args:
        x: the shape should be BCH(WD).
        dim: dimension to calculate gradient along.
    Returns:
        gradient_dx: the shape should be BCH(WD)
    """
    slice_1 = slice(1, -1)
    slice_2_s = slice(2, None)
    slice_2_e = slice(None, -2)
    slice_all = slice(None)
    slicing_s, slicing_e = [slice_all, slice_all], [slice_all, slice_all]
    while len(slicing_s) < x.ndim:
        slicing_s = slicing_s + [slice_1]
        slicing_e = slicing_e + [slice_1]
    slicing_s[dim] = slice_2_s
    slicing_e[dim] = slice_2_e
    return (x[slicing_s] - x[slicing_e]) / 2.0


class BendingEnergyLoss(_Loss):
    """
    Calculate the bending energy based on second-order differentiation of pred using central finite difference.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, normalize: bool=False, reduction: Union[LossReduction, str]=LossReduction.MEAN) ->None:
        """
        Args:
            normalize:
                Whether to divide out spatial sizes in order to make the computation roughly
                invariant to image scale (i.e. vector field sampling resolution). Defaults to False.
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.
        """
        super().__init__(reduction=LossReduction(reduction).value)
        self.normalize = normalize

    def forward(self, pred: torch.Tensor) ->torch.Tensor:
        """
        Args:
            pred: the shape should be BCH(WD)

        Raises:
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].

        """
        if pred.ndim not in [3, 4, 5]:
            raise ValueError(f'Expecting 3-d, 4-d or 5-d pred, instead got pred of shape {pred.shape}')
        for i in range(pred.ndim - 2):
            if pred.shape[-i - 1] <= 4:
                raise ValueError(f'All spatial dimensions must be > 4, got spatial dimensions {pred.shape[2:]}')
        if pred.shape[1] != pred.ndim - 2:
            raise ValueError(f'Number of vector components, {pred.shape[1]}, does not match number of spatial dimensions, {pred.ndim - 2}')
        first_order_gradient = [spatial_gradient(pred, dim) for dim in range(2, pred.ndim)]
        if self.normalize:
            spatial_dims = torch.tensor(pred.shape, device=pred.device)[2:].reshape((1, -1) + (pred.ndim - 2) * (1,))
        energy = torch.tensor(0)
        for dim_1, g in enumerate(first_order_gradient):
            dim_1 += 2
            if self.normalize:
                g *= pred.shape[dim_1] / spatial_dims
                energy = energy + (spatial_gradient(g, dim_1) * pred.shape[dim_1]) ** 2
            else:
                energy = energy + spatial_gradient(g, dim_1) ** 2
            for dim_2 in range(dim_1 + 1, pred.ndim):
                if self.normalize:
                    energy = energy + 2 * (spatial_gradient(g, dim_2) * pred.shape[dim_2]) ** 2
                else:
                    energy = energy + 2 * spatial_gradient(g, dim_2) ** 2
        if self.reduction == LossReduction.MEAN.value:
            energy = torch.mean(energy)
        elif self.reduction == LossReduction.SUM.value:
            energy = torch.sum(energy)
        elif self.reduction != LossReduction.NONE.value:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return energy


class MaskedLoss(_Loss):
    """
    This is a wrapper class for the loss functions.  It allows for additional
    weighting masks to be applied to both input and target.

    See Also:
        - :py:class:`monai.losses.MaskedDiceLoss`
    """

    def __init__(self, loss: Union[Callable, _Loss], *loss_args, **loss_kwargs) ->None:
        """
        Args:
            loss: loss function to be wrapped, this could be a loss class or an instance of a loss class.
            loss_args: arguments to the loss function's constructor if `loss` is a class.
            loss_kwargs: keyword arguments to the loss function's constructor if `loss` is a class.
        """
        super().__init__()
        self.loss = loss(*loss_args, **loss_kwargs) if inspect.isclass(loss) else loss
        if not callable(self.loss):
            raise ValueError('The loss function is not callable.')

    def forward(self, input: torch.Tensor, target: torch.Tensor, mask: Optional[torch.Tensor]=None):
        """
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].
            mask: the shape should be B1H[WD] or 11H[WD].
        """
        if mask is None:
            warnings.warn('No mask value specified for the MaskedLoss.')
            return self.loss(input, target)
        if input.dim() != mask.dim():
            warnings.warn(f'Dim of input ({input.shape}) is different from mask ({mask.shape}).')
        if input.shape[0] != mask.shape[0] and mask.shape[0] != 1:
            raise ValueError(f'Batch size of mask ({mask.shape}) must be one or equal to input ({input.shape}).')
        if target.dim() > 1:
            if mask.shape[1] != 1:
                raise ValueError(f'Mask ({mask.shape}) must have only one channel.')
            if input.shape[2:] != mask.shape[2:]:
                warnings.warn(f'Spatial size of input ({input.shape}) is different from mask ({mask.shape}).')
        return self.loss(input * mask, target * mask)


class MaskedDiceLoss(DiceLoss):
    """
    Add an additional `masking` process before `DiceLoss`, accept a binary mask ([0, 1]) indicating a region,
    `input` and `target` will be masked by the region: region with mask `1` will keep the original value,
    region with `0` mask will be converted to `0`. Then feed `input` and `target` to normal `DiceLoss` computation.
    This has the effect of ensuring only the masked region contributes to the loss computation and
    hence gradient calculation.

    """

    def __init__(self, *args, **kwargs) ->None:
        """
        Args follow :py:class:`monai.losses.DiceLoss`.
        """
        super().__init__(*args, **kwargs)
        self.spatial_weighted = MaskedLoss(loss=super().forward)

    def forward(self, input: torch.Tensor, target: torch.Tensor, mask: Optional[torch.Tensor]=None):
        """
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].
            mask: the shape should B1H[WD] or 11H[WD].
        """
        return self.spatial_weighted(input=input, target=target, mask=mask)


class Weight(StrEnum):
    """
    See also: :py:class:`monai.losses.dice.GeneralizedDiceLoss`
    """
    SQUARE = 'square'
    SIMPLE = 'simple'
    UNIFORM = 'uniform'


class GeneralizedDiceLoss(_Loss):
    """
    Compute the generalised Dice loss defined in:

        Sudre, C. et. al. (2017) Generalised Dice overlap as a deep learning
        loss function for highly unbalanced segmentations. DLMIA 2017.

    Adapted from:
        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L279
    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Optional[Callable]=None, w_type: Union[Weight, str]=Weight.SQUARE, reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False) ->None:
        """
        Args:
            include_background: If False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert the ``target`` into the one-hot format,
                using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
            sigmoid: If True, apply a sigmoid function to the prediction.
            softmax: If True, apply a softmax function to the prediction.
            other_act: callable function to execute other activation layers, Defaults to ``None``. for example:
                ``other_act = torch.tanh``.
            w_type: {``"square"``, ``"simple"``, ``"uniform"``}
                Type of function to transform ground truth volume to a weight factor. Defaults to ``"square"``.
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.
            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, intersection over union is computed from each item in the batch.

        Raises:
            TypeError: When ``other_act`` is not an ``Optional[Callable]``.
            ValueError: When more than 1 of [``sigmoid=True``, ``softmax=True``, ``other_act is not None``].
                Incompatible values.

        """
        super().__init__(reduction=LossReduction(reduction).value)
        if other_act is not None and not callable(other_act):
            raise TypeError(f'other_act must be None or callable but is {type(other_act).__name__}.')
        if int(sigmoid) + int(softmax) + int(other_act is not None) > 1:
            raise ValueError('Incompatible values: more than 1 of [sigmoid=True, softmax=True, other_act is not None].')
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        self.sigmoid = sigmoid
        self.softmax = softmax
        self.other_act = other_act
        self.w_type = look_up_option(w_type, Weight)
        self.smooth_nr = float(smooth_nr)
        self.smooth_dr = float(smooth_dr)
        self.batch = batch

    def w_func(self, grnd):
        if self.w_type == str(Weight.SIMPLE):
            return torch.reciprocal(grnd)
        if self.w_type == str(Weight.SQUARE):
            return torch.reciprocal(grnd * grnd)
        return torch.ones_like(grnd)

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].

        Raises:
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].

        """
        if self.sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if self.softmax:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `softmax=True` ignored.')
            else:
                input = torch.softmax(input, 1)
        if self.other_act is not None:
            input = self.other_act(input)
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        if not self.include_background:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `include_background=False` ignored.')
            else:
                target = target[:, 1:]
                input = input[:, 1:]
        if target.shape != input.shape:
            raise AssertionError(f'ground truth has differing shape ({target.shape}) from input ({input.shape})')
        reduce_axis: List[int] = torch.arange(2, len(input.shape)).tolist()
        if self.batch:
            reduce_axis = [0] + reduce_axis
        intersection = torch.sum(target * input, reduce_axis)
        ground_o = torch.sum(target, reduce_axis)
        pred_o = torch.sum(input, reduce_axis)
        denominator = ground_o + pred_o
        w = self.w_func(ground_o.float())
        infs = torch.isinf(w)
        if self.batch:
            w[infs] = 0.0
            w = w + infs * torch.max(w)
        else:
            w[infs] = 0.0
            max_values = torch.max(w, dim=1)[0].unsqueeze(dim=1)
            w = w + infs * max_values
        numer = 2.0 * (intersection * w) + self.smooth_nr
        denom = denominator * w + self.smooth_dr
        f: torch.Tensor = 1.0 - numer / denom
        if self.reduction == LossReduction.MEAN.value:
            f = torch.mean(f)
        elif self.reduction == LossReduction.SUM.value:
            f = torch.sum(f)
        elif self.reduction == LossReduction.NONE.value:
            broadcast_shape = list(f.shape[0:2]) + [1] * (len(input.shape) - 2)
            f = f.view(broadcast_shape)
        else:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return f


class GeneralizedWassersteinDiceLoss(_Loss):
    """
    Compute the generalized Wasserstein Dice Loss defined in:

        Fidon L. et al. (2017) Generalised Wasserstein Dice Score for Imbalanced Multi-class
        Segmentation using Holistic Convolutional Networks. BrainLes 2017.

    Or its variant (use the option weighting_mode="GDL") defined in the Appendix of:

        Tilborghs, S. et al. (2020) Comparative study of deep learning methods for the automatic
        segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients.
        arXiv preprint arXiv:2007.15546

    Adapted from:
        https://github.com/LucasFidon/GeneralizedWassersteinDiceLoss
    """

    def __init__(self, dist_matrix: Union[np.ndarray, torch.Tensor], weighting_mode: str='default', reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=1e-05, smooth_dr: float=1e-05) ->None:
        """
        Args:
            dist_matrix: 2d tensor or 2d numpy array; matrix of distances between the classes.
            It must have dimension C x C where C is the number of classes.
            weighting_mode: {``"default"``, ``"GDL"``}
                Specifies how to weight the class-specific sum of errors.
                Default to ``"default"``.

                - ``"default"``: (recommended) use the original weighting method as in:
                    Fidon L. et al. (2017) Generalised Wasserstein Dice Score for Imbalanced Multi-class
                    Segmentation using Holistic Convolutional Networks. BrainLes 2017.
                - ``"GDL"``: use a GDL-like weighting method as in the Appendix of:
                    Tilborghs, S. et al. (2020) Comparative study of deep learning methods for the automatic
                    segmentation of lung, lesion and lesion type in CT scans of COVID-19 patients.
                    arXiv preprint arXiv:2007.15546
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.
            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.

        Raises:
            ValueError: When ``dist_matrix`` is not a square matrix.

        Example:
            .. code-block:: python

                import torch
                import numpy as np
                from monai.losses import GeneralizedWassersteinDiceLoss

                # Example with 3 classes (including the background: label 0).
                # The distance between the background class (label 0) and the other classes is the maximum, equal to 1.
                # The distance between class 1 and class 2 is 0.5.
                dist_mat = np.array([[0.0, 1.0, 1.0], [1.0, 0.0, 0.5], [1.0, 0.5, 0.0]], dtype=np.float32)
                wass_loss = GeneralizedWassersteinDiceLoss(dist_matrix=dist_mat)

                pred_score = torch.tensor([[1000, 0, 0], [0, 1000, 0], [0, 0, 1000]], dtype=torch.float32)
                grnd = torch.tensor([0, 1, 2], dtype=torch.int64)
                wass_loss(pred_score, grnd)  # 0

        """
        super().__init__(reduction=LossReduction(reduction).value)
        if dist_matrix.shape[0] != dist_matrix.shape[1]:
            raise ValueError(f'dist_matrix must be C x C, got {dist_matrix.shape[0]} x {dist_matrix.shape[1]}.')
        if weighting_mode not in ['default', 'GDL']:
            raise ValueError("weighting_mode must be either 'default' or 'GDL, got %s." % weighting_mode)
        self.m = dist_matrix
        if isinstance(self.m, np.ndarray):
            self.m = torch.from_numpy(self.m)
        if torch.max(self.m) != 1:
            self.m = self.m / torch.max(self.m)
        self.alpha_mode = weighting_mode
        self.num_classes = self.m.size(0)
        self.smooth_nr = float(smooth_nr)
        self.smooth_dr = float(smooth_dr)

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].

        """
        flat_input = input.reshape(input.size(0), input.size(1), -1)
        flat_target = target.reshape(target.size(0), -1).long()
        probs = F.softmax(flat_input, dim=1)
        wass_dist_map = self.wasserstein_distance_map(probs, flat_target)
        alpha = self._compute_alpha_generalized_true_positives(flat_target)
        if self.alpha_mode == 'GDL':
            true_pos = self._compute_generalized_true_positive(alpha, flat_target, wass_dist_map)
            denom = self._compute_denominator(alpha, flat_target, wass_dist_map)
        else:
            true_pos = self._compute_generalized_true_positive(alpha, flat_target, wass_dist_map)
            all_error = torch.sum(wass_dist_map, dim=1)
            denom = 2 * true_pos + all_error
        wass_dice: torch.Tensor = (2.0 * true_pos + self.smooth_nr) / (denom + self.smooth_dr)
        wass_dice_loss: torch.Tensor = 1.0 - wass_dice
        if self.reduction == LossReduction.MEAN.value:
            wass_dice_loss = torch.mean(wass_dice_loss)
        elif self.reduction == LossReduction.SUM.value:
            wass_dice_loss = torch.sum(wass_dice_loss)
        elif self.reduction == LossReduction.NONE.value:
            broadcast_shape = input.shape[0:2] + (1,) * (len(input.shape) - 2)
            wass_dice_loss = wass_dice_loss.view(broadcast_shape)
        else:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return wass_dice_loss

    def wasserstein_distance_map(self, flat_proba: torch.Tensor, flat_target: torch.Tensor) ->torch.Tensor:
        """
        Compute the voxel-wise Wasserstein distance between the
        flattened prediction and the flattened labels (ground_truth) with respect
        to the distance matrix on the label space M.
        This corresponds to eq. 6 in:

            Fidon L. et al. (2017) Generalised Wasserstein Dice Score for Imbalanced Multi-class
            Segmentation using Holistic Convolutional Networks. BrainLes 2017.

        Args:
            flat_proba: the probabilities of input(predicted) tensor.
            flat_target: the target tensor.
        """
        m = torch.clone(torch.as_tensor(self.m))
        m_extended = torch.unsqueeze(m, dim=0)
        m_extended = torch.unsqueeze(m_extended, dim=3)
        m_extended = m_extended.expand((flat_proba.size(0), m_extended.size(1), m_extended.size(2), flat_proba.size(2)))
        flat_target_extended = torch.unsqueeze(flat_target, dim=1)
        flat_target_extended = flat_target_extended.expand((flat_target.size(0), m_extended.size(1), flat_target.size(1)))
        flat_target_extended = torch.unsqueeze(flat_target_extended, dim=1)
        m_extended = torch.gather(m_extended, dim=1, index=flat_target_extended)
        m_extended = torch.squeeze(m_extended, dim=1)
        wasserstein_map = m_extended * flat_proba
        wasserstein_map = torch.sum(wasserstein_map, dim=1)
        return wasserstein_map

    def _compute_generalized_true_positive(self, alpha: torch.Tensor, flat_target: torch.Tensor, wasserstein_distance_map: torch.Tensor) ->torch.Tensor:
        """
        Args:
            alpha: generalised number of true positives of target class.
            flat_target: the target tensor.
            wasserstein_distance_map: the map obtained from the above function.
        """
        alpha_extended = torch.unsqueeze(alpha, dim=2)
        alpha_extended = alpha_extended.expand((flat_target.size(0), self.num_classes, flat_target.size(1)))
        flat_target_extended = torch.unsqueeze(flat_target, dim=1)
        alpha_extended = torch.gather(alpha_extended, index=flat_target_extended, dim=1)
        return torch.sum(alpha_extended * (1.0 - wasserstein_distance_map), dim=[1, 2])

    def _compute_denominator(self, alpha: torch.Tensor, flat_target: torch.Tensor, wasserstein_distance_map: torch.Tensor) ->torch.Tensor:
        """
        Args:
            alpha: generalised number of true positives of target class.
            flat_target: the target tensor.
            wasserstein_distance_map: the map obtained from the above function.
        """
        alpha_extended = torch.unsqueeze(alpha, dim=2)
        alpha_extended = alpha_extended.expand((flat_target.size(0), self.num_classes, flat_target.size(1)))
        flat_target_extended = torch.unsqueeze(flat_target, dim=1)
        alpha_extended = torch.gather(alpha_extended, index=flat_target_extended, dim=1)
        return torch.sum(alpha_extended * (2.0 - wasserstein_distance_map), dim=[1, 2])

    def _compute_alpha_generalized_true_positives(self, flat_target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            flat_target: the target tensor.
        """
        alpha: torch.Tensor = torch.ones((flat_target.size(0), self.num_classes)).float()
        if self.alpha_mode == 'GDL':
            one_hot_f = F.one_hot(flat_target, num_classes=self.num_classes).permute(0, 2, 1).float()
            volumes = torch.sum(one_hot_f, dim=2)
            alpha = 1.0 / (volumes + 1.0)
        else:
            alpha[:, 0] = 0.0
        return alpha


class DiceCEReduction(StrEnum):
    """
    See also:
        - :py:class:`monai.losses.dice.DiceCELoss`
    """
    MEAN = 'mean'
    SUM = 'sum'


class DiceCELoss(_Loss):
    """
    Compute both Dice loss and Cross Entropy Loss, and return the weighted sum of these two losses.
    The details of Dice loss is shown in ``monai.losses.DiceLoss``.
    The details of Cross Entropy Loss is shown in ``torch.nn.CrossEntropyLoss``. In this implementation,
    two deprecated parameters ``size_average`` and ``reduce``, and the parameter ``ignore_index`` are
    not supported.

    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Optional[Callable]=None, squared_pred: bool=False, jaccard: bool=False, reduction: str='mean', smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False, ce_weight: Optional[torch.Tensor]=None, lambda_dice: float=1.0, lambda_ce: float=1.0) ->None:
        """
        Args:
            ``ce_weight`` and ``lambda_ce`` are only used for cross entropy loss.
            ``reduction`` is used for both losses and other parameters are only used for dice loss.

            include_background: if False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert the ``target`` into the one-hot format,
                using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
            sigmoid: if True, apply a sigmoid function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `CrossEntropyLoss`.
            softmax: if True, apply a softmax function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `CrossEntropyLoss`.
            other_act: callable function to execute other activation layers, Defaults to ``None``. for example:
                ``other_act = torch.tanh``. only used by the `DiceLoss`, not for the `CrossEntropyLoss`.
            squared_pred: use squared versions of targets and predictions in the denominator or not.
            jaccard: compute Jaccard Index (soft IoU) instead of dice or not.
            reduction: {``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``. The dice loss should
                as least reduce the spatial dimensions, which is different from cross entropy loss, thus here
                the ``none`` option cannot be used.

                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.

            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, a Dice loss value is computed independently from each item in the batch
                before any `reduction`.
            ce_weight: a rescaling weight given to each class for cross entropy loss.
                See ``torch.nn.CrossEntropyLoss()`` for more information.
            lambda_dice: the trade-off weight value for dice loss. The value should be no less than 0.0.
                Defaults to 1.0.
            lambda_ce: the trade-off weight value for cross entropy loss. The value should be no less than 0.0.
                Defaults to 1.0.

        """
        super().__init__()
        reduction = look_up_option(reduction, DiceCEReduction).value
        self.dice = DiceLoss(include_background=include_background, to_onehot_y=to_onehot_y, sigmoid=sigmoid, softmax=softmax, other_act=other_act, squared_pred=squared_pred, jaccard=jaccard, reduction=reduction, smooth_nr=smooth_nr, smooth_dr=smooth_dr, batch=batch)
        self.cross_entropy = nn.CrossEntropyLoss(weight=ce_weight, reduction=reduction)
        if lambda_dice < 0.0:
            raise ValueError('lambda_dice should be no less than 0.0.')
        if lambda_ce < 0.0:
            raise ValueError('lambda_ce should be no less than 0.0.')
        self.lambda_dice = lambda_dice
        self.lambda_ce = lambda_ce
        self.old_pt_ver = not pytorch_after(1, 10)

    def ce(self, input: torch.Tensor, target: torch.Tensor):
        """
        Compute CrossEntropy loss for the input and target.
        Will remove the channel dim according to PyTorch CrossEntropyLoss:
        https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?#torch.nn.CrossEntropyLoss.

        """
        n_pred_ch, n_target_ch = input.shape[1], target.shape[1]
        if n_pred_ch != n_target_ch and n_target_ch == 1:
            target = torch.squeeze(target, dim=1)
            target = target.long()
        elif self.old_pt_ver:
            warnings.warn(f'Multichannel targets are not supported in this older Pytorch version {torch.__version__}. Using argmax (as a workaround) to convert target to a single channel.')
            target = torch.argmax(target, dim=1)
        elif not torch.is_floating_point(target):
            target = target
        return self.cross_entropy(input, target)

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When number of dimensions for input and target are different.
            ValueError: When number of channels for target is neither 1 nor the same as input.

        """
        if len(input.shape) != len(target.shape):
            raise ValueError(f'the number of dimensions for input and target should be the same, got shape {input.shape} and {target.shape}.')
        dice_loss = self.dice(input, target)
        ce_loss = self.ce(input, target)
        total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_ce * ce_loss
        return total_loss


class FocalLoss(_Loss):
    """
    FocalLoss is an extension of BCEWithLogitsLoss that down-weights loss from
    high confidence correct predictions.

    Reimplementation of the Focal Loss (with a build-in sigmoid activation) described in:

        - "Focal Loss for Dense Object Detection", T. Lin et al., ICCV 2017
        - "AnatomyNet: Deep learning for fast and fully automated whole‐volume segmentation of head and neck anatomy",
          Zhu et al., Medical Physics 2018

    Example:
        >>> import torch
        >>> from monai.losses import FocalLoss
        >>> from torch.nn import BCEWithLogitsLoss
        >>> shape = B, N, *DIMS = 2, 3, 5, 7, 11
        >>> input = torch.rand(*shape)
        >>> target = torch.rand(*shape)
        >>> # Demonstrate equivalence to BCE when gamma=0
        >>> fl_g0_criterion = FocalLoss(reduction='none', gamma=0)
        >>> fl_g0_loss = fl_g0_criterion(input, target)
        >>> bce_criterion = BCEWithLogitsLoss(reduction='none')
        >>> bce_loss = bce_criterion(input, target)
        >>> assert torch.allclose(fl_g0_loss, bce_loss)
        >>> # Demonstrate "focus" by setting gamma > 0.
        >>> fl_g2_criterion = FocalLoss(reduction='none', gamma=2)
        >>> fl_g2_loss = fl_g2_criterion(input, target)
        >>> # Mark easy and hard cases
        >>> is_easy = (target > 0.7) & (input > 0.7)
        >>> is_hard = (target > 0.7) & (input < 0.3)
        >>> easy_loss_g0 = fl_g0_loss[is_easy].mean()
        >>> hard_loss_g0 = fl_g0_loss[is_hard].mean()
        >>> easy_loss_g2 = fl_g2_loss[is_easy].mean()
        >>> hard_loss_g2 = fl_g2_loss[is_hard].mean()
        >>> # Gamma > 0 causes the loss function to "focus" on the hard
        >>> # cases.  IE, easy cases are downweighted, so hard cases
        >>> # receive a higher proportion of the loss.
        >>> hard_to_easy_ratio_g2 = hard_loss_g2 / easy_loss_g2
        >>> hard_to_easy_ratio_g0 = hard_loss_g0 / easy_loss_g0
        >>> assert hard_to_easy_ratio_g2 > hard_to_easy_ratio_g0
    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, gamma: float=2.0, weight: Optional[Union[Sequence[float], float, int, torch.Tensor]]=None, reduction: Union[LossReduction, str]=LossReduction.MEAN) ->None:
        """
        Args:
            include_background: if False, channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert `y` into the one-hot format. Defaults to False.
            gamma: value of the exponent gamma in the definition of the Focal loss.
            weight: weights to apply to the voxels of each class. If None no weights are applied.
                This corresponds to the weights `lpha` in [1].
                The input can be a single value (same weight for all classes), a sequence of values (the length
                of the sequence should be the same as the number of classes, if not ``include_background``, the
                number should not include class 0).
                The value/values should be no less than 0. Defaults to None.
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.

        Example:
            >>> import torch
            >>> from monai.losses import FocalLoss
            >>> pred = torch.tensor([[1, 0], [0, 1], [1, 0]], dtype=torch.float32)
            >>> grnd = torch.tensor([[0], [1], [0]], dtype=torch.int64)
            >>> fl = FocalLoss(to_onehot_y=True)
            >>> fl(pred, grnd)
        """
        super().__init__(reduction=LossReduction(reduction).value)
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        self.gamma = gamma
        self.weight: Optional[Union[Sequence[float], float, int, torch.Tensor]] = weight

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD], where N is the number of classes.
                The input should be the original logits since it will be transformed by
                a sigmoid in the forward function.
            target: the shape should be BNH[WD] or B1H[WD], where N is the number of classes.

        Raises:
            ValueError: When input and target (after one hot transform if set)
                have different shapes.
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].
            ValueError: When ``self.weight`` is a sequence and the length is not equal to the
                number of classes.
            ValueError: When ``self.weight`` is/contains a value that is less than 0.

        """
        n_pred_ch = input.shape[1]
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        if not self.include_background:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `include_background=False` ignored.')
            else:
                target = target[:, 1:]
                input = input[:, 1:]
        if target.shape != input.shape:
            raise ValueError(f'ground truth has different shape ({target.shape}) from input ({input.shape})')
        i = input
        t = target
        b, n = t.shape[:2]
        i = i.reshape(b, n, -1)
        t = t.reshape(b, n, -1)
        max_val = (-i).clamp(min=0)
        ce = i - i * t + max_val + ((-max_val).exp() + (-i - max_val).exp()).log()
        if self.weight is not None:
            class_weight: Optional[torch.Tensor] = None
            if isinstance(self.weight, (float, int)):
                class_weight = torch.as_tensor([self.weight] * i.size(1))
            else:
                class_weight = torch.as_tensor(self.weight)
                if class_weight.size(0) != i.size(1):
                    raise ValueError('the length of the weight sequence should be the same as the number of classes. ' + 'If `include_background=False`, the number should not include class 0.')
            if class_weight.min() < 0:
                raise ValueError('the value/values of weights should be no less than 0.')
            class_weight = class_weight
            at = class_weight[None, :, None]
            at = at.expand((t.size(0), -1, t.size(2)))
            ce = ce * at
        p = F.logsigmoid(-i * (t * 2.0 - 1.0))
        flat_loss: torch.Tensor = (p * self.gamma).exp() * ce
        average_spatial_dims = True
        if self.reduction == LossReduction.SUM.value:
            if average_spatial_dims:
                flat_loss = flat_loss.mean(dim=-1)
            loss = flat_loss.sum()
        elif self.reduction == LossReduction.MEAN.value:
            if average_spatial_dims:
                flat_loss = flat_loss.mean(dim=-1)
            loss = flat_loss.mean()
        elif self.reduction == LossReduction.NONE.value:
            spacetime_dims = input.shape[2:]
            loss = flat_loss.reshape([b, n] + list(spacetime_dims))
        else:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return loss


class DiceFocalLoss(_Loss):
    """
    Compute both Dice loss and Focal Loss, and return the weighted sum of these two losses.
    The details of Dice loss is shown in ``monai.losses.DiceLoss``.
    The details of Focal Loss is shown in ``monai.losses.FocalLoss``.

    ``gamma``, ``focal_weight`` and ``lambda_focal`` are only used for the focal loss.
    ``include_background`` and ``reduction`` are used for both losses
    and other parameters are only used for dice loss.

    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Optional[Callable]=None, squared_pred: bool=False, jaccard: bool=False, reduction: str='mean', smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False, gamma: float=2.0, focal_weight: Optional[Union[Sequence[float], float, int, torch.Tensor]]=None, lambda_dice: float=1.0, lambda_focal: float=1.0) ->None:
        """
        Args:
            include_background: if False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert the ``target`` into the one-hot format,
                using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
            sigmoid: if True, apply a sigmoid function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `FocalLoss`.
            softmax: if True, apply a softmax function to the prediction, only used by the `DiceLoss`,
                don't need to specify activation function for `FocalLoss`.
            other_act: callable function to execute other activation layers, Defaults to ``None``.
                for example: `other_act = torch.tanh`. only used by the `DiceLoss`, not for `FocalLoss`.
            squared_pred: use squared versions of targets and predictions in the denominator or not.
            jaccard: compute Jaccard Index (soft IoU) instead of dice or not.
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.

            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, a Dice loss value is computed independently from each item in the batch
                before any `reduction`.
            gamma: value of the exponent gamma in the definition of the Focal loss.
            focal_weight: weights to apply to the voxels of each class. If None no weights are applied.
                The input can be a single value (same weight for all classes), a sequence of values (the length
                of the sequence should be the same as the number of classes).
            lambda_dice: the trade-off weight value for dice loss. The value should be no less than 0.0.
                Defaults to 1.0.
            lambda_focal: the trade-off weight value for focal loss. The value should be no less than 0.0.
                Defaults to 1.0.

        """
        super().__init__()
        self.dice = DiceLoss(include_background=include_background, to_onehot_y=False, sigmoid=sigmoid, softmax=softmax, other_act=other_act, squared_pred=squared_pred, jaccard=jaccard, reduction=reduction, smooth_nr=smooth_nr, smooth_dr=smooth_dr, batch=batch)
        self.focal = FocalLoss(include_background=include_background, to_onehot_y=False, gamma=gamma, weight=focal_weight, reduction=reduction)
        if lambda_dice < 0.0:
            raise ValueError('lambda_dice should be no less than 0.0.')
        if lambda_focal < 0.0:
            raise ValueError('lambda_focal should be no less than 0.0.')
        self.lambda_dice = lambda_dice
        self.lambda_focal = lambda_focal
        self.to_onehot_y = to_onehot_y

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD]. The input should be the original logits
                due to the restriction of ``monai.losses.FocalLoss``.
            target: the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When number of dimensions for input and target are different.
            ValueError: When number of channels for target is neither 1 nor the same as input.

        """
        if len(input.shape) != len(target.shape):
            raise ValueError(f'the number of dimensions for input and target should be the same, got shape {input.shape} and {target.shape}.')
        if self.to_onehot_y:
            n_pred_ch = input.shape[1]
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        dice_loss = self.dice(input, target)
        focal_loss = self.focal(input, target)
        total_loss: torch.Tensor = self.lambda_dice * dice_loss + self.lambda_focal * focal_loss
        return total_loss


class GeneralizedDiceFocalLoss(torch.nn.modules.loss._Loss):
    """Compute both Generalized Dice Loss and Focal Loss, and return their weighted average. The details of Generalized Dice Loss
    and Focal Loss are available at ``monai.losses.GeneralizedDiceLoss`` and ``monai.losses.FocalLoss``.

    Args:
        include_background (bool, optional): if False channel index 0 (background category) is excluded from the calculation.
            Defaults to True.
        to_onehot_y: whether to convert the ``target`` into the one-hot format,
            using the number of classes inferred from `input` (``input.shape[1]``). Defaults to False.
        sigmoid (bool, optional): if True, apply a sigmoid function to the prediction. Defaults to False.
        softmax (bool, optional): if True, apply a softmax function to the prediction. Defaults to False.
        other_act (Optional[Callable], optional): callable function to execute other activation layers,
            Defaults to ``None``. for example: `other_act = torch.tanh`.
            only used by the `GeneralizedDiceLoss`, not for the `FocalLoss`.
        w_type (Union[Weight, str], optional): {``"square"``, ``"simple"``, ``"uniform"``}. Type of function to transform
            ground-truth volume to a weight factor. Defaults to ``"square"``.
        reduction (Union[LossReduction, str], optional): {``"none"``, ``"mean"``, ``"sum"``}. Specified the reduction to
            apply to the output. Defaults to ``"mean"``.
            - ``"none"``: no reduction will be applied.
            - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
            - ``"sum"``: the output will be summed.
        smooth_nr (float, optional): a small constant added to the numerator to avoid zero. Defaults to 1e-5.
        smooth_dr (float, optional): a small constant added to the denominator to avoid nan. Defaults to 1e-5.
        batch (bool, optional): whether to sum the intersection and union areas over the batch dimension before the dividing.
            Defaults to False, i.e., the areas are computed for each item in the batch.
        gamma (float, optional): value of the exponent gamma in the definition of the Focal loss. Defaults to 2.0.
        focal_weight (Optional[Union[Sequence[float], float, int, torch.Tensor]], optional): weights to apply to
            the voxels of each class. If None no weights are applied. The input can be a single value
            (same weight for all classes), a sequence of values (the length of the sequence hould be the same as
            the number of classes). Defaults to None.
        lambda_gdl (float, optional): the trade-off weight value for Generalized Dice Loss. The value should be
            no less than 0.0. Defaults to 1.0.
        lambda_focal (float, optional): the trade-off weight value for Focal Loss. The value should be no less
            than 0.0. Defaults to 1.0.

    Raises:
        ValueError: if either `lambda_gdl` or `lambda_focal` is less than 0.
    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Optional[Callable]=None, w_type: Union[Weight, str]=Weight.SQUARE, reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False, gamma: float=2.0, focal_weight: Optional[Union[Sequence[float], float, int, torch.Tensor]]=None, lambda_gdl: float=1.0, lambda_focal: float=1.0) ->None:
        super().__init__()
        self.generalized_dice = GeneralizedDiceLoss(include_background=include_background, to_onehot_y=to_onehot_y, sigmoid=sigmoid, softmax=softmax, other_act=other_act, w_type=w_type, reduction=reduction, smooth_nr=smooth_nr, smooth_dr=smooth_dr, batch=batch)
        self.focal = FocalLoss(include_background=include_background, to_onehot_y=to_onehot_y, gamma=gamma, weight=focal_weight, reduction=reduction)
        if lambda_gdl < 0.0:
            raise ValueError('lambda_gdl should be no less than 0.0.')
        if lambda_focal < 0.0:
            raise ValueError('lambda_focal should be no less than 0.0.')
        self.lambda_gdl = lambda_gdl
        self.lambda_focal = lambda_focal

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input (torch.Tensor): the shape should be BNH[WD]. The input should be the original logits
                due to the restriction of ``monai.losses.FocalLoss``.
            target (torch.Tensor): the shape should be BNH[WD] or B1H[WD].

        Raises:
            ValueError: When the input and target tensors have different numbers of dimensions, or the target
                channel isn't either one-hot encoded or categorical with the same shape of the input.

        Returns:
            torch.Tensor: value of the loss.
        """
        if input.dim() != target.dim():
            raise ValueError(f'Input - {input.shape} - and target - {target.shape} - must have the same number of dimensions.')
        gdl_loss = self.generalized_dice(input, target)
        focal_loss = self.focal(input, target)
        total_loss: torch.Tensor = self.lambda_gdl * gdl_loss + self.lambda_focal * focal_loss
        return total_loss


class DeepSupervisionLoss(_Loss):
    """
    Wrapper class around the main loss function to accept a list of tensors returned from a deeply
    supervised networks. The final loss is computed as the sum of weighted losses for each of deep supervision levels.
    """

    def __init__(self, loss: _Loss, weight_mode: str='exp', weights: Optional[List[float]]=None) ->None:
        """
        Args:
            loss: main loss instance, e.g DiceLoss().
            weight_mode: {``"same"``, ``"exp"``, ``"two"``}
                Specifies the weights calculation for each image level. Defaults to ``"exp"``.
                - ``"same"``: all weights are equal to 1.
                - ``"exp"``: exponentially decreasing weights by a power of 2: 0, 0.5, 0.25, 0.125, etc .
                - ``"two"``: equal smaller weights for lower levels: 1, 0.5, 0.5, 0.5, 0.5, etc
            weights: a list of weights to apply to each deeply supervised sub-loss, if provided, this will be used
                regardless of the weight_mode
        """
        super().__init__()
        self.loss = loss
        self.weight_mode = weight_mode
        self.weights = weights
        self.interp_mode = 'nearest-exact' if pytorch_after(1, 11) else 'nearest'

    def get_weights(self, levels: int=1) ->List[float]:
        """
        Calculates weights for a given number of scale levels
        """
        levels = max(1, levels)
        if self.weights is not None and len(self.weights) >= levels:
            weights = self.weights[:levels]
        elif self.weight_mode == 'same':
            weights = [1.0] * levels
        elif self.weight_mode == 'exp':
            weights = [max(0.5 ** l, 0.0625) for l in range(levels)]
        elif self.weight_mode == 'two':
            weights = [(1.0 if l == 0 else 0.5) for l in range(levels)]
        else:
            weights = [1.0] * levels
        return weights

    def get_loss(self, input: torch.Tensor, target: torch.Tensor):
        """
        Calculates a loss output accounting for differences in shapes,
        and downsizing targets if necessary (using nearest neighbor interpolation)
        Generally downsizing occurs for all level, except for the first (level==0)
        """
        if input.shape[2:] != target.shape[2:]:
            target = F.interpolate(target, size=input.shape[2:], mode=self.interp_mode)
        return self.loss(input, target)

    def forward(self, input: Union[torch.Tensor, List[torch.Tensor]], target: torch.Tensor):
        if isinstance(input, (list, tuple)):
            weights = self.get_weights(levels=len(input))
            loss = torch.tensor(0, dtype=torch.float, device=target.device)
            for l in range(len(input)):
                loss += weights[l] * self.get_loss(input[l].float(), target)
            return loss
        return self.loss(input.float(), target)


def box_pair_giou(boxes1: NdarrayOrTensor, boxes2: NdarrayOrTensor) ->NdarrayOrTensor:
    """
    Compute the generalized intersection over union (GIoU) of a pair of boxes.
    The two inputs should have the same shape and the func return an (N,) array,
    (in contrary to :func:`~monai.data.box_utils.box_giou` , which does not require the inputs to have the same
    shape and returns ``NxM`` matrix).

    Args:
        boxes1: bounding boxes, Nx4 or Nx6 torch tensor or ndarray. The box mode is assumed to be ``StandardMode``
        boxes2: bounding boxes, same shape with boxes1. The box mode is assumed to be ``StandardMode``

    Returns:
        paired GIoU, with size of (N,) and same data type as ``boxes1``

    Reference:
        https://giou.stanford.edu/GIoU.pdf

    """
    if not isinstance(boxes1, type(boxes2)):
        warnings.warn(f'boxes1 is {type(boxes1)}, while boxes2 is {type(boxes2)}. The result will be {type(boxes1)}.')
    boxes1_t, *_ = convert_data_type(boxes1, torch.Tensor)
    boxes2_t, *_ = convert_data_type(boxes2, torch.Tensor)
    if boxes1_t.shape != boxes2_t.shape:
        raise ValueError('boxes1 and boxes2 should be paired and have same shape.')
    spatial_dims = get_spatial_dims(boxes=boxes1_t)
    box_dtype = boxes1_t.dtype
    area1 = box_area(boxes=boxes1_t)
    area2 = box_area(boxes=boxes2_t)
    lt = torch.max(boxes1_t[:, :spatial_dims], boxes2_t[:, :spatial_dims])
    rb = torch.min(boxes1_t[:, spatial_dims:], boxes2_t[:, spatial_dims:])
    wh = (rb - lt + TO_REMOVE).clamp(min=0)
    inter = torch.prod(wh, dim=-1, keepdim=False)
    union = area1 + area2 - inter
    iou = inter / (union + torch.finfo(COMPUTE_DTYPE).eps)
    lt = torch.min(boxes1_t[:, :spatial_dims], boxes2_t[:, :spatial_dims])
    rb = torch.max(boxes1_t[:, spatial_dims:], boxes2_t[:, spatial_dims:])
    wh = (rb - lt + TO_REMOVE).clamp(min=0)
    enclosure = torch.prod(wh, dim=-1, keepdim=False)
    giou_t = iou - (enclosure - union) / (enclosure + torch.finfo(COMPUTE_DTYPE).eps)
    giou_t = giou_t
    if torch.isnan(giou_t).any() or torch.isinf(giou_t).any():
        raise ValueError('Box GIoU is NaN or Inf.')
    giou, *_ = convert_to_dst_type(src=giou_t, dst=boxes1)
    return giou


class BoxGIoULoss(_Loss):
    """
    Compute the generalized intersection over union (GIoU) loss of a pair of boxes.
    The two inputs should have the same shape. giou_loss = 1.0 - giou

    The range of GIoU is (-1.0, 1.0]. Thus the range of GIoU loss is [0.0, 2.0).

    Args:
        reduction: {``"none"``, ``"mean"``, ``"sum"``}
            Specifies the reduction to apply to the output. Defaults to ``"mean"``.
            - ``"none"``: no reduction will be applied.
            - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
            - ``"sum"``: the output will be summed.
    """

    def __init__(self, reduction: Union[LossReduction, str]=LossReduction.MEAN) ->None:
        super().__init__(reduction=LossReduction(reduction).value)

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: predicted bounding boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``
            target: GT bounding boxes, Nx4 or Nx6 torch tensor. The box mode is assumed to be ``StandardMode``

        Raises:
            ValueError: When the two inputs have different shape.
        """
        if target.shape != input.shape:
            raise ValueError(f'ground truth has different shape ({target.shape}) from input ({input.shape})')
        box_dtype = input.dtype
        giou: torch.Tensor = box_pair_giou(target, input)
        loss: torch.Tensor = 1.0 - giou
        if self.reduction == LossReduction.MEAN.value:
            loss = loss.mean()
        elif self.reduction == LossReduction.SUM.value:
            loss = loss.sum()
        elif self.reduction == LossReduction.NONE.value:
            pass
        else:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return loss


def make_gaussian_kernel(sigma: int) ->torch.Tensor:
    if sigma <= 0:
        raise ValueError(f'expecting positive sigma, got sigma={sigma}')
    return gaussian_1d(sigma=torch.tensor(sigma), truncated=3, approx='sampled', normalize=False)


def make_rectangular_kernel(kernel_size: int) ->torch.Tensor:
    return torch.ones(kernel_size)


def make_triangular_kernel(kernel_size: int) ->torch.Tensor:
    fsize = (kernel_size + 1) // 2
    if fsize % 2 == 0:
        fsize -= 1
    f = torch.ones((1, 1, fsize), dtype=torch.float).div(fsize)
    padding = (kernel_size - fsize) // 2 + fsize // 2
    return F.conv1d(f, f, padding=padding).reshape(-1)


kernel_dict = {'rectangular': make_rectangular_kernel, 'triangular': make_triangular_kernel, 'gaussian': make_gaussian_kernel}


class LocalNormalizedCrossCorrelationLoss(_Loss):
    """
    Local squared zero-normalized cross-correlation.
    The loss is based on a moving kernel/window over the y_true/y_pred,
    within the window the square of zncc is calculated.
    The kernel can be a rectangular / triangular / gaussian window.
    The final loss is the averaged loss over all windows.

    Adapted from:
        https://github.com/voxelmorph/voxelmorph/blob/legacy/src/losses.py
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int=3, kernel_size: int=3, kernel_type: str='rectangular', reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=0.0, smooth_dr: float=1e-05) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions, {``1``, ``2``, ``3``}. Defaults to 3.
            kernel_size: kernel spatial size, must be odd.
            kernel_type: {``"rectangular"``, ``"triangular"``, ``"gaussian"``}. Defaults to ``"rectangular"``.
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.
            smooth_nr: a small constant added to the numerator to avoid nan.
            smooth_dr: a small constant added to the denominator to avoid nan.

        """
        super().__init__(reduction=LossReduction(reduction).value)
        self.ndim = spatial_dims
        if self.ndim not in {1, 2, 3}:
            raise ValueError(f'Unsupported ndim: {self.ndim}-d, only 1-d, 2-d, and 3-d inputs are supported')
        self.kernel_size = kernel_size
        if self.kernel_size % 2 == 0:
            raise ValueError(f'kernel_size must be odd, got {self.kernel_size}')
        _kernel = look_up_option(kernel_type, kernel_dict)
        self.kernel = _kernel(self.kernel_size)
        self.kernel.require_grads = False
        self.kernel_vol = self.get_kernel_vol()
        self.smooth_nr = float(smooth_nr)
        self.smooth_dr = float(smooth_dr)

    def get_kernel_vol(self):
        vol = self.kernel
        for _ in range(self.ndim - 1):
            vol = torch.matmul(vol.unsqueeze(-1), self.kernel.unsqueeze(0))
        return torch.sum(vol)

    def forward(self, pred: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            pred: the shape should be BNH[WD].
            target: the shape should be BNH[WD].
        Raises:
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].
        """
        if pred.ndim - 2 != self.ndim:
            raise ValueError(f'expecting pred with {self.ndim} spatial dimensions, got pred of shape {pred.shape}')
        if target.shape != pred.shape:
            raise ValueError(f'ground truth has differing shape ({target.shape}) from pred ({pred.shape})')
        t2, p2, tp = target * target, pred * pred, target * pred
        kernel, kernel_vol = self.kernel, self.kernel_vol
        kernels = [kernel] * self.ndim
        t_sum = separable_filtering(target, kernels=kernels)
        p_sum = separable_filtering(pred, kernels=kernels)
        t2_sum = separable_filtering(t2, kernels=kernels)
        p2_sum = separable_filtering(p2, kernels=kernels)
        tp_sum = separable_filtering(tp, kernels=kernels)
        t_avg = t_sum / kernel_vol
        p_avg = p_sum / kernel_vol
        cross = tp_sum - p_avg * t_sum
        t_var = torch.max(t2_sum - t_avg * t_sum, torch.as_tensor(self.smooth_dr, dtype=t2_sum.dtype, device=t2_sum.device))
        p_var = torch.max(p2_sum - p_avg * p_sum, torch.as_tensor(self.smooth_dr, dtype=p2_sum.dtype, device=p2_sum.device))
        ncc: torch.Tensor = (cross * cross + self.smooth_nr) / (t_var * p_var)
        if self.reduction == LossReduction.SUM.value:
            return torch.sum(ncc).neg()
        if self.reduction == LossReduction.NONE.value:
            return ncc.neg()
        if self.reduction == LossReduction.MEAN.value:
            return torch.mean(ncc).neg()
        raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')


class GlobalMutualInformationLoss(_Loss):
    """
    Differentiable global mutual information loss via Parzen windowing method.

    Reference:
        https://dspace.mit.edu/handle/1721.1/123142, Section 3.1, equation 3.1-3.5, Algorithm 1
    """

    def __init__(self, kernel_type: str='gaussian', num_bins: int=23, sigma_ratio: float=0.5, reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=1e-07, smooth_dr: float=1e-07) ->None:
        """
        Args:
            kernel_type: {``"gaussian"``, ``"b-spline"``}
                ``"gaussian"``: adapted from DeepReg
                Reference: https://dspace.mit.edu/handle/1721.1/123142, Section 3.1, equation 3.1-3.5, Algorithm 1.
                ``"b-spline"``: based on the method of Mattes et al [1,2] and adapted from ITK
                References:
                  [1] "Nonrigid multimodality image registration"
                      D. Mattes, D. R. Haynor, H. Vesselle, T. Lewellen and W. Eubank
                      Medical Imaging 2001: Image Processing, 2001, pp. 1609-1620.
                  [2] "PET-CT Image Registration in the Chest Using Free-form Deformations"
                      D. Mattes, D. R. Haynor, H. Vesselle, T. Lewellen and W. Eubank
                      IEEE Transactions in Medical Imaging. Vol.22, No.1,
                      January 2003. pp.120-128.

            num_bins: number of bins for intensity
            sigma_ratio: a hyper param for gaussian function
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.
            smooth_nr: a small constant added to the numerator to avoid nan.
            smooth_dr: a small constant added to the denominator to avoid nan.
        """
        super().__init__(reduction=LossReduction(reduction).value)
        if num_bins <= 0:
            raise ValueError('num_bins must > 0, got {num_bins}')
        bin_centers = torch.linspace(0.0, 1.0, num_bins)
        sigma = torch.mean(bin_centers[1:] - bin_centers[:-1]) * sigma_ratio
        self.kernel_type = look_up_option(kernel_type, ['gaussian', 'b-spline'])
        self.num_bins = num_bins
        self.kernel_type = kernel_type
        if self.kernel_type == 'gaussian':
            self.preterm = 1 / (2 * sigma ** 2)
            self.bin_centers = bin_centers[None, None, ...]
        self.smooth_nr = float(smooth_nr)
        self.smooth_dr = float(smooth_dr)

    def parzen_windowing(self, pred: torch.Tensor, target: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        if self.kernel_type == 'gaussian':
            pred_weight, pred_probability = self.parzen_windowing_gaussian(pred)
            target_weight, target_probability = self.parzen_windowing_gaussian(target)
        elif self.kernel_type == 'b-spline':
            pred_weight, pred_probability = self.parzen_windowing_b_spline(pred, order=3)
            target_weight, target_probability = self.parzen_windowing_b_spline(target, order=0)
        else:
            raise ValueError
        return pred_weight, pred_probability, target_weight, target_probability

    def parzen_windowing_b_spline(self, img: torch.Tensor, order: int) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Parzen windowing with b-spline kernel (adapted from ITK)

        Args:
            img: the shape should be B[NDHW].
            order: int.
        """
        _max, _min = torch.max(img), torch.min(img)
        padding = 2
        bin_size = (_max - _min) / (self.num_bins - 2 * padding)
        norm_min = torch.div(_min, bin_size) - padding
        window_term = torch.div(img, bin_size) - norm_min
        window_term = torch.clamp(window_term, padding, self.num_bins - padding - 1)
        window_term = window_term.reshape(window_term.shape[0], -1, 1)
        bins = torch.arange(self.num_bins, device=window_term.device).reshape(1, 1, -1)
        sample_bin_matrix = torch.abs(bins - window_term)
        weight = torch.zeros_like(sample_bin_matrix, dtype=torch.float)
        if order == 0:
            weight = weight + (sample_bin_matrix < 0.5) + (sample_bin_matrix == 0.5) * 0.5
        elif order == 3:
            weight = weight + (4 - 6 * sample_bin_matrix ** 2 + 3 * sample_bin_matrix ** 3) * (sample_bin_matrix < 1) / 6
            weight = weight + (2 - sample_bin_matrix) ** 3 * (sample_bin_matrix >= 1) * (sample_bin_matrix < 2) / 6
        else:
            raise ValueError(f'Do not support b-spline {order}-order parzen windowing')
        weight = weight / torch.sum(weight, dim=-1, keepdim=True)
        probability = torch.mean(weight, dim=-2, keepdim=True)
        return weight, probability

    def parzen_windowing_gaussian(self, img: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Parzen windowing with gaussian kernel (adapted from DeepReg implementation)
        Note: the input is expected to range between 0 and 1
        Args:
            img: the shape should be B[NDHW].
        """
        img = torch.clamp(img, 0, 1)
        img = img.reshape(img.shape[0], -1, 1)
        weight = torch.exp(-self.preterm * (img - self.bin_centers) ** 2)
        weight = weight / torch.sum(weight, dim=-1, keepdim=True)
        probability = torch.mean(weight, dim=-2, keepdim=True)
        return weight, probability

    def forward(self, pred: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            pred: the shape should be B[NDHW].
            target: the shape should be same as the pred shape.
        Raises:
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].
        """
        if target.shape != pred.shape:
            raise ValueError(f'ground truth has differing shape ({target.shape}) from pred ({pred.shape})')
        wa, pa, wb, pb = self.parzen_windowing(pred, target)
        pab = torch.bmm(wa.permute(0, 2, 1), wb).div(wa.shape[1])
        papb = torch.bmm(pa.permute(0, 2, 1), pb)
        mi = torch.sum(pab * torch.log((pab + self.smooth_nr) / (papb + self.smooth_dr) + self.smooth_dr), dim=(1, 2))
        if self.reduction == LossReduction.SUM.value:
            return torch.sum(mi).neg()
        if self.reduction == LossReduction.NONE.value:
            return mi.neg()
        if self.reduction == LossReduction.MEAN.value:
            return torch.mean(mi).neg()
        raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')


def make_cauchy_kernel(sigma: int) ->torch.Tensor:
    if sigma <= 0:
        raise ValueError(f'expecting positive sigma, got sigma={sigma}')
    tail = int(sigma * 5)
    k = torch.tensor([((x / sigma) ** 2 + 1) for x in range(-tail, tail + 1)])
    k = torch.reciprocal(k)
    k = k / torch.sum(k)
    return k


kernel_fn_dict = {'gaussian': make_gaussian_kernel, 'cauchy': make_cauchy_kernel}


class MultiScaleLoss(_Loss):
    """
    This is a wrapper class.
    It smooths the input and target at different scales before passing them into the wrapped loss function.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, loss: _Loss, scales: Optional[List]=None, kernel: str='gaussian', reduction: Union[LossReduction, str]=LossReduction.MEAN) ->None:
        """
        Args:
            loss: loss function to be wrapped
            scales: list of scalars or None, if None, do not apply any scaling.
            kernel: gaussian or cauchy.
        """
        super().__init__(reduction=LossReduction(reduction).value)
        if kernel not in kernel_fn_dict:
            raise ValueError(f'got unsupported kernel type: {kernel}', 'only support gaussian and cauchy')
        self.kernel_fn = kernel_fn_dict[kernel]
        self.loss = loss
        self.scales = scales

    def forward(self, y_true: torch.Tensor, y_pred: torch.Tensor) ->torch.Tensor:
        if self.scales is None:
            loss: torch.Tensor = self.loss(y_pred, y_true)
        else:
            loss_list = []
            for s in self.scales:
                if s == 0:
                    loss_list.append(self.loss(y_pred, y_true))
                else:
                    loss_list.append(self.loss(separable_filtering(y_pred, [self.kernel_fn(s)] * (y_true.ndim - 2)), separable_filtering(y_true, [self.kernel_fn(s)] * (y_true.ndim - 2))))
            loss = torch.stack(loss_list, dim=0)
        if self.reduction == LossReduction.MEAN.value:
            loss = torch.mean(loss)
        elif self.reduction == LossReduction.SUM.value:
            loss = torch.sum(loss)
        elif self.reduction != LossReduction.NONE.value:
            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')
        return loss


class MetricReduction(StrEnum):
    """
    See also: :py:func:`monai.metrics.utils.do_metric_reduction`
    """
    NONE = 'none'
    MEAN = 'mean'
    SUM = 'sum'
    MEAN_BATCH = 'mean_batch'
    SUM_BATCH = 'sum_batch'
    MEAN_CHANNEL = 'mean_channel'
    SUM_CHANNEL = 'sum_channel'


def get_dist_device():
    """
    Get the expected target device in the native PyTorch distributed data parallel.
    For NCCL backend, return GPU device of current process.
    For GLOO backend, return CPU.
    For any other backends, return None as the default, tensor.to(None) will not change the device.

    """
    if dist.is_initialized():
        backend = dist.get_backend()
        if backend == 'nccl' and torch.cuda.is_available():
            return torch.device(f'cuda:{torch.cuda.current_device()}')
        if backend == 'gloo':
            return torch.device('cpu')
    return None


class IgniteInfo:
    """
    Config information of the PyTorch ignite package.

    """
    OPT_IMPORT_VERSION = '0.4.4'


Events, has_ignite = optional_import('ignite.engine', IgniteInfo.OPT_IMPORT_VERSION, min_version, 'Events')


idist, has_ignite = optional_import('ignite', IgniteInfo.OPT_IMPORT_VERSION, min_version, 'distributed')


def evenly_divisible_all_gather(data: torch.Tensor, concat: bool=True):
    """
    Utility function for distributed data parallel to pad at first dim to make it evenly divisible and all_gather.
    The input data of every rank should have the same number of dimensions, only the first dim can be different.

    Note: If has ignite installed, will execute based on ignite distributed APIs, otherwise, if the native
    PyTorch distributed group initialized, will execute based on native PyTorch distributed APIs.

    Args:
        data: source tensor to pad and execute all_gather in distributed data parallel.
        concat: whether to concat the gathered list to be a Tensor, if False, return a list
            of Tensors, similar behavior as torch.distributed.all_gather(). default to True.

    Note:
        The input data on different ranks must have exactly same `dtype`.

    """
    if not isinstance(data, torch.Tensor):
        raise ValueError('input data must be PyTorch Tensor.')
    ndims = data.ndimension()
    length: int = data.shape[0] if ndims > 0 else 1

    def _torch_all_gather(data: torch.Tensor) ->List[torch.Tensor]:
        """
        Implementation based on native PyTorch distributed data parallel APIs.

        """
        device = get_dist_device()
        orig_device = data.device
        data = data
        data = data.unsqueeze(0) if ndims == 0 else data
        length_tensor = torch.as_tensor([length], device=device)
        all_lens = [torch.zeros_like(length_tensor) for _ in range(dist.get_world_size())]
        dist.all_gather(all_lens, length_tensor)
        all_lens_: List[int] = [int(i.item()) for i in all_lens]
        max_len: int = max(all_lens_)
        if length < max_len:
            size = [max_len - length] + list(data.shape[1:])
            data = torch.cat([data, data.new_full(size, 0)], dim=0)
        output = [torch.zeros_like(data) for _ in range(dist.get_world_size())]
        dist.all_gather(output, data)
        return [(o.squeeze(0) if ndims == 0 else o[:l, ...]) for o, l in zip(output, all_lens_)]

    def _ignite_all_gather(data: torch.Tensor) ->List[torch.Tensor]:
        """
        Implementation based on PyTorch ignite package, it can support more kinds of backends.

        """
        data = data.unsqueeze(0) if ndims == 0 else data
        all_lens: List[int] = idist.all_gather(length)
        max_len: int = max(all_lens)
        if length < max_len:
            size = [max_len - length] + list(data.shape[1:])
            data = torch.cat([data, data.new_full(size, 0)], dim=0)
        output = idist.all_gather(data)
        if ndims == 0:
            return list(torch.unbind(output, dim=0))
        return [output[i * max_len:i * max_len + l, ...] for i, l in enumerate(all_lens)]
    output: List[torch.Tensor]
    if has_ignite:
        if idist.get_world_size() <= 1:
            return data
        output = _ignite_all_gather(data=data)
    elif dist.is_available() and dist.is_initialized():
        if dist.get_world_size() <= 1:
            return data
        output = _torch_all_gather(data=data)
    else:
        return data
    return torch.cat(output, dim=0) if concat else output


class Cumulative:
    """
    Utility class for the typical cumulative computation process based on PyTorch Tensors.
    It provides interfaces to accumulate values in the local buffers, synchronize buffers across distributed nodes,
    and aggregate the buffered values.

    In multi-processing, PyTorch programs usually distribute data to multiple nodes. Each node runs with a subset
    of the data, adds values to its local buffers. Calling `get_buffer` could gather all the results and
    `aggregate` can further handle the results to generate the final outcomes.

    Users can implement their own `aggregate` method to handle the results,
    using `get_buffer` to get the buffered contents.

    Note: the data list should have the same length every time calling `add()` in a round,
    it will automatically create buffers according to the length of data list.

    Typically, this class is expected to execute the following steps:

    .. code-block:: python

        from monai.metrics import Cumulative

        c = Cumulative()
        c.append(1)  # adds a value
        c.extend([2, 3])  # adds a batch of values
        c.extend([4, 5, 6])  # adds a batch of values
        print(c.get_buffer())  # tensor([1, 2, 3, 4, 5, 6])
        print(len(c))  # 6
        c.reset()
        print(len(c))  # 0

    The following is an example of maintaining two internal buffers:

    .. code-block:: python

        from monai.metrics import Cumulative

        c = Cumulative()
        c.append(1, 2)  # adds a value to two buffers respectively
        c.extend([3, 4], [5, 6])  # adds batches of values
        print(c.get_buffer())  # [tensor([1, 3, 4]), tensor([2, 5, 6])]
        print(len(c))

    The following is an example of extending with variable length data:

    .. code-block:: python

        import torch
        from monai.metrics import Cumulative

        c = Cumulative()
        c.extend(torch.zeros((8, 2)), torch.zeros((6, 2)))  # adds batches
        c.append(torch.zeros((2, )))  # adds a value
        print(c.get_buffer())  # [torch.zeros((9, 2)), torch.zeros((6, 2))]
        print(len(c))

    """

    def __init__(self) ->None:
        """
        Initialize the internal buffers.
        `self._buffers` are local buffers, they are not usually used directly.
        `self._sync_buffers` are the buffers with all the results across all the nodes.
        """
        self._buffers: Optional[List[List[torch.Tensor]]] = None
        self._synced_tensors: Optional[List[Optional[torch.Tensor]]] = None
        self._synced: bool = False
        self.reset()

    def reset(self):
        """
        Reset the buffers for cumulative tensors and the synced results.

        """
        self._buffers = None
        self._synced_tensors = None
        self._synced = False

    def extend(self, *data) ->None:
        """
        Extend the local buffers with new ("batch-first") data.
        A buffer will be allocated for each `data` item.
        Compared with `self.append`, this method adds a "batch" of data to the local buffers.

        Args:
            data: each item can be a "batch-first" tensor or a list of "channel-first" tensors.
                they will be concatenated at the 0-th dimension when `get_buffer()` is called.
        """
        if self._buffers is None:
            self._buffers = [[] for _ in data]
        for b, d in zip(self._buffers, data):
            d_t, *_ = convert_data_type(d, output_type=torch.Tensor, wrap_sequence=True)
            try:
                b.extend([x[0] for x in torch.split(d_t, 1, dim=0)])
            except (AttributeError, IndexError, RuntimeError) as e:
                raise TypeError(f'{e}. `data` should be a batch-first tensor or a list of channel-first tensors, got {type(d_t)}') from e
        self._synced = False

    def append(self, *data) ->None:
        """
        Add samples to the local cumulative buffers.
        A buffer will be allocated for each `data` item.
        Compared with `self.extend`, this method adds a single sample (instead
        of a "batch") to the local buffers.

        Args:
            data: each item will be converted into a torch tensor.
                they will be stacked at the 0-th dim with a new dimension when `get_buffer()` is called.

        """
        if self._buffers is None:
            self._buffers = [[] for _ in data]
        for b, d in zip(self._buffers, data):
            d_t, *_ = convert_data_type(d, output_type=torch.Tensor, wrap_sequence=True)
            b.append(d_t)
        self._synced = False

    @abstractmethod
    def aggregate(self, *args: Any, **kwargs: Any):
        """
        Aggregate final results based on the gathered buffers.
        This method is expected to use `get_buffer` to gather the local buffer contents.

        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')

    def _sync(self):
        """
        All gather the buffers across distributed ranks for aggregating.
        Each buffer will be concatenated as a PyTorch Tensor.

        """
        if self._synced or self._buffers is None:
            return
        try:
            self._synced_tensors = [evenly_divisible_all_gather(torch.stack(b, dim=0), concat=True) for b in self._buffers]
        except (RuntimeError, TypeError, ValueError) as e:
            raise TypeError(f'{e}. unable to sync buffer contents: {self._buffers}.') from e
        self._synced = True

    def __len__(self):
        """
        Return the length of the largest buffer.
        Note that the method will trigger synchronization of the local buffers.
        """
        self._sync()
        if self._synced_tensors is None:
            return 0
        return max(len(x) for x in self._synced_tensors if x is not None)

    def get_buffer(self):
        """
        Get the synchronized list of buffers.
        A typical usage is to generate the metrics report based on the raw metric details.
        Each buffer is a PyTorch Tensor.

        """
        self._sync()
        if self._synced_tensors is None:
            return self._synced_tensors
        buffers = [(x.detach().clone() if isinstance(x, torch.Tensor) else x) for x in self._synced_tensors]
        return buffers[0] if len(buffers) == 1 else buffers


class Metric(ABC):
    """
    Base class for metric computation for evaluating the performance of a model.
    `__call__` is designed to execute the computation.

    """

    @abstractmethod
    def __call__(self, *args: Any, **kwargs: Any):
        """
        This method should take raw model outputs as inputs, and return values that measure the models' quality.
        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')


TensorOrList = Union[torch.Tensor, Sequence[torch.Tensor]]


class IterationMetric(Metric):
    """
    Base class for metrics computation at the iteration level, that is, on a min-batch of samples
    usually using the model outcome of one iteration.

    `__call__` is designed to handle `y_pred` and `y` (optional) in torch tensors or a list/tuple of tensors.

    Subclasses typically implement the `_compute_tensor` function for the actual tensor computation logic.
    """

    def __call__(self, y_pred: TensorOrList, y: Optional[TensorOrList]=None):
        """
        Execute basic computation for model prediction `y_pred` and ground truth `y` (optional).
        It supports inputs of a list of "channel-first" Tensor and a "batch-first" Tensor.

        Args:
            y_pred: the raw model prediction data at one iteration, must be a list of `channel-first` Tensor
                or a `batch-first` Tensor.
            y: the ground truth to compute, must be a list of `channel-first` Tensor
                or a `batch-first` Tensor.

        Returns:
            The computed metric values at the iteration level.
            The output shape could be a `batch-first` tensor or a list of `batch-first` tensors.
            When it's a list of tensors, each item in the list can represent a specific type of metric.

        """
        if isinstance(y_pred, (list, tuple)) or isinstance(y, (list, tuple)):
            return self._compute_list(y_pred, y)
        if isinstance(y_pred, torch.Tensor):
            y_ = y.detach() if isinstance(y, torch.Tensor) else None
            return self._compute_tensor(y_pred.detach(), y_)
        raise ValueError('y_pred or y must be a list/tuple of `channel-first` Tensors or a `batch-first` Tensor.')

    def _compute_list(self, y_pred: TensorOrList, y: Optional[TensorOrList]=None):
        """
        Execute the metric computation for `y_pred` and `y` in a list of "channel-first" tensors.

        The return value is a "batch-first" tensor, or a list of "batch-first" tensors.
        When it's a list of tensors, each item in the list can represent a specific type of metric values.

        For example, `self._compute_tensor` may be implemented as returning a list of `batch_size` items,
        where each item is a tuple of three values `tp`, `fp`, `fn` for true positives, false positives,
        and false negatives respectively. This function will return a list of three items,
        (`tp_batched`, `fp_batched`, `fn_batched`), where each item is a `batch_size`-length tensor.

        Note: subclass may enhance the operation to have multi-thread support.
        """
        if y is not None:
            ret = [self._compute_tensor(p.detach().unsqueeze(0), y_.detach().unsqueeze(0)) for p, y_ in zip(y_pred, y)]
        else:
            ret = [self._compute_tensor(p_.detach().unsqueeze(0), None) for p_ in y_pred]
        if isinstance(ret[0], torch.Tensor):
            return torch.cat(ret, dim=0)
        if isinstance(ret[0], (list, tuple)) and all(isinstance(i, torch.Tensor) for i in ret[0]):
            return [torch.cat(batch_i, dim=0) for batch_i in zip(*ret)]
        return ret

    @abstractmethod
    def _compute_tensor(self, y_pred: torch.Tensor, y: Optional[torch.Tensor]=None):
        """
        Computation logic for `y_pred` and `y` of an iteration, the data should be "batch-first" Tensors.
        A subclass should implement its own computation logic.
        The return value is usually a "batch_first" tensor, or a list of "batch_first" tensors.
        """
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')


class CumulativeIterationMetric(Cumulative, IterationMetric):
    """
    Base class of cumulative metric which collects metrics on each mini-batch data at the iteration level.

    Typically, it computes some intermediate results for each iteration, adds them to the buffers,
    then the buffer contents could be gathered and aggregated for the final result when epoch completed.
    Currently,``Cumulative.aggregate()`` and ``IterationMetric._compute_tensor()`` are expected to be implemented.

    For example, `MeanDice` inherits this class and the usage is as follows:

    .. code-block:: python

        dice_metric = DiceMetric(include_background=True, reduction="mean")

        for val_data in val_loader:
            val_outputs = model(val_data["img"])
            val_outputs = [postprocessing_transform(i) for i in decollate_batch(val_outputs)]
            # compute metric for current iteration
            dice_metric(y_pred=val_outputs, y=val_data["seg"])  # callable to add metric to the buffer

        # aggregate the final mean dice result
        metric = dice_metric.aggregate().item()

        # reset the status for next computation round
        dice_metric.reset()

    And to load `predictions` and `labels` from files, then compute metrics with multi-processing, please refer to:
    https://github.com/Project-MONAI/tutorials/blob/master/modules/compute_metric.py.

    """

    def __call__(self, y_pred: TensorOrList, y: Optional[TensorOrList]=None):
        """
        Execute basic computation for model prediction and ground truth.
        It can support  both `list of channel-first Tensor` and `batch-first Tensor`.
        Users call this API to execute computation on every batch of data, then accumulate the results,
        or accumulate the original `y_pred` and `y`, then execute on the accumulated data.

        Args:
            y_pred: the model prediction data to compute, must be a list of `channel-first` Tensor
                or a `batch-first` Tensor.
            y: the ground truth to compute, must be a list of `channel-first` Tensor
                or a `batch-first` Tensor.

        Returns:
            The computed metric values at the iteration level. The output shape should be
            a `batch-first` tensor (BC[HWD]) or a list of `batch-first` tensors.
        """
        ret = super().__call__(y_pred=y_pred, y=y)
        if isinstance(ret, (tuple, list)):
            self.extend(*ret)
        else:
            self.extend(ret)
        return ret


def do_metric_reduction(f: torch.Tensor, reduction: Union[MetricReduction, str]=MetricReduction.MEAN):
    """
    This function is to do the metric reduction for calculated `not-nan` metrics of each sample's each class.
    The function also returns `not_nans`, which counts the number of not nans for the metric.

    Args:
        f: a tensor that contains the calculated metric scores per batch and
            per class. The first two dims should be batch and class.
        reduction: define the mode to reduce metrics, will only apply reduction on `not-nan` values,
            available reduction modes: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
            ``"mean_channel"``, ``"sum_channel"``}, default to ``"mean"``.
            if "none", return the input f tensor and not_nans.

    Raises:
        ValueError: When ``reduction`` is not one of
            ["mean", "sum", "mean_batch", "sum_batch", "mean_channel", "sum_channel" "none"].
    """
    nans = torch.isnan(f)
    not_nans = (~nans).float()
    t_zero = torch.zeros(1, device=f.device, dtype=f.dtype)
    reduction = look_up_option(reduction, MetricReduction)
    if reduction == MetricReduction.NONE:
        return f, not_nans
    f[nans] = 0
    if reduction == MetricReduction.MEAN:
        not_nans = not_nans.sum(dim=1)
        f = torch.where(not_nans > 0, f.sum(dim=1) / not_nans, t_zero)
        not_nans = (not_nans > 0).float().sum(dim=0)
        f = torch.where(not_nans > 0, f.sum(dim=0) / not_nans, t_zero)
    elif reduction == MetricReduction.SUM:
        not_nans = not_nans.sum(dim=[0, 1])
        f = torch.sum(f, dim=[0, 1])
    elif reduction == MetricReduction.MEAN_BATCH:
        not_nans = not_nans.sum(dim=0)
        f = torch.where(not_nans > 0, f.sum(dim=0) / not_nans, t_zero)
    elif reduction == MetricReduction.SUM_BATCH:
        not_nans = not_nans.sum(dim=0)
        f = f.sum(dim=0)
    elif reduction == MetricReduction.MEAN_CHANNEL:
        not_nans = not_nans.sum(dim=1)
        f = torch.where(not_nans > 0, f.sum(dim=1) / not_nans, t_zero)
    elif reduction == MetricReduction.SUM_CHANNEL:
        not_nans = not_nans.sum(dim=1)
        f = f.sum(dim=1)
    elif reduction != MetricReduction.NONE:
        raise ValueError(f'Unsupported reduction: {reduction}, available options are ["mean", "sum", "mean_batch", "sum_batch", "mean_channel", "sum_channel" "none"].')
    return f, not_nans


class RegressionMetric(CumulativeIterationMetric):
    """
    Base class for regression metrics.
    Input `y_pred` is compared with ground truth `y`.
    Both `y_pred` and `y` are expected to be real-valued, where `y_pred` is output from a regression model.
    `y_preds` and `y` can be a list of channel-first Tensor (CHW[D]) or a batch-first Tensor (BCHW[D]).

    Example of the typical execution steps of this metric class follows :py:class:`monai.metrics.metric.Cumulative`.

    Args:
        reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
            available reduction modes: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
            ``"mean_channel"``, ``"sum_channel"``}, default to ``"mean"``. if "none", will not do reduction.
        get_not_nans: whether to return the `not_nans` count, if True, aggregate() returns (metric, not_nans).
            Here `not_nans` count the number of not nans for the metric, thus its shape equals to the shape of the metric.

    """

    def __init__(self, reduction: Union[MetricReduction, str]=MetricReduction.MEAN, get_not_nans: bool=False) ->None:
        super().__init__()
        self.reduction = reduction
        self.get_not_nans = get_not_nans

    def aggregate(self, reduction: Union[MetricReduction, str, None]=None):
        """
        Args:
            reduction: define mode of reduction to the metrics, will only apply reduction on `not-nan` values,
                available reduction modes: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
                ``"mean_channel"``, ``"sum_channel"``}, default to `self.reduction`. if "none", will not do reduction.
        """
        data = self.get_buffer()
        if not isinstance(data, torch.Tensor):
            raise ValueError('the data to aggregate must be PyTorch Tensor.')
        f, not_nans = do_metric_reduction(data, reduction or self.reduction)
        return (f, not_nans) if self.get_not_nans else f

    def _check_shape(self, y_pred: torch.Tensor, y: torch.Tensor) ->None:
        if y_pred.shape != y.shape:
            raise ValueError(f'y_pred and y shapes dont match, received y_pred: [{y_pred.shape}] and y: [{y.shape}]')
        if len(y_pred.shape) < 2:
            raise ValueError('either channel or spatial dimensions required, found only batch dimension')

    @abstractmethod
    def _compute_metric(self, y_pred: torch.Tensor, y: torch.Tensor) ->torch.Tensor:
        raise NotImplementedError(f'Subclass {self.__class__.__name__} must implement this method.')

    def _compute_tensor(self, y_pred: torch.Tensor, y: torch.Tensor):
        if not isinstance(y_pred, torch.Tensor) or not isinstance(y, torch.Tensor):
            raise ValueError('y_pred and y must be PyTorch Tensor.')
        self._check_shape(y_pred, y)
        return self._compute_metric(y_pred, y)


class SSIMMetric(RegressionMetric):
    """
    Build a Pytorch version of the SSIM metric based on the original formula of SSIM

    .. math::
        \\operatorname {SSIM}(x,y) =\\frac {(2 \\mu_x \\mu_y + c_1)(2 \\sigma_{xy} + c_2)}{((\\mu_x^2 + \\
                \\mu_y^2 + c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}

    For more info, visit
        https://vicuesoft.com/glossary/term/ssim-ms-ssim/

    Modified and adopted from:
        https://github.com/facebookresearch/fastMRI/blob/main/banding_removal/fastmri/ssim_loss_mixin.py

    SSIM reference paper:
        Wang, Zhou, et al. "Image quality assessment: from error visibility to structural
        similarity." IEEE transactions on image processing 13.4 (2004): 600-612.

    Args:
        data_range: dynamic range of the data
        win_size: gaussian weighting window size
        k1: stability constant used in the luminance denominator
        k2: stability constant used in the contrast denominator
        spatial_dims: if 2, input shape is expected to be (B,C,W,H). if 3, it is expected to be (B,C,W,H,D)
        reduction: define the mode to reduce metrics, will only execute reduction on `not-nan` values,
            available reduction modes: {``"none"``, ``"mean"``, ``"sum"``, ``"mean_batch"``, ``"sum_batch"``,
            ``"mean_channel"``, ``"sum_channel"``}, default to ``"mean"``. if "none", will not do reduction
        get_not_nans: whether to return the `not_nans` count, if True, aggregate() returns (metric, not_nans)
    """

    def __init__(self, data_range: torch.Tensor, win_size: int=7, k1: float=0.01, k2: float=0.03, spatial_dims: int=2, reduction: Union[MetricReduction, str]=MetricReduction.MEAN, get_not_nans: bool=False):
        super().__init__(reduction=reduction, get_not_nans=get_not_nans)
        self.data_range = data_range
        self.win_size = win_size
        self.k1, self.k2 = k1, k2
        self.spatial_dims = spatial_dims
        self.cov_norm = win_size ** 2 / (win_size ** 2 - 1)
        self.w = torch.ones([1, 1] + [win_size for _ in range(spatial_dims)]) / win_size ** spatial_dims

    def _compute_intermediate_statistics(self, x: torch.Tensor, y: torch.Tensor) ->Tuple[torch.Tensor, ...]:
        data_range = self.data_range[(None,) * (self.spatial_dims + 2)]
        conv = getattr(F, f'conv{self.spatial_dims}d')
        w = convert_to_dst_type(src=self.w, dst=x)[0]
        c1 = (self.k1 * data_range) ** 2
        c2 = (self.k2 * data_range) ** 2
        ux = conv(x, w)
        uy = conv(y, w)
        uxx = conv(x * x, w)
        uyy = conv(y * y, w)
        uxy = conv(x * y, w)
        vx = self.cov_norm * (uxx - ux * ux)
        vy = self.cov_norm * (uyy - uy * uy)
        vxy = self.cov_norm * (uxy - ux * uy)
        return c1, c2, ux, uy, vx, vy, vxy

    def _compute_metric(self, x: torch.Tensor, y: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: first sample (e.g., the reference image). Its shape is (B,C,W,H) for 2D data and (B,C,W,H,D) for 3D.
                A fastMRI sample should use the 2D format with C being the number of slices.
            y: second sample (e.g., the reconstructed image). It has similar shape as x

        Returns:
            ssim_value

        Example:
            .. code-block:: python

                import torch
                x = torch.ones([1,1,10,10])/2 # ground truth
                y = torch.ones([1,1,10,10])/2 # prediction
                data_range = x.max().unsqueeze(0)
                # the following line should print 1.0 (or 0.9999)
                print(SSIMMetric(data_range=data_range,spatial_dims=2)._compute_metric(x,y))
        """
        if x.shape[1] > 1:
            if x.shape[1] != y.shape[1]:
                raise ValueError(f'x and y should have the same number of channels, but x has {x.shape[1]} channels and y has {y.shape[1]} channels.')
            ssim = torch.stack([SSIMMetric(self.data_range, self.win_size, self.k1, self.k2, self.spatial_dims)(x[:, i, ...].unsqueeze(1), y[:, i, ...].unsqueeze(1)) for i in range(x.shape[1])])
            channel_wise_ssim = ssim.mean(1).view(-1, 1)
            return channel_wise_ssim
        c1, c2, ux, uy, vx, vy, vxy = self._compute_intermediate_statistics(x, y)
        numerator = (2 * ux * uy + c1) * (2 * vxy + c2)
        denom = (ux ** 2 + uy ** 2 + c1) * (vx + vy + c2)
        ssim_value = numerator / denom
        ssim_per_batch: torch.Tensor = ssim_value.view(ssim_value.shape[1], -1).mean(1, keepdim=True)
        return ssim_per_batch

    def _compute_metric_and_contrast(self, x: torch.Tensor, y: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: first sample (e.g., the reference image). Its shape is (B,C,W,H) for 2D data and (B,C,W,H,D) for 3D.
                A fastMRI sample should use the 2D format with C being the number of slices.
            y: second sample (e.g., the reconstructed image). It has similar shape as x

        Returns:
            ssim_value, cs_value
        """
        if x.shape[1] > 1:
            if x.shape[1] != y.shape[1]:
                raise ValueError(f'x and y should have the same number of channels, but x has {x.shape[1]} channels and y has {y.shape[1]} channels.')
            ssim_ls = []
            cs_ls = []
            for i in range(x.shape[1]):
                ssim_val, cs_val = SSIMMetric(self.data_range, self.win_size, self.k1, self.k2, self.spatial_dims)._compute_metric_and_contrast(x[:, i, ...].unsqueeze(1), y[:, i, ...].unsqueeze(1))
                ssim_ls.append(ssim_val)
                cs_ls.append(cs_val)
            channel_wise_ssim: torch.Tensor = torch.stack(ssim_ls).mean(1).view(-1, 1)
            channel_wise_cs: torch.Tensor = torch.stack(cs_ls).mean(1).view(-1, 1)
            return channel_wise_ssim, channel_wise_cs
        c1, c2, ux, uy, vx, vy, vxy = self._compute_intermediate_statistics(x, y)
        numerator = (2 * ux * uy + c1) * (2 * vxy + c2)
        denom = (ux ** 2 + uy ** 2 + c1) * (vx + vy + c2)
        ssim_value = numerator / denom
        ssim_per_batch: torch.Tensor = ssim_value.view(ssim_value.shape[1], -1).mean(1, keepdim=True)
        cs_per_batch: torch.Tensor = (2 * vxy + c2) / (vx + vy + c2)
        cs_per_batch = cs_per_batch.view(cs_per_batch.shape[0], -1).mean(1, keepdim=True)
        return ssim_per_batch, cs_per_batch


class SSIMLoss(_Loss):
    """
    Build a Pytorch version of the SSIM loss function based on the original formula of SSIM

    Modified and adopted from:
        https://github.com/facebookresearch/fastMRI/blob/main/banding_removal/fastmri/ssim_loss_mixin.py

    For more info, visit
        https://vicuesoft.com/glossary/term/ssim-ms-ssim/

    SSIM reference paper:
        Wang, Zhou, et al. "Image quality assessment: from error visibility to structural
        similarity." IEEE transactions on image processing 13.4 (2004): 600-612.
    """

    def __init__(self, win_size: int=7, k1: float=0.01, k2: float=0.03, spatial_dims: int=2):
        """
        Args:
            win_size: gaussian weighting window size
            k1: stability constant used in the luminance denominator
            k2: stability constant used in the contrast denominator
            spatial_dims: if 2, input shape is expected to be (B,C,H,W). if 3, it is expected to be (B,C,H,W,D)
        """
        super().__init__()
        self.win_size = win_size
        self.k1, self.k2 = k1, k2
        self.spatial_dims = spatial_dims

    def forward(self, x: torch.Tensor, y: torch.Tensor, data_range: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: first sample (e.g., the reference image). Its shape is (B,C,W,H) for 2D and pseudo-3D data,
                and (B,C,W,H,D) for 3D data,
            y: second sample (e.g., the reconstructed image). It has similar shape as x.
            data_range: dynamic range of the data

        Returns:
            1-ssim_value (recall this is meant to be a loss function)

        Example:
            .. code-block:: python

                import torch

                # 2D data
                x = torch.ones([1,1,10,10])/2
                y = torch.ones([1,1,10,10])/2
                data_range = x.max().unsqueeze(0)
                # the following line should print 1.0 (or 0.9999)
                print(1-SSIMLoss(spatial_dims=2)(x,y,data_range))

                # pseudo-3D data
                x = torch.ones([1,5,10,10])/2  # 5 could represent number of slices
                y = torch.ones([1,5,10,10])/2
                data_range = x.max().unsqueeze(0)
                # the following line should print 1.0 (or 0.9999)
                print(1-SSIMLoss(spatial_dims=2)(x,y,data_range))

                # 3D data
                x = torch.ones([1,1,10,10,10])/2
                y = torch.ones([1,1,10,10,10])/2
                data_range = x.max().unsqueeze(0)
                # the following line should print 1.0 (or 0.9999)
                print(1-SSIMLoss(spatial_dims=3)(x,y,data_range))
        """
        if x.shape[0] == 1:
            ssim_value: torch.Tensor = SSIMMetric(data_range, self.win_size, self.k1, self.k2, self.spatial_dims)._compute_tensor(x, y)
        elif x.shape[0] > 1:
            for i in range(x.shape[0]):
                ssim_val: torch.Tensor = SSIMMetric(data_range, self.win_size, self.k1, self.k2, self.spatial_dims)._compute_tensor(x[i:i + 1], y[i:i + 1])
                if i == 0:
                    ssim_value = ssim_val
                else:
                    ssim_value = torch.cat((ssim_value.view(1), ssim_val.view(1)), dim=0)
        else:
            raise ValueError('Batch size is not nonnegative integer value')
        ssim_value = ssim_value.view(-1, 1)
        loss: torch.Tensor = 1 - ssim_value.mean()
        return loss


class TverskyLoss(_Loss):
    """
    Compute the Tversky loss defined in:

        Sadegh et al. (2017) Tversky loss function for image segmentation
        using 3D fully convolutional deep networks. (https://arxiv.org/abs/1706.05721)

    Adapted from:
        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L631

    """

    def __init__(self, include_background: bool=True, to_onehot_y: bool=False, sigmoid: bool=False, softmax: bool=False, other_act: Optional[Callable]=None, alpha: float=0.5, beta: float=0.5, reduction: Union[LossReduction, str]=LossReduction.MEAN, smooth_nr: float=1e-05, smooth_dr: float=1e-05, batch: bool=False) ->None:
        """
        Args:
            include_background: If False channel index 0 (background category) is excluded from the calculation.
            to_onehot_y: whether to convert `y` into the one-hot format. Defaults to False.
            sigmoid: If True, apply a sigmoid function to the prediction.
            softmax: If True, apply a softmax function to the prediction.
            other_act: if don't want to use `sigmoid` or `softmax`, use other callable function to execute
                other activation layers, Defaults to ``None``. for example:
                `other_act = torch.tanh`.
            alpha: weight of false positives
            beta: weight of false negatives
            reduction: {``"none"``, ``"mean"``, ``"sum"``}
                Specifies the reduction to apply to the output. Defaults to ``"mean"``.

                - ``"none"``: no reduction will be applied.
                - ``"mean"``: the sum of the output will be divided by the number of elements in the output.
                - ``"sum"``: the output will be summed.

            smooth_nr: a small constant added to the numerator to avoid zero.
            smooth_dr: a small constant added to the denominator to avoid nan.
            batch: whether to sum the intersection and union areas over the batch dimension before the dividing.
                Defaults to False, a Dice loss value is computed independently from each item in the batch
                before any `reduction`.

        Raises:
            TypeError: When ``other_act`` is not an ``Optional[Callable]``.
            ValueError: When more than 1 of [``sigmoid=True``, ``softmax=True``, ``other_act is not None``].
                Incompatible values.

        """
        super().__init__(reduction=LossReduction(reduction).value)
        if other_act is not None and not callable(other_act):
            raise TypeError(f'other_act must be None or callable but is {type(other_act).__name__}.')
        if int(sigmoid) + int(softmax) + int(other_act is not None) > 1:
            raise ValueError('Incompatible values: more than 1 of [sigmoid=True, softmax=True, other_act is not None].')
        self.include_background = include_background
        self.to_onehot_y = to_onehot_y
        self.sigmoid = sigmoid
        self.softmax = softmax
        self.other_act = other_act
        self.alpha = alpha
        self.beta = beta
        self.smooth_nr = float(smooth_nr)
        self.smooth_dr = float(smooth_dr)
        self.batch = batch

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """
        Args:
            input: the shape should be BNH[WD].
            target: the shape should be BNH[WD].

        Raises:
            ValueError: When ``self.reduction`` is not one of ["mean", "sum", "none"].

        """
        if self.sigmoid:
            input = torch.sigmoid(input)
        n_pred_ch = input.shape[1]
        if self.softmax:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `softmax=True` ignored.')
            else:
                input = torch.softmax(input, 1)
        if self.other_act is not None:
            input = self.other_act(input)
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                target = one_hot(target, num_classes=n_pred_ch)
        if not self.include_background:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `include_background=False` ignored.')
            else:
                target = target[:, 1:]
                input = input[:, 1:]
        if target.shape != input.shape:
            raise AssertionError(f'ground truth has differing shape ({target.shape}) from input ({input.shape})')
        p0 = input
        p1 = 1 - p0
        g0 = target
        g1 = 1 - g0
        reduce_axis: List[int] = torch.arange(2, len(input.shape)).tolist()
        if self.batch:
            reduce_axis = [0] + reduce_axis
        tp = torch.sum(p0 * g0, reduce_axis)
        fp = self.alpha * torch.sum(p0 * g1, reduce_axis)
        fn = self.beta * torch.sum(p1 * g0, reduce_axis)
        numerator = tp + self.smooth_nr
        denominator = tp + fp + fn + self.smooth_dr
        score: torch.Tensor = 1.0 - numerator / denominator
        if self.reduction == LossReduction.SUM.value:
            return torch.sum(score)
        if self.reduction == LossReduction.NONE.value:
            return score
        if self.reduction == LossReduction.MEAN.value:
            return torch.mean(score)
        raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')


class AsymmetricFocalTverskyLoss(_Loss):
    """
    AsymmetricFocalTverskyLoss is a variant of FocalTverskyLoss, which attentions to the foreground class.

    Actually, it's only supported for binary image segmentation now.

    Reimplementation of the Asymmetric Focal Tversky Loss described in:

    - "Unified Focal Loss: Generalising Dice and Cross Entropy-based Losses to Handle Class Imbalanced Medical Image Segmentation",
    Michael Yeung, Computerized Medical Imaging and Graphics
    """

    def __init__(self, to_onehot_y: bool=False, delta: float=0.7, gamma: float=0.75, epsilon: float=1e-07, reduction: Union[LossReduction, str]=LossReduction.MEAN) ->None:
        """
        Args:
            to_onehot_y: whether to convert `y` into the one-hot format. Defaults to False.
            delta : weight of the background. Defaults to 0.7.
            gamma : value of the exponent gamma in the definition of the Focal loss  . Defaults to 0.75.
            epsilon : it defines a very small number each time. simmily smooth value. Defaults to 1e-7.
        """
        super().__init__(reduction=LossReduction(reduction).value)
        self.to_onehot_y = to_onehot_y
        self.delta = delta
        self.gamma = gamma
        self.epsilon = epsilon

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) ->torch.Tensor:
        n_pred_ch = y_pred.shape[1]
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                y_true = one_hot(y_true, num_classes=n_pred_ch)
        if y_true.shape != y_pred.shape:
            raise ValueError(f'ground truth has different shape ({y_true.shape}) from input ({y_pred.shape})')
        y_pred = torch.clamp(y_pred, self.epsilon, 1.0 - self.epsilon)
        axis = list(range(2, len(y_pred.shape)))
        tp = torch.sum(y_true * y_pred, dim=axis)
        fn = torch.sum(y_true * (1 - y_pred), dim=axis)
        fp = torch.sum((1 - y_true) * y_pred, dim=axis)
        dice_class = (tp + self.epsilon) / (tp + self.delta * fn + (1 - self.delta) * fp + self.epsilon)
        back_dice = 1 - dice_class[:, 0]
        fore_dice = (1 - dice_class[:, 1]) * torch.pow(1 - dice_class[:, 1], -self.gamma)
        loss = torch.mean(torch.stack([back_dice, fore_dice], dim=-1))
        return loss


class AsymmetricFocalLoss(_Loss):
    """
    AsymmetricFocalLoss is a variant of FocalTverskyLoss, which attentions to the foreground class.

    Actually, it's only supported for binary image segmentation now.

    Reimplementation of the Asymmetric Focal Loss described in:

    - "Unified Focal Loss: Generalising Dice and Cross Entropy-based Losses to Handle Class Imbalanced Medical Image Segmentation",
    Michael Yeung, Computerized Medical Imaging and Graphics
    """

    def __init__(self, to_onehot_y: bool=False, delta: float=0.7, gamma: float=2, epsilon: float=1e-07, reduction: Union[LossReduction, str]=LossReduction.MEAN):
        """
        Args:
            to_onehot_y : whether to convert `y` into the one-hot format. Defaults to False.
            delta : weight of the background. Defaults to 0.7.
            gamma : value of the exponent gamma in the definition of the Focal loss  . Defaults to 0.75.
            epsilon : it defines a very small number each time. simmily smooth value. Defaults to 1e-7.
        """
        super().__init__(reduction=LossReduction(reduction).value)
        self.to_onehot_y = to_onehot_y
        self.delta = delta
        self.gamma = gamma
        self.epsilon = epsilon

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) ->torch.Tensor:
        n_pred_ch = y_pred.shape[1]
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                y_true = one_hot(y_true, num_classes=n_pred_ch)
        if y_true.shape != y_pred.shape:
            raise ValueError(f'ground truth has different shape ({y_true.shape}) from input ({y_pred.shape})')
        y_pred = torch.clamp(y_pred, self.epsilon, 1.0 - self.epsilon)
        cross_entropy = -y_true * torch.log(y_pred)
        back_ce = torch.pow(1 - y_pred[:, 0], self.gamma) * cross_entropy[:, 0]
        back_ce = (1 - self.delta) * back_ce
        fore_ce = cross_entropy[:, 1]
        fore_ce = self.delta * fore_ce
        loss = torch.mean(torch.sum(torch.stack([back_ce, fore_ce], dim=1), dim=1))
        return loss


class AsymmetricUnifiedFocalLoss(_Loss):
    """
    AsymmetricUnifiedFocalLoss is a variant of Focal Loss.

    Actually, it's only supported for binary image segmentation now

    Reimplementation of the Asymmetric Unified Focal Tversky Loss described in:

    - "Unified Focal Loss: Generalising Dice and Cross Entropy-based Losses to Handle Class Imbalanced Medical Image Segmentation",
    Michael Yeung, Computerized Medical Imaging and Graphics
    """

    def __init__(self, to_onehot_y: bool=False, num_classes: int=2, weight: float=0.5, gamma: float=0.5, delta: float=0.7, reduction: Union[LossReduction, str]=LossReduction.MEAN):
        """
        Args:
            to_onehot_y : whether to convert `y` into the one-hot format. Defaults to False.
            num_classes : number of classes, it only supports 2 now. Defaults to 2.
            delta : weight of the background. Defaults to 0.7.
            gamma : value of the exponent gamma in the definition of the Focal loss. Defaults to 0.75.
            epsilon : it defines a very small number each time. simmily smooth value. Defaults to 1e-7.
            weight : weight for each loss function, if it's none it's 0.5. Defaults to None.

        Example:
            >>> import torch
            >>> from monai.losses import AsymmetricUnifiedFocalLoss
            >>> pred = torch.ones((1,1,32,32), dtype=torch.float32)
            >>> grnd = torch.ones((1,1,32,32), dtype=torch.int64)
            >>> fl = AsymmetricUnifiedFocalLoss(to_onehot_y=True)
            >>> fl(pred, grnd)
        """
        super().__init__(reduction=LossReduction(reduction).value)
        self.to_onehot_y = to_onehot_y
        self.num_classes = num_classes
        self.gamma = gamma
        self.delta = delta
        self.weight: float = weight
        self.asy_focal_loss = AsymmetricFocalLoss(gamma=self.gamma, delta=self.delta)
        self.asy_focal_tversky_loss = AsymmetricFocalTverskyLoss(gamma=self.gamma, delta=self.delta)

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) ->torch.Tensor:
        """
        Args:
            y_pred : the shape should be BNH[WD], where N is the number of classes.
                It only supports binary segmentation.
                The input should be the original logits since it will be transformed by
                    a sigmoid in the forward function.
            y_true : the shape should be BNH[WD], where N is the number of classes.
                It only supports binary segmentation.

        Raises:
            ValueError: When input and target are different shape
            ValueError: When len(y_pred.shape) != 4 and len(y_pred.shape) != 5
            ValueError: When num_classes
            ValueError: When the number of classes entered does not match the expected number
        """
        if y_pred.shape != y_true.shape:
            raise ValueError(f'ground truth has different shape ({y_true.shape}) from input ({y_pred.shape})')
        if len(y_pred.shape) != 4 and len(y_pred.shape) != 5:
            raise ValueError(f'input shape must be 4 or 5, but got {y_pred.shape}')
        if y_pred.shape[1] == 1:
            y_pred = one_hot(y_pred, num_classes=self.num_classes)
            y_true = one_hot(y_true, num_classes=self.num_classes)
        if torch.max(y_true) != self.num_classes - 1:
            raise ValueError(f'Please make sure the number of classes is {self.num_classes - 1}')
        n_pred_ch = y_pred.shape[1]
        if self.to_onehot_y:
            if n_pred_ch == 1:
                warnings.warn('single channel prediction, `to_onehot_y=True` ignored.')
            else:
                y_true = one_hot(y_true, num_classes=n_pred_ch)
        asy_focal_loss = self.asy_focal_loss(y_pred, y_true)
        asy_focal_tversky_loss = self.asy_focal_tversky_loss(y_pred, y_true)
        loss: torch.Tensor = self.weight * asy_focal_loss + (1 - self.weight) * asy_focal_tversky_loss
        if self.reduction == LossReduction.SUM.value:
            return torch.sum(loss)
        if self.reduction == LossReduction.NONE.value:
            return loss
        if self.reduction == LossReduction.MEAN.value:
            return torch.mean(loss)
        raise ValueError(f'Unsupported reduction: {self.reduction}, available options are ["mean", "sum", "none"].')


class Swish(nn.Module):
    """Applies the element-wise function:

    .. math::
        \\text{Swish}(x) = x * \\text{Sigmoid}(\\alpha * x) ~~~~\\text{for constant value}~ \\alpha.

    Citation: Searching for Activation Functions, Ramachandran et al., 2017, https://arxiv.org/abs/1710.05941.


    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional dimensions
        - Output: :math:`(N, *)`, same shape as the input


    Examples::

        >>> import torch
        >>> from monai.networks.layers.factories import Act
        >>> m = Act['swish']()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """

    def __init__(self, alpha=1.0):
        super().__init__()
        self.alpha = alpha

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        return input * torch.sigmoid(self.alpha * input)


class MemoryEfficientSwish(nn.Module):
    """Applies the element-wise function:

    .. math::
        \\text{Swish}(x) = x * \\text{Sigmoid}(\\alpha * x) ~~~~\\text{for constant value}~ \\alpha=1.

    Memory efficient implementation for training following recommendation from:
    https://github.com/lukemelas/EfficientNet-PyTorch/issues/18#issuecomment-511677853

    Results in ~ 30% memory saving during training as compared to Swish()

    Citation: Searching for Activation Functions, Ramachandran et al., 2017, https://arxiv.org/abs/1710.05941.

    From Pytorch 1.7.0+, the optimized version of `Swish` named `SiLU` is implemented,
    this class will utilize `torch.nn.functional.silu` to do the calculation if meets the version.

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input


    Examples::

        >>> import torch
        >>> from monai.networks.layers.factories import Act
        >>> m = Act['memswish']()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """

    def __init__(self, inplace: bool=False):
        super().__init__()
        self.inplace = inplace

    def forward(self, input: torch.Tensor):
        return monai_swish(input, self.inplace)


class Mish(nn.Module):
    """Applies the element-wise function:

    .. math::
        \\text{Mish}(x) = x * tanh(\\text{softplus}(x)).

    Citation: Mish: A Self Regularized Non-Monotonic Activation Function, Diganta Misra, 2019, https://arxiv.org/abs/1908.08681.

    From Pytorch 1.9.0+, the optimized version of `Mish` is implemented,
    this class will utilize `torch.nn.functional.mish` to do the calculation if meets the version.

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional dimensions
        - Output: :math:`(N, *)`, same shape as the input


    Examples::

        >>> import torch
        >>> from monai.networks.layers.factories import Act
        >>> m = Act['mish']()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """

    def __init__(self, inplace: bool=False):
        super().__init__()
        self.inplace = inplace

    def forward(self, input: torch.Tensor):
        return monai_mish(input, self.inplace)


class SimpleASPP(nn.Module):
    """
    A simplified version of the atrous spatial pyramid pooling (ASPP) module.

    Chen et al., Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.
    https://arxiv.org/abs/1802.02611

    Wang et al., A Noise-robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions
    from CT Images. https://ieeexplore.ieee.org/document/9109297
    """

    def __init__(self, spatial_dims: int, in_channels: int, conv_out_channels: int, kernel_sizes: Sequence[int]=(1, 3, 3, 3), dilations: Sequence[int]=(1, 2, 4, 6), norm_type: Optional[Union[Tuple, str]]='BATCH', acti_type: Optional[Union[Tuple, str]]='LEAKYRELU', bias: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions, could be 1, 2, or 3.
            in_channels: number of input channels.
            conv_out_channels: number of output channels of each atrous conv.
                The final number of output channels is conv_out_channels * len(kernel_sizes).
            kernel_sizes: a sequence of four convolutional kernel sizes.
                Defaults to (1, 3, 3, 3) for four (dilated) convolutions.
            dilations: a sequence of four convolutional dilation parameters.
                Defaults to (1, 2, 4, 6) for four (dilated) convolutions.
            norm_type: final kernel-size-one convolution normalization type.
                Defaults to batch norm.
            acti_type: final kernel-size-one convolution activation type.
                Defaults to leaky ReLU.
            bias: whether to have a bias term in convolution blocks. Defaults to False.
                According to `Performance Tuning Guide <https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html>`_,
                if a conv layer is directly followed by a batch norm layer, bias should be False.

        Raises:
            ValueError: When ``kernel_sizes`` length differs from ``dilations``.

        See also:

            :py:class:`monai.networks.layers.Act`
            :py:class:`monai.networks.layers.Conv`
            :py:class:`monai.networks.layers.Norm`

        """
        super().__init__()
        if len(kernel_sizes) != len(dilations):
            raise ValueError(f'kernel_sizes and dilations length must match, got kernel_sizes={len(kernel_sizes)} dilations={len(dilations)}.')
        pads = tuple(same_padding(k, d) for k, d in zip(kernel_sizes, dilations))
        self.convs = nn.ModuleList()
        for k, d, p in zip(kernel_sizes, dilations, pads):
            _conv = Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=conv_out_channels, kernel_size=k, dilation=d, padding=p)
            self.convs.append(_conv)
        out_channels = conv_out_channels * len(pads)
        self.conv_k1 = Convolution(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=1, act=acti_type, norm=norm_type, bias=bias)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: in shape (batch, channel, spatial_1[, spatial_2, ...]).
        """
        x_out = torch.cat([conv(x) for conv in self.convs], dim=1)
        x_out = self.conv_k1(x_out)
        return x_out


class ExtraFPNBlock(nn.Module):
    """
    Base class for the extra block in the FPN.

    Same code as https://github.com/pytorch/vision/blob/release/0.12/torchvision/ops/feature_pyramid_network.py
    """

    def forward(self, results: List[Tensor], x: List[Tensor], names: List[str]):
        """
        Compute extended set of results of the FPN and their names.

        Args:
            results: the result of the FPN
            x: the original feature maps
            names: the names for each one of the original feature maps

        Returns:
            - the extended set of results of the FPN
            - the extended set of names for the results
        """
        pass


class FeaturePyramidNetwork(nn.Module):
    """
    Module that adds a FPN from on top of a set of feature maps. This is based on
    `"Feature Pyramid Network for Object Detection" <https://arxiv.org/abs/1612.03144>`_.

    The feature maps are currently supposed to be in increasing depth
    order.

    The input to the model is expected to be an OrderedDict[Tensor], containing
    the feature maps on top of which the FPN will be added.

    Args:
        spatial_dims: 2D or 3D images
        in_channels_list: number of channels for each feature map that
            is passed to the module
        out_channels: number of channels of the FPN representation
        extra_blocks: if provided, extra operations will
            be performed. It is expected to take the fpn features, the original
            features and the names of the original features as input, and returns
            a new list of feature maps and their corresponding names

    Examples::

        >>> m = FeaturePyramidNetwork(2, [10, 20, 30], 5)
        >>> # get some dummy data
        >>> x = OrderedDict()
        >>> x['feat0'] = torch.rand(1, 10, 64, 64)
        >>> x['feat2'] = torch.rand(1, 20, 16, 16)
        >>> x['feat3'] = torch.rand(1, 30, 8, 8)
        >>> # compute the FPN on top of x
        >>> output = m(x)
        >>> print([(k, v.shape) for k, v in output.items()])
        >>> # returns
        >>>   [('feat0', torch.Size([1, 5, 64, 64])),
        >>>    ('feat2', torch.Size([1, 5, 16, 16])),
        >>>    ('feat3', torch.Size([1, 5, 8, 8]))]

    """

    def __init__(self, spatial_dims: int, in_channels_list: List[int], out_channels: int, extra_blocks: Optional[ExtraFPNBlock]=None):
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        self.inner_blocks = nn.ModuleList()
        self.layer_blocks = nn.ModuleList()
        for in_channels in in_channels_list:
            if in_channels == 0:
                raise ValueError('in_channels=0 is currently not supported')
            inner_block_module = conv_type(in_channels, out_channels, 1)
            layer_block_module = conv_type(out_channels, out_channels, 3, padding=1)
            self.inner_blocks.append(inner_block_module)
            self.layer_blocks.append(layer_block_module)
        conv_type_: Type[nn.Module] = Conv[Conv.CONV, spatial_dims]
        for m in self.modules():
            if isinstance(m, conv_type_):
                nn.init.kaiming_uniform_(m.weight, a=1)
                nn.init.constant_(m.bias, 0.0)
        if extra_blocks is not None:
            if not isinstance(extra_blocks, ExtraFPNBlock):
                raise AssertionError
        self.extra_blocks = extra_blocks

    def get_result_from_inner_blocks(self, x: Tensor, idx: int) ->Tensor:
        """
        This is equivalent to self.inner_blocks[idx](x),
        but torchscript doesn't support this yet
        """
        num_blocks = len(self.inner_blocks)
        if idx < 0:
            idx += num_blocks
        out = x
        for i, module in enumerate(self.inner_blocks):
            if i == idx:
                out = module(x)
        return out

    def get_result_from_layer_blocks(self, x: Tensor, idx: int) ->Tensor:
        """
        This is equivalent to self.layer_blocks[idx](x),
        but torchscript doesn't support this yet
        """
        num_blocks = len(self.layer_blocks)
        if idx < 0:
            idx += num_blocks
        out = x
        for i, module in enumerate(self.layer_blocks):
            if i == idx:
                out = module(x)
        return out

    def forward(self, x: Dict[str, Tensor]) ->Dict[str, Tensor]:
        """
        Computes the FPN for a set of feature maps.

        Args:
            x: feature maps for each feature level.

        Returns:
            feature maps after FPN layers. They are ordered from highest resolution first.
        """
        names = list(x.keys())
        x_values: List[Tensor] = list(x.values())
        last_inner = self.get_result_from_inner_blocks(x_values[-1], -1)
        results = []
        results.append(self.get_result_from_layer_blocks(last_inner, -1))
        for idx in range(len(x_values) - 2, -1, -1):
            inner_lateral = self.get_result_from_inner_blocks(x_values[idx], idx)
            feat_shape = inner_lateral.shape[2:]
            inner_top_down = F.interpolate(last_inner, size=feat_shape, mode='nearest')
            last_inner = inner_lateral + inner_top_down
            results.insert(0, self.get_result_from_layer_blocks(last_inner, idx))
        if self.extra_blocks is not None:
            results, names = self.extra_blocks(results, x_values, names)
        out = OrderedDict(list(zip(names, results)))
        return out


class LastLevelMaxPool(ExtraFPNBlock):
    """
    Applies a max_pool2d or max_pool3d on top of the last feature map. Serves as an ``extra_blocks``
    in :class:`~monai.networks.blocks.feature_pyramid_network.FeaturePyramidNetwork` .
    """

    def __init__(self, spatial_dims: int):
        super().__init__()
        pool_type: Type[Union[nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        self.maxpool = pool_type(kernel_size=1, stride=2, padding=0)

    def forward(self, results: List[Tensor], x: List[Tensor], names: List[str]) ->Tuple[List[Tensor], List[str]]:
        names.append('pool')
        results.append(self.maxpool(results[-1]))
        return results, names


torchvision_models, _ = optional_import('torchvision.models')


class BackboneWithFPN(nn.Module):
    """
    Adds an FPN on top of a model.
    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to
    extract a submodel that returns the feature maps specified in return_layers.
    The same limitations of IntermediateLayerGetter apply here.

    Same code as https://github.com/pytorch/vision/blob/release/0.12/torchvision/models/detection/backbone_utils.py
    Except that this class uses spatial_dims

    Args:
        backbone: backbone network
        return_layers: a dict containing the names
            of the modules for which the activations will be returned as
            the key of the dict, and the value of the dict is the name
            of the returned activation (which the user can specify).
        in_channels_list: number of channels for each feature map
            that is returned, in the order they are present in the OrderedDict
        out_channels: number of channels in the FPN.
        spatial_dims: 2D or 3D images
    """

    def __init__(self, backbone: nn.Module, return_layers: Dict[str, str], in_channels_list: List[int], out_channels: int, spatial_dims: Union[int, None]=None, extra_blocks: Optional[ExtraFPNBlock]=None) ->None:
        super().__init__()
        if spatial_dims is None:
            if hasattr(backbone, 'spatial_dims') and isinstance(backbone.spatial_dims, int):
                spatial_dims = backbone.spatial_dims
            elif isinstance(backbone.conv1, nn.Conv2d):
                spatial_dims = 2
            elif isinstance(backbone.conv1, nn.Conv3d):
                spatial_dims = 3
            else:
                raise ValueError('Could not find spatial_dims of backbone, please specify it.')
        if extra_blocks is None:
            extra_blocks = LastLevelMaxPool(spatial_dims)
        self.body = torchvision_models._utils.IntermediateLayerGetter(backbone, return_layers=return_layers)
        self.fpn = FeaturePyramidNetwork(spatial_dims=spatial_dims, in_channels_list=in_channels_list, out_channels=out_channels, extra_blocks=extra_blocks)
        self.out_channels = out_channels

    def forward(self, x: Tensor) ->Dict[str, Tensor]:
        """
        Computes the resulted feature maps of the network.

        Args:
            x: input images

        Returns:
            feature maps after FPN layers. They are ordered from highest resolution first.
        """
        x = self.body(x)
        y: Dict[str, Tensor] = self.fpn(x)
        return y


class ResidualUnit(nn.Module):
    """
    Residual module with multiple convolutions and a residual connection.

    For example:

    .. code-block:: python

        from monai.networks.blocks import ResidualUnit

        convs = ResidualUnit(
            spatial_dims=3,
            in_channels=1,
            out_channels=1,
            adn_ordering="AN",
            act=("prelu", {"init": 0.2}),
            norm=("layer", {"normalized_shape": (10, 10, 10)}),
        )
        print(convs)

    output::

        ResidualUnit(
          (conv): Sequential(
            (unit0): Convolution(
              (conv): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (adn): ADN(
                (A): PReLU(num_parameters=1)
                (N): LayerNorm((10, 10, 10), eps=1e-05, elementwise_affine=True)
              )
            )
            (unit1): Convolution(
              (conv): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
              (adn): ADN(
                (A): PReLU(num_parameters=1)
                (N): LayerNorm((10, 10, 10), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (residual): Identity()
        )

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        strides: convolution stride. Defaults to 1.
        kernel_size: convolution kernel size. Defaults to 3.
        subunits: number of convolutions. Defaults to 2.
        adn_ordering: a string representing the ordering of activation, normalization, and dropout.
            Defaults to "NDA".
        act: activation type and arguments. Defaults to PReLU.
        norm: feature normalization type and arguments. Defaults to instance norm.
        dropout: dropout ratio. Defaults to no dropout.
        dropout_dim: determine the dimensions of dropout. Defaults to 1.

            - When dropout_dim = 1, randomly zeroes some of the elements for each channel.
            - When dropout_dim = 2, Randomly zero out entire channels (a channel is a 2D feature map).
            - When dropout_dim = 3, Randomly zero out entire channels (a channel is a 3D feature map).

            The value of dropout_dim should be no larger than the value of `dimensions`.
        dilation: dilation rate. Defaults to 1.
        bias: whether to have a bias term. Defaults to True.
        last_conv_only: for the last subunit, whether to use the convolutional layer only.
            Defaults to False.
        padding: controls the amount of implicit zero-paddings on both sides for padding number of points
            for each dimension. Defaults to None.

    See also:

        :py:class:`monai.networks.blocks.Convolution`

    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, strides: Union[Sequence[int], int]=1, kernel_size: Union[Sequence[int], int]=3, subunits: int=2, adn_ordering: str='NDA', act: Optional[Union[Tuple, str]]='PRELU', norm: Optional[Union[Tuple, str]]='INSTANCE', dropout: Optional[Union[Tuple, str, float]]=None, dropout_dim: Optional[int]=1, dilation: Union[Sequence[int], int]=1, bias: bool=True, last_conv_only: bool=False, padding: Optional[Union[Sequence[int], int]]=None) ->None:
        super().__init__()
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.conv = nn.Sequential()
        self.residual = nn.Identity()
        if not padding:
            padding = same_padding(kernel_size, dilation)
        schannels = in_channels
        sstrides = strides
        subunits = max(1, subunits)
        for su in range(subunits):
            conv_only = last_conv_only and su == subunits - 1
            unit = Convolution(self.spatial_dims, schannels, out_channels, strides=sstrides, kernel_size=kernel_size, adn_ordering=adn_ordering, act=act, norm=norm, dropout=dropout, dropout_dim=dropout_dim, dilation=dilation, bias=bias, conv_only=conv_only, padding=padding)
            self.conv.add_module(f'unit{su:d}', unit)
            schannels = out_channels
            sstrides = 1
        if np.prod(strides) != 1 or in_channels != out_channels:
            rkernel_size = kernel_size
            rpadding = padding
            if np.prod(strides) == 1:
                rkernel_size = 1
                rpadding = 0
            conv_type = Conv[Conv.CONV, self.spatial_dims]
            self.residual = conv_type(in_channels, out_channels, rkernel_size, strides, rpadding, bias=bias)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        res: torch.Tensor = self.residual(x)
        cx: torch.Tensor = self.conv(x)
        return cx + res


_C, _ = optional_import('monai._C')


class PHLFilter(torch.autograd.Function):
    """
    Filters input based on arbitrary feature vectors. Uses a permutohedral
    lattice data structure to efficiently approximate n-dimensional gaussian
    filtering. Complexity is broadly independent of kernel size. Most applicable
    to higher filter dimensions and larger kernel sizes.

    See:
        https://graphics.stanford.edu/papers/permutohedral/

    Args:
        input: input tensor to be filtered.

        features: feature tensor used to filter the input.

        sigmas: the standard deviations of each feature in the filter.

    Returns:
        output (torch.Tensor): output tensor.
    """

    @staticmethod
    def forward(ctx, input, features, sigmas=None):
        scaled_features = features
        if sigmas is not None:
            for i in range(features.size(1)):
                scaled_features[:, i, ...] /= sigmas[i]
        ctx.save_for_backward(scaled_features)
        output_data = _C.phl_filter(input, scaled_features)
        return output_data

    @staticmethod
    def backward(ctx, grad_output):
        raise NotImplementedError('PHLFilter does not currently support Backpropagation')


def meshgrid_ij(*tensors):
    if torch.meshgrid.__kwdefaults__ is not None and 'indexing' in torch.meshgrid.__kwdefaults__:
        return torch.meshgrid(*tensors, indexing='ij')
    return torch.meshgrid(*tensors)


def _create_coordinate_tensor(tensor):
    axes = [torch.arange(tensor.size(i)) for i in range(2, tensor.dim())]
    grids = meshgrid_ij(axes)
    coords = torch.stack(grids)
    return torch.stack(tensor.size(0) * [coords], dim=0)


class CRF(torch.nn.Module):
    """
    Conditional Random Field: Combines message passing with a class
    compatibility convolution into an iterative process designed
    to successively minimise the energy of the class labeling.

    In this implementation, the message passing step is a weighted
    combination of a gaussian filter and a bilateral filter.
    The bilateral term is included to respect existing structure
    within the reference tensor.

    See:
        https://arxiv.org/abs/1502.03240
    """

    def __init__(self, iterations: int=5, bilateral_weight: float=1.0, gaussian_weight: float=1.0, bilateral_spatial_sigma: float=5.0, bilateral_color_sigma: float=0.5, gaussian_spatial_sigma: float=5.0, update_factor: float=3.0, compatibility_matrix: Optional[torch.Tensor]=None):
        """
        Args:
            iterations: the number of iterations.
            bilateral_weight: the weighting of the bilateral term in the message passing step.
            gaussian_weight: the weighting of the gaussian term in the message passing step.
            bilateral_spatial_sigma: standard deviation in spatial coordinates for the bilateral term.
            bilateral_color_sigma: standard deviation in color space for the bilateral term.
            gaussian_spatial_sigma: standard deviation in spatial coordinates for the gaussian term.
            update_factor: determines the magnitude of each update.
            compatibility_matrix: a matrix describing class compatibility,
                should be NxN where N is the number of classes.
        """
        super().__init__()
        self.iterations = iterations
        self.bilateral_weight = bilateral_weight
        self.gaussian_weight = gaussian_weight
        self.bilateral_spatial_sigma = bilateral_spatial_sigma
        self.bilateral_color_sigma = bilateral_color_sigma
        self.gaussian_spatial_sigma = gaussian_spatial_sigma
        self.update_factor = update_factor
        self.compatibility_matrix = compatibility_matrix

    def forward(self, input_tensor: torch.Tensor, reference_tensor: torch.Tensor):
        """
        Args:
            input_tensor: tensor containing initial class logits.
            reference_tensor: the reference tensor used to guide the message passing.

        Returns:
            output (torch.Tensor): output tensor.
        """
        spatial_features = _create_coordinate_tensor(reference_tensor)
        bilateral_features = torch.cat([spatial_features / self.bilateral_spatial_sigma, reference_tensor / self.bilateral_color_sigma], dim=1)
        gaussian_features = spatial_features / self.gaussian_spatial_sigma
        output_tensor = softmax(input_tensor, dim=1)
        for _ in range(self.iterations):
            bilateral_output = PHLFilter.apply(output_tensor, bilateral_features)
            gaussian_output = PHLFilter.apply(output_tensor, gaussian_features)
            combined_output = self.bilateral_weight * bilateral_output + self.gaussian_weight * gaussian_output
            if self.compatibility_matrix is not None:
                flat = combined_output.flatten(start_dim=2).permute(0, 2, 1)
                flat = torch.matmul(flat, self.compatibility_matrix)
                combined_output = flat.permute(0, 2, 1).reshape(combined_output.shape)
            output_tensor = softmax(input_tensor + self.update_factor * combined_output, dim=1)
        return output_tensor


class Pseudo3DLayer(nn.Module):

    def __init__(self, spatial_dims: int, num_input_features: int, growth_rate: int, bn_size: int, dropout_prob: float):
        super().__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        self.bn1 = norm_type(num_input_features)
        self.relu1 = relu_type(inplace=True)
        self.conv1 = conv_type(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)
        self.bn2 = norm_type(bn_size * growth_rate)
        self.relu2 = relu_type(inplace=True)
        self.conv2 = conv_type(bn_size * growth_rate, growth_rate, kernel_size=(3, 3, 1)[-spatial_dims:], stride=1, padding=(1, 1, 0)[-spatial_dims:], bias=False)
        self.bn3 = norm_type(growth_rate)
        self.relu3 = relu_type(inplace=True)
        self.conv3 = conv_type(growth_rate, growth_rate, kernel_size=(1, 1, 3)[-spatial_dims:], stride=1, padding=(0, 0, 1)[-spatial_dims:], bias=False)
        self.bn4 = norm_type(growth_rate)
        self.relu4 = relu_type(inplace=True)
        self.conv4 = conv_type(growth_rate, growth_rate, kernel_size=1, stride=1, bias=False)
        self.dropout_prob = dropout_prob

    def forward(self, x):
        inx = x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.conv1(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x3x3x1 = self.conv2(x)
        x = self.bn3(x3x3x1)
        x = self.relu3(x)
        x1x1x3 = self.conv3(x)
        x = x3x3x1 + x1x1x3
        x = self.bn4(x)
        x = self.relu4(x)
        new_features = self.conv4(x)
        self.dropout_prob = 0.0
        if self.dropout_prob > 0.0:
            new_features = F.dropout(new_features, p=self.dropout_prob, training=self.training)
        return torch.cat([inx, new_features], 1)


class DenseBlock(nn.Sequential):

    def __init__(self, spatial_dims: int, num_layers: int, num_input_features: int, bn_size: int, growth_rate: int, dropout_prob: float):
        super().__init__()
        for i in range(num_layers):
            layer = Pseudo3DLayer(spatial_dims, num_input_features + i * growth_rate, growth_rate, bn_size, dropout_prob)
            self.add_module('denselayer%d' % (i + 1), layer)


class FactorizedIncreaseBlock(torch.nn.Sequential):
    """
    Up-sampling the features by two using linear interpolation and convolutions.
    """

    def __init__(self, in_channel: int, out_channel: int, spatial_dims: int=3, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        """
        Args:
            in_channel: number of input channels
            out_channel: number of output channels
            spatial_dims: number of spatial dimensions
            act_name: activation layer type and arguments.
            norm_name: feature normalization type and arguments.
        """
        super().__init__()
        self._in_channel = in_channel
        self._out_channel = out_channel
        self._spatial_dims = spatial_dims
        if self._spatial_dims not in (2, 3):
            raise ValueError('spatial_dims must be 2 or 3.')
        conv_type = Conv[Conv.CONV, self._spatial_dims]
        mode = 'trilinear' if self._spatial_dims == 3 else 'bilinear'
        self.add_module('up', torch.nn.Upsample(scale_factor=2, mode=mode, align_corners=True))
        self.add_module('acti', get_act_layer(name=act_name))
        self.add_module('conv', conv_type(in_channels=self._in_channel, out_channels=self._out_channel, kernel_size=1, stride=1, padding=0, groups=1, bias=False, dilation=1))
        self.add_module('norm', get_norm_layer(name=norm_name, spatial_dims=self._spatial_dims, channels=self._out_channel))


class FactorizedReduceBlock(torch.nn.Module):
    """
    Down-sampling the feature by 2 using stride.
    The length along each spatial dimension must be a multiple of 2.
    """

    def __init__(self, in_channel: int, out_channel: int, spatial_dims: int=3, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        """
        Args:
            in_channel: number of input channels
            out_channel: number of output channels.
            spatial_dims: number of spatial dimensions.
            act_name: activation layer type and arguments.
            norm_name: feature normalization type and arguments.
        """
        super().__init__()
        self._in_channel = in_channel
        self._out_channel = out_channel
        self._spatial_dims = spatial_dims
        if self._spatial_dims not in (2, 3):
            raise ValueError('spatial_dims must be 2 or 3.')
        conv_type = Conv[Conv.CONV, self._spatial_dims]
        self.act = get_act_layer(name=act_name)
        self.conv_1 = conv_type(in_channels=self._in_channel, out_channels=self._out_channel // 2, kernel_size=1, stride=2, padding=0, groups=1, bias=False, dilation=1)
        self.conv_2 = conv_type(in_channels=self._in_channel, out_channels=self._out_channel - self._out_channel // 2, kernel_size=1, stride=2, padding=0, groups=1, bias=False, dilation=1)
        self.norm = get_norm_layer(name=norm_name, spatial_dims=self._spatial_dims, channels=self._out_channel)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        The length along each spatial dimension must be a multiple of 2.
        """
        x = self.act(x)
        if self._spatial_dims == 3:
            out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:, 1:])], dim=1)
        else:
            out = torch.cat([self.conv_1(x), self.conv_2(x[:, :, 1:, 1:])], dim=1)
        out = self.norm(out)
        return out


class P3DActiConvNormBlock(torch.nn.Sequential):
    """
    -- (act) -- (conv) -- (norm) --
    """

    def __init__(self, in_channel: int, out_channel: int, kernel_size: int, padding: int, mode: int=0, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        """
        Args:
            in_channel: number of input channels.
            out_channel: number of output channels.
            kernel_size: kernel size to be expanded to 3D.
            padding: padding size to be expanded to 3D.
            mode: mode for the anisotropic kernels:

                - 0: ``(k, k, 1)``, ``(1, 1, k)``,
                - 1: ``(k, 1, k)``, ``(1, k, 1)``,
                - 2: ``(1, k, k)``. ``(k, 1, 1)``.

            act_name: activation layer type and arguments.
            norm_name: feature normalization type and arguments.
        """
        super().__init__()
        self._in_channel = in_channel
        self._out_channel = out_channel
        self._p3dmode = int(mode)
        conv_type = Conv[Conv.CONV, 3]
        if self._p3dmode == 0:
            kernel_size0 = kernel_size, kernel_size, 1
            kernel_size1 = 1, 1, kernel_size
            padding0 = padding, padding, 0
            padding1 = 0, 0, padding
        elif self._p3dmode == 1:
            kernel_size0 = kernel_size, 1, kernel_size
            kernel_size1 = 1, kernel_size, 1
            padding0 = padding, 0, padding
            padding1 = 0, padding, 0
        elif self._p3dmode == 2:
            kernel_size0 = 1, kernel_size, kernel_size
            kernel_size1 = kernel_size, 1, 1
            padding0 = 0, padding, padding
            padding1 = padding, 0, 0
        else:
            raise ValueError('`mode` must be 0, 1, or 2.')
        self.add_module('acti', get_act_layer(name=act_name))
        self.add_module('conv', conv_type(in_channels=self._in_channel, out_channels=self._in_channel, kernel_size=kernel_size0, stride=1, padding=padding0, groups=1, bias=False, dilation=1))
        self.add_module('conv_1', conv_type(in_channels=self._in_channel, out_channels=self._out_channel, kernel_size=kernel_size1, stride=1, padding=padding1, groups=1, bias=False, dilation=1))
        self.add_module('norm', get_norm_layer(name=norm_name, spatial_dims=3, channels=self._out_channel))


class ActiConvNormBlock(torch.nn.Sequential):
    """
    -- (Acti) -- (Conv) -- (Norm) --
    """

    def __init__(self, in_channel: int, out_channel: int, kernel_size: int=3, padding: int=1, spatial_dims: int=3, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        """
        Args:
            in_channel: number of input channels.
            out_channel: number of output channels.
            kernel_size: kernel size of the convolution.
            padding: padding size of the convolution.
            spatial_dims: number of spatial dimensions.
            act_name: activation layer type and arguments.
            norm_name: feature normalization type and arguments.
        """
        super().__init__()
        self._in_channel = in_channel
        self._out_channel = out_channel
        self._spatial_dims = spatial_dims
        conv_type = Conv[Conv.CONV, self._spatial_dims]
        self.add_module('acti', get_act_layer(name=act_name))
        self.add_module('conv', conv_type(in_channels=self._in_channel, out_channels=self._out_channel, kernel_size=kernel_size, stride=1, padding=padding, groups=1, bias=False, dilation=1))
        self.add_module('norm', get_norm_layer(name=norm_name, spatial_dims=self._spatial_dims, channels=self._out_channel))


class MaxAvgPool(nn.Module):
    """
    Downsample with both maxpooling and avgpooling,
    double the channel size by concatenating the downsampled feature maps.
    """

    def __init__(self, spatial_dims: int, kernel_size: Union[Sequence[int], int], stride: Optional[Union[Sequence[int], int]]=None, padding: Union[Sequence[int], int]=0, ceil_mode: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions of the input image.
            kernel_size: the kernel size of both pooling operations.
            stride: the stride of the window. Default value is `kernel_size`.
            padding: implicit zero padding to be added to both pooling operations.
            ceil_mode: when True, will use ceil instead of floor to compute the output shape.
        """
        super().__init__()
        _params = {'kernel_size': ensure_tuple_rep(kernel_size, spatial_dims), 'stride': None if stride is None else ensure_tuple_rep(stride, spatial_dims), 'padding': ensure_tuple_rep(padding, spatial_dims), 'ceil_mode': ceil_mode}
        self.max_pool = Pool[Pool.MAX, spatial_dims](**_params)
        self.avg_pool = Pool[Pool.AVG, spatial_dims](**_params)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: Tensor in shape (batch, channel, spatial_1[, spatial_2, ...]).

        Returns:
            Tensor in shape (batch, 2*channel, spatial_1[, spatial_2, ...]).
        """
        return torch.cat([self.max_pool(x), self.avg_pool(x)], dim=1)


def get_conv_layer(spatial_dims: int, in_channels: int, out_channels: int, kernel_size: int=3, stride: int=1, bias: bool=False):
    return Convolution(spatial_dims, in_channels, out_channels, strides=stride, kernel_size=kernel_size, bias=bias, conv_only=True)


class UnetResBlock(nn.Module):
    """
    A skip-connection based module that can be used for DynUNet, based on:
    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.
    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        kernel_size: convolution kernel size.
        stride: convolution stride.
        norm_name: feature normalization type and arguments.
        act_name: activation layer type and arguments.
        dropout: dropout probability.

    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], norm_name: Union[Tuple, str], act_name: Union[Tuple, str]=('leakyrelu', {'inplace': True, 'negative_slope': 0.01}), dropout: Optional[Union[Tuple, str, float]]=None):
        super().__init__()
        self.conv1 = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=kernel_size, stride=stride, dropout=dropout, act=None, norm=None, conv_only=False)
        self.conv2 = get_conv_layer(spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, act=None, norm=None, conv_only=False)
        self.lrelu = get_act_layer(name=act_name)
        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.downsample = in_channels != out_channels
        stride_np = np.atleast_1d(stride)
        if not np.all(stride_np == 1):
            self.downsample = True
        if self.downsample:
            self.conv3 = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=1, stride=stride, dropout=dropout, act=None, norm=None, conv_only=False)
            self.norm3 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)

    def forward(self, inp):
        residual = inp
        out = self.conv1(inp)
        out = self.norm1(out)
        out = self.lrelu(out)
        out = self.conv2(out)
        out = self.norm2(out)
        if hasattr(self, 'conv3'):
            residual = self.conv3(residual)
        if hasattr(self, 'norm3'):
            residual = self.norm3(residual)
        out += residual
        out = self.lrelu(out)
        return out


class UnetBasicBlock(nn.Module):
    """
    A CNN module that can be used for DynUNet, based on:
    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.
    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        kernel_size: convolution kernel size.
        stride: convolution stride.
        norm_name: feature normalization type and arguments.
        act_name: activation layer type and arguments.
        dropout: dropout probability.

    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], norm_name: Union[Tuple, str], act_name: Union[Tuple, str]=('leakyrelu', {'inplace': True, 'negative_slope': 0.01}), dropout: Optional[Union[Tuple, str, float]]=None):
        super().__init__()
        self.conv1 = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=kernel_size, stride=stride, dropout=dropout, act=None, norm=None, conv_only=False)
        self.conv2 = get_conv_layer(spatial_dims, out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, act=None, norm=None, conv_only=False)
        self.lrelu = get_act_layer(name=act_name)
        self.norm1 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)
        self.norm2 = get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=out_channels)

    def forward(self, inp):
        out = self.conv1(inp)
        out = self.norm1(out)
        out = self.lrelu(out)
        out = self.conv2(out)
        out = self.norm2(out)
        out = self.lrelu(out)
        return out


class UnetUpBlock(nn.Module):
    """
    An upsampling module that can be used for DynUNet, based on:
    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.
    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        kernel_size: convolution kernel size.
        stride: convolution stride.
        upsample_kernel_size: convolution kernel size for transposed convolution layers.
        norm_name: feature normalization type and arguments.
        act_name: activation layer type and arguments.
        dropout: dropout probability.
        trans_bias: transposed convolution bias.

    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], upsample_kernel_size: Union[Sequence[int], int], norm_name: Union[Tuple, str], act_name: Union[Tuple, str]=('leakyrelu', {'inplace': True, 'negative_slope': 0.01}), dropout: Optional[Union[Tuple, str, float]]=None, trans_bias: bool=False):
        super().__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_stride, dropout=dropout, bias=trans_bias, act=None, norm=None, conv_only=False, is_transposed=True)
        self.conv_block = UnetBasicBlock(spatial_dims, out_channels + out_channels, out_channels, kernel_size=kernel_size, stride=1, dropout=dropout, norm_name=norm_name, act_name=act_name)

    def forward(self, inp, skip):
        out = self.transp_conv(inp)
        out = torch.cat((out, skip), dim=1)
        out = self.conv_block(out)
        return out


class UnetOutBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, dropout: Optional[Union[Tuple, str, float]]=None):
        super().__init__()
        self.conv = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=1, stride=1, dropout=dropout, bias=True, act=None, norm=None, conv_only=False)

    def forward(self, inp):
        return self.conv(inp)


class GCN(nn.Module):
    """
    The Global Convolutional Network module using large 1D
    Kx1 and 1xK kernels to represent 2D kernels.
    """

    def __init__(self, inplanes: int, planes: int, ks: int=7):
        """
        Args:
            inplanes: number of input channels.
            planes: number of output channels.
            ks: kernel size for one dimension. Defaults to 7.
        """
        super().__init__()
        conv2d_type: Type[nn.Conv2d] = Conv[Conv.CONV, 2]
        self.conv_l1 = conv2d_type(in_channels=inplanes, out_channels=planes, kernel_size=(ks, 1), padding=(ks // 2, 0))
        self.conv_l2 = conv2d_type(in_channels=planes, out_channels=planes, kernel_size=(1, ks), padding=(0, ks // 2))
        self.conv_r1 = conv2d_type(in_channels=inplanes, out_channels=planes, kernel_size=(1, ks), padding=(0, ks // 2))
        self.conv_r2 = conv2d_type(in_channels=planes, out_channels=planes, kernel_size=(ks, 1), padding=(ks // 2, 0))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: in shape (batch, inplanes, spatial_1, spatial_2).
        """
        x_l = self.conv_l1(x)
        x_l = self.conv_l2(x_l)
        x_r = self.conv_r1(x)
        x_r = self.conv_r2(x_r)
        x = x_l + x_r
        return x


class Refine(nn.Module):
    """
    Simple residual block to refine the details of the activation maps.
    """

    def __init__(self, planes: int):
        """
        Args:
            planes: number of input channels.
        """
        super().__init__()
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        conv2d_type: Type[nn.Conv2d] = Conv[Conv.CONV, 2]
        norm2d_type: Type[nn.BatchNorm2d] = Norm[Norm.BATCH, 2]
        self.bn = norm2d_type(num_features=planes)
        self.relu = relu_type(inplace=True)
        self.conv1 = conv2d_type(in_channels=planes, out_channels=planes, kernel_size=3, padding=1)
        self.conv2 = conv2d_type(in_channels=planes, out_channels=planes, kernel_size=3, padding=1)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: in shape (batch, planes, spatial_1, spatial_2).
        """
        residual = x
        x = self.bn(x)
        x = self.relu(x)
        x = self.conv1(x)
        x = self.bn(x)
        x = self.relu(x)
        x = self.conv2(x)
        return residual + x


models, _ = optional_import('torchvision.models')


class FCN(nn.Module):
    """
    2D FCN network with 3 input channels. The small decoder is built
    with the GCN and Refine modules.
    The code is adapted from `lsqshr's official 2D code <https://github.com/lsqshr/AH-Net/blob/master/net2d.py>`_.

    Args:
        out_channels: number of output channels. Defaults to 1.
        upsample_mode: [``"transpose"``, ``"bilinear"``]
            The mode of upsampling manipulations.
            Using the second mode cannot guarantee the model's reproducibility. Defaults to ``bilinear``.

            - ``transpose``, uses transposed convolution layers.
            - ``bilinear``, uses bilinear interpolation.

        pretrained: If True, returns a model pre-trained on ImageNet
        progress: If True, displays a progress bar of the download to stderr.
    """

    def __init__(self, out_channels: int=1, upsample_mode: str='bilinear', pretrained: bool=True, progress: bool=True):
        super().__init__()
        conv2d_type: Type[nn.Conv2d] = Conv[Conv.CONV, 2]
        self.upsample_mode = upsample_mode
        self.conv2d_type = conv2d_type
        self.out_channels = out_channels
        resnet = models.resnet50(pretrained=pretrained, progress=progress)
        self.conv1 = resnet.conv1
        self.bn0 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool
        self.layer1 = resnet.layer1
        self.layer2 = resnet.layer2
        self.layer3 = resnet.layer3
        self.layer4 = resnet.layer4
        self.gcn1 = GCN(2048, self.out_channels)
        self.gcn2 = GCN(1024, self.out_channels)
        self.gcn3 = GCN(512, self.out_channels)
        self.gcn4 = GCN(64, self.out_channels)
        self.gcn5 = GCN(64, self.out_channels)
        self.refine1 = Refine(self.out_channels)
        self.refine2 = Refine(self.out_channels)
        self.refine3 = Refine(self.out_channels)
        self.refine4 = Refine(self.out_channels)
        self.refine5 = Refine(self.out_channels)
        self.refine6 = Refine(self.out_channels)
        self.refine7 = Refine(self.out_channels)
        self.refine8 = Refine(self.out_channels)
        self.refine9 = Refine(self.out_channels)
        self.refine10 = Refine(self.out_channels)
        self.transformer = self.conv2d_type(in_channels=256, out_channels=64, kernel_size=1)
        if self.upsample_mode == 'transpose':
            self.up_conv = UpSample(spatial_dims=2, in_channels=self.out_channels, scale_factor=2, mode='deconv')

    def forward(self, x: torch.Tensor):
        """
        Args:
            x: in shape (batch, 3, spatial_1, spatial_2).
        """
        org_input = x
        x = self.conv1(x)
        x = self.bn0(x)
        x = self.relu(x)
        conv_x = x
        x = self.maxpool(x)
        pool_x = x
        fm1 = self.layer1(x)
        fm2 = self.layer2(fm1)
        fm3 = self.layer3(fm2)
        fm4 = self.layer4(fm3)
        gcfm1 = self.refine1(self.gcn1(fm4))
        gcfm2 = self.refine2(self.gcn2(fm3))
        gcfm3 = self.refine3(self.gcn3(fm2))
        gcfm4 = self.refine4(self.gcn4(pool_x))
        gcfm5 = self.refine5(self.gcn5(conv_x))
        if self.upsample_mode == 'transpose':
            fs1 = self.refine6(self.up_conv(gcfm1) + gcfm2)
            fs2 = self.refine7(self.up_conv(fs1) + gcfm3)
            fs3 = self.refine8(self.up_conv(fs2) + gcfm4)
            fs4 = self.refine9(self.up_conv(fs3) + gcfm5)
            return self.refine10(self.up_conv(fs4))
        fs1 = self.refine6(F.interpolate(gcfm1, fm3.size()[2:], mode=self.upsample_mode, align_corners=True) + gcfm2)
        fs2 = self.refine7(F.interpolate(fs1, fm2.size()[2:], mode=self.upsample_mode, align_corners=True) + gcfm3)
        fs3 = self.refine8(F.interpolate(fs2, pool_x.size()[2:], mode=self.upsample_mode, align_corners=True) + gcfm4)
        fs4 = self.refine9(F.interpolate(fs3, conv_x.size()[2:], mode=self.upsample_mode, align_corners=True) + gcfm5)
        return self.refine10(F.interpolate(fs4, org_input.size()[2:], mode=self.upsample_mode, align_corners=True))


class MCFCN(FCN):
    """
    The multi-channel version of the 2D FCN module.
    Adds a projection layer to take arbitrary number of inputs.

    Args:
        in_channels: number of input channels. Defaults to 3.
        out_channels: number of output channels. Defaults to 1.
        upsample_mode: [``"transpose"``, ``"bilinear"``]
            The mode of upsampling manipulations.
            Using the second mode cannot guarantee the model's reproducibility. Defaults to ``bilinear``.

            - ``transpose``, uses transposed convolution layers.
            - ``bilinear``, uses bilinear interpolate.
        pretrained: If True, returns a model pre-trained on ImageNet
        progress: If True, displays a progress bar of the download to stderr.
    """

    def __init__(self, in_channels: int=3, out_channels: int=1, upsample_mode: str='bilinear', pretrained: bool=True, progress: bool=True):
        super().__init__(out_channels=out_channels, upsample_mode=upsample_mode, pretrained=pretrained, progress=progress)
        self.init_proj = Convolution(spatial_dims=2, in_channels=in_channels, out_channels=3, kernel_size=1, act=('relu', {'inplace': True}), norm=Norm.BATCH, bias=False)

    def forward(self, x: torch.Tensor):
        """
        Args:
            x: in shape (batch, in_channels, spatial_1, spatial_2).
        """
        x = self.init_proj(x)
        return super().forward(x)


class LastLevelP6P7(ExtraFPNBlock):
    """
    This module is used in RetinaNet to generate extra layers, P6 and P7.
    Serves as an ``extra_blocks``
    in :class:`~monai.networks.blocks.feature_pyramid_network.FeaturePyramidNetwork` .
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int):
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        self.p6 = conv_type(in_channels, out_channels, kernel_size=3, stride=2, padding=1)
        self.p7 = conv_type(out_channels, out_channels, kernel_size=3, stride=2, padding=1)
        for module in [self.p6, self.p7]:
            nn.init.kaiming_uniform_(module.weight, a=1)
            nn.init.constant_(module.bias, 0)
        self.use_P5 = in_channels == out_channels

    def forward(self, results: List[Tensor], x: List[Tensor], names: List[str]) ->Tuple[List[Tensor], List[str]]:
        p5, c5 = results[-1], x[-1]
        x5 = p5 if self.use_P5 else c5
        p6 = self.p6(x5)
        p7 = self.p7(F.relu(p6))
        results.extend([p6, p7])
        names.extend(['p6', 'p7'])
        return results, names


def get_conv_block(spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int]=3, strides: int=1, padding: Optional[Union[Tuple[int, ...], int]]=None, act: Optional[Union[Tuple, str]]='RELU', norm: Optional[Union[Tuple, str]]='BATCH', initializer: Optional[str]='kaiming_uniform') ->nn.Module:
    if padding is None:
        padding = same_padding(kernel_size)
    conv_block: nn.Module = Convolution(spatial_dims, in_channels, out_channels, kernel_size=kernel_size, strides=strides, act=act, norm=norm, bias=False, conv_only=False, padding=padding)
    conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
    for m in conv_block.modules():
        if isinstance(m, conv_type):
            if initializer == 'kaiming_uniform':
                nn.init.kaiming_normal_(torch.as_tensor(m.weight))
            elif initializer == 'zeros':
                nn.init.zeros_(torch.as_tensor(m.weight))
            else:
                raise ValueError(f'initializer {initializer} is not supported, currently supporting kaiming_uniform and zeros')
    return conv_block


class ResidualBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int]) ->None:
        super().__init__()
        if in_channels != out_channels:
            raise ValueError(f'expecting in_channels == out_channels, got in_channels={in_channels}, out_channels={out_channels}')
        self.conv_block = get_conv_block(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)
        self.conv = get_conv_layer(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)
        self.norm = Norm[Norm.BATCH, spatial_dims](out_channels)
        self.relu = nn.ReLU()

    def forward(self, x) ->torch.Tensor:
        out: torch.Tensor = self.relu(self.norm(self.conv(self.conv_block(x))) + x)
        return out


class LocalNetResidualBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int) ->None:
        super().__init__()
        if in_channels != out_channels:
            raise ValueError(f'expecting in_channels == out_channels, got in_channels={in_channels}, out_channels={out_channels}')
        self.conv_layer = get_conv_layer(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels)
        self.norm = Norm[Norm.BATCH, spatial_dims](out_channels)
        self.relu = nn.ReLU()

    def forward(self, x, mid) ->torch.Tensor:
        out: torch.Tensor = self.relu(self.norm(self.conv_layer(x)) + mid)
        return out


class LocalNetDownSampleBlock(nn.Module):
    """
    A down-sample module that can be used for LocalNet, based on:
    `Weakly-supervised convolutional neural networks for multimodal image registration
    <https://doi.org/10.1016/j.media.2018.07.002>`_.
    `Label-driven weakly-supervised learning for multimodal deformable image registration
    <https://arxiv.org/abs/1711.01666>`_.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int]) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernel_size: convolution kernel size.
        Raises:
            NotImplementedError: when ``kernel_size`` is even
        """
        super().__init__()
        self.conv_block = get_conv_block(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)
        self.residual_block = ResidualBlock(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)
        self.max_pool = Pool[Pool.MAX, spatial_dims](kernel_size=2)

    def forward(self, x) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Halves the spatial dimensions.
        A tuple of (x, mid) is returned:

            -  x is the downsample result, in shape (batch, ``out_channels``, insize_1 / 2, insize_2 / 2, [insize_3 / 2]),
            -  mid is the mid-level feature, in shape (batch, ``out_channels``, insize_1, insize_2, [insize_3])

        Args:
            x: Tensor in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])

        Raises:
            ValueError: when input spatial dimensions are not even.
        """
        for i in x.shape[2:]:
            if i % 2 != 0:
                raise ValueError(f'expecting x spatial dimensions be even, got x of shape {x.shape}')
        x = self.conv_block(x)
        mid = self.residual_block(x)
        x = self.max_pool(mid)
        return x, mid


def get_deconv_block(spatial_dims: int, in_channels: int, out_channels: int) ->nn.Module:
    mod: nn.Module = Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, strides=2, act='RELU', norm='BATCH', bias=False, is_transposed=True, padding=1, output_padding=1)
    return mod


class LocalNetUpSampleBlock(nn.Module):
    """
    A up-sample module that can be used for LocalNet, based on:
    `Weakly-supervised convolutional neural networks for multimodal image registration
    <https://doi.org/10.1016/j.media.2018.07.002>`_.
    `Label-driven weakly-supervised learning for multimodal deformable image registration
    <https://arxiv.org/abs/1711.01666>`_.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
        Raises:
            ValueError: when ``in_channels != 2 * out_channels``
        """
        super().__init__()
        self.deconv_block = get_deconv_block(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels)
        self.conv_block = get_conv_block(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels)
        self.residual_block = LocalNetResidualBlock(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels)
        if in_channels / out_channels != 2:
            raise ValueError(f'expecting in_channels == 2 * out_channels, got in_channels={in_channels}, out_channels={out_channels}')
        self.out_channels = out_channels

    def addictive_upsampling(self, x, mid) ->torch.Tensor:
        x = F.interpolate(x, mid.shape[2:])
        x = x.split(split_size=int(self.out_channels), dim=1)
        out: torch.Tensor = torch.sum(torch.stack(x, dim=-1), dim=-1)
        return out

    def forward(self, x, mid) ->torch.Tensor:
        """
        Halves the channel and doubles the spatial dimensions.

        Args:
            x: feature to be up-sampled, in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])
            mid: mid-level feature saved during down-sampling,
                in shape (batch, ``out_channels``, midsize_1, midsize_2, [midsize_3])

        Raises:
            ValueError: when ``midsize != insize * 2``
        """
        for i, j in zip(x.shape[2:], mid.shape[2:]):
            if j != 2 * i:
                raise ValueError(f'expecting mid spatial dimensions be exactly the double of x spatial dimensions, got x of shape {x.shape}, mid of shape {mid.shape}')
        h0 = self.deconv_block(x) + self.addictive_upsampling(x, mid)
        r1 = h0 + mid
        r2 = self.conv_block(h0)
        out: torch.Tensor = self.residual_block(r2, r1)
        return out


class LocalNetFeatureExtractorBlock(nn.Module):
    """
    A feature-extraction module that can be used for LocalNet, based on:
    `Weakly-supervised convolutional neural networks for multimodal image registration
    <https://doi.org/10.1016/j.media.2018.07.002>`_.
    `Label-driven weakly-supervised learning for multimodal deformable image registration
    <https://arxiv.org/abs/1711.01666>`_.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, act: Optional[Union[Tuple, str]]='RELU', initializer: str='kaiming_uniform') ->None:
        """
        Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        act: activation type and arguments. Defaults to ReLU.
        kernel_initializer: kernel initializer. Defaults to None.
        """
        super().__init__()
        self.conv_block = get_conv_block(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, act=act, norm=None)
        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
        for m in self.conv_block.modules():
            if isinstance(m, conv_type):
                if initializer == 'kaiming_uniform':
                    nn.init.kaiming_normal_(torch.as_tensor(m.weight))
                elif initializer == 'zeros':
                    nn.init.zeros_(torch.as_tensor(m.weight))
                else:
                    raise ValueError(f'initializer {initializer} is not supported, currently supporting kaiming_uniform and zeros')

    def forward(self, x) ->torch.Tensor:
        """
        Args:
            x: Tensor in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])
        """
        out: torch.Tensor = self.conv_block(x)
        return out


SUPPORTED_DROPOUT_MODE = {'vit', 'swin'}


class MLPBlock(nn.Module):
    """
    A multi-layer perceptron block, based on: "Dosovitskiy et al.,
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>"
    """

    def __init__(self, hidden_size: int, mlp_dim: int, dropout_rate: float=0.0, act: Union[Tuple, str]='GELU', dropout_mode='vit') ->None:
        """
        Args:
            hidden_size: dimension of hidden layer.
            mlp_dim: dimension of feedforward layer. If 0, `hidden_size` will be used.
            dropout_rate: faction of the input units to drop.
            act: activation type and arguments. Defaults to GELU.
            dropout_mode: dropout mode, can be "vit" or "swin".
                "vit" mode uses two dropout instances as implemented in
                https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py#L87
                "swin" corresponds to one instance as implemented in
                https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_mlp.py#L23


        """
        super().__init__()
        if not 0 <= dropout_rate <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        mlp_dim = mlp_dim or hidden_size
        self.linear1 = nn.Linear(hidden_size, mlp_dim)
        self.linear2 = nn.Linear(mlp_dim, hidden_size)
        self.fn = get_act_layer(act)
        self.drop1 = nn.Dropout(dropout_rate)
        dropout_opt = look_up_option(dropout_mode, SUPPORTED_DROPOUT_MODE)
        if dropout_opt == 'vit':
            self.drop2 = nn.Dropout(dropout_rate)
        elif dropout_opt == 'swin':
            self.drop2 = self.drop1
        else:
            raise ValueError(f'dropout_mode should be one of {SUPPORTED_DROPOUT_MODE}')

    def forward(self, x):
        x = self.fn(self.linear1(x))
        x = self.drop1(x)
        x = self.linear2(x)
        x = self.drop2(x)
        return x


Rearrange, _ = optional_import('einops.layers.torch', name='Rearrange')


SUPPORTED_EMBEDDING_TYPES = {'conv', 'perceptron'}


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    """Tensor initialization with truncated normal distribution.
    Based on:
    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    https://github.com/rwightman/pytorch-image-models

    Args:
       tensor: an n-dimensional `torch.Tensor`.
       mean: the mean of the normal distribution.
       std: the standard deviation of the normal distribution.
       a: the minimum cutoff value.
       b: the maximum cutoff value.
    """

    def norm_cdf(x):
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0
    with torch.no_grad():
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)
        tensor.uniform_(2 * l - 1, 2 * u - 1)
        tensor.erfinv_()
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    """Tensor initialization with truncated normal distribution.
    Based on:
    https://github.com/rwightman/pytorch-image-models

    Args:
       tensor: an n-dimensional `torch.Tensor`
       mean: the mean of the normal distribution
       std: the standard deviation of the normal distribution
       a: the minimum cutoff value
       b: the maximum cutoff value
    """
    if std <= 0:
        raise ValueError('the standard deviation should be greater than zero.')
    if a >= b:
        raise ValueError('minimum cutoff value (a) should be smaller than maximum cutoff value (b).')
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class PatchEmbeddingBlock(nn.Module):
    """
    A patch embedding block, based on: "Dosovitskiy et al.,
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>"

    Example::

        >>> from monai.networks.blocks import PatchEmbeddingBlock
        >>> PatchEmbeddingBlock(in_channels=4, img_size=32, patch_size=8, hidden_size=32, num_heads=4, pos_embed="conv")

    """

    def __init__(self, in_channels: int, img_size: Union[Sequence[int], int], patch_size: Union[Sequence[int], int], hidden_size: int, num_heads: int, pos_embed: str, dropout_rate: float=0.0, spatial_dims: int=3) ->None:
        """
        Args:
            in_channels: dimension of input channels.
            img_size: dimension of input image.
            patch_size: dimension of patch size.
            hidden_size: dimension of hidden layer.
            num_heads: number of attention heads.
            pos_embed: position embedding layer type.
            dropout_rate: faction of the input units to drop.
            spatial_dims: number of spatial dimensions.


        """
        super().__init__()
        if not 0 <= dropout_rate <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        if hidden_size % num_heads != 0:
            raise ValueError('hidden size should be divisible by num_heads.')
        self.pos_embed = look_up_option(pos_embed, SUPPORTED_EMBEDDING_TYPES)
        img_size = ensure_tuple_rep(img_size, spatial_dims)
        patch_size = ensure_tuple_rep(patch_size, spatial_dims)
        for m, p in zip(img_size, patch_size):
            if m < p:
                raise ValueError('patch_size should be smaller than img_size.')
            if self.pos_embed == 'perceptron' and m % p != 0:
                raise ValueError('patch_size should be divisible by img_size for perceptron.')
        self.n_patches = np.prod([(im_d // p_d) for im_d, p_d in zip(img_size, patch_size)])
        self.patch_dim = int(in_channels * np.prod(patch_size))
        self.patch_embeddings: nn.Module
        if self.pos_embed == 'conv':
            self.patch_embeddings = Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=hidden_size, kernel_size=patch_size, stride=patch_size)
        elif self.pos_embed == 'perceptron':
            chars = (('h', 'p1'), ('w', 'p2'), ('d', 'p3'))[:spatial_dims]
            from_chars = 'b c ' + ' '.join(f'({k} {v})' for k, v in chars)
            to_chars = f"b ({' '.join([c[0] for c in chars])}) ({' '.join([c[1] for c in chars])} c)"
            axes_len = {f'p{i + 1}': p for i, p in enumerate(patch_size)}
            self.patch_embeddings = nn.Sequential(Rearrange(f'{from_chars} -> {to_chars}', **axes_len), nn.Linear(self.patch_dim, hidden_size))
        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches, hidden_size))
        self.dropout = nn.Dropout(dropout_rate)
        trunc_normal_(self.position_embeddings, mean=0.0, std=0.02, a=-2.0, b=2.0)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, mean=0.0, std=0.02, a=-2.0, b=2.0)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        x = self.patch_embeddings(x)
        if self.pos_embed == 'conv':
            x = x.flatten(2).transpose(-1, -2)
        embeddings = x + self.position_embeddings
        embeddings = self.dropout(embeddings)
        return embeddings


class PatchEmbed(nn.Module):
    """
    Patch embedding block based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer

    Unlike ViT patch embedding block: (1) input is padded to satisfy window size requirements (2) normalized if
    specified (3) position embedding is not used.

    Example::

        >>> from monai.networks.blocks import PatchEmbed
        >>> PatchEmbed(patch_size=2, in_chans=1, embed_dim=48, norm_layer=nn.LayerNorm, spatial_dims=3)
    """

    def __init__(self, patch_size: Union[Sequence[int], int]=2, in_chans: int=1, embed_dim: int=48, norm_layer: Type[LayerNorm]=nn.LayerNorm, spatial_dims: int=3) ->None:
        """
        Args:
            patch_size: dimension of patch size.
            in_chans: dimension of input channels.
            embed_dim: number of linear projection output channels.
            norm_layer: normalization layer.
            spatial_dims: spatial dimension.
        """
        super().__init__()
        if spatial_dims not in (2, 3):
            raise ValueError('spatial dimension should be 2 or 3.')
        patch_size = ensure_tuple_rep(patch_size, spatial_dims)
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.proj = Conv[Conv.CONV, spatial_dims](in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        x_shape = x.size()
        if len(x_shape) == 5:
            _, _, d, h, w = x_shape
            if w % self.patch_size[2] != 0:
                x = F.pad(x, (0, self.patch_size[2] - w % self.patch_size[2]))
            if h % self.patch_size[1] != 0:
                x = F.pad(x, (0, 0, 0, self.patch_size[1] - h % self.patch_size[1]))
            if d % self.patch_size[0] != 0:
                x = F.pad(x, (0, 0, 0, 0, 0, self.patch_size[0] - d % self.patch_size[0]))
        elif len(x_shape) == 4:
            _, _, h, w = x_shape
            if w % self.patch_size[1] != 0:
                x = F.pad(x, (0, self.patch_size[1] - w % self.patch_size[1]))
            if h % self.patch_size[0] != 0:
                x = F.pad(x, (0, 0, 0, self.patch_size[0] - h % self.patch_size[0]))
        x = self.proj(x)
        if self.norm is not None:
            x_shape = x.size()
            x = x.flatten(2).transpose(1, 2)
            x = self.norm(x)
            if len(x_shape) == 5:
                d, wh, ww = x_shape[2], x_shape[3], x_shape[4]
                x = x.transpose(1, 2).view(-1, self.embed_dim, d, wh, ww)
            elif len(x_shape) == 4:
                wh, ww = x_shape[2], x_shape[3]
                x = x.transpose(1, 2).view(-1, self.embed_dim, wh, ww)
        return x


class RegistrationResidualConvBlock(nn.Module):
    """
    A block with skip links and layer - norm - activation.
    Only changes the number of channels, the spatial size is kept same.
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, num_layers: int=2, kernel_size: int=3):
        """

        Args:
            spatial_dims: number of spatial dimensions
            in_channels: number of input channels
            out_channels: number of output channels
            num_layers: number of layers inside the block
            kernel_size: kernel_size
        """
        super().__init__()
        self.num_layers = num_layers
        self.layers = nn.ModuleList([get_conv_layer(spatial_dims=spatial_dims, in_channels=in_channels if i == 0 else out_channels, out_channels=out_channels, kernel_size=kernel_size) for i in range(num_layers)])
        self.norms = nn.ModuleList([Norm[Norm.BATCH, spatial_dims](out_channels) for _ in range(num_layers)])
        self.acts = nn.ModuleList([nn.ReLU() for _ in range(num_layers)])

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """

        Args:
            x: Tensor in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])

        Returns:
            Tensor in shape (batch, ``out_channels``, insize_1, insize_2, [insize_3]),
            with the same spatial size as ``x``
        """
        skip = x
        for i, (conv, norm, act) in enumerate(zip(self.layers, self.norms, self.acts)):
            x = conv(x)
            x = norm(x)
            if i == self.num_layers - 1:
                x = x + skip
            x = act(x)
        return x


class RegistrationDownSampleBlock(nn.Module):
    """
    A down-sample module used in RegUNet to half the spatial size.
    The number of channels is kept same.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int, channels: int, pooling: bool) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            channels: channels
            pooling: use MaxPool if True, strided conv if False
        """
        super().__init__()
        if pooling:
            self.layer = Pool[Pool.MAX, spatial_dims](kernel_size=2)
        else:
            self.layer = get_conv_block(spatial_dims=spatial_dims, in_channels=channels, out_channels=channels, kernel_size=2, strides=2, padding=0)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Halves the spatial dimensions and keeps the same channel.
        output in shape (batch, ``channels``, insize_1 / 2, insize_2 / 2, [insize_3 / 2]),

        Args:
            x: Tensor in shape (batch, ``channels``, insize_1, insize_2, [insize_3])

        Raises:
            ValueError: when input spatial dimensions are not even.
        """
        for i in x.shape[2:]:
            if i % 2 != 0:
                raise ValueError(f'expecting x spatial dimensions be even, got x of shape {x.shape}')
        out: torch.Tensor = self.layer(x)
        return out


class RegistrationExtractionBlock(nn.Module):
    """
    The Extraction Block used in RegUNet.
    Extracts feature from each ``extract_levels`` and takes the average.
    """

    def __init__(self, spatial_dims: int, extract_levels: Tuple[int], num_channels: Union[Tuple[int], List[int]], out_channels: int, kernel_initializer: Optional[str]='kaiming_uniform', activation: Optional[str]=None):
        """

        Args:
            spatial_dims: number of spatial dimensions
            extract_levels: spatial levels to extract feature from, 0 refers to the input scale
            num_channels: number of channels at each scale level,
                List or Tuple of length equals to `depth` of the RegNet
            out_channels: number of output channels
            kernel_initializer: kernel initializer
            activation: kernel activation function
        """
        super().__init__()
        self.extract_levels = extract_levels
        self.max_level = max(extract_levels)
        self.layers = nn.ModuleList([get_conv_block(spatial_dims=spatial_dims, in_channels=num_channels[d], out_channels=out_channels, norm=None, act=activation, initializer=kernel_initializer) for d in extract_levels])

    def forward(self, x: List[torch.Tensor], image_size: List[int]) ->torch.Tensor:
        """

        Args:
            x: Decoded feature at different spatial levels, sorted from deep to shallow
            image_size: output image size

        Returns:
            Tensor of shape (batch, `out_channels`, size1, size2, size3), where (size1, size2, size3) = ``image_size``
        """
        feature_list = [F.interpolate(layer(x[self.max_level - level]), size=image_size) for layer, level in zip(self.layers, self.extract_levels)]
        out: torch.Tensor = torch.mean(torch.stack(feature_list, dim=0), dim=0)
        return out


class ResBlock(nn.Module):
    """
    ResBlock employs skip connection and two convolution blocks and is used
    in SegResNet based on `3D MRI brain tumor segmentation using autoencoder regularization
    <https://arxiv.org/pdf/1810.11654.pdf>`_.
    """

    def __init__(self, spatial_dims: int, in_channels: int, norm: Union[Tuple, str], kernel_size: int=3, act: Union[Tuple, str]=('RELU', {'inplace': True})) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions, could be 1, 2 or 3.
            in_channels: number of input channels.
            norm: feature normalization type and arguments.
            kernel_size: convolution kernel size, the value should be an odd number. Defaults to 3.
            act: activation type and arguments. Defaults to ``RELU``.
        """
        super().__init__()
        if kernel_size % 2 != 1:
            raise AssertionError('kernel_size should be an odd number.')
        self.norm1 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=in_channels)
        self.norm2 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=in_channels)
        self.act = get_act_layer(act)
        self.conv1 = get_conv_layer(spatial_dims, in_channels=in_channels, out_channels=in_channels)
        self.conv2 = get_conv_layer(spatial_dims, in_channels=in_channels, out_channels=in_channels)

    def forward(self, x):
        identity = x
        x = self.norm1(x)
        x = self.act(x)
        x = self.conv1(x)
        x = self.norm2(x)
        x = self.act(x)
        x = self.conv2(x)
        x += identity
        return x


class SABlock(nn.Module):
    """
    A self-attention block, based on: "Dosovitskiy et al.,
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>"
    """

    def __init__(self, hidden_size: int, num_heads: int, dropout_rate: float=0.0, qkv_bias: bool=False) ->None:
        """
        Args:
            hidden_size: dimension of hidden layer.
            num_heads: number of attention heads.
            dropout_rate: faction of the input units to drop.
            qkv_bias: bias term for the qkv linear layer.

        """
        super().__init__()
        if not 0 <= dropout_rate <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        if hidden_size % num_heads != 0:
            raise ValueError('hidden size should be divisible by num_heads.')
        self.num_heads = num_heads
        self.out_proj = nn.Linear(hidden_size, hidden_size)
        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=qkv_bias)
        self.input_rearrange = Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=num_heads)
        self.out_rearrange = Rearrange('b h l d -> b l (h d)')
        self.drop_output = nn.Dropout(dropout_rate)
        self.drop_weights = nn.Dropout(dropout_rate)
        self.head_dim = hidden_size // num_heads
        self.scale = self.head_dim ** -0.5

    def forward(self, x):
        output = self.input_rearrange(self.qkv(x))
        q, k, v = output[0], output[1], output[2]
        att_mat = (torch.einsum('blxd,blyd->blxy', q, k) * self.scale).softmax(dim=-1)
        att_mat = self.drop_weights(att_mat)
        x = torch.einsum('bhxy,bhyd->bhxd', att_mat, v)
        x = self.out_rearrange(x)
        x = self.out_proj(x)
        x = self.drop_output(x)
        return x


class ChannelSELayer(nn.Module):
    """
    Re-implementation of the Squeeze-and-Excitation block based on:
    "Hu et al., Squeeze-and-Excitation Networks, https://arxiv.org/abs/1709.01507".
    """

    def __init__(self, spatial_dims: int, in_channels: int, r: int=2, acti_type_1: Union[Tuple[str, Dict], str]=('relu', {'inplace': True}), acti_type_2: Union[Tuple[str, Dict], str]='sigmoid', add_residual: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions, could be 1, 2, or 3.
            in_channels: number of input channels.
            r: the reduction ratio r in the paper. Defaults to 2.
            acti_type_1: activation type of the hidden squeeze layer. Defaults to ``("relu", {"inplace": True})``.
            acti_type_2: activation type of the output squeeze layer. Defaults to "sigmoid".

        Raises:
            ValueError: When ``r`` is nonpositive or larger than ``in_channels``.

        See also:

            :py:class:`monai.networks.layers.Act`

        """
        super().__init__()
        self.add_residual = add_residual
        pool_type = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        self.avg_pool = pool_type(1)
        channels = int(in_channels // r)
        if channels <= 0:
            raise ValueError(f'r must be positive and smaller than in_channels, got r={r} in_channels={in_channels}.')
        act_1, act_1_args = split_args(acti_type_1)
        act_2, act_2_args = split_args(acti_type_2)
        self.fc = nn.Sequential(nn.Linear(in_channels, channels, bias=True), Act[act_1](**act_1_args), nn.Linear(channels, in_channels, bias=True), Act[act_2](**act_2_args))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: in shape (batch, in_channels, spatial_1[, spatial_2, ...]).
        """
        b, c = x.shape[:2]
        y: torch.Tensor = self.avg_pool(x).view(b, c)
        y = self.fc(y).view([b, c] + [1] * (x.ndim - 2))
        result = x * y
        if self.add_residual:
            result += x
        return result


class ResidualSELayer(ChannelSELayer):
    """
    A "squeeze-and-excitation"-like layer with a residual connection::

        --+-- SE --o--
          |        |
          +--------+
    """

    def __init__(self, spatial_dims: int, in_channels: int, r: int=2, acti_type_1: Union[Tuple[str, Dict], str]='leakyrelu', acti_type_2: Union[Tuple[str, Dict], str]='relu') ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions, could be 1, 2, or 3.
            in_channels: number of input channels.
            r: the reduction ratio r in the paper. Defaults to 2.
            acti_type_1: defaults to "leakyrelu".
            acti_type_2: defaults to "relu".

        See also:
            :py:class:`monai.networks.blocks.ChannelSELayer`
        """
        super().__init__(spatial_dims=spatial_dims, in_channels=in_channels, r=r, acti_type_1=acti_type_1, acti_type_2=acti_type_2, add_residual=True)


class SEBlock(nn.Module):
    """
    Residual module enhanced with Squeeze-and-Excitation::

        ----+- conv1 --  conv2 -- conv3 -- SE -o---
            |                                  |
            +---(channel project if needed)----+

    Re-implementation of the SE-Resnet block based on:
    "Hu et al., Squeeze-and-Excitation Networks, https://arxiv.org/abs/1709.01507".
    """

    def __init__(self, spatial_dims: int, in_channels: int, n_chns_1: int, n_chns_2: int, n_chns_3: int, conv_param_1: Optional[Dict]=None, conv_param_2: Optional[Dict]=None, conv_param_3: Optional[Dict]=None, project: Optional[Convolution]=None, r: int=2, acti_type_1: Union[Tuple[str, Dict], str]=('relu', {'inplace': True}), acti_type_2: Union[Tuple[str, Dict], str]='sigmoid', acti_type_final: Optional[Union[Tuple[str, Dict], str]]=('relu', {'inplace': True})):
        """
        Args:
            spatial_dims: number of spatial dimensions, could be 1, 2, or 3.
            in_channels: number of input channels.
            n_chns_1: number of output channels in the 1st convolution.
            n_chns_2: number of output channels in the 2nd convolution.
            n_chns_3: number of output channels in the 3rd convolution.
            conv_param_1: additional parameters to the 1st convolution.
                Defaults to ``{"kernel_size": 1, "norm": Norm.BATCH, "act": ("relu", {"inplace": True})}``
            conv_param_2: additional parameters to the 2nd convolution.
                Defaults to ``{"kernel_size": 3, "norm": Norm.BATCH, "act": ("relu", {"inplace": True})}``
            conv_param_3: additional parameters to the 3rd convolution.
                Defaults to ``{"kernel_size": 1, "norm": Norm.BATCH, "act": None}``
            project: in the case of residual chns and output chns doesn't match, a project
                (Conv) layer/block is used to adjust the number of chns. In SENET, it is
                consisted with a Conv layer as well as a Norm layer.
                Defaults to None (chns are matchable) or a Conv layer with kernel size 1.
            r: the reduction ratio r in the paper. Defaults to 2.
            acti_type_1: activation type of the hidden squeeze layer. Defaults to "relu".
            acti_type_2: activation type of the output squeeze layer. Defaults to "sigmoid".
            acti_type_final: activation type of the end of the block. Defaults to "relu".

        See also:

            :py:class:`monai.networks.blocks.ChannelSELayer`

        """
        super().__init__()
        if not conv_param_1:
            conv_param_1 = {'kernel_size': 1, 'norm': Norm.BATCH, 'act': ('relu', {'inplace': True})}
        self.conv1 = Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=n_chns_1, **conv_param_1)
        if not conv_param_2:
            conv_param_2 = {'kernel_size': 3, 'norm': Norm.BATCH, 'act': ('relu', {'inplace': True})}
        self.conv2 = Convolution(spatial_dims=spatial_dims, in_channels=n_chns_1, out_channels=n_chns_2, **conv_param_2)
        if not conv_param_3:
            conv_param_3 = {'kernel_size': 1, 'norm': Norm.BATCH, 'act': None}
        self.conv3 = Convolution(spatial_dims=spatial_dims, in_channels=n_chns_2, out_channels=n_chns_3, **conv_param_3)
        self.se_layer = ChannelSELayer(spatial_dims=spatial_dims, in_channels=n_chns_3, r=r, acti_type_1=acti_type_1, acti_type_2=acti_type_2)
        if project is None and in_channels != n_chns_3:
            self.project = Conv[Conv.CONV, spatial_dims](in_channels, n_chns_3, kernel_size=1)
        elif project is None:
            self.project = nn.Identity()
        else:
            self.project = project
        if acti_type_final is not None:
            act_final, act_final_args = split_args(acti_type_final)
            self.act = Act[act_final](**act_final_args)
        else:
            self.act = nn.Identity()

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: in shape (batch, in_channels, spatial_1[, spatial_2, ...]).
        """
        residual = self.project(x)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.se_layer(x)
        x += residual
        x = self.act(x)
        return x


class SEBottleneck(SEBlock):
    """
    Bottleneck for SENet154.
    """
    expansion = 4

    def __init__(self, spatial_dims: int, inplanes: int, planes: int, groups: int, reduction: int, stride: int=1, downsample: Optional[Convolution]=None) ->None:
        conv_param_1 = {'strides': 1, 'kernel_size': 1, 'act': ('relu', {'inplace': True}), 'norm': Norm.BATCH, 'bias': False}
        conv_param_2 = {'strides': stride, 'kernel_size': 3, 'act': ('relu', {'inplace': True}), 'norm': Norm.BATCH, 'groups': groups, 'bias': False}
        conv_param_3 = {'strides': 1, 'kernel_size': 1, 'act': None, 'norm': Norm.BATCH, 'bias': False}
        super().__init__(spatial_dims=spatial_dims, in_channels=inplanes, n_chns_1=planes * 2, n_chns_2=planes * 4, n_chns_3=planes * 4, conv_param_1=conv_param_1, conv_param_2=conv_param_2, conv_param_3=conv_param_3, project=downsample, r=reduction)


class SEResNetBottleneck(SEBlock):
    """
    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe
    implementation and uses `strides=stride` in `conv1` and not in `conv2`
    (the latter is used in the torchvision implementation of ResNet).
    """
    expansion = 4

    def __init__(self, spatial_dims: int, inplanes: int, planes: int, groups: int, reduction: int, stride: int=1, downsample: Optional[Convolution]=None) ->None:
        conv_param_1 = {'strides': stride, 'kernel_size': 1, 'act': ('relu', {'inplace': True}), 'norm': Norm.BATCH, 'bias': False}
        conv_param_2 = {'strides': 1, 'kernel_size': 3, 'act': ('relu', {'inplace': True}), 'norm': Norm.BATCH, 'groups': groups, 'bias': False}
        conv_param_3 = {'strides': 1, 'kernel_size': 1, 'act': None, 'norm': Norm.BATCH, 'bias': False}
        super().__init__(spatial_dims=spatial_dims, in_channels=inplanes, n_chns_1=planes, n_chns_2=planes, n_chns_3=planes * 4, conv_param_1=conv_param_1, conv_param_2=conv_param_2, conv_param_3=conv_param_3, project=downsample, r=reduction)


class SEResNeXtBottleneck(SEBlock):
    """
    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.
    """
    expansion = 4

    def __init__(self, spatial_dims: int, inplanes: int, planes: int, groups: int, reduction: int, stride: int=1, downsample: Optional[Convolution]=None, base_width: int=4) ->None:
        conv_param_1 = {'strides': 1, 'kernel_size': 1, 'act': ('relu', {'inplace': True}), 'norm': Norm.BATCH, 'bias': False}
        conv_param_2 = {'strides': stride, 'kernel_size': 3, 'act': ('relu', {'inplace': True}), 'norm': Norm.BATCH, 'groups': groups, 'bias': False}
        conv_param_3 = {'strides': 1, 'kernel_size': 1, 'act': None, 'norm': Norm.BATCH, 'bias': False}
        width = math.floor(planes * (base_width / 64)) * groups
        super().__init__(spatial_dims=spatial_dims, in_channels=inplanes, n_chns_1=width, n_chns_2=width, n_chns_3=planes * 4, conv_param_1=conv_param_1, conv_param_2=conv_param_2, conv_param_3=conv_param_3, project=downsample, r=reduction)


class TransformerBlock(nn.Module):
    """
    A transformer block, based on: "Dosovitskiy et al.,
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>"
    """

    def __init__(self, hidden_size: int, mlp_dim: int, num_heads: int, dropout_rate: float=0.0, qkv_bias: bool=False) ->None:
        """
        Args:
            hidden_size: dimension of hidden layer.
            mlp_dim: dimension of feedforward layer.
            num_heads: number of attention heads.
            dropout_rate: faction of the input units to drop.
            qkv_bias: apply bias term for the qkv linear layer

        """
        super().__init__()
        if not 0 <= dropout_rate <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        if hidden_size % num_heads != 0:
            raise ValueError('hidden_size should be divisible by num_heads.')
        self.mlp = MLPBlock(hidden_size, mlp_dim, dropout_rate)
        self.norm1 = nn.LayerNorm(hidden_size)
        self.attn = SABlock(hidden_size, num_heads, dropout_rate, qkv_bias)
        self.norm2 = nn.LayerNorm(hidden_size)

    def forward(self, x):
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x


class UnetrUpBlock(nn.Module):
    """
    An upsampling module that can be used for UNETR: "Hatamizadeh et al.,
    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int], upsample_kernel_size: Union[Sequence[int], int], norm_name: Union[Tuple, str], res_block: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernel_size: convolution kernel size.
            upsample_kernel_size: convolution kernel size for transposed convolution layers.
            norm_name: feature normalization type and arguments.
            res_block: bool argument to determine if residual block is used.

        """
        super().__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_stride, conv_only=True, is_transposed=True)
        if res_block:
            self.conv_block = UnetResBlock(spatial_dims, out_channels + out_channels, out_channels, kernel_size=kernel_size, stride=1, norm_name=norm_name)
        else:
            self.conv_block = UnetBasicBlock(spatial_dims, out_channels + out_channels, out_channels, kernel_size=kernel_size, stride=1, norm_name=norm_name)

    def forward(self, inp, skip):
        out = self.transp_conv(inp)
        out = torch.cat((out, skip), dim=1)
        out = self.conv_block(out)
        return out


class UnetrPrUpBlock(nn.Module):
    """
    A projection upsampling module that can be used for UNETR: "Hatamizadeh et al.,
    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, num_layer: int, kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], upsample_kernel_size: Union[Sequence[int], int], norm_name: Union[Tuple, str], conv_block: bool=False, res_block: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            num_layer: number of upsampling blocks.
            kernel_size: convolution kernel size.
            stride: convolution stride.
            upsample_kernel_size: convolution kernel size for transposed convolution layers.
            norm_name: feature normalization type and arguments.
            conv_block: bool argument to determine if convolutional block is used.
            res_block: bool argument to determine if residual block is used.

        """
        super().__init__()
        upsample_stride = upsample_kernel_size
        self.transp_conv_init = get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_stride, conv_only=True, is_transposed=True)
        if conv_block:
            if res_block:
                self.blocks = nn.ModuleList([nn.Sequential(get_conv_layer(spatial_dims, out_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_stride, conv_only=True, is_transposed=True), UnetResBlock(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, norm_name=norm_name)) for i in range(num_layer)])
            else:
                self.blocks = nn.ModuleList([nn.Sequential(get_conv_layer(spatial_dims, out_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_stride, conv_only=True, is_transposed=True), UnetBasicBlock(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, norm_name=norm_name)) for i in range(num_layer)])
        else:
            self.blocks = nn.ModuleList([get_conv_layer(spatial_dims, out_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_stride, conv_only=True, is_transposed=True) for i in range(num_layer)])

    def forward(self, x):
        x = self.transp_conv_init(x)
        for blk in self.blocks:
            x = blk(x)
        return x


class UnetrBasicBlock(nn.Module):
    """
    A CNN module that can be used for UNETR, based on: "Hatamizadeh et al.,
    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], norm_name: Union[Tuple, str], res_block: bool=False) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernel_size: convolution kernel size.
            stride: convolution stride.
            norm_name: feature normalization type and arguments.
            res_block: bool argument to determine if residual block is used.

        """
        super().__init__()
        if res_block:
            self.layer = UnetResBlock(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, norm_name=norm_name)
        else:
            self.layer = UnetBasicBlock(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, norm_name=norm_name)

    def forward(self, inp):
        return self.layer(inp)


class GridSampleMode(StrEnum):
    """
    See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html

    interpolation mode of `torch.nn.functional.grid_sample`

    Note:
        (documentation from `torch.nn.functional.grid_sample`)
        `mode='bicubic'` supports only 4-D input.
        When `mode='bilinear'` and the input is 5-D, the interpolation mode used internally will actually be trilinear.
        However, when the input is 4-D, the interpolation mode will legitimately be bilinear.
    """
    NEAREST = 'nearest'
    BILINEAR = 'bilinear'
    BICUBIC = 'bicubic'


class GridSamplePadMode(StrEnum):
    """
    See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
    """
    ZEROS = 'zeros'
    BORDER = 'border'
    REFLECTION = 'reflection'


class _GridPull(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input, grid, interpolation, bound, extrapolate):
        opt = bound, interpolation, extrapolate
        output = _C.grid_pull(input, grid, *opt)
        if input.requires_grad or grid.requires_grad:
            ctx.opt = opt
            ctx.save_for_backward(input, grid)
        return output

    @staticmethod
    def backward(ctx, grad):
        if not (ctx.needs_input_grad[0] or ctx.needs_input_grad[1]):
            return None, None, None, None, None
        var = ctx.saved_tensors
        opt = ctx.opt
        grads = _C.grid_pull_backward(grad, *var, *opt)
        if ctx.needs_input_grad[0]:
            return grads[0], grads[1] if ctx.needs_input_grad[1] else None, None, None, None
        if ctx.needs_input_grad[1]:
            return None, grads[0], None, None, None


def grid_pull(input: torch.Tensor, grid: torch.Tensor, interpolation='linear', bound='zero', extrapolate: bool=True) ->torch.Tensor:
    """
    Sample an image with respect to a deformation field.

    `interpolation` can be an int, a string or an InterpolationType.
    Possible values are::

        - 0 or 'nearest'    or InterpolationType.nearest
        - 1 or 'linear'     or InterpolationType.linear
        - 2 or 'quadratic'  or InterpolationType.quadratic
        - 3 or 'cubic'      or InterpolationType.cubic
        - 4 or 'fourth'     or InterpolationType.fourth
        - 5 or 'fifth'      or InterpolationType.fifth
        - 6 or 'sixth'      or InterpolationType.sixth
        - 7 or 'seventh'    or InterpolationType.seventh

    A list of values can be provided, in the order [W, H, D],
    to specify dimension-specific interpolation orders.

    `bound` can be an int, a string or a BoundType.
    Possible values are::

        - 0 or 'replicate' or 'nearest'      or BoundType.replicate or 'border'
        - 1 or 'dct1'      or 'mirror'       or BoundType.dct1
        - 2 or 'dct2'      or 'reflect'      or BoundType.dct2
        - 3 or 'dst1'      or 'antimirror'   or BoundType.dst1
        - 4 or 'dst2'      or 'antireflect'  or BoundType.dst2
        - 5 or 'dft'       or 'wrap'         or BoundType.dft
        - 7 or 'zero'      or 'zeros'        or BoundType.zero

    A list of values can be provided, in the order [W, H, D],
    to specify dimension-specific boundary conditions.
    `sliding` is a specific condition than only applies to flow fields
    (with as many channels as dimensions). It cannot be dimension-specific.
    Note that:

        - `dft` corresponds to circular padding
        - `dct2` corresponds to Neumann boundary conditions (symmetric)
        - `dst2` corresponds to Dirichlet boundary conditions (antisymmetric)

    See Also:
        - https://en.wikipedia.org/wiki/Discrete_cosine_transform
        - https://en.wikipedia.org/wiki/Discrete_sine_transform
        - ``help(monai._C.BoundType)``
        - ``help(monai._C.InterpolationType)``

    Args:
        input: Input image. `(B, C, Wi, Hi, Di)`.
        grid: Deformation field. `(B, Wo, Ho, Do, 1|2|3)`.
        interpolation (int or list[int] , optional): Interpolation order.
            Defaults to `'linear'`.
        bound (BoundType, or list[BoundType], optional): Boundary conditions.
            Defaults to `'zero'`.
        extrapolate: Extrapolate out-of-bound data.
            Defaults to `True`.

    Returns:
        output (torch.Tensor): Deformed image `(B, C, Wo, Ho, Do)`.

    """
    bound = [(_C.BoundType.__members__[b] if isinstance(b, str) else _C.BoundType(b)) for b in ensure_tuple(bound)]
    interpolation = [(_C.InterpolationType.__members__[i] if isinstance(i, str) else _C.InterpolationType(i)) for i in ensure_tuple(interpolation)]
    out: torch.Tensor
    out = _GridPull.apply(input, grid, interpolation, bound, extrapolate)
    if isinstance(input, monai.data.MetaTensor):
        out = convert_to_dst_type(out, dst=input)[0]
    return out


class Warp(nn.Module):
    """
    Warp an image with given dense displacement field (DDF).
    """

    def __init__(self, mode=GridSampleMode.BILINEAR.value, padding_mode=GridSamplePadMode.BORDER.value):
        """
        For pytorch native APIs, the possible values are:

            - mode: ``"nearest"``, ``"bilinear"``, ``"bicubic"``.
            - padding_mode: ``"zeros"``, ``"border"``, ``"reflection"``

        See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html

        For MONAI C++/CUDA extensions, the possible values are:

            - mode: ``"nearest"``, ``"bilinear"``, ``"bicubic"``, 0, 1, ...
            - padding_mode: ``"zeros"``, ``"border"``, ``"reflection"``, 0, 1, ...

        See also: :py:class:`monai.networks.layers.grid_pull`
        """
        super().__init__()
        if USE_COMPILED:
            if mode in (inter.value for inter in GridSampleMode):
                mode = GridSampleMode(mode)
                if mode == GridSampleMode.BILINEAR:
                    mode = 1
                elif mode == GridSampleMode.NEAREST:
                    mode = 0
                elif mode == GridSampleMode.BICUBIC:
                    mode = 3
                else:
                    mode = 1
            self._interp_mode = mode
        else:
            warnings.warn('monai.networks.blocks.Warp: Using PyTorch native grid_sample.')
            self._interp_mode = GridSampleMode(mode).value
        if USE_COMPILED:
            if padding_mode in (pad.value for pad in GridSamplePadMode):
                padding_mode = GridSamplePadMode(padding_mode)
                if padding_mode == GridSamplePadMode.ZEROS:
                    padding_mode = 7
                elif padding_mode == GridSamplePadMode.BORDER:
                    padding_mode = 0
                elif padding_mode == GridSamplePadMode.REFLECTION:
                    padding_mode = 1
                else:
                    padding_mode = 0
            self._padding_mode = padding_mode
        else:
            self._padding_mode = GridSamplePadMode(padding_mode).value
        self.ref_grid = None

    def get_reference_grid(self, ddf: torch.Tensor) ->torch.Tensor:
        if self.ref_grid is not None and self.ref_grid.shape[0] == ddf.shape[0] and self.ref_grid.shape[1:] == ddf.shape[2:]:
            return self.ref_grid
        mesh_points = [torch.arange(0, dim) for dim in ddf.shape[2:]]
        grid = torch.stack(meshgrid_ij(*mesh_points), dim=0)
        grid = torch.stack([grid] * ddf.shape[0], dim=0)
        self.ref_grid = grid
        self.ref_grid.requires_grad = False
        return self.ref_grid

    def forward(self, image: torch.Tensor, ddf: torch.Tensor):
        """
        Args:
            image: Tensor in shape (batch, num_channels, H, W[, D])
            ddf: Tensor in the same spatial size as image, in shape (batch, ``spatial_dims``, H, W[, D])

        Returns:
            warped_image in the same shape as image (batch, num_channels, H, W[, D])
        """
        spatial_dims = len(image.shape) - 2
        if spatial_dims not in (2, 3):
            raise NotImplementedError(f'got unsupported spatial_dims={spatial_dims}, currently support 2 or 3.')
        ddf_shape = (image.shape[0], spatial_dims) + tuple(image.shape[2:])
        if ddf.shape != ddf_shape:
            raise ValueError(f'Given input {spatial_dims}-d image shape {image.shape}, the input DDF shape must be {ddf_shape}, Got {ddf.shape} instead.')
        grid = self.get_reference_grid(ddf) + ddf
        grid = grid.permute([0] + list(range(2, 2 + spatial_dims)) + [1])
        if not USE_COMPILED:
            for i, dim in enumerate(grid.shape[1:-1]):
                grid[..., i] = grid[..., i] * 2 / (dim - 1) - 1
            index_ordering: List[int] = list(range(spatial_dims - 1, -1, -1))
            grid = grid[..., index_ordering]
            return F.grid_sample(image, grid, mode=self._interp_mode, padding_mode=f'{self._padding_mode}', align_corners=True)
        return grid_pull(image, grid, bound=self._padding_mode, extrapolate=True, interpolation=self._interp_mode)


class DVF2DDF(nn.Module):
    """
    Layer calculates a dense displacement field (DDF) from a dense velocity field (DVF)
    with scaling and squaring.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)

    """

    def __init__(self, num_steps: int=7, mode=GridSampleMode.BILINEAR.value, padding_mode=GridSamplePadMode.ZEROS.value):
        super().__init__()
        if num_steps <= 0:
            raise ValueError(f'expecting positive num_steps, got {num_steps}')
        self.num_steps = num_steps
        self.warp_layer = Warp(mode=mode, padding_mode=padding_mode)

    def forward(self, dvf: torch.Tensor) ->torch.Tensor:
        """
        Args:
            dvf: dvf to be transformed, in shape (batch, ``spatial_dims``, H, W[,D])

        Returns:
            a dense displacement field
        """
        ddf: torch.Tensor = dvf / 2 ** self.num_steps
        for _ in range(self.num_steps):
            ddf = ddf + self.warp_layer(image=ddf, ddf=ddf)
        return ddf


class DropPath(nn.Module):
    """Stochastic drop paths per sample for residual blocks.
    Based on:
    https://github.com/rwightman/pytorch-image-models
    """

    def __init__(self, drop_prob: float=0.0, scale_by_keep: bool=True) ->None:
        """
        Args:
            drop_prob: drop path probability.
            scale_by_keep: scaling by non-dropped probability.
        """
        super().__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep
        if not 0 <= drop_prob <= 1:
            raise ValueError('Drop path prob should be between 0 and 1.')

    def drop_path(self, x, drop_prob: float=0.0, training: bool=False, scale_by_keep: bool=True):
        if drop_prob == 0.0 or not training:
            return x
        keep_prob = 1 - drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
        if keep_prob > 0.0 and scale_by_keep:
            random_tensor.div_(keep_prob)
        return x * random_tensor

    def forward(self, x):
        return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)


class ChannelMatching(StrEnum):
    """
    See also: :py:class:`monai.networks.nets.HighResBlock`
    """
    PAD = 'pad'
    PROJECT = 'project'


class ChannelPad(nn.Module):
    """
    Expand the input tensor's channel dimension from length `in_channels` to `out_channels`,
    by padding or a projection.
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, mode: Union[ChannelMatching, str]=ChannelMatching.PAD):
        """

        Args:
            spatial_dims: number of spatial dimensions of the input image.
            in_channels: number of input channels.
            out_channels: number of output channels.
            mode: {``"pad"``, ``"project"``}
                Specifies handling residual branch and conv branch channel mismatches. Defaults to ``"pad"``.

                - ``"pad"``: with zero padding.
                - ``"project"``: with a trainable conv with kernel size one.
        """
        super().__init__()
        self.project = None
        self.pad = None
        if in_channels == out_channels:
            return
        mode = look_up_option(mode, ChannelMatching)
        if mode == ChannelMatching.PROJECT:
            conv_type = Conv[Conv.CONV, spatial_dims]
            self.project = conv_type(in_channels, out_channels, kernel_size=1)
            return
        if mode == ChannelMatching.PAD:
            if in_channels > out_channels:
                raise ValueError('Incompatible values: channel_matching="pad" and in_channels > out_channels.')
            pad_1 = (out_channels - in_channels) // 2
            pad_2 = out_channels - in_channels - pad_1
            pad = [0, 0] * spatial_dims + [pad_1, pad_2] + [0, 0]
            self.pad = tuple(pad)
            return

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        if self.project is not None:
            return torch.as_tensor(self.project(x))
        if self.pad is not None:
            return F.pad(x, self.pad)
        return x


class SkipMode(StrEnum):
    """
    See also: :py:class:`monai.networks.layers.SkipConnection`
    """
    CAT = 'cat'
    ADD = 'add'
    MUL = 'mul'


class SkipConnection(nn.Module):
    """
    Combine the forward pass input with the result from the given submodule::

        --+--submodule--o--
          |_____________|

    The available modes are ``"cat"``, ``"add"``, ``"mul"``.
    """

    def __init__(self, submodule, dim: int=1, mode: Union[str, SkipMode]='cat') ->None:
        """

        Args:
            submodule: the module defines the trainable branch.
            dim: the dimension over which the tensors are concatenated.
                Used when mode is ``"cat"``.
            mode: ``"cat"``, ``"add"``, ``"mul"``. defaults to ``"cat"``.
        """
        super().__init__()
        self.submodule = submodule
        self.dim = dim
        self.mode = look_up_option(mode, SkipMode).value

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        y = self.submodule(x)
        if self.mode == 'cat':
            return torch.cat([x, y], dim=self.dim)
        if self.mode == 'add':
            return torch.add(x, y)
        if self.mode == 'mul':
            return torch.mul(x, y)
        raise NotImplementedError(f'Unsupported mode {self.mode}.')


class Flatten(nn.Module):
    """
    Flattens the given input in the forward pass to be [B,-1] in shape.
    """

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        return x.view(x.size(0), -1)


class Reshape(nn.Module):
    """
    Reshapes input tensors to the given shape (minus batch dimension), retaining original batch size.
    """

    def __init__(self, *shape: int) ->None:
        """
        Given a shape list/tuple `shape` of integers (s0, s1, ... , sn), this layer will reshape input tensors of
        shape (batch, s0 * s1 * ... * sn) to shape (batch, s0, s1, ... , sn).

        Args:
            shape: list/tuple of integer shape dimensions
        """
        super().__init__()
        self.shape = (1,) + tuple(shape)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        shape = list(self.shape)
        shape[0] = x.shape[0]
        return x.reshape(shape)


class SavitzkyGolayFilter(nn.Module):
    """
    Convolve a Tensor along a particular axis with a Savitzky-Golay kernel.

    Args:
        window_length: Length of the filter window, must be a positive odd integer.
        order: Order of the polynomial to fit to each window, must be less than ``window_length``.
        axis (optional): Axis along which to apply the filter kernel. Default 2 (first spatial dimension).
        mode (string, optional): padding mode passed to convolution class. ``'zeros'``, ``'reflect'``, ``'replicate'`` or
        ``'circular'``. Default: ``'zeros'``. See torch.nn.Conv1d() for more information.
    """

    def __init__(self, window_length: int, order: int, axis: int=2, mode: str='zeros'):
        super().__init__()
        if order >= window_length:
            raise ValueError('order must be less than window_length.')
        self.axis = axis
        self.mode = mode
        self.coeffs = self._make_coeffs(window_length, order)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: Tensor or array-like to filter. Must be real, in shape ``[Batch, chns, spatial1, spatial2, ...]`` and
                have a device type of ``'cpu'``.
        Returns:
            torch.Tensor: ``x`` filtered by Savitzky-Golay kernel with window length ``self.window_length`` using
            polynomials of order ``self.order``, along axis specified in ``self.axis``.
        """
        x = torch.as_tensor(x, device=x.device if isinstance(x, torch.Tensor) else None)
        if torch.is_complex(x):
            raise ValueError('x must be real.')
        x = x
        if self.axis < 0 or self.axis > len(x.shape) - 1:
            raise ValueError(f'Invalid axis for shape of x, got axis {self.axis} and shape {x.shape}.')
        n_spatial_dims = len(x.shape) - 2
        spatial_processing_axis = self.axis - 2
        new_dims_before = spatial_processing_axis
        new_dims_after = n_spatial_dims - spatial_processing_axis - 1
        kernel_list = [self.coeffs]
        for _ in range(new_dims_before):
            kernel_list.insert(0, torch.ones(1, device=x.device, dtype=x.dtype))
        for _ in range(new_dims_after):
            kernel_list.append(torch.ones(1, device=x.device, dtype=x.dtype))
        return separable_filtering(x, kernel_list, mode=self.mode)

    @staticmethod
    def _make_coeffs(window_length, order):
        half_length, rem = divmod(window_length, 2)
        if rem == 0:
            raise ValueError('window_length must be odd.')
        idx = torch.arange(window_length - half_length - 1, -half_length - 1, -1, dtype=torch.float, device='cpu')
        a = idx ** torch.arange(order + 1, dtype=torch.float, device='cpu').reshape(-1, 1)
        y = torch.zeros(order + 1, dtype=torch.float, device='cpu')
        y[0] = 1.0
        return torch.lstsq(y, a).solution.squeeze() if not pytorch_after(1, 11) else torch.linalg.lstsq(a, y).solution.squeeze()


fft, _ = optional_import('torch.fft')


class HilbertTransform(nn.Module):
    """
    Determine the analytical signal of a Tensor along a particular axis.

    Args:
        axis: Axis along which to apply Hilbert transform. Default 2 (first spatial dimension).
        n: Number of Fourier components (i.e. FFT size). Default: ``x.shape[axis]``.
    """

    def __init__(self, axis: int=2, n: Union[int, None]=None) ->None:
        super().__init__()
        self.axis = axis
        self.n = n

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x: Tensor or array-like to transform. Must be real and in shape ``[Batch, chns, spatial1, spatial2, ...]``.
        Returns:
            torch.Tensor: Analytical signal of ``x``, transformed along axis specified in ``self.axis`` using
            FFT of size ``self.N``. The absolute value of ``x_ht`` relates to the envelope of ``x`` along axis ``self.axis``.
        """
        x = torch.as_tensor(x, device=x.device if isinstance(x, torch.Tensor) else None)
        if torch.is_complex(x):
            raise ValueError('x must be real.')
        x = x
        if self.axis < 0 or self.axis > len(x.shape) - 1:
            raise ValueError(f'Invalid axis for shape of x, got axis {self.axis} and shape {x.shape}.')
        n = x.shape[self.axis] if self.n is None else self.n
        if n <= 0:
            raise ValueError('N must be positive.')
        x = torch.as_tensor(x, dtype=torch.complex64)
        f = torch.cat([torch.true_divide(torch.arange(0, (n - 1) // 2 + 1, device=x.device), float(n)), torch.true_divide(torch.arange(-(n // 2), 0, device=x.device), float(n))])
        xf = fft.fft(x, n=n, dim=self.axis)
        u = torch.heaviside(f, torch.tensor([0.5], device=f.device))
        u = torch.as_tensor(u, dtype=x.dtype, device=u.device)
        new_dims_before = self.axis
        new_dims_after = len(xf.shape) - self.axis - 1
        for _ in range(new_dims_before):
            u.unsqueeze_(0)
        for _ in range(new_dims_after):
            u.unsqueeze_(-1)
        ht = fft.ifft(xf * 2 * u, dim=self.axis)
        return torch.as_tensor(ht, device=ht.device, dtype=ht.dtype)


def get_binary_kernel(window_size: Sequence[int], dtype=torch.float, device=None) ->torch.Tensor:
    """
    Create a binary kernel to extract the patches.
    The window size HxWxD will create a (H*W*D)xHxWxD kernel.
    """
    win_size = convert_to_tensor(window_size, int, wrap_sequence=True)
    prod = torch.prod(win_size)
    s = [prod, 1, *win_size]
    return torch.diag(torch.ones(prod, dtype=dtype, device=device)).view(s)


def median_filter(in_tensor: torch.Tensor, kernel_size: Sequence[int]=(3, 3, 3), spatial_dims: int=3, kernel: Optional[torch.Tensor]=None, **kwargs) ->torch.Tensor:
    """
    Apply median filter to an image.

    Args:
        in_tensor: input tensor; median filtering will be applied to the last `spatial_dims` dimensions.
        kernel_size: the convolution kernel size.
        spatial_dims: number of spatial dimensions to apply median filtering.
        kernel: an optional customized kernel.
        kwargs: additional parameters to the `conv`.

    Returns:
        the filtered input tensor, shape remains the same as ``in_tensor``

    Example::

        >>> from monai.networks.layers import median_filter
        >>> import torch
        >>> x = torch.rand(4, 5, 7, 6)
        >>> output = median_filter(x, (3, 3, 3))
        >>> output.shape
        torch.Size([4, 5, 7, 6])

    """
    if not isinstance(in_tensor, torch.Tensor):
        raise TypeError(f'Input type is not a torch.Tensor. Got {type(in_tensor)}')
    original_shape = in_tensor.shape
    oshape, sshape = original_shape[:len(original_shape) - spatial_dims], original_shape[-spatial_dims:]
    oprod = torch.prod(convert_to_tensor(oshape, int, wrap_sequence=True))
    if kernel is None:
        kernel_size = ensure_tuple_rep(kernel_size, spatial_dims)
        kernel = get_binary_kernel(kernel_size, in_tensor.dtype, in_tensor.device)
    else:
        kernel = kernel
    conv = [F.conv1d, F.conv2d, F.conv3d][spatial_dims - 1]
    reshaped_input: torch.Tensor = in_tensor.reshape(oprod, 1, *sshape)
    padding = [((k - 1) // 2) for k in reversed(kernel.shape[2:]) for _ in range(2)]
    padded_input: torch.Tensor = F.pad(reshaped_input, pad=padding, mode='replicate')
    features: torch.Tensor = conv(padded_input, kernel, padding=0, stride=1, **kwargs)
    features = features.view(oprod, -1, *sshape)
    median: torch.Tensor = torch.median(features, dim=1)[0]
    median = median.reshape(original_shape)
    return median


class MedianFilter(nn.Module):
    """
    Apply median filter to an image.

    Args:
        radius: the blurring kernel radius (radius of 1 corresponds to 3x3x3 kernel when spatial_dims=3).

    Returns:
        filtered input tensor.

    Example::

        >>> from monai.networks.layers import MedianFilter
        >>> import torch
        >>> in_tensor = torch.rand(4, 5, 7, 6)
        >>> blur = MedianFilter([1, 1, 1])  # 3x3x3 kernel
        >>> output = blur(in_tensor)
        >>> output.shape
        torch.Size([4, 5, 7, 6])

    """

    def __init__(self, radius: Union[Sequence[int], int], spatial_dims: int=3, device='cpu') ->None:
        super().__init__()
        self.spatial_dims = spatial_dims
        self.radius: Sequence[int] = ensure_tuple_rep(radius, spatial_dims)
        self.window: Sequence[int] = [(1 + 2 * deepcopy(r)) for r in self.radius]
        self.kernel = get_binary_kernel(self.window, device=device)

    def forward(self, in_tensor: torch.Tensor, number_of_passes=1) ->torch.Tensor:
        """
        Args:
            in_tensor: input tensor, median filtering will be applied to the last `spatial_dims` dimensions.
            number_of_passes: median filtering will be repeated this many times
        """
        x = in_tensor
        for _ in range(number_of_passes):
            x = median_filter(x, kernel=self.kernel, spatial_dims=self.spatial_dims)
        return x


class LLTMFunction(Function):

    @staticmethod
    def forward(ctx, input, weights, bias, old_h, old_cell):
        outputs = _C.lltm_forward(input, weights, bias, old_h, old_cell)
        new_h, new_cell = outputs[:2]
        variables = outputs[1:] + [weights]
        ctx.save_for_backward(*variables)
        return new_h, new_cell

    @staticmethod
    def backward(ctx, grad_h, grad_cell):
        outputs = _C.lltm_backward(grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)
        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs[:5]
        return d_input, d_weights, d_bias, d_old_h, d_old_cell


class LLTM(nn.Module):
    """
    This recurrent unit is similar to an LSTM, but differs in that it lacks a forget
    gate and uses an Exponential Linear Unit (ELU) as its internal activation function.
    Because this unit never forgets, call it LLTM, or Long-Long-Term-Memory unit.
    It has both C++ and CUDA implementation, automatically switch according to the
    target device where put this module to.

    Args:
        input_features: size of input feature data
        state_size: size of the state of recurrent unit

    Referring to: https://pytorch.org/tutorials/advanced/cpp_extension.html
    """

    def __init__(self, input_features: int, state_size: int):
        super().__init__()
        self.input_features = input_features
        self.state_size = state_size
        self.weights = nn.Parameter(torch.empty(3 * state_size, input_features + state_size))
        self.bias = nn.Parameter(torch.empty(1, 3 * state_size))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.state_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, +stdv)

    def forward(self, input, state):
        return LLTMFunction.apply(input, self.weights, self.bias, *state)


def normalize_transform(shape, device: Optional[torch.device]=None, dtype: Optional[torch.dtype]=None, align_corners: bool=False, zero_centered: bool=False) ->torch.Tensor:
    """
    Compute an affine matrix according to the input shape.
    The transform normalizes the homogeneous image coordinates to the
    range of `[-1, 1]`.  Currently the following source coordinates are supported:

        - `align_corners=False`, `zero_centered=False`, normalizing from ``[-0.5, d-0.5]``.
        - `align_corners=True`, `zero_centered=False`, normalizing from ``[0, d-1]``.
        - `align_corners=False`, `zero_centered=True`, normalizing from ``[-(d+1)/2, (d-1)/2]``.
        - `align_corners=True`, `zero_centered=True`, normalizing from ``[-(d-1)/2, (d-1)/2]``.

    Args:
        shape: input spatial shape, a sequence of integers.
        device: device on which the returned affine will be allocated.
        dtype: data type of the returned affine
        align_corners: if True, consider -1 and 1 to refer to the centers of the
            corner pixels rather than the image corners.
            See also: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.grid_sample
        zero_centered: whether the coordinates are normalized from a zero-centered range, default to `False`.
            Setting this flag and `align_corners` will jointly specify the normalization source range.
    """
    shape = convert_to_tensor(shape, torch.float64, device=device, wrap_sequence=True, track_meta=False)
    norm = shape.clone().detach()
    if align_corners:
        norm[norm <= 1.0] = 2.0
        norm = 2.0 / (norm - 1.0)
        norm = torch.diag(torch.cat((norm, torch.ones((1,), dtype=torch.float64, device=device))))
        if not zero_centered:
            norm[:-1, -1] = -1.0
    else:
        norm[norm <= 0.0] = 2.0
        norm = 2.0 / norm
        norm = torch.diag(torch.cat((norm, torch.ones((1,), dtype=torch.float64, device=device))))
        norm[:-1, -1] = 1.0 / shape - (0.0 if zero_centered else 1.0)
    norm = norm.unsqueeze(0)
    norm.requires_grad = False
    return norm


def to_norm_affine(affine: torch.Tensor, src_size: Sequence[int], dst_size: Sequence[int], align_corners: bool=False, zero_centered: bool=False) ->torch.Tensor:
    """
    Given ``affine`` defined for coordinates in the pixel space, compute the corresponding affine
    for the normalized coordinates.

    Args:
        affine: Nxdxd batched square matrix
        src_size: source image spatial shape
        dst_size: target image spatial shape
        align_corners: if True, consider -1 and 1 to refer to the centers of the
            corner pixels rather than the image corners.
            See also: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.grid_sample
        zero_centered: whether the coordinates are normalized from a zero-centered range, default to `False`.
            See also: :py:func:`monai.networks.utils.normalize_transform`.

    Raises:
        TypeError: When ``affine`` is not a ``torch.Tensor``.
        ValueError: When ``affine`` is not Nxdxd.
        ValueError: When ``src_size`` or ``dst_size`` dimensions differ from ``affine``.

    """
    if not isinstance(affine, torch.Tensor):
        raise TypeError(f'affine must be a torch.Tensor but is {type(affine).__name__}.')
    if affine.ndimension() != 3 or affine.shape[1] != affine.shape[2]:
        raise ValueError(f'affine must be Nxdxd, got {tuple(affine.shape)}.')
    sr = affine.shape[1] - 1
    if sr != len(src_size) or sr != len(dst_size):
        raise ValueError(f'affine suggests {sr}D, got src={len(src_size)}D, dst={len(dst_size)}D.')
    src_xform = normalize_transform(src_size, affine.device, affine.dtype, align_corners, zero_centered)
    dst_xform = normalize_transform(dst_size, affine.device, affine.dtype, align_corners, zero_centered)
    return src_xform @ affine @ torch.inverse(dst_xform)


class AffineTransform(nn.Module):

    def __init__(self, spatial_size: Optional[Union[Sequence[int], int]]=None, normalized: bool=False, mode: str=GridSampleMode.BILINEAR, padding_mode: str=GridSamplePadMode.ZEROS, align_corners: bool=False, reverse_indexing: bool=True, zero_centered: Optional[bool]=None) ->None:
        """
        Apply affine transformations with a batch of affine matrices.

        When `normalized=False` and `reverse_indexing=True`,
        it does the commonly used resampling in the 'pull' direction
        following the ``scipy.ndimage.affine_transform`` convention.
        In this case `theta` is equivalent to (ndim+1, ndim+1) input ``matrix`` of ``scipy.ndimage.affine_transform``,
        operates on homogeneous coordinates.
        See also: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html

        When `normalized=True` and `reverse_indexing=False`,
        it applies `theta` to the normalized coordinates (coords. in the range of [-1, 1]) directly.
        This is often used with `align_corners=False` to achieve resolution-agnostic resampling,
        thus useful as a part of trainable modules such as the spatial transformer networks.
        See also: https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html

        Args:
            spatial_size: output spatial shape, the full output shape will be
                `[N, C, *spatial_size]` where N and C are inferred from the `src` input of `self.forward`.
            normalized: indicating whether the provided affine matrix `theta` is defined
                for the normalized coordinates. If `normalized=False`, `theta` will be converted
                to operate on normalized coordinates as pytorch affine_grid works with the normalized
                coordinates.
            mode: {``"bilinear"``, ``"nearest"``}
                Interpolation mode to calculate output values. Defaults to ``"bilinear"``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
            padding_mode: {``"zeros"``, ``"border"``, ``"reflection"``}
                Padding mode for outside grid values. Defaults to ``"zeros"``.
                See also: https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
            align_corners: see also https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html.
            reverse_indexing: whether to reverse the spatial indexing of image and coordinates.
                set to `False` if `theta` follows pytorch's default "D, H, W" convention.
                set to `True` if `theta` follows `scipy.ndimage` default "i, j, k" convention.
            zero_centered: whether the affine is applied to coordinates in a zero-centered value range.
                With `zero_centered=True`, for example, the center of rotation will be the
                spatial center of the input; with `zero_centered=False`, the center of rotation will be the
                origin of the input. This option is only available when `normalized=False`,
                where the default behaviour is `False` if unspecified.
                See also: :py:func:`monai.networks.utils.normalize_transform`.
        """
        super().__init__()
        self.spatial_size = ensure_tuple(spatial_size) if spatial_size is not None else None
        self.normalized = normalized
        self.mode: str = look_up_option(mode, GridSampleMode)
        self.padding_mode: str = look_up_option(padding_mode, GridSamplePadMode)
        self.align_corners = align_corners
        self.reverse_indexing = reverse_indexing
        if zero_centered is not None and self.normalized:
            raise ValueError('`normalized=True` is not compatible with the `zero_centered` option.')
        self.zero_centered = zero_centered if zero_centered is not None else False

    def forward(self, src: torch.Tensor, theta: torch.Tensor, spatial_size: Optional[Union[Sequence[int], int]]=None) ->torch.Tensor:
        """
        ``theta`` must be an affine transformation matrix with shape
        3x3 or Nx3x3 or Nx2x3 or 2x3 for spatial 2D transforms,
        4x4 or Nx4x4 or Nx3x4 or 3x4 for spatial 3D transforms,
        where `N` is the batch size. `theta` will be converted into float Tensor for the computation.

        Args:
            src (array_like): image in spatial 2D or 3D (N, C, spatial_dims),
                where N is the batch dim, C is the number of channels.
            theta (array_like): Nx3x3, Nx2x3, 3x3, 2x3 for spatial 2D inputs,
                Nx4x4, Nx3x4, 3x4, 4x4 for spatial 3D inputs. When the batch dimension is omitted,
                `theta` will be repeated N times, N is the batch dim of `src`.
            spatial_size: output spatial shape, the full output shape will be
                `[N, C, *spatial_size]` where N and C are inferred from the `src`.

        Raises:
            TypeError: When ``theta`` is not a ``torch.Tensor``.
            ValueError: When ``theta`` is not one of [Nxdxd, dxd].
            ValueError: When ``theta`` is not one of [Nx3x3, Nx4x4].
            TypeError: When ``src`` is not a ``torch.Tensor``.
            ValueError: When ``src`` spatially is not one of [2D, 3D].
            ValueError: When affine and image batch dimension differ.

        """
        if not isinstance(theta, torch.Tensor):
            raise TypeError(f'theta must be torch.Tensor but is {type(theta).__name__}.')
        if theta.dim() not in (2, 3):
            raise ValueError(f'theta must be Nxdxd or dxd, got {theta.shape}.')
        if theta.dim() == 2:
            theta = theta[None]
        theta = theta.clone()
        theta_shape = tuple(theta.shape[1:])
        if theta_shape in ((2, 3), (3, 4)):
            pad_affine = torch.tensor([0, 0, 1] if theta_shape[0] == 2 else [0, 0, 0, 1])
            pad_affine = pad_affine.repeat(theta.shape[0], 1, 1)
            pad_affine.requires_grad = False
            theta = torch.cat([theta, pad_affine], dim=1)
        if tuple(theta.shape[1:]) not in ((3, 3), (4, 4)):
            raise ValueError(f'theta must be Nx3x3 or Nx4x4, got {theta.shape}.')
        if not isinstance(src, torch.Tensor):
            raise TypeError(f'src must be torch.Tensor but is {type(src).__name__}.')
        sr = src.dim() - 2
        if sr not in (2, 3):
            raise ValueError(f'Unsupported src dimension: {sr}, available options are [2, 3].')
        src_size = tuple(src.shape)
        dst_size = src_size
        if self.spatial_size is not None:
            dst_size = src_size[:2] + self.spatial_size
        if spatial_size is not None:
            dst_size = src_size[:2] + ensure_tuple(spatial_size)
        if not self.normalized:
            theta = to_norm_affine(affine=theta, src_size=src_size[2:], dst_size=dst_size[2:], align_corners=self.align_corners, zero_centered=self.zero_centered)
        if self.reverse_indexing:
            rev_idx = torch.as_tensor(range(sr - 1, -1, -1), device=src.device)
            theta[:, :sr] = theta[:, rev_idx]
            theta[:, :, :sr] = theta[:, :, rev_idx]
        if theta.shape[0] == 1 and src_size[0] > 1:
            theta = theta.repeat(src_size[0], 1, 1)
        if theta.shape[0] != src_size[0]:
            raise ValueError(f'affine and image batch dimension must match, got affine={theta.shape[0]} image={src_size[0]}.')
        grid = nn.functional.affine_grid(theta=theta[:, :sr], size=list(dst_size), align_corners=self.align_corners)
        dst = nn.functional.grid_sample(input=src.contiguous(), grid=grid, mode=self.mode, padding_mode=self.padding_mode, align_corners=self.align_corners)
        return dst


class Bottleneck3x3x1(nn.Module):
    expansion = 4

    def __init__(self, spatial_dims: int, inplanes: int, planes: int, stride: Union[Sequence[int], int]=1, downsample: Optional[nn.Sequential]=None) ->None:
        super().__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        pool_type: Type[Union[nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        self.conv1 = conv_type(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = norm_type(planes)
        self.conv2 = conv_type(planes, planes, kernel_size=(3, 3, 1)[-spatial_dims:], stride=stride, padding=(1, 1, 0)[-spatial_dims:], bias=False)
        self.bn2 = norm_type(planes)
        self.conv3 = conv_type(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = norm_type(planes * 4)
        self.relu = relu_type(inplace=True)
        self.downsample = downsample
        self.stride = stride
        self.pool = pool_type(kernel_size=(1, 1, 2)[-spatial_dims:], stride=(1, 1, 2)[-spatial_dims:])

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
            if out.size() != residual.size():
                out = self.pool(out)
        out += residual
        out = self.relu(out)
        return out


class Projection(nn.Sequential):

    def __init__(self, spatial_dims: int, num_input_features: int, num_output_features: int):
        super().__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        self.add_module('norm', norm_type(num_input_features))
        self.add_module('relu', relu_type(inplace=True))
        self.add_module('conv', conv_type(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))


def get_acti_layer(act: Union[Tuple[str, Dict], str], nchan: int=0):
    if act == 'prelu':
        act = 'prelu', {'num_parameters': nchan}
    act_name, act_args = split_args(act)
    act_type = Act[act_name]
    return act_type(**act_args)


class LUConv(nn.Module):

    def __init__(self, spatial_dims: int, nchan: int, act: Union[Tuple[str, Dict], str], bias: bool=False):
        super().__init__()
        self.act_function = get_acti_layer(act, nchan)
        self.conv_block = Convolution(spatial_dims=spatial_dims, in_channels=nchan, out_channels=nchan, kernel_size=5, act=None, norm=Norm.BATCH, bias=bias)

    def forward(self, x):
        out = self.conv_block(x)
        out = self.act_function(out)
        return out


def _make_nconv(spatial_dims: int, nchan: int, depth: int, act: Union[Tuple[str, Dict], str], bias: bool=False):
    layers = []
    for _ in range(depth):
        layers.append(LUConv(spatial_dims, nchan, act, bias))
    return nn.Sequential(*layers)


class UpTransition(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, nconvs: int, act: Union[Tuple[str, Dict], str], dropout_prob: Optional[float]=None, dropout_dim: int=3):
        super().__init__()
        conv_trans_type: Type[Union[nn.ConvTranspose2d, nn.ConvTranspose3d]] = Conv[Conv.CONVTRANS, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        dropout_type: Type[Union[nn.Dropout, nn.Dropout2d, nn.Dropout3d]] = Dropout[Dropout.DROPOUT, dropout_dim]
        self.up_conv = conv_trans_type(in_channels, out_channels // 2, kernel_size=2, stride=2)
        self.bn1 = norm_type(out_channels // 2)
        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None
        self.dropout2 = dropout_type(0.5)
        self.act_function1 = get_acti_layer(act, out_channels // 2)
        self.act_function2 = get_acti_layer(act, out_channels)
        self.ops = _make_nconv(spatial_dims, out_channels, nconvs, act)

    def forward(self, x, skipx):
        if self.dropout is not None:
            out = self.dropout(x)
        else:
            out = x
        skipxdo = self.dropout2(skipx)
        out = self.act_function1(self.bn1(self.up_conv(out)))
        xcat = torch.cat((out, skipxdo), 1)
        out = self.ops(xcat)
        out = self.act_function2(torch.add(out, xcat))
        return out


class Final(nn.Sequential):

    def __init__(self, spatial_dims: int, num_input_features: int, num_output_features: int, upsample_mode: str='transpose'):
        super().__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        self.add_module('norm', norm_type(num_input_features))
        self.add_module('relu', relu_type(inplace=True))
        self.add_module('conv', conv_type(num_input_features, num_output_features, kernel_size=(3, 3, 1)[-spatial_dims:], stride=1, padding=(1, 1, 0)[-spatial_dims:], bias=False))
        if upsample_mode == 'transpose':
            conv_trans_type = Conv[Conv.CONVTRANS, spatial_dims]
            self.add_module('up', conv_trans_type(num_output_features, num_output_features, kernel_size=2, stride=2, bias=False))
        else:
            align_corners: Optional[bool] = None
            if upsample_mode in ['trilinear', 'bilinear']:
                align_corners = True
            self.add_module('up', nn.Upsample(scale_factor=2, mode=upsample_mode, align_corners=align_corners))


class PSP(nn.Module):

    def __init__(self, spatial_dims: int, psp_block_num: int, in_ch: int, upsample_mode: str='transpose'):
        super().__init__()
        self.up_modules = nn.ModuleList()
        conv_type = Conv[Conv.CONV, spatial_dims]
        pool_type: Type[Union[nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        self.pool_modules = nn.ModuleList()
        self.project_modules = nn.ModuleList()
        for i in range(psp_block_num):
            size = (2 ** (i + 3), 2 ** (i + 3), 1)[-spatial_dims:]
            self.pool_modules.append(pool_type(kernel_size=size, stride=size))
            self.project_modules.append(conv_type(in_ch, 1, kernel_size=(1, 1, 1)[-spatial_dims:], stride=1, padding=(1, 1, 0)[-spatial_dims:]))
        self.spatial_dims = spatial_dims
        self.psp_block_num = psp_block_num
        self.upsample_mode = upsample_mode
        if self.upsample_mode == 'transpose':
            conv_trans_type = Conv[Conv.CONVTRANS, spatial_dims]
            for i in range(psp_block_num):
                size = (2 ** (i + 3), 2 ** (i + 3), 1)[-spatial_dims:]
                pad_size = (2 ** (i + 3), 2 ** (i + 3), 0)[-spatial_dims:]
                self.up_modules.append(conv_trans_type(1, 1, kernel_size=size, stride=size, padding=pad_size))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        outputs = []
        if self.upsample_mode == 'transpose':
            for project_module, pool_module, up_module in zip(self.project_modules, self.pool_modules, self.up_modules):
                output = up_module(project_module(pool_module(x)))
                outputs.append(output)
        else:
            for project_module, pool_module in zip(self.project_modules, self.pool_modules):
                interpolate_size = x.shape[2:]
                align_corners: Optional[bool] = None
                if self.upsample_mode in ['trilinear', 'bilinear']:
                    align_corners = True
                output = F.interpolate(project_module(pool_module(x)), size=interpolate_size, mode=self.upsample_mode, align_corners=align_corners)
                outputs.append(output)
        x = torch.cat(outputs, dim=1)
        return x


def copy_bn_param(module2d, module3d):
    for p2d, p3d in zip(module2d.parameters(), module3d.parameters()):
        p3d.data[:] = p2d.data[:]


def copy_conv_param(module2d, module3d):
    for p2d, p3d in zip(module2d.parameters(), module3d.parameters()):
        p3d.data[:] = p2d.data.unsqueeze(dim=4).clone()[:]


class AHNet(nn.Module):
    """
    AHNet based on `Anisotropic Hybrid Network <https://arxiv.org/pdf/1711.08580.pdf>`_.
    Adapted from `lsqshr's official code <https://github.com/lsqshr/AH-Net/blob/master/net3d.py>`_.
    Except from the original network that supports 3D inputs, this implementation also supports 2D inputs.
    According to the `tests for deconvolutions <https://github.com/Project-MONAI/MONAI/issues/1023>`_, using
    ``"transpose"`` rather than linear interpolations is faster. Therefore, this implementation sets ``"transpose"``
    as the default upsampling method.

    To meet the requirements of the structure, the input size for each spatial dimension
    (except the last one) should be: divisible by 2 ** (psp_block_num + 3) and no less than 32 in ``transpose`` mode,
    and should be divisible by 32 and no less than 2 ** (psp_block_num + 3) in other upsample modes.
    In addition, the input size for the last spatial dimension should be divisible by 32, and at least one spatial size
    should be no less than 64.

    Args:
        layers: number of residual blocks for 4 layers of the network (layer1...layer4). Defaults to ``(3, 4, 6, 3)``.
        spatial_dims: spatial dimension of the input data. Defaults to 3.
        in_channels: number of input channels for the network. Default to 1.
        out_channels: number of output channels for the network. Defaults to 1.
        psp_block_num: the number of pyramid volumetric pooling modules used at the end of the network before the final
            output layer for extracting multiscale features. The number should be an integer that belongs to [0,4]. Defaults
            to 4.
        upsample_mode: [``"transpose"``, ``"bilinear"``, ``"trilinear"``, ``nearest``]
            The mode of upsampling manipulations.
            Using the last two modes cannot guarantee the model's reproducibility. Defaults to ``transpose``.

            - ``"transpose"``, uses transposed convolution layers.
            - ``"bilinear"``, uses bilinear interpolate.
            - ``"trilinear"``, uses trilinear interpolate.
            - ``"nearest"``, uses nearest interpolate.
        pretrained: whether to load pretrained weights from ResNet50 to initialize convolution layers, default to False.
        progress: If True, displays a progress bar of the download of pretrained weights to stderr.
    """

    def __init__(self, layers: tuple=(3, 4, 6, 3), spatial_dims: int=3, in_channels: int=1, out_channels: int=1, psp_block_num: int=4, upsample_mode: str='transpose', pretrained: bool=False, progress: bool=True):
        self.inplanes = 64
        super().__init__()
        conv_type = Conv[Conv.CONV, spatial_dims]
        conv_trans_type = Conv[Conv.CONVTRANS, spatial_dims]
        norm_type = Norm[Norm.BATCH, spatial_dims]
        pool_type: Type[Union[nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        conv2d_type: Type[nn.Conv2d] = Conv[Conv.CONV, 2]
        norm2d_type: Type[nn.BatchNorm2d] = Norm[Norm.BATCH, 2]
        self.conv2d_type = conv2d_type
        self.norm2d_type = norm2d_type
        self.conv_type = conv_type
        self.norm_type = norm_type
        self.relu_type = relu_type
        self.pool_type = pool_type
        self.spatial_dims = spatial_dims
        self.psp_block_num = psp_block_num
        self.psp: PSP
        if spatial_dims not in [2, 3]:
            raise AssertionError('spatial_dims can only be 2 or 3.')
        if psp_block_num not in [0, 1, 2, 3, 4]:
            raise AssertionError('psp_block_num should be an integer that belongs to [0, 4].')
        self.conv1 = conv_type(in_channels, 64, kernel_size=(7, 7, 3)[-spatial_dims:], stride=(2, 2, 1)[-spatial_dims:], padding=(3, 3, 1)[-spatial_dims:], bias=False)
        self.pool1 = pool_type(kernel_size=(1, 1, 2)[-spatial_dims:], stride=(1, 1, 2)[-spatial_dims:])
        self.bn0 = norm_type(64)
        self.relu = relu_type(inplace=True)
        if upsample_mode in ['transpose', 'nearest']:
            self.maxpool = pool_type(kernel_size=(2, 2, 2)[-spatial_dims:], stride=2)
        else:
            self.maxpool = pool_type(kernel_size=(3, 3, 3)[-spatial_dims:], stride=2, padding=1)
        self.layer1 = self._make_layer(Bottleneck3x3x1, 64, layers[0], stride=1)
        self.layer2 = self._make_layer(Bottleneck3x3x1, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(Bottleneck3x3x1, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(Bottleneck3x3x1, 512, layers[3], stride=2)
        densegrowth = 20
        densebn = 4
        ndenselayer = 3
        num_init_features = 64
        noutres1 = 256
        noutres2 = 512
        noutres3 = 1024
        noutres4 = 2048
        self.up0 = UpTransition(spatial_dims, noutres4, noutres3, upsample_mode)
        self.dense0 = DenseBlock(spatial_dims, ndenselayer, noutres3, densebn, densegrowth, 0.0)
        noutdense = noutres3 + ndenselayer * densegrowth
        self.up1 = UpTransition(spatial_dims, noutdense, noutres2, upsample_mode)
        self.dense1 = DenseBlock(spatial_dims, ndenselayer, noutres2, densebn, densegrowth, 0.0)
        noutdense1 = noutres2 + ndenselayer * densegrowth
        self.up2 = UpTransition(spatial_dims, noutdense1, noutres1, upsample_mode)
        self.dense2 = DenseBlock(spatial_dims, ndenselayer, noutres1, densebn, densegrowth, 0.0)
        noutdense2 = noutres1 + ndenselayer * densegrowth
        self.trans1 = Projection(spatial_dims, noutdense2, num_init_features)
        self.dense3 = DenseBlock(spatial_dims, ndenselayer, num_init_features, densebn, densegrowth, 0.0)
        noutdense3 = num_init_features + densegrowth * ndenselayer
        self.up3 = UpTransition(spatial_dims, noutdense3, num_init_features, upsample_mode)
        self.dense4 = DenseBlock(spatial_dims, ndenselayer, num_init_features, densebn, densegrowth, 0.0)
        noutdense4 = num_init_features + densegrowth * ndenselayer
        self.psp = PSP(spatial_dims, psp_block_num, noutdense4, upsample_mode)
        self.final = Final(spatial_dims, psp_block_num + noutdense4, out_channels, upsample_mode)
        for m in self.modules():
            if isinstance(m, (conv_type, conv_trans_type)):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2.0 / n))
            elif isinstance(m, norm_type):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
        if pretrained:
            net2d = FCN(pretrained=True, progress=progress)
            self.copy_from(net2d)

    def _make_layer(self, block: Type[Bottleneck3x3x1], planes: int, blocks: int, stride: int=1) ->nn.Sequential:
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(self.conv_type(self.inplanes, planes * block.expansion, kernel_size=1, stride=(stride, stride, 1)[:self.spatial_dims], bias=False), self.pool_type(kernel_size=(1, 1, stride)[:self.spatial_dims], stride=(1, 1, stride)[:self.spatial_dims]), self.norm_type(planes * block.expansion))
        layers = []
        layers.append(block(self.spatial_dims, self.inplanes, planes, (stride, stride, 1)[:self.spatial_dims], downsample))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.spatial_dims, self.inplanes, planes))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.bn0(x)
        x = self.relu(x)
        conv_x = x
        x = self.maxpool(x)
        pool_x = x
        fm1 = self.layer1(x)
        fm2 = self.layer2(fm1)
        fm3 = self.layer3(fm2)
        fm4 = self.layer4(fm3)
        sum0 = self.up0(fm4) + fm3
        d0 = self.dense0(sum0)
        sum1 = self.up1(d0) + fm2
        d1 = self.dense1(sum1)
        sum2 = self.up2(d1) + fm1
        d2 = self.dense2(sum2)
        sum3 = self.trans1(d2) + pool_x
        d3 = self.dense3(sum3)
        sum4 = self.up3(d3) + conv_x
        d4 = self.dense4(sum4)
        if self.psp_block_num > 0:
            psp = self.psp(d4)
            x = torch.cat((psp, d4), dim=1)
        else:
            x = d4
        return self.final(x)

    def copy_from(self, net):
        p2d, p3d = next(net.conv1.parameters()), next(self.conv1.parameters())
        weights = p2d.data.unsqueeze(dim=4).permute(0, 4, 2, 3, 1).clone()
        p3d.data = weights.repeat([1, p3d.shape[1], 1, 1, 1])
        copy_bn_param(net.bn0, self.bn0)
        for i in range(1, 5):
            layer_num = 'layer' + str(i)
            layer_2d = []
            layer_3d = []
            for m1 in vars(net)['_modules'][layer_num].modules():
                if isinstance(m1, (self.norm2d_type, self.conv2d_type)):
                    layer_2d.append(m1)
            for m2 in vars(self)['_modules'][layer_num].modules():
                if isinstance(m2, (self.norm_type, self.conv_type)):
                    layer_3d.append(m2)
            for m1, m2 in zip(layer_2d, layer_3d):
                if isinstance(m1, self.conv2d_type):
                    copy_conv_param(m1, m2)
                if isinstance(m1, self.norm2d_type):
                    copy_bn_param(m1, m2)


class ConvBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: int=3, strides: int=1, dropout=0.0):
        super().__init__()
        layers = [Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, strides=strides, padding=None, adn_ordering='NDA', act='relu', norm=Norm.BATCH, dropout=dropout), Convolution(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, strides=1, padding=None, adn_ordering='NDA', act='relu', norm=Norm.BATCH, dropout=dropout)]
        self.conv = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x_c: torch.Tensor = self.conv(x)
        return x_c


class UpConv(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size=3, strides=2, dropout=0.0):
        super().__init__()
        self.up = Convolution(spatial_dims, in_channels, out_channels, strides=strides, kernel_size=kernel_size, act='relu', adn_ordering='NDA', norm=Norm.BATCH, dropout=dropout, is_transposed=True)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x_u: torch.Tensor = self.up(x)
        return x_u


class AttentionBlock(nn.Module):

    def __init__(self, spatial_dims: int, f_int: int, f_g: int, f_l: int, dropout=0.0):
        super().__init__()
        self.W_g = nn.Sequential(Convolution(spatial_dims=spatial_dims, in_channels=f_g, out_channels=f_int, kernel_size=1, strides=1, padding=0, dropout=dropout, conv_only=True), Norm[Norm.BATCH, spatial_dims](f_int))
        self.W_x = nn.Sequential(Convolution(spatial_dims=spatial_dims, in_channels=f_l, out_channels=f_int, kernel_size=1, strides=1, padding=0, dropout=dropout, conv_only=True), Norm[Norm.BATCH, spatial_dims](f_int))
        self.psi = nn.Sequential(Convolution(spatial_dims=spatial_dims, in_channels=f_int, out_channels=1, kernel_size=1, strides=1, padding=0, dropout=dropout, conv_only=True), Norm[Norm.BATCH, spatial_dims](1), nn.Sigmoid())
        self.relu = nn.ReLU()

    def forward(self, g: torch.Tensor, x: torch.Tensor) ->torch.Tensor:
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi: torch.Tensor = self.relu(g1 + x1)
        psi = self.psi(psi)
        return x * psi


class AttentionLayer(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, submodule: nn.Module, up_kernel_size=3, strides=2, dropout=0.0):
        super().__init__()
        self.attention = AttentionBlock(spatial_dims=spatial_dims, f_g=in_channels, f_l=in_channels, f_int=in_channels // 2)
        self.upconv = UpConv(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=in_channels, strides=strides, kernel_size=up_kernel_size)
        self.merge = Convolution(spatial_dims=spatial_dims, in_channels=2 * in_channels, out_channels=in_channels, dropout=dropout)
        self.submodule = submodule

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        fromlower = self.upconv(self.submodule(x))
        att = self.attention(g=fromlower, x=x)
        att_m: torch.Tensor = self.merge(torch.cat((att, fromlower), dim=1))
        return att_m


class AttentionUnet(nn.Module):
    """
    Attention Unet based on
    Otkay et al. "Attention U-Net: Learning Where to Look for the Pancreas"
    https://arxiv.org/abs/1804.03999

    Args:
        spatial_dims: number of spatial dimensions of the input image.
        in_channels: number of the input channel.
        out_channels: number of the output classes.
        channels (Sequence[int]): sequence of channels. Top block first. The length of `channels` should be no less than 2.
        strides (Sequence[int]): stride to use for convolutions.
        kernel_size: convolution kernel size.
        up_kernel_size: convolution kernel size for transposed convolution layers.
        dropout: dropout ratio. Defaults to no dropout.
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, channels: Sequence[int], strides: Sequence[int], kernel_size: Union[Sequence[int], int]=3, up_kernel_size: Union[Sequence[int], int]=3, dropout: float=0.0):
        super().__init__()
        self.dimensions = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.channels = channels
        self.strides = strides
        self.kernel_size = kernel_size
        self.dropout = dropout
        head = ConvBlock(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=channels[0], dropout=dropout)
        reduce_channels = Convolution(spatial_dims=spatial_dims, in_channels=channels[0], out_channels=out_channels, kernel_size=1, strides=1, padding=0, conv_only=True)
        self.up_kernel_size = up_kernel_size

        def _create_block(channels: Sequence[int], strides: Sequence[int]) ->nn.Module:
            if len(channels) > 2:
                subblock = _create_block(channels[1:], strides[1:])
                return AttentionLayer(spatial_dims=spatial_dims, in_channels=channels[0], out_channels=channels[1], submodule=nn.Sequential(ConvBlock(spatial_dims=spatial_dims, in_channels=channels[0], out_channels=channels[1], strides=strides[0], dropout=self.dropout), subblock), up_kernel_size=self.up_kernel_size, strides=strides[0], dropout=dropout)
            else:
                return self._get_bottom_layer(channels[0], channels[1], strides[0])
        encdec = _create_block(self.channels, self.strides)
        self.model = nn.Sequential(head, encdec, reduce_channels)

    def _get_bottom_layer(self, in_channels: int, out_channels: int, strides: int) ->nn.Module:
        return AttentionLayer(spatial_dims=self.dimensions, in_channels=in_channels, out_channels=out_channels, submodule=ConvBlock(spatial_dims=self.dimensions, in_channels=in_channels, out_channels=out_channels, strides=strides, dropout=self.dropout), up_kernel_size=self.up_kernel_size, strides=strides, dropout=self.dropout)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x_m: torch.Tensor = self.model(x)
        return x_m


class BasicUNetPlusPlus(nn.Module):

    def __init__(self, spatial_dims: int=3, in_channels: int=1, out_channels: int=2, features: Sequence[int]=(32, 32, 64, 128, 256, 32), deep_supervision: bool=False, act: Union[str, tuple]=('LeakyReLU', {'negative_slope': 0.1, 'inplace': True}), norm: Union[str, tuple]=('instance', {'affine': True}), bias: bool=True, dropout: Union[float, tuple]=0.0, upsample: str='deconv'):
        """
        A UNet++ implementation with 1D/2D/3D supports.

        Based on:

            Zhou et al. "UNet++: A Nested U-Net Architecture for Medical Image
            Segmentation". 4th Deep Learning in Medical Image Analysis (DLMIA)
            Workshop, DOI: https://doi.org/10.48550/arXiv.1807.10165


        Args:
            spatial_dims: number of spatial dimensions. Defaults to 3 for spatial 3D inputs.
            in_channels: number of input channels. Defaults to 1.
            out_channels: number of output channels. Defaults to 2.
            features: six integers as numbers of features.
                Defaults to ``(32, 32, 64, 128, 256, 32)``,

                - the first five values correspond to the five-level encoder feature sizes.
                - the last value corresponds to the feature size after the last upsampling.

            deep_supervision: whether to prune the network at inference time. Defaults to False. If true, returns a list,
                whose elements correspond to outputs at different nodes.
            act: activation type and arguments. Defaults to LeakyReLU.
            norm: feature normalization type and arguments. Defaults to instance norm.
            bias: whether to have a bias term in convolution blocks. Defaults to True.
                According to `Performance Tuning Guide <https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html>`_,
                if a conv layer is directly followed by a batch norm layer, bias should be False.
            dropout: dropout ratio. Defaults to no dropout.
            upsample: upsampling mode, available options are
                ``"deconv"``, ``"pixelshuffle"``, ``"nontrainable"``.

        Examples::

            # for spatial 2D
            >>> net = BasicUNetPlusPlus(spatial_dims=2, features=(64, 128, 256, 512, 1024, 128))

            # for spatial 2D, with deep supervision enabled
            >>> net = BasicUNetPlusPlus(spatial_dims=2, features=(64, 128, 256, 512, 1024, 128), deep_supervision=True)

            # for spatial 2D, with group norm
            >>> net = BasicUNetPlusPlus(spatial_dims=2, features=(64, 128, 256, 512, 1024, 128), norm=("group", {"num_groups": 4}))

            # for spatial 3D
            >>> net = BasicUNetPlusPlus(spatial_dims=3, features=(32, 32, 64, 128, 256, 32))

        See Also
            - :py:class:`monai.networks.nets.BasicUNet`
            - :py:class:`monai.networks.nets.DynUNet`
            - :py:class:`monai.networks.nets.UNet`

        """
        super().__init__()
        self.deep_supervision = deep_supervision
        fea = ensure_tuple_rep(features, 6)
        None
        self.conv_0_0 = TwoConv(spatial_dims, in_channels, fea[0], act, norm, bias, dropout)
        self.conv_1_0 = Down(spatial_dims, fea[0], fea[1], act, norm, bias, dropout)
        self.conv_2_0 = Down(spatial_dims, fea[1], fea[2], act, norm, bias, dropout)
        self.conv_3_0 = Down(spatial_dims, fea[2], fea[3], act, norm, bias, dropout)
        self.conv_4_0 = Down(spatial_dims, fea[3], fea[4], act, norm, bias, dropout)
        self.upcat_0_1 = UpCat(spatial_dims, fea[1], fea[0], fea[0], act, norm, bias, dropout, upsample, halves=False)
        self.upcat_1_1 = UpCat(spatial_dims, fea[2], fea[1], fea[1], act, norm, bias, dropout, upsample)
        self.upcat_2_1 = UpCat(spatial_dims, fea[3], fea[2], fea[2], act, norm, bias, dropout, upsample)
        self.upcat_3_1 = UpCat(spatial_dims, fea[4], fea[3], fea[3], act, norm, bias, dropout, upsample)
        self.upcat_0_2 = UpCat(spatial_dims, fea[1], fea[0] * 2, fea[0], act, norm, bias, dropout, upsample, halves=False)
        self.upcat_1_2 = UpCat(spatial_dims, fea[2], fea[1] * 2, fea[1], act, norm, bias, dropout, upsample)
        self.upcat_2_2 = UpCat(spatial_dims, fea[3], fea[2] * 2, fea[2], act, norm, bias, dropout, upsample)
        self.upcat_0_3 = UpCat(spatial_dims, fea[1], fea[0] * 3, fea[0], act, norm, bias, dropout, upsample, halves=False)
        self.upcat_1_3 = UpCat(spatial_dims, fea[2], fea[1] * 3, fea[1], act, norm, bias, dropout, upsample)
        self.upcat_0_4 = UpCat(spatial_dims, fea[1], fea[0] * 4, fea[5], act, norm, bias, dropout, upsample, halves=False)
        self.final_conv_0_1 = Conv['conv', spatial_dims](fea[0], out_channels, kernel_size=1)
        self.final_conv_0_2 = Conv['conv', spatial_dims](fea[0], out_channels, kernel_size=1)
        self.final_conv_0_3 = Conv['conv', spatial_dims](fea[0], out_channels, kernel_size=1)
        self.final_conv_0_4 = Conv['conv', spatial_dims](fea[5], out_channels, kernel_size=1)

    def forward(self, x: torch.Tensor):
        """
        Args:
            x: input should have spatially N dimensions
                ``(Batch, in_channels, dim_0[, dim_1, ..., dim_N-1])``, N is defined by `dimensions`.
                It is recommended to have ``dim_n % 16 == 0`` to ensure all maxpooling inputs have
                even edge lengths.

        Returns:
            A torch Tensor of "raw" predictions in shape
            ``(Batch, out_channels, dim_0[, dim_1, ..., dim_N-1])``.
        """
        x_0_0 = self.conv_0_0(x)
        x_1_0 = self.conv_1_0(x_0_0)
        x_0_1 = self.upcat_0_1(x_1_0, x_0_0)
        x_2_0 = self.conv_2_0(x_1_0)
        x_1_1 = self.upcat_1_1(x_2_0, x_1_0)
        x_0_2 = self.upcat_0_2(x_1_1, torch.cat([x_0_0, x_0_1], dim=1))
        x_3_0 = self.conv_3_0(x_2_0)
        x_2_1 = self.upcat_2_1(x_3_0, x_2_0)
        x_1_2 = self.upcat_1_2(x_2_1, torch.cat([x_1_0, x_1_1], dim=1))
        x_0_3 = self.upcat_0_3(x_1_2, torch.cat([x_0_0, x_0_1, x_0_2], dim=1))
        x_4_0 = self.conv_4_0(x_3_0)
        x_3_1 = self.upcat_3_1(x_4_0, x_3_0)
        x_2_2 = self.upcat_2_2(x_3_1, torch.cat([x_2_0, x_2_1], dim=1))
        x_1_3 = self.upcat_1_3(x_2_2, torch.cat([x_1_0, x_1_1, x_1_2], dim=1))
        x_0_4 = self.upcat_0_4(x_1_3, torch.cat([x_0_0, x_0_1, x_0_2, x_0_3], dim=1))
        output_0_1 = self.final_conv_0_1(x_0_1)
        output_0_2 = self.final_conv_0_2(x_0_2)
        output_0_3 = self.final_conv_0_3(x_0_3)
        output_0_4 = self.final_conv_0_4(x_0_4)
        if self.deep_supervision:
            output = [output_0_1, output_0_2, output_0_3, output_0_4]
        else:
            output = [output_0_4]
        return output


class _DenseLayer(nn.Sequential):

    def __init__(self, num_features: int, in_channels: int, out_channels: int, dropout_prob: float=0.0, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch', drop_first_norm_relu: int=0, kernel_size: int=3) ->None:
        """Dense Convolutional Block.

        References:
            Huang, Gao, et al. "Densely connected convolutional networks."
            Proceedings of the IEEE conference on computer vision and
            pattern recognition. 2017.

        Args:
            num_features: number of internal channels used for the layer
            in_channels: number of the input channels.
            out_channels: number of the output channels.
            dropout_prob: dropout rate after each dense layer.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
            drop_first_norm_relu - omits the first norm/relu for the first layer
            kernel_size: size of the kernel for >1 convolutions (dependent on mode)
        """
        super().__init__()
        self.layers = nn.Sequential()
        conv_type: Callable = Conv[Conv.CONV, 2]
        dropout_type: Callable = Dropout[Dropout.DROPOUT, 2]
        if not drop_first_norm_relu:
            self.layers.add_module('preact/bn', get_norm_layer(name=norm, spatial_dims=2, channels=in_channels))
            self.layers.add_module('preact/relu', get_act_layer(name=act))
        self.layers.add_module('conv1', conv_type(in_channels, num_features, kernel_size=1, padding=0, bias=False))
        self.layers.add_module('conv1/bn', get_norm_layer(name=norm, spatial_dims=2, channels=num_features))
        self.layers.add_module('conv1/relu', get_act_layer(name=act))
        if in_channels != 64 and drop_first_norm_relu:
            self.layers.add_module('conv2', conv_type(num_features, num_features, kernel_size=kernel_size, stride=2, padding=2, bias=False))
        else:
            self.layers.add_module('conv2', conv_type(num_features, num_features, kernel_size=kernel_size, padding=1, bias=False))
        self.layers.add_module('conv2/bn', get_norm_layer(name=norm, spatial_dims=2, channels=num_features))
        self.layers.add_module('conv2/relu', get_act_layer(name=act))
        self.layers.add_module('conv3', conv_type(num_features, out_channels, kernel_size=1, padding=0, bias=False))
        if dropout_prob > 0:
            self.layers.add_module('dropout', dropout_type(dropout_prob))


class _DenseBlock(nn.Sequential):

    def __init__(self, spatial_dims: int, layers: int, in_channels: int, bn_size: int, growth_rate: int, dropout_prob: float, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch') ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions of the input image.
            layers: number of layers in the block.
            in_channels: number of the input channel.
            bn_size: multiplicative factor for number of bottle neck layers.
                (i.e. bn_size * k features in the bottleneck layer)
            growth_rate: how many filters to add each layer (k in paper).
            dropout_prob: dropout rate after each dense layer.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
        """
        super().__init__()
        for i in range(layers):
            layer = _DenseLayer(spatial_dims, in_channels, growth_rate, bn_size, dropout_prob, act=act, norm=norm)
            in_channels += growth_rate
            self.add_module('denselayer%d' % (i + 1), layer)


class _Transition(nn.Sequential):

    def __init__(self, in_channels: int, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch') ->None:
        """
        Args:
            in_channels: number of the input channel.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
        """
        super().__init__()
        self.add_module('bn', get_norm_layer(name=norm, spatial_dims=2, channels=in_channels))
        self.add_module('relu', get_act_layer(name=act))


class DenseNet(nn.Module):
    """
    Densenet based on: `Densely Connected Convolutional Networks <https://arxiv.org/pdf/1608.06993.pdf>`_.
    Adapted from PyTorch Hub 2D version: https://pytorch.org/vision/stable/models.html#id16.
    This network is non-deterministic When `spatial_dims` is 3 and CUDA is enabled. Please check the link below
    for more details:
    https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms

    Args:
        spatial_dims: number of spatial dimensions of the input image.
        in_channels: number of the input channel.
        out_channels: number of the output classes.
        init_features: number of filters in the first convolution layer.
        growth_rate: how many filters to add each layer (k in paper).
        block_config: how many layers in each pooling block.
        bn_size: multiplicative factor for number of bottle neck layers.
            (i.e. bn_size * k features in the bottleneck layer)
        act: activation type and arguments. Defaults to relu.
        norm: feature normalization type and arguments. Defaults to batch norm.
        dropout_prob: dropout rate after each dense layer.
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, init_features: int=64, growth_rate: int=32, block_config: Sequence[int]=(6, 12, 24, 16), bn_size: int=4, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch', dropout_prob: float=0.0) ->None:
        super().__init__()
        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
        pool_type: Type[Union[nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        avg_pool_type: Type[Union[nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d]] = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        self.features = nn.Sequential(OrderedDict([('conv0', conv_type(in_channels, init_features, kernel_size=7, stride=2, padding=3, bias=False)), ('norm0', get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=init_features)), ('relu0', get_act_layer(name=act)), ('pool0', pool_type(kernel_size=3, stride=2, padding=1))]))
        in_channels = init_features
        for i, num_layers in enumerate(block_config):
            block = _DenseBlock(spatial_dims=spatial_dims, layers=num_layers, in_channels=in_channels, bn_size=bn_size, growth_rate=growth_rate, dropout_prob=dropout_prob, act=act, norm=norm)
            self.features.add_module(f'denseblock{i + 1}', block)
            in_channels += num_layers * growth_rate
            if i == len(block_config) - 1:
                self.features.add_module('norm5', get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=in_channels))
            else:
                _out_channels = in_channels // 2
                trans = _Transition(spatial_dims, in_channels=in_channels, out_channels=_out_channels, act=act, norm=norm)
                self.features.add_module(f'transition{i + 1}', trans)
                in_channels = _out_channels
        self.class_layers = nn.Sequential(OrderedDict([('relu', get_act_layer(name=act)), ('pool', avg_pool_type(1)), ('flatten', nn.Flatten(1)), ('out', nn.Linear(in_channels, out_channels))]))
        for m in self.modules():
            if isinstance(m, conv_type):
                nn.init.kaiming_normal_(torch.as_tensor(m.weight))
            elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                nn.init.constant_(torch.as_tensor(m.weight), 1)
                nn.init.constant_(torch.as_tensor(m.bias), 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(torch.as_tensor(m.bias), 0)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x = self.features(x)
        x = self.class_layers(x)
        return x


SE_NET_MODELS = {'senet154': 'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth', 'se_resnet50': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth', 'se_resnet101': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth', 'se_resnet152': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth', 'se_resnext50_32x4d': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth', 'se_resnext101_32x4d': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth'}


_, has_tqdm = optional_import('tqdm')


def _download_with_progress(url, filepath, progress: bool=True):
    """
    Retrieve file from `url` to `filepath`, optionally showing a progress bar.
    """
    try:
        if has_tqdm and progress:


            class TqdmUpTo(tqdm):
                """
                Provides `update_to(n)` which uses `tqdm.update(delta_n)`.
                Inspired by the example in https://github.com/tqdm/tqdm.
                """

                def update_to(self, b: int=1, bsize: int=1, tsize: Optional[int]=None):
                    """
                    Args:
                        b: number of blocks transferred so far, default: 1.
                        bsize: size of each block (in tqdm units), default: 1.
                        tsize: total size (in tqdm units). if None, remains unchanged.
                    """
                    if tsize is not None:
                        self.total = tsize
                    self.update(b * bsize - self.n)
            with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=_basename(filepath)) as t:
                urlretrieve(url, filepath, reporthook=t.update_to)
        else:
            if not has_tqdm and progress:
                warnings.warn('tqdm is not installed, will not show the downloading progress bar.')
            urlretrieve(url, filepath)
    except (URLError, HTTPError, ContentTooShortError, OSError) as e:
        logger.error(f'Download failed from {url} to {filepath}.')
        raise e


gdown, has_gdown = optional_import('gdown', '4.4')


def _load_state_dict(model: nn.Module, arch: str, progress: bool):
    """
    This function is used to load pretrained models.
    """
    model_url = look_up_option(arch, SE_NET_MODELS, None)
    if model_url is None:
        raise ValueError("only 'senet154', 'se_resnet50', 'se_resnet101',  'se_resnet152', 'se_resnext50_32x4d', " + 'and se_resnext101_32x4d are supported to load pretrained weights.')
    pattern_conv = re.compile('^(layer[1-4]\\.\\d\\.(?:conv)\\d\\.)(\\w*)$')
    pattern_bn = re.compile('^(layer[1-4]\\.\\d\\.)(?:bn)(\\d\\.)(\\w*)$')
    pattern_se = re.compile('^(layer[1-4]\\.\\d\\.)(?:se_module.fc1.)(\\w*)$')
    pattern_se2 = re.compile('^(layer[1-4]\\.\\d\\.)(?:se_module.fc2.)(\\w*)$')
    pattern_down_conv = re.compile('^(layer[1-4]\\.\\d\\.)(?:downsample.0.)(\\w*)$')
    pattern_down_bn = re.compile('^(layer[1-4]\\.\\d\\.)(?:downsample.1.)(\\w*)$')
    if isinstance(model_url, dict):
        download_url(model_url['url'], filepath=model_url['filename'])
        state_dict = torch.load(model_url['filename'], map_location=None)
    else:
        state_dict = load_state_dict_from_url(model_url, progress=progress)
    for key in list(state_dict.keys()):
        new_key = None
        if pattern_conv.match(key):
            new_key = re.sub(pattern_conv, '\\1conv.\\2', key)
        elif pattern_bn.match(key):
            new_key = re.sub(pattern_bn, '\\1conv\\2adn.N.\\3', key)
        elif pattern_se.match(key):
            state_dict[key] = state_dict[key].squeeze()
            new_key = re.sub(pattern_se, '\\1se_layer.fc.0.\\2', key)
        elif pattern_se2.match(key):
            state_dict[key] = state_dict[key].squeeze()
            new_key = re.sub(pattern_se2, '\\1se_layer.fc.2.\\2', key)
        elif pattern_down_conv.match(key):
            new_key = re.sub(pattern_down_conv, '\\1project.conv.\\2', key)
        elif pattern_down_bn.match(key):
            new_key = re.sub(pattern_down_bn, '\\1project.adn.N.\\2', key)
        if new_key:
            state_dict[new_key] = state_dict[key]
            del state_dict[key]
    model_dict = model.state_dict()
    state_dict = {k: v for k, v in state_dict.items() if k in model_dict and model_dict[k].shape == state_dict[k].shape}
    model_dict.update(state_dict)
    model.load_state_dict(model_dict)


class DenseNet121(DenseNet):
    """DenseNet121 with optional pretrained support when `spatial_dims` is 2."""

    def __init__(self, init_features: int=64, growth_rate: int=32, block_config: Sequence[int]=(6, 12, 24, 16), pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(init_features=init_features, growth_rate=growth_rate, block_config=block_config, **kwargs)
        if pretrained:
            if kwargs['spatial_dims'] > 2:
                raise NotImplementedError('Parameter `spatial_dims` is > 2 ; currently PyTorch Hub does notprovide pretrained models for more than two spatial dimensions.')
            _load_state_dict(self, 'densenet121', progress)


class DenseNet169(DenseNet):
    """DenseNet169 with optional pretrained support when `spatial_dims` is 2."""

    def __init__(self, init_features: int=64, growth_rate: int=32, block_config: Sequence[int]=(6, 12, 32, 32), pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(init_features=init_features, growth_rate=growth_rate, block_config=block_config, **kwargs)
        if pretrained:
            if kwargs['spatial_dims'] > 2:
                raise NotImplementedError('Parameter `spatial_dims` is > 2 ; currently PyTorch Hub does notprovide pretrained models for more than two spatial dimensions.')
            _load_state_dict(self, 'densenet169', progress)


class DenseNet201(DenseNet):
    """DenseNet201 with optional pretrained support when `spatial_dims` is 2."""

    def __init__(self, init_features: int=64, growth_rate: int=32, block_config: Sequence[int]=(6, 12, 48, 32), pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(init_features=init_features, growth_rate=growth_rate, block_config=block_config, **kwargs)
        if pretrained:
            if kwargs['spatial_dims'] > 2:
                raise NotImplementedError('Parameter `spatial_dims` is > 2 ; currently PyTorch Hub does notprovide pretrained models for more than two spatial dimensions.')
            _load_state_dict(self, 'densenet201', progress)


class DenseNet264(DenseNet):
    """DenseNet264"""

    def __init__(self, init_features: int=64, growth_rate: int=32, block_config: Sequence[int]=(6, 12, 64, 48), pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(init_features=init_features, growth_rate=growth_rate, block_config=block_config, **kwargs)
        if pretrained:
            raise NotImplementedError('Currently PyTorch Hub does not provide densenet264 pretrained models.')


class _IdentityWithRAMCost(nn.Identity):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.ram_cost = 0


class _CloseWithRAMCost(nn.Module):

    def __init__(self):
        super().__init__()
        self.ram_cost = 0

    def forward(self, x):
        return torch.tensor(0.0, requires_grad=False)


class _ActiConvNormBlockWithRAMCost(ActiConvNormBlock):
    """The class wraps monai layers with ram estimation. The ram_cost = total_ram/output_size is estimated.
    Here is the estimation:
     feature_size = output_size/out_channel
     total_ram = ram_cost * output_size
     total_ram = in_channel * feature_size (activation map) +
                 in_channel * feature_size (convolution map) +
                 out_channel * feature_size (normalization)
               = (2*in_channel + out_channel) * output_size/out_channel
     ram_cost = total_ram/output_size = 2 * in_channel/out_channel + 1
    """

    def __init__(self, in_channel: int, out_channel: int, kernel_size: int, padding: int, spatial_dims: int=3, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        super().__init__(in_channel, out_channel, kernel_size, padding, spatial_dims, act_name, norm_name)
        self.ram_cost = 1 + in_channel / out_channel * 2


class _P3DActiConvNormBlockWithRAMCost(P3DActiConvNormBlock):

    def __init__(self, in_channel: int, out_channel: int, kernel_size: int, padding: int, p3dmode: int=0, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        super().__init__(in_channel, out_channel, kernel_size, padding, p3dmode, act_name, norm_name)
        self.ram_cost = 2 + 2 * in_channel / out_channel


class _FactorizedIncreaseBlockWithRAMCost(FactorizedIncreaseBlock):

    def __init__(self, in_channel: int, out_channel: int, spatial_dims: int=3, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        super().__init__(in_channel, out_channel, spatial_dims, act_name, norm_name)
        self.ram_cost = 2 * in_channel / out_channel + 2


class _FactorizedReduceBlockWithRAMCost(FactorizedReduceBlock):

    def __init__(self, in_channel: int, out_channel: int, spatial_dims: int=3, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True})):
        super().__init__(in_channel, out_channel, spatial_dims, act_name, norm_name)
        self.ram_cost = in_channel / out_channel * 2 ** self._spatial_dims + 3


class MixedOp(nn.Module):
    """
    The weighted averaging of cell operations.
    Args:
        c: number of output channels.
        ops: a dictionary of operations. See also: ``Cell.OPS2D`` or ``Cell.OPS3D``.
        arch_code_c: binary cell operation code. It represents the operation results added to the output.
    """

    def __init__(self, c: int, ops: dict, arch_code_c=None):
        super().__init__()
        if arch_code_c is None:
            arch_code_c = np.ones(len(ops))
        self.ops = nn.ModuleList()
        for arch_c, op_name in zip(arch_code_c, ops):
            self.ops.append(_CloseWithRAMCost() if arch_c == 0 else ops[op_name](c))

    def forward(self, x: torch.Tensor, weight: torch.Tensor):
        """
        Args:
            x: input tensor.
            weight: learnable architecture weights for cell operations. arch_code_c are derived from it.
        Return:
            out: weighted average of the operation results.
        """
        out = 0.0
        weight = weight
        for idx, _op in enumerate(self.ops):
            out = out + _op(x) * weight[idx]
        return out


class DiNTS(nn.Module):
    """
    Reimplementation of DiNTS based on
    "DiNTS: Differentiable Neural Network Topology Search for 3D Medical Image Segmentation
    <https://arxiv.org/abs/2103.15954>".

    The model contains a pre-defined multi-resolution stem block (defined in this class) and a
    DiNTS space (defined in :py:class:`monai.networks.nets.TopologyInstance` and
    :py:class:`monai.networks.nets.TopologySearch`).

    The stem block is for: 1) input downsample and 2) output upsample to original size.
    The model downsamples the input image by 2 (if ``use_downsample=True``).
    The downsampled image is downsampled by [1, 2, 4, 8] times (``num_depths=4``) and used as input to the
    DiNTS search space (``TopologySearch``) or the DiNTS instance (``TopologyInstance``).

        - ``TopologyInstance`` is the final searched model. The initialization requires the searched architecture codes.
        - ``TopologySearch`` is a multi-path topology and cell operation search space.
          The architecture codes will be initialized as one.
        - ``TopologyConstruction`` is the parent class which constructs the instance and search space.

    To meet the requirements of the structure, the input size for each spatial dimension should be:
    divisible by 2 ** (num_depths + 1).

    Args:
        dints_space: DiNTS search space. The value should be instance of `TopologyInstance` or `TopologySearch`.
        in_channels: number of input image channels.
        num_classes: number of output segmentation classes.
        act_name: activation name, default to 'RELU'.
        norm_name: normalization used in convolution blocks. Default to `InstanceNorm`.
        spatial_dims: spatial 2D or 3D inputs.
        use_downsample: use downsample in the stem.
            If ``False``, the search space will be in resolution [1, 1/2, 1/4, 1/8],
            if ``True``, the search space will be in resolution [1/2, 1/4, 1/8, 1/16].
        node_a: node activation numpy matrix. Its shape is `(num_depths, num_blocks + 1)`.
            +1 for multi-resolution inputs.
            In model searching stage, ``node_a`` can be None. In deployment stage, ``node_a`` cannot be None.
    """

    def __init__(self, dints_space, in_channels: int, num_classes: int, act_name: Union[Tuple, str]='RELU', norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True}), spatial_dims: int=3, use_downsample: bool=True, node_a=None):
        super().__init__()
        self.dints_space = dints_space
        self.filter_nums = dints_space.filter_nums
        self.num_blocks = dints_space.num_blocks
        self.num_depths = dints_space.num_depths
        if spatial_dims not in (2, 3):
            raise NotImplementedError(f'Spatial dimensions {spatial_dims} is not supported.')
        self._spatial_dims = spatial_dims
        if node_a is None:
            self.node_a = torch.ones((self.num_blocks + 1, self.num_depths))
        else:
            self.node_a = node_a
        conv_type = Conv[Conv.CONV, spatial_dims]
        self.stem_down = nn.ModuleDict()
        self.stem_up = nn.ModuleDict()
        self.stem_finals = nn.Sequential(ActiConvNormBlock(self.filter_nums[0], self.filter_nums[0], act_name=act_name, norm_name=norm_name, spatial_dims=spatial_dims), conv_type(in_channels=self.filter_nums[0], out_channels=num_classes, kernel_size=1, stride=1, padding=0, groups=1, bias=True, dilation=1))
        mode = 'trilinear' if self._spatial_dims == 3 else 'bilinear'
        for res_idx in range(self.num_depths):
            if use_downsample:
                self.stem_down[str(res_idx)] = StemTS(nn.Upsample(scale_factor=1 / 2 ** res_idx, mode=mode, align_corners=True), conv_type(in_channels=in_channels, out_channels=self.filter_nums[res_idx], kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1), get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=self.filter_nums[res_idx]), get_act_layer(name=act_name), conv_type(in_channels=self.filter_nums[res_idx], out_channels=self.filter_nums[res_idx + 1], kernel_size=3, stride=2, padding=1, groups=1, bias=False, dilation=1), get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=self.filter_nums[res_idx + 1]))
                self.stem_up[str(res_idx)] = StemTS(get_act_layer(name=act_name), conv_type(in_channels=self.filter_nums[res_idx + 1], out_channels=self.filter_nums[res_idx], kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1), get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=self.filter_nums[res_idx]), nn.Upsample(scale_factor=2, mode=mode, align_corners=True))
            else:
                self.stem_down[str(res_idx)] = StemTS(nn.Upsample(scale_factor=1 / 2 ** res_idx, mode=mode, align_corners=True), conv_type(in_channels=in_channels, out_channels=self.filter_nums[res_idx], kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1), get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=self.filter_nums[res_idx]))
                self.stem_up[str(res_idx)] = StemTS(get_act_layer(name=act_name), conv_type(in_channels=self.filter_nums[res_idx], out_channels=self.filter_nums[max(res_idx - 1, 0)], kernel_size=3, stride=1, padding=1, groups=1, bias=False, dilation=1), get_norm_layer(name=norm_name, spatial_dims=spatial_dims, channels=self.filter_nums[max(res_idx - 1, 0)]), nn.Upsample(scale_factor=2 ** (res_idx != 0), mode=mode, align_corners=True))

    def weight_parameters(self):
        return [param for name, param in self.named_parameters()]

    def forward(self, x: torch.Tensor):
        """
        Prediction based on dynamic arch_code.

        Args:
            x: input tensor.
        """
        inputs = []
        for d in range(self.num_depths):
            _mod_w: StemInterface = self.stem_down[str(d)]
            x_out = _mod_w.forward(x)
            if self.node_a[0][d]:
                inputs.append(x_out)
            else:
                inputs.append(torch.zeros_like(x_out))
        outputs = self.dints_space(inputs)
        blk_idx = self.num_blocks - 1
        start = False
        _temp: torch.Tensor = torch.empty(0)
        for res_idx in range(self.num_depths - 1, -1, -1):
            _mod_up: StemInterface = self.stem_up[str(res_idx)]
            if start:
                _temp = _mod_up.forward(outputs[res_idx] + _temp)
            elif self.node_a[blk_idx + 1][res_idx]:
                start = True
                _temp = _mod_up.forward(outputs[res_idx])
        prediction = self.stem_finals(_temp)
        return prediction


def _dfs(node, paths):
    """use depth first search to find all path activation combination"""
    if node == paths:
        return [[0], [1]]
    child = _dfs(node + 1, paths)
    return [([0] + _) for _ in child] + [([1] + _) for _ in child]


csr_matrix, _ = optional_import('scipy.sparse', name='csr_matrix')


dijkstra, _ = optional_import('scipy.sparse.csgraph', name='dijkstra')


class DynUNetSkipLayer(nn.Module):
    """
    Defines a layer in the UNet topology which combines the downsample and upsample pathways with the skip connection.
    The member `next_layer` may refer to instances of this class or the final bottleneck layer at the bottom the UNet
    structure. The purpose of using a recursive class like this is to get around the Torchscript restrictions on
    looping over lists of layers and accumulating lists of output tensors which must be indexed. The `heads` list is
    shared amongst all the instances of this class and is used to store the output from the supervision heads during
    forward passes of the network.
    """
    heads: Optional[List[torch.Tensor]]

    def __init__(self, index, downsample, upsample, next_layer, heads=None, super_head=None):
        super().__init__()
        self.downsample = downsample
        self.next_layer = next_layer
        self.upsample = upsample
        self.super_head = super_head
        self.heads = heads
        self.index = index

    def forward(self, x):
        downout = self.downsample(x)
        nextout = self.next_layer(downout)
        upout = self.upsample(nextout, downout)
        if self.super_head is not None and self.heads is not None and self.index > 0:
            self.heads[self.index - 1] = self.super_head(upout)
        return upout


class DynUNet(nn.Module):
    """
    This reimplementation of a dynamic UNet (DynUNet) is based on:
    `Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>`_.
    `nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>`_.
    `Optimized U-Net for Brain Tumor Segmentation <https://arxiv.org/pdf/2110.03352.pdf>`_.

    This model is more flexible compared with ``monai.networks.nets.UNet`` in three
    places:

        - Residual connection is supported in conv blocks.
        - Anisotropic kernel sizes and strides can be used in each layers.
        - Deep supervision heads can be added.

    The model supports 2D or 3D inputs and is consisted with four kinds of blocks:
    one input block, `n` downsample blocks, one bottleneck and `n+1` upsample blocks. Where, `n>0`.
    The first and last kernel and stride values of the input sequences are used for input block and
    bottleneck respectively, and the rest value(s) are used for downsample and upsample blocks.
    Therefore, pleasure ensure that the length of input sequences (``kernel_size`` and ``strides``)
    is no less than 3 in order to have at least one downsample and upsample blocks.

    To meet the requirements of the structure, the input size for each spatial dimension should be divisible
    by the product of all strides in the corresponding dimension. In addition, the minimal spatial size should have
    at least one dimension that has twice the size of the product of all strides.
    For example, if `strides=((1, 2, 4), 2, 2, 1)`, the spatial size should be divisible by `(4, 8, 16)`,
    and the minimal spatial size is `(8, 8, 16)` or `(4, 16, 16)` or `(4, 8, 32)`.

    The output size for each spatial dimension equals to the input size of the corresponding dimension divided by the
    stride in strides[0].
    For example, if `strides=((1, 2, 4), 2, 2, 1)` and the input size is `(64, 32, 32)`, the output size is `(64, 16, 8)`.

    For backwards compatibility with old weights, please set `strict=False` when calling `load_state_dict`.

    Usage example with medical segmentation decathlon dataset is available at:
    https://github.com/Project-MONAI/tutorials/tree/master/modules/dynunet_pipeline.

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels.
        out_channels: number of output channels.
        kernel_size: convolution kernel size.
        strides: convolution strides for each blocks.
        upsample_kernel_size: convolution kernel size for transposed convolution layers. The values should
            equal to strides[1:].
        filters: number of output channels for each blocks. Different from nnU-Net, in this implementation we add
            this argument to make the network more flexible. As shown in the third reference, one way to determine
            this argument is like:
            ``[64, 96, 128, 192, 256, 384, 512, 768, 1024][: len(strides)]``.
            The above way is used in the network that wins task 1 in the BraTS21 Challenge.
            If not specified, the way which nnUNet used will be employed. Defaults to ``None``.
        dropout: dropout ratio. Defaults to no dropout.
        norm_name: feature normalization type and arguments. Defaults to ``INSTANCE``.
            `INSTANCE_NVFUSER` is a faster version of the instance norm layer, it can be used when:
            1) `spatial_dims=3`, 2) CUDA device is available, 3) `apex` is installed and 4) non-Windows OS is used.
        act_name: activation layer type and arguments. Defaults to ``leakyrelu``.
        deep_supervision: whether to add deep supervision head before output. Defaults to ``False``.
            If ``True``, in training mode, the forward function will output not only the final feature map
            (from `output_block`), but also the feature maps that come from the intermediate up sample layers.
            In order to unify the return type (the restriction of TorchScript), all intermediate
            feature maps are interpolated into the same size as the final feature map and stacked together
            (with a new dimension in the first axis)into one single tensor.
            For instance, if there are two intermediate feature maps with shapes: (1, 2, 16, 12) and
            (1, 2, 8, 6), and the final feature map has the shape (1, 2, 32, 24), then all intermediate feature maps
            will be interpolated into (1, 2, 32, 24), and the stacked tensor will has the shape (1, 3, 2, 32, 24).
            When calculating the loss, you can use torch.unbind to get all feature maps can compute the loss
            one by one with the ground truth, then do a weighted average for all losses to achieve the final loss.
        deep_supr_num: number of feature maps that will output during deep supervision head. The
            value should be larger than 0 and less than the number of up sample layers.
            Defaults to 1.
        res_block: whether to use residual connection based convolution blocks during the network.
            Defaults to ``False``.
        trans_bias: whether to set the bias parameter in transposed convolution layers. Defaults to ``False``.
    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Sequence[Union[Sequence[int], int]], strides: Sequence[Union[Sequence[int], int]], upsample_kernel_size: Sequence[Union[Sequence[int], int]], filters: Optional[Sequence[int]]=None, dropout: Optional[Union[Tuple, str, float]]=None, norm_name: Union[Tuple, str]=('INSTANCE', {'affine': True}), act_name: Union[Tuple, str]=('leakyrelu', {'inplace': True, 'negative_slope': 0.01}), deep_supervision: bool=False, deep_supr_num: int=1, res_block: bool=False, trans_bias: bool=False):
        super().__init__()
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.strides = strides
        self.upsample_kernel_size = upsample_kernel_size
        self.norm_name = norm_name
        self.act_name = act_name
        self.dropout = dropout
        self.conv_block = UnetResBlock if res_block else UnetBasicBlock
        self.trans_bias = trans_bias
        if filters is not None:
            self.filters = filters
            self.check_filters()
        else:
            self.filters = [min(2 ** (5 + i), 320 if spatial_dims == 3 else 512) for i in range(len(strides))]
        self.input_block = self.get_input_block()
        self.downsamples = self.get_downsamples()
        self.bottleneck = self.get_bottleneck()
        self.upsamples = self.get_upsamples()
        self.output_block = self.get_output_block(0)
        self.deep_supervision = deep_supervision
        self.deep_supr_num = deep_supr_num
        self.heads: List[torch.Tensor] = [torch.rand(1)] * self.deep_supr_num
        if self.deep_supervision:
            self.deep_supervision_heads = self.get_deep_supervision_heads()
            self.check_deep_supr_num()
        self.apply(self.initialize_weights)
        self.check_kernel_stride()

        def create_skips(index, downsamples, upsamples, bottleneck, superheads=None):
            """
            Construct the UNet topology as a sequence of skip layers terminating with the bottleneck layer. This is
            done recursively from the top down since a recursive nn.Module subclass is being used to be compatible
            with Torchscript. Initially the length of `downsamples` will be one more than that of `superheads`
            since the `input_block` is passed to this function as the first item in `downsamples`, however this
            shouldn't be associated with a supervision head.
            """
            if len(downsamples) != len(upsamples):
                raise ValueError(f'{len(downsamples)} != {len(upsamples)}')
            if len(downsamples) == 0:
                return bottleneck
            if superheads is None:
                next_layer = create_skips(1 + index, downsamples[1:], upsamples[1:], bottleneck)
                return DynUNetSkipLayer(index, downsample=downsamples[0], upsample=upsamples[0], next_layer=next_layer)
            super_head_flag = False
            if index == 0:
                rest_heads = superheads
            elif len(superheads) > 0:
                super_head_flag = True
                rest_heads = superheads[1:]
            else:
                rest_heads = nn.ModuleList()
            next_layer = create_skips(1 + index, downsamples[1:], upsamples[1:], bottleneck, superheads=rest_heads)
            if super_head_flag:
                return DynUNetSkipLayer(index, downsample=downsamples[0], upsample=upsamples[0], next_layer=next_layer, heads=self.heads, super_head=superheads[0])
            return DynUNetSkipLayer(index, downsample=downsamples[0], upsample=upsamples[0], next_layer=next_layer)
        if not self.deep_supervision:
            self.skip_layers = create_skips(0, [self.input_block] + list(self.downsamples), self.upsamples[::-1], self.bottleneck)
        else:
            self.skip_layers = create_skips(0, [self.input_block] + list(self.downsamples), self.upsamples[::-1], self.bottleneck, superheads=self.deep_supervision_heads)

    def check_kernel_stride(self):
        kernels, strides = self.kernel_size, self.strides
        error_msg = 'length of kernel_size and strides should be the same, and no less than 3.'
        if len(kernels) != len(strides) or len(kernels) < 3:
            raise ValueError(error_msg)
        for idx, k_i in enumerate(kernels):
            kernel, stride = k_i, strides[idx]
            if not isinstance(kernel, int):
                error_msg = f'length of kernel_size in block {idx} should be the same as spatial_dims.'
                if len(kernel) != self.spatial_dims:
                    raise ValueError(error_msg)
            if not isinstance(stride, int):
                error_msg = f'length of stride in block {idx} should be the same as spatial_dims.'
                if len(stride) != self.spatial_dims:
                    raise ValueError(error_msg)

    def check_deep_supr_num(self):
        deep_supr_num, strides = self.deep_supr_num, self.strides
        num_up_layers = len(strides) - 1
        if deep_supr_num >= num_up_layers:
            raise ValueError('deep_supr_num should be less than the number of up sample layers.')
        if deep_supr_num < 1:
            raise ValueError('deep_supr_num should be larger than 0.')

    def check_filters(self):
        filters = self.filters
        if len(filters) < len(self.strides):
            raise ValueError('length of filters should be no less than the length of strides.')
        else:
            self.filters = filters[:len(self.strides)]

    def forward(self, x):
        out = self.skip_layers(x)
        out = self.output_block(out)
        if self.training and self.deep_supervision:
            out_all = [out]
            for feature_map in self.heads:
                out_all.append(interpolate(feature_map, out.shape[2:]))
            return torch.stack(out_all, dim=1)
        return out

    def get_input_block(self):
        return self.conv_block(self.spatial_dims, self.in_channels, self.filters[0], self.kernel_size[0], self.strides[0], self.norm_name, self.act_name, dropout=self.dropout)

    def get_bottleneck(self):
        return self.conv_block(self.spatial_dims, self.filters[-2], self.filters[-1], self.kernel_size[-1], self.strides[-1], self.norm_name, self.act_name, dropout=self.dropout)

    def get_output_block(self, idx: int):
        return UnetOutBlock(self.spatial_dims, self.filters[idx], self.out_channels, dropout=self.dropout)

    def get_downsamples(self):
        inp, out = self.filters[:-2], self.filters[1:-1]
        strides, kernel_size = self.strides[1:-1], self.kernel_size[1:-1]
        return self.get_module_list(inp, out, kernel_size, strides, self.conv_block)

    def get_upsamples(self):
        inp, out = self.filters[1:][::-1], self.filters[:-1][::-1]
        strides, kernel_size = self.strides[1:][::-1], self.kernel_size[1:][::-1]
        upsample_kernel_size = self.upsample_kernel_size[::-1]
        return self.get_module_list(inp, out, kernel_size, strides, UnetUpBlock, upsample_kernel_size, trans_bias=self.trans_bias)

    def get_module_list(self, in_channels: Sequence[int], out_channels: Sequence[int], kernel_size: Sequence[Union[Sequence[int], int]], strides: Sequence[Union[Sequence[int], int]], conv_block: Type[nn.Module], upsample_kernel_size: Optional[Sequence[Union[Sequence[int], int]]]=None, trans_bias: bool=False):
        layers = []
        if upsample_kernel_size is not None:
            for in_c, out_c, kernel, stride, up_kernel in zip(in_channels, out_channels, kernel_size, strides, upsample_kernel_size):
                params = {'spatial_dims': self.spatial_dims, 'in_channels': in_c, 'out_channels': out_c, 'kernel_size': kernel, 'stride': stride, 'norm_name': self.norm_name, 'act_name': self.act_name, 'dropout': self.dropout, 'upsample_kernel_size': up_kernel, 'trans_bias': trans_bias}
                layer = conv_block(**params)
                layers.append(layer)
        else:
            for in_c, out_c, kernel, stride in zip(in_channels, out_channels, kernel_size, strides):
                params = {'spatial_dims': self.spatial_dims, 'in_channels': in_c, 'out_channels': out_c, 'kernel_size': kernel, 'stride': stride, 'norm_name': self.norm_name, 'act_name': self.act_name, 'dropout': self.dropout}
                layer = conv_block(**params)
                layers.append(layer)
        return nn.ModuleList(layers)

    def get_deep_supervision_heads(self):
        return nn.ModuleList([self.get_output_block(i + 1) for i in range(self.deep_supr_num)])

    @staticmethod
    def initialize_weights(module):
        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):
            module.weight = nn.init.kaiming_normal_(module.weight, a=0.01)
            if module.bias is not None:
                module.bias = nn.init.constant_(module.bias, 0)


def _calculate_output_image_size(input_image_size: List[int], stride: Union[int, Tuple[int]]):
    """
    Calculates the output image size when using _make_same_padder with a stride.
    Required for static padding.

    Args:
        input_image_size: input image/feature spatial size.
        stride: Conv2d operation"s stride.

    Returns:
        output_image_size: output image/feature spatial size.
    """
    if isinstance(stride, tuple):
        all_strides_equal = all(stride[0] == s for s in stride)
        if not all_strides_equal:
            raise ValueError(f'unequal strides are not possible, got {stride}')
        stride = stride[0]
    return [int(math.ceil(im_sz / stride)) for im_sz in input_image_size]


def _get_same_padding_conv_nd(image_size: List[int], kernel_size: Tuple[int, ...], dilation: Tuple[int, ...], stride: Tuple[int, ...]) ->List[int]:
    """
    Helper for getting padding (nn.ConstantPadNd) to be used to get SAME padding
    conv operations similar to Tensorflow's SAME padding.

    This function is generalized for MONAI's N-Dimensional spatial operations (e.g. Conv1D, Conv2D, Conv3D)

    Args:
        image_size: input image/feature spatial size.
        kernel_size: conv kernel's spatial size.
        dilation: conv dilation rate for Atrous conv.
        stride: stride for conv operation.

    Returns:
        paddings for ConstantPadNd padder to be used on input tensor to conv op.
    """
    num_dims = len(kernel_size)
    if len(dilation) == 1:
        dilation = dilation * num_dims
    if len(stride) == 1:
        stride = stride * num_dims
    _pad_size: List[int] = [max((math.ceil(_i_s / _s) - 1) * _s + (_k_s - 1) * _d + 1 - _i_s, 0) for _i_s, _k_s, _d, _s in zip(image_size, kernel_size, dilation, stride)]
    _paddings: List[Tuple[int, int]] = [(_p // 2, _p - _p // 2) for _p in _pad_size]
    _paddings_ret: List[int] = [outer for inner in reversed(_paddings) for outer in inner]
    return _paddings_ret


def _make_same_padder(conv_op: Union[nn.Conv1d, nn.Conv2d, nn.Conv3d], image_size: List[int]):
    """
    Helper for initializing ConstantPadNd with SAME padding similar to Tensorflow.
    Uses output of _get_same_padding_conv_nd() to get the padding size.

    This function is generalized for MONAI's N-Dimensional spatial operations (e.g. Conv1D, Conv2D, Conv3D)

    Args:
        conv_op: nn.ConvNd operation to extract parameters for op from
        image_size: input image/feature spatial size

    Returns:
        If padding required then nn.ConstandNd() padder initialized to paddings otherwise nn.Identity()
    """
    padding: List[int] = _get_same_padding_conv_nd(image_size, conv_op.kernel_size, conv_op.dilation, conv_op.stride)
    padder = Pad['constantpad', len(padding) // 2]
    if sum(padding) > 0:
        return padder(padding=padding, value=0.0)
    return nn.Identity()


def drop_connect(inputs: torch.Tensor, p: float, training: bool) ->torch.Tensor:
    """
    Drop connect layer that drops individual connections.
    Differs from dropout as dropconnect drops connections instead of whole neurons as in dropout.

    Based on `Deep Networks with Stochastic Depth <https://arxiv.org/pdf/1603.09382.pdf>`_.
    Adapted from `Official Tensorflow EfficientNet utils
    <https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/utils.py>`_.

    This function is generalized for MONAI's N-Dimensional spatial activations
    e.g. 1D activations [B, C, H], 2D activations [B, C, H, W] and 3D activations [B, C, H, W, D]

    Args:
        inputs: input tensor with [B, C, dim_1, dim_2, ..., dim_N] where N=spatial_dims.
        p: probability to use for dropping connections.
        training: whether in training or evaluation mode.

    Returns:
        output: output tensor after applying drop connection.
    """
    if p < 0.0 or p > 1.0:
        raise ValueError(f'p must be in range of [0, 1], found {p}')
    if not training:
        return inputs
    batch_size: int = inputs.shape[0]
    keep_prob: float = 1 - p
    num_dims: int = len(inputs.shape) - 2
    random_tensor_shape: List[int] = [batch_size, 1] + [1] * num_dims
    random_tensor: torch.Tensor = torch.rand(random_tensor_shape, dtype=inputs.dtype, device=inputs.device)
    random_tensor += keep_prob
    binary_tensor: torch.Tensor = torch.floor(random_tensor)
    output: torch.Tensor = inputs / keep_prob * binary_tensor
    return output


class MBConvBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: int, stride: int, image_size: List[int], expand_ratio: int, se_ratio: Optional[float], id_skip: Optional[bool]=True, norm: Union[str, tuple]=('batch', {'eps': 0.001, 'momentum': 0.01}), drop_connect_rate: Optional[float]=0.2) ->None:
        """
        Mobile Inverted Residual Bottleneck Block.

        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernel_size: size of the kernel for conv ops.
            stride: stride to use for conv ops.
            image_size: input image resolution.
            expand_ratio: expansion ratio for inverted bottleneck.
            se_ratio: squeeze-excitation ratio for se layers.
            id_skip: whether to use skip connection.
            norm: feature normalization type and arguments. Defaults to batch norm.
            drop_connect_rate: dropconnect rate for drop connection (individual weights) layers.

        References:
            [1] https://arxiv.org/abs/1704.04861 (MobileNet v1)
            [2] https://arxiv.org/abs/1801.04381 (MobileNet v2)
            [3] https://arxiv.org/abs/1905.02244 (MobileNet v3)
        """
        super().__init__()
        conv_type = Conv['conv', spatial_dims]
        adaptivepool_type = Pool['adaptiveavg', spatial_dims]
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.id_skip = id_skip
        self.stride = stride
        self.expand_ratio = expand_ratio
        self.drop_connect_rate = drop_connect_rate
        if se_ratio is not None and 0.0 < se_ratio <= 1.0:
            self.has_se = True
            self.se_ratio = se_ratio
        else:
            self.has_se = False
        inp = in_channels
        oup = in_channels * expand_ratio
        if self.expand_ratio != 1:
            self._expand_conv = conv_type(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)
            self._expand_conv_padding = _make_same_padder(self._expand_conv, image_size)
            self._bn0 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=oup)
        else:
            self._expand_conv = nn.Identity()
            self._expand_conv_padding = nn.Identity()
            self._bn0 = nn.Identity()
        self._depthwise_conv = conv_type(in_channels=oup, out_channels=oup, groups=oup, kernel_size=kernel_size, stride=self.stride, bias=False)
        self._depthwise_conv_padding = _make_same_padder(self._depthwise_conv, image_size)
        self._bn1 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=oup)
        image_size = _calculate_output_image_size(image_size, self.stride)
        if self.has_se:
            self._se_adaptpool = adaptivepool_type(1)
            num_squeezed_channels = max(1, int(in_channels * self.se_ratio))
            self._se_reduce = conv_type(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)
            self._se_reduce_padding = _make_same_padder(self._se_reduce, [1, 1])
            self._se_expand = conv_type(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)
            self._se_expand_padding = _make_same_padder(self._se_expand, [1, 1])
        final_oup = out_channels
        self._project_conv = conv_type(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)
        self._project_conv_padding = _make_same_padder(self._project_conv, image_size)
        self._bn2 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=final_oup)
        self._swish = Act['memswish'](inplace=True)

    def forward(self, inputs: torch.Tensor):
        """MBConvBlock"s forward function.

        Args:
            inputs: Input tensor.

        Returns:
            Output of this block after processing.
        """
        x = inputs
        if self.expand_ratio != 1:
            x = self._expand_conv(self._expand_conv_padding(x))
            x = self._bn0(x)
            x = self._swish(x)
        x = self._depthwise_conv(self._depthwise_conv_padding(x))
        x = self._bn1(x)
        x = self._swish(x)
        if self.has_se:
            x_squeezed = self._se_adaptpool(x)
            x_squeezed = self._se_reduce(self._se_reduce_padding(x_squeezed))
            x_squeezed = self._swish(x_squeezed)
            x_squeezed = self._se_expand(self._se_expand_padding(x_squeezed))
            x = torch.sigmoid(x_squeezed) * x
        x = self._project_conv(self._project_conv_padding(x))
        x = self._bn2(x)
        if self.id_skip and self.stride == 1 and self.in_channels == self.out_channels:
            if self.drop_connect_rate:
                x = drop_connect(x, p=self.drop_connect_rate, training=self.training)
            x = x + inputs
        return x

    def set_swish(self, memory_efficient: bool=True) ->None:
        """Sets swish function as memory efficient (for training) or standard (for export).

        Args:
            memory_efficient (bool): Whether to use memory-efficient version of swish.
        """
        self._swish = Act['memswish'](inplace=True) if memory_efficient else Act['swish'](alpha=1.0)


class BlockArgs(NamedTuple):
    """
    BlockArgs object to assist in decoding string notation
        of arguments for MBConvBlock definition.
    """
    num_repeat: int
    kernel_size: int
    stride: int
    expand_ratio: int
    input_filters: int
    output_filters: int
    id_skip: bool
    se_ratio: Optional[float] = None

    @staticmethod
    def from_string(block_string: str):
        """
        Get a BlockArgs object from a string notation of arguments.

        Args:
            block_string (str): A string notation of arguments.
                                Examples: "r1_k3_s11_e1_i32_o16_se0.25".

        Returns:
            BlockArgs: namedtuple defined at the top of this function.
        """
        ops = block_string.split('_')
        options = {}
        for op in ops:
            splits = re.split('(\\d.*)', op)
            if len(splits) >= 2:
                key, value = splits[:2]
                options[key] = value
        stride_check = 's' in options and len(options['s']) == 1 or len(options['s']) == 2 and options['s'][0] == options['s'][1] or len(options['s']) == 3 and options['s'][0] == options['s'][1] and options['s'][0] == options['s'][2]
        if not stride_check:
            raise ValueError('invalid stride option received')
        return BlockArgs(num_repeat=int(options['r']), kernel_size=int(options['k']), stride=int(options['s'][0]), expand_ratio=int(options['e']), input_filters=int(options['i']), output_filters=int(options['o']), id_skip='noskip' not in block_string, se_ratio=float(options['se']) if 'se' in options else None)

    def to_string(self):
        """
        Return a block string notation for current BlockArgs object

        Returns:
            A string notation of BlockArgs object arguments.
                Example: "r1_k3_s11_e1_i32_o16_se0.25_noskip".
        """
        string = f'r{self.num_repeat}_k{self.kernel_size}_s{self.stride}{self.stride}_e{self.expand_ratio}_i{self.input_filters}_o{self.output_filters}_se{self.se_ratio}'
        if not self.id_skip:
            string += '_noskip'
        return string


def _round_filters(filters: int, width_coefficient: Optional[float], depth_divisor: float) ->int:
    """
    Calculate and round number of filters based on width coefficient multiplier and depth divisor.

    Args:
        filters: number of input filters.
        width_coefficient: width coefficient for model.
        depth_divisor: depth divisor to use.

    Returns:
        new_filters: new number of filters after calculation.
    """
    if not width_coefficient:
        return filters
    multiplier: float = width_coefficient
    divisor: float = depth_divisor
    filters_float: float = filters * multiplier
    new_filters: float = max(divisor, int(filters_float + divisor / 2) // divisor * divisor)
    if new_filters < 0.9 * filters_float:
        new_filters += divisor
    return int(new_filters)


def _round_repeats(repeats: int, depth_coefficient: Optional[float]) ->int:
    """
    Re-calculate module's repeat number of a block based on depth coefficient multiplier.

    Args:
        repeats: number of original repeats.
        depth_coefficient: depth coefficient for model.

    Returns:
        new repeat: new number of repeat after calculating.
    """
    if not depth_coefficient:
        return repeats
    return int(math.ceil(depth_coefficient * repeats))


class EfficientNet(nn.Module):

    def __init__(self, blocks_args_str: List[str], spatial_dims: int=2, in_channels: int=3, num_classes: int=1000, width_coefficient: float=1.0, depth_coefficient: float=1.0, dropout_rate: float=0.2, image_size: int=224, norm: Union[str, tuple]=('batch', {'eps': 0.001, 'momentum': 0.01}), drop_connect_rate: float=0.2, depth_divisor: int=8) ->None:
        """
        EfficientNet based on `Rethinking Model Scaling for Convolutional Neural Networks <https://arxiv.org/pdf/1905.11946.pdf>`_.
        Adapted from `EfficientNet-PyTorch <https://github.com/lukemelas/EfficientNet-PyTorch>`_.

        Args:
            blocks_args_str: block definitions.
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            num_classes: number of output classes.
            width_coefficient: width multiplier coefficient (w in paper).
            depth_coefficient: depth multiplier coefficient (d in paper).
            dropout_rate: dropout rate for dropout layers.
            image_size: input image resolution.
            norm: feature normalization type and arguments.
            drop_connect_rate: dropconnect rate for drop connection (individual weights) layers.
            depth_divisor: depth divisor for channel rounding.

        """
        super().__init__()
        if spatial_dims not in (1, 2, 3):
            raise ValueError('spatial_dims can only be 1, 2 or 3.')
        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv['conv', spatial_dims]
        adaptivepool_type: Type[Union[nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d]] = Pool['adaptiveavg', spatial_dims]
        blocks_args = [BlockArgs.from_string(s) for s in blocks_args_str]
        if not isinstance(blocks_args, list):
            raise ValueError('blocks_args must be a list')
        if blocks_args == []:
            raise ValueError('block_args must be non-empty')
        self._blocks_args = blocks_args
        self.num_classes = num_classes
        self.in_channels = in_channels
        self.drop_connect_rate = drop_connect_rate
        current_image_size = [image_size] * spatial_dims
        stride = 2
        out_channels = _round_filters(32, width_coefficient, depth_divisor)
        self._conv_stem = conv_type(self.in_channels, out_channels, kernel_size=3, stride=stride, bias=False)
        self._conv_stem_padding = _make_same_padder(self._conv_stem, current_image_size)
        self._bn0 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=out_channels)
        current_image_size = _calculate_output_image_size(current_image_size, stride)
        num_blocks = 0
        self._blocks = nn.Sequential()
        self.extract_stacks = []
        for idx, block_args in enumerate(self._blocks_args):
            block_args = block_args._replace(input_filters=_round_filters(block_args.input_filters, width_coefficient, depth_divisor), output_filters=_round_filters(block_args.output_filters, width_coefficient, depth_divisor), num_repeat=_round_repeats(block_args.num_repeat, depth_coefficient))
            self._blocks_args[idx] = block_args
            num_blocks += block_args.num_repeat
            if block_args.stride > 1:
                self.extract_stacks.append(idx)
        self.extract_stacks.append(len(self._blocks_args))
        idx = 0
        for stack_idx, block_args in enumerate(self._blocks_args):
            blk_drop_connect_rate = self.drop_connect_rate
            if blk_drop_connect_rate:
                blk_drop_connect_rate *= float(idx) / num_blocks
            sub_stack = nn.Sequential()
            sub_stack.add_module(str(idx), MBConvBlock(spatial_dims=spatial_dims, in_channels=block_args.input_filters, out_channels=block_args.output_filters, kernel_size=block_args.kernel_size, stride=block_args.stride, image_size=current_image_size, expand_ratio=block_args.expand_ratio, se_ratio=block_args.se_ratio, id_skip=block_args.id_skip, norm=norm, drop_connect_rate=blk_drop_connect_rate))
            idx += 1
            current_image_size = _calculate_output_image_size(current_image_size, block_args.stride)
            if block_args.num_repeat > 1:
                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)
            for _ in range(block_args.num_repeat - 1):
                blk_drop_connect_rate = self.drop_connect_rate
                if blk_drop_connect_rate:
                    blk_drop_connect_rate *= float(idx) / num_blocks
                sub_stack.add_module(str(idx), MBConvBlock(spatial_dims=spatial_dims, in_channels=block_args.input_filters, out_channels=block_args.output_filters, kernel_size=block_args.kernel_size, stride=block_args.stride, image_size=current_image_size, expand_ratio=block_args.expand_ratio, se_ratio=block_args.se_ratio, id_skip=block_args.id_skip, norm=norm, drop_connect_rate=blk_drop_connect_rate))
                idx += 1
            self._blocks.add_module(str(stack_idx), sub_stack)
        if idx != num_blocks:
            raise ValueError('total number of blocks created != num_blocks')
        head_in_channels = block_args.output_filters
        out_channels = _round_filters(1280, width_coefficient, depth_divisor)
        self._conv_head = conv_type(head_in_channels, out_channels, kernel_size=1, bias=False)
        self._conv_head_padding = _make_same_padder(self._conv_head, current_image_size)
        self._bn1 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=out_channels)
        self._avg_pooling = adaptivepool_type(1)
        self._dropout = nn.Dropout(dropout_rate)
        self._fc = nn.Linear(out_channels, self.num_classes)
        self._swish = Act['memswish']()
        self._initialize_weights()

    def set_swish(self, memory_efficient: bool=True) ->None:
        """
        Sets swish function as memory efficient (for training) or standard (for JIT export).

        Args:
            memory_efficient: whether to use memory-efficient version of swish.

        """
        self._swish = Act['memswish']() if memory_efficient else Act['swish'](alpha=1.0)
        for sub_stack in self._blocks:
            for block in sub_stack:
                block.set_swish(memory_efficient)

    def forward(self, inputs: torch.Tensor):
        """
        Args:
            inputs: input should have spatially N dimensions
            ``(Batch, in_channels, dim_0[, dim_1, ..., dim_N])``, N is defined by `dimensions`.

        Returns:
            a torch Tensor of classification prediction in shape ``(Batch, num_classes)``.
        """
        x = self._conv_stem(self._conv_stem_padding(inputs))
        x = self._swish(self._bn0(x))
        x = self._blocks(x)
        x = self._conv_head(self._conv_head_padding(x))
        x = self._swish(self._bn1(x))
        x = self._avg_pooling(x)
        x = x.flatten(start_dim=1)
        x = self._dropout(x)
        x = self._fc(x)
        return x

    def _initialize_weights(self) ->None:
        """
        Args:
            None, initializes weights for conv/linear/batchnorm layers
            following weight init methods from
            `official Tensorflow EfficientNet implementation
            <https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py#L61>`_.
            Adapted from `EfficientNet-PyTorch's init method
            <https://github.com/rwightman/gen-efficientnet-pytorch/blob/master/geffnet/efficientnet_builder.py>`_.
        """
        for _, m in self.named_modules():
            if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
                fan_out = reduce(operator.mul, m.kernel_size, 1) * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
                if m.bias is not None:
                    m.bias.data.zero_()
            elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                m.weight.data.fill_(1.0)
                m.bias.data.zero_()
            elif isinstance(m, nn.Linear):
                fan_out = m.weight.size(0)
                fan_in = 0
                init_range = 1.0 / math.sqrt(fan_in + fan_out)
                m.weight.data.uniform_(-init_range, init_range)
                m.bias.data.zero_()


efficientnet_params = {'efficientnet-b0': (1.0, 1.0, 224, 0.2, 0.2), 'efficientnet-b1': (1.0, 1.1, 240, 0.2, 0.2), 'efficientnet-b2': (1.1, 1.2, 260, 0.3, 0.2), 'efficientnet-b3': (1.2, 1.4, 300, 0.3, 0.2), 'efficientnet-b4': (1.4, 1.8, 380, 0.4, 0.2), 'efficientnet-b5': (1.6, 2.2, 456, 0.4, 0.2), 'efficientnet-b6': (1.8, 2.6, 528, 0.5, 0.2), 'efficientnet-b7': (2.0, 3.1, 600, 0.5, 0.2), 'efficientnet-b8': (2.2, 3.6, 672, 0.5, 0.2), 'efficientnet-l2': (4.3, 5.3, 800, 0.5, 0.2)}


class EfficientNetBN(EfficientNet):

    def __init__(self, model_name: str, pretrained: bool=True, progress: bool=True, spatial_dims: int=2, in_channels: int=3, num_classes: int=1000, norm: Union[str, tuple]=('batch', {'eps': 0.001, 'momentum': 0.01}), adv_prop: bool=False) ->None:
        """
        Generic wrapper around EfficientNet, used to initialize EfficientNet-B0 to EfficientNet-B7 models
        model_name is mandatory argument as there is no EfficientNetBN itself,
        it needs the N in [0, 1, 2, 3, 4, 5, 6, 7, 8] to be a model

        Args:
            model_name: name of model to initialize, can be from [efficientnet-b0, ..., efficientnet-b8, efficientnet-l2].
            pretrained: whether to initialize pretrained ImageNet weights, only available for spatial_dims=2 and batch
                norm is used.
            progress: whether to show download progress for pretrained weights download.
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            num_classes: number of output classes.
            norm: feature normalization type and arguments.
            adv_prop: whether to use weights trained with adversarial examples.
                This argument only works when `pretrained` is `True`.

        Examples::

            # for pretrained spatial 2D ImageNet
            >>> image_size = get_efficientnet_image_size("efficientnet-b0")
            >>> inputs = torch.rand(1, 3, image_size, image_size)
            >>> model = EfficientNetBN("efficientnet-b0", pretrained=True)
            >>> model.eval()
            >>> outputs = model(inputs)

            # create spatial 2D
            >>> model = EfficientNetBN("efficientnet-b0", spatial_dims=2)

            # create spatial 3D
            >>> model = EfficientNetBN("efficientnet-b0", spatial_dims=3)

            # create EfficientNetB7 for spatial 2D
            >>> model = EfficientNetBN("efficientnet-b7", spatial_dims=2)

        """
        blocks_args_str = ['r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25', 'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25', 'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25', 'r1_k3_s11_e6_i192_o320_se0.25']
        if model_name not in efficientnet_params:
            model_name_string = ', '.join(efficientnet_params.keys())
            raise ValueError(f'invalid model_name {model_name} found, must be one of {model_name_string} ')
        weight_coeff, depth_coeff, image_size, dropout_rate, dropconnect_rate = efficientnet_params[model_name]
        super().__init__(blocks_args_str=blocks_args_str, spatial_dims=spatial_dims, in_channels=in_channels, num_classes=num_classes, width_coefficient=weight_coeff, depth_coefficient=depth_coeff, dropout_rate=dropout_rate, image_size=image_size, drop_connect_rate=dropconnect_rate, norm=norm)
        if pretrained and spatial_dims == 2:
            _load_state_dict(self, model_name, progress, adv_prop)


class EfficientNetBNFeatures(EfficientNet):

    def __init__(self, model_name: str, pretrained: bool=True, progress: bool=True, spatial_dims: int=2, in_channels: int=3, num_classes: int=1000, norm: Union[str, tuple]=('batch', {'eps': 0.001, 'momentum': 0.01}), adv_prop: bool=False) ->None:
        """
        Initialize EfficientNet-B0 to EfficientNet-B7 models as a backbone, the backbone can
        be used as an encoder for segmentation and objection models.
        Compared with the class `EfficientNetBN`, the only different place is the forward function.

        This class refers to `PyTorch image models <https://github.com/rwightman/pytorch-image-models>`_.

        """
        blocks_args_str = ['r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25', 'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25', 'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25', 'r1_k3_s11_e6_i192_o320_se0.25']
        if model_name not in efficientnet_params:
            model_name_string = ', '.join(efficientnet_params.keys())
            raise ValueError(f'invalid model_name {model_name} found, must be one of {model_name_string} ')
        weight_coeff, depth_coeff, image_size, dropout_rate, dropconnect_rate = efficientnet_params[model_name]
        super().__init__(blocks_args_str=blocks_args_str, spatial_dims=spatial_dims, in_channels=in_channels, num_classes=num_classes, width_coefficient=weight_coeff, depth_coefficient=depth_coeff, dropout_rate=dropout_rate, image_size=image_size, drop_connect_rate=dropconnect_rate, norm=norm)
        if pretrained and spatial_dims == 2:
            _load_state_dict(self, model_name, progress, adv_prop)

    def forward(self, inputs: torch.Tensor):
        """
        Args:
            inputs: input should have spatially N dimensions
            ``(Batch, in_channels, dim_0[, dim_1, ..., dim_N])``, N is defined by `dimensions`.

        Returns:
            a list of torch Tensors.
        """
        x = self._conv_stem(self._conv_stem_padding(inputs))
        x = self._swish(self._bn0(x))
        features = []
        if 0 in self.extract_stacks:
            features.append(x)
        for i, block in enumerate(self._blocks):
            x = block(x)
            if i + 1 in self.extract_stacks:
                features.append(x)
        return features


class UNetDecoder(nn.Module):
    """
    UNet Decoder.
    This class refers to `segmentation_models.pytorch
    <https://github.com/qubvel/segmentation_models.pytorch>`_.

    Args:
        spatial_dims: number of spatial dimensions.
        encoder_channels: number of output channels for all feature maps in encoder.
            `len(encoder_channels)` should be no less than 2.
        decoder_channels: number of output channels for all feature maps in decoder.
            `len(decoder_channels)` should equal to `len(encoder_channels) - 1`.
        act: activation type and arguments.
        norm: feature normalization type and arguments.
        dropout: dropout ratio.
        bias: whether to have a bias term in convolution blocks in this decoder.
        upsample: upsampling mode, available options are
            ``"deconv"``, ``"pixelshuffle"``, ``"nontrainable"``.
        pre_conv: a conv block applied before upsampling.
            Only used in the "nontrainable" or "pixelshuffle" mode.
        interp_mode: {``"nearest"``, ``"linear"``, ``"bilinear"``, ``"bicubic"``, ``"trilinear"``}
            Only used in the "nontrainable" mode.
        align_corners: set the align_corners parameter for upsample. Defaults to True.
            Only used in the "nontrainable" mode.
        is_pad: whether to pad upsampling features to fit the encoder spatial dims.

    """

    def __init__(self, spatial_dims: int, encoder_channels: Sequence[int], decoder_channels: Sequence[int], act: Union[str, tuple], norm: Union[str, tuple], dropout: Union[float, tuple], bias: bool, upsample: str, pre_conv: Optional[str], interp_mode: str, align_corners: Optional[bool], is_pad: bool):
        super().__init__()
        if len(encoder_channels) < 2:
            raise ValueError('the length of `encoder_channels` should be no less than 2.')
        if len(decoder_channels) != len(encoder_channels) - 1:
            raise ValueError('`len(decoder_channels)` should equal to `len(encoder_channels) - 1`.')
        in_channels = [encoder_channels[-1]] + list(decoder_channels[:-1])
        skip_channels = list(encoder_channels[1:-1][::-1]) + [0]
        halves = [True] * (len(skip_channels) - 1)
        halves.append(False)
        blocks = []
        for in_chn, skip_chn, out_chn, halve in zip(in_channels, skip_channels, decoder_channels, halves):
            blocks.append(UpCat(spatial_dims=spatial_dims, in_chns=in_chn, cat_chns=skip_chn, out_chns=out_chn, act=act, norm=norm, dropout=dropout, bias=bias, upsample=upsample, pre_conv=pre_conv, interp_mode=interp_mode, align_corners=align_corners, halves=halve, is_pad=is_pad))
        self.blocks = nn.ModuleList(blocks)

    def forward(self, features: List[torch.Tensor], skip_connect: int=4):
        skips = features[:-1][::-1]
        features = features[1:][::-1]
        x = features[0]
        for i, block in enumerate(self.blocks):
            if i < skip_connect:
                skip = skips[i]
            else:
                skip = None
            x = block(x, skip)
        return x


class SegmentationHead(nn.Sequential):
    """
    Segmentation head.
    This class refers to `segmentation_models.pytorch
    <https://github.com/qubvel/segmentation_models.pytorch>`_.

    Args:
        spatial_dims: number of spatial dimensions.
        in_channels: number of input channels for the block.
        out_channels: number of output channels for the block.
        kernel_size: kernel size for the conv layer.
        act: activation type and arguments.
        scale_factor: multiplier for spatial size. Has to match input size if it is a tuple.

    """

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernel_size: int=3, act: Optional[Union[Tuple, str]]=None, scale_factor: float=1.0):
        conv_layer = Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2)
        up_layer: nn.Module = nn.Identity()
        if scale_factor > 1.0:
            up_layer = UpSample(spatial_dims=spatial_dims, scale_factor=scale_factor, mode='nontrainable', pre_conv=None, interp_mode=InterpolateMode.LINEAR)
        if act is not None:
            act_layer = get_act_layer(act)
        else:
            act_layer = nn.Identity()
        super().__init__(conv_layer, up_layer, act_layer)


class FlexUNetEncoderRegister:
    """
    A register to regist backbones for the flexible unet. All backbones can be found in
    register_dict. Please notice each output of backbone must be 2x downsample in spatial
    dimension of last output. For example, if given a 512x256 2D image and a backbone with
    4 outputs. Then spatial size of each encoder output should be 256x128, 128x64, 64x32
    and 32x16.
    """

    def __init__(self):
        self.register_dict = {}

    def register_class(self, name: Union[Type, str]):
        """
        Register a given class to the encoder dict. Please notice that input class must be a
        subclass of BaseEncoder.
        """
        if isinstance(name, str):
            tmp_name, has_built_in = optional_import('monai.networks.nets', name=f'{name}')
            if not has_built_in:
                tmp_name = locate(f'{name}')
            name = tmp_name
            if not isinstance(name, type):
                raise ValueError(f'Cannot find {name} class.')
        if not issubclass(name, BaseEncoder):
            warnings.warn(f'{name} would better be derived from monai.networks.blocks.BaseEncoder or implement all interfaces specified by it.')
        name_string_list = name.get_encoder_names()
        feature_number_list = name.num_outputs()
        feature_channel_list = name.num_channels_per_output()
        parameter_list = name.get_encoder_parameters()
        assert len(name_string_list) == len(feature_number_list) == len(feature_channel_list) == len(parameter_list)
        for cnt, name_string in enumerate(name_string_list):
            cur_dict = {'type': name, 'feature_number': feature_number_list[cnt], 'feature_channel': feature_channel_list[cnt], 'parameter': parameter_list[cnt]}
            self.register_dict[name_string] = cur_dict


FLEXUNET_BACKBONE = FlexUNetEncoderRegister()


class FlexibleUNet(nn.Module):
    """
    A flexible implementation of UNet-like encoder-decoder architecture.
    """

    def __init__(self, in_channels: int, out_channels: int, backbone: str, pretrained: bool=False, decoder_channels: Tuple=(256, 128, 64, 32, 16), spatial_dims: int=2, norm: Union[str, tuple]=('batch', {'eps': 0.001, 'momentum': 0.1}), act: Union[str, tuple]=('relu', {'inplace': True}), dropout: Union[float, tuple]=0.0, decoder_bias: bool=False, upsample: str='nontrainable', interp_mode: str='nearest', is_pad: bool=True) ->None:
        """
        A flexible implement of UNet, in which the backbone/encoder can be replaced with
        any efficient network. Currently the input must have a 2 or 3 spatial dimension
        and the spatial size of each dimension must be a multiple of 32 if is_pad parameter
        is False.
        Please notice each output of backbone must be 2x downsample in spatial dimension
        of last output. For example, if given a 512x256 2D image and a backbone with 4 outputs.
        Spatial size of each encoder output should be 256x128, 128x64, 64x32 and 32x16.

        Args:
            in_channels: number of input channels.
            out_channels: number of output channels.
            backbone: name of backbones to initialize, only support efficientnet right now,
                can be from [efficientnet-b0,..., efficientnet-b8, efficientnet-l2].
            pretrained: whether to initialize pretrained ImageNet weights, only available
                for spatial_dims=2 and batch norm is used, default to False.
            decoder_channels: number of output channels for all feature maps in decoder.
                `len(decoder_channels)` should equal to `len(encoder_channels) - 1`,default
                to (256, 128, 64, 32, 16).
            spatial_dims: number of spatial dimensions, default to 2.
            norm: normalization type and arguments, default to ("batch", {"eps": 1e-3,
                "momentum": 0.1}).
            act: activation type and arguments, default to ("relu", {"inplace": True}).
            dropout: dropout ratio, default to 0.0.
            decoder_bias: whether to have a bias term in decoder's convolution blocks.
            upsample: upsampling mode, available options are``"deconv"``, ``"pixelshuffle"``,
                ``"nontrainable"``.
            interp_mode: {``"nearest"``, ``"linear"``, ``"bilinear"``, ``"bicubic"``, ``"trilinear"``}
                Only used in the "nontrainable" mode.
            is_pad: whether to pad upsampling features to fit features from encoder. Default to True.
                If this parameter is set to "True", the spatial dim of network input can be arbitrary
                size, which is not supported by TensorRT. Otherwise, it must be a multiple of 32.
        """
        super().__init__()
        if backbone not in FLEXUNET_BACKBONE.register_dict:
            raise ValueError(f'invalid model_name {backbone} found, must be one of {FLEXUNET_BACKBONE.register_dict.keys()}.')
        if spatial_dims not in (2, 3):
            raise ValueError('spatial_dims can only be 2 or 3.')
        encoder = FLEXUNET_BACKBONE.register_dict[backbone]
        self.backbone = backbone
        self.spatial_dims = spatial_dims
        encoder_parameters = encoder['parameter']
        if not ('spatial_dims' in encoder_parameters and 'in_channels' in encoder_parameters and 'pretrained' in encoder_parameters):
            raise ValueError('The backbone init method must have spatial_dims, in_channels and pretrained parameters.')
        encoder_feature_num = encoder['feature_number']
        if encoder_feature_num > 5:
            raise ValueError('Flexible unet can only accept no more than 5 encoder feature maps.')
        decoder_channels = decoder_channels[:encoder_feature_num]
        self.skip_connect = encoder_feature_num - 1
        encoder_parameters.update({'spatial_dims': spatial_dims, 'in_channels': in_channels, 'pretrained': pretrained})
        encoder_channels = tuple([in_channels] + list(encoder['feature_channel']))
        encoder_type = encoder['type']
        self.encoder = encoder_type(**encoder_parameters)
        self.decoder = UNetDecoder(spatial_dims=spatial_dims, encoder_channels=encoder_channels, decoder_channels=decoder_channels, act=act, norm=norm, dropout=dropout, bias=decoder_bias, upsample=upsample, interp_mode=interp_mode, pre_conv=None, align_corners=None, is_pad=is_pad)
        self.segmentation_head = SegmentationHead(spatial_dims=spatial_dims, in_channels=decoder_channels[-1], out_channels=out_channels, kernel_size=3, act=None)

    def forward(self, inputs: torch.Tensor):
        """
        Do a typical encoder-decoder-header inference.

        Args:
            inputs: input should have spatially N dimensions ``(Batch, in_channels, dim_0[, dim_1, ..., dim_N])``,
                N is defined by `dimensions`.

        Returns:
            A torch Tensor of "raw" predictions in shape ``(Batch, out_channels, dim_0[, dim_1, ..., dim_N])``.

        """
        x = inputs
        enc_out = self.encoder(x)
        decoder_out = self.decoder(enc_out, self.skip_connect)
        x_seg = self.segmentation_head(decoder_out)
        return x_seg


def _get_adn_layer(act: Optional[Union[Tuple, str]], dropout: Optional[Union[Tuple, str, float]], ordering: Optional[str]) ->ADN:
    if ordering:
        return ADN(act=act, dropout=dropout, dropout_dim=1, ordering=ordering)
    return ADN(act=act, dropout=dropout, dropout_dim=1)


class HighResBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, kernels: Sequence[int]=(3, 3), dilation: Union[Sequence[int], int]=1, norm_type: Union[Tuple, str]=('batch', {'affine': True}), acti_type: Union[Tuple, str]=('relu', {'inplace': True}), bias: bool=False, channel_matching: Union[ChannelMatching, str]=ChannelMatching.PAD) ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions of the input image.
            in_channels: number of input channels.
            out_channels: number of output channels.
            kernels: each integer k in `kernels` corresponds to a convolution layer with kernel size k.
            dilation: spacing between kernel elements.
            norm_type: feature normalization type and arguments.
                Defaults to ``("batch", {"affine": True})``.
            acti_type: {``"relu"``, ``"prelu"``, ``"relu6"``}
                Non-linear activation using ReLU or PReLU. Defaults to ``"relu"``.
            bias: whether to have a bias term in convolution blocks. Defaults to False.
                According to `Performance Tuning Guide <https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html>`_,
                if a conv layer is directly followed by a batch norm layer, bias should be False.
            channel_matching: {``"pad"``, ``"project"``}
                Specifies handling residual branch and conv branch channel mismatches. Defaults to ``"pad"``.

                - ``"pad"``: with zero padding.
                - ``"project"``: with a trainable conv with kernel size one.

        Raises:
            ValueError: When ``channel_matching=pad`` and ``in_channels > out_channels``. Incompatible values.

        """
        super().__init__()
        self.chn_pad = ChannelPad(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, mode=channel_matching)
        layers = nn.ModuleList()
        _in_chns, _out_chns = in_channels, out_channels
        for kernel_size in kernels:
            layers.append(ADN(ordering='NA', in_channels=_in_chns, act=acti_type, norm=norm_type, norm_dim=spatial_dims))
            layers.append(Convolution(spatial_dims=spatial_dims, in_channels=_in_chns, out_channels=_out_chns, kernel_size=kernel_size, dilation=dilation, bias=bias, conv_only=True))
            _in_chns = _out_chns
        self.layers = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x_conv: torch.Tensor = self.layers(x)
        return x_conv + torch.as_tensor(self.chn_pad(x))


DEFAULT_LAYER_PARAMS_3D = {'name': 'conv_0', 'n_features': 16, 'kernel_size': 3}, {'name': 'res_1', 'n_features': 16, 'kernels': (3, 3), 'repeat': 3}, {'name': 'res_2', 'n_features': 32, 'kernels': (3, 3), 'repeat': 3}, {'name': 'res_3', 'n_features': 64, 'kernels': (3, 3), 'repeat': 3}, {'name': 'conv_1', 'n_features': 80, 'kernel_size': 1}, {'name': 'conv_2', 'kernel_size': 1}


class HighResNet(nn.Module):
    """
    Reimplementation of highres3dnet based on
    Li et al., "On the compactness, efficiency, and representation of 3D
    convolutional networks: Brain parcellation as a pretext task", IPMI '17

    Adapted from:
    https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/network/highres3dnet.py
    https://github.com/fepegar/highresnet

    Args:
        spatial_dims: number of spatial dimensions of the input image.
        in_channels: number of input channels.
        out_channels: number of output channels.
        norm_type: feature normalization type and arguments.
            Defaults to ``("batch", {"affine": True})``.
        acti_type: activation type and arguments.
            Defaults to ``("relu", {"inplace": True})``.
        dropout_prob: probability of the feature map to be zeroed
            (only applies to the penultimate conv layer).
        bias: whether to have a bias term in convolution blocks. Defaults to False.
            According to `Performance Tuning Guide <https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html>`_,
            if a conv layer is directly followed by a batch norm layer, bias should be False.
        layer_params: specifying key parameters of each layer/block.
        channel_matching: {``"pad"``, ``"project"``}
            Specifies handling residual branch and conv branch channel mismatches. Defaults to ``"pad"``.

            - ``"pad"``: with zero padding.
            - ``"project"``: with a trainable conv with kernel size one.
    """

    def __init__(self, spatial_dims: int=3, in_channels: int=1, out_channels: int=1, norm_type: Union[str, tuple]=('batch', {'affine': True}), acti_type: Union[str, tuple]=('relu', {'inplace': True}), dropout_prob: Optional[Union[Tuple, str, float]]=0.0, bias: bool=False, layer_params: Sequence[Dict]=DEFAULT_LAYER_PARAMS_3D, channel_matching: Union[ChannelMatching, str]=ChannelMatching.PAD) ->None:
        super().__init__()
        blocks = nn.ModuleList()
        params = layer_params[0]
        _in_chns, _out_chns = in_channels, params['n_features']
        blocks.append(Convolution(spatial_dims=spatial_dims, in_channels=_in_chns, out_channels=_out_chns, kernel_size=params['kernel_size'], adn_ordering='NA', act=acti_type, norm=norm_type, bias=bias))
        for idx, params in enumerate(layer_params[1:-2]):
            _in_chns, _out_chns = _out_chns, params['n_features']
            _dilation = 2 ** idx
            for _ in range(params['repeat']):
                blocks.append(HighResBlock(spatial_dims=spatial_dims, in_channels=_in_chns, out_channels=_out_chns, kernels=params['kernels'], dilation=_dilation, norm_type=norm_type, acti_type=acti_type, bias=bias, channel_matching=channel_matching))
                _in_chns = _out_chns
        params = layer_params[-2]
        _in_chns, _out_chns = _out_chns, params['n_features']
        blocks.append(Convolution(spatial_dims=spatial_dims, in_channels=_in_chns, out_channels=_out_chns, kernel_size=params['kernel_size'], adn_ordering='NAD', act=acti_type, norm=norm_type, bias=bias, dropout=dropout_prob))
        params = layer_params[-1]
        _in_chns = _out_chns
        blocks.append(Convolution(spatial_dims=spatial_dims, in_channels=_in_chns, out_channels=out_channels, kernel_size=params['kernel_size'], adn_ordering='NAD', act=acti_type, norm=norm_type, bias=bias, dropout=dropout_prob))
        self.blocks = nn.Sequential(*blocks)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        return torch.as_tensor(self.blocks(x))


class _DenseLayerDecoder(nn.Module):

    def __init__(self, num_features: int, in_channels: int, out_channels: int, dropout_prob: float=0.0, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch', kernel_size: int=3, padding: int=0) ->None:
        """
        Args:
            num_features: number of internal channels used for the layer
            in_channels: number of the input channels.
            out_channels: number of the output channels.
            dropout_prob: dropout rate after each dense layer.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
            kernel_size: size of the kernel for >1 convolutions (dependent on mode)
            padding: padding value for >1 convolutions.
        """
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, 2]
        dropout_type: Callable = Dropout[Dropout.DROPOUT, 2]
        self.layers = nn.Sequential()
        self.layers.add_module('preact_bna/bn', get_norm_layer(name=norm, spatial_dims=2, channels=in_channels))
        self.layers.add_module('preact_bna/relu', get_act_layer(name=act))
        self.layers.add_module('conv1', conv_type(in_channels, num_features, kernel_size=1, bias=False))
        self.layers.add_module('conv1/norm', get_norm_layer(name=norm, spatial_dims=2, channels=num_features))
        self.layers.add_module('conv1/relu2', get_act_layer(name=act))
        self.layers.add_module('conv2', conv_type(num_features, out_channels, kernel_size=kernel_size, padding=padding, groups=4, bias=False))
        if dropout_prob > 0:
            self.layers.add_module('dropout', dropout_type(dropout_prob))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x1 = self.layers(x)
        if x1.shape[-1] != x.shape[-1]:
            trim = (x.shape[-1] - x1.shape[-1]) // 2
            x = x[:, :, trim:-trim, trim:-trim]
        x = torch.cat([x, x1], 1)
        return x


class _DecoderBlock(nn.Sequential):

    def __init__(self, layers: int, num_features: int, in_channels: int, out_channels: int, dropout_prob: float=0.0, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch', kernel_size: int=3, same_padding: bool=False) ->None:
        """
        Args:
            layers: number of layers in the block.
            num_features: number of internal features used.
            in_channels: number of the input channel.
            out_channels: number of the output channel.
            dropout_prob: dropout rate after each dense layer.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
            kernel_size: size of the kernel for >1 convolutions (dependent on mode)
            same_padding: whether to do padding for >1 convolutions to ensure
                the output size is the same as the input size.
        """
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, 2]
        padding: int = kernel_size // 2 if same_padding else 0
        self.add_module('conva', conv_type(in_channels, in_channels // 4, kernel_size=kernel_size, padding=padding, bias=False))
        _in_channels = in_channels // 4
        for i in range(layers):
            layer = _DenseLayerDecoder(num_features, _in_channels, out_channels, dropout_prob, act=act, norm=norm, kernel_size=kernel_size, padding=padding)
            _in_channels += out_channels
            self.add_module('denselayerdecoder%d' % (i + 1), layer)
        trans = _Transition(_in_channels, act=act, norm=norm)
        self.add_module('bna_block', trans)
        self.add_module('convf', conv_type(_in_channels, _in_channels, kernel_size=1, bias=False))


class _ResidualBlock(nn.Module):

    def __init__(self, layers: int, num_features: int, in_channels: int, out_channels: int, dropout_prob: float=0.0, act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch', freeze_dense_layer: bool=False, freeze_block: bool=False) ->None:
        """Residual block.

        References:
            He, Kaiming, et al. "Deep residual learning for image
            recognition." Proceedings of the IEEE conference on computer
            vision and pattern recognition. 2016.

        Args:
            layers: number of layers in the block.
            num_features: number of internal features used.
            in_channels: number of the input channel.
            out_channels: number of the output channel.
            dropout_prob: dropout rate after each dense layer.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
            freeze_dense_layer: whether to freeze all dense layers within the block.
            freeze_block: whether to freeze the whole block.

        """
        super().__init__()
        self.layers = nn.Sequential()
        conv_type: Callable = Conv[Conv.CONV, 2]
        if in_channels == 64:
            self.shortcut = conv_type(in_channels, out_channels, kernel_size=1, bias=False)
        else:
            self.shortcut = conv_type(in_channels, out_channels, kernel_size=1, stride=2, padding=1, bias=False)
        layer = _DenseLayer(num_features, in_channels, out_channels, dropout_prob, act=act, norm=norm, drop_first_norm_relu=True)
        self.layers.add_module('denselayer_0', layer)
        for i in range(1, layers):
            layer = _DenseLayer(num_features, out_channels, out_channels, dropout_prob, act=act, norm=norm)
            self.layers.add_module(f'denselayer_{i}', layer)
        self.bna_block = _Transition(out_channels, act=act, norm=norm)
        if freeze_dense_layer:
            self.layers.requires_grad_(False)
        if freeze_block:
            self.requires_grad_(False)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        sc = self.shortcut(x)
        if self.shortcut.stride == (2, 2):
            sc = sc[:, :, :-1, :-1]
        for layer in self.layers:
            x = layer.forward(x)
            if x.shape[-2:] != sc.shape[-2:]:
                x = x[:, :, :-1, :-1]
            x = x + sc
            sc = x
        x = self.bna_block(x)
        return x


class _DecoderBranch(nn.ModuleList):

    def __init__(self, decode_config: Sequence[int]=(8, 4), act: Union[str, tuple]=('relu', {'inplace': True}), norm: Union[str, tuple]='batch', dropout_prob: float=0.0, out_channels: int=2, kernel_size: int=3, same_padding: bool=False) ->None:
        """
        Args:
            decode_config: number of layers for each block.
            act: activation type and arguments. Defaults to relu.
            norm: feature normalization type and arguments. Defaults to batch norm.
            dropout_prob: dropout rate after each dense layer.
            out_channels: number of the output channel.
            kernel_size: size of the kernel for >1 convolutions (dependent on mode)
            same_padding: whether to do padding for >1 convolutions to ensure
                the output size is the same as the input size.
        """
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, 2]
        _in_channels = 1024
        _num_features = 128
        _out_channels = 32
        self.decoder_blocks = nn.Sequential()
        for i, num_layers in enumerate(decode_config):
            block = _DecoderBlock(layers=num_layers, num_features=_num_features, in_channels=_in_channels, out_channels=_out_channels, dropout_prob=dropout_prob, act=act, norm=norm, kernel_size=kernel_size, same_padding=same_padding)
            self.decoder_blocks.add_module(f'decoderblock{i + 1}', block)
            _in_channels = 512
        self.output_features = nn.Sequential()
        _i = len(decode_config)
        _pad_size = (kernel_size - 1) // 2
        _seq_block = nn.Sequential(OrderedDict([('conva', conv_type(256, 64, kernel_size=kernel_size, stride=1, bias=False, padding=_pad_size))]))
        self.output_features.add_module(f'decoderblock{_i + 1}', _seq_block)
        _seq_block = nn.Sequential(OrderedDict([('bn', get_norm_layer(name=norm, spatial_dims=2, channels=64)), ('relu', get_act_layer(name=act)), ('conv', conv_type(64, out_channels, kernel_size=1, stride=1))]))
        self.output_features.add_module(f'decoderblock{_i + 2}', _seq_block)
        self.upsample = UpSample(2, scale_factor=2, mode=UpsampleMode.NONTRAINABLE, interp_mode=InterpolateMode.BILINEAR, bias=False)

    def forward(self, xin: torch.Tensor, short_cuts: List[torch.Tensor]) ->torch.Tensor:
        block_number = len(short_cuts) - 1
        x = xin + short_cuts[block_number]
        for block in self.decoder_blocks:
            x = block(x)
            x = self.upsample(x)
            block_number -= 1
            trim = (short_cuts[block_number].shape[-1] - x.shape[-1]) // 2
            if trim > 0:
                x += short_cuts[block_number][:, :, trim:-trim, trim:-trim]
        for block in self.output_features:
            x = block(x)
        return x


class HoVerNetMode(StrEnum):
    """
    Modes for HoVerNet model:
    `FAST`: a faster implementation (than original)
    `ORIGINAL`: the original implementation
    """
    FAST = 'FAST'
    ORIGINAL = 'ORIGINAL'


def _load_pretrained_encoder(model: nn.Module, state_dict: Union[OrderedDict, Dict]):
    model_dict = model.state_dict()
    state_dict = {k: v for k, v in state_dict.items() if k in model_dict and model_dict[k].shape == state_dict[k].shape}
    model_dict.update(state_dict)
    model.load_state_dict(model_dict)


def _remap_preact_resnet_model(model_url: str):
    pattern_conv0 = re.compile('^(conv0\\.\\/)(.+)$')
    pattern_block = re.compile('^(d\\d+)\\.(.+)$')
    pattern_layer = re.compile('^(.+\\.d\\d+)\\.units\\.(\\d+)(.+)$')
    pattern_bna = re.compile('^(.+\\.d\\d+)\\.blk_bna\\.(.+)')
    weights_dir = os.path.join(torch.hub.get_dir(), 'preact-resnet50.pth')
    download_url(model_url, fuzzy=True, filepath=weights_dir, progress=False)
    state_dict = torch.load(weights_dir, map_location=None)['desc']
    for key in list(state_dict.keys()):
        new_key = None
        if pattern_conv0.match(key):
            new_key = re.sub(pattern_conv0, 'conv0.conv\\2', key)
        elif pattern_block.match(key):
            new_key = re.sub(pattern_block, 'res_blocks.\\1.\\2', key)
            if pattern_layer.match(new_key):
                new_key = re.sub(pattern_layer, '\\1.layers.denselayer_\\2.layers\\3', new_key)
            elif pattern_bna.match(new_key):
                new_key = re.sub(pattern_bna, '\\1.bna_block.\\2', new_key)
        if new_key:
            state_dict[new_key] = state_dict[key]
            del state_dict[key]
        if 'upsample2x' in key:
            del state_dict[key]
    return state_dict


def _remap_standard_resnet_model(model_url: str):
    pattern_conv0 = re.compile('^conv1\\.(.+)$')
    pattern_bn1 = re.compile('^bn1\\.(.+)$')
    pattern_block = re.compile('^layer(\\d+)\\.(\\d+)\\.(.+)$')
    pattern_block_bn3 = re.compile('^(res_blocks.d\\d+\\.layers\\.denselayer_)(\\d+)\\.layers\\.bn3\\.(.+)$')
    pattern_block_bn = re.compile('^(res_blocks.d\\d+\\.layers\\.denselayer_\\d+\\.layers)\\.bn(\\d+)\\.(.+)$')
    pattern_downsample0 = re.compile('^(res_blocks.d\\d+).+\\.downsample\\.0\\.(.+)')
    pattern_downsample1 = re.compile('^(res_blocks.d\\d+).+\\.downsample\\.1\\.(.+)')
    weights_dir = os.path.join(torch.hub.get_dir(), 'resnet50.pth')
    download_url(model_url, fuzzy=True, filepath=weights_dir, progress=False)
    state_dict = torch.load(weights_dir, map_location=None)
    for key in list(state_dict.keys()):
        new_key = None
        if pattern_conv0.match(key):
            new_key = re.sub(pattern_conv0, 'conv0.conv.\\1', key)
        elif pattern_bn1.match(key):
            new_key = re.sub(pattern_bn1, 'conv0.bn.\\1', key)
        elif pattern_block.match(key):
            new_key = re.sub(pattern_block, lambda s: 'res_blocks.d' + str(int(s.group(1)) - 1) + '.layers.denselayer_' + s.group(2) + '.layers.' + s.group(3), key)
            if pattern_block_bn3.match(new_key):
                new_key = re.sub(pattern_block_bn3, lambda s: s.group(1) + str(int(s.group(2)) + 1) + '.layers.preact/bn.' + s.group(3), new_key)
            elif pattern_block_bn.match(new_key):
                new_key = re.sub(pattern_block_bn, '\\1.conv\\2/bn.\\3', new_key)
            elif pattern_downsample0.match(new_key):
                new_key = re.sub(pattern_downsample0, '\\1.shortcut.\\2', new_key)
            elif pattern_downsample1.match(new_key):
                new_key = re.sub(pattern_downsample1, '\\1.bna_block.bn.\\2', new_key)
        if new_key:
            state_dict[new_key] = state_dict[key]
            del state_dict[key]
    return state_dict


def export(modname):
    """
    Make the decorated object a member of the named module. This will also add the object under its aliases if it has
    a `__aliases__` member, thus this decorator should be before the `alias` decorator to pick up those names. Alias
    names which conflict with package names or existing members will be ignored.
    """

    def _inner(obj):
        mod = import_module(modname)
        if not hasattr(mod, obj.__name__):
            setattr(mod, obj.__name__, obj)
            for alias in getattr(obj, '__aliases__', ()):
                if not hasattr(mod, alias):
                    setattr(mod, alias, obj)
        return obj
    return _inner


class MILModel(nn.Module):
    """
    Multiple Instance Learning (MIL) model, with a backbone classification model.
    Currently, it only works for 2D images, a typical use case is for classification of the
    digital pathology whole slide images. The expected shape of input data is `[B, N, C, H, W]`,
    where `B` is the batch_size of PyTorch Dataloader and `N` is the number of instances
    extracted from every original image in the batch. A tutorial example is available at:
    https://github.com/Project-MONAI/tutorials/tree/master/pathology/multiple_instance_learning.

    Args:
        num_classes: number of output classes.
        mil_mode: MIL algorithm, available values (Defaults to ``"att"``):

            - ``"mean"`` - average features from all instances, equivalent to pure CNN (non MIL).
            - ``"max"`` - retain only the instance with the max probability for loss calculation.
            - ``"att"`` - attention based MIL https://arxiv.org/abs/1802.04712.
            - ``"att_trans"`` - transformer MIL https://arxiv.org/abs/2111.01556.
            - ``"att_trans_pyramid"`` - transformer pyramid MIL https://arxiv.org/abs/2111.01556.

        pretrained: init backbone with pretrained weights, defaults to ``True``.
        backbone: Backbone classifier CNN (either ``None``, a ``nn.Module`` that returns features,
            or a string name of a torchvision model).
            Defaults to ``None``, in which case ResNet50 is used.
        backbone_num_features: Number of output features of the backbone CNN
            Defaults to ``None`` (necessary only when using a custom backbone)
        trans_blocks: number of the blocks in `TransformEncoder` layer.
        trans_dropout: dropout rate in `TransformEncoder` layer.

    """

    def __init__(self, num_classes: int, mil_mode: str='att', pretrained: bool=True, backbone: Optional[Union[str, nn.Module]]=None, backbone_num_features: Optional[int]=None, trans_blocks: int=4, trans_dropout: float=0.0) ->None:
        super().__init__()
        if num_classes <= 0:
            raise ValueError('Number of classes must be positive: ' + str(num_classes))
        if mil_mode.lower() not in ['mean', 'max', 'att', 'att_trans', 'att_trans_pyramid']:
            raise ValueError('Unsupported mil_mode: ' + str(mil_mode))
        self.mil_mode = mil_mode.lower()
        self.attention = nn.Sequential()
        self.transformer = None
        if backbone is None:
            net = models.resnet50(pretrained=pretrained)
            nfc = net.fc.in_features
            net.fc = torch.nn.Identity()
            self.extra_outputs: Dict[str, torch.Tensor] = {}
            if mil_mode == 'att_trans_pyramid':

                def forward_hook(layer_name):

                    def hook(module, input, output):
                        self.extra_outputs[layer_name] = output
                    return hook
                net.layer1.register_forward_hook(forward_hook('layer1'))
                net.layer2.register_forward_hook(forward_hook('layer2'))
                net.layer3.register_forward_hook(forward_hook('layer3'))
                net.layer4.register_forward_hook(forward_hook('layer4'))
        elif isinstance(backbone, str):
            torch_model = getattr(models, backbone, None)
            if torch_model is None:
                raise ValueError('Unknown torch vision model' + str(backbone))
            net = torch_model(pretrained=pretrained)
            if getattr(net, 'fc', None) is not None:
                nfc = net.fc.in_features
                net.fc = torch.nn.Identity()
            else:
                raise ValueError('Unable to detect FC layer for the torchvision model ' + str(backbone), '. Please initialize the backbone model manually.')
        elif isinstance(backbone, nn.Module):
            net = backbone
            nfc = backbone_num_features
            if backbone_num_features is None:
                raise ValueError('Number of endencoder features must be provided for a custom backbone model')
        else:
            raise ValueError('Unsupported backbone')
        if backbone is not None and mil_mode not in ['mean', 'max', 'att', 'att_trans']:
            raise ValueError('Custom backbone is not supported for the mode:' + str(mil_mode))
        if self.mil_mode in ['mean', 'max']:
            pass
        elif self.mil_mode == 'att':
            self.attention = nn.Sequential(nn.Linear(nfc, 2048), nn.Tanh(), nn.Linear(2048, 1))
        elif self.mil_mode == 'att_trans':
            transformer = nn.TransformerEncoderLayer(d_model=nfc, nhead=8, dropout=trans_dropout)
            self.transformer = nn.TransformerEncoder(transformer, num_layers=trans_blocks)
            self.attention = nn.Sequential(nn.Linear(nfc, 2048), nn.Tanh(), nn.Linear(2048, 1))
        elif self.mil_mode == 'att_trans_pyramid':
            transformer_list = nn.ModuleList([nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=256, nhead=8, dropout=trans_dropout), num_layers=trans_blocks), nn.Sequential(nn.Linear(768, 256), nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=256, nhead=8, dropout=trans_dropout), num_layers=trans_blocks)), nn.Sequential(nn.Linear(1280, 256), nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=256, nhead=8, dropout=trans_dropout), num_layers=trans_blocks)), nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=2304, nhead=8, dropout=trans_dropout), num_layers=trans_blocks)])
            self.transformer = transformer_list
            nfc = nfc + 256
            self.attention = nn.Sequential(nn.Linear(nfc, 2048), nn.Tanh(), nn.Linear(2048, 1))
        else:
            raise ValueError('Unsupported mil_mode: ' + str(mil_mode))
        self.myfc = nn.Linear(nfc, num_classes)
        self.net = net

    def calc_head(self, x: torch.Tensor) ->torch.Tensor:
        sh = x.shape
        if self.mil_mode == 'mean':
            x = self.myfc(x)
            x = torch.mean(x, dim=1)
        elif self.mil_mode == 'max':
            x = self.myfc(x)
            x, _ = torch.max(x, dim=1)
        elif self.mil_mode == 'att':
            a = self.attention(x)
            a = torch.softmax(a, dim=1)
            x = torch.sum(x * a, dim=1)
            x = self.myfc(x)
        elif self.mil_mode == 'att_trans' and self.transformer is not None:
            x = x.permute(1, 0, 2)
            x = self.transformer(x)
            x = x.permute(1, 0, 2)
            a = self.attention(x)
            a = torch.softmax(a, dim=1)
            x = torch.sum(x * a, dim=1)
            x = self.myfc(x)
        elif self.mil_mode == 'att_trans_pyramid' and self.transformer is not None:
            l1 = torch.mean(self.extra_outputs['layer1'], dim=(2, 3)).reshape(sh[0], sh[1], -1).permute(1, 0, 2)
            l2 = torch.mean(self.extra_outputs['layer2'], dim=(2, 3)).reshape(sh[0], sh[1], -1).permute(1, 0, 2)
            l3 = torch.mean(self.extra_outputs['layer3'], dim=(2, 3)).reshape(sh[0], sh[1], -1).permute(1, 0, 2)
            l4 = torch.mean(self.extra_outputs['layer4'], dim=(2, 3)).reshape(sh[0], sh[1], -1).permute(1, 0, 2)
            transformer_list = cast(nn.ModuleList, self.transformer)
            x = transformer_list[0](l1)
            x = transformer_list[1](torch.cat((x, l2), dim=2))
            x = transformer_list[2](torch.cat((x, l3), dim=2))
            x = transformer_list[3](torch.cat((x, l4), dim=2))
            x = x.permute(1, 0, 2)
            a = self.attention(x)
            a = torch.softmax(a, dim=1)
            x = torch.sum(x * a, dim=1)
            x = self.myfc(x)
        else:
            raise ValueError('Wrong model mode' + str(self.mil_mode))
        return x

    def forward(self, x: torch.Tensor, no_head: bool=False) ->torch.Tensor:
        sh = x.shape
        x = x.reshape(sh[0] * sh[1], sh[2], sh[3], sh[4])
        x = self.net(x)
        x = x.reshape(sh[0], sh[1], -1)
        if not no_head:
            x = self.calc_head(x)
        return x


get_graph_node_names, _has_utils = optional_import('torchvision.models.feature_extraction', name='get_graph_node_names')


create_feature_extractor, _ = optional_import('torchvision.models.feature_extraction', name='create_feature_extractor')


def get_pool_layer(name: Union[Tuple, str], spatial_dims: Optional[int]=1):
    """
    Create a pooling layer instance.

    For example, to create adaptiveavg layer:

    .. code-block:: python

        from monai.networks.layers import get_pool_layer

        pool_layer = get_pool_layer(("adaptiveavg", {"output_size": (1, 1, 1)}), spatial_dims=3)

    Args:
        name: a pooling type string or a tuple of type string and parameters.
        spatial_dims: number of spatial dimensions of the input.

    """
    if name == '':
        return torch.nn.Identity()
    pool_name, pool_args = split_args(name)
    pool_type = Pool[pool_name, spatial_dims]
    return pool_type(**pool_args)


def look_up_named_module(name: str, mod, print_all_options=False):
    """
    get the named module in `mod` by the attribute name,
    for example ``look_up_named_module(net, "features.3.1.attn")``

    Args:
        name: a string representing the module attribute.
        mod: a pytorch module to be searched (in ``mod.named_modules()``).
        print_all_options: whether to print all named modules when `name` is not found in `mod`. Defaults to False.

    Returns:
        the corresponding pytorch module's subcomponent such as ``net.features[3][1].attn``
    """
    name_str = look_up_option(name, {n[0] for n in mod.named_modules()}, default=None, print_all_options=print_all_options)
    if name_str is None:
        return None
    if name_str == '':
        return mod
    for n in name_str.split('.'):
        if n.isdigit():
            mod = mod[int(n)]
        else:
            n = look_up_option(n, {item[0] for item in mod.named_modules()}, default=None, print_all_options=False)
            if n is None:
                return None
            mod = getattr(mod, n)
    return mod


def set_named_module(mod, name: str, new_layer):
    """
    look up `name` in `mod` and replace the layer with `new_layer`, return the updated `mod`.

    Args:
        mod: a pytorch module to be updated.
        name: a string representing the target module attribute.
        new_layer: a new module replacing the corresponding layer at ``mod.name``.

    Returns:
        an updated ``mod``

    See also: :py:func:`monai.networks.utils.look_up_named_module`.
    """
    mods_attr = name.rsplit('.', 1)
    submods, attr = mods_attr if len(mods_attr) == 2 else ('', name)
    if not attr:
        return new_layer
    _mod = look_up_named_module(submods, mod)
    setattr(_mod, attr, new_layer)
    return mod


class NetAdapter(torch.nn.Module):
    """
    Wrapper to replace the last layer of model by convolutional layer or FC layer.

    See also: :py:class:`monai.networks.nets.TorchVisionFCModel`

    Args:
        model: a PyTorch model, which can be both 2D and 3D models. typically, it can be a pretrained model
            in Torchvision, like: ``resnet18``, ``resnet34``, ``resnet50``, ``resnet101``, ``resnet152``, etc.
            more details: https://pytorch.org/vision/stable/models.html.
        num_classes: number of classes for the last classification layer. Default to 1.
        dim: number of supported spatial dimensions in the specified model, depends on the model implementation.
            default to 2 as most Torchvision models are for 2D image processing.
        in_channels: number of the input channels of last layer. if None, get it from `in_features` of last layer.
        use_conv: whether to use convolutional layer to replace the last layer, default to False.
        pool: parameters for the pooling layer, it should be a tuple, the first item is name of the pooling layer,
            the second item is dictionary of the initialization args. if None, will not replace the `layers[-2]`.
            default to `("avg", {"kernel_size": 7, "stride": 1})`.
        bias: the bias value when replacing the last layer. if False, the layer will not learn an additive bias,
            default to True.
        fc_name: the corresponding layer attribute of the last fully connected layer. Defaults to ``"fc"``.
        node_name: the corresponding feature extractor node name of `model`.
            Defaults to "", the extractor is not in use.

    """

    def __init__(self, model: torch.nn.Module, num_classes: int=1, dim: int=2, in_channels: Optional[int]=None, use_conv: bool=False, pool: Optional[Tuple[str, Dict[str, Any]]]=('avg', {'kernel_size': 7, 'stride': 1}), bias: bool=True, fc_name: str='fc', node_name: str=''):
        super().__init__()
        layers = list(model.children())
        orig_fc = look_up_named_module(fc_name, model)
        if orig_fc is None:
            orig_fc = layers[-1]
        in_channels_: int
        if in_channels is None:
            if not hasattr(orig_fc, 'in_features'):
                raise ValueError('please specify input channels of the last fully connected layer with `in_channels`.')
            in_channels_ = orig_fc.in_features
        else:
            in_channels_ = in_channels
        if pool is None:
            if node_name != '':
                raise ValueError("`node_name` is not compatible with `pool=None`, please set `pool=''`.")
            if look_up_named_module(fc_name, model):
                self.features = set_named_module(model, fc_name, torch.nn.Identity())
            else:
                self.features = torch.nn.Sequential(*layers[:-1])
            self.pool = None
        else:
            if node_name and _has_utils:
                node_name = look_up_option(node_name, get_graph_node_names(model)[0 if model.training else 1])
                self.features = create_feature_extractor(model, [node_name])
            else:
                self.features = torch.nn.Sequential(*layers[:-2])
            self.pool = get_pool_layer(name=pool, spatial_dims=dim)
        self.fc: Union[torch.nn.Linear, torch.nn.Conv2d, torch.nn.Conv3d]
        if use_conv:
            self.fc = Conv[Conv.CONV, dim](in_channels=in_channels_, out_channels=num_classes, kernel_size=1, bias=bias)
        else:
            self.fc = torch.nn.Linear(in_features=in_channels_, out_features=num_classes, bias=bias)
        self.use_conv = use_conv
        self.dim = dim
        self.node_name = node_name

    def forward(self, x):
        x = self.features(x)
        if isinstance(x, tuple):
            x = x[0]
        elif torch.jit.isinstance(x, Dict[str, torch.Tensor]):
            x = x[self.node_name]
        if self.pool is not None:
            x = self.pool(x)
        if not self.use_conv:
            x = torch.flatten(x, 1)
        else:
            while len(x.shape) < self.dim + 2:
                x = x[..., None]
        x = self.fc(x)
        return x


def calculate_out_shape(in_shape: Union[Sequence[int], int, np.ndarray], kernel_size: Union[Sequence[int], int], stride: Union[Sequence[int], int], padding: Union[Sequence[int], int]) ->Union[Tuple[int, ...], int]:
    """
    Calculate the output tensor shape when applying a convolution to a tensor of shape `inShape` with kernel size
    `kernel_size`, stride value `stride`, and input padding value `padding`. All arguments can be scalars or multiple
    values, return value is a scalar if all inputs are scalars.
    """
    in_shape_np = np.atleast_1d(in_shape)
    kernel_size_np = np.atleast_1d(kernel_size)
    stride_np = np.atleast_1d(stride)
    padding_np = np.atleast_1d(padding)
    out_shape_np = (in_shape_np - kernel_size_np + padding_np + padding_np) // stride_np + 1
    out_shape = tuple(int(s) for s in out_shape_np)
    return out_shape


class RegUNet(nn.Module):
    """
    Class that implements an adapted UNet. This class also serve as the parent class of LocalNet and GlobalNet

    Reference:
        O. Ronneberger, P. Fischer, and T. Brox,
        “U-net: Convolutional networks for biomedical image segmentation,”,
        Lecture Notes in Computer Science, 2015, vol. 9351, pp. 234–241.
        https://arxiv.org/abs/1505.04597

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int, in_channels: int, num_channel_initial: int, depth: int, out_kernel_initializer: Optional[str]='kaiming_uniform', out_activation: Optional[str]=None, out_channels: int=3, extract_levels: Optional[Tuple[int]]=None, pooling: bool=True, concat_skip: bool=False, encode_kernel_sizes: Union[int, List[int]]=3):
        """
        Args:
            spatial_dims: number of spatial dims
            in_channels: number of input channels
            num_channel_initial: number of initial channels
            depth: input is at level 0, bottom is at level depth.
            out_kernel_initializer: kernel initializer for the last layer
            out_activation: activation at the last layer
            out_channels: number of channels for the output
            extract_levels: list, which levels from net to extract. The maximum level must equal to ``depth``
            pooling: for down-sampling, use non-parameterized pooling if true, otherwise use conv3d
            concat_skip: when up-sampling, concatenate skipped tensor if true, otherwise use addition
            encode_kernel_sizes: kernel size for down-sampling
        """
        super().__init__()
        if not extract_levels:
            extract_levels = depth,
        if max(extract_levels) != depth:
            raise AssertionError
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.num_channel_initial = num_channel_initial
        self.depth = depth
        self.out_kernel_initializer = out_kernel_initializer
        self.out_activation = out_activation
        self.out_channels = out_channels
        self.extract_levels = extract_levels
        self.pooling = pooling
        self.concat_skip = concat_skip
        if isinstance(encode_kernel_sizes, int):
            encode_kernel_sizes = [encode_kernel_sizes] * (self.depth + 1)
        if len(encode_kernel_sizes) != self.depth + 1:
            raise AssertionError
        self.encode_kernel_sizes: List[int] = encode_kernel_sizes
        self.num_channels = [(self.num_channel_initial * 2 ** d) for d in range(self.depth + 1)]
        self.min_extract_level = min(self.extract_levels)
        self.encode_convs: nn.ModuleList
        self.encode_pools: nn.ModuleList
        self.bottom_block: nn.Sequential
        self.decode_deconvs: nn.ModuleList
        self.decode_convs: nn.ModuleList
        self.output_block: nn.Module
        self.build_layers()

    def build_layers(self):
        self.build_encode_layers()
        self.build_decode_layers()

    def build_encode_layers(self):
        self.encode_convs = nn.ModuleList([self.build_conv_block(in_channels=self.in_channels if d == 0 else self.num_channels[d - 1], out_channels=self.num_channels[d], kernel_size=self.encode_kernel_sizes[d]) for d in range(self.depth)])
        self.encode_pools = nn.ModuleList([self.build_down_sampling_block(channels=self.num_channels[d]) for d in range(self.depth)])
        self.bottom_block = self.build_bottom_block(in_channels=self.num_channels[-2], out_channels=self.num_channels[-1])

    def build_conv_block(self, in_channels, out_channels, kernel_size):
        return nn.Sequential(get_conv_block(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size), RegistrationResidualConvBlock(spatial_dims=self.spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size))

    def build_down_sampling_block(self, channels: int):
        return RegistrationDownSampleBlock(spatial_dims=self.spatial_dims, channels=channels, pooling=self.pooling)

    def build_bottom_block(self, in_channels: int, out_channels: int):
        kernel_size = self.encode_kernel_sizes[self.depth]
        return nn.Sequential(get_conv_block(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size), RegistrationResidualConvBlock(spatial_dims=self.spatial_dims, in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size))

    def build_decode_layers(self):
        self.decode_deconvs = nn.ModuleList([self.build_up_sampling_block(in_channels=self.num_channels[d + 1], out_channels=self.num_channels[d]) for d in range(self.depth - 1, self.min_extract_level - 1, -1)])
        self.decode_convs = nn.ModuleList([self.build_conv_block(in_channels=2 * self.num_channels[d] if self.concat_skip else self.num_channels[d], out_channels=self.num_channels[d], kernel_size=3) for d in range(self.depth - 1, self.min_extract_level - 1, -1)])
        self.output_block = self.build_output_block()

    def build_up_sampling_block(self, in_channels: int, out_channels: int) ->nn.Module:
        return get_deconv_block(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels)

    def build_output_block(self) ->nn.Module:
        return RegistrationExtractionBlock(spatial_dims=self.spatial_dims, extract_levels=self.extract_levels, num_channels=self.num_channels, out_channels=self.out_channels, kernel_initializer=self.out_kernel_initializer, activation=self.out_activation)

    def forward(self, x):
        """
        Args:
            x: Tensor in shape (batch, ``in_channels``, insize_1, insize_2, [insize_3])

        Returns:
            Tensor in shape (batch, ``out_channels``, insize_1, insize_2, [insize_3]), with the same spatial size as ``x``
        """
        image_size = x.shape[2:]
        skips = []
        encoded = x
        for encode_conv, encode_pool in zip(self.encode_convs, self.encode_pools):
            skip = encode_conv(encoded)
            encoded = encode_pool(skip)
            skips.append(skip)
        decoded = self.bottom_block(encoded)
        outs = [decoded]
        for i, (decode_deconv, decode_conv) in enumerate(zip(self.decode_deconvs, self.decode_convs)):
            decoded = decode_deconv(decoded)
            if self.concat_skip:
                decoded = torch.cat([decoded, skips[-i - 1]], dim=1)
            else:
                decoded = decoded + skips[-i - 1]
            decoded = decode_conv(decoded)
            outs.append(decoded)
        out = self.output_block(outs, image_size=image_size)
        return out


class AffineHead(nn.Module):

    def __init__(self, spatial_dims: int, image_size: List[int], decode_size: List[int], in_channels: int):
        super().__init__()
        self.spatial_dims = spatial_dims
        if spatial_dims == 2:
            in_features = in_channels * decode_size[0] * decode_size[1]
            out_features = 6
            out_init = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float)
        elif spatial_dims == 3:
            in_features = in_channels * decode_size[0] * decode_size[1] * decode_size[2]
            out_features = 12
            out_init = torch.tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], dtype=torch.float)
        else:
            raise ValueError(f'only support 2D/3D operation, got spatial_dims={spatial_dims}')
        self.fc = nn.Linear(in_features=in_features, out_features=out_features)
        self.grid = self.get_reference_grid(image_size)
        self.fc.weight.data.zero_()
        self.fc.bias.data.copy_(out_init)

    @staticmethod
    def get_reference_grid(image_size: Union[Tuple[int], List[int]]) ->torch.Tensor:
        mesh_points = [torch.arange(0, dim) for dim in image_size]
        grid = torch.stack(meshgrid_ij(*mesh_points), dim=0)
        return grid

    def affine_transform(self, theta: torch.Tensor):
        grid_padded = torch.cat([self.grid, torch.ones_like(self.grid[:1])])
        if self.spatial_dims == 2:
            grid_warped = torch.einsum('qij,bpq->bpij', grid_padded, theta.reshape(-1, 2, 3))
        elif self.spatial_dims == 3:
            grid_warped = torch.einsum('qijk,bpq->bpijk', grid_padded, theta.reshape(-1, 3, 4))
        else:
            raise ValueError(f'do not support spatial_dims={self.spatial_dims}')
        return grid_warped

    def forward(self, x: List[torch.Tensor], image_size: List[int]) ->torch.Tensor:
        f = x[0]
        self.grid = self.grid
        theta = self.fc(f.reshape(f.shape[0], -1))
        out: torch.Tensor = self.affine_transform(theta) - self.grid
        return out


class GlobalNet(RegUNet):
    """
    Build GlobalNet for image registration.

    Reference:
        Hu, Yipeng, et al.
        "Label-driven weakly-supervised learning
        for multimodal deformable image registration,"
        https://arxiv.org/abs/1711.01666
    """

    def __init__(self, image_size: List[int], spatial_dims: int, in_channels: int, num_channel_initial: int, depth: int, out_kernel_initializer: Optional[str]='kaiming_uniform', out_activation: Optional[str]=None, pooling: bool=True, concat_skip: bool=False, encode_kernel_sizes: Union[int, List[int]]=3):
        for size in image_size:
            if size % 2 ** depth != 0:
                raise ValueError(f'given depth {depth}, all input spatial dimension must be divisible by {2 ** depth}, got input of size {image_size}')
        self.image_size = image_size
        self.decode_size = [(size // 2 ** depth) for size in image_size]
        super().__init__(spatial_dims=spatial_dims, in_channels=in_channels, num_channel_initial=num_channel_initial, depth=depth, out_kernel_initializer=out_kernel_initializer, out_activation=out_activation, out_channels=spatial_dims, pooling=pooling, concat_skip=concat_skip, encode_kernel_sizes=encode_kernel_sizes)

    def build_output_block(self):
        return AffineHead(spatial_dims=self.spatial_dims, image_size=self.image_size, decode_size=self.decode_size, in_channels=self.num_channels[-1])


class AdditiveUpSampleBlock(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int):
        super().__init__()
        self.deconv = get_deconv_block(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        output_size = [(size * 2) for size in x.shape[2:]]
        deconved = self.deconv(x)
        resized = F.interpolate(x, output_size)
        resized = torch.sum(torch.stack(resized.split(split_size=resized.shape[1] // 2, dim=1), dim=-1), dim=-1)
        out: torch.Tensor = deconved + resized
        return out


class LocalNet(RegUNet):
    """
    Reimplementation of LocalNet, based on:
    `Weakly-supervised convolutional neural networks for multimodal image registration
    <https://doi.org/10.1016/j.media.2018.07.002>`_.
    `Label-driven weakly-supervised learning for multimodal deformable image registration
    <https://arxiv.org/abs/1711.01666>`_.

    Adapted from:
        DeepReg (https://github.com/DeepRegNet/DeepReg)
    """

    def __init__(self, spatial_dims: int, in_channels: int, num_channel_initial: int, extract_levels: Tuple[int], out_kernel_initializer: Optional[str]='kaiming_uniform', out_activation: Optional[str]=None, out_channels: int=3, pooling: bool=True, use_addictive_sampling: bool=True, concat_skip: bool=False):
        """
        Args:
            spatial_dims: number of spatial dims
            in_channels: number of input channels
            num_channel_initial: number of initial channels
            out_kernel_initializer: kernel initializer for the last layer
            out_activation: activation at the last layer
            out_channels: number of channels for the output
            extract_levels: list, which levels from net to extract. The maximum level must equal to ``depth``
            pooling: for down-sampling, use non-parameterized pooling if true, otherwise use conv3d
            use_addictive_sampling: whether use additive up-sampling layer for decoding.
            concat_skip: when up-sampling, concatenate skipped tensor if true, otherwise use addition
        """
        self.use_additive_upsampling = use_addictive_sampling
        super().__init__(spatial_dims=spatial_dims, in_channels=in_channels, num_channel_initial=num_channel_initial, extract_levels=extract_levels, depth=max(extract_levels), out_kernel_initializer=out_kernel_initializer, out_activation=out_activation, out_channels=out_channels, pooling=pooling, concat_skip=concat_skip, encode_kernel_sizes=[7] + [3] * max(extract_levels))

    def build_bottom_block(self, in_channels: int, out_channels: int):
        kernel_size = self.encode_kernel_sizes[self.depth]
        return get_conv_block(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size)

    def build_up_sampling_block(self, in_channels: int, out_channels: int) ->nn.Module:
        if self.use_additive_upsampling:
            return AdditiveUpSampleBlock(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels)
        return get_deconv_block(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels)


class ResNetBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes: int, planes: int, spatial_dims: int=3, stride: int=1, downsample: Union[nn.Module, partial, None]=None) ->None:
        """
        Args:
            in_planes: number of input channels.
            planes: number of output channels.
            spatial_dims: number of spatial dimensions of the input image.
            stride: stride to use for first conv layer.
            downsample: which downsample layer to use.
        """
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]
        self.conv1 = conv_type(in_planes, planes, kernel_size=3, padding=1, stride=stride, bias=False)
        self.bn1 = norm_type(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv_type(planes, planes, kernel_size=3, padding=1, bias=False)
        self.bn2 = norm_type(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        residual = x
        out: torch.Tensor = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


class ResNetBottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes: int, planes: int, spatial_dims: int=3, stride: int=1, downsample: Union[nn.Module, partial, None]=None) ->None:
        """
        Args:
            in_planes: number of input channels.
            planes: number of output channels (taking expansion into account).
            spatial_dims: number of spatial dimensions of the input image.
            stride: stride to use for second conv layer.
            downsample: which downsample layer to use.
        """
        super().__init__()
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]
        self.conv1 = conv_type(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = norm_type(planes)
        self.conv2 = conv_type(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = norm_type(planes)
        self.conv3 = conv_type(planes, planes * self.expansion, kernel_size=1, bias=False)
        self.bn3 = norm_type(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        residual = x
        out: torch.Tensor = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        out = self.bn3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


def get_avgpool():
    return [0, 1, (1, 1), (1, 1, 1)]


class ResNet(nn.Module):
    """
    ResNet based on: `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`_
    and `Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? <https://arxiv.org/pdf/1711.09577.pdf>`_.
    Adapted from `<https://github.com/kenshohara/3D-ResNets-PyTorch/tree/master/models>`_.

    Args:
        block: which ResNet block to use, either Basic or Bottleneck.
            ResNet block class or str.
            for Basic: ResNetBlock or 'basic'
            for Bottleneck: ResNetBottleneck or 'bottleneck'
        layers: how many layers to use.
        block_inplanes: determine the size of planes at each step. Also tunable with widen_factor.
        spatial_dims: number of spatial dimensions of the input image.
        n_input_channels: number of input channels for first convolutional layer.
        conv1_t_size: size of first convolution layer, determines kernel and padding.
        conv1_t_stride: stride of first convolution layer.
        no_max_pool: bool argument to determine if to use maxpool layer.
        shortcut_type: which downsample block to use. Options are 'A', 'B', default to 'B'.
            - 'A': using `self._downsample_basic_block`.
            - 'B': kernel_size 1 conv + norm.
        widen_factor: widen output for each layer.
        num_classes: number of output (classifications).
        feed_forward: whether to add the FC layer for the output, default to `True`.
        bias_downsample: whether to use bias term in the downsampling block when `shortcut_type` is 'B', default to `True`.

    """

    def __init__(self, block: Union[Type[Union[ResNetBlock, ResNetBottleneck]], str], layers: List[int], block_inplanes: List[int], spatial_dims: int=3, n_input_channels: int=3, conv1_t_size: Union[Tuple[int], int]=7, conv1_t_stride: Union[Tuple[int], int]=1, no_max_pool: bool=False, shortcut_type: str='B', widen_factor: float=1.0, num_classes: int=400, feed_forward: bool=True, bias_downsample: bool=True) ->None:
        super().__init__()
        if isinstance(block, str):
            if block == 'basic':
                block = ResNetBlock
            elif block == 'bottleneck':
                block = ResNetBottleneck
            else:
                raise ValueError("Unknown block '%s', use basic or bottleneck" % block)
        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        pool_type: Type[Union[nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        avgp_type: Type[Union[nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d]] = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        block_avgpool = get_avgpool()
        block_inplanes = [int(x * widen_factor) for x in block_inplanes]
        self.in_planes = block_inplanes[0]
        self.no_max_pool = no_max_pool
        self.bias_downsample = bias_downsample
        conv1_kernel_size = ensure_tuple_rep(conv1_t_size, spatial_dims)
        conv1_stride = ensure_tuple_rep(conv1_t_stride, spatial_dims)
        self.conv1 = conv_type(n_input_channels, self.in_planes, kernel_size=conv1_kernel_size, stride=conv1_stride, padding=tuple(k // 2 for k in conv1_kernel_size), bias=False)
        self.bn1 = norm_type(self.in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = pool_type(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0], spatial_dims, shortcut_type)
        self.layer2 = self._make_layer(block, block_inplanes[1], layers[1], spatial_dims, shortcut_type, stride=2)
        self.layer3 = self._make_layer(block, block_inplanes[2], layers[2], spatial_dims, shortcut_type, stride=2)
        self.layer4 = self._make_layer(block, block_inplanes[3], layers[3], spatial_dims, shortcut_type, stride=2)
        self.avgpool = avgp_type(block_avgpool[spatial_dims])
        self.fc = nn.Linear(block_inplanes[3] * block.expansion, num_classes) if feed_forward else None
        for m in self.modules():
            if isinstance(m, conv_type):
                nn.init.kaiming_normal_(torch.as_tensor(m.weight), mode='fan_out', nonlinearity='relu')
            elif isinstance(m, norm_type):
                nn.init.constant_(torch.as_tensor(m.weight), 1)
                nn.init.constant_(torch.as_tensor(m.bias), 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(torch.as_tensor(m.bias), 0)

    def _downsample_basic_block(self, x: torch.Tensor, planes: int, stride: int, spatial_dims: int=3) ->torch.Tensor:
        out: torch.Tensor = get_pool_layer(('avg', {'kernel_size': 1, 'stride': stride}), spatial_dims=spatial_dims)(x)
        zero_pads = torch.zeros(out.size(0), planes - out.size(1), *out.shape[2:], dtype=out.dtype, device=out.device)
        out = torch.cat([out.data, zero_pads], dim=1)
        return out

    def _make_layer(self, block: Type[Union[ResNetBlock, ResNetBottleneck]], planes: int, blocks: int, spatial_dims: int, shortcut_type: str, stride: int=1) ->nn.Sequential:
        conv_type: Callable = Conv[Conv.CONV, spatial_dims]
        norm_type: Callable = Norm[Norm.BATCH, spatial_dims]
        downsample: Union[nn.Module, partial, None] = None
        if stride != 1 or self.in_planes != planes * block.expansion:
            if look_up_option(shortcut_type, {'A', 'B'}) == 'A':
                downsample = partial(self._downsample_basic_block, planes=planes * block.expansion, stride=stride, spatial_dims=spatial_dims)
            else:
                downsample = nn.Sequential(conv_type(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=self.bias_downsample), norm_type(planes * block.expansion))
        layers = [block(in_planes=self.in_planes, planes=planes, spatial_dims=spatial_dims, stride=stride, downsample=downsample)]
        self.in_planes = planes * block.expansion
        for _i in range(1, blocks):
            layers.append(block(self.in_planes, planes, spatial_dims=spatial_dims))
        return nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        if not self.no_max_pool:
            x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        if self.fc is not None:
            x = self.fc(x)
        return x


def get_upsample_layer(spatial_dims: int, in_channels: int, upsample_mode: Union[UpsampleMode, str]='nontrainable', scale_factor: int=2):
    return UpSample(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=in_channels, scale_factor=scale_factor, mode=upsample_mode, interp_mode=InterpolateMode.LINEAR, align_corners=False)


class SegResNet(nn.Module):
    """
    SegResNet based on `3D MRI brain tumor segmentation using autoencoder regularization
    <https://arxiv.org/pdf/1810.11654.pdf>`_.
    The module does not include the variational autoencoder (VAE).
    The model supports 2D or 3D inputs.

    Args:
        spatial_dims: spatial dimension of the input data. Defaults to 3.
        init_filters: number of output channels for initial convolution layer. Defaults to 8.
        in_channels: number of input channels for the network. Defaults to 1.
        out_channels: number of output channels for the network. Defaults to 2.
        dropout_prob: probability of an element to be zero-ed. Defaults to ``None``.
        act: activation type and arguments. Defaults to ``RELU``.
        norm: feature normalization type and arguments. Defaults to ``GROUP``.
        norm_name: deprecating option for feature normalization type.
        num_groups: deprecating option for group norm. parameters.
        use_conv_final: if add a final convolution block to output. Defaults to ``True``.
        blocks_down: number of down sample blocks in each layer. Defaults to ``[1,2,2,4]``.
        blocks_up: number of up sample blocks in each layer. Defaults to ``[1,1,1]``.
        upsample_mode: [``"deconv"``, ``"nontrainable"``, ``"pixelshuffle"``]
            The mode of upsampling manipulations.
            Using the ``nontrainable`` modes cannot guarantee the model's reproducibility. Defaults to``nontrainable``.

            - ``deconv``, uses transposed convolution layers.
            - ``nontrainable``, uses non-trainable `linear` interpolation.
            - ``pixelshuffle``, uses :py:class:`monai.networks.blocks.SubpixelUpsample`.

    """

    def __init__(self, spatial_dims: int=3, init_filters: int=8, in_channels: int=1, out_channels: int=2, dropout_prob: Optional[float]=None, act: Union[Tuple, str]=('RELU', {'inplace': True}), norm: Union[Tuple, str]=('GROUP', {'num_groups': 8}), norm_name: str='', num_groups: int=8, use_conv_final: bool=True, blocks_down: tuple=(1, 2, 2, 4), blocks_up: tuple=(1, 1, 1), upsample_mode: Union[UpsampleMode, str]=UpsampleMode.NONTRAINABLE):
        super().__init__()
        if spatial_dims not in (2, 3):
            raise ValueError('`spatial_dims` can only be 2 or 3.')
        self.spatial_dims = spatial_dims
        self.init_filters = init_filters
        self.in_channels = in_channels
        self.blocks_down = blocks_down
        self.blocks_up = blocks_up
        self.dropout_prob = dropout_prob
        self.act = act
        self.act_mod = get_act_layer(act)
        if norm_name:
            if norm_name.lower() != 'group':
                raise ValueError(f"Deprecating option 'norm_name={norm_name}', please use 'norm' instead.")
            norm = 'group', {'num_groups': num_groups}
        self.norm = norm
        self.upsample_mode = UpsampleMode(upsample_mode)
        self.use_conv_final = use_conv_final
        self.convInit = get_conv_layer(spatial_dims, in_channels, init_filters)
        self.down_layers = self._make_down_layers()
        self.up_layers, self.up_samples = self._make_up_layers()
        self.conv_final = self._make_final_conv(out_channels)
        if dropout_prob is not None:
            self.dropout = Dropout[Dropout.DROPOUT, spatial_dims](dropout_prob)

    def _make_down_layers(self):
        down_layers = nn.ModuleList()
        blocks_down, spatial_dims, filters, norm = self.blocks_down, self.spatial_dims, self.init_filters, self.norm
        for i, item in enumerate(blocks_down):
            layer_in_channels = filters * 2 ** i
            pre_conv = get_conv_layer(spatial_dims, layer_in_channels // 2, layer_in_channels, stride=2) if i > 0 else nn.Identity()
            down_layer = nn.Sequential(pre_conv, *[ResBlock(spatial_dims, layer_in_channels, norm=norm, act=self.act) for _ in range(item)])
            down_layers.append(down_layer)
        return down_layers

    def _make_up_layers(self):
        up_layers, up_samples = nn.ModuleList(), nn.ModuleList()
        upsample_mode, blocks_up, spatial_dims, filters, norm = self.upsample_mode, self.blocks_up, self.spatial_dims, self.init_filters, self.norm
        n_up = len(blocks_up)
        for i in range(n_up):
            sample_in_channels = filters * 2 ** (n_up - i)
            up_layers.append(nn.Sequential(*[ResBlock(spatial_dims, sample_in_channels // 2, norm=norm, act=self.act) for _ in range(blocks_up[i])]))
            up_samples.append(nn.Sequential(*[get_conv_layer(spatial_dims, sample_in_channels, sample_in_channels // 2, kernel_size=1), get_upsample_layer(spatial_dims, sample_in_channels // 2, upsample_mode=upsample_mode)]))
        return up_layers, up_samples

    def _make_final_conv(self, out_channels: int):
        return nn.Sequential(get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=self.init_filters), self.act_mod, get_conv_layer(self.spatial_dims, self.init_filters, out_channels, kernel_size=1, bias=True))

    def encode(self, x: torch.Tensor) ->Tuple[torch.Tensor, List[torch.Tensor]]:
        x = self.convInit(x)
        if self.dropout_prob is not None:
            x = self.dropout(x)
        down_x = []
        for down in self.down_layers:
            x = down(x)
            down_x.append(x)
        return x, down_x

    def decode(self, x: torch.Tensor, down_x: List[torch.Tensor]) ->torch.Tensor:
        for i, (up, upl) in enumerate(zip(self.up_samples, self.up_layers)):
            x = up(x) + down_x[i + 1]
            x = upl(x)
        if self.use_conv_final:
            x = self.conv_final(x)
        return x

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x, down_x = self.encode(x)
        down_x.reverse()
        x = self.decode(x, down_x)
        return x


class SegResNetVAE(SegResNet):
    """
    SegResNetVAE based on `3D MRI brain tumor segmentation using autoencoder regularization
    <https://arxiv.org/pdf/1810.11654.pdf>`_.
    The module contains the variational autoencoder (VAE).
    The model supports 2D or 3D inputs.

    Args:
        input_image_size: the size of images to input into the network. It is used to
            determine the in_features of the fc layer in VAE.
        vae_estimate_std: whether to estimate the standard deviations in VAE. Defaults to ``False``.
        vae_default_std: if not to estimate the std, use the default value. Defaults to 0.3.
        vae_nz: number of latent variables in VAE. Defaults to 256.
            Where, 128 to represent mean, and 128 to represent std.
        spatial_dims: spatial dimension of the input data. Defaults to 3.
        init_filters: number of output channels for initial convolution layer. Defaults to 8.
        in_channels: number of input channels for the network. Defaults to 1.
        out_channels: number of output channels for the network. Defaults to 2.
        dropout_prob: probability of an element to be zero-ed. Defaults to ``None``.
        act: activation type and arguments. Defaults to ``RELU``.
        norm: feature normalization type and arguments. Defaults to ``GROUP``.
        use_conv_final: if add a final convolution block to output. Defaults to ``True``.
        blocks_down: number of down sample blocks in each layer. Defaults to ``[1,2,2,4]``.
        blocks_up: number of up sample blocks in each layer. Defaults to ``[1,1,1]``.
        upsample_mode: [``"deconv"``, ``"nontrainable"``, ``"pixelshuffle"``]
            The mode of upsampling manipulations.
            Using the ``nontrainable`` modes cannot guarantee the model's reproducibility. Defaults to``nontrainable``.

            - ``deconv``, uses transposed convolution layers.
            - ``nontrainable``, uses non-trainable `linear` interpolation.
            - ``pixelshuffle``, uses :py:class:`monai.networks.blocks.SubpixelUpsample`.
    """

    def __init__(self, input_image_size: Sequence[int], vae_estimate_std: bool=False, vae_default_std: float=0.3, vae_nz: int=256, spatial_dims: int=3, init_filters: int=8, in_channels: int=1, out_channels: int=2, dropout_prob: Optional[float]=None, act: Union[str, tuple]=('RELU', {'inplace': True}), norm: Union[Tuple, str]=('GROUP', {'num_groups': 8}), use_conv_final: bool=True, blocks_down: tuple=(1, 2, 2, 4), blocks_up: tuple=(1, 1, 1), upsample_mode: Union[UpsampleMode, str]=UpsampleMode.NONTRAINABLE):
        super().__init__(spatial_dims=spatial_dims, init_filters=init_filters, in_channels=in_channels, out_channels=out_channels, dropout_prob=dropout_prob, act=act, norm=norm, use_conv_final=use_conv_final, blocks_down=blocks_down, blocks_up=blocks_up, upsample_mode=upsample_mode)
        self.input_image_size = input_image_size
        self.smallest_filters = 16
        zoom = 2 ** (len(self.blocks_down) - 1)
        self.fc_insize = [(s // (2 * zoom)) for s in self.input_image_size]
        self.vae_estimate_std = vae_estimate_std
        self.vae_default_std = vae_default_std
        self.vae_nz = vae_nz
        self._prepare_vae_modules()
        self.vae_conv_final = self._make_final_conv(in_channels)

    def _prepare_vae_modules(self):
        zoom = 2 ** (len(self.blocks_down) - 1)
        v_filters = self.init_filters * zoom
        total_elements = int(self.smallest_filters * np.prod(self.fc_insize))
        self.vae_down = nn.Sequential(get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=v_filters), self.act_mod, get_conv_layer(self.spatial_dims, v_filters, self.smallest_filters, stride=2, bias=True), get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=self.smallest_filters), self.act_mod)
        self.vae_fc1 = nn.Linear(total_elements, self.vae_nz)
        self.vae_fc2 = nn.Linear(total_elements, self.vae_nz)
        self.vae_fc3 = nn.Linear(self.vae_nz, total_elements)
        self.vae_fc_up_sample = nn.Sequential(get_conv_layer(self.spatial_dims, self.smallest_filters, v_filters, kernel_size=1), get_upsample_layer(self.spatial_dims, v_filters, upsample_mode=self.upsample_mode), get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=v_filters), self.act_mod)

    def _get_vae_loss(self, net_input: torch.Tensor, vae_input: torch.Tensor):
        """
        Args:
            net_input: the original input of the network.
            vae_input: the input of VAE module, which is also the output of the network's encoder.
        """
        x_vae = self.vae_down(vae_input)
        x_vae = x_vae.view(-1, self.vae_fc1.in_features)
        z_mean = self.vae_fc1(x_vae)
        z_mean_rand = torch.randn_like(z_mean)
        z_mean_rand.requires_grad_(False)
        if self.vae_estimate_std:
            z_sigma = self.vae_fc2(x_vae)
            z_sigma = F.softplus(z_sigma)
            vae_reg_loss = 0.5 * torch.mean(z_mean ** 2 + z_sigma ** 2 - torch.log(1e-08 + z_sigma ** 2) - 1)
            x_vae = z_mean + z_sigma * z_mean_rand
        else:
            z_sigma = self.vae_default_std
            vae_reg_loss = torch.mean(z_mean ** 2)
            x_vae = z_mean + z_sigma * z_mean_rand
        x_vae = self.vae_fc3(x_vae)
        x_vae = self.act_mod(x_vae)
        x_vae = x_vae.view([-1, self.smallest_filters] + self.fc_insize)
        x_vae = self.vae_fc_up_sample(x_vae)
        for up, upl in zip(self.up_samples, self.up_layers):
            x_vae = up(x_vae)
            x_vae = upl(x_vae)
        x_vae = self.vae_conv_final(x_vae)
        vae_mse_loss = F.mse_loss(net_input, x_vae)
        vae_loss = vae_reg_loss + vae_mse_loss
        return vae_loss

    def forward(self, x):
        net_input = x
        x, down_x = self.encode(x)
        down_x.reverse()
        vae_input = x
        x = self.decode(x, down_x)
        if self.training:
            vae_loss = self._get_vae_loss(net_input, vae_input)
            return x, vae_loss
        return x, None


class SegResBlock(nn.Module):
    """
    Residual network block used SegResNet based on `3D MRI brain tumor segmentation using autoencoder regularization
    <https://arxiv.org/pdf/1810.11654.pdf>`_.
    """

    def __init__(self, spatial_dims: int, in_channels: int, norm: Union[Tuple, str], kernel_size: Union[Tuple, int]=3, act: Union[Tuple, str]='relu') ->None:
        """
        Args:
            spatial_dims: number of spatial dimensions, could be 1, 2 or 3.
            in_channels: number of input channels.
            norm: feature normalization type and arguments.
            kernel_size: convolution kernel size. Defaults to 3.
            act: activation type and arguments. Defaults to ``RELU``.
        """
        super().__init__()
        if isinstance(kernel_size, (tuple, list)):
            padding = tuple(k // 2 for k in kernel_size)
        else:
            padding = kernel_size // 2
        self.norm1 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=in_channels)
        self.act1 = get_act_layer(act)
        self.conv1 = Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)
        self.norm2 = get_norm_layer(name=norm, spatial_dims=spatial_dims, channels=in_channels)
        self.act2 = get_act_layer(act)
        self.conv2 = Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)

    def forward(self, x):
        identity = x
        x = self.conv1(self.act1(self.norm1(x)))
        x = self.conv2(self.act2(self.norm2(x)))
        x += identity
        return x


def aniso_kernel(scale: Union[Tuple, List]):
    """
    A helper function to compute kernel_size, padding and stride for the given scale

    Args:
        scale: scale from a current scale level
    """
    kernel_size = [(3 if scale[k] > 1 else 1) for k in range(len(scale))]
    padding = [(k // 2) for k in kernel_size]
    return kernel_size, padding, scale


class SegResEncoder(nn.Module):
    """
    SegResEncoder based on the econder structure in `3D MRI brain tumor segmentation using autoencoder regularization
    <https://arxiv.org/pdf/1810.11654.pdf>`_.

    Args:
        spatial_dims: spatial dimension of the input data. Defaults to 3.
        init_filters: number of output channels for initial convolution layer. Defaults to 32.
        in_channels: number of input channels for the network. Defaults to 1.
        out_channels: number of output channels for the network. Defaults to 2.
        act: activation type and arguments. Defaults to ``RELU``.
        norm: feature normalization type and arguments. Defaults to ``BATCH``.
        blocks_down: number of downsample blocks in each layer. Defaults to ``[1,2,2,4]``.
        head_module: optional callable module to apply to the final features.
        anisotropic_scales: optional list of scale for each scale level.
    """

    def __init__(self, spatial_dims: int=3, init_filters: int=32, in_channels: int=1, act: Union[Tuple, str]='relu', norm: Union[Tuple, str]='batch', blocks_down: tuple=(1, 2, 2, 4), head_module: Optional[nn.Module]=None, anisotropic_scales: Optional[Tuple]=None):
        super().__init__()
        if spatial_dims not in (1, 2, 3):
            raise ValueError('`spatial_dims` can only be 1, 2 or 3.')
        norm = split_args(norm)
        if has_option(Norm[norm[0], spatial_dims], 'affine'):
            norm[1].setdefault('affine', True)
        act = split_args(act)
        if has_option(Act[act[0]], 'inplace'):
            act[1].setdefault('inplace', True)
        filters = init_filters
        kernel_size, padding, _ = aniso_kernel(anisotropic_scales[0]) if anisotropic_scales else (3, 1, 1)
        self.conv_init = Conv[Conv.CONV, spatial_dims](in_channels=in_channels, out_channels=filters, kernel_size=kernel_size, padding=padding, stride=1, bias=False)
        self.layers = nn.ModuleList()
        for i in range(len(blocks_down)):
            level = nn.ModuleDict()
            kernel_size, padding, stride = aniso_kernel(anisotropic_scales[i]) if anisotropic_scales else (3, 1, 2)
            blocks = [SegResBlock(spatial_dims=spatial_dims, in_channels=filters, kernel_size=kernel_size, norm=norm, act=act) for _ in range(blocks_down[i])]
            level['blocks'] = nn.Sequential(*blocks)
            if i < len(blocks_down) - 1:
                level['downsample'] = Conv[Conv.CONV, spatial_dims](in_channels=filters, out_channels=2 * filters, bias=False, kernel_size=kernel_size, stride=stride, padding=padding)
            else:
                level['downsample'] = nn.Identity()
            self.layers.append(level)
            filters *= 2
        self.head_module = head_module
        self.in_channels = in_channels
        self.blocks_down = blocks_down
        self.init_filters = init_filters
        self.norm = norm
        self.act = act
        self.spatial_dims = spatial_dims

    def _forward(self, x: torch.Tensor) ->List[torch.Tensor]:
        outputs = []
        x = self.conv_init(x)
        for level in self.layers:
            x = level['blocks'](x)
            outputs.append(x)
            x = level['downsample'](x)
        if self.head_module is not None:
            outputs = self.head_module(outputs)
        return outputs

    def forward(self, x: torch.Tensor) ->List[torch.Tensor]:
        return self._forward(x)


def scales_for_resolution(resolution: Union[Tuple, List], n_stages: Optional[int]=None):
    """
    A helper function to compute a schedule of scale at different downsampling levels,
    given the input resolution.

    .. code-block:: python

        scales_for_resolution(resolution=[1,1,5], n_stages=5)

    Args:
        resolution: input image resolution (in mm)
        n_stages: optionally the number of stages of the network
    """
    ndim = len(resolution)
    res = np.array(resolution)
    if not all(res > 0):
        raise ValueError('Resolution must be positive')
    nl = np.floor(np.log2(np.max(res) / res)).astype(np.int32)
    scales = [tuple(np.where(2 ** i >= 2 ** nl, 1, 2)) for i in range(max(nl))]
    if n_stages and n_stages > max(nl):
        scales = scales + [(2,) * ndim] * (n_stages - max(nl))
    else:
        scales = scales[:n_stages]
    return scales


class SegResNetDS(nn.Module):
    """
    SegResNetDS based on `3D MRI brain tumor segmentation using autoencoder regularization
    <https://arxiv.org/pdf/1810.11654.pdf>`_.
    It is similar to https://docs.monai.io/en/stable/networks.html#segresnet, with several
    improvements including deep supervision and non-isotropic kernel support.

    Args:
        spatial_dims: spatial dimension of the input data. Defaults to 3.
        init_filters: number of output channels for initial convolution layer. Defaults to 32.
        in_channels: number of input channels for the network. Defaults to 1.
        out_channels: number of output channels for the network. Defaults to 2.
        act: activation type and arguments. Defaults to ``RELU``.
        norm: feature normalization type and arguments. Defaults to ``BATCH``.
        blocks_down: number of downsample blocks in each layer. Defaults to ``[1,2,2,4]``.
        blocks_up: number of upsample blocks (optional).
        dsdepth: number of levels for deep supervision. This will be the length of the list of outputs at each scale level.
                 At dsdepth==1,only a single output is returned.
        preprocess: optional callable function to apply before the model's forward pass
        resolution: optional input image resolution. When provided, the network will first use non-isotropic kernels to bring
                    image spacing into an approximately isotropic space.
                    Otherwise, by default, the kernel size and downsampling is always isotropic.

    """

    def __init__(self, spatial_dims: int=3, init_filters: int=32, in_channels: int=1, out_channels: int=2, act: Union[Tuple, str]='relu', norm: Union[Tuple, str]='batch', blocks_down: tuple=(1, 2, 2, 4), blocks_up: Optional[Tuple]=None, dsdepth: int=1, preprocess: Optional[Union[nn.Module, Callable]]=None, upsample_mode: Union[UpsampleMode, str]='deconv', resolution: Optional[Tuple]=None):
        super().__init__()
        if spatial_dims not in (1, 2, 3):
            raise ValueError('`spatial_dims` can only be 1, 2 or 3.')
        self.spatial_dims = spatial_dims
        self.init_filters = init_filters
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.act = act
        self.norm = norm
        self.blocks_down = blocks_down
        self.dsdepth = max(dsdepth, 1)
        self.resolution = resolution
        self.preprocess = preprocess
        if resolution is not None:
            if not isinstance(resolution, (list, tuple)):
                raise TypeError('resolution must be a tuple')
            elif not all(r > 0 for r in resolution):
                raise ValueError('resolution must be positive')
        norm = split_args(norm)
        if has_option(Norm[norm[0], spatial_dims], 'affine'):
            norm[1].setdefault('affine', True)
        act = split_args(act)
        if has_option(Act[act[0]], 'inplace'):
            act[1].setdefault('inplace', True)
        anisotropic_scales = None
        if resolution:
            anisotropic_scales = scales_for_resolution(resolution, n_stages=len(blocks_down))
        self.anisotropic_scales = anisotropic_scales
        self.encoder = SegResEncoder(spatial_dims=spatial_dims, init_filters=init_filters, in_channels=in_channels, act=act, norm=norm, blocks_down=blocks_down, anisotropic_scales=anisotropic_scales)
        n_up = len(blocks_down) - 1
        if blocks_up is None:
            blocks_up = (1,) * n_up
        self.blocks_up = blocks_up
        filters = init_filters * 2 ** n_up
        self.up_layers = nn.ModuleList()
        for i in range(n_up):
            filters = filters // 2
            kernel_size, _, stride = aniso_kernel(anisotropic_scales[len(blocks_up) - i - 1]) if anisotropic_scales else (3, 1, 2)
            level = nn.ModuleDict()
            level['upsample'] = UpSample(mode=upsample_mode, spatial_dims=spatial_dims, in_channels=2 * filters, out_channels=filters, kernel_size=kernel_size, scale_factor=stride, bias=False, align_corners=False)
            blocks = [SegResBlock(spatial_dims=spatial_dims, in_channels=filters, kernel_size=kernel_size, norm=norm, act=act) for _ in range(blocks_up[i])]
            level['blocks'] = nn.Sequential(*blocks)
            if len(blocks_up) - i <= dsdepth:
                level['head'] = Conv[Conv.CONV, spatial_dims](in_channels=filters, out_channels=out_channels, kernel_size=1, bias=True)
            else:
                level['head'] = nn.Identity()
            self.up_layers.append(level)
        if n_up == 0:
            level = nn.ModuleDict({'upsample': nn.Identity(), 'blocks': nn.Identity(), 'head': Conv[Conv.CONV, spatial_dims](in_channels=filters, out_channels=out_channels, kernel_size=1, bias=True)})
            self.up_layers.append(level)

    def shape_factor(self):
        """
        Calculate the factors (divisors) that the input image shape must be divisible by
        """
        if self.anisotropic_scales is None:
            d = [2 ** (len(self.blocks_down) - 1)] * self.spatial_dims
        else:
            d = list(np.prod(np.array(self.anisotropic_scales[:-1]), axis=0))
        return d

    def is_valid_shape(self, x):
        """
        Calculate if the input shape is divisible by the minimum factors for the current network configuration
        """
        a = [(i % j == 0) for i, j in zip(x.shape[2:], self.shape_factor())]
        return all(a)

    def _forward(self, x: torch.Tensor) ->Union[torch.Tensor, List[torch.Tensor]]:
        if self.preprocess is not None:
            x = self.preprocess(x)
        if not self.is_valid_shape(x):
            raise ValueError(f'Input spatial dims {x.shape} must be divisible by {self.shape_factor()}')
        x_down = self.encoder(x)
        x_down.reverse()
        x = x_down.pop(0)
        if len(x_down) == 0:
            x_down = [torch.zeros(1, device=x.device, dtype=x.dtype)]
        outputs: List[torch.Tensor] = []
        i = 0
        for level in self.up_layers:
            x = level['upsample'](x)
            x = x + x_down[i]
            x = level['blocks'](x)
            if len(self.up_layers) - i <= self.dsdepth:
                outputs.append(level['head'](x))
            i = i + 1
        outputs.reverse()
        if not self.training or len(outputs) == 1:
            return outputs[0]
        return outputs

    def forward(self, x: torch.Tensor) ->Union[torch.Tensor, List[torch.Tensor]]:
        return self._forward(x)


class SENet(nn.Module):
    """
    SENet based on `Squeeze-and-Excitation Networks <https://arxiv.org/pdf/1709.01507.pdf>`_.
    Adapted from `Cadene Hub 2D version
    <https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py>`_.

    Args:
        spatial_dims: spatial dimension of the input data.
        in_channels: channel number of the input data.
        block: SEBlock class or str.
            for SENet154: SEBottleneck or 'se_bottleneck'
            for SE-ResNet models: SEResNetBottleneck or 'se_resnet_bottleneck'
            for SE-ResNeXt models:  SEResNeXtBottleneck or 'se_resnetxt_bottleneck'
        layers: number of residual blocks for 4 layers of the network (layer1...layer4).
        groups: number of groups for the 3x3 convolution in each bottleneck block.
            for SENet154: 64
            for SE-ResNet models: 1
            for SE-ResNeXt models:  32
        reduction: reduction ratio for Squeeze-and-Excitation modules.
            for all models: 16
        dropout_prob: drop probability for the Dropout layer.
            if `None` the Dropout layer is not used.
            for SENet154: 0.2
            for SE-ResNet models: None
            for SE-ResNeXt models: None
        dropout_dim: determine the dimensions of dropout. Defaults to 1.
            When dropout_dim = 1, randomly zeroes some of the elements for each channel.
            When dropout_dim = 2, Randomly zeroes out entire channels (a channel is a 2D feature map).
            When dropout_dim = 3, Randomly zeroes out entire channels (a channel is a 3D feature map).
        inplanes:  number of input channels for layer1.
            for SENet154: 128
            for SE-ResNet models: 64
            for SE-ResNeXt models: 64
        downsample_kernel_size: kernel size for downsampling convolutions in layer2, layer3 and layer4.
            for SENet154: 3
            for SE-ResNet models: 1
            for SE-ResNeXt models: 1
        input_3x3: If `True`, use three 3x3 convolutions instead of
            a single 7x7 convolution in layer0.
            - For SENet154: True
            - For SE-ResNet models: False
            - For SE-ResNeXt models: False
        num_classes: number of outputs in `last_linear` layer.
            for all models: 1000
    """

    def __init__(self, spatial_dims: int, in_channels: int, block: Union[Type[Union[SEBottleneck, SEResNetBottleneck, SEResNeXtBottleneck]], str], layers: Sequence[int], groups: int, reduction: int, dropout_prob: Optional[float]=0.2, dropout_dim: int=1, inplanes: int=128, downsample_kernel_size: int=3, input_3x3: bool=True, num_classes: int=1000) ->None:
        super().__init__()
        if isinstance(block, str):
            if block == 'se_bottleneck':
                block = SEBottleneck
            elif block == 'se_resnet_bottleneck':
                block = SEResNetBottleneck
            elif block == 'se_resnetxt_bottleneck':
                block = SEResNeXtBottleneck
            else:
                raise ValueError("Unknown block '%s', use se_bottleneck, se_resnet_bottleneck or se_resnetxt_bottleneck" % block)
        relu_type: Type[nn.ReLU] = Act[Act.RELU]
        conv_type: Type[Union[nn.Conv1d, nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
        pool_type: Type[Union[nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]] = Pool[Pool.MAX, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        dropout_type: Type[Union[nn.Dropout, nn.Dropout2d, nn.Dropout3d]] = Dropout[Dropout.DROPOUT, dropout_dim]
        avg_pool_type: Type[Union[nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d]] = Pool[Pool.ADAPTIVEAVG, spatial_dims]
        self.inplanes = inplanes
        self.spatial_dims = spatial_dims
        layer0_modules: List[Tuple[str, Any]]
        if input_3x3:
            layer0_modules = [('conv1', conv_type(in_channels=in_channels, out_channels=64, kernel_size=3, stride=2, padding=1, bias=False)), ('bn1', norm_type(num_features=64)), ('relu1', relu_type(inplace=True)), ('conv2', conv_type(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)), ('bn2', norm_type(num_features=64)), ('relu2', relu_type(inplace=True)), ('conv3', conv_type(in_channels=64, out_channels=inplanes, kernel_size=3, stride=1, padding=1, bias=False)), ('bn3', norm_type(num_features=inplanes)), ('relu3', relu_type(inplace=True))]
        else:
            layer0_modules = [('conv1', conv_type(in_channels=in_channels, out_channels=inplanes, kernel_size=7, stride=2, padding=3, bias=False)), ('bn1', norm_type(num_features=inplanes)), ('relu1', relu_type(inplace=True))]
        layer0_modules.append(('pool', pool_type(kernel_size=3, stride=2, ceil_mode=True)))
        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))
        self.layer1 = self._make_layer(block, planes=64, blocks=layers[0], groups=groups, reduction=reduction, downsample_kernel_size=1)
        self.layer2 = self._make_layer(block, planes=128, blocks=layers[1], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.layer3 = self._make_layer(block, planes=256, blocks=layers[2], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.layer4 = self._make_layer(block, planes=512, blocks=layers[3], stride=2, groups=groups, reduction=reduction, downsample_kernel_size=downsample_kernel_size)
        self.adaptive_avg_pool = avg_pool_type(1)
        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None
        self.last_linear = nn.Linear(512 * block.expansion, num_classes)
        for m in self.modules():
            if isinstance(m, conv_type):
                nn.init.kaiming_normal_(torch.as_tensor(m.weight))
            elif isinstance(m, norm_type):
                nn.init.constant_(torch.as_tensor(m.weight), 1)
                nn.init.constant_(torch.as_tensor(m.bias), 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(torch.as_tensor(m.bias), 0)

    def _make_layer(self, block: Type[Union[SEBottleneck, SEResNetBottleneck, SEResNeXtBottleneck]], planes: int, blocks: int, groups: int, reduction: int, stride: int=1, downsample_kernel_size: int=1) ->nn.Sequential:
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = Convolution(spatial_dims=self.spatial_dims, in_channels=self.inplanes, out_channels=planes * block.expansion, strides=stride, kernel_size=downsample_kernel_size, act=None, norm=Norm.BATCH, bias=False)
        layers = []
        layers.append(block(spatial_dims=self.spatial_dims, inplanes=self.inplanes, planes=planes, groups=groups, reduction=reduction, stride=stride, downsample=downsample))
        self.inplanes = planes * block.expansion
        for _num in range(1, blocks):
            layers.append(block(spatial_dims=self.spatial_dims, inplanes=self.inplanes, planes=planes, groups=groups, reduction=reduction))
        return nn.Sequential(*layers)

    def features(self, x: torch.Tensor):
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x

    def logits(self, x: torch.Tensor):
        x = self.adaptive_avg_pool(x)
        if self.dropout is not None:
            x = self.dropout(x)
        x = torch.flatten(x, 1)
        x = self.last_linear(x)
        return x

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x = self.features(x)
        x = self.logits(x)
        return x


class SENet154(SENet):
    """SENet154 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2."""

    def __init__(self, layers: Sequence[int]=(3, 8, 36, 3), groups: int=64, reduction: int=16, pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(block=SEBottleneck, layers=layers, groups=groups, reduction=reduction, **kwargs)
        if pretrained:
            _load_state_dict(self, 'senet154', progress)


class SEResNet50(SENet):
    """SEResNet50 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2."""

    def __init__(self, layers: Sequence[int]=(3, 4, 6, 3), groups: int=1, reduction: int=16, dropout_prob: Optional[float]=None, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(block=SEResNetBottleneck, layers=layers, groups=groups, reduction=reduction, dropout_prob=dropout_prob, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnet50', progress)


class SEResNet101(SENet):
    """
    SEResNet101 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2.
    """

    def __init__(self, layers: Sequence[int]=(3, 4, 23, 3), groups: int=1, reduction: int=16, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(block=SEResNetBottleneck, layers=layers, groups=groups, reduction=reduction, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnet101', progress)


class SEResNet152(SENet):
    """
    SEResNet152 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2.
    """

    def __init__(self, layers: Sequence[int]=(3, 8, 36, 3), groups: int=1, reduction: int=16, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(block=SEResNetBottleneck, layers=layers, groups=groups, reduction=reduction, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnet152', progress)


class SEResNext50(SENet):
    """
    SEResNext50 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2.
    """

    def __init__(self, layers: Sequence[int]=(3, 4, 6, 3), groups: int=32, reduction: int=16, dropout_prob: Optional[float]=None, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(block=SEResNeXtBottleneck, layers=layers, groups=groups, dropout_prob=dropout_prob, reduction=reduction, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnext50_32x4d', progress)


class SEResNext101(SENet):
    """
    SEResNext101 based on `Squeeze-and-Excitation Networks` with optional pretrained support when spatial_dims is 2.
    """

    def __init__(self, layers: Sequence[int]=(3, 4, 23, 3), groups: int=32, reduction: int=16, dropout_prob: Optional[float]=None, inplanes: int=64, downsample_kernel_size: int=1, input_3x3: bool=False, pretrained: bool=False, progress: bool=True, **kwargs) ->None:
        super().__init__(block=SEResNeXtBottleneck, layers=layers, groups=groups, dropout_prob=dropout_prob, reduction=reduction, inplanes=inplanes, downsample_kernel_size=downsample_kernel_size, input_3x3=input_3x3, **kwargs)
        if pretrained:
            _load_state_dict(self, 'se_resnext101_32x4d', progress)


class PatchMergingV2(nn.Module):
    """
    Patch merging layer based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer
    """

    def __init__(self, dim: int, norm_layer: Type[LayerNorm]=nn.LayerNorm, spatial_dims: int=3) ->None:
        """
        Args:
            dim: number of feature channels.
            norm_layer: normalization layer.
            spatial_dims: number of spatial dims.
        """
        super().__init__()
        self.dim = dim
        if spatial_dims == 3:
            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)
            self.norm = norm_layer(8 * dim)
        elif spatial_dims == 2:
            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
            self.norm = norm_layer(4 * dim)

    def forward(self, x):
        x_shape = x.size()
        if len(x_shape) == 5:
            b, d, h, w, c = x_shape
            pad_input = h % 2 == 1 or w % 2 == 1 or d % 2 == 1
            if pad_input:
                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))
            x = torch.cat([x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1)
        elif len(x_shape) == 4:
            b, h, w, c = x_shape
            pad_input = h % 2 == 1 or w % 2 == 1
            if pad_input:
                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))
            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)
        x = self.norm(x)
        x = self.reduction(x)
        return x


class PatchMerging(PatchMergingV2):
    """The `PatchMerging` module previously defined in v0.9.0."""

    def forward(self, x):
        x_shape = x.size()
        if len(x_shape) == 4:
            return super().forward(x)
        if len(x_shape) != 5:
            raise ValueError(f'expecting 5D x, got {x.shape}.')
        b, d, h, w, c = x_shape
        pad_input = h % 2 == 1 or w % 2 == 1 or d % 2 == 1
        if pad_input:
            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))
        x0 = x[:, 0::2, 0::2, 0::2, :]
        x1 = x[:, 1::2, 0::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, 0::2, :]
        x3 = x[:, 0::2, 0::2, 1::2, :]
        x4 = x[:, 1::2, 0::2, 1::2, :]
        x5 = x[:, 0::2, 1::2, 0::2, :]
        x6 = x[:, 0::2, 0::2, 1::2, :]
        x7 = x[:, 1::2, 1::2, 1::2, :]
        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)
        x = self.norm(x)
        x = self.reduction(x)
        return x


MERGING_MODE = {'merging': PatchMerging, 'mergingv2': PatchMergingV2}


class WindowAttention(nn.Module):
    """
    Window based multi-head self attention module with relative position bias based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer
    """

    def __init__(self, dim: int, num_heads: int, window_size: Sequence[int], qkv_bias: bool=False, attn_drop: float=0.0, proj_drop: float=0.0) ->None:
        """
        Args:
            dim: number of feature channels.
            num_heads: number of attention heads.
            window_size: local window size.
            qkv_bias: add a learnable bias to query, key, value.
            attn_drop: attention dropout rate.
            proj_drop: dropout rate of output.
        """
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5
        mesh_args = torch.meshgrid.__kwdefaults__
        if len(self.window_size) == 3:
            self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1), num_heads))
            coords_d = torch.arange(self.window_size[0])
            coords_h = torch.arange(self.window_size[1])
            coords_w = torch.arange(self.window_size[2])
            if mesh_args is not None:
                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing='ij'))
            else:
                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))
            coords_flatten = torch.flatten(coords, 1)
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()
            relative_coords[:, :, 0] += self.window_size[0] - 1
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 2] += self.window_size[2] - 1
            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)
            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1
        elif len(self.window_size) == 2:
            self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))
            coords_h = torch.arange(self.window_size[0])
            coords_w = torch.arange(self.window_size[1])
            if mesh_args is not None:
                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))
            else:
                coords = torch.stack(torch.meshgrid(coords_h, coords_w))
            coords_flatten = torch.flatten(coords, 1)
            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
            relative_coords = relative_coords.permute(1, 2, 0).contiguous()
            relative_coords[:, :, 0] += self.window_size[0] - 1
            relative_coords[:, :, 1] += self.window_size[1] - 1
            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer('relative_position_index', relative_position_index)
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.relative_position_bias_table, std=0.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask):
        b, n, c = x.shape
        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.clone()[:n, :n].reshape(-1)].reshape(n, n, -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)
        if mask is not None:
            nw = mask.shape[0]
            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, n, n)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).reshape(b, n, c)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


def get_window_size(x_size, window_size, shift_size=None):
    """Computing window size based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer

     Args:
        x_size: input size.
        window_size: local window size.
        shift_size: window shifting size.
    """
    use_window_size = list(window_size)
    if shift_size is not None:
        use_shift_size = list(shift_size)
    for i in range(len(x_size)):
        if x_size[i] <= window_size[i]:
            use_window_size[i] = x_size[i]
            if shift_size is not None:
                use_shift_size[i] = 0
    if shift_size is None:
        return tuple(use_window_size)
    else:
        return tuple(use_window_size), tuple(use_shift_size)


def window_partition(x, window_size):
    """window partition operation based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer

     Args:
        x: input tensor.
        window_size: local window size.
    """
    x_shape = x.size()
    if len(x_shape) == 5:
        b, d, h, w, c = x_shape
        x = x.view(b, d // window_size[0], window_size[0], h // window_size[1], window_size[1], w // window_size[2], window_size[2], c)
        windows = x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)
    elif len(x_shape) == 4:
        b, h, w, c = x.shape
        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)
        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)
    return windows


def window_reverse(windows, window_size, dims):
    """window reverse operation based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer

     Args:
        windows: windows tensor.
        window_size: local window size.
        dims: dimension values.
    """
    if len(dims) == 4:
        b, d, h, w = dims
        x = windows.view(b, d // window_size[0], h // window_size[1], w // window_size[2], window_size[0], window_size[1], window_size[2], -1)
        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)
    elif len(dims) == 3:
        b, h, w = dims
        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)
        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)
    return x


class SwinTransformerBlock(nn.Module):
    """
    Swin Transformer block based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer
    """

    def __init__(self, dim: int, num_heads: int, window_size: Sequence[int], shift_size: Sequence[int], mlp_ratio: float=4.0, qkv_bias: bool=True, drop: float=0.0, attn_drop: float=0.0, drop_path: float=0.0, act_layer: str='GELU', norm_layer: Type[LayerNorm]=nn.LayerNorm, use_checkpoint: bool=False) ->None:
        """
        Args:
            dim: number of feature channels.
            num_heads: number of attention heads.
            window_size: local window size.
            shift_size: window shift size.
            mlp_ratio: ratio of mlp hidden dim to embedding dim.
            qkv_bias: add a learnable bias to query, key, value.
            drop: dropout rate.
            attn_drop: attention dropout rate.
            drop_path: stochastic depth rate.
            act_layer: activation layer.
            norm_layer: normalization layer.
            use_checkpoint: use gradient checkpointing for reduced memory usage.
        """
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        self.use_checkpoint = use_checkpoint
        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(dim, window_size=self.window_size, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode='swin')

    def forward_part1(self, x, mask_matrix):
        x_shape = x.size()
        x = self.norm1(x)
        if len(x_shape) == 5:
            b, d, h, w, c = x.shape
            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)
            pad_l = pad_t = pad_d0 = 0
            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]
            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]
            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]
            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))
            _, dp, hp, wp, _ = x.shape
            dims = [b, dp, hp, wp]
        elif len(x_shape) == 4:
            b, h, w, c = x.shape
            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)
            pad_l = pad_t = 0
            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]
            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]
            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
            _, hp, wp, _ = x.shape
            dims = [b, hp, wp]
        if any(i > 0 for i in shift_size):
            if len(x_shape) == 5:
                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))
            elif len(x_shape) == 4:
                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))
            attn_mask = mask_matrix
        else:
            shifted_x = x
            attn_mask = None
        x_windows = window_partition(shifted_x, window_size)
        attn_windows = self.attn(x_windows, mask=attn_mask)
        attn_windows = attn_windows.view(-1, *(window_size + (c,)))
        shifted_x = window_reverse(attn_windows, window_size, dims)
        if any(i > 0 for i in shift_size):
            if len(x_shape) == 5:
                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))
            elif len(x_shape) == 4:
                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))
        else:
            x = shifted_x
        if len(x_shape) == 5:
            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:
                x = x[:, :d, :h, :w, :].contiguous()
        elif len(x_shape) == 4:
            if pad_r > 0 or pad_b > 0:
                x = x[:, :h, :w, :].contiguous()
        return x

    def forward_part2(self, x):
        return self.drop_path(self.mlp(self.norm2(x)))

    def load_from(self, weights, n_block, layer):
        root = f'module.{layer}.0.blocks.{n_block}.'
        block_names = ['norm1.weight', 'norm1.bias', 'attn.relative_position_bias_table', 'attn.relative_position_index', 'attn.qkv.weight', 'attn.qkv.bias', 'attn.proj.weight', 'attn.proj.bias', 'norm2.weight', 'norm2.bias', 'mlp.fc1.weight', 'mlp.fc1.bias', 'mlp.fc2.weight', 'mlp.fc2.bias']
        with torch.no_grad():
            self.norm1.weight.copy_(weights['state_dict'][root + block_names[0]])
            self.norm1.bias.copy_(weights['state_dict'][root + block_names[1]])
            self.attn.relative_position_bias_table.copy_(weights['state_dict'][root + block_names[2]])
            self.attn.relative_position_index.copy_(weights['state_dict'][root + block_names[3]])
            self.attn.qkv.weight.copy_(weights['state_dict'][root + block_names[4]])
            self.attn.qkv.bias.copy_(weights['state_dict'][root + block_names[5]])
            self.attn.proj.weight.copy_(weights['state_dict'][root + block_names[6]])
            self.attn.proj.bias.copy_(weights['state_dict'][root + block_names[7]])
            self.norm2.weight.copy_(weights['state_dict'][root + block_names[8]])
            self.norm2.bias.copy_(weights['state_dict'][root + block_names[9]])
            self.mlp.linear1.weight.copy_(weights['state_dict'][root + block_names[10]])
            self.mlp.linear1.bias.copy_(weights['state_dict'][root + block_names[11]])
            self.mlp.linear2.weight.copy_(weights['state_dict'][root + block_names[12]])
            self.mlp.linear2.bias.copy_(weights['state_dict'][root + block_names[13]])

    def forward(self, x, mask_matrix):
        shortcut = x
        if self.use_checkpoint:
            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix)
        else:
            x = self.forward_part1(x, mask_matrix)
        x = shortcut + self.drop_path(x)
        if self.use_checkpoint:
            x = x + checkpoint.checkpoint(self.forward_part2, x)
        else:
            x = x + self.forward_part2(x)
        return x


def compute_mask(dims, window_size, shift_size, device):
    """Computing region masks based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer

     Args:
        dims: dimension values.
        window_size: local window size.
        shift_size: shift size.
        device: device.
    """
    cnt = 0
    if len(dims) == 3:
        d, h, w = dims
        img_mask = torch.zeros((1, d, h, w, 1), device=device)
        for d in (slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None)):
            for h in (slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None)):
                for w in (slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None)):
                    img_mask[:, d, h, w, :] = cnt
                    cnt += 1
    elif len(dims) == 2:
        h, w = dims
        img_mask = torch.zeros((1, h, w, 1), device=device)
        for h in (slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None)):
            for w in (slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None)):
                img_mask[:, h, w, :] = cnt
                cnt += 1
    mask_windows = window_partition(img_mask, window_size)
    mask_windows = mask_windows.squeeze(-1)
    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
    return attn_mask


rearrange, _ = optional_import('einops', name='rearrange')


class BasicLayer(nn.Module):
    """
    Basic Swin Transformer layer in one stage based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer
    """

    def __init__(self, dim: int, depth: int, num_heads: int, window_size: Sequence[int], drop_path: list, mlp_ratio: float=4.0, qkv_bias: bool=False, drop: float=0.0, attn_drop: float=0.0, norm_layer: Type[LayerNorm]=nn.LayerNorm, downsample: Optional[nn.Module]=None, use_checkpoint: bool=False) ->None:
        """
        Args:
            dim: number of feature channels.
            depth: number of layers in each stage.
            num_heads: number of attention heads.
            window_size: local window size.
            drop_path: stochastic depth rate.
            mlp_ratio: ratio of mlp hidden dim to embedding dim.
            qkv_bias: add a learnable bias to query, key, value.
            drop: dropout rate.
            attn_drop: attention dropout rate.
            norm_layer: normalization layer.
            downsample: an optional downsampling layer at the end of the layer.
            use_checkpoint: use gradient checkpointing for reduced memory usage.
        """
        super().__init__()
        self.window_size = window_size
        self.shift_size = tuple(i // 2 for i in window_size)
        self.no_shift = tuple(0 for i in window_size)
        self.depth = depth
        self.use_checkpoint = use_checkpoint
        self.blocks = nn.ModuleList([SwinTransformerBlock(dim=dim, num_heads=num_heads, window_size=self.window_size, shift_size=self.no_shift if i % 2 == 0 else self.shift_size, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path, norm_layer=norm_layer, use_checkpoint=use_checkpoint) for i in range(depth)])
        self.downsample = downsample
        if callable(self.downsample):
            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))

    def forward(self, x):
        x_shape = x.size()
        if len(x_shape) == 5:
            b, c, d, h, w = x_shape
            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)
            x = rearrange(x, 'b c d h w -> b d h w c')
            dp = int(np.ceil(d / window_size[0])) * window_size[0]
            hp = int(np.ceil(h / window_size[1])) * window_size[1]
            wp = int(np.ceil(w / window_size[2])) * window_size[2]
            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)
            for blk in self.blocks:
                x = blk(x, attn_mask)
            x = x.view(b, d, h, w, -1)
            if self.downsample is not None:
                x = self.downsample(x)
            x = rearrange(x, 'b d h w c -> b c d h w')
        elif len(x_shape) == 4:
            b, c, h, w = x_shape
            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)
            x = rearrange(x, 'b c h w -> b h w c')
            hp = int(np.ceil(h / window_size[0])) * window_size[0]
            wp = int(np.ceil(w / window_size[1])) * window_size[1]
            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)
            for blk in self.blocks:
                x = blk(x, attn_mask)
            x = x.view(b, h, w, -1)
            if self.downsample is not None:
                x = self.downsample(x)
            x = rearrange(x, 'b h w c -> b c h w')
        return x


class SwinTransformer(nn.Module):
    """
    Swin Transformer based on: "Liu et al.,
    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    <https://arxiv.org/abs/2103.14030>"
    https://github.com/microsoft/Swin-Transformer
    """

    def __init__(self, in_chans: int, embed_dim: int, window_size: Sequence[int], patch_size: Sequence[int], depths: Sequence[int], num_heads: Sequence[int], mlp_ratio: float=4.0, qkv_bias: bool=True, drop_rate: float=0.0, attn_drop_rate: float=0.0, drop_path_rate: float=0.0, norm_layer: Type[LayerNorm]=nn.LayerNorm, patch_norm: bool=False, use_checkpoint: bool=False, spatial_dims: int=3, downsample='merging') ->None:
        """
        Args:
            in_chans: dimension of input channels.
            embed_dim: number of linear projection output channels.
            window_size: local window size.
            patch_size: patch size.
            depths: number of layers in each stage.
            num_heads: number of attention heads.
            mlp_ratio: ratio of mlp hidden dim to embedding dim.
            qkv_bias: add a learnable bias to query, key, value.
            drop_rate: dropout rate.
            attn_drop_rate: attention dropout rate.
            drop_path_rate: stochastic depth rate.
            norm_layer: normalization layer.
            patch_norm: add normalization after patch embedding.
            use_checkpoint: use gradient checkpointing for reduced memory usage.
            spatial_dims: spatial dimension.
            downsample: module used for downsampling, available options are `"mergingv2"`, `"merging"` and a
                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.
                The default is currently `"merging"` (the original version defined in v0.9.0).
        """
        super().__init__()
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.patch_norm = patch_norm
        self.window_size = window_size
        self.patch_size = patch_size
        self.patch_embed = PatchEmbed(patch_size=self.patch_size, in_chans=in_chans, embed_dim=embed_dim, norm_layer=norm_layer if self.patch_norm else None, spatial_dims=spatial_dims)
        self.pos_drop = nn.Dropout(p=drop_rate)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        self.layers1 = nn.ModuleList()
        self.layers2 = nn.ModuleList()
        self.layers3 = nn.ModuleList()
        self.layers4 = nn.ModuleList()
        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer), depth=depths[i_layer], num_heads=num_heads[i_layer], window_size=self.window_size, drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])], mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate, norm_layer=norm_layer, downsample=down_sample_mod, use_checkpoint=use_checkpoint)
            if i_layer == 0:
                self.layers1.append(layer)
            elif i_layer == 1:
                self.layers2.append(layer)
            elif i_layer == 2:
                self.layers3.append(layer)
            elif i_layer == 3:
                self.layers4.append(layer)
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))

    def proj_out(self, x, normalize=False):
        if normalize:
            x_shape = x.size()
            if len(x_shape) == 5:
                n, ch, d, h, w = x_shape
                x = rearrange(x, 'n c d h w -> n d h w c')
                x = F.layer_norm(x, [ch])
                x = rearrange(x, 'n d h w c -> n c d h w')
            elif len(x_shape) == 4:
                n, ch, h, w = x_shape
                x = rearrange(x, 'n c h w -> n h w c')
                x = F.layer_norm(x, [ch])
                x = rearrange(x, 'n h w c -> n c h w')
        return x

    def forward(self, x, normalize=True):
        x0 = self.patch_embed(x)
        x0 = self.pos_drop(x0)
        x0_out = self.proj_out(x0, normalize)
        x1 = self.layers1[0](x0.contiguous())
        x1_out = self.proj_out(x1, normalize)
        x2 = self.layers2[0](x1.contiguous())
        x2_out = self.proj_out(x2, normalize)
        x3 = self.layers3[0](x2.contiguous())
        x3_out = self.proj_out(x3, normalize)
        x4 = self.layers4[0](x3.contiguous())
        x4_out = self.proj_out(x4, normalize)
        return [x0_out, x1_out, x2_out, x3_out, x4_out]


class SwinUNETR(nn.Module):
    """
    Swin UNETR based on: "Hatamizadeh et al.,
    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images
    <https://arxiv.org/abs/2201.01266>"
    """

    def __init__(self, img_size: Union[Sequence[int], int], in_channels: int, out_channels: int, depths: Sequence[int]=(2, 2, 2, 2), num_heads: Sequence[int]=(3, 6, 12, 24), feature_size: int=24, norm_name: Union[Tuple, str]='instance', drop_rate: float=0.0, attn_drop_rate: float=0.0, dropout_path_rate: float=0.0, normalize: bool=True, use_checkpoint: bool=False, spatial_dims: int=3, downsample='merging') ->None:
        """
        Args:
            img_size: dimension of input image.
            in_channels: dimension of input channels.
            out_channels: dimension of output channels.
            feature_size: dimension of network feature size.
            depths: number of layers in each stage.
            num_heads: number of attention heads.
            norm_name: feature normalization type and arguments.
            drop_rate: dropout rate.
            attn_drop_rate: attention dropout rate.
            dropout_path_rate: drop path rate.
            normalize: normalize output intermediate features in each stage.
            use_checkpoint: use gradient checkpointing for reduced memory usage.
            spatial_dims: number of spatial dims.
            downsample: module used for downsampling, available options are `"mergingv2"`, `"merging"` and a
                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.
                The default is currently `"merging"` (the original version defined in v0.9.0).

        Examples::

            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.
            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)

            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.
            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))

            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.
            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)

        """
        super().__init__()
        img_size = ensure_tuple_rep(img_size, spatial_dims)
        patch_size = ensure_tuple_rep(2, spatial_dims)
        window_size = ensure_tuple_rep(7, spatial_dims)
        if spatial_dims not in (2, 3):
            raise ValueError('spatial dimension should be 2 or 3.')
        for m, p in zip(img_size, patch_size):
            for i in range(5):
                if m % np.power(p, i + 1) != 0:
                    raise ValueError('input image size (img_size) should be divisible by stage-wise image resolution.')
        if not 0 <= drop_rate <= 1:
            raise ValueError('dropout rate should be between 0 and 1.')
        if not 0 <= attn_drop_rate <= 1:
            raise ValueError('attention dropout rate should be between 0 and 1.')
        if not 0 <= dropout_path_rate <= 1:
            raise ValueError('drop path rate should be between 0 and 1.')
        if feature_size % 12 != 0:
            raise ValueError('feature_size should be divisible by 12.')
        self.normalize = normalize
        self.swinViT = SwinTransformer(in_chans=in_channels, embed_dim=feature_size, window_size=window_size, patch_size=patch_size, depths=depths, num_heads=num_heads, mlp_ratio=4.0, qkv_bias=True, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=dropout_path_rate, norm_layer=nn.LayerNorm, use_checkpoint=use_checkpoint, spatial_dims=spatial_dims, downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample)
        self.encoder1 = UnetrBasicBlock(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=feature_size, kernel_size=3, stride=1, norm_name=norm_name, res_block=True)
        self.encoder2 = UnetrBasicBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=feature_size, kernel_size=3, stride=1, norm_name=norm_name, res_block=True)
        self.encoder3 = UnetrBasicBlock(spatial_dims=spatial_dims, in_channels=2 * feature_size, out_channels=2 * feature_size, kernel_size=3, stride=1, norm_name=norm_name, res_block=True)
        self.encoder4 = UnetrBasicBlock(spatial_dims=spatial_dims, in_channels=4 * feature_size, out_channels=4 * feature_size, kernel_size=3, stride=1, norm_name=norm_name, res_block=True)
        self.encoder10 = UnetrBasicBlock(spatial_dims=spatial_dims, in_channels=16 * feature_size, out_channels=16 * feature_size, kernel_size=3, stride=1, norm_name=norm_name, res_block=True)
        self.decoder5 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=16 * feature_size, out_channels=8 * feature_size, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=True)
        self.decoder4 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size * 8, out_channels=feature_size * 4, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=True)
        self.decoder3 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size * 4, out_channels=feature_size * 2, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=True)
        self.decoder2 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size * 2, out_channels=feature_size, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=True)
        self.decoder1 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=feature_size, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=True)
        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)

    def load_from(self, weights):
        with torch.no_grad():
            self.swinViT.patch_embed.proj.weight.copy_(weights['state_dict']['module.patch_embed.proj.weight'])
            self.swinViT.patch_embed.proj.bias.copy_(weights['state_dict']['module.patch_embed.proj.bias'])
            for bname, block in self.swinViT.layers1[0].blocks.named_children():
                block.load_from(weights, n_block=bname, layer='layers1')
            self.swinViT.layers1[0].downsample.reduction.weight.copy_(weights['state_dict']['module.layers1.0.downsample.reduction.weight'])
            self.swinViT.layers1[0].downsample.norm.weight.copy_(weights['state_dict']['module.layers1.0.downsample.norm.weight'])
            self.swinViT.layers1[0].downsample.norm.bias.copy_(weights['state_dict']['module.layers1.0.downsample.norm.bias'])
            for bname, block in self.swinViT.layers2[0].blocks.named_children():
                block.load_from(weights, n_block=bname, layer='layers2')
            self.swinViT.layers2[0].downsample.reduction.weight.copy_(weights['state_dict']['module.layers2.0.downsample.reduction.weight'])
            self.swinViT.layers2[0].downsample.norm.weight.copy_(weights['state_dict']['module.layers2.0.downsample.norm.weight'])
            self.swinViT.layers2[0].downsample.norm.bias.copy_(weights['state_dict']['module.layers2.0.downsample.norm.bias'])
            for bname, block in self.swinViT.layers3[0].blocks.named_children():
                block.load_from(weights, n_block=bname, layer='layers3')
            self.swinViT.layers3[0].downsample.reduction.weight.copy_(weights['state_dict']['module.layers3.0.downsample.reduction.weight'])
            self.swinViT.layers3[0].downsample.norm.weight.copy_(weights['state_dict']['module.layers3.0.downsample.norm.weight'])
            self.swinViT.layers3[0].downsample.norm.bias.copy_(weights['state_dict']['module.layers3.0.downsample.norm.bias'])
            for bname, block in self.swinViT.layers4[0].blocks.named_children():
                block.load_from(weights, n_block=bname, layer='layers4')
            self.swinViT.layers4[0].downsample.reduction.weight.copy_(weights['state_dict']['module.layers4.0.downsample.reduction.weight'])
            self.swinViT.layers4[0].downsample.norm.weight.copy_(weights['state_dict']['module.layers4.0.downsample.norm.weight'])
            self.swinViT.layers4[0].downsample.norm.bias.copy_(weights['state_dict']['module.layers4.0.downsample.norm.bias'])

    def forward(self, x_in):
        hidden_states_out = self.swinViT(x_in, self.normalize)
        enc0 = self.encoder1(x_in)
        enc1 = self.encoder2(hidden_states_out[0])
        enc2 = self.encoder3(hidden_states_out[1])
        enc3 = self.encoder4(hidden_states_out[2])
        dec4 = self.encoder10(hidden_states_out[4])
        dec3 = self.decoder5(dec4, hidden_states_out[3])
        dec2 = self.decoder4(dec3, enc3)
        dec1 = self.decoder3(dec2, enc2)
        dec0 = self.decoder2(dec1, enc1)
        out = self.decoder1(dec0, enc0)
        logits = self.out(out)
        return logits


class TorchVisionFCModel(NetAdapter):
    """
    Customize the fully connected layer of (pretrained) TorchVision model or replace it by convolutional layer.

    This class supports two primary use cases:

        - use ``pool=None`` to indicate no modification in the pooling layers. It should be used with ``fc_name``
          to locate the target FC layer to modify:
          In this case, the class will load a torchvision classification model,
          replace the last fully connected (FC) layer with a new FC layer with ``num_classes`` outputs,
          example input arguments: ``use_conv=False, pool=None, fc_name="heads.head"``.
          The ``heads.head`` specifies the target FC of the input model, could be found by ``model.named_modules()``,
          for example::

              from torchvision.models import vit_b_16
              print([name[0] for name in vit_b_16().named_modules()])

        - use ``pool=""`` or set it to a tuple of pooling parameters to indicate modifications of both
          the pooling and the FC layer. It should be used with ``node_name`` to locate the model feature outputs:
          In this case, the class will load a torchvision model, remove the existing pooling and FC layers, and

          - append an additional convolution layer:
            ``use_conv=True, pool="", node_name="permute"``
          - append an additional pooling and FC layers:
            ``use_conv=False, pool=("avg", {"kernel_size": 7, "stride": 1}), node_name="permute"``
          - append an additional pooling and convolution layers:
            ``use_conv=True, pool=("avg", {"kernel_size": 7, "stride": 1}), node_name="permute"``

          The ``permute`` in the example is the target feature extraction node of the input
          `model_name`, could be found by using the torchvision feature extraction utilities, for example::

              from torchvision.models.feature_extraction import get_graph_node_names
              from torchvision.models import swin_t
              print(get_graph_node_names(swin_t())[0])

    Args:
        model_name: name of any torchvision model with fully connected layer at the end.
            ``resnet18`` (default), ``resnet34``, ``resnet50``, ``resnet101``, ``resnet152``,
            ``resnext50_32x4d``, ``resnext101_32x8d``, ``wide_resnet50_2``, ``wide_resnet101_2``, ``inception_v3``.
            model details: https://pytorch.org/vision/stable/models.html.
        num_classes: number of classes for the last classification layer. Default to 1.
        dim: number of supported spatial dimensions in the specified model, depends on the model implementation.
            default to 2 as most Torchvision models are for 2D image processing.
        in_channels: number of the input channels of last layer. if None, get it from `in_features` of last layer.
        use_conv: whether to use convolutional layer to replace the last layer, default to False.
        pool: parameters for the pooling layer, when it's a tuple, the first item is name of the pooling layer,
            the second item is dictionary of the initialization args. If None, will not replace the `layers[-2]`.
            default to `("avg", {"kernel_size": 7, "stride": 1})`. ``""`` indicates not adding a pooling layer.
        bias: the bias value when replacing the last layer. if False, the layer will not learn an additive bias,
            default to True.
        pretrained: whether to use the imagenet pretrained weights. Default to False.
        fc_name: the corresponding layer attribute of the last fully connected layer. Defaults to ``"fc"``.
        node_name: the corresponding feature extractor node name of `model`. Defaults to "", not in use.
        weights: additional weights enum for the torchvision model.
        kwargs: additional parameters for the torchvision model.

    Example::

        import torch
        from torchvision.models.inception import Inception_V3_Weights

        from monai.networks.nets import TorchVisionFCModel

        model = TorchVisionFCModel(
            "inception_v3",
            num_classes=4,
            weights=Inception_V3_Weights.IMAGENET1K_V1,
            use_conv=False,
            pool=None,
        )
        # model = TorchVisionFCModel("vit_b_16", num_classes=4, pool=None, in_channels=768, fc_name="heads")
        output = model.forward(torch.randn(2, 3, 299, 299))
        print(output.shape)  # torch.Size([2, 4])

    """

    def __init__(self, model_name: str='resnet18', num_classes: int=1, dim: int=2, in_channels: Optional[int]=None, use_conv: bool=False, pool: Optional[Tuple[str, Dict[str, Any]]]=('avg', {'kernel_size': 7, 'stride': 1}), bias: bool=True, pretrained: bool=False, fc_name: str='fc', node_name: str='', weights=None, **kwargs):
        if weights is not None:
            model = getattr(models, model_name)(weights=weights, **kwargs)
        else:
            model = getattr(models, model_name)(pretrained=pretrained, **kwargs)
        super().__init__(model=model, num_classes=num_classes, dim=dim, in_channels=in_channels, use_conv=use_conv, pool=pool, bias=bias, fc_name=fc_name, node_name=node_name)


cached_path = optional_import('transformers.file_utils', name='cached_path')[0]


load_tf_weights_in_bert = optional_import('transformers', name='load_tf_weights_in_bert')[0]


class BertPreTrainedModel(nn.Module):
    """Module to load BERT pre-trained weights.
    Based on:
    LXMERT
    https://github.com/airsplay/lxmert
    BERT (pytorch-transformer)
    https://github.com/huggingface/transformers
    """

    def __init__(self, *inputs, **kwargs) ->None:
        super().__init__()

    def init_bert_weights(self, module):
        if isinstance(module, (nn.Linear, nn.Embedding)):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
        elif isinstance(module, torch.nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        if isinstance(module, nn.Linear) and module.bias is not None:
            module.bias.data.zero_()

    @classmethod
    def from_pretrained(cls, num_language_layers, num_vision_layers, num_mixed_layers, bert_config, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs):
        archive_file = 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz'
        resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)
        tempdir = None
        if os.path.isdir(resolved_archive_file) or from_tf:
            serialization_dir = resolved_archive_file
        else:
            tempdir = tempfile.mkdtemp()
            with tarfile.open(resolved_archive_file, 'r:gz') as archive:

                def is_within_directory(directory, target):
                    abs_directory = os.path.abspath(directory)
                    abs_target = os.path.abspath(target)
                    prefix = os.path.commonprefix([abs_directory, abs_target])
                    return prefix == abs_directory

                def safe_extract(tar, path='.', members=None, *, numeric_owner=False):
                    for member in tar.getmembers():
                        member_path = os.path.join(path, member.name)
                        if not is_within_directory(path, member_path):
                            raise Exception('Attempted Path Traversal in Tar File')
                    tar.extractall(path, members, numeric_owner=numeric_owner)
                safe_extract(archive, tempdir)
            serialization_dir = tempdir
        model = cls(num_language_layers, num_vision_layers, num_mixed_layers, bert_config, *inputs, **kwargs)
        if state_dict is None and not from_tf:
            weights_path = os.path.join(serialization_dir, 'pytorch_model.bin')
            state_dict = torch.load(weights_path, map_location='cpu' if not torch.cuda.is_available() else None)
        if tempdir:
            shutil.rmtree(tempdir)
        if from_tf:
            weights_path = os.path.join(serialization_dir, 'model.ckpt')
            return load_tf_weights_in_bert(model, weights_path)
        old_keys = []
        new_keys = []
        for key in state_dict.keys():
            new_key = None
            if 'gamma' in key:
                new_key = key.replace('gamma', 'weight')
            if 'beta' in key:
                new_key = key.replace('beta', 'bias')
            if new_key:
                old_keys.append(key)
                new_keys.append(new_key)
        for old_key, new_key in zip(old_keys, new_keys):
            state_dict[new_key] = state_dict.pop(old_key)
        missing_keys: List = []
        unexpected_keys: List = []
        error_msgs: List = []
        metadata = getattr(state_dict, '_metadata', None)
        state_dict = state_dict.copy()
        if metadata is not None:
            state_dict._metadata = metadata

        def load(module, prefix=''):
            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
            module._load_from_state_dict(state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)
            for name, child in module._modules.items():
                if child is not None:
                    load(child, prefix + name + '.')
        start_prefix = ''
        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):
            start_prefix = 'bert.'
        load(model, prefix=start_prefix)
        return model


class BertAttention(nn.Module):
    """BERT attention layer.
    Based on: BERT (pytorch-transformer)
    https://github.com/huggingface/transformers
    """

    def __init__(self, config) ->None:
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, context):
        mixed_query_layer = self.query(hidden_states)
        mixed_key_layer = self.key(context)
        mixed_value_layer = self.value(context)
        query_layer = self.transpose_for_scores(mixed_query_layer)
        key_layer = self.transpose_for_scores(mixed_key_layer)
        value_layer = self.transpose_for_scores(mixed_value_layer)
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        attention_probs = self.dropout(nn.Softmax(dim=-1)(attention_scores))
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)
        return context_layer


class BertOutput(nn.Module):
    """BERT output layer.
    Based on: BERT (pytorch-transformer)
    https://github.com/huggingface/transformers
    """

    def __init__(self, config) ->None:
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states


class BertMixedLayer(nn.Module):
    """BERT cross attention layer.
    Based on: BERT (pytorch-transformer)
    https://github.com/huggingface/transformers
    """

    def __init__(self, config) ->None:
        super().__init__()
        self.att_x = BertAttention(config)
        self.output_x = BertOutput(config)
        self.att_y = BertAttention(config)
        self.output_y = BertOutput(config)

    def forward(self, x, y):
        output_x = self.att_x(x, y)
        output_y = self.att_y(y, x)
        return self.output_x(output_x, x), self.output_y(output_y, y)


class Pooler(nn.Module):
    """BERT pooler layer.
    Based on: BERT (pytorch-transformer)
    https://github.com/huggingface/transformers
    """

    def __init__(self, hidden_size) ->None:
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output


BertEmbeddings = optional_import('transformers.models.bert.modeling_bert', name='BertEmbeddings')[0]


BertLayer = optional_import('transformers.models.bert.modeling_bert', name='BertLayer')[0]


class MultiModal(BertPreTrainedModel):
    """
    Multimodal Transformers From Pretrained BERT Weights"
    """

    def __init__(self, num_language_layers: int, num_vision_layers: int, num_mixed_layers: int, bert_config: dict) ->None:
        """
        Args:
            num_language_layers: number of language transformer layers.
            num_vision_layers: number of vision transformer layers.
            bert_config: configuration for bert language transformer encoder.

        """
        super().__init__()
        self.config = type('obj', (object,), bert_config)
        self.embeddings = BertEmbeddings(self.config)
        self.language_encoder = nn.ModuleList([BertLayer(self.config) for _ in range(num_language_layers)])
        self.vision_encoder = nn.ModuleList([BertLayer(self.config) for _ in range(num_vision_layers)])
        self.mixed_encoder = nn.ModuleList([BertMixedLayer(self.config) for _ in range(num_mixed_layers)])
        self.apply(self.init_bert_weights)

    def forward(self, input_ids, token_type_ids=None, vision_feats=None, attention_mask=None):
        language_features = self.embeddings(input_ids, token_type_ids)
        for layer in self.vision_encoder:
            vision_feats = layer(vision_feats, None)[0]
        for layer in self.language_encoder:
            language_features = layer(language_features, attention_mask)[0]
        for layer in self.mixed_encoder:
            language_features, vision_feats = layer(language_features, vision_feats)
        return language_features, vision_feats


class Transchex(torch.nn.Module):
    """
    TransChex based on: "Hatamizadeh et al.,TransCheX: Self-Supervised Pretraining of Vision-Language
    Transformers for Chest X-ray Analysis"
    """

    def __init__(self, in_channels: int, img_size: Union[Sequence[int], int], patch_size: Union[int, Tuple[int, int]], num_classes: int, num_language_layers: int, num_vision_layers: int, num_mixed_layers: int, hidden_size: int=768, drop_out: float=0.0, attention_probs_dropout_prob: float=0.1, gradient_checkpointing: bool=False, hidden_act: str='gelu', hidden_dropout_prob: float=0.1, initializer_range: float=0.02, intermediate_size: int=3072, layer_norm_eps: float=1e-12, max_position_embeddings: int=512, model_type: str='bert', num_attention_heads: int=12, num_hidden_layers: int=12, pad_token_id: int=0, position_embedding_type: str='absolute', transformers_version: str='4.10.2', type_vocab_size: int=2, use_cache: bool=True, vocab_size: int=30522, chunk_size_feed_forward: int=0, is_decoder: bool=False, add_cross_attention: bool=False) ->None:
        """
        Args:
            in_channels: dimension of input channels.
            img_size: dimension of input image.
            patch_size: dimension of patch size.
            num_classes: number of classes if classification is used.
            num_language_layers: number of language transformer layers.
            num_vision_layers: number of vision transformer layers.
            num_mixed_layers: number of mixed transformer layers.
            drop_out: faction of the input units to drop.

        The other parameters are part of the `bert_config` to `MultiModal.from_pretrained`.

        Examples:

        .. code-block:: python

            # for 3-channel with image size of (224,224), patch size of (32,32), 3 classes, 2 language layers,
            # 2 vision layers, 2 mixed modality layers and dropout of 0.2 in the classification head
            net = Transchex(in_channels=3,
                                 img_size=(224, 224),
                                 num_classes=3,
                                 num_language_layers=2,
                                 num_vision_layers=2,
                                 num_mixed_layers=2,
                                 drop_out=0.2)

        """
        super().__init__()
        bert_config = {'attention_probs_dropout_prob': attention_probs_dropout_prob, 'classifier_dropout': None, 'gradient_checkpointing': gradient_checkpointing, 'hidden_act': hidden_act, 'hidden_dropout_prob': hidden_dropout_prob, 'hidden_size': hidden_size, 'initializer_range': initializer_range, 'intermediate_size': intermediate_size, 'layer_norm_eps': layer_norm_eps, 'max_position_embeddings': max_position_embeddings, 'model_type': model_type, 'num_attention_heads': num_attention_heads, 'num_hidden_layers': num_hidden_layers, 'pad_token_id': pad_token_id, 'position_embedding_type': position_embedding_type, 'transformers_version': transformers_version, 'type_vocab_size': type_vocab_size, 'use_cache': use_cache, 'vocab_size': vocab_size, 'chunk_size_feed_forward': chunk_size_feed_forward, 'is_decoder': is_decoder, 'add_cross_attention': add_cross_attention}
        if not 0 <= drop_out <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        if img_size[0] % patch_size[0] != 0 or img_size[1] % patch_size[1] != 0:
            raise ValueError('img_size should be divisible by patch_size.')
        self.multimodal = MultiModal.from_pretrained(num_language_layers=num_language_layers, num_vision_layers=num_vision_layers, num_mixed_layers=num_mixed_layers, bert_config=bert_config)
        self.patch_size = patch_size
        self.num_patches = img_size[0] // self.patch_size[0] * (img_size[1] // self.patch_size[1])
        self.vision_proj = nn.Conv2d(in_channels=in_channels, out_channels=hidden_size, kernel_size=self.patch_size, stride=self.patch_size)
        self.norm_vision_pos = nn.LayerNorm(hidden_size)
        self.pos_embed_vis = nn.Parameter(torch.zeros(1, self.num_patches, hidden_size))
        self.pooler = Pooler(hidden_size=hidden_size)
        self.drop = torch.nn.Dropout(drop_out)
        self.cls_head = torch.nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, token_type_ids=None, vision_feats=None):
        attention_mask = torch.ones_like(input_ids).unsqueeze(1).unsqueeze(2)
        attention_mask = attention_mask
        attention_mask = (1.0 - attention_mask) * -10000.0
        vision_feats = self.vision_proj(vision_feats).flatten(2).transpose(1, 2)
        vision_feats = self.norm_vision_pos(vision_feats)
        vision_feats = vision_feats + self.pos_embed_vis
        hidden_state_lang, hidden_state_vis = self.multimodal(input_ids=input_ids, token_type_ids=token_type_ids, vision_feats=vision_feats, attention_mask=attention_mask)
        pooled_features = self.pooler(hidden_state_lang)
        logits = self.cls_head(self.drop(pooled_features))
        return logits


GlobalAliases = {}


def alias(*names):
    """
    Stores the decorated function or class in the global aliases table under the given names and as the `__aliases__`
    member of the decorated object. This new member will contain all alias names declared for that object.
    """

    def _outer(obj):
        for n in names:
            with alias_lock:
                GlobalAliases[n] = obj
        obj.__aliases__ = getattr(obj, '__aliases__', ()) + tuple(names)
        return obj
    return _outer


class ViT(nn.Module):
    """
    Vision Transformer (ViT), based on: "Dosovitskiy et al.,
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>"

    ViT supports Torchscript but only works for Pytorch after 1.8.
    """

    def __init__(self, in_channels: int, img_size: Union[Sequence[int], int], patch_size: Union[Sequence[int], int], hidden_size: int=768, mlp_dim: int=3072, num_layers: int=12, num_heads: int=12, pos_embed: str='conv', classification: bool=False, num_classes: int=2, dropout_rate: float=0.0, spatial_dims: int=3, post_activation='Tanh', qkv_bias: bool=False) ->None:
        """
        Args:
            in_channels: dimension of input channels.
            img_size: dimension of input image.
            patch_size: dimension of patch size.
            hidden_size: dimension of hidden layer.
            mlp_dim: dimension of feedforward layer.
            num_layers: number of transformer blocks.
            num_heads: number of attention heads.
            pos_embed: position embedding layer type.
            classification: bool argument to determine if classification is used.
            num_classes: number of classes if classification is used.
            dropout_rate: faction of the input units to drop.
            spatial_dims: number of spatial dimensions.
            post_activation: add a final acivation function to the classification head when `classification` is True.
                Default to "Tanh" for `nn.Tanh()`. Set to other values to remove this function.
            qkv_bias: apply bias to the qkv linear layer in self attention block

        Examples::

            # for single channel input with image size of (96,96,96), conv position embedding and segmentation backbone
            >>> net = ViT(in_channels=1, img_size=(96,96,96), pos_embed='conv')

            # for 3-channel with image size of (128,128,128), 24 layers and classification backbone
            >>> net = ViT(in_channels=3, img_size=(128,128,128), pos_embed='conv', classification=True)

            # for 3-channel with image size of (224,224), 12 layers and classification backbone
            >>> net = ViT(in_channels=3, img_size=(224,224), pos_embed='conv', classification=True, spatial_dims=2)

        """
        super().__init__()
        if not 0 <= dropout_rate <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        if hidden_size % num_heads != 0:
            raise ValueError('hidden_size should be divisible by num_heads.')
        self.classification = classification
        self.patch_embedding = PatchEmbeddingBlock(in_channels=in_channels, img_size=img_size, patch_size=patch_size, hidden_size=hidden_size, num_heads=num_heads, pos_embed=pos_embed, dropout_rate=dropout_rate, spatial_dims=spatial_dims)
        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, mlp_dim, num_heads, dropout_rate, qkv_bias) for i in range(num_layers)])
        self.norm = nn.LayerNorm(hidden_size)
        if self.classification:
            self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))
            if post_activation == 'Tanh':
                self.classification_head = nn.Sequential(nn.Linear(hidden_size, num_classes), nn.Tanh())
            else:
                self.classification_head = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = self.patch_embedding(x)
        if hasattr(self, 'cls_token'):
            cls_token = self.cls_token.expand(x.shape[0], -1, -1)
            x = torch.cat((cls_token, x), dim=1)
        hidden_states_out = []
        for blk in self.blocks:
            x = blk(x)
            hidden_states_out.append(x)
        x = self.norm(x)
        if hasattr(self, 'classification_head'):
            x = self.classification_head(x[:, 0])
        return x, hidden_states_out


class UNETR(nn.Module):
    """
    UNETR based on: "Hatamizadeh et al.,
    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
    """

    def __init__(self, in_channels: int, out_channels: int, img_size: Union[Sequence[int], int], feature_size: int=16, hidden_size: int=768, mlp_dim: int=3072, num_heads: int=12, pos_embed: str='conv', norm_name: Union[Tuple, str]='instance', conv_block: bool=True, res_block: bool=True, dropout_rate: float=0.0, spatial_dims: int=3, qkv_bias: bool=False) ->None:
        """
        Args:
            in_channels: dimension of input channels.
            out_channels: dimension of output channels.
            img_size: dimension of input image.
            feature_size: dimension of network feature size.
            hidden_size: dimension of hidden layer.
            mlp_dim: dimension of feedforward layer.
            num_heads: number of attention heads.
            pos_embed: position embedding layer type.
            norm_name: feature normalization type and arguments.
            conv_block: bool argument to determine if convolutional block is used.
            res_block: bool argument to determine if residual block is used.
            dropout_rate: faction of the input units to drop.
            spatial_dims: number of spatial dims.
            qkv_bias: apply the bias term for the qkv linear layer in self attention block

        Examples::

            # for single channel input 4-channel output with image size of (96,96,96), feature size of 32 and batch norm
            >>> net = UNETR(in_channels=1, out_channels=4, img_size=(96,96,96), feature_size=32, norm_name='batch')

             # for single channel input 4-channel output with image size of (96,96), feature size of 32 and batch norm
            >>> net = UNETR(in_channels=1, out_channels=4, img_size=96, feature_size=32, norm_name='batch', spatial_dims=2)

            # for 4-channel input 3-channel output with image size of (128,128,128), conv position embedding and instance norm
            >>> net = UNETR(in_channels=4, out_channels=3, img_size=(128,128,128), pos_embed='conv', norm_name='instance')

        """
        super().__init__()
        if not 0 <= dropout_rate <= 1:
            raise ValueError('dropout_rate should be between 0 and 1.')
        if hidden_size % num_heads != 0:
            raise ValueError('hidden_size should be divisible by num_heads.')
        self.num_layers = 12
        img_size = ensure_tuple_rep(img_size, spatial_dims)
        self.patch_size = ensure_tuple_rep(16, spatial_dims)
        self.feat_size = tuple(img_d // p_d for img_d, p_d in zip(img_size, self.patch_size))
        self.hidden_size = hidden_size
        self.classification = False
        self.vit = ViT(in_channels=in_channels, img_size=img_size, patch_size=self.patch_size, hidden_size=hidden_size, mlp_dim=mlp_dim, num_layers=self.num_layers, num_heads=num_heads, pos_embed=pos_embed, classification=self.classification, dropout_rate=dropout_rate, spatial_dims=spatial_dims, qkv_bias=qkv_bias)
        self.encoder1 = UnetrBasicBlock(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=feature_size, kernel_size=3, stride=1, norm_name=norm_name, res_block=res_block)
        self.encoder2 = UnetrPrUpBlock(spatial_dims=spatial_dims, in_channels=hidden_size, out_channels=feature_size * 2, num_layer=2, kernel_size=3, stride=1, upsample_kernel_size=2, norm_name=norm_name, conv_block=conv_block, res_block=res_block)
        self.encoder3 = UnetrPrUpBlock(spatial_dims=spatial_dims, in_channels=hidden_size, out_channels=feature_size * 4, num_layer=1, kernel_size=3, stride=1, upsample_kernel_size=2, norm_name=norm_name, conv_block=conv_block, res_block=res_block)
        self.encoder4 = UnetrPrUpBlock(spatial_dims=spatial_dims, in_channels=hidden_size, out_channels=feature_size * 8, num_layer=0, kernel_size=3, stride=1, upsample_kernel_size=2, norm_name=norm_name, conv_block=conv_block, res_block=res_block)
        self.decoder5 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=hidden_size, out_channels=feature_size * 8, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=res_block)
        self.decoder4 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size * 8, out_channels=feature_size * 4, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=res_block)
        self.decoder3 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size * 4, out_channels=feature_size * 2, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=res_block)
        self.decoder2 = UnetrUpBlock(spatial_dims=spatial_dims, in_channels=feature_size * 2, out_channels=feature_size, kernel_size=3, upsample_kernel_size=2, norm_name=norm_name, res_block=res_block)
        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)
        self.proj_axes = (0, spatial_dims + 1) + tuple(d + 1 for d in range(spatial_dims))
        self.proj_view_shape = list(self.feat_size) + [self.hidden_size]

    def proj_feat(self, x):
        new_view = [x.size(0)] + self.proj_view_shape
        x = x.view(new_view)
        x = x.permute(self.proj_axes).contiguous()
        return x

    def forward(self, x_in):
        x, hidden_states_out = self.vit(x_in)
        enc1 = self.encoder1(x_in)
        x2 = hidden_states_out[3]
        enc2 = self.encoder2(self.proj_feat(x2))
        x3 = hidden_states_out[6]
        enc3 = self.encoder3(self.proj_feat(x3))
        x4 = hidden_states_out[9]
        enc4 = self.encoder4(self.proj_feat(x4))
        dec4 = self.proj_feat(x)
        dec3 = self.decoder5(dec4, enc4)
        dec2 = self.decoder4(dec3, enc3)
        dec1 = self.decoder3(dec2, enc2)
        out = self.decoder2(dec1, enc1)
        return self.out(out)


class ViTAutoEnc(nn.Module):
    """
    Vision Transformer (ViT), based on: "Dosovitskiy et al.,
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>"

    Modified to also give same dimension outputs as the input size of the image
    """

    def __init__(self, in_channels: int, img_size: Union[Sequence[int], int], patch_size: Union[Sequence[int], int], out_channels: int=1, deconv_chns: int=16, hidden_size: int=768, mlp_dim: int=3072, num_layers: int=12, num_heads: int=12, pos_embed: str='conv', dropout_rate: float=0.0, spatial_dims: int=3) ->None:
        """
        Args:
            in_channels: dimension of input channels or the number of channels for input
            img_size: dimension of input image.
            patch_size: dimension of patch size.
            hidden_size: dimension of hidden layer.
            out_channels: number of output channels.
            deconv_chns: number of channels for the deconvolution layers.
            mlp_dim: dimension of feedforward layer.
            num_layers: number of transformer blocks.
            num_heads: number of attention heads.
            pos_embed: position embedding layer type.
            dropout_rate: faction of the input units to drop.
            spatial_dims: number of spatial dimensions.

        Examples::

            # for single channel input with image size of (96,96,96), conv position embedding and segmentation backbone
            # It will provide an output of same size as that of the input
            >>> net = ViTAutoEnc(in_channels=1, patch_size=(16,16,16), img_size=(96,96,96), pos_embed='conv')

            # for 3-channel with image size of (128,128,128), output will be same size as of input
            >>> net = ViTAutoEnc(in_channels=3, patch_size=(16,16,16), img_size=(128,128,128), pos_embed='conv')

        """
        super().__init__()
        self.patch_size = ensure_tuple_rep(patch_size, spatial_dims)
        self.spatial_dims = spatial_dims
        self.patch_embedding = PatchEmbeddingBlock(in_channels=in_channels, img_size=img_size, patch_size=patch_size, hidden_size=hidden_size, num_heads=num_heads, pos_embed=pos_embed, dropout_rate=dropout_rate, spatial_dims=self.spatial_dims)
        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, mlp_dim, num_heads, dropout_rate) for i in range(num_layers)])
        self.norm = nn.LayerNorm(hidden_size)
        new_patch_size = [4] * self.spatial_dims
        conv_trans = Conv[Conv.CONVTRANS, self.spatial_dims]
        self.conv3d_transpose = conv_trans(hidden_size, deconv_chns, kernel_size=new_patch_size, stride=new_patch_size)
        self.conv3d_transpose_1 = conv_trans(in_channels=deconv_chns, out_channels=out_channels, kernel_size=new_patch_size, stride=new_patch_size)

    def forward(self, x):
        """
        Args:
            x: input tensor must have isotropic spatial dimensions,
                such as ``[batch_size, channels, sp_size, sp_size[, sp_size]]``.
        """
        spatial_size = x.shape[2:]
        x = self.patch_embedding(x)
        hidden_states_out = []
        for blk in self.blocks:
            x = blk(x)
            hidden_states_out.append(x)
        x = self.norm(x)
        x = x.transpose(1, 2)
        d = [(s // p) for s, p in zip(spatial_size, self.patch_size)]
        x = torch.reshape(x, [x.shape[0], x.shape[1], *d])
        x = self.conv3d_transpose(x)
        x = self.conv3d_transpose_1(x)
        return x, hidden_states_out


class InputTransition(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, act: Union[Tuple[str, Dict], str], bias: bool=False):
        super().__init__()
        if out_channels % in_channels != 0:
            raise ValueError(f'out channels should be divisible by in_channels. Got in_channels={in_channels}, out_channels={out_channels}.')
        self.spatial_dims = spatial_dims
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.act_function = get_acti_layer(act, out_channels)
        self.conv_block = Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=5, act=None, norm=Norm.BATCH, bias=bias)

    def forward(self, x):
        out = self.conv_block(x)
        repeat_num = self.out_channels // self.in_channels
        x16 = x.repeat([1, repeat_num, 1, 1, 1][:self.spatial_dims + 2])
        out = self.act_function(torch.add(out, x16))
        return out


class DownTransition(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, nconvs: int, act: Union[Tuple[str, Dict], str], dropout_prob: Optional[float]=None, dropout_dim: int=3, bias: bool=False):
        super().__init__()
        conv_type: Type[Union[nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
        norm_type: Type[Union[nn.BatchNorm2d, nn.BatchNorm3d]] = Norm[Norm.BATCH, spatial_dims]
        dropout_type: Type[Union[nn.Dropout, nn.Dropout2d, nn.Dropout3d]] = Dropout[Dropout.DROPOUT, dropout_dim]
        out_channels = 2 * in_channels
        self.down_conv = conv_type(in_channels, out_channels, kernel_size=2, stride=2, bias=bias)
        self.bn1 = norm_type(out_channels)
        self.act_function1 = get_acti_layer(act, out_channels)
        self.act_function2 = get_acti_layer(act, out_channels)
        self.ops = _make_nconv(spatial_dims, out_channels, nconvs, act, bias)
        self.dropout = dropout_type(dropout_prob) if dropout_prob is not None else None

    def forward(self, x):
        down = self.act_function1(self.bn1(self.down_conv(x)))
        if self.dropout is not None:
            out = self.dropout(down)
        else:
            out = down
        out = self.ops(out)
        out = self.act_function2(torch.add(out, down))
        return out


class OutputTransition(nn.Module):

    def __init__(self, spatial_dims: int, in_channels: int, out_channels: int, act: Union[Tuple[str, Dict], str], bias: bool=False):
        super().__init__()
        conv_type: Type[Union[nn.Conv2d, nn.Conv3d]] = Conv[Conv.CONV, spatial_dims]
        self.act_function1 = get_acti_layer(act, out_channels)
        self.conv_block = Convolution(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, kernel_size=5, act=None, norm=Norm.BATCH, bias=bias)
        self.conv2 = conv_type(out_channels, out_channels, kernel_size=1)

    def forward(self, x):
        out = self.conv_block(x)
        out = self.act_function1(out)
        out = self.conv2(out)
        return out


class VNet(nn.Module):
    """
    V-Net based on `Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation
    <https://arxiv.org/pdf/1606.04797.pdf>`_.
    Adapted from `the official Caffe implementation
    <https://github.com/faustomilletari/VNet>`_. and `another pytorch implementation
    <https://github.com/mattmacy/vnet.pytorch/blob/master/vnet.py>`_.
    The model supports 2D or 3D inputs.

    Args:
        spatial_dims: spatial dimension of the input data. Defaults to 3.
        in_channels: number of input channels for the network. Defaults to 1.
            The value should meet the condition that ``16 % in_channels == 0``.
        out_channels: number of output channels for the network. Defaults to 1.
        act: activation type in the network. Defaults to ``("elu", {"inplace": True})``.
        dropout_prob: dropout ratio. Defaults to 0.5.
        dropout_dim: determine the dimensions of dropout. Defaults to 3.

            - ``dropout_dim = 1``, randomly zeroes some of the elements for each channel.
            - ``dropout_dim = 2``, Randomly zeroes out entire channels (a channel is a 2D feature map).
            - ``dropout_dim = 3``, Randomly zeroes out entire channels (a channel is a 3D feature map).
        bias: whether to have a bias term in convolution blocks. Defaults to False.
            According to `Performance Tuning Guide <https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html>`_,
            if a conv layer is directly followed by a batch norm layer, bias should be False.

    """

    def __init__(self, spatial_dims: int=3, in_channels: int=1, out_channels: int=1, act: Union[Tuple[str, Dict], str]=('elu', {'inplace': True}), dropout_prob: float=0.5, dropout_dim: int=3, bias: bool=False):
        super().__init__()
        if spatial_dims not in (2, 3):
            raise AssertionError('spatial_dims can only be 2 or 3.')
        self.in_tr = InputTransition(spatial_dims, in_channels, 16, act, bias=bias)
        self.down_tr32 = DownTransition(spatial_dims, 16, 1, act, bias=bias)
        self.down_tr64 = DownTransition(spatial_dims, 32, 2, act, bias=bias)
        self.down_tr128 = DownTransition(spatial_dims, 64, 3, act, dropout_prob=dropout_prob, bias=bias)
        self.down_tr256 = DownTransition(spatial_dims, 128, 2, act, dropout_prob=dropout_prob, bias=bias)
        self.up_tr256 = UpTransition(spatial_dims, 256, 256, 2, act, dropout_prob=dropout_prob)
        self.up_tr128 = UpTransition(spatial_dims, 256, 128, 2, act, dropout_prob=dropout_prob)
        self.up_tr64 = UpTransition(spatial_dims, 128, 64, 1, act)
        self.up_tr32 = UpTransition(spatial_dims, 64, 32, 1, act)
        self.out_tr = OutputTransition(spatial_dims, 32, out_channels, act, bias=bias)

    def forward(self, x):
        out16 = self.in_tr(x)
        out32 = self.down_tr32(out16)
        out64 = self.down_tr64(out32)
        out128 = self.down_tr128(out64)
        out256 = self.down_tr256(out128)
        x = self.up_tr256(out256, out128)
        x = self.up_tr128(x, out64)
        x = self.up_tr64(x, out32)
        x = self.up_tr32(x, out16)
        x = self.out_tr(x)
        return x


class _AutoGradReLU(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x):
        pos_mask = (x > 0).type_as(x)
        output = torch.mul(x, pos_mask)
        ctx.save_for_backward(x, output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        x, _ = ctx.saved_tensors
        pos_mask_1 = (x > 0).type_as(grad_output)
        pos_mask_2 = (grad_output > 0).type_as(grad_output)
        y = torch.mul(grad_output, pos_mask_1)
        grad_input = torch.mul(y, pos_mask_2)
        return grad_input


class _GradReLU(torch.nn.Module):
    """
    A customized ReLU with the backward pass imputed for guided backpropagation (https://arxiv.org/abs/1412.6806).
    """

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        out: torch.Tensor = _AutoGradReLU.apply(x)
        return out


class _TestModelOne(torch.nn.Module):

    def __init__(self, n_n, n_m, n_class):
        super().__init__()
        self.layer = torch.nn.Linear(n_n, n_m)
        self.class_layer = torch.nn.Linear(n_m, n_class)

    def forward(self, x):
        x = self.layer(x)
        x = self.class_layer(x)
        return x


class _TestModelTwo(torch.nn.Module):

    def __init__(self, n_n, n_m, n_d, n_class):
        super().__init__()
        self.layer = torch.nn.Linear(n_n, n_m)
        self.layer_1 = torch.nn.Linear(n_m, n_d)
        self.class_layer = torch.nn.Linear(n_d, n_class)

    def forward(self, x):
        x = self.layer(x)
        x = self.layer_1(x)
        x = self.class_layer(x)
        return x


class ToyNet(Module):

    def __init__(self, value):
        super().__init__()
        self.value = value

    def forward(self, input):
        return input

    def get_value(self):
        return self.value

    def set_value(self, value):
        self.value = value


class STNBenchmark(nn.Module):
    """
    adapted from https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html
    """

    def __init__(self, is_ref=True, reverse_indexing=False):
        super().__init__()
        self.is_ref = is_ref
        self.localization = nn.Sequential(nn.Conv2d(1, 8, kernel_size=7), nn.MaxPool2d(2, stride=2), nn.ReLU(True), nn.Conv2d(8, 10, kernel_size=5), nn.MaxPool2d(2, stride=2), nn.ReLU(True))
        self.fc_loc = nn.Sequential(nn.Linear(10 * 3 * 3, 32), nn.ReLU(True), nn.Linear(32, 3 * 2))
        self.fc_loc[2].weight.data.zero_()
        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))
        if not self.is_ref:
            self.xform = AffineTransform(normalized=True, reverse_indexing=reverse_indexing)

    def stn_ref(self, x):
        xs = self.localization(x)
        xs = xs.view(-1, 10 * 3 * 3)
        theta = self.fc_loc(xs)
        theta = theta.view(-1, 2, 3)
        grid = F.affine_grid(theta, x.size(), align_corners=False)
        x = F.grid_sample(x, grid, align_corners=False)
        return x

    def stn(self, x):
        xs = self.localization(x)
        xs = xs.view(-1, 10 * 3 * 3)
        theta = self.fc_loc(xs)
        theta = theta.view(-1, 2, 3)
        x = self.xform(x, theta, spatial_size=x.size()[2:])
        return x

    def forward(self, x):
        if self.is_ref:
            return self.stn_ref(x)
        return self.stn(x)


class SchedulerTestNet(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(1, 1, 1)
        self.conv2 = torch.nn.Conv2d(1, 1, 1)

    def forward(self, x):
        return self.conv2(torch.nn.functional.relu(self.conv1(x)))


class DenseNetAdjoint(DenseNet121):

    def __call__(self, x, adjoint_info):
        if adjoint_info != 42:
            raise ValueError
        return super().__call__(x)


class TestNet(torch.nn.Module):

    def forward(self, x: torch.Tensor):
        return {HoVerNetBranch.NP: torch.tensor([1, 2]), HoVerNetBranch.NC: torch.tensor([4, 4]), HoVerNetBranch.HV: 16}


class NaiveNetwork(torch.nn.Module):

    def __init__(self, spatial_dims, num_classes, **kwargs):
        super().__init__()
        self.spatial_dims = spatial_dims
        self.num_classes = num_classes
        self.num_anchors = 1
        self.cls_key = 'cls'
        self.box_reg_key = 'box_reg'
        self.size_divisible = 1

    def forward(self, images):
        out_cls_shape = (images.shape[0], self.num_classes * self.num_anchors) + images.shape[-self.spatial_dims:]
        out_box_reg_shape = (images.shape[0], 2 * self.spatial_dims * self.num_anchors) + images.shape[-self.spatial_dims:]
        return {self.cls_key: torch.randn(out_cls_shape), self.box_reg_key: [torch.randn(out_box_reg_shape)]}


class NaiveNetwork2(torch.nn.Module):

    def __init__(self, spatial_dims, num_classes, **kwargs):
        super().__init__()
        self.spatial_dims = spatial_dims
        self.num_classes = num_classes
        self.num_anchors = 1
        self.cls_key = 'cls'
        self.box_reg_key = 'box_reg'
        self.size_divisible = 1

    def forward(self, images):
        out_cls_shape = (images.shape[0], self.num_classes * self.num_anchors) + images.shape[-self.spatial_dims:]
        out_box_reg_shape = (images.shape[0], 2 * self.spatial_dims * self.num_anchors) + images.shape[-self.spatial_dims:]
        return {self.cls_key: [torch.randn(out_cls_shape)] * 2, self.box_reg_key: [torch.randn(out_box_reg_shape)] * 2}


class TestModule(torch.nn.Module):

    def forward(self, x):
        return x + 10


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AnchorGeneratorWithAnchorShape,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (AsymmetricFocalLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (AsymmetricFocalTverskyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (BertAttention,
     lambda: ([], {'config': _mock_config(num_attention_heads=4, hidden_size=4, attention_probs_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (BertMixedLayer,
     lambda: ([], {'config': _mock_config(num_attention_heads=4, hidden_size=4, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (BertOutput,
     lambda: ([], {'config': _mock_config(hidden_size=4, hidden_dropout_prob=0.5)}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (ChannelPad,
     lambda: ([], {'spatial_dims': 4, 'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ContrastiveLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (DiceFocalLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (DiceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (DropPath,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ExtraFPNBlock,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FocalLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (GeneralizedDiceFocalLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (GeneralizedDiceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalMutualInformationLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (MaskedDiceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (MaskedLoss,
     lambda: ([], {'loss': MSELoss()}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (MultiScaleLoss,
     lambda: ([], {'loss': MSELoss()}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (NaiveNetwork,
     lambda: ([], {'spatial_dims': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (NaiveNetwork2,
     lambda: ([], {'spatial_dims': 4, 'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PatchMerging,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 8, 8])], {}),
     False),
    (PatchMergingV2,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 8, 8])], {}),
     False),
    (Pooler,
     lambda: ([], {'hidden_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Reshape,
     lambda: ([], {}),
     lambda: ([torch.rand([4])], {}),
     True),
    (SchedulerTestNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (SkipConnection,
     lambda: ([], {'submodule': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Swish,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TestModule,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TestNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ToyNet,
     lambda: ([], {'value': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TverskyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (WindowAttention,
     lambda: ([], {'dim': 4, 'num_heads': 4, 'window_size': [4, 4]}),
     lambda: ([torch.rand([16, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (_CloseWithRAMCost,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (_GradReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (_IdentityWithRAMCost,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (_TestModelOne,
     lambda: ([], {'n_n': 4, 'n_m': 4, 'n_class': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (_TestModelTwo,
     lambda: ([], {'n_n': 4, 'n_m': 4, 'n_d': 4, 'n_class': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_Project_MONAI_MONAI(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])


import sys
_module = sys.modules[__name__]
del sys
train_forest_cover_calibrated = _module
train_mushroom_edibility_calibrated = _module
model_training = _module
train = _module
display_kfold_cv_results = _module
prepare_classification_data_set = _module
model = _module
train_alpaca = _module
phi_2_dequantization = _module
simple_model_training = _module
train_imdb_ray = _module
train_alpaca_ray = _module
advanced_model_training = _module
assess_model_performance = _module
ecd_freezing_with_regex_training = _module
llm_freezing_with_regex_training = _module
camseq = _module
client_program = _module
train_higgs_medium = _module
train_higgs_small = _module
multiple_model_training = _module
train_twitter_bots = _module
train_twitter_bots_text_only = _module
train_nmt = _module
ludwig = _module
accounting = _module
used_tokens = _module
api = _module
api_annotations = _module
automl = _module
auto_tune_config = _module
base_config = _module
backend = _module
_ray210_compat = _module
base = _module
datasource = _module
deepspeed = _module
horovod = _module
ray = _module
utils = _module
storage = _module
benchmarking = _module
artifacts = _module
benchmark = _module
process_config = _module
profiler = _module
profiler_callbacks = _module
profiler_dataclasses = _module
reporting = _module
summarize = _module
summary_dataclasses = _module
utils = _module
callbacks = _module
check = _module
cli = _module
collect = _module
combiners = _module
combiners = _module
config_sampling = _module
explore_schema = _module
parameter_sampling = _module
config_validation = _module
checks = _module
preprocessing = _module
validation = _module
constants = _module
contrib = _module
contribs = _module
aim = _module
comet = _module
mlflow = _module
wandb = _module
data = _module
batcher = _module
bucketed = _module
iterable = _module
random_access = _module
test_batcher = _module
cache = _module
manager = _module
types = _module
util = _module
concatenate_datasets = _module
dataframe = _module
dask = _module
modin = _module
pandas = _module
dataset = _module
ray = _module
dataset_synthesizer = _module
negative_sampling = _module
postprocessing = _module
preprocessing = _module
prompt = _module
sampler = _module
split = _module
split_dataset = _module
datasets = _module
archives = _module
configs = _module
dataset_config = _module
kaggle = _module
loaders = _module
adult_census_income = _module
agnews = _module
allstate_claims_severity = _module
code_alpaca_loader = _module
consumer_complaints_loader = _module
creditcard_fraud = _module
dataset_loader = _module
ethos_binary = _module
flickr8k = _module
forest_cover = _module
goemotions = _module
higgs = _module
hugging_face = _module
ieee_fraud = _module
insurance_lite = _module
kdd_loader = _module
mnist = _module
naval = _module
rossman_store_sales = _module
santander_value_prediction = _module
sarcastic_headlines = _module
sarcos = _module
split_loaders = _module
sst = _module
model_configs = _module
decoders = _module
generic_decoders = _module
image_decoders = _module
llm_decoders = _module
registry = _module
sequence_decoder_utils = _module
sequence_decoders = _module
sequence_tagger = _module
utils = _module
distributed = _module
_ray_210_compat = _module
base = _module
ddp = _module
deepspeed = _module
fsdp = _module
horovod = _module
encoders = _module
bag_encoders = _module
base = _module
category_encoders = _module
date_encoders = _module
generic_encoders = _module
h3_encoders = _module
image = _module
base = _module
torchvision = _module
sequence_encoders = _module
set_encoders = _module
text_encoders = _module
types = _module
error = _module
evaluate = _module
experiment = _module
explain = _module
captum = _module
captum_ray = _module
explainer = _module
explanation = _module
gbm = _module
util = _module
export = _module
features = _module
audio_feature = _module
bag_feature = _module
base_feature = _module
binary_feature = _module
category_feature = _module
date_feature = _module
feature_registries = _module
feature_utils = _module
h3_feature = _module
image_feature = _module
number_feature = _module
sequence_feature = _module
set_feature = _module
text_feature = _module
timeseries_feature = _module
vector_feature = _module
forecast = _module
globals = _module
hyperopt = _module
execution = _module
results = _module
run = _module
syncer = _module
hyperopt_cli = _module
base_model_exporter = _module
onnx_exporter = _module
models = _module
base = _module
calibrator = _module
ecd = _module
embedder = _module
gbm = _module
inference = _module
llm = _module
predictor = _module
retrieval = _module
modules = _module
attention_modules = _module
convolutional_modules = _module
embedding_modules = _module
fully_connected_modules = _module
initializer_modules = _module
loss_implementations = _module
corn = _module
loss_modules = _module
lr_scheduler = _module
metric_modules = _module
metric_registry = _module
mlp_mixer_modules = _module
normalization_modules = _module
optimization_modules = _module
recurrent_modules = _module
reduction_modules = _module
tabnet_modules = _module
training_hooks = _module
predict = _module
preprocess = _module
progress_bar = _module
schema = _module
common_transformer_options = _module
comparator = _module
concat = _module
project_aggregate = _module
sequence = _module
sequence_concat = _module
tab_transformer = _module
tabnet = _module
transformer = _module
common_fields = _module
defaults = _module
sequence_encoders = _module
text = _module
hf_model_params = _module
augmentation = _module
loss = _module
audio = _module
bag = _module
binary = _module
category = _module
date = _module
h3 = _module
number = _module
set = _module
timeseries = _module
vector = _module
executor = _module
parameter = _module
scheduler = _module
search_algorithm = _module
llms = _module
base_model = _module
generation = _module
model_parameters = _module
peft = _module
quantization = _module
metadata = _module
feature_metadata = _module
parameter_metadata = _module
model_config = _module
model_types = _module
optimizers = _module
profiler = _module
trainer = _module
serve = _module
trainers = _module
trainer = _module
trainer_lightgbm = _module
trainer_llm = _module
upload = _module
algorithms_utils = _module
audio_utils = _module
augmentation_utils = _module
data_source = _module
field_info = _module
ray_utils = _module
type_inference = _module
backward_compatibility = _module
batch_size_tuner = _module
calibration = _module
carton_utils = _module
checkpoint_utils = _module
config_utils = _module
data_utils = _module
dataframe_utils = _module
dataset_utils = _module
date_utils = _module
entmax = _module
activations = _module
losses = _module
root_finding = _module
error_handling_utils = _module
eval_utils = _module
fs_utils = _module
gbm_utils = _module
h3_util = _module
heuristics = _module
hf_utils = _module
horovod_utils = _module
html_utils = _module
image_utils = _module
inference_utils = _module
llm_quantization_utils = _module
llm_utils = _module
logging_utils = _module
loss_utils = _module
math_utils = _module
metric_utils = _module
metrics_printed_table = _module
misc_utils = _module
model_utils = _module
neuropod_utils = _module
nlp_utils = _module
numerical_test_utils = _module
output_feature_utils = _module
package_utils = _module
print_utils = _module
server_utils = _module
state_dict_backward_compatibility = _module
strings_utils = _module
structural_warning = _module
system_utils = _module
time_utils = _module
tokenizers = _module
torch_utils = _module
trainer_utils = _module
triton_utils = _module
types = _module
upload_utils = _module
version_transformation = _module
visualization_utils = _module
vector_index = _module
faiss = _module
visualize = _module
setup = _module
tests = _module
conftest = _module
integration_tests = _module
parameter_update_utils = _module
run_train_aim = _module
run_train_comet = _module
run_train_horovod = _module
run_train_wandb = _module
synthetic_test_data = _module
test_api = _module
test_audio_feature = _module
test_automl = _module
test_cache_manager = _module
test_cached_preprocessing = _module
test_carton = _module
test_class_imbalance_feature = _module
test_cli = _module
test_collect = _module
test_config_global_defaults = _module
test_contrib_aim = _module
test_contrib_comet = _module
test_contrib_wandb = _module
test_custom_components = _module
test_date_feature = _module
test_dependencies = _module
test_experiment = _module
test_explain = _module
test_gbm = _module
test_graph_execution = _module
test_horovod = _module
test_hyperopt = _module
test_hyperopt_ray = _module
test_hyperopt_ray_horovod = _module
test_input_feature_tied = _module
test_kfold_cv = _module
test_llm = _module
test_missing_value_strategy = _module
test_mlflow = _module
test_model_save_and_load = _module
test_model_training_options = _module
test_neuropod = _module
test_number_feature = _module
test_peft = _module
test_postprocessing = _module
test_preprocessing = _module
test_ray = _module
test_reducers = _module
test_regularizers = _module
test_remote = _module
test_reproducibility = _module
test_sequence_decoders = _module
test_sequence_encoders = _module
test_sequence_features = _module
test_server = _module
test_simple_features = _module
test_timeseries_feature = _module
test_torchscript = _module
test_trainer = _module
test_triton = _module
test_visualization = _module
test_visualization_api = _module
utils = _module
test_used_tokens = _module
test_augmentation_pipeline = _module
test_auto_augmentation = _module
test_image_augmentation = _module
test_base_config = _module
test_data_source = _module
test_tune_config = _module
test_utils = _module
test_ray = _module
test_benchmarking = _module
test_profiler = _module
test_combiners = _module
test_config_sampling = _module
test_checks = _module
test_validate_config_combiner = _module
test_validate_config_encoder = _module
test_validate_config_features = _module
test_validate_config_hyperopt = _module
test_validate_config_misc = _module
test_validate_config_preprocessing = _module
test_validate_config_trainer = _module
test_contrib = _module
test_dask = _module
test_cache_util = _module
test_dataset_synthesizer = _module
test_negative_sampling = _module
test_postprocessing = _module
test_ray_data = _module
test_split = _module
download_all_datasets = _module
test_mnist_workflow = _module
train_all_model_configs = _module
test_dataset_configs = _module
test_dataset_links = _module
test_datasets = _module
test_model_configs = _module
test_titanic_workflow = _module
test_image_decoder = _module
test_llm_decoders = _module
test_sequence_decoder = _module
test_sequence_decoder_utils = _module
test_sequence_tagger = _module
test_bag_encoders = _module
test_category_encoders = _module
test_date_encoders = _module
test_generic_encoders = _module
test_h3_encoders = _module
test_image_encoders = _module
test_llm_encoders = _module
test_sequence_encoders = _module
test_set_encoders = _module
test_text_encoders = _module
test_evaluation = _module
test_captum = _module
test_util = _module
test_audio_feature = _module
test_bag_feature = _module
test_binary_feature = _module
test_category_feature = _module
test_date_feature = _module
test_feature_utils = _module
test_h3_feature = _module
test_image_feature = _module
test_number_feature = _module
test_sequence_features = _module
test_set_feature = _module
test_text_feature = _module
test_timeseries_feature = _module
test_fields_misc = _module
test_fields_optimization = _module
test_fields_preprocessing = _module
test_marshmallow_misc = _module
test_onnx_exporter = _module
test_trainable_image_layers = _module
test_training_determinism = _module
test_training_success = _module
test_attention = _module
test_convolutional_modules = _module
test_embedding_modules = _module
test_encoder = _module
test_fully_connected_modules = _module
test_initializer_modules = _module
test_loss_modules = _module
test_lr_scheduler = _module
test_metric_modules = _module
test_mlp_mixer_modules = _module
test_normalization_modules = _module
test_recurrent_modules = _module
test_reduction_modules = _module
test_regex_freezing = _module
test_tabnet_modules = _module
test_utils = _module
test_scheduler = _module
test_search_algorithm = _module
test_model_config = _module
test_schema_utils = _module
test_type_inference = _module
test_losses = _module
test_mask = _module
test_root_finding = _module
test_topk = _module
test_algorithm_utils = _module
test_audio_utils = _module
test_backward_compatibility = _module
test_calibration = _module
test_class_balancing = _module
test_config_utils = _module
test_data_utils = _module
test_dataframe_utils = _module
test_dataset_utils = _module
test_date_utils = _module
test_defaults = _module
test_error_handling_utils = _module
test_errors = _module
test_fs_utils = _module
test_heuristics = _module
test_hf_utils = _module
test_hyperopt_ray_utils = _module
test_image_utils = _module
test_llm_utils = _module
test_metric_utils = _module
test_model_utils = _module
test_normalization = _module
test_numerical_test_utils = _module
test_output_feature_utils = _module
test_server_utils = _module
test_state_dict_backward_compatibility = _module
test_strings_utils = _module
test_tokenizers = _module
test_torch_utils = _module
test_trainer_utils = _module
test_upload_utils = _module
test_version_transformation = _module
update_golden_types = _module
test_auto_type_inference = _module
expected_metric = _module
test_model_performance = _module
test_old_models = _module
training_success = _module

from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import logging


import pandas as pd


import torch


from torchvision.utils import save_image


from typing import Dict


from typing import Union


import copy


import time


from collections import OrderedDict


from typing import Any


from typing import ClassVar


from typing import List


from typing import Optional


from typing import Tuple


import numpy as np


from abc import ABC


from abc import abstractmethod


from typing import Callable


from typing import Generator


from typing import TYPE_CHECKING


from typing import Type


from functools import partial


from queue import Empty as EmptyQueueException


from queue import Queue


from collections import Counter


from collections import defaultdict


from torch._C._autograd import _KinetoEvent


from torch.autograd import DeviceType


from torch.autograd import profiler_util


import functools


import uuid


from types import ModuleType


from functools import lru_cache


from torch.nn import Linear


from torch.nn import ModuleList


import math


import queue


from typing import Iterable


from typing import Iterator


from typing import Literal


import random


import string


import torchaudio


import warnings


import re


import torch.nn as nn


from torch import Tensor


from torch import nn


from torch.optim import Optimizer


import torch.distributed as dist


from torch.nn.parallel import DistributedDataParallel as DDP


from typing import Mapping


from torch.optim.optimizer import Optimizer


from torch.distributed.fsdp import FullyShardedDataParallel as FSDP


import torchvision.models as tvm


import inspect


from typing import TypeVar


from typing import TypedDict


import numpy.typing as npt


from torch.autograd import Variable


from copy import deepcopy


from abc import abstractstaticmethod


from torchvision import transforms


from torchvision.transforms import functional as F


from torchvision.transforms.functional import normalize


from abc import ABCMeta


from torch.nn import functional as F


from torch.nn import Dropout


import torch.nn.functional as F


from torch.nn import HuberLoss as _HuberLoss


from torch.nn import L1Loss


from torch.nn import MSELoss as _MSELoss


from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts


from torch.optim.lr_scheduler import LambdaLR


from torch.optim.lr_scheduler import ReduceLROnPlateau


from torch.optim.lr_scheduler import SequentialLR


from torch import tensor


from torch.nn import BatchNorm1d


from torch.nn import BatchNorm2d


from torch.nn import LayerNorm


from torch.nn import Module


from torch.nn import GRU


from torch.nn import LSTM


from torch.nn import RNN


from torchvision.io import decode_image


from torch.utils.tensorboard import SummaryWriter


from torch.autograd import Function


from numpy import typing as npt


from collections.abc import Iterable


import torchvision.transforms.functional as F


from torchvision.io import ImageReadMode


from torchvision.models._api import WeightsEnum


from collections import namedtuple


from collections.abc import Mapping


import numpy


from torch.nn import ModuleDict


import torchvision


from typing import Set


from random import choice


from string import ascii_lowercase


from string import ascii_uppercase


from string import digits


from torchvision.models import resnet18


from torchvision.models import ResNet18_Weights


from torch.optim import SGD


from torch.autograd import gradcheck


from itertools import product


def _get_indent(docstring: 'str') ->int:
    """
    Example:
        >>> def f():
        ...     '''Docstring summary.'''
        >>> f.__doc__
        'Docstring summary.'
        >>> _get_indent(f.__doc__)
        0
        >>> def g(foo):
        ...     '''Docstring summary.
        ...
        ...     Args:
        ...         foo: Does bar.
        ...     '''
        >>> g.__doc__
        'Docstring summary.\\n\\n    Args:\\n        foo: Does bar.\\n    '
        >>> _get_indent(g.__doc__)
        4
        >>> class A:
        ...     def h():
        ...         '''Docstring summary.
        ...
        ...         Returns:
        ...             None.
        ...         '''
        >>> A.h.__doc__
        'Docstring summary.\\n\\n        Returns:\\n            None.\\n        '
        >>> _get_indent(A.h.__doc__)
        8
    """
    if not docstring:
        return 0
    non_empty_lines = list(filter(bool, docstring.splitlines()))
    if len(non_empty_lines) == 1:
        return 0
    return len(non_empty_lines[1]) - len(non_empty_lines[1].lstrip())


def _append_doc(obj, message: 'str', directive: 'Optional[str]'=None) ->str:
    """
    Args:
        message: An additional message to append to the end of docstring for a class
                 or method that uses one of the API annotations
        directive: A shorter message that provides contexts for the message and indents it.
                For example, this could be something like 'warning' or 'info'.
    """
    if not obj.__doc__:
        obj.__doc__ = ''
    obj.__doc__ = obj.__doc__.rstrip()
    indent = _get_indent(obj.__doc__)
    obj.__doc__ += '\n\n'
    if directive is not None:
        obj.__doc__ += f"{' ' * indent}.. {directive}::\n"
        obj.__doc__ += f"{' ' * (indent + 4)}{message}"
    else:
        obj.__doc__ += f"{' ' * indent}{message}"
    obj.__doc__ += f"\n{' ' * indent}"


def _mark_annotated(obj) ->None:
    if hasattr(obj, '__name__'):
        obj._annotated = obj.__name__


def DeveloperAPI(*args, **kwargs):
    """Annotation for documenting developer APIs. Developer APIs are lower-level methods explicitly exposed to
    advanced Ludwig users and library developers. Their interfaces may change across minor Ludwig releases (for
    e.g., Ludwig 0.6.1 and Ludwig 0.6.2).

    Examples:
        >>> from api_annotations import DeveloperAPI
        >>> @DeveloperAPI
        ... def func(x):
        ...     return x
    """
    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
        return DeveloperAPI()(args[0])

    def wrap(obj):
        _append_doc(obj, message='DeveloperAPI: This API may change across minor Ludwig releases.')
        _mark_annotated(obj)
        return obj
    return wrap


@DeveloperAPI
class RNNDecoder(nn.Module):
    """GRU or RNN-based decoder."""

    def __init__(self, hidden_size: 'int', vocab_size: 'int', cell_type: 'str', num_layers: 'int'=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        if cell_type == 'gru':
            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)
        else:
            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)
        self.out = nn.Linear(hidden_size, vocab_size)
        self.out.weight = self.embedding.weight

    def forward(self, input: 'torch.Tensor', hidden: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor]:
        """Runs a single decoding time step.

        Modeled off of https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.

        Args:
            input: [batch_size] tensor with the previous step's predicted symbol.
            hidden: [batch_size, hidden_size] tensor with the previous step's hidden state.

        Returns:
            Tuple of two tensors:
            - output: [batch_size, 1, vocab_size] tensor with the logits.
            - hidden: [num_layers, batch_size, hidden_size] tensor with the hidden state for the next time step.
        """
        input = input.unsqueeze(1)
        output = self.embedding(input)
        output, hidden = self.rnn(output, hidden)
        output_logits = self.out(output)
        return output_logits, hidden


@DeveloperAPI
class LSTMDecoder(nn.Module):
    """LSTM-based decoder."""

    def __init__(self, hidden_size: 'int', vocab_size: 'int', num_layers: 'int'=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)
        self.out = nn.Linear(hidden_size, vocab_size)
        self.out.weight = self.embedding.weight

    def forward(self, input: 'torch.Tensor', hidden_state: 'torch.Tensor', cell_state: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Runs a single decoding time step.

        Modeled off of https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.

        Args:
            input: [batch_size] tensor with the previous step's predicted symbol.
            hidden_state: [batch_size, hidden_size] tensor with the previous step's hidden state.
            cell_state: [batch_size, hidden_size] tensor with the previous step's cell state.

        Returns:
            Tuple of 3 tensors:
            - output: [batch_size, vocab_size] tensor with the logits.
            - hidden_state: [batch_size, hidden_size] tensor with the hidden state for the next time step.
            - cell_state: [batch_size, hidden_size] tensor with the cell state for the next time step.
        """
        input = input.unsqueeze(1)
        output = self.embedding(input)
        output, (hidden_state, cell_state) = self.lstm(output, (hidden_state, cell_state))
        output_logits = self.out(output)
        return output_logits, hidden_state, cell_state


ENCODER_OUTPUT = 'encoder_output'


@DeveloperAPI
class LudwigModule(Module):

    def __init__(self):
        super().__init__()
        self._losses = {}
        self.register_buffer('device_tensor', torch.zeros(0), persistent=False)

    @property
    def device(self):
        return self.device_tensor.device

    def prepare_for_training(self):
        """This is called from within the Trainer object to do any final instantiation before model training."""
        pass

    def losses(self):
        collected_losses = []
        for loss in self._losses.values():
            collected_losses.append(loss)
        for child in self.children():
            if isinstance(child, LudwigModule):
                collected_losses.extend(child.losses())
            elif isinstance(child, ModuleDict):
                for c in child.values():
                    if hasattr(c, 'losses'):
                        collected_losses.extend(c.losses())
            elif isinstance(child, Module):
                pass
            else:
                raise ValueError
        return collected_losses

    def update_loss(self, key: 'str', loss: 'torch.Tensor'):
        """This should be called in the forward pass to add a custom loss term to the combined loss."""
        self._losses[key] = loss

    @property
    def input_dtype(self):
        return torch.float32

    @property
    @abstractmethod
    def input_shape(self) ->torch.Size:
        """Returns size of the input tensor without the batch dimension."""
        pass

    @property
    def output_shape(self) ->torch.Size:
        """Returns size of the output tensor without the batch dimension."""
        return self._computed_output_shape()

    @lru_cache(maxsize=1)
    def _computed_output_shape(self) ->torch.Size:
        dummy_input = torch.rand(2, *self.input_shape, device=self.device)
        output_tensor = self.forward(dummy_input.type(self.input_dtype))
        if isinstance(output_tensor, torch.Tensor):
            return output_tensor.size()[1:]
        elif isinstance(output_tensor, dict) and ENCODER_OUTPUT in output_tensor:
            return output_tensor[ENCODER_OUTPUT].size()[1:]
        else:
            raise ValueError('Unknown output tensor type.')


@DeveloperAPI
def get_from_registry(key, registry):
    if hasattr(key, 'lower'):
        key = key.lower()
    if key in registry:
        return registry[key]
    else:
        raise ValueError(f"Key '{key}' not in registry, available options: {registry.keys()}")


activations = {'elu': nn.ELU, 'leakyRelu': nn.LeakyReLU, 'logSigmoid': nn.LogSigmoid, 'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh, 'softmax': nn.Softmax, None: nn.Identity}


@DeveloperAPI
def get_activation(activation):
    return activations[activation]()


class FeedForwardAttentionReducer(LudwigModule):

    def __init__(self, input_size, hidden_size=256, activation='tanh'):
        super().__init__()
        self.fc_layer1 = nn.Linear(input_size, hidden_size)
        self.fc_layer1_activation = get_activation(activation)
        self.fc_layer2 = nn.Linear(hidden_size, 1, bias=False)
        self.input_shape_var = None
        self.output_shape_var = None

    def forward(self, inputs, mask=None):
        self.input_shape_var = inputs.size()[1:]
        hidden = self.fc_layer1(inputs)
        hidden = self.fc_layer1_activation(hidden)
        hidden = self.fc_layer2(hidden)
        attention = F.softmax(hidden, dim=1)
        gated_inputs = torch.sum(attention * inputs, dim=1)
        self.output_shape_var = gated_inputs.size()[1:]
        return gated_inputs

    @property
    def input_shape(self) ->torch.Size:
        return self.input_shape_var

    @property
    def output_shape(self) ->torch.Size:
        return self.output_shape_var


class ReduceConcat(torch.nn.Module):

    def forward(self, inputs, mask=None):
        if inputs.dim() > 2:
            return inputs.reshape(-1, inputs.shape[-1] * inputs.shape[-2])
        return inputs


@DeveloperAPI
def sequence_length_3D(sequence: 'torch.Tensor') ->torch.Tensor:
    """Returns the number of non-zero elements per sequence in batch.

    :param sequence: (torch.Tensor) A 3D tensor of shape [batch size x max sequence length x hidden size].

    # Return
    :returns: (torch.Tensor) The count on non-zero elements per sequence.
    """
    used = torch.sign(torch.amax(torch.abs(sequence), dim=2))
    length = torch.sum(used, 1)
    length = length.int()
    return length


class ReduceLast(torch.nn.Module):

    def forward(self, inputs, mask=None):
        batch_size = inputs.shape[0]
        sequence_length = sequence_length_3D(inputs) - 1
        sequence_length[sequence_length < 0] = 0
        gathered = inputs[torch.arange(batch_size), sequence_length.type(torch.int64)]
        return gathered


class ReduceMax(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return torch.amax(inputs, dim=1)


class ReduceMean(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return torch.mean(inputs, dim=1)


class ReduceNone(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return inputs


class ReduceSum(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return torch.sum(inputs, dim=1)


reduce_mode_registry = {'last': ReduceLast, 'sum': ReduceSum, 'mean': ReduceMean, 'avg': ReduceMean, 'max': ReduceMax, 'concat': ReduceConcat, 'attention': FeedForwardAttentionReducer, 'none': ReduceNone, 'None': ReduceNone, None: ReduceNone}


class SequenceReducer(LudwigModule):
    """Reduces the sequence dimension of an input tensor according to the specified reduce_mode.  Any additional
    kwargs are passed on to the reduce mode's constructor.  If using reduce_mode=="attention", the input_size kwarg
    must also be specified.

    A sequence is a tensor of 2 or more dimensions, where the shape is [batch size x sequence length x ...].

    :param reduce_mode: The reduction mode, one of {"last", "sum", "mean", "max", "concat", "attention", "none"}
    :param max_sequence_length The maximum sequence length.  Only used for computation of shapes - inputs passed
                               at runtime may have a smaller sequence length.
    :param encoding_size The size of each sequence element/embedding vector, or None if input is a sequence of scalars.
    """

    def __init__(self, reduce_mode: 'str'=None, max_sequence_length: 'int'=256, encoding_size: 'int'=None, **kwargs):
        super().__init__()
        self._reduce_mode = reduce_mode
        self._max_sequence_length = max_sequence_length
        self._encoding_size = encoding_size
        if reduce_mode == 'attention' and encoding_size and 'input_size' not in kwargs:
            kwargs['input_size'] = encoding_size
        self._reduce_obj = get_from_registry(reduce_mode, reduce_mode_registry)(**kwargs)

    def forward(self, inputs, mask=None):
        """Forward pass of reducer.

        :param inputs: A tensor of 2 or more dimensions, where the shape is [batch size x sequence length x ...].
        :param mask: A mask tensor of 2 dimensions [batch size x sequence length].  Not yet implemented.

        :return: The input after applying the reduction operation to sequence dimension.
        """
        return self._reduce_obj(inputs, mask=mask)

    @property
    def input_shape(self) ->torch.Size:
        """Returns size of the input tensor without the batch dimension."""
        if self._encoding_size is None:
            return torch.Size([self._max_sequence_length])
        else:
            return torch.Size([self._max_sequence_length, self._encoding_size])

    @property
    def output_shape(self) ->torch.Size:
        """Returns size of the output tensor without the batch dimension."""
        input_shape = self.input_shape
        if self._reduce_mode in {None, 'none', 'None'}:
            return input_shape
        elif self._reduce_mode == 'concat':
            if len(input_shape) > 1:
                return input_shape[:-2] + (input_shape[-1] * input_shape[-2],)
            return input_shape
        else:
            return input_shape[1:]


ENCODER_OUTPUT_STATE = 'encoder_output_state'


HIDDEN = 'hidden'


def repeat_2D_tensor(tensor, k):
    """Repeats a 2D-tensor k times over the first dimension.

    For example:
    Input: Tensor of [batch_size, state_size], k=2
    Output: Tensor of [k, batch_size, state_size]
    """
    if len(tensor.size()) > 2:
        raise ValueError('Cannot repeat a non-2D tensor with this method.')
    return tensor.repeat(k, 1, 1)


def get_rnn_init_state(combiner_outputs: 'Dict[str, torch.Tensor]', sequence_reducer: 'SequenceReducer', num_layers: 'int') ->torch.Tensor:
    """Computes the hidden state that the RNN decoder should start with.

    Args:
        combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
        sequence_reducer: SequenceReducer to reduce rank-3 to rank-2.
        num_layers: Number of layers the decoder uses.

    Returns:
        Tensor of [num_layers, batch_size, hidden_size].
    """
    if ENCODER_OUTPUT_STATE not in combiner_outputs:
        encoder_output_state = combiner_outputs[HIDDEN]
    else:
        encoder_output_state = combiner_outputs[ENCODER_OUTPUT_STATE]
        if isinstance(encoder_output_state, tuple):
            if len(encoder_output_state) == 2:
                encoder_output_state = encoder_output_state[0]
            elif len(encoder_output_state) == 4:
                encoder_output_state = torch.mean([encoder_output_state[0], encoder_output_state[2]])
            else:
                raise ValueError(f'Invalid sequence decoder inputs with keys: {combiner_outputs.keys()} with extracted encoder ' + f'state: {encoder_output_state.size()} that was invalid. Please double check the compatibility ' + 'of your encoder and decoder.')
    if len(encoder_output_state.size()) > 3:
        raise ValueError('Init state for RNN decoders only works for 1d or 2d tensors (encoder_output).')
    if len(encoder_output_state.size()) == 3:
        encoder_output_state = sequence_reducer(encoder_output_state)
    return repeat_2D_tensor(encoder_output_state, num_layers)


@DeveloperAPI
class SequenceRNNDecoder(nn.Module):
    """RNN-based decoder over multiple time steps."""

    def __init__(self, hidden_size: 'int', vocab_size: 'int', max_sequence_length: 'int', cell_type: 'str', num_layers: 'int'=1, reduce_input='sum'):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.rnn_decoder = RNNDecoder(hidden_size, vocab_size, cell_type, num_layers=num_layers)
        self.max_sequence_length = max_sequence_length
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_input)
        self.num_layers = num_layers
        self.register_buffer('logits', torch.zeros([max_sequence_length, vocab_size]))
        self.register_buffer('decoder_input', torch.Tensor([strings_utils.SpecialSymbol.START.value]))

    def forward(self, combiner_outputs: 'Dict[str, torch.Tensor]', target: 'torch.Tensor'):
        """Runs max_sequence_length RNN decoding time steps.

        Args:
            combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
            target: Tensor [batch_size, max_sequence_length] with target symbols.

        Returns:
            Tensor of logits [batch_size, max_sequence_length, vocab_size].
        """
        decoder_hidden = get_rnn_init_state(combiner_outputs, self.reduce_sequence, self.num_layers)
        batch_size = decoder_hidden.size()[1]
        logits = self.logits.unsqueeze(0).repeat(batch_size, 1, 1)
        decoder_input = self.decoder_input.repeat(batch_size)
        for di in range(self.max_sequence_length):
            decoder_output, decoder_hidden = self.rnn_decoder(decoder_input, decoder_hidden)
            logits[:, di, :] = decoder_output.squeeze(1)
            if target is None:
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(1).squeeze(1).detach()
            else:
                decoder_input = target[:, di]
        return logits


def get_lstm_init_state(combiner_outputs: 'Dict[str, torch.Tensor]', sequence_reducer: 'SequenceReducer', num_layers: 'int') ->Tuple[torch.Tensor, torch.Tensor]:
    """Returns the states that the LSTM decoder should start with.

    Args:
        combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
        sequence_reducer: SequenceReducer to reduce rank-3 to rank-2.
        num_layers: Number of layers the decoder uses.

    Returns:
        Tuple of 2 tensors (decoder hidden state, decoder cell state), each [num_layers, batch_size, hidden_size].
    """
    if ENCODER_OUTPUT_STATE not in combiner_outputs:
        decoder_hidden_state = combiner_outputs[HIDDEN]
        decoder_cell_state = torch.clone(decoder_hidden_state)
    else:
        encoder_output_state = combiner_outputs[ENCODER_OUTPUT_STATE]
        if not isinstance(encoder_output_state, tuple):
            decoder_hidden_state = encoder_output_state
            decoder_cell_state = decoder_hidden_state
        elif len(encoder_output_state) == 2:
            decoder_hidden_state, decoder_cell_state = encoder_output_state
        elif len(encoder_output_state) == 4:
            decoder_hidden_state = torch.mean([encoder_output_state[0], encoder_output_state[2]])
            decoder_cell_state = torch.mean([encoder_output_state[1], encoder_output_state[3]])
        else:
            raise ValueError(f'Invalid sequence decoder inputs with keys: {combiner_outputs.keys()} with extracted encoder ' + f'state: {encoder_output_state} that was invalid. Please double check the compatibility of your ' + 'encoder and decoder.')
    if len(decoder_hidden_state.size()) > 3 or len(decoder_cell_state.size()) > 3:
        raise ValueError(f'Invalid sequence decoder inputs with keys: {combiner_outputs.keys()} with extracted encoder ' + f'state: {decoder_hidden_state.size()} that was invalid. Please double check the compatibility ' + 'of your encoder and decoder.')
    if len(decoder_hidden_state.size()) == 3:
        decoder_hidden_state = sequence_reducer(decoder_hidden_state)
    if len(decoder_cell_state.size()) == 3:
        decoder_cell_state = sequence_reducer(decoder_cell_state)
    return repeat_2D_tensor(decoder_hidden_state, num_layers), repeat_2D_tensor(decoder_cell_state, num_layers)


@DeveloperAPI
class SequenceLSTMDecoder(nn.Module):
    """LSTM-based decoder over multiple time steps."""

    def __init__(self, hidden_size: 'int', vocab_size: 'int', max_sequence_length: 'int', reduce_input: 'str'='sum', num_layers: 'int'=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.lstm_decoder = LSTMDecoder(hidden_size, vocab_size, num_layers)
        self.max_sequence_length = max_sequence_length
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_input)
        self.num_layers = num_layers
        self.register_buffer('logits', torch.zeros([max_sequence_length, vocab_size]))
        self.register_buffer('decoder_input', torch.Tensor([strings_utils.SpecialSymbol.START.value]))

    def forward(self, combiner_outputs: 'Dict[str, torch.Tensor]', target: 'torch.Tensor') ->torch.Tensor:
        """Runs max_sequence_length LSTM decoding time steps.

        Args:
            combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
            target: Tensor [batch_size, max_sequence_length] with target symbols.

        Returns:
            Tensor of logits [batch_size, max_sequence_length, vocab_size].
        """
        decoder_hidden, decoder_cell_state = get_lstm_init_state(combiner_outputs, self.reduce_sequence, self.num_layers)
        batch_size = decoder_hidden.size()[1]
        decoder_input = self.decoder_input.repeat(batch_size)
        logits = self.logits.unsqueeze(0).repeat(batch_size, 1, 1)
        for di in range(self.max_sequence_length):
            decoder_output, decoder_hidden, decoder_cell_state = self.lstm_decoder(decoder_input, decoder_hidden, decoder_cell_state)
            logits[:, di, :] = decoder_output.squeeze(1)
            if target is None:
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(1).squeeze(1).detach()
            else:
                decoder_input = target[:, di]
        return logits


class WrapperModule(nn.Module):

    def __init__(self, encoder: 'LLMEncoder'):
        super().__init__()
        self.encoder = encoder


AUDIO_FEATURE_KEYS = ['type', 'window_length_in_s', 'window_shift_in_s', 'num_fft_points', 'window_type', 'num_filter_bands']


AUDIO = 'audio'


FeatureMetadataDict = Dict[str, Any]


COLUMN = 'column'


NAME = 'name'


PREPROCESSING = 'preprocessing'


PROC_COLUMN = 'proc_column'


SRC = 'dataset_src'


TYPE = 'type'


@DeveloperAPI
def calculate_mean(sum1, count):
    return sum1 / float(count)


@DeveloperAPI
def calculate_var(sum1, sum2, count):
    return (sum2 - sum1 * sum1 / float(count)) / float(count - 1) if count > 1 else 0.0


@DeveloperAPI
def has_remote_protocol(url):
    protocol, _ = split_protocol(url)
    return protocol and protocol != 'file'


@DeveloperAPI
def get_abs_path(src_path, file_path):
    if has_remote_protocol(file_path):
        return file_path
    elif src_path is not None:
        return os.path.join(src_path, file_path)
    else:
        return file_path


DEFAULT_AUDIO_TENSOR_LENGTH = 70000


TorchAudioTuple = Tuple[torch.Tensor, int]


@DeveloperAPI
def get_default_audio(audio_lst: 'List[TorchAudioTuple]') ->TorchAudioTuple:
    sampling_rates = [audio[1] for audio in audio_lst]
    tensor_list = [audio[0] for audio in audio_lst]
    for i, tensor in enumerate(tensor_list):
        if tensor.shape[1] > DEFAULT_AUDIO_TENSOR_LENGTH:
            tensor_list[i] = tensor[:, :DEFAULT_AUDIO_TENSOR_LENGTH]
        else:
            pad_size = DEFAULT_AUDIO_TENSOR_LENGTH - tensor.shape[1]
            tensor_list[i] = F.pad(tensor, (0, pad_size))
    default_audio_tensor = torch.mean(torch.stack(tensor_list), dim=0)
    default_sampling_rate = calculate_mean(sum(sampling_rates), len(sampling_rates))
    return default_audio_tensor, default_sampling_rate


def _convert_hz_to_mel(hz: 'int') ->float:
    return float(2595.0 * torch.log10(torch.tensor(1 + hz / 700.0)))


def _convert_mel_to_hz(mel):
    return 700.0 * (10 ** (mel / 2595.0) - 1)


def _create_triangular_filter(start_bin_freq: 'torch.Tensor', middle_bin_freq: 'torch.Tensor', end_bin_freq: 'torch.Tensor', num_ess_fft_points: 'int'):
    filter_window = torch.zeros(num_ess_fft_points, dtype=torch.float32, device=start_bin_freq.device)
    filt_support_begin = middle_bin_freq - start_bin_freq
    filt_support_end = end_bin_freq - middle_bin_freq
    for freq in range(int(start_bin_freq), int(middle_bin_freq)):
        filter_window[freq] = (freq - start_bin_freq) / filt_support_begin
    for freq in range(int(middle_bin_freq), int(end_bin_freq)):
        filter_window[freq] = (end_bin_freq - freq) / filt_support_end
    return filter_window


@DeveloperAPI
def get_non_symmetric_length(symmetric_length: 'int') ->int:
    return int(symmetric_length / 2) + 1


def _get_mel_fbank_matrix(list_mel_points: 'torch.Tensor', num_filter_bands: 'int', num_fft_points: 'int', sampling_rate_in_hz: 'int') ->torch.Tensor:
    num_ess_fft_points = get_non_symmetric_length(num_fft_points)
    freq_scale = (num_fft_points + 1) / sampling_rate_in_hz
    freq_bins_on_mel_scale = torch.floor(freq_scale * _convert_mel_to_hz(list_mel_points))
    mel_scaled_fbank = torch.zeros((num_filter_bands, num_ess_fft_points), dtype=torch.float32, device=list_mel_points.device)
    for filt_idx in range(num_filter_bands):
        start_bin_freq = freq_bins_on_mel_scale[filt_idx]
        middle_bin_freq = freq_bins_on_mel_scale[filt_idx + 1]
        end_bin_freq = freq_bins_on_mel_scale[filt_idx + 2]
        mel_scaled_fbank[filt_idx] = _create_triangular_filter(start_bin_freq, middle_bin_freq, end_bin_freq, num_ess_fft_points)
    return mel_scaled_fbank


def _pre_emphasize_data(data: 'torch.Tensor', emphasize_value: 'float'=0.97):
    filter_window = torch.tensor([1.0, -emphasize_value], dtype=torch.float64, device=data.device)
    a_coeffs = torch.tensor([1, 0], dtype=torch.float64, device=data.device)
    pre_emphasized_data = torchaudio.functional.lfilter(data.to(dtype=torch.float64), a_coeffs, filter_window, clamp=False)
    return pre_emphasized_data


@DeveloperAPI
def get_num_output_padded_to_fit_input(num_input: 'int', window_length_in_samp: 'int', window_shift_in_samp: 'int') ->int:
    num_output_valid = torch.tensor((num_input - window_length_in_samp) / window_shift_in_samp + 1)
    return int(torch.ceil(num_output_valid))


def _preprocess_to_padded_matrix(data: 'torch.Tensor', window_length_in_samp: 'int', window_shift_in_samp: 'int', zero_mean_offset: 'bool'=False) ->torch.Tensor:
    num_input = data.shape[0]
    num_output = get_num_output_padded_to_fit_input(num_input, window_length_in_samp, window_shift_in_samp)
    zero_padded_matrix = torch.zeros((num_output, window_length_in_samp), dtype=torch.float32, device=data.device)
    for num_output_idx in range(num_output):
        start_idx = window_shift_in_samp * num_output_idx
        is_last_output = num_output_idx == num_output - 1
        end_idx = start_idx + window_length_in_samp if not is_last_output else num_input
        end_padded_idx = window_length_in_samp if not is_last_output else end_idx - start_idx
        window_data = data[start_idx:end_idx]
        if zero_mean_offset:
            window_data = window_data - torch.mean(window_data)
        zero_padded_matrix[num_output_idx, :end_padded_idx] = window_data
    return zero_padded_matrix


@DeveloperAPI
def get_window(window_type: 'str', window_length_in_samp: 'int', device: 'Optional[torch.device]'=None) ->torch.Tensor:
    if window_type == 'bartlett':
        return torch.bartlett_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    elif window_type == 'blackman':
        return torch.blackman_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    elif window_type == 'hamming':
        return torch.hamming_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    elif window_type == 'hann':
        return torch.hann_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    else:
        raise ValueError(f'Unknown window type: {window_type}')


def _weight_data_matrix(data_matrix: 'torch.Tensor', window_type: 'str', data_transformation: 'Optional[str]'=None) ->torch.Tensor:
    window_length_in_samp = data_matrix[0].shape[0]
    window = get_window(window_type, window_length_in_samp, device=data_matrix.device)
    if data_transformation is not None and data_transformation == 'group_delay':
        window *= torch.arange(window_length_in_samp, device=data_matrix.device).float()
    return data_matrix * window


@DeveloperAPI
def get_length_in_samp(sampling_rate_in_hz: 'Union[float, int]', length_in_s: 'Union[float, int]') ->int:
    return int(sampling_rate_in_hz * length_in_s)


def _short_time_fourier_transform(data: 'torch.Tensor', sampling_rate_in_hz: 'int', window_length_in_s: 'float', window_shift_in_s: 'float', num_fft_points: 'int', window_type: 'str', data_transformation: 'Optional[str]'=None, zero_mean_offset: 'bool'=False) ->torch.Tensor:
    window_length_in_samp: 'int' = get_length_in_samp(window_length_in_s, sampling_rate_in_hz)
    window_shift_in_samp: 'int' = get_length_in_samp(window_shift_in_s, sampling_rate_in_hz)
    preprocessed_data_matrix = _preprocess_to_padded_matrix(data[0], window_length_in_samp, window_shift_in_samp, zero_mean_offset=zero_mean_offset)
    weighted_data_matrix = _weight_data_matrix(preprocessed_data_matrix, window_type, data_transformation=data_transformation)
    fft = torch.fft.fft(weighted_data_matrix, n=num_fft_points)
    return fft


@DeveloperAPI
def get_non_symmetric_data(data: 'torch.Tensor') ->torch.Tensor:
    num_fft_points = data.shape[-1]
    num_ess_fft_points = get_non_symmetric_length(num_fft_points)
    return data[:, :num_ess_fft_points]


def _get_stft(raw_data: 'torch.Tensor', sampling_rate_in_hz: 'int', window_length_in_s: 'float', window_shift_in_s: 'float', num_fft_points: 'int', window_type: 'str', data_transformation: 'Optional[str]'=None, zero_mean_offset: 'bool'=False) ->torch.Tensor:
    pre_emphasized_data = _pre_emphasize_data(raw_data)
    stft = _short_time_fourier_transform(pre_emphasized_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type, data_transformation, zero_mean_offset)
    non_symmetric_stft = get_non_symmetric_data(stft)
    return non_symmetric_stft


@DeveloperAPI
def get_fbank(raw_data: 'torch.Tensor', sampling_rate_in_hz: 'int', window_length_in_s: 'float', window_shift_in_s: 'float', num_fft_points: 'int', window_type: 'str', num_filter_bands: 'int') ->torch.Tensor:
    stft = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type, zero_mean_offset=True)
    stft_power = torch.abs(stft) ** 2
    upper_limit_freq = int(sampling_rate_in_hz / 2)
    upper_limit_mel = _convert_hz_to_mel(upper_limit_freq)
    lower_limit_mel = 0
    list_mel_points = torch.linspace(lower_limit_mel, upper_limit_mel, num_filter_bands + 2, device=raw_data.device)
    mel_fbank_matrix = _get_mel_fbank_matrix(list_mel_points, num_filter_bands, num_fft_points, sampling_rate_in_hz)
    mel_fbank_feature = torch.matmul(stft_power, torch.transpose(mel_fbank_matrix, 0, 1))
    log_mel_fbank_feature = torch.log(mel_fbank_feature + 1e-10)
    return torch.transpose(log_mel_fbank_feature, 0, 1)


@DeveloperAPI
def get_group_delay(raw_data: 'torch.Tensor', sampling_rate_in_hz: 'int', window_length_in_s: 'float', window_shift_in_s: 'float', num_fft_points: 'int', window_type: 'str'):
    X_stft_transform = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type)
    Y_stft_transform = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type, data_transformation='group_delay')
    X_stft_transform_real = torch.real(X_stft_transform)
    X_stft_transform_imag = torch.imag(X_stft_transform)
    Y_stft_transform_real = torch.real(Y_stft_transform)
    Y_stft_transform_imag = torch.imag(Y_stft_transform)
    nominator = torch.multiply(X_stft_transform_real, Y_stft_transform_real) + torch.multiply(X_stft_transform_imag, Y_stft_transform_imag)
    denominator = torch.square(torch.abs(X_stft_transform))
    group_delay = torch.divide(nominator, denominator + 1e-10)
    assert not torch.isnan(group_delay).any(), 'There are NaN values in group delay'
    return torch.transpose(group_delay, 0, 1)


@DeveloperAPI
def get_max_length_stft_based(length_in_samp, window_length_in_s, window_shift_in_s, sampling_rate_in_hz):
    window_length_in_samp = get_length_in_samp(window_length_in_s, sampling_rate_in_hz)
    window_shift_in_samp = get_length_in_samp(window_shift_in_s, sampling_rate_in_hz)
    return get_num_output_padded_to_fit_input(length_in_samp, window_length_in_samp, window_shift_in_samp)


@DeveloperAPI
def get_phase_stft_magnitude(raw_data: 'torch.Tensor', sampling_rate_in_hz: 'int', window_length_in_s: 'float', window_shift_in_s: 'float', num_fft_points: 'int', window_type: 'str') ->torch.Tensor:
    stft = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type)
    abs_stft = torch.abs(stft)
    phase = torch.angle(stft)
    stft_phase = torch.cat([phase, abs_stft], dim=1)
    return torch.transpose(stft_phase, 0, 1)


@DeveloperAPI
def get_stft_magnitude(raw_data: 'torch.Tensor', sampling_rate_in_hz: 'int', window_length_in_s: 'float', window_shift_in_s: 'float', num_fft_points: 'int', window_type: 'str'):
    stft = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type)
    stft_magnitude = torch.abs(stft)
    return torch.transpose(stft_magnitude, 0, 1)


@DeveloperAPI
def is_torch_audio_tuple(audio: 'Any') ->bool:
    if isinstance(audio, tuple):
        if len(audio) == 2 and isinstance(audio[0], torch.Tensor) and isinstance(audio[1], int):
            return True
    return False


logger = logging.getLogger(__name__)


@DeveloperAPI
@functools.lru_cache(maxsize=32)
def read_audio_from_bytes_obj(bytes_obj: 'bytes') ->Optional[TorchAudioTuple]:
    try:
        f = BytesIO(bytes_obj)
        if _TORCH_AUDIO_210:
            return torchaudio.load(f, backend='sox')
        elif _TORCH_AUDIO_201:
            return torchaudio.backend.sox_io_backend.load(f)
        else:
            return torchaudio.backend.sox_backend.load(f)
    except Exception as e:
        logger.warning(e)
        return None


@DeveloperAPI
def read_audio_from_path(path: 'str') ->Optional[TorchAudioTuple]:
    """Reads audio from path.

    Useful for reading from a small number of paths. For more intensive reads, use backend.read_binary_files instead.
    """
    try:
        if _TORCH_AUDIO_210:
            return torchaudio.load(path, backend='sox')
        elif _TORCH_AUDIO_201:
            return torchaudio.backend.sox_io_backend.load(path)
        else:
            return torchaudio.backend.sox_backend.load(path)
    except Exception as e:
        logger.warning(e)
        return None


@DeveloperAPI
def set_default_value(dictionary, key, value):
    if key not in dictionary:
        dictionary[key] = value


class _AudioPreprocessing(torch.nn.Module):
    audio_feature_dict: 'Dict[str, Union[float, int, str]]'

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.audio_feature_dict = {key: value for key, value in metadata['preprocessing'].items() if key in AUDIO_FEATURE_KEYS and value is not None}
        self.feature_dim = metadata['feature_dim']
        self.max_length = metadata['max_length']
        self.padding_value = metadata['preprocessing']['padding_value']
        self.normalization_type = metadata['preprocessing']['norm']

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if not torch.jit.isinstance(v, List[Tuple[torch.Tensor, int]]):
            raise ValueError(f'Unsupported input: {v}')
        processed_audio_matrix = []
        for audio, sampling_rate_in_hz in v:
            processed_audio = AudioFeatureMixin._transform_to_feature(audio, sampling_rate_in_hz, self.audio_feature_dict, self.feature_dim, self.max_length, self.padding_value, self.normalization_type)
            processed_audio_matrix.append(processed_audio)
        return torch.stack(processed_audio_matrix)


LOGITS = 'logits'


PREDICTIONS = 'predictions'


PROBABILITIES = 'probabilities'


class PredictModule(torch.nn.Module):
    """Base class for all modules that convert model outputs to predictions.

    Explicit member variables needed here for scripting, as Torchscript will not be able to recognize global variables
    during scripting.
    """

    def __init__(self):
        super().__init__()
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES
        self.logits_key = LOGITS


class PassthroughPreprocModule(torch.nn.Module):
    """Combines preprocessing and encoding into a single module for TorchScript inference.

    For encoder outputs that were cached during preprocessing, the encoder is simply the identity function in the ECD
    module. As such, we need this module to apply the encoding that would normally be done during preprocessing for
    realtime inference.
    """

    def __init__(self, preproc: 'torch.nn.Module', encoder: 'torch.nn.Module'):
        self.preproc = preproc
        self.encoder = encoder

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        preproc_v = self.preproc(v)
        return self.encoder(preproc_v)


class _BinaryPreprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        str2bool = metadata.get('str2bool')
        self.str2bool = str2bool or {v: (True) for v in strings_utils.BOOL_TRUE_STRS}
        self.should_lower = str2bool is None

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if torch.jit.isinstance(v, List[Tuple[torch.Tensor, int]]):
            raise ValueError(f'Unsupported input: {v}')
        if torch.jit.isinstance(v, List[torch.Tensor]):
            v = torch.stack(v)
        if torch.jit.isinstance(v, torch.Tensor):
            return v
        v = [s.strip() for s in v]
        if self.should_lower:
            v = [s.lower() for s in v]
        indices = [self.str2bool.get(s, False) for s in v]
        return torch.tensor(indices, dtype=torch.float32)


FeaturePostProcessingOutputDict = Dict[str, Any]


class _BinaryPostprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        bool2str = metadata.get('bool2str')
        self.bool2str = {i: v for i, v in enumerate(bool2str)} if bool2str is not None else None
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        if self.bool2str is not None:
            predictions = predictions
            predictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]
        probabilities = torch.stack([1 - probabilities, probabilities], dim=-1)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities}


class _BinaryPredict(PredictModule):

    def __init__(self, threshold, calibration_module=None):
        super().__init__()
        self.threshold = threshold
        self.calibration_module = calibration_module

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        if self.calibration_module is not None:
            probabilities = self.calibration_module(logits)
        else:
            probabilities = torch.sigmoid(logits)
        predictions = probabilities >= self.threshold
        return {self.probabilities_key: probabilities, self.predictions_key: predictions, self.logits_key: logits}


UNKNOWN_SYMBOL = '<UNK>'


class _CategoryPreprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.str2idx = metadata['str2idx']
        if UNKNOWN_SYMBOL in self.str2idx:
            self.unk = self.str2idx[UNKNOWN_SYMBOL]
        else:
            self.unk = 0

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if not torch.jit.isinstance(v, List[str]):
            raise ValueError(f'Unsupported input: {v}')
        indices = [self.str2idx.get(s.strip(), self.unk) for s in v]
        return torch.tensor(indices, dtype=torch.int32)


class _CategoryPostprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.idx2str = {i: v for i, v in enumerate(metadata['idx2str'])}
        self.unk = UNKNOWN_SYMBOL
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        inv_preds = [self.idx2str.get(pred, self.unk) for pred in predictions]
        return {self.predictions_key: inv_preds, self.probabilities_key: probabilities}


class _CategoryPredict(PredictModule):

    def __init__(self, calibration_module=None, use_cumulative_probs=False):
        super().__init__()
        self.calibration_module = calibration_module
        self.use_cumulative_probs = use_cumulative_probs

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        if self.use_cumulative_probs:
            if self.calibration_module is not None:
                probabilities = self.calibration_module(logits)
            else:
                probabilities = torch.sigmoid(logits)
            probabilities = torch.cumprod(probabilities, dim=1)
            predict_levels = probabilities > 0.5
            predictions = torch.sum(predict_levels, dim=1)
        else:
            if self.calibration_module is not None:
                probabilities = self.calibration_module(logits)
            else:
                probabilities = torch.softmax(logits, -1)
            predictions = torch.argmax(probabilities, -1)
        predictions = predictions.long()
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.logits_key: logits}


class _DatePreprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if torch.jit.isinstance(v, List[torch.Tensor]):
            v = torch.stack(v)
        if torch.jit.isinstance(v, torch.Tensor):
            return v
        else:
            raise ValueError(f'Unsupported input: {v}')


FEATURE_NAME_SUFFIX = '__ludwig'


def get_module_dict_key_from_name(name: 'str', feature_name_suffix: 'str'=FEATURE_NAME_SUFFIX) ->str:
    """Returns a key that's guaranteed to be compatible with torch."""
    key = name.replace('.', '__ludwig_punct_period__')
    return key + feature_name_suffix


FEATURE_NAME_SUFFIX_LENGTH = len(FEATURE_NAME_SUFFIX)


def get_name_from_module_dict_key(key: 'str', feature_name_suffix_length: 'int'=FEATURE_NAME_SUFFIX_LENGTH) ->str:
    """Reverse of get_module_dict_key_from_name."""
    name = key.replace('__ludwig_punct_period__', '.')
    return name[:-feature_name_suffix_length]


class LudwigFeatureDict(torch.nn.Module):
    """Torch ModuleDict wrapper that permits keys with any name.

    Torch's ModuleDict implementation doesn't allow certain keys to be used if they conflict with existing class
    attributes, e.g.

    > torch.nn.ModuleDict({'type': torch.nn.Module()})  # Raises KeyError.

    This class is a simple wrapper around torch's ModuleDict that mitigates possible conflicts by using a key-suffixing
    protocol.

    This is also tracked in Pytorch: https://github.com/pytorch/pytorch/issues/71203.
    """

    def __init__(self):
        super().__init__()
        self.module_dict = torch.nn.ModuleDict()
        self.internal_key_to_original_name_map = {}

    def get(self, key) ->torch.nn.Module:
        return self.module_dict[get_module_dict_key_from_name(key)]

    def set(self, key: 'str', module: 'torch.nn.Module') ->None:
        module_dict_key_name = get_module_dict_key_from_name(key)
        self.internal_key_to_original_name_map[module_dict_key_name] = key
        self.module_dict[module_dict_key_name] = module

    def __len__(self) ->int:
        return len(self.module_dict)

    def __next__(self) ->None:
        return next(iter(self))

    def __iter__(self) ->None:
        return iter(self.keys())

    def keys(self) ->List[str]:
        return [get_name_from_module_dict_key(feature_name) for feature_name in self.internal_key_to_original_name_map.keys()]

    def values(self) ->List[torch.nn.Module]:
        return [module for _, module in self.module_dict.items()]

    def items(self) ->List[Tuple[str, torch.nn.Module]]:
        return [(get_name_from_module_dict_key(feature_name), module) for feature_name, module in self.module_dict.items()]

    def update(self, modules: 'Dict[str, torch.nn.Module]') ->None:
        for feature_name, module in modules.items():
            self.set(feature_name, module)


H3_PADDING_VALUE = 7


MAX_H3_RESOLUTION = 15


def bitslice(x: 'int', start_bit: 'int', slice_length: 'int') ->int:
    ones_mask: 'int' = int(2 ** slice_length - 1)
    return (x & ones_mask << start_bit) >> start_bit


def h3_base_cell(h3_long: 'int') ->int:
    return bitslice(h3_long, 64 - 19, 7)


def h3_component(h3_long: 'int', i: 'int') ->int:
    return bitslice(h3_long, 64 - 19 - 3 * i, 3)


def h3_resolution(h3_long: 'int') ->int:
    return bitslice(h3_long, 64 - 12, 4)


def h3_components(h3_long: 'int') ->List[int]:
    return [h3_component(h3_long, i) for i in range(1, h3_resolution(h3_long) + 1)]


def h3_edge(h3_long: 'int') ->int:
    return bitslice(h3_long, 64 - 8, 3)


def h3_index_mode(h3_long: 'int') ->int:
    return bitslice(h3_long, 64 - 5, 4)


class _H3Preprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.max_h3_resolution = MAX_H3_RESOLUTION
        self.h3_padding_value = H3_PADDING_VALUE
        self.computed_fill_value = float(metadata['preprocessing']['computed_fill_value'])

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if torch.jit.isinstance(v, List[torch.Tensor]):
            v = torch.stack(v)
        if not torch.jit.isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        v = torch.nan_to_num(v, nan=self.computed_fill_value)
        v = v.long()
        outputs: 'List[torch.Tensor]' = []
        for v_i in v:
            components = h3_to_components(v_i)
            header: 'List[int]' = [components.mode, components.edge, components.resolution, components.base_cell]
            cells_padding: 'List[int]' = [self.h3_padding_value] * (self.max_h3_resolution - len(components.cells))
            output = torch.tensor(header + components.cells + cells_padding, dtype=torch.uint8, device=v.device)
            outputs.append(output)
        return torch.stack(outputs)


class AutoAugment(torch.nn.Module):

    def __init__(self, config: 'AutoAugmentationConfig'):
        super().__init__()
        self.auto_augmentation_method = config.method
        self.augmentation_method = self.get_augmentation_method()

    def get_augmentation_method(self):
        if self.auto_augmentation_method == 'trivial_augment':
            return transforms.TrivialAugmentWide()
        if self.auto_augmentation_method == 'auto_augment':
            return transforms.AutoAugment()
        if self.auto_augmentation_method == 'rand_augment':
            return transforms.RandAugment()
        raise ValueError(f'Unsupported auto-augmentation method: {self.auto_augmentation_method}')

    def forward(self, imgs: 'torch.Tensor') ->torch.Tensor:
        method = self.augmentation_method
        uint8imgs = imgs
        augmented_imgs = method(uint8imgs)
        return augmented_imgs


class RandomVFlip(torch.nn.Module):

    def __init__(self, config: 'RandomVerticalFlipConfig'):
        super().__init__()

    def forward(self, imgs):
        if torch.rand(1) < 0.5:
            imgs = F.vflip(imgs)
        return imgs


class RandomHFlip(torch.nn.Module):

    def __init__(self, config: 'RandomHorizontalFlipConfig'):
        super().__init__()

    def forward(self, imgs):
        if torch.rand(1) < 0.5:
            imgs = F.hflip(imgs)
        return imgs


class RandomRotate(torch.nn.Module):

    def __init__(self, config: 'RandomRotateConfig'):
        super().__init__()
        self.degree = config.degree

    def forward(self, imgs):
        if torch.rand(1) < 0.5:
            angle = (torch.rand(1) * 2 * self.degree - self.degree).item()
            return F.rotate(imgs, angle)
        else:
            return imgs


class RandomContrast(torch.nn.Module):

    def __init__(self, config: 'RandomContrastConfig'):
        super().__init__()
        self.min_contrast = config.min
        self.contrast_adjustment_range = config.max - config.min

    def forward(self, imgs):
        if torch.rand(1) < 0.5:
            adjust_factor = (torch.rand(1) * self.contrast_adjustment_range + self.min_contrast).item()
            return F.adjust_contrast(imgs, adjust_factor)
        else:
            return imgs


class RandomBrightness(torch.nn.Module):

    def __init__(self, config: 'RandomBrightnessConfig'):
        super().__init__()
        self.min_brightness = config.min
        self.brightness_adjustment_range = config.max - config.min

    def forward(self, imgs):
        if torch.rand(1) < 0.5:
            adjust_factor = (torch.rand(1) * self.brightness_adjustment_range + self.min_brightness).item()
            return F.adjust_brightness(imgs, adjust_factor)
        else:
            return imgs


class RandomBlur(torch.nn.Module):

    def __init__(self, config: 'RandomBlurConfig'):
        super().__init__()
        self.kernel_size = [config.kernel_size, config.kernel_size]

    def forward(self, imgs):
        if torch.rand(1) < 0.5:
            imgs = F.gaussian_blur(imgs, self.kernel_size)
        return imgs


IMAGE = 'image'


DEFAULT_KEYS = ['None', 'none', 'null', None]


BAG = 'bag'


BINARY = 'binary'


CATEGORY = 'category'


def PublicAPI(*args, **kwargs):
    """Annotation for documenting public APIs. Public APIs are classes and methods exposed to end users of Ludwig.

    If stability="stable", the APIs will remain backwards compatible across minor Ludwig releases
    (e.g., Ludwig 0.6 -> Ludwig 0.7).

    If stability="experimental", the APIs can be used by advanced users who are tolerant to and expect
    breaking changes. This will likely be seen in the case of incremental new feature development.

    Args:
        stability: One of {"stable", "experimental"}

    Examples:
        >>> from api_annotations import PublicAPI
        >>> @PublicAPI
        ... def func1(x):
        ...     return x
        >>> @PublicAPI(stability="experimental")
        ... def func2(y):
        ...     return y
    """
    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
        return PublicAPI(stability='stable')(args[0])
    if 'stability' in kwargs:
        stability = kwargs['stability']
        assert stability in ['stable', 'experimental'], stability
    elif kwargs:
        raise ValueError(f'Unknown kwargs: {kwargs.keys()}')
    else:
        stability = 'stable'

    def wrap(obj):
        if stability == 'experimental':
            message = f'PublicAPI ({stability}): This API is {stability} and may change before becoming stable.'
        else:
            message = 'PublicAPI: This API is stable across Ludwig releases.'
        _append_doc(obj, message=message)
        _mark_annotated(obj)
        return obj
    return wrap


@PublicAPI
class LudwigError(Exception):
    """Base class for all custom exceptions raised by the Ludwig framework."""

    def __reduce__(self):
        """Docs: https://docs.python.org/3/library/pickle.html#object.__reduce__."""
        raise NotImplementedError("Implement __reduce__ for all subclasses of LudwigError as it's necessary for serialization by Ray. See https://github.com/ludwig-ai/ludwig/pull/2695.")


@PublicAPI
class ConfigValidationError(LudwigError, ValueError):
    """Exception raised for errors in the Ludwig configuration.

    Appropriate for bad configuration values, missing required configuration values, etc...

    Attributes:
        message - An error message describing the situation.
    """

    def __init__(self, message: 'str'):
        self.message = message
        super().__init__(message)

    def __reduce__(self):
        return type(self), (self.message,)


DATE = 'date'


H3 = 'h3'


NUMBER = 'number'


@DeveloperAPI
def memoized_method(*lru_args, **lru_kwargs):

    def decorator(func):

        @functools.wraps(func)
        def wrapped_func(self, *args, **kwargs):
            self_weak = weakref.ref(self)

            @functools.wraps(func)
            @functools.lru_cache(*lru_args, **lru_kwargs)
            def cached_method(*args, **kwargs):
                return func(self_weak(), *args, **kwargs)
            setattr(self, func.__name__, cached_method)
            return cached_method(*args, **kwargs)
        return wrapped_func
    return decorator


SEQUENCE = 'sequence'


SET = 'set'


TEXT = 'text'


TIMESERIES = 'timeseries'


VECTOR = 'vector'


@DeveloperAPI
def get_augmentation_op(feature_type: 'str', op_name: 'str'):
    return get_augmentation_op_registry()[feature_type][op_name]


class ImageAugmentation(torch.nn.Module):

    def __init__(self, augmentation_list: 'List[BaseAugmentationConfig]', normalize_mean: 'Optional[List[float]]'=None, normalize_std: 'Optional[List[float]]'=None):
        super().__init__()
        logger.info(f'Creating Augmentation pipline: {augmentation_list}')
        self.normalize_mean = normalize_mean
        self.normalize_std = normalize_std
        if self.training:
            self.augmentation_steps = torch.nn.Sequential()
            for aug_config in augmentation_list:
                try:
                    aug_op = get_augmentation_op(IMAGE, aug_config.type)
                    self.augmentation_steps.append(aug_op(aug_config))
                except KeyError:
                    raise ValueError(f'Invalid augmentation operation specification: {aug_config}')
        else:
            self.augmentation_steps = None

    def forward(self, imgs):
        if self.augmentation_steps:
            imgs = self._convert_back_to_uint8(imgs)
            logger.debug(f'Executing augmentation pipeline steps:\n{self.augmentation_steps}')
            imgs = self.augmentation_steps(imgs)
            imgs = self._renormalize_image(imgs)
        return imgs

    def _convert_back_to_uint8(self, images):
        if self.normalize_mean:
            mean = torch.as_tensor(self.normalize_mean, dtype=torch.float32).view(-1, 1, 1)
            std = torch.as_tensor(self.normalize_std, dtype=torch.float32).view(-1, 1, 1)
            return images.mul(std).add(mean).mul(255.0).type(torch.uint8)
        else:
            return images.mul(255.0).type(torch.uint8)

    def _renormalize_image(self, images):
        if self.normalize_mean:
            mean = torch.as_tensor(self.normalize_mean, dtype=torch.float32).view(-1, 1, 1)
            std = torch.as_tensor(self.normalize_std, dtype=torch.float32).view(-1, 1, 1)
            return images.type(torch.float32).div(255.0).sub(mean).div(std)
        else:
            return images.type(torch.float32).div(255.0)


@DeveloperAPI
def num_channels_in_image(img: 'torch.Tensor'):
    """Returns number of channels in image."""
    if img is None or img.ndim < 2:
        raise ValueError('Invalid image data')
    if img.ndim == 2:
        return 1
    else:
        return img.shape[0]


@DeveloperAPI
def get_class_mask_from_image(channel_class_map: 'torch.Tensor', img: 'torch.Tensor') ->torch.Tensor:
    """Returns a masked image where each mask value is the channel class of the input.
    Args:
        channel_class_map: A tensor mapping channel values to classes, where dim=0 is the class.
        img: An input image of dimensions [C x H x W] or [H x W], where C is the channel dimension

    Return:
        [mask] A masked image of dimensions [H x W] where each value is the channel class of the input
    """
    num_classes = channel_class_map.shape[0]
    mask = torch.full((img.shape[-2], img.shape[-1]), num_classes, dtype=torch.uint8)
    if img.ndim == 2:
        img = img.unsqueeze(0)
    if num_classes == 2 and num_channels_in_image(img) == 1:
        img = img.type(torch.float32) / 255
        img = img.round() * 255
        img = img.type(torch.uint8)
    img = img.permute(1, 2, 0)
    for nclass, value in enumerate(channel_class_map):
        mask[(img == value).all(-1)] = nclass
    if torch.any(mask.ge(num_classes)):
        raise ValueError(f'Image channel could not be mapped to a class because an unknown channel value was detected. {num_classes} classes were inferred from the first set of images. This image has a channel value that was not previously seen in the first set of images. Check preprocessing parameters for image resizing, num channels, num classes and num samples. Image resizing may affect channel values. ')
    return mask


@DeveloperAPI
def grayscale(img: 'torch.Tensor') ->torch.Tensor:
    """Grayscales RGB image."""
    return F.rgb_to_grayscale(img)


CROP_OR_PAD = 'crop_or_pad'


INTERPOLATE = 'interpolate'


@DeveloperAPI
def to_tuple(v: 'Union[int, Tuple[int, int]]') ->Tuple[int, int]:
    """Converts int or tuple to tuple of ints."""
    if torch.jit.isinstance(v, int):
        return v, v
    else:
        return v


@DeveloperAPI
def crop(img: 'torch.Tensor', new_size: 'Union[int, Tuple[int, int]]') ->torch.Tensor:
    """torchscript-compatible implementation of crop.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to crop
        size (Union[int, Tuple[int, int]]): size to crop to. If int, crops to square image of that size.

    Returns:
        torch.Tensor: cropped image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    return F.center_crop(img, output_size=new_size)


@DeveloperAPI
def pad(img: 'torch.Tensor', new_size: 'Union[int, Tuple[int, int]]') ->torch.Tensor:
    """torchscript-compatible implementation of pad.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to pad
        new_size (Union[int, Tuple[int, int]]): size to pad to. If int, resizes to square image of that size.

    Returns:
        torch.Tensor: padded image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    old_size = img.shape[-2:]
    pad_size = (torch.tensor(new_size) - torch.tensor(old_size)) / 2
    padding = torch.cat((torch.floor(pad_size), torch.ceil(pad_size)))
    padding[padding < 0] = 0
    padding = [int(x) for x in padding]
    return F.pad(img, padding=padding, padding_mode='edge')


@DeveloperAPI
def crop_or_pad(img: 'torch.Tensor', new_size: 'Union[int, Tuple[int, int]]'):
    """torchscript-compatible implementation of resize using constants.CROP_OR_PAD.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to resize
        new_size (Union[int, Tuple[int, int]]): size to resize to. If int, resizes to square image of that size.

    Returns:
        torch.Tensor: resized image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    if list(new_size) == list(img.shape[-2:]):
        return img
    img = pad(img, new_size)
    img = crop(img, new_size)
    return img


@DeveloperAPI
def resize_image(img: 'torch.Tensor', new_size: 'Union[int, Tuple[int, int]]', resize_method: 'str', crop_or_pad_constant: 'str'=CROP_OR_PAD, interpolate_constant: 'str'=INTERPOLATE) ->torch.Tensor:
    """torchscript-compatible implementation of resize.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to resize
        new_size (Union[int, Tuple[int, int]]): size to resize to. If int, resizes to square image of that size.
        resize_method (str): method to use for resizing. Either constants.CROP_OR_PAD or constants.INTERPOLATE.

    Returns:
        torch.Tensor: resized image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    if list(img.shape[-2:]) != list(new_size):
        if resize_method == crop_or_pad_constant:
            return crop_or_pad(img, new_size)
        elif resize_method == interpolate_constant:
            return F.resize(img, new_size)
        raise ValueError(f'Invalid image resize method: {resize_method}')
    return img


class _ImagePreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by ImageFeatureMixin.add_feature_data."""

    def __init__(self, metadata: 'TrainingSetMetadataDict', torchvision_transform: 'Optional[torch.nn.Module]'=None, transform_metadata: 'Optional[ImageTransformMetadata]'=None):
        super().__init__()
        self.resize_method = metadata['preprocessing']['resize_method']
        self.torchvision_transform = torchvision_transform
        if transform_metadata is not None:
            self.height = transform_metadata.height
            self.width = transform_metadata.width
            self.num_channels = transform_metadata.num_channels
            self.channel_class_map = torch.Tensor([])
        else:
            self.height = metadata['preprocessing']['height']
            self.width = metadata['preprocessing']['width']
            self.num_channels = metadata['preprocessing']['num_channels']
            self.channel_class_map = torch.ByteTensor(metadata['preprocessing']['channel_class_map'])

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        """Takes a list of images and adjusts the size and number of channels as specified in the metadata.

        If `v` is already a torch.Tensor, we assume that the images are already preprocessed to be the same size.
        """
        if not torch.jit.isinstance(v, List[torch.Tensor]):
            if not torch.jit.isinstance(v, torch.Tensor):
                raise ValueError(f'Unsupported input: {v}')
        if self.torchvision_transform is not None:
            if torch.jit.isinstance(v, List[torch.Tensor]):
                imgs = [self.torchvision_transform(img) for img in v]
            else:
                imgs = [self.torchvision_transform(img) for img in torch.unbind(v)]
            imgs_stacked = torch.stack(imgs)
        else:
            if torch.jit.isinstance(v, List[torch.Tensor]):
                imgs = [resize_image(img, (self.height, self.width), self.resize_method) for img in v]
                imgs_stacked = torch.stack(imgs)
            else:
                imgs_stacked = v
            _, num_channels, height, width = imgs_stacked.shape
            if height != self.height or width != self.width:
                imgs_stacked = resize_image(imgs_stacked, (self.height, self.width), self.resize_method)
            if num_channels != self.num_channels:
                if self.num_channels == 1:
                    imgs_stacked = grayscale(imgs_stacked)
                elif num_channels < self.num_channels:
                    extra_channels = self.num_channels - num_channels
                    imgs_stacked = torch.nn.functional.pad(imgs_stacked, [0, 0, 0, 0, 0, extra_channels])
                else:
                    raise ValueError(f'Number of channels cannot be reconciled. metadata.num_channels = {self.num_channels}, but imgs.shape[1] = {num_channels}')
            if self.channel_class_map.shape[0]:
                masks = []
                for img in imgs_stacked:
                    mask = get_class_mask_from_image(self.channel_class_map, img)
                    masks.append(mask)
                imgs_stacked = torch.stack(masks)
            else:
                imgs_stacked = imgs_stacked.type(torch.float32) / 255
        return imgs_stacked


class _ImagePostprocessing(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.logits_key = LOGITS
        self.predictions_key = PREDICTIONS

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        logits = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.logits_key)
        return {self.predictions_key: predictions, self.logits_key: logits}


class _ImagePredict(PredictModule):

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        predictions = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.predictions_key)
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        return {self.predictions_key: predictions, self.logits_key: logits}


class NumberTransformer(nn.Module, ABC):

    @abstractmethod
    def transform(self, x: 'np.ndarray') ->np.ndarray:
        pass

    @abstractmethod
    def inverse_transform(self, x: 'np.ndarray') ->np.ndarray:
        pass

    @abstractmethod
    def transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        pass

    @abstractmethod
    def inverse_transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        pass

    @staticmethod
    @abstractmethod
    def fit_transform_params(column: 'np.ndarray', backend: 'Any') ->Dict[str, Any]:
        pass


class ZScoreTransformer(NumberTransformer):

    def __init__(self, mean: 'float'=None, std: 'float'=None, **kwargs: dict):
        super().__init__()
        self.mu = float(mean) if mean is not None else mean
        self.sigma = float(std) if std is not None else std
        self.feature_name = kwargs.get(NAME, '')
        if self.sigma == 0:
            raise RuntimeError(f'Cannot apply zscore normalization to `{self.feature_name}` since it has a standard deviation of 0. This is most likely because `{self.feature_name}` has a constant value of {self.mu} for all rows in the dataset. Consider removing this feature from your Ludwig config since it is not useful for your machine learning model.')

    def transform(self, x: 'np.ndarray') ->np.ndarray:
        return (x - self.mu) / self.sigma

    def inverse_transform(self, x: 'np.ndarray') ->np.ndarray:
        return x * self.sigma + self.mu

    def transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return (x - self.mu) / self.sigma

    def inverse_transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return x * self.sigma + self.mu

    @staticmethod
    def fit_transform_params(column: 'np.ndarray', backend: "'Backend'") ->Dict[str, Any]:
        compute = backend.df_engine.compute
        return {'mean': compute(column.astype(np.float32).mean()), 'std': compute(column.astype(np.float32).std())}


class MinMaxTransformer(NumberTransformer):

    def __init__(self, min: 'float'=None, max: 'float'=None, **kwargs: dict):
        super().__init__()
        self.min_value = float(min) if min is not None else min
        self.max_value = float(max) if max is not None else max
        if self.min_value is None or self.max_value is None:
            self.range = None
        else:
            self.range = self.max_value - self.min_value

    def transform(self, x: 'np.ndarray') ->np.ndarray:
        return (x - self.min_value) / self.range

    def inverse_transform(self, x: 'np.ndarray') ->np.ndarray:
        if self.range is None:
            raise ValueError('Numeric transformer needs to be instantiated with min and max values.')
        return x * self.range + self.min_value

    def transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return (x - self.min_value) / self.range

    def inverse_transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        if self.range is None:
            raise ValueError('Numeric transformer needs to be instantiated with min and max values.')
        return x * self.range + self.min_value

    @staticmethod
    def fit_transform_params(column: 'np.ndarray', backend: "'Backend'") ->Dict[str, Any]:
        compute = backend.df_engine.compute
        return {'min': compute(column.astype(np.float32).min()), 'max': compute(column.astype(np.float32).max())}


class InterQuartileTransformer(NumberTransformer):

    def __init__(self, q1: 'float'=None, q2: 'float'=None, q3: 'float'=None, **kwargs: dict):
        super().__init__()
        self.q1 = float(q1) if q1 is not None else q1
        self.q2 = float(q2) if q2 is not None else q2
        self.q3 = float(q3) if q3 is not None else q3
        if self.q1 is None or self.q3 is None:
            self.interquartile_range = None
        else:
            self.interquartile_range = self.q3 - self.q1
        self.feature_name = kwargs.get(NAME, '')
        if self.interquartile_range == 0:
            raise RuntimeError(f'Cannot apply InterQuartileNormalization to `{self.feature_name}` sincethe interquartile range is 0, which will result in a ZeroDivisionError.')

    def transform(self, x: 'np.ndarray') ->np.ndarray:
        return (x - self.q2) / self.interquartile_range

    def inverse_transform(self, x: 'np.ndarray') ->np.ndarray:
        return x * self.interquartile_range + self.q2

    def transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return (x - self.q2) / self.interquartile_range

    def inverse_transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return x * self.interquartile_range + self.q2

    @staticmethod
    def fit_transform_params(column: 'np.ndarray', backend: "'Backend'") ->Dict[str, Any]:
        return {'q1': np.percentile(column.astype(np.float32), 25), 'q2': np.percentile(column.astype(np.float32), 50), 'q3': np.percentile(column.astype(np.float32), 75)}


class Log1pTransformer(NumberTransformer):

    def __init__(self, **kwargs: dict):
        super().__init__()
        self.feature_name = kwargs.get(NAME, '')

    def transform(self, x: 'np.ndarray') ->np.ndarray:
        if np.any(x <= 0):
            raise ValueError(f'One or more values in the `{self.feature_name}` feature are non-positive.  log1p normalization is defined only for positive values.')
        return np.log1p(x)

    def inverse_transform(self, x: 'np.ndarray') ->np.ndarray:
        return np.expm1(x)

    def transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return torch.log1p(x)

    def inverse_transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return torch.expm1(x)

    @staticmethod
    def fit_transform_params(column: 'np.ndarray', backend: "'Backend'") ->Dict[str, Any]:
        return {}


class IdentityTransformer(NumberTransformer):

    def __init__(self, **kwargs):
        super().__init__()

    def transform(self, x: 'np.ndarray') ->np.ndarray:
        return x

    def inverse_transform(self, x: 'np.ndarray') ->np.ndarray:
        return x

    def transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return x

    def inverse_transform_inference(self, x: 'torch.Tensor') ->torch.Tensor:
        return x

    @staticmethod
    def fit_transform_params(column: 'np.ndarray', backend: "'Backend'") ->Dict[str, Any]:
        return {}


class _OutlierReplacer(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.zscore_transformer = ZScoreTransformer(**metadata)
        self.outlier_threshold = metadata['preprocessing'].get('outlier_threshold')
        self.computed_outlier_fill_value = float(metadata['preprocessing']['computed_outlier_fill_value'])

    def forward(self, v: 'torch.Tensor') ->torch.Tensor:
        outliers = self.zscore_transformer.transform_inference(v).abs().gt(self.outlier_threshold)
        v_masked = torch.masked_fill(v, outliers, torch.nan)
        v = torch.nan_to_num(v_masked, nan=self.computed_outlier_fill_value)
        return v


numeric_transformation_registry = {'minmax': MinMaxTransformer, 'zscore': ZScoreTransformer, 'log1p': Log1pTransformer, 'iq': InterQuartileTransformer, None: IdentityTransformer}


def get_transformer(metadata, preprocessing_parameters) ->NumberTransformer:
    return get_from_registry(preprocessing_parameters.get('normalization', None), numeric_transformation_registry)(**metadata)


class _NumberPreprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.computed_fill_value = float(metadata['preprocessing']['computed_fill_value'])
        self.numeric_transformer = get_transformer(metadata, metadata['preprocessing'])
        self.outlier_replacer = None
        if metadata['preprocessing'].get('outlier_strategy') is not None:
            self.outlier_replacer = _OutlierReplacer(metadata)

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if not torch.jit.isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        v = torch.nan_to_num(v, nan=self.computed_fill_value)
        v = v
        if self.outlier_replacer is not None:
            v = self.outlier_replacer(v)
        return self.numeric_transformer.transform_inference(v)


class _NumberPostprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.numeric_transformer = get_transformer(metadata, metadata['preprocessing'])
        self.predictions_key = PREDICTIONS

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        return {self.predictions_key: self.numeric_transformer.inverse_transform_inference(predictions)}


class _NumberPredict(PredictModule):

    def __init__(self, clip):
        super().__init__()
        self.clip = clip

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        predictions = logits
        if self.clip is not None:
            predictions = torch.clamp(logits, self.clip[0], self.clip[1])
            logger.debug(f'  clipped_predictions: {predictions}')
        return {self.predictions_key: predictions, self.logits_key: logits}


START_SYMBOL = '<SOS>'


STOP_SYMBOL = '<EOS>'


TORCHTEXT_0_12_0_TOKENIZERS = {'sentencepiece', 'clip', 'gpt2bpe'}


TORCHTEXT_0_13_0_TOKENIZERS = {'bert'}


class CharactersToListTokenizer(torch.nn.Module):
    """Implements torchscript-compatible characters tokenization."""

    def __init__(self, **kwargs):
        super().__init__()

    def forward(self, v: 'Union[str, List[str], torch.Tensor]') ->Any:
        if isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        inputs: 'List[str]' = []
        if isinstance(v, str):
            inputs.append(v)
        else:
            inputs.extend(v)
        tokens: 'List[List[str]]' = []
        for sequence in inputs:
            split_sequence = [char for char in sequence]
            token_sequence: 'List[str]' = []
            for token in self.get_tokens(split_sequence):
                if len(token) > 0:
                    token_sequence.append(token)
            tokens.append(token_sequence)
        return tokens[0] if isinstance(v, str) else tokens

    def get_tokens(self, tokens: 'List[str]') ->List[str]:
        return tokens


class BaseTokenizer:

    @abstractmethod
    def __init__(self, **kwargs):
        pass

    @abstractmethod
    def __call__(self, text: 'str'):
        pass

    def convert_token_to_id(self, token: 'str') ->int:
        raise NotImplementedError()


@DeveloperAPI
def get_fs_and_path(url):
    protocol, path = split_protocol(url)
    path = unquote(urlparse(path).path)
    path = os.fspath(pathlib.PurePosixPath(path))
    fs = fsspec.filesystem(protocol)
    return fs, path


@DeveloperAPI
def download(rpath, lpath):
    fs, path = get_fs_and_path(rpath)
    pyarrow.fs.copy_files(path, lpath, source_filesystem=pyarrow.fs.PyFileSystem(pyarrow.fs.FSSpecHandler(fs)))


language_module_registry = {'en': 'en_core_web_sm', 'it': 'it_core_news_sm', 'es': 'es_core_news_sm', 'de': 'de_core_news_sm', 'fr': 'fr_core_news_sm', 'pt': 'pt_core_news_sm', 'nl': 'nl_core_news_sm', 'el': 'el_core_news_sm', 'nb': 'nb_core_news_sm', 'lt': 'lt_core_news_sm', 'da': 'da_core_news_sm', 'pl': 'pl_core_news_sm', 'ro': 'ro_core_news_sm', 'ja': 'ja_core_news_sm', 'zh': 'zh_core_web_sm', 'xx': 'xx_ent_wiki_sm'}


punctuation = {'.', ',', '@', '$', '%', '/', ':', ';', '+', '='}


def pass_filters(token, filter_numbers=False, filter_punctuation=False, filter_short_tokens=False, filter_stopwords=False):
    passes_filters = True
    if filter_numbers:
        passes_filters = not token.like_num
    if passes_filters and filter_punctuation:
        passes_filters = not bool(set(token.orth_) & punctuation)
    if passes_filters and filter_short_tokens:
        passes_filters = len(token) > 2
    if passes_filters and filter_stopwords:
        passes_filters = not token.is_stop
    return passes_filters


def process_text(text, nlp_pipeline, return_lemma=False, filter_numbers=False, filter_punctuation=False, filter_short_tokens=False, filter_stopwords=False):
    doc = nlp_pipeline(text)
    return [(token.lemma_ if return_lemma else token.text) for token in doc if pass_filters(token, filter_numbers, filter_punctuation, filter_short_tokens, filter_stopwords)]


class ChineseFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ChineseLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ChineseLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), return_lemma=True, filter_stopwords=True)


class ChineseLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), return_lemma=True)


class ChineseRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), filter_stopwords=True)


class ChineseTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'))


class StringSplitTokenizer(torch.nn.Module):

    def __init__(self, split_string, **kwargs):
        super().__init__()
        self.split_string = split_string

    def forward(self, v: 'Union[str, List[str], torch.Tensor]') ->Any:
        if isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        inputs: 'List[str]' = []
        if isinstance(v, str):
            inputs.append(v)
        else:
            inputs.extend(v)
        tokens: 'List[List[str]]' = []
        for sequence in inputs:
            split_sequence = sequence.strip().split(self.split_string)
            token_sequence: 'List[str]' = []
            for token in self.get_tokens(split_sequence):
                if len(token) > 0:
                    token_sequence.append(token)
            tokens.append(token_sequence)
        return tokens[0] if isinstance(v, str) else tokens

    def get_tokens(self, tokens: 'List[str]') ->List[str]:
        return tokens


class CommaStringToListTokenizer(StringSplitTokenizer):
    """Implements torchscript-compatible comma tokenization."""

    def __init__(self, **kwargs):
        super().__init__(split_string=',', **kwargs)


class DanishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DanishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DanishLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), return_lemma=True, filter_stopwords=True)


class DanishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), return_lemma=True)


class DanishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), filter_stopwords=True)


class DanishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'))


class DutchFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DutchLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DutchLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), return_lemma=True, filter_stopwords=True)


class DutchLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), return_lemma=True)


class DutchRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), filter_stopwords=True)


class DutchTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'))


class EnglishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class EnglishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class EnglishLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), return_lemma=True, filter_stopwords=True)


class EnglishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        process_text(text, load_nlp_pipeline('en'), return_lemma=True)


class EnglishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), filter_stopwords=True)


class EnglishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'))


class FrenchFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class FrenchLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class FrenchLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), return_lemma=True, filter_stopwords=True)


class FrenchLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), return_lemma=True)


class FrenchRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), filter_stopwords=True)


class FrenchTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'))


class GermanFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GermanLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GermanLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), return_lemma=True, filter_stopwords=True)


class GermanLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), return_lemma=True)


class GermanRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), filter_stopwords=True)


class GermanTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'))


class GreekFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GreekLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GreekLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), return_lemma=True, filter_stopwords=True)


class GreekLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), return_lemma=True)


class GreekRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), filter_stopwords=True)


class GreekTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'))


class ItalianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ItalianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ItalianLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), return_lemma=True, filter_stopwords=True)


class ItalianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), return_lemma=True)


class ItalianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), filter_stopwords=True)


class ItalianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'))


class JapaneseFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class JapaneseLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class JapaneseLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), return_lemma=True, filter_stopwords=True)


class JapaneseLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), return_lemma=True)


class JapaneseRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), filter_stopwords=True)


class JapaneseTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'))


class LithuanianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class LithuanianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class LithuanianLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), return_lemma=True, filter_stopwords=True)


class LithuanianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), return_lemma=True)


class LithuanianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), filter_stopwords=True)


class LithuanianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'))


class MultiFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class MultiLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class MultiLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), return_lemma=True, filter_stopwords=True)


class MultiLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), return_lemma=True)


class MultiRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), filter_stopwords=True)


class MultiTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class SpaceStringToListTokenizer(StringSplitTokenizer):
    """Implements torchscript-compatible whitespace tokenization."""

    def __init__(self, **kwargs):
        super().__init__(split_string=' ', **kwargs)


class NgramTokenizer(SpaceStringToListTokenizer):
    """Implements torchscript-compatible n-gram tokenization."""

    def __init__(self, ngram_size: 'int'=2, **kwargs):
        super().__init__()
        self.n = ngram_size or 2

    def get_tokens(self, tokens: 'List[str]') ->List[str]:
        return list(ngrams_iterator(tokens, ngrams=self.n))


class NorwegianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class NorwegianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class NorwegianLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), return_lemma=True, filter_stopwords=True)


class NorwegianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), return_lemma=True)


class NorwegianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), filter_stopwords=True)


class NorwegianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'))


class PolishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PolishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PolishLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), return_lemma=True, filter_stopwords=True)


class PolishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), return_lemma=True)


class PolishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), filter_stopwords=True)


class PolishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'))


class PortugueseFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PortugueseLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PortugueseLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), return_lemma=True, filter_stopwords=True)


class PortugueseLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), return_lemma=True)


class PortugueseRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), filter_stopwords=True)


class PortugueseTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'))


class RomanianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class RomanianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class RomanianLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), return_lemma=True, filter_stopwords=True)


class RomanianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), return_lemma=True)


class RomanianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), filter_stopwords=True)


class RomanianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'))


class SpacePunctuationStringToListTokenizer(torch.nn.Module):
    """Implements torchscript-compatible space_punct tokenization."""

    def __init__(self, **kwargs):
        super().__init__()

    def is_regex_w(self, c: 'str') ->bool:
        return c.isalnum() or c == '_'

    def forward(self, v: 'Union[str, List[str], torch.Tensor]') ->Any:
        if isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        inputs: 'List[str]' = []
        if isinstance(v, str):
            inputs.append(v)
        else:
            inputs.extend(v)
        tokens: 'List[List[str]]' = []
        for sequence in inputs:
            token_sequence: 'List[str]' = []
            word: 'List[str]' = []
            for c in sequence:
                if self.is_regex_w(c):
                    word.append(c)
                elif len(word) > 0:
                    token_sequence.append(''.join(word))
                    word.clear()
                if not self.is_regex_w(c) and not c.isspace():
                    token_sequence.append(c)
            if len(word) > 0:
                token_sequence.append(''.join(word))
            tokens.append(token_sequence)
        return tokens[0] if isinstance(v, str) else tokens


class SpanishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class SpanishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class SpanishLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), return_lemma=True, filter_stopwords=True)


class SpanishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), return_lemma=True)


class SpanishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), filter_stopwords=True)


class SpanishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'))


class StrippedStringToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return [text.strip()]


class UnderscoreStringToListTokenizer(StringSplitTokenizer):
    """Implements torchscript-compatible underscore tokenization."""

    def __init__(self, **kwargs):
        super().__init__(split_string='_', **kwargs)


class UntokenizedStringToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return [text]


tokenizer_registry = {'space': SpaceStringToListTokenizer, 'space_punct': SpacePunctuationStringToListTokenizer, 'ngram': NgramTokenizer, 'characters': CharactersToListTokenizer, 'underscore': UnderscoreStringToListTokenizer, 'comma': CommaStringToListTokenizer, 'untokenized': UntokenizedStringToListTokenizer, 'stripped': StrippedStringToListTokenizer, 'english_tokenize': EnglishTokenizer, 'english_tokenize_filter': EnglishFilterTokenizer, 'english_tokenize_remove_stopwords': EnglishRemoveStopwordsTokenizer, 'english_lemmatize': EnglishLemmatizeTokenizer, 'english_lemmatize_filter': EnglishLemmatizeFilterTokenizer, 'english_lemmatize_remove_stopwords': EnglishLemmatizeRemoveStopwordsTokenizer, 'italian_tokenize': ItalianTokenizer, 'italian_tokenize_filter': ItalianFilterTokenizer, 'italian_tokenize_remove_stopwords': ItalianRemoveStopwordsTokenizer, 'italian_lemmatize': ItalianLemmatizeTokenizer, 'italian_lemmatize_filter': ItalianLemmatizeFilterTokenizer, 'italian_lemmatize_remove_stopwords': ItalianLemmatizeRemoveStopwordsTokenizer, 'spanish_tokenize': SpanishTokenizer, 'spanish_tokenize_filter': SpanishFilterTokenizer, 'spanish_tokenize_remove_stopwords': SpanishRemoveStopwordsTokenizer, 'spanish_lemmatize': SpanishLemmatizeTokenizer, 'spanish_lemmatize_filter': SpanishLemmatizeFilterTokenizer, 'spanish_lemmatize_remove_stopwords': SpanishLemmatizeRemoveStopwordsTokenizer, 'german_tokenize': GermanTokenizer, 'german_tokenize_filter': GermanFilterTokenizer, 'german_tokenize_remove_stopwords': GermanRemoveStopwordsTokenizer, 'german_lemmatize': GermanLemmatizeTokenizer, 'german_lemmatize_filter': GermanLemmatizeFilterTokenizer, 'german_lemmatize_remove_stopwords': GermanLemmatizeRemoveStopwordsTokenizer, 'french_tokenize': FrenchTokenizer, 'french_tokenize_filter': FrenchFilterTokenizer, 'french_tokenize_remove_stopwords': FrenchRemoveStopwordsTokenizer, 'french_lemmatize': FrenchLemmatizeTokenizer, 'french_lemmatize_filter': FrenchLemmatizeFilterTokenizer, 'french_lemmatize_remove_stopwords': FrenchLemmatizeRemoveStopwordsTokenizer, 'portuguese_tokenize': PortugueseTokenizer, 'portuguese_tokenize_filter': PortugueseFilterTokenizer, 'portuguese_tokenize_remove_stopwords': PortugueseRemoveStopwordsTokenizer, 'portuguese_lemmatize': PortugueseLemmatizeTokenizer, 'portuguese_lemmatize_filter': PortugueseLemmatizeFilterTokenizer, 'portuguese_lemmatize_remove_stopwords': PortugueseLemmatizeRemoveStopwordsTokenizer, 'dutch_tokenize': DutchTokenizer, 'dutch_tokenize_filter': DutchFilterTokenizer, 'dutch_tokenize_remove_stopwords': DutchRemoveStopwordsTokenizer, 'dutch_lemmatize': DutchLemmatizeTokenizer, 'dutch_lemmatize_filter': DutchLemmatizeFilterTokenizer, 'dutch_lemmatize_remove_stopwords': DutchLemmatizeRemoveStopwordsTokenizer, 'greek_tokenize': GreekTokenizer, 'greek_tokenize_filter': GreekFilterTokenizer, 'greek_tokenize_remove_stopwords': GreekRemoveStopwordsTokenizer, 'greek_lemmatize': GreekLemmatizeTokenizer, 'greek_lemmatize_filter': GreekLemmatizeFilterTokenizer, 'greek_lemmatize_remove_stopwords': GreekLemmatizeRemoveStopwordsFilterTokenizer, 'norwegian_tokenize': NorwegianTokenizer, 'norwegian_tokenize_filter': NorwegianFilterTokenizer, 'norwegian_tokenize_remove_stopwords': NorwegianRemoveStopwordsTokenizer, 'norwegian_lemmatize': NorwegianLemmatizeTokenizer, 'norwegian_lemmatize_filter': NorwegianLemmatizeFilterTokenizer, 'norwegian_lemmatize_remove_stopwords': NorwegianLemmatizeRemoveStopwordsFilterTokenizer, 'lithuanian_tokenize': LithuanianTokenizer, 'lithuanian_tokenize_filter': LithuanianFilterTokenizer, 'lithuanian_tokenize_remove_stopwords': LithuanianRemoveStopwordsTokenizer, 'lithuanian_lemmatize': LithuanianLemmatizeTokenizer, 'lithuanian_lemmatize_filter': LithuanianLemmatizeFilterTokenizer, 'lithuanian_lemmatize_remove_stopwords': LithuanianLemmatizeRemoveStopwordsFilterTokenizer, 'danish_tokenize': DanishTokenizer, 'danish_tokenize_filter': DanishFilterTokenizer, 'danish_tokenize_remove_stopwords': DanishRemoveStopwordsTokenizer, 'danish_lemmatize': DanishLemmatizeTokenizer, 'danish_lemmatize_filter': DanishLemmatizeFilterTokenizer, 'danish_lemmatize_remove_stopwords': DanishLemmatizeRemoveStopwordsFilterTokenizer, 'polish_tokenize': PolishTokenizer, 'polish_tokenize_filter': PolishFilterTokenizer, 'polish_tokenize_remove_stopwords': PolishRemoveStopwordsTokenizer, 'polish_lemmatize': PolishLemmatizeTokenizer, 'polish_lemmatize_filter': PolishLemmatizeFilterTokenizer, 'polish_lemmatize_remove_stopwords': PolishLemmatizeRemoveStopwordsFilterTokenizer, 'romanian_tokenize': RomanianTokenizer, 'romanian_tokenize_filter': RomanianFilterTokenizer, 'romanian_tokenize_remove_stopwords': RomanianRemoveStopwordsTokenizer, 'romanian_lemmatize': RomanianLemmatizeTokenizer, 'romanian_lemmatize_filter': RomanianLemmatizeFilterTokenizer, 'romanian_lemmatize_remove_stopwords': RomanianLemmatizeRemoveStopwordsFilterTokenizer, 'japanese_tokenize': JapaneseTokenizer, 'japanese_tokenize_filter': JapaneseFilterTokenizer, 'japanese_tokenize_remove_stopwords': JapaneseRemoveStopwordsTokenizer, 'japanese_lemmatize': JapaneseLemmatizeTokenizer, 'japanese_lemmatize_filter': JapaneseLemmatizeFilterTokenizer, 'japanese_lemmatize_remove_stopwords': JapaneseLemmatizeRemoveStopwordsFilterTokenizer, 'chinese_tokenize': ChineseTokenizer, 'chinese_tokenize_filter': ChineseFilterTokenizer, 'chinese_tokenize_remove_stopwords': ChineseRemoveStopwordsTokenizer, 'chinese_lemmatize': ChineseLemmatizeTokenizer, 'chinese_lemmatize_filter': ChineseLemmatizeFilterTokenizer, 'chinese_lemmatize_remove_stopwords': ChineseLemmatizeRemoveStopwordsFilterTokenizer, 'multi_tokenize': MultiTokenizer, 'multi_tokenize_filter': MultiFilterTokenizer, 'multi_tokenize_remove_stopwords': MultiRemoveStopwordsTokenizer, 'multi_lemmatize': MultiLemmatizeTokenizer, 'multi_lemmatize_filter': MultiLemmatizeFilterTokenizer, 'multi_lemmatize_remove_stopwords': MultiLemmatizeRemoveStopwordsTokenizer}


def get_tokenizer_from_registry(tokenizer_name: 'str') ->torch.nn.Module:
    """Returns the appropriate tokenizer from the tokenizer registry.

    Raises a KeyError if a tokenizer that does not exist in the registry is requested, with additional help text if the
    requested tokenizer would be available for a different version of torchtext.
    """
    if tokenizer_name in tokenizer_registry:
        return tokenizer_registry[tokenizer_name]
    if torch.torch_version.TorchVersion(torchtext.__version__) < (0, 12, 0) and tokenizer_name in TORCHTEXT_0_12_0_TOKENIZERS:
        raise KeyError(f"torchtext>=0.12.0 is not installed, so '{tokenizer_name}' and the following tokenizers are not available: {TORCHTEXT_0_12_0_TOKENIZERS}")
    if torch.torch_version.TorchVersion(torchtext.__version__) < (0, 13, 0) and tokenizer_name in TORCHTEXT_0_13_0_TOKENIZERS:
        raise KeyError(f"torchtext>=0.13.0 is not installed, so '{tokenizer_name}' and the following tokenizers are not available: {TORCHTEXT_0_13_0_TOKENIZERS}")
    raise KeyError(f"Invalid tokenizer name: '{tokenizer_name}'. Available tokenizers: {tokenizer_registry.keys()}")


class _SequencePreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by SequenceFeatureMixin.add_feature_data."""

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.lowercase = metadata['preprocessing']['lowercase']
        self.tokenizer_type = metadata['preprocessing']['tokenizer']
        self.tokenizer = get_tokenizer_from_registry(self.tokenizer_type)(pretrained_model_name_or_path=metadata['preprocessing'].get('pretrained_model_name_or_path', None))
        if not isinstance(self.tokenizer, torch.nn.Module):
            raise ValueError(f'tokenizer must be a torch.nn.Module, got {self.tokenizer}')
        self.padding_symbol = metadata['preprocessing']['padding_symbol']
        self.unknown_symbol = metadata['preprocessing']['unknown_symbol']
        self.start_symbol = START_SYMBOL
        self.stop_symbol = STOP_SYMBOL
        self.max_sequence_length = int(metadata['max_sequence_length'])
        self.unit_to_id = metadata['str2idx']
        self.computed_fill_value = metadata['preprocessing']['computed_fill_value']

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        """Takes a list of strings and returns a tensor of token ids."""
        if not torch.jit.isinstance(v, List[str]):
            raise ValueError(f'Unsupported input: {v}')
        futures: 'List[torch.jit.Future[torch.Tensor]]' = []
        for sequence in v:
            futures.append(torch.jit.fork(self._process_sequence, sequence))
        sequence_matrix = []
        for future in futures:
            sequence_matrix.append(torch.jit.wait(future))
        return torch.stack(sequence_matrix)

    def _process_sequence(self, sequence: 'str') ->torch.Tensor:
        sequence = self.computed_fill_value if sequence == 'nan' else sequence
        if self.lowercase and self.tokenizer_type != 'hf_tokenizer':
            sequence_str: 'str' = sequence.lower()
        else:
            sequence_str: 'str' = sequence
        sequence_vector = torch.full([self.max_sequence_length], self.unit_to_id[self.padding_symbol])
        if self.tokenizer_type == 'hf_tokenizer':
            unit_sequence = self.tokenizer(sequence)
            assert torch.jit.isinstance(unit_sequence, List[int])
            sequence_length = min(len(unit_sequence), self.max_sequence_length)
            sequence_vector[:sequence_length] = torch.tensor(unit_sequence)[:sequence_length]
            return sequence_vector
        unit_sequence = self.tokenizer(sequence_str)
        assert torch.jit.isinstance(unit_sequence, List[str])
        sequence_vector[0] = self.unit_to_id[self.start_symbol]
        if len(unit_sequence) + 1 < self.max_sequence_length:
            sequence_length = len(unit_sequence)
            sequence_vector[len(unit_sequence) + 1] = self.unit_to_id[self.stop_symbol]
        else:
            sequence_length = self.max_sequence_length - 1
        for i in range(sequence_length):
            curr_unit = unit_sequence[i]
            if curr_unit in self.unit_to_id:
                curr_id = self.unit_to_id[curr_unit]
            else:
                curr_id = self.unit_to_id[self.unknown_symbol]
            sequence_vector[i + 1] = curr_id
        return sequence_vector


PROBABILITY = 'probability'


class _SequencePostprocessing(torch.nn.Module):

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.max_sequence_length = int(metadata['max_sequence_length'])
        self.idx2str = metadata['idx2str']
        self.unknown_symbol = UNKNOWN_SYMBOL
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES
        self.probability_key = PROBABILITY

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        pred_predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        pred_probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        predictions: 'List[List[str]]' = []
        for sequence in pred_predictions:
            sequence_predictions: 'List[str]' = []
            for i in range(self.max_sequence_length):
                unit_id = int(sequence[i].item())
                if unit_id < len(self.idx2str):
                    unit_prediction = self.idx2str[unit_id]
                else:
                    unit_prediction = self.unknown_symbol
                sequence_predictions.append(unit_prediction)
            predictions.append(sequence_predictions)
        probabilities, _ = torch.max(pred_probabilities, dim=-1)
        probability = torch.sum(torch.log(probabilities), dim=-1)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.probability_key: probability}


class _SequencePredict(PredictModule):

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        probabilities = torch.softmax(logits, -1)
        predictions = torch.argmax(logits, -1)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.logits_key: logits}


TORCHSCRIPT_COMPATIBLE_TOKENIZERS = {'space', 'space_punct', 'comma', 'underscore', 'characters'}


class _SetPreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by SetFeatureMixin.add_feature_data.

    If is_bag is true, forward returns a vector for each sample indicating counts of each token. Else, forward returns a
    multi-hot vector for each sample indicating presence of each token.
    """

    def __init__(self, metadata: 'TrainingSetMetadataDict', is_bag: 'bool'=False):
        super().__init__()
        if metadata['preprocessing']['tokenizer'] not in TORCHSCRIPT_COMPATIBLE_TOKENIZERS:
            raise ValueError(f"{metadata['preprocessing']['tokenizer']} is not supported by torchscript. Please use one of {TORCHSCRIPT_COMPATIBLE_TOKENIZERS}.")
        self.lowercase = metadata['preprocessing']['lowercase']
        self.tokenizer = get_tokenizer_from_registry(metadata['preprocessing']['tokenizer'])()
        self.vocab_size = metadata['vocab_size']
        self.unknown_symbol = UNKNOWN_SYMBOL
        self.unit_to_id = metadata['str2idx']
        self.is_bag = is_bag

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        """Takes a list of strings and returns a tensor of counts for each token."""
        if not torch.jit.isinstance(v, List[str]):
            raise ValueError(f'Unsupported input: {v}')
        if self.lowercase:
            sequences = [sequence.lower() for sequence in v]
        else:
            sequences = v
        unit_sequences = self.tokenizer(sequences)
        assert torch.jit.isinstance(unit_sequences, List[List[str]]), 'unit_sequences is not a list of lists.'
        set_matrix = torch.zeros(len(unit_sequences), self.vocab_size, dtype=torch.float32)
        for sample_idx, unit_sequence in enumerate(unit_sequences):
            sequence_length = len(unit_sequence)
            for i in range(sequence_length):
                curr_unit = unit_sequence[i]
                if curr_unit in self.unit_to_id:
                    curr_id = self.unit_to_id[curr_unit]
                else:
                    curr_id = self.unit_to_id[self.unknown_symbol]
                if self.is_bag:
                    set_matrix[sample_idx][curr_id] += 1
                else:
                    set_matrix[sample_idx][curr_id] = 1
        return set_matrix


class _SetPostprocessing(torch.nn.Module):
    """Torchscript-enabled version of postprocessing done by SetFeatureMixin.add_feature_data."""

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.idx2str = {i: v for i, v in enumerate(metadata['idx2str'])}
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES
        self.unk = UNKNOWN_SYMBOL

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        inv_preds: 'List[List[str]]' = []
        filtered_probs: 'List[torch.Tensor]' = []
        for sample_idx, sample in enumerate(predictions):
            sample_preds: 'List[str]' = []
            pos_sample_idxs: 'List[int]' = []
            pos_class_idxs: 'List[int]' = []
            for class_idx, is_positive in enumerate(sample):
                if is_positive == 1:
                    sample_preds.append(self.idx2str.get(class_idx, self.unk))
                    pos_sample_idxs.append(sample_idx)
                    pos_class_idxs.append(class_idx)
            inv_preds.append(sample_preds)
            filtered_probs.append(probabilities[pos_sample_idxs, pos_class_idxs])
        return {self.predictions_key: inv_preds, self.probabilities_key: filtered_probs}


class _SetPredict(PredictModule):

    def __init__(self, threshold):
        super().__init__()
        self.threshold = threshold

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        probabilities = torch.sigmoid(logits)
        predictions = torch.greater_equal(probabilities, self.threshold)
        predictions = predictions.type(torch.int64)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.logits_key: logits}


class _TimeseriesPreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by TimeseriesFeatureMixin.add_feature_data."""

    def __init__(self, metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        if metadata['preprocessing']['tokenizer'] not in TORCHSCRIPT_COMPATIBLE_TOKENIZERS:
            raise ValueError(f"{metadata['preprocessing']['tokenizer']} is not supported by torchscript. Please use one of {TORCHSCRIPT_COMPATIBLE_TOKENIZERS}.")
        self.tokenizer = get_tokenizer_from_registry(metadata['preprocessing']['tokenizer'])()
        self.padding = metadata['preprocessing']['padding']
        self.padding_value = float(metadata['preprocessing']['padding_value'])
        self.max_timeseries_length = int(metadata['max_timeseries_length'])
        self.computed_fill_value = metadata['preprocessing']['computed_fill_value']

    def _process_str_sequence(self, sequence: 'List[str]', limit: 'int') ->torch.Tensor:
        float_sequence = [float(s) for s in sequence[:limit]]
        return torch.tensor(float_sequence)

    def _nan_to_fill_value(self, v: 'torch.Tensor') ->torch.Tensor:
        if v.isnan().any():
            tokenized_fill_value = self.tokenizer(self.computed_fill_value)
            assert torch.jit.isinstance(tokenized_fill_value, List[str])
            return self._process_str_sequence(tokenized_fill_value, self.max_timeseries_length)
        return v

    def forward_list_of_tensors(self, v: 'List[torch.Tensor]') ->torch.Tensor:
        v = [self._nan_to_fill_value(v_i) for v_i in v]
        if self.padding == 'right':
            timeseries_matrix = torch.nn.utils.rnn.pad_sequence(v, batch_first=True, padding_value=self.padding_value)
            timeseries_matrix = timeseries_matrix[:, :self.max_timeseries_length]
        else:
            reversed_timeseries = [torch.flip(v_i[:self.max_timeseries_length], dims=(0,)) for v_i in v]
            reversed_timeseries_padded = torch.nn.utils.rnn.pad_sequence(reversed_timeseries, batch_first=True, padding_value=self.padding_value)
            timeseries_matrix = torch.flip(reversed_timeseries_padded, dims=(1,))
        return timeseries_matrix

    def forward_list_of_strs(self, v: 'List[str]') ->torch.Tensor:
        v = [(self.computed_fill_value if s == 'nan' else s) for s in v]
        sequences = self.tokenizer(v)
        assert torch.jit.isinstance(sequences, List[List[str]]), 'sequences is not a list of lists.'
        timeseries_matrix = torch.full([len(sequences), self.max_timeseries_length], self.padding_value, dtype=torch.float32)
        for sample_idx, str_sequence in enumerate(sequences):
            limit = min(len(str_sequence), self.max_timeseries_length)
            float_sequence = self._process_str_sequence(str_sequence, limit)
            if self.padding == 'right':
                timeseries_matrix[sample_idx][:limit] = float_sequence
            else:
                timeseries_matrix[sample_idx][self.max_timeseries_length - limit:] = float_sequence
        return timeseries_matrix

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        """Takes a list of float values and creates a padded torch.Tensor."""
        if torch.jit.isinstance(v, List[torch.Tensor]):
            return self.forward_list_of_tensors(v)
        if torch.jit.isinstance(v, List[str]):
            return self.forward_list_of_strs(v)
        raise ValueError(f'Unsupported input: {v}')


class _VectorPreprocessing(torch.nn.Module):

    def forward(self, v: 'TorchscriptPreprocessingInput') ->torch.Tensor:
        if torch.jit.isinstance(v, torch.Tensor):
            out = v
        elif torch.jit.isinstance(v, List[torch.Tensor]):
            out = torch.stack(v)
        elif torch.jit.isinstance(v, List[str]):
            vectors = []
            for sample in v:
                vector = torch.tensor([float(x) for x in sample.split()], dtype=torch.float32)
                vectors.append(vector)
            out = torch.stack(vectors)
        else:
            raise ValueError(f'Unsupported input: {v}')
        if out.isnan().any():
            raise ValueError('Scripted NaN handling not implemented for Vector feature')
        return out


class _VectorPostprocessing(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.predictions_key = PREDICTIONS
        self.logits_key = LOGITS

    def forward(self, preds: 'Dict[str, torch.Tensor]', feature_name: 'str') ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        logits = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.logits_key)
        return {self.predictions_key: predictions, self.logits_key: logits}


class _VectorPredict(PredictModule):

    def forward(self, inputs: 'Dict[str, torch.Tensor]', feature_name: 'str') ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        return {self.predictions_key: logits, self.logits_key: logits}


class LudwigTorchWrapper(torch.nn.Module):
    """Base class that establishes the contract for exporting to different file formats."""

    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, x):
        return self.model({'image_path': x})


@DeveloperAPI
def get_torch_device():
    if torch.cuda.is_available():
        return 'cuda'
    if bool(os.environ.get('LUDWIG_ENABLE_MPS')):
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            if not bool(os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK')):
                warnings.warn('LUDWIG_ENABLE_MPS is set and MPS is available, but PYTORCH_ENABLE_MPS_FALLBACK has not been set. Depending on your model config, some operations may not be compatible. If errors occur, try setting `PYTORCH_ENABLE_MPS_FALLBACK=1` and resubmitting.')
            return 'mps'
        else:
            warnings.warn('LUDWIG_ENABLE_MPS is set but MPS is not available, falling back to CPU.')
    return 'cpu'


MODEL_HYPERPARAMETERS_FILE_NAME = 'model_hyperparameters.json'


POSTPROCESSOR = 'postprocessor'


PREDICTOR = 'predictor'


PREPROCESSOR = 'preprocessor'


TRAIN_SET_METADATA_FILE_NAME = 'training_set_metadata.json'


def get_filename_from_stage(stage: 'str', device: 'TorchDevice') ->str:
    """Returns the filename for a stage of inference."""
    if stage not in [PREPROCESSOR, PREDICTOR, POSTPROCESSOR]:
        raise ValueError(f'Invalid stage: {stage}.')
    if stage == PREDICTOR:
        return f'inference_{stage}-{device}.pt'
    else:
        return f'inference_{stage}.pt'


def _init_inference_stages_from_directory(directory: 'str', device: 'TorchDevice') ->Dict[str, torch.nn.Module]:
    """Initializes inference stage modules from directory."""
    stage_to_filenames = {stage: get_filename_from_stage(stage, device) for stage in [PREPROCESSOR, PREDICTOR, POSTPROCESSOR]}
    stage_to_module = {}
    for stage in [PREPROCESSOR, PREDICTOR, POSTPROCESSOR]:
        stage_to_module[stage] = torch.jit.load(os.path.join(directory, stage_to_filenames[stage]))
        None
    return stage_to_module


class _InferencePostprocessor(nn.Module):
    """Wraps postprocessing modules into a single nn.Module.

    The forward call of this module returns a flattened dictionary in order to support Triton input/output.

    TODO(geoffrey): Implement torchscript-compatible feature_utils.LudwigFeatureDict to replace
    get_module_dict_key_from_name and get_name_from_module_dict_key usage.
    """

    def __init__(self, model: "'BaseModel'", training_set_metadata: 'TrainingSetMetadataDict'):
        super().__init__()
        self.postproc_modules = nn.ModuleDict()
        for feature_name, feature in model.output_features.items():
            module_dict_key = get_module_dict_key_from_name(feature_name)
            self.postproc_modules[module_dict_key] = feature.create_postproc_module(training_set_metadata[feature_name])

    def forward(self, predictions_flattened: 'Dict[str, torch.Tensor]') ->Dict[str, Any]:
        postproc_outputs_flattened: 'Dict[str, Any]' = {}
        for module_dict_key, postproc in self.postproc_modules.items():
            feature_name = get_name_from_module_dict_key(module_dict_key)
            feature_postproc_outputs = postproc(predictions_flattened, feature_name)
            for postproc_key, tensor_values in feature_postproc_outputs.items():
                postproc_concat_key = output_feature_utils.get_feature_concat_name(feature_name, postproc_key)
                postproc_outputs_flattened[postproc_concat_key] = tensor_values
        return postproc_outputs_flattened


class _InferencePredictor(nn.Module):
    """Wraps model forward pass + predictions into a single nn.Module.

    The forward call of this module returns a flattened dictionary in order to support Triton input/output.

    TODO(geoffrey): Implement torchscript-compatible feature_utils.LudwigFeatureDict to replace
    get_module_dict_key_from_name and get_name_from_module_dict_key usage.
    """

    def __init__(self, model: "'BaseModel'", device: 'TorchDevice'):
        super().__init__()
        self.device = torch.device(device)
        self.model = model.to_torchscript(self.device)
        self.predict_modules = nn.ModuleDict()
        for feature_name, feature in model.output_features.items():
            module_dict_key = get_module_dict_key_from_name(feature_name)
            self.predict_modules[module_dict_key] = feature.prediction_module

    def forward(self, preproc_inputs: 'Dict[str, torch.Tensor]') ->Dict[str, torch.Tensor]:
        model_outputs = self.model(preproc_inputs)
        predictions_flattened: 'Dict[str, torch.Tensor]' = {}
        for module_dict_key, predict in self.predict_modules.items():
            feature_name = get_name_from_module_dict_key(module_dict_key)
            feature_predictions = predict(model_outputs, feature_name)
            for predict_key, tensor_values in feature_predictions.items():
                predict_concat_key = output_feature_utils.get_feature_concat_name(feature_name, predict_key)
                predictions_flattened[predict_concat_key] = tensor_values
        return predictions_flattened


ConfigT = TypeVar('ConfigT', bound='HFEncoderConfig')


LUDWIG_SCHEMA_VALIDATION_POLICY = 'LUDWIG_SCHEMA_VALIDATION_POLICY'


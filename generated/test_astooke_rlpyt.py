import sys
_module = sys.modules[__name__]
del sys
conf = _module
atari_dqn_async_cpu = _module
atari_dqn_async_gpu = _module
atari_dqn_async_serial = _module
example_1 = _module
example_2 = _module
example_3 = _module
example_4 = _module
example_5 = _module
example_6 = _module
example_6a = _module
example_7 = _module
rlpyt = _module
agents = _module
base = _module
dqn = _module
atari = _module
atari_catdqn_agent = _module
atari_dqn_agent = _module
atari_r2d1_agent = _module
mixin = _module
catdqn_agent = _module
dqn_agent = _module
epsilon_greedy = _module
r2d1_agent = _module
pg = _module
categorical = _module
gaussian = _module
mujoco = _module
qpg = _module
ddpg_agent = _module
sac_agent = _module
sac_v_agent = _module
td3_agent = _module
algos = _module
cat_dqn = _module
dqn = _module
r2d1 = _module
a2c = _module
base = _module
ppo = _module
ddpg = _module
sac = _module
sac_v = _module
td3 = _module
utils = _module
distributions = _module
base = _module
categorical = _module
discrete = _module
epsilon_greedy = _module
gaussian = _module
envs = _module
atari_env = _module
gym = _module
gym_schema = _module
experiments = _module
configs = _module
atari_dqn = _module
atari_dqn_debug = _module
atari_r2d1 = _module
atari_ff_a2c = _module
atari_ff_ppo = _module
atari_lstm_a2c = _module
atari_lstm_ppo = _module
mujoco_a2c = _module
mujoco_ppo = _module
mujoco_ddpg = _module
mujoco_sac = _module
mujoco_sac_v = _module
mujoco_td3 = _module
launch_atari_r2d1_async_alt = _module
launch_atari_r2d1_async_alt_seaquest = _module
launch_atari_r2d1_async_gpu = _module
launch_atari_dqn_cpu_basic_1of2 = _module
launch_atari_dqn_cpu_basic_2of2 = _module
launch_atari_dqn_gpu_pong = _module
launch_atari_r2d1_async_alt_amidar = _module
launch_atari_r2d1_async_alt_gravitar = _module
launch_atari_r2d1_async_alt_pong = _module
launch_atari_r2d1_async_gpu_amidar = _module
launch_atari_r2d1_async_gpu_test = _module
launch_atari_r2d1_long_4tr_gravitar = _module
launch_atari_catdqn_gpu_basic = _module
launch_atari_dpd_dqn_gpu_basic = _module
launch_atari_dqn_cpu_basic = _module
launch_atari_dqn_gpu = _module
launch_atari_dqn_gpu_basic = _module
launch_atari_ernbw_gpu_basic = _module
launch_atari_r2d1_gpu_basic = _module
launch_atari_dqn_async_cpu = _module
launch_atari_dqn_async_gpu = _module
launch_atari_dqn_async_gpu_scale_pri = _module
launch_atari_dqn_async_serial = _module
launch_atari_dqn_gpu_noeval = _module
launch_atari_dqn_serial = _module
launch_atari_r2d1_async_alt_chopper_command = _module
launch_atari_r2d1_async_alt_qbert = _module
launch_atari_r2d1_async_gpu_qbert = _module
launch_atari_r2d1_long_4tr_asteroids = _module
launch_atari_r2d1_long_4tr_chopper_command = _module
launch_atari_r2d1_long_4tr_seaquest = _module
launch_atari_r2d1_long_gt_ad = _module
launch_atari_r2d1_long_sq_cc = _module
atari_catdqn_gpu = _module
atari_dqn_cpu = _module
atari_dqn_gpu = _module
atari_dqn_gpu_noeval = _module
atari_dqn_serial = _module
atari_r2d1_async_alt = _module
atari_r2d1_async_gpu = _module
atari_r2d1_gpu = _module
launch_atari_ff_a2c_gpu_basic = _module
launch_atari_ff_a2c_gpu_low_lr = _module
launch_atari_ff_a2c_gpu_multi = _module
launch_atari_ff_lstm_a2c_ppo_gpu = _module
launch_atari_ff_ppo_gpu = _module
launch_atari_ff_ppo_gpu_basic = _module
launch_atari_lstm_a2c_gpu_basic = _module
launch_atari_lstm_ppo_gpu_basic = _module
launch_atari_ff_a2c_cpu = _module
launch_atari_lstm_a2c_cpu = _module
launch_atari_lstm_a2c_gpu = _module
atari_ff_a2c_cpu = _module
atari_ff_a2c_gpu = _module
atari_ff_a2c_gpu_multi = _module
atari_ff_ppo_gpu = _module
atari_lstm_a2c_cpu = _module
atari_lstm_a2c_gpu = _module
atari_lstm_ppo_gpu = _module
launch_mujoco_ppo_ddpg_td3_sac_serial = _module
launch_mujoco_sac_serial = _module
launch_mujoco_ppo_serial = _module
launch_mujoco_ppo_serial_hc = _module
launch_mujoco_a2c_cpu = _module
launch_mujoco_ppo_cpu = _module
launch_mujoco_ppo_gpu = _module
mujoco_ff_a2c_cpu = _module
mujoco_ff_ppo_cpu = _module
mujoco_ff_ppo_gpu = _module
mujoco_ff_ppo_serial = _module
launch_mujoco_ddpg_async_serial = _module
launch_mujoco_ddpg_serial = _module
launch_mujoco_ddpg_serial_bstl = _module
launch_mujoco_sac_async_gpu = _module
launch_mujoco_sac_serial_bstl = _module
launch_mujoco_sac_serial_hc = _module
launch_mujoco_td3_async_cpu = _module
launch_mujoco_td3_serial = _module
launch_mujoco_td3_serial_bstl = _module
launch_mujoco_td3_serial_hc = _module
launch_mujoco_ddpg_cpu = _module
launch_mujoco_ddpg_td3_sac_serial = _module
mujoco_ddpg_async_serial = _module
mujoco_ddpg_cpu = _module
mujoco_ddpg_serial = _module
mujoco_sac_async_gpu = _module
mujoco_sac_serial = _module
mujoco_td3_async_cpu = _module
mujoco_td3_serial = _module
models = _module
conv2d = _module
atari_catdqn_model = _module
atari_dqn_model = _module
atari_r2d1_model = _module
dueling = _module
mlp = _module
atari_ff_model = _module
atari_lstm_model = _module
mujoco_ff_model = _module
mujoco_lstm_model = _module
mlp = _module
running_mean_std = _module
utils = _module
projects = _module
safe = _module
cppo_agent = _module
cppo_model = _module
cppo_pid = _module
launch_cppo_main_point = _module
train_cppo = _module
safety_gym_env = _module
replays = _module
async_ = _module
frame = _module
n_step = _module
non_sequence = _module
n_step = _module
prioritized = _module
time_limit = _module
uniform = _module
sequence = _module
sum_tree = _module
runners = _module
async_rl = _module
minibatch_rl = _module
sync_rl = _module
samplers = _module
action_server = _module
alternating_sampler = _module
collectors = _module
cpu_sampler = _module
gpu_sampler = _module
serial_sampler = _module
buffer = _module
collections = _module
parallel = _module
cpu = _module
collectors = _module
sampler = _module
gpu = _module
sampler = _module
worker = _module
serial = _module
sampler = _module
spaces = _module
composite = _module
float_box = _module
gym_wrapper = _module
gym_wrapper_schema = _module
int_box = _module
ul = _module
atari_dqn_agent = _module
atari_pg_agent = _module
dmc_sac_agent = _module
dmlab_pg_agent = _module
rl_from_ul = _module
dqn_from_ul = _module
rad_sac_from_ul = _module
rl_with_ul = _module
dqn_with_ul = _module
ppo_with_ul = _module
sac_with_ul = _module
ul_for_rl = _module
augmented_temporal_contrast = _module
augmented_temporal_similarity = _module
base = _module
cpc = _module
inverse = _module
pixel_control = _module
replay_saver = _module
stdim = _module
vae = _module
data_augs = _module
warmup_scheduler = _module
weight_decay = _module
dmcontrol = _module
dmlab = _module
atari_dqn_from_ul = _module
atari_ppo_from_ul = _module
dmc_sac_from_ul = _module
dmlab_ppo_from_ul = _module
launch_atari_ppo_from_atc_1 = _module
launch_atari_ppo_from_atc_7game_1 = _module
launch_atari_ppo_from_ats_1 = _module
launch_atari_ppo_from_inv_first_1 = _module
launch_atari_ppo_from_pc_first_1 = _module
launch_atari_ppo_from_stdim_first_1 = _module
launch_atari_ppo_from_vae_first_1a = _module
launch_atari_ppo_from_vae_first_1b = _module
atari_ppo_from_ul_serial = _module
launch_dmc_sac_from_ac_1 = _module
launch_dmc_sac_from_atc_multi4_1 = _module
launch_dmc_sac_from_atc_multi4_1a = _module
launch_dmc_sac_from_atc_multi4_1b = _module
launch_dmc_sac_from_noload_multi4_1b = _module
launch_dmc_sac_from_vae_1 = _module
dmc_sac_from_ul_serial = _module
launch_dmlab_ppo_from_cpc_1 = _module
launch_dmlab_ppo_from_cpc_1a = _module
launch_dmlab_ppo_from_cpc_1b = _module
launch_dmlab_ppo_from_cpc_2 = _module
dmlab_ppo_from_ul_alt = _module
atari_dqn_ul = _module
atari_ppo_ul = _module
dmc_sac_with_ul = _module
dmlab_ppo_with_ul = _module
launch_atari_dqn_with_ul_schedule_1 = _module
launch_atari_ppo_with_ul_final_1 = _module
launch_atari_ppo_with_ul_final_1b = _module
launch_atari_ppo_with_ul_priority_1 = _module
launch_atari_ppo_with_ul_schedule_1 = _module
launch_atari_ppo_with_ul_schedule_2 = _module
launch_atari_ppo_with_ul_schedule_3 = _module
launch_atari_ppo_with_ul_schedule_4 = _module
launch_atari_ppo_with_ul_stpgrd_1 = _module
launch_atari_ppo_with_ul_warmup_1 = _module
atari_dqn_with_ul_serial = _module
atari_ppo_with_ul_serial = _module
atari_ppo_with_ul_serial_test = _module
launch_dmc_sac_with_ul_cheetah_1 = _module
launch_dmc_sac_with_ul_cheetah_2 = _module
launch_dmc_sac_with_ul_final_1 = _module
launch_dmc_sac_with_ul_priority_1 = _module
launch_dmc_sac_with_ul_randconv_1 = _module
launch_dmc_sac_with_ul_schedule_1 = _module
launch_dmc_sac_with_ul_sparse_1 = _module
launch_dmc_sac_with_ul_warmup_1 = _module
dmc_sac_with_ul_serial = _module
dmc_sac_with_ul_serial_test = _module
launch_dmlab_ppo_with_ul_explore_1 = _module
launch_dmlab_ppo_with_ul_exploresmall_1 = _module
launch_dmlab_ppo_with_ul_large_baseline_1 = _module
launch_dmlab_ppo_with_ul_large_baseline_2 = _module
launch_dmlab_ppo_with_ul_lasertag_1 = _module
launch_dmlab_ppo_with_ul_lasertagsmall_1 = _module
launch_dmlab_ppo_with_ul_priority_1 = _module
launch_dmlab_ppo_with_ul_priority_2 = _module
launch_dmlab_ppo_with_ul_priority_3 = _module
launch_dmlab_ppo_with_ul_priority_4 = _module
launch_dmlab_ppo_with_ul_schedule_1 = _module
dmlab_ppo_with_ul_alt = _module
atari_atc = _module
atari_ats = _module
atari_inverse = _module
atari_pc = _module
atari_stdim = _module
atari_vae = _module
dmc_atc = _module
dmc_vae = _module
dmlab_atc = _module
dmlab_cpc = _module
dmlab_pc = _module
launch_atari_atc_7game_1 = _module
launch_atari_atc_final_single = _module
launch_atari_ats_final_single = _module
launch_atari_inv_first_1 = _module
launch_atari_pc_first_1 = _module
launch_atari_stdim_first_1 = _module
launch_atari_vae_first_1 = _module
launch_atari_vae_first_1b = _module
atari_pixel_control = _module
launch_dmc_ac_pretrain_1 = _module
launch_dmc_atc_multi4_1 = _module
launch_dmc_atc_pretrain_1 = _module
launch_dmc_vae_pretrain_1 = _module
launch_dmlab_atc_pretrain_1 = _module
launch_dmlab_cpc_pretrain_1 = _module
launch_dmlab_cpc_pretrain_2 = _module
dmlab_conv2d = _module
rl = _module
atari_rl_models = _module
dmlab_rl_models = _module
sac_rl_models = _module
ul_models = _module
atc_models = _module
encoders = _module
inverse_models = _module
pixel_control_models = _module
stdim_models = _module
vae_models = _module
rl_with_ul_replay = _module
ul_for_rl_replay = _module
envstep_runner = _module
minibatch_rl_replaysaver = _module
unsupervised_learning = _module
array = _module
buffer = _module
collections = _module
launching = _module
affinity = _module
exp_launcher = _module
variant = _module
logging = _module
autoargs = _module
console = _module
context = _module
logger = _module
tabulate = _module
misc = _module
prog_bar = _module
quick_args = _module
seed = _module
shmemarray = _module
synchronize = _module
tensor = _module
setup = _module
tests = _module
test_gym_wrapper = _module
test_rlpyt = _module
test_serial_sampler = _module

from paritybench._paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from torch.nn.parallel import DistributedDataParallel as DDP


import numpy as np


from collections import namedtuple


import math


import torch.nn.functional as F


import torch.distributed as dist


from collections import deque


import time


import torch.distributed


import copy


from collections import OrderedDict


from torch.optim.lr_scheduler import CosineAnnealingLR


from torch.optim.lr_scheduler import _LRScheduler


import torch.nn as nn


from inspect import Signature as Sig


from inspect import Parameter as Param


import string


from enum import Enum


import queue


def conv2d_output_shape(h, w, kernel_size=1, stride=1, padding=0, dilation=1):
    """
    Returns output H, W after convolution/pooling on input H, W.
    """
    kh, kw = kernel_size if isinstance(kernel_size, tuple) else (kernel_size,) * 2
    sh, sw = stride if isinstance(stride, tuple) else (stride,) * 2
    ph, pw = padding if isinstance(padding, tuple) else (padding,) * 2
    d = dilation
    h = (h + 2 * ph - d * (kh - 1) - 1) // sh + 1
    w = (w + 2 * pw - d * (kw - 1) - 1) // sw + 1
    return h, w


class Conv2dModel(torch.nn.Module):
    """2-D Convolutional model component, with option for max-pooling vs
    downsampling for strides > 1.  Requires number of input channels, but
    not input shape.  Uses ``torch.nn.Conv2d``.
    """

    def __init__(self, in_channels, channels, kernel_sizes, strides, paddings=None, nonlinearity=torch.nn.ReLU, use_maxpool=False, head_sizes=None):
        super().__init__()
        if paddings is None:
            paddings = [(0) for _ in range(len(channels))]
        assert len(channels) == len(kernel_sizes) == len(strides) == len(paddings)
        in_channels = [in_channels] + channels[:-1]
        ones = [(1) for _ in range(len(strides))]
        if use_maxpool:
            maxp_strides = strides
            strides = ones
        else:
            maxp_strides = ones
        conv_layers = [torch.nn.Conv2d(in_channels=ic, out_channels=oc, kernel_size=k, stride=s, padding=p) for ic, oc, k, s, p in zip(in_channels, channels, kernel_sizes, strides, paddings)]
        sequence = list()
        for conv_layer, maxp_stride in zip(conv_layers, maxp_strides):
            sequence.extend([conv_layer, nonlinearity()])
            if maxp_stride > 1:
                sequence.append(torch.nn.MaxPool2d(maxp_stride))
        self.conv = torch.nn.Sequential(*sequence)

    def forward(self, input):
        """Computes the convolution stack on the input; assumes correct shape
        already: [B,C,H,W]."""
        return self.conv(input)

    def conv_out_size(self, h, w, c=None):
        """Helper function ot return the output size for a given input shape,
        without actually performing a forward pass through the model."""
        for child in self.conv.children():
            try:
                h, w = conv2d_output_shape(h, w, child.kernel_size, child.stride, child.padding)
            except AttributeError:
                pass
            try:
                c = child.out_channels
            except AttributeError:
                pass
        return h * w * c


class MlpModel(torch.nn.Module):
    """Multilayer Perceptron with last layer linear.

    Args:
        input_size (int): number of inputs
        hidden_sizes (list): can be empty list for none (linear model).
        output_size: linear layer at output, or if ``None``, the last hidden size will be the output size and will have nonlinearity applied
        nonlinearity: torch nonlinearity Module (not Functional).
    """

    def __init__(self, input_size, hidden_sizes, output_size=None, nonlinearity=torch.nn.ReLU):
        super().__init__()
        if isinstance(hidden_sizes, int):
            hidden_sizes = [hidden_sizes]
        elif hidden_sizes is None:
            hidden_sizes = []
        hidden_layers = [torch.nn.Linear(n_in, n_out) for n_in, n_out in zip([input_size] + hidden_sizes[:-1], hidden_sizes)]
        sequence = list()
        for layer in hidden_layers:
            sequence.extend([layer, nonlinearity()])
        if output_size is not None:
            last_size = hidden_sizes[-1] if hidden_sizes else input_size
            sequence.append(torch.nn.Linear(last_size, output_size))
        self.model = torch.nn.Sequential(*sequence)
        self._output_size = hidden_sizes[-1] if output_size is None else output_size

    def forward(self, input):
        """Compute the model on the input, assuming input shape [B,input_size]."""
        return self.model(input)

    @property
    def output_size(self):
        """Retuns the output size of the model."""
        return self._output_size


class Conv2dHeadModel(torch.nn.Module):
    """Model component composed of a ``Conv2dModel`` component followed by 
    a fully-connected ``MlpModel`` head.  Requires full input image shape to
    instantiate the MLP head.
    """

    def __init__(self, image_shape, channels, kernel_sizes, strides, hidden_sizes, output_size=None, paddings=None, nonlinearity=torch.nn.ReLU, use_maxpool=False):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels, kernel_sizes=kernel_sizes, strides=strides, paddings=paddings, nonlinearity=nonlinearity, use_maxpool=use_maxpool)
        conv_out_size = self.conv.conv_out_size(h, w)
        if hidden_sizes or output_size:
            self.head = MlpModel(conv_out_size, hidden_sizes, output_size=output_size, nonlinearity=nonlinearity)
            if output_size is not None:
                self._output_size = output_size
            else:
                self._output_size = hidden_sizes if isinstance(hidden_sizes, int) else hidden_sizes[-1]
        else:
            self.head = lambda x: x
            self._output_size = conv_out_size

    def forward(self, input):
        """Compute the convolution and fully connected head on the input;
        assumes correct input shape: [B,C,H,W]."""
        return self.head(self.conv(input).view(input.shape[0], -1))

    @property
    def output_size(self):
        """Returns the final output size after MLP head."""
        return self._output_size


class DistributionalHeadModel(torch.nn.Module):
    """An MLP head which reshapes output to [B, output_size, n_atoms]."""

    def __init__(self, input_size, layer_sizes, output_size, n_atoms):
        super().__init__()
        self.mlp = MlpModel(input_size, layer_sizes, output_size * n_atoms)
        self._output_size = output_size
        self._n_atoms = n_atoms

    def forward(self, input):
        return self.mlp(input).view(-1, self._output_size, self._n_atoms)


class ScaleGrad(torch.autograd.Function):
    """Model component to scale gradients back from layer, without affecting
    the forward pass.  Used e.g. in dueling heads DQN models."""

    @staticmethod
    def forward(ctx, tensor, scale):
        """Stores the ``scale`` input to ``ctx`` for application in
        ``backward()``; simply returns the input ``tensor``."""
        ctx.scale = scale
        return tensor

    @staticmethod
    def backward(ctx, grad_output):
        """Return the ``grad_output`` multiplied by ``ctx.scale``.  Also returns
        a ``None`` as placeholder corresponding to (non-existent) gradient of 
        the input ``scale`` of ``forward()``."""
        return grad_output * ctx.scale, None


scale_grad = getattr(ScaleGrad, 'apply', None)


class DistributionalDuelingHeadModel(torch.nn.Module):
    """Model component for Dueling Distributional (Categorical) DQN, like
    ``DuelingHeadModel``, but handles `n_atoms` outputs for each state-action
    Q-value distribution.
    """

    def __init__(self, input_size, hidden_sizes, output_size, n_atoms, grad_scale=2 ** (-1 / 2)):
        super().__init__()
        if isinstance(hidden_sizes, int):
            hidden_sizes = [hidden_sizes]
        self.advantage_hidden = MlpModel(input_size, hidden_sizes)
        self.advantage_out = torch.nn.Linear(hidden_sizes[-1], output_size * n_atoms, bias=False)
        self.advantage_bias = torch.nn.Parameter(torch.zeros(n_atoms))
        self.value = MlpModel(input_size, hidden_sizes, output_size=n_atoms)
        self._grad_scale = grad_scale
        self._output_size = output_size
        self._n_atoms = n_atoms

    def forward(self, input):
        x = scale_grad(input, self._grad_scale)
        advantage = self.advantage(x)
        value = self.value(x).view(-1, 1, self._n_atoms)
        return value + (advantage - advantage.mean(dim=1, keepdim=True))

    def advantage(self, input):
        x = self.advantage_hidden(input)
        x = self.advantage_out(x)
        x = x.view(-1, self._output_size, self._n_atoms)
        return x + self.advantage_bias


def infer_leading_dims(tensor, dim):
    """Looks for up to two leading dimensions in ``tensor``, before
    the data dimensions, of which there are assumed to be ``dim`` number.
    For use at beginning of model's ``forward()`` method, which should 
    finish with ``restore_leading_dims()`` (see that function for help.)
    Returns:
    lead_dim: int --number of leading dims found.
    T: int --size of first leading dim, if two leading dims, o/w 1.
    B: int --size of first leading dim if one, second leading dim if two, o/w 1.
    shape: tensor shape after leading dims.
    """
    lead_dim = tensor.dim() - dim
    assert lead_dim in (0, 1, 2)
    if lead_dim == 2:
        T, B = tensor.shape[:2]
    else:
        T = 1
        B = 1 if lead_dim == 0 else tensor.shape[0]
    shape = tensor.shape[lead_dim:]
    return lead_dim, T, B, shape


def restore_leading_dims(tensors, lead_dim, T=1, B=1):
    """Reshapes ``tensors`` (one or `tuple`, `list`) to to have ``lead_dim``
    leading dimensions, which will become [], [B], or [T,B].  Assumes input
    tensors already have a leading Batch dimension, which might need to be
    removed. (Typically the last layer of model will compute with leading
    batch dimension.)  For use in model ``forward()`` method, so that output
    dimensions match input dimensions, and the same model can be used for any
    such case.  Use with outputs from ``infer_leading_dims()``."""
    is_seq = isinstance(tensors, (tuple, list))
    tensors = tensors if is_seq else (tensors,)
    if lead_dim == 2:
        tensors = tuple(t.view((T, B) + t.shape[1:]) for t in tensors)
    if lead_dim == 0:
        assert B == 1
        tensors = tuple(t.squeeze(0) for t in tensors)
    return tensors if is_seq else tensors[0]


class AtariCatDqnModel(torch.nn.Module):
    """2D conlutional network feeding into MLP with ``n_atoms`` outputs
    per action, representing a discrete probability distribution of Q-values."""

    def __init__(self, image_shape, output_size, n_atoms=51, fc_sizes=512, dueling=False, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiates the neural network according to arguments; network defaults
        stored within this method."""
        super().__init__()
        self.dueling = dueling
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings or [0, 1, 1], use_maxpool=use_maxpool)
        conv_out_size = self.conv.conv_out_size(h, w)
        if dueling:
            self.head = DistributionalDuelingHeadModel(conv_out_size, fc_sizes, output_size=output_size, n_atoms=n_atoms)
        else:
            self.head = DistributionalHeadModel(conv_out_size, fc_sizes, output_size=output_size, n_atoms=n_atoms)

    def forward(self, observation, prev_action, prev_reward):
        """Returns the probability masses ``num_atoms x num_actions`` for the Q-values
        for each state/observation, using softmax output nonlinearity."""
        img = observation.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv_out = self.conv(img.view(T * B, *img_shape))
        p = self.head(conv_out.view(T * B, -1))
        p = F.softmax(p, dim=-1)
        p = restore_leading_dims(p, lead_dim, T, B)
        return p


class RunningMeanStdModel(torch.nn.Module):
    """Adapted from OpenAI baselines.  Maintains a running estimate of mean
    and variance of data along each dimension, accessible in the `mean` and
    `var` attributes.  Supports multi-GPU training by all-reducing statistics
    across GPUs."""

    def __init__(self, shape):
        super().__init__()
        self.register_buffer('mean', torch.zeros(shape))
        self.register_buffer('var', torch.ones(shape))
        self.register_buffer('count', torch.zeros(()))
        self.shape = shape

    def update(self, x):
        _, T, B, _ = infer_leading_dims(x, len(self.shape))
        x = x.view(T * B, *self.shape)
        batch_mean = x.mean(dim=0)
        batch_var = x.var(dim=0, unbiased=False)
        batch_count = T * B
        if dist.is_initialized():
            mean_var = torch.stack([batch_mean, batch_var])
            dist.all_reduce(mean_var)
            world_size = dist.get_world_size()
            mean_var /= world_size
            batch_count *= world_size
            batch_mean, batch_var = mean_var[0], mean_var[1]
        if self.count == 0:
            self.mean[:] = batch_mean
            self.var[:] = batch_var
        else:
            delta = batch_mean - self.mean
            total = self.count + batch_count
            self.mean[:] = self.mean + delta * batch_count / total
            m_a = self.var * self.count
            m_b = batch_var * batch_count
            M2 = m_a + m_b + delta ** 2 * self.count * batch_count / total
            self.var[:] = M2 / total
        self.count += batch_count


def weight_init(m):
    """Kaiming_normal is standard for relu networks, sometimes."""
    if isinstance(m, (torch.nn.Linear, torch.nn.Conv2d)):
        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
        torch.nn.init.zeros_(m.bias)


class AtariDqnModel(torch.nn.Module):
    """Can feed in conv and/or fc1 layer from pre-trained model, or have it
    initialize new ones (if initializing new, must provide image_shape)."""

    def __init__(self, image_shape, action_size, hidden_sizes=512, stop_conv_grad=False, channels=None, kernel_sizes=None, strides=None, paddings=None, kiaming_init=True, normalize_conv_out=False):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings)
        self._conv_out_size = self.conv.conv_out_size(h=h, w=w)
        self.q_mlp = MlpModel(input_size=self._conv_out_size, hidden_sizes=hidden_sizes, output_size=action_size)
        if kiaming_init:
            self.apply(weight_init)
        self.stop_conv_grad = stop_conv_grad
        logger.log('Model stopping gradient at CONV.' if stop_conv_grad else 'Modeul using gradients on all parameters.')
        if normalize_conv_out:
            logger.log('Model normalizing conv output across all pixels.')
            self.conv_rms = RunningMeanStdModel((1,))
            self.var_clip = 1e-06
        self.normalize_conv_out = normalize_conv_out

    def forward(self, observation, prev_action, prev_reward):
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv = self.conv(img.view(T * B, *img_shape))
        if self.stop_conv_grad:
            conv = conv.detach()
        if self.normalize_conv_out:
            conv_var = self.conv_rms.var
            conv_var = torch.clamp(conv_var, min=self.var_clip)
            conv = torch.clamp(0.29 * conv / conv_var.sqrt(), 0, 10)
        q = self.q_mlp(conv.view(T * B, -1))
        q, conv = restore_leading_dims((q, conv), lead_dim, T, B)
        return q, conv

    def update_conv_rms(self, observation):
        if self.normalize_conv_out:
            with torch.no_grad():
                if observation.dtype == torch.uint8:
                    img = observation.type(torch.float)
                    img = img.mul_(1.0 / 255)
                else:
                    img = observation
                lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
                conv = self.conv(img.view(T * B, *img_shape))
                self.conv_rms.update(conv.view(-1, 1))

    def parameters(self):
        if not self.stop_conv_grad:
            yield from self.conv.parameters()
        yield from self.q_mlp.parameters()

    def named_parameters(self):
        if not self.stop_conv_grad:
            yield from self.conv.named_parameters()
        yield from self.q_mlp.named_parameters()

    @property
    def conv_out_size(self):
        return self._conv_out_size


class DuelingHeadModel(torch.nn.Module):
    """Model component for dueling DQN.  For each state Q-value, uses a scalar
    output for mean (bias), and vector output for relative advantages
    associated with each action, so the Q-values are computed as: Mean +
    (Advantages - mean(Advantages)).  Uses a shared bias for all Advantage
    outputs.Gradient scaling can be applied, affecting preceding layers in the
    backward pass.
    """

    def __init__(self, input_size, hidden_sizes, output_size, grad_scale=2 ** (-1 / 2)):
        super().__init__()
        if isinstance(hidden_sizes, int):
            hidden_sizes = [hidden_sizes]
        self.advantage_hidden = MlpModel(input_size, hidden_sizes)
        self.advantage_out = torch.nn.Linear(hidden_sizes[-1], output_size, bias=False)
        self.advantage_bias = torch.nn.Parameter(torch.zeros(1))
        self.value = MlpModel(input_size, hidden_sizes, output_size=1)
        self._grad_scale = grad_scale

    def forward(self, input):
        """Computes Q-values through value and advantage heads; applies gradient
        scaling."""
        x = scale_grad(input, self._grad_scale)
        advantage = self.advantage(x)
        value = self.value(x)
        return value + (advantage - advantage.mean(dim=-1, keepdim=True))

    def advantage(self, input):
        """Computes shared-bias advantages."""
        x = self.advantage_hidden(input)
        return self.advantage_out(x) + self.advantage_bias


RESERVED_NAMES = 'get', 'items'


def tuple_itemgetter(i):

    def _tuple_itemgetter(obj):
        return tuple.__getitem__(obj, i)
    return _tuple_itemgetter


def namedarraytuple(typename, field_names, return_namedtuple_cls=False, classname_suffix=False):
    """
    Returns a new subclass of a namedtuple which exposes indexing / slicing
    reads and writes applied to all contained objects, which must share
    indexing (__getitem__) behavior (e.g. numpy arrays or torch tensors).

    (Code follows pattern of collections.namedtuple.)

    >>> PointsCls = namedarraytuple('Points', ['x', 'y'])
    >>> p = PointsCls(np.array([0, 1]), y=np.array([10, 11]))
    >>> p
    Points(x=array([0, 1]), y=array([10, 11]))
    >>> p.x                         # fields accessible by name
    array([0, 1])
    >>> p[0]                        # get location across all fields
    Points(x=0, y=10)               # (location can be index or slice)
    >>> p.get(0)                    # regular tuple-indexing into field
    array([0, 1])
    >>> x, y = p                    # unpack like a regular tuple
    >>> x
    array([0, 1])
    >>> p[1] = 2                    # assign value to location of all fields
    >>> p
    Points(x=array([0, 2]), y=array([10, 2]))
    >>> p[1] = PointsCls(3, 30)     # assign to location field-by-field
    >>> p
    Points(x=array([0, 3]), y=array([10, 30]))
    >>> 'x' in p                    # check field name instead of object
    True
    """
    nt_typename = typename
    if classname_suffix:
        nt_typename += '_nt'
        typename += '_nat'
    try:
        module = sys._getframe(1).f_globals.get('__name__', '__main__')
    except (AttributeError, ValueError):
        module = None
    NtCls = namedtuple(nt_typename, field_names, module=module)

    def __getitem__(self, loc):
        try:
            return type(self)(*(None if s is None else s[loc] for s in self))
        except IndexError as e:
            for j, s in enumerate(self):
                if s is None:
                    continue
                try:
                    _ = s[loc]
                except IndexError:
                    raise Exception(f"Occured in {self.__class__} at field '{self._fields[j]}'.") from e
    __getitem__.__doc__ = f'Return a new {typename} instance containing the selected index or slice from each field.'

    def __setitem__(self, loc, value):
        """
        If input value is the same named[array]tuple type, iterate through its
        fields and assign values into selected index or slice of corresponding
        field.  Else, assign whole of value to selected index or slice of
        all fields.  Ignore fields that are both None.
        """
        if not (isinstance(value, tuple) and getattr(value, '_fields', None) == self._fields):
            value = tuple(None if s is None else value for s in self)
        try:
            for j, (s, v) in enumerate(zip(self, value)):
                if s is not None or v is not None:
                    s[loc] = v
        except (ValueError, IndexError, TypeError) as e:
            raise Exception(f"Occured in {self.__class__} at field '{self._fields[j]}'.") from e

    def __contains__(self, key):
        """Checks presence of field name (unlike tuple; like dict)."""
        return key in self._fields

    def get(self, index):
        """Retrieve value as if indexing into regular tuple."""
        return tuple.__getitem__(self, index)

    def items(self):
        """Iterate ordered (field_name, value) pairs (like OrderedDict)."""
        for k, v in zip(self._fields, self):
            yield k, v
    for method in (__getitem__, __setitem__, get, items):
        method.__qualname__ = f'{typename}.{method.__name__}'
    arg_list = repr(NtCls._fields).replace("'", '')[1:-1]
    class_namespace = {'__doc__': f'{typename}({arg_list})', '__slots__': (), '__getitem__': __getitem__, '__setitem__': __setitem__, '__contains__': __contains__, 'get': get, 'items': items}
    for index, name in enumerate(NtCls._fields):
        if name in RESERVED_NAMES:
            raise ValueError(f'Disallowed field name: {name}.')
        itemgetter_object = tuple_itemgetter(index)
        doc = f'Alias for field number {index}'
        class_namespace[name] = property(itemgetter_object, doc=doc)
    result = type(typename, (NtCls,), class_namespace)
    result.__module__ = NtCls.__module__
    if return_namedtuple_cls:
        return result, NtCls
    return result


RnnState = namedarraytuple('RnnState', ['h', 'c'])


class AtariR2d1Model(torch.nn.Module):
    """2D convolutional neural network (for multiple video frames per
    observation) feeding into an LSTM and MLP output for Q-value outputs for
    the action set."""

    def __init__(self, image_shape, output_size, fc_size=512, lstm_size=512, head_size=512, dueling=False, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiates the neural network according to arguments; network defaults
        stored within this method."""
        super().__init__()
        self.dueling = dueling
        self.conv = Conv2dHeadModel(image_shape=image_shape, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings or [0, 1, 1], use_maxpool=use_maxpool, hidden_sizes=fc_size)
        self.lstm = torch.nn.LSTM(self.conv.output_size + output_size + 1, lstm_size)
        if dueling:
            self.head = DuelingHeadModel(lstm_size, head_size, output_size)
        else:
            self.head = MlpModel(lstm_size, head_size, output_size=output_size)

    def forward(self, observation, prev_action, prev_reward, init_rnn_state):
        """Feedforward layers process as [T*B,H]. Return same leading dims as
        input, can be [T,B], [B], or []."""
        img = observation.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv_out = self.conv(img.view(T * B, *img_shape))
        lstm_input = torch.cat([conv_out.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        q = self.head(lstm_out.view(T * B, -1))
        q = restore_leading_dims(q, lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return q, next_rnn_state


class AtariFfModel(torch.nn.Module):
    """
    Feedforward model for Atari agents: a convolutional network feeding an
    MLP with outputs for action probabilities and state-value estimate.
    """

    def __init__(self, image_shape, output_size, fc_sizes=512, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiate neural net module according to inputs."""
        super().__init__()
        self.conv = Conv2dHeadModel(image_shape=image_shape, channels=channels or [16, 32], kernel_sizes=kernel_sizes or [8, 4], strides=strides or [4, 2], paddings=paddings or [0, 1], use_maxpool=use_maxpool, hidden_sizes=fc_sizes)
        self.pi = torch.nn.Linear(self.conv.output_size, output_size)
        self.value = torch.nn.Linear(self.conv.output_size, 1)

    def forward(self, image, prev_action, prev_reward):
        """
        Compute action probabilities and value estimate from input state.
        Infers leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Convolution layers process as [T*B,
        *image_shape], with T=1,B=1 when not given.  Expects uint8 images in
        [0,255] and converts them to float32 in [0,1] (to minimize image data
        storage and transfer).  Used in both sampler and in algorithm (both
        via the agent).
        """
        img = image.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        fc_out = self.conv(img.view(T * B, *img_shape))
        pi = F.softmax(self.pi(fc_out), dim=-1)
        v = self.value(fc_out).squeeze(-1)
        pi, v = restore_leading_dims((pi, v), lead_dim, T, B)
        return pi, v


class AtariLstmModel(torch.nn.Module):
    """Recurrent model for Atari agents: a convolutional network into an FC layer
    into an LSTM which outputs action probabilities and state-value estimate.
    """

    def __init__(self, image_shape, output_size, fc_sizes=512, lstm_size=512, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiate neural net module according to inputs."""
        super().__init__()
        self.conv = Conv2dHeadModel(image_shape=image_shape, channels=channels or [16, 32], kernel_sizes=kernel_sizes or [8, 4], strides=strides or [4, 2], paddings=paddings or [0, 1], use_maxpool=use_maxpool, hidden_sizes=fc_sizes)
        self.lstm = torch.nn.LSTM(self.conv.output_size + output_size + 1, lstm_size)
        self.pi = torch.nn.Linear(lstm_size, output_size)
        self.value = torch.nn.Linear(lstm_size, 1)

    def forward(self, image, prev_action, prev_reward, init_rnn_state):
        """
        Compute action probabilities and value estimate from input state.
        Infers leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Convolution layers process as [T*B,
        *image_shape], with T=1,B=1 when not given.  Expects uint8 images in
        [0,255] and converts them to float32 in [0,1] (to minimize image data
        storage and transfer).  Recurrent layers processed as [T,B,H]. Used in
        both sampler and in algorithm (both via the agent).  Also returns the
        next RNN state.
        """
        img = image.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        fc_out = self.conv(img.view(T * B, *img_shape))
        lstm_input = torch.cat([fc_out.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        pi = F.softmax(self.pi(lstm_out.view(T * B, -1)), dim=-1)
        v = self.value(lstm_out.view(T * B, -1)).squeeze(-1)
        pi, v = restore_leading_dims((pi, v), lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return pi, v, next_rnn_state


class MujocoFfModel(torch.nn.Module):
    """
    Model commonly used in Mujoco locomotion agents: an MLP which outputs
    distribution means, separate parameter for learned log_std, and separate
    MLP for state-value estimate.
    """

    def __init__(self, observation_shape, action_size, hidden_sizes=None, hidden_nonlinearity=torch.nn.Tanh, mu_nonlinearity=torch.nn.Tanh, init_log_std=0.0, normalize_observation=False, norm_obs_clip=10, norm_obs_var_clip=1e-06):
        """Instantiate neural net modules according to inputs."""
        super().__init__()
        self._obs_ndim = len(observation_shape)
        input_size = int(np.prod(observation_shape))
        hidden_sizes = hidden_sizes or [64, 64]
        mu_mlp = MlpModel(input_size=input_size, hidden_sizes=hidden_sizes, output_size=action_size, nonlinearity=hidden_nonlinearity)
        if mu_nonlinearity is not None:
            self.mu = torch.nn.Sequential(mu_mlp, mu_nonlinearity())
        else:
            self.mu = mu_mlp
        self.v = MlpModel(input_size=input_size, hidden_sizes=hidden_sizes, output_size=1, nonlinearity=hidden_nonlinearity)
        self.log_std = torch.nn.Parameter(init_log_std * torch.ones(action_size))
        if normalize_observation:
            self.obs_rms = RunningMeanStdModel(observation_shape)
            self.norm_obs_clip = norm_obs_clip
            self.norm_obs_var_clip = norm_obs_var_clip
        self.normalize_observation = normalize_observation

    def forward(self, observation, prev_action, prev_reward):
        """
        Compute mean, log_std, and value estimate from input state. Infers
        leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Intermediate feedforward layers
        process as [T*B,H], with T=1,B=1 when not given. Used both in sampler
        and in algorithm (both via the agent).
        """
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        if self.normalize_observation:
            obs_var = self.obs_rms.var
            if self.norm_obs_var_clip is not None:
                obs_var = torch.clamp(obs_var, min=self.norm_obs_var_clip)
            observation = torch.clamp((observation - self.obs_rms.mean) / obs_var.sqrt(), -self.norm_obs_clip, self.norm_obs_clip)
        obs_flat = observation.view(T * B, -1)
        mu = self.mu(obs_flat)
        v = self.v(obs_flat).squeeze(-1)
        log_std = self.log_std.repeat(T * B, 1)
        mu, log_std, v = restore_leading_dims((mu, log_std, v), lead_dim, T, B)
        return mu, log_std, v

    def update_obs_rms(self, observation):
        if self.normalize_observation:
            self.obs_rms.update(observation)


class MujocoLstmModel(torch.nn.Module):
    """
    Recurrent model for Mujoco locomotion agents: an MLP into an LSTM which
    outputs distribution means, log_std, and state-value estimate.
    """

    def __init__(self, observation_shape, action_size, hidden_sizes=None, lstm_size=256, nonlinearity=torch.nn.ReLU, normalize_observation=False, norm_obs_clip=10, norm_obs_var_clip=1e-06):
        super().__init__()
        self._obs_n_dim = len(observation_shape)
        self._action_size = action_size
        hidden_sizes = hidden_sizes or [256, 256]
        mlp_input_size = int(np.prod(observation_shape))
        self.mlp = MlpModel(input_size=mlp_input_size, hidden_sizes=hidden_sizes, output_size=None, nonlinearity=nonlinearity)
        mlp_output_size = hidden_sizes[-1] if hidden_sizes else mlp_input_size
        self.lstm = torch.nn.LSTM(mlp_output_size + action_size + 1, lstm_size)
        self.head = torch.nn.Linear(lstm_size, action_size * 2 + 1)
        if normalize_observation:
            self.obs_rms = RunningMeanStdModel(observation_shape)
            self.norm_obs_clip = norm_obs_clip
            self.norm_obs_var_clip = norm_obs_var_clip
        self.normalize_observation = normalize_observation

    def forward(self, observation, prev_action, prev_reward, init_rnn_state):
        """
        Compute mean, log_std, and value estimate from input state. Infer
        leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Intermediate feedforward layers
        process as [T*B,H], and recurrent layers as [T,B,H], with T=1,B=1 when
        not given. Used both in sampler and in algorithm (both via the agent).
        Also returns the next RNN state.
        """
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_n_dim)
        if self.normalize_observation:
            obs_var = self.obs_rms.var
            if self.norm_obs_var_clip is not None:
                obs_var = torch.clamp(obs_var, min=self.norm_obs_var_clip)
            observation = torch.clamp((observation - self.obs_rms.mean) / obs_var.sqrt(), -self.norm_obs_clip, self.norm_obs_clip)
        mlp_out = self.mlp(observation.view(T * B, -1))
        lstm_input = torch.cat([mlp_out.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        outputs = self.head(lstm_out.view(T * B, -1))
        mu = outputs[:, :self._action_size]
        log_std = outputs[:, self._action_size:-1]
        v = outputs[:, -1]
        mu, log_std, v = restore_leading_dims((mu, log_std, v), lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return mu, log_std, v, next_rnn_state

    def update_obs_rms(self, observation):
        if self.normalize_observation:
            self.obs_rms.update(observation)


class MuMlpModel(torch.nn.Module):
    """MLP neural net for action mean (mu) output for DDPG agent."""

    def __init__(self, observation_shape, hidden_sizes, action_size, output_max=1):
        """Instantiate neural net according to inputs."""
        super().__init__()
        self._output_max = output_max
        self._obs_ndim = len(observation_shape)
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)), hidden_sizes=hidden_sizes, output_size=action_size)

    def forward(self, observation, prev_action, prev_reward):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        mu = self._output_max * torch.tanh(self.mlp(observation.view(T * B, -1)))
        mu = restore_leading_dims(mu, lead_dim, T, B)
        return mu


class PiMlpModel(torch.nn.Module):
    """Action distrubition MLP model for SAC agent."""

    def __init__(self, observation_shape, hidden_sizes, action_size):
        super().__init__()
        self._obs_ndim = len(observation_shape)
        self._action_size = action_size
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)), hidden_sizes=hidden_sizes, output_size=action_size * 2)

    def forward(self, observation, prev_action, prev_reward):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        output = self.mlp(observation.view(T * B, -1))
        mu, log_std = output[:, :self._action_size], output[:, self._action_size:]
        mu, log_std = restore_leading_dims((mu, log_std), lead_dim, T, B)
        return mu, log_std


class QofMuMlpModel(torch.nn.Module):
    """Q portion of the model for DDPG, an MLP."""

    def __init__(self, observation_shape, hidden_sizes, action_size):
        """Instantiate neural net according to inputs."""
        super().__init__()
        self._obs_ndim = len(observation_shape)
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)) + action_size, hidden_sizes=hidden_sizes, output_size=1)

    def forward(self, observation, prev_action, prev_reward, action):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        q_input = torch.cat([observation.view(T * B, -1), action.view(T * B, -1)], dim=1)
        q = self.mlp(q_input).squeeze(-1)
        q = restore_leading_dims(q, lead_dim, T, B)
        return q


class VMlpModel(torch.nn.Module):

    def __init__(self, observation_shape, hidden_sizes, action_size=None):
        super().__init__()
        self._obs_ndim = len(observation_shape)
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)), hidden_sizes=hidden_sizes, output_size=1)

    def forward(self, observation, prev_action, prev_reward):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        v = self.mlp(observation.view(T * B, -1)).squeeze(-1)
        v = restore_leading_dims(v, lead_dim, T, B)
        return v


ValueInfo = namedarraytuple('ValueInfo', ['value', 'c_value'])


class CppoModel(torch.nn.Module):

    def __init__(self, observation_shape, action_size, hidden_sizes=None, lstm_size=None, lstm_skip=True, constraint=True, hidden_nonlinearity='tanh', mu_nonlinearity='tanh', init_log_std=0.0, normalize_observation=True, var_clip=1e-06):
        super().__init__()
        if hidden_nonlinearity == 'tanh':
            hidden_nonlinearity = torch.nn.Tanh
        elif hidden_nonlinearity == 'relu':
            hidden_nonlinearity = torch.nn.ReLU
        else:
            raise ValueError(f'Unrecognized hidden_nonlinearity string: {hidden_nonlinearity}')
        if mu_nonlinearity == 'tanh':
            mu_nonlinearity = torch.nn.Tanh
        elif mu_nonlinearity == 'relu':
            mu_nonlinearity = torch.nn.ReLU
        else:
            raise ValueError(f'Unrecognized mu_nonlinearity string: {mu_nonlinearity}')
        self._obs_ndim = len(observation_shape)
        input_size = int(np.prod(observation_shape))
        self.body = MlpModel(input_size=input_size, hidden_sizes=hidden_sizes or [256, 256], nonlinearity=hidden_nonlinearity)
        last_size = self.body.output_size
        if lstm_size:
            lstm_input_size = last_size + action_size + 1
            self.lstm = torch.nn.LSTM(lstm_input_size, lstm_size)
            last_size = lstm_size
        else:
            self.lstm = None
        mu_linear = torch.nn.Linear(last_size, action_size)
        if mu_nonlinearity is not None:
            self.mu = torch.nn.Sequential(mu_linear, mu_nonlinearity())
        else:
            self.mu = mu_linear
        self.value = torch.nn.Linear(last_size, 1)
        if constraint:
            self.constraint = torch.nn.Linear(last_size, 1)
        else:
            self.constraint = None
        self.log_std = torch.nn.Parameter(init_log_std * torch.ones(action_size))
        self._lstm_skip = lstm_skip
        if normalize_observation:
            self.obs_rms = RunningMeanStdModel(observation_shape)
            self.var_clip = var_clip
        self.normalize_observation = normalize_observation

    def forward(self, observation, prev_action, prev_reward, init_rnn_state=None):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        if self.normalize_observation:
            obs_var = self.obs_rms.var
            if self.var_clip is not None:
                obs_var = torch.clamp(obs_var, min=self.var_clip)
            observation = torch.clamp((observation - self.obs_rms.mean) / obs_var.sqrt(), -10, 10)
        fc_x = self.body(observation.view(T * B, -1))
        if self.lstm is not None:
            lstm_inputs = [fc_x, prev_action, prev_reward]
            lstm_input = torch.cat([x.view(T, B, -1) for x in lstm_inputs], dim=2)
            init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
            lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
            lstm_out = lstm_out.view(T * B, -1)
            if self._lstm_skip:
                fc_x = fc_x + lstm_out
            else:
                fc_x = lstm_out
        mu = self.mu(fc_x)
        log_std = self.log_std.repeat(T * B, 1)
        v = self.value(fc_x).squeeze(-1)
        mu, log_std, v = restore_leading_dims((mu, log_std, v), lead_dim, T, B)
        if self.constraint is None:
            value = ValueInfo(value=v, c_value=None)
        else:
            c = self.constraint(fc_x).squeeze(-1)
            c = restore_leading_dims(c, lead_dim, T, B)
            value = ValueInfo(value=v, c_value=c)
        outputs = mu, log_std, value
        if self.lstm is not None:
            outputs += RnnState(h=hn, c=cn),
        return outputs

    def update_obs_rms(self, observation):
        if not self.normalize_observation:
            return
        self.obs_rms.update(observation)


class DmlabConv2dModel(torch.nn.Module):

    def __init__(self, in_channels, use_fourth_layer=True, skip_connections=True, use_maxpool=False):
        super().__init__()
        self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=1 if use_maxpool else 4, padding=2 if use_maxpool else 0)
        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=4, stride=4) if use_maxpool else None
        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=1 if use_maxpool else 2, padding=1 if use_maxpool else 0)
        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) if use_maxpool else None
        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)
        if use_fourth_layer:
            self.conv4 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)
        else:
            self.conv4 = None
        self.skip_connections = skip_connections

    def forward(self, input):
        conv1 = F.relu(self.conv1(input))
        if self.maxpool1 is not None:
            conv1 = self.maxpool1(conv1)
        conv2 = F.relu(self.conv2(conv1))
        if self.maxpool2 is not None:
            conv2 = self.maxpool2(conv2)
        conv3_pre = self.conv3(conv2)
        if self.skip_connections:
            conv3_pre = conv3_pre + conv2
        conv3 = F.relu(conv3_pre)
        if self.conv4 is None:
            return conv3
        conv4_pre = self.conv4(conv3)
        if self.skip_connections:
            conv4_pre = conv4_pre + conv3_pre
        conv4 = F.relu(conv4_pre)
        return conv4

    def output_shape(self, h, w, c=None):
        """Helper function ot return the output shape for a given input shape,
        without actually performing a forward pass through the model."""
        for child in self.children():
            try:
                h, w = conv2d_output_shape(h, w, child.kernel_size, child.stride, child.padding)
            except AttributeError:
                pass
            try:
                c = child.out_channels
            except AttributeError:
                pass
        return c, h, w

    def output_size(self, h, w, c=None):
        """Helper function ot return the output size for a given input shape,
        without actually performing a forward pass through the model."""
        c, h, w = self.output_shape(h=h, w=w, c=c)
        return c * h * w


class AtariPgModel(torch.nn.Module):
    """Can feed in conv and/or fc1 layer from pre-trained model, or have it
    initialize new ones (if initializing new, must provide image_shape)."""

    def __init__(self, image_shape, action_size, hidden_sizes=512, stop_conv_grad=False, channels=None, kernel_sizes=None, strides=None, paddings=None, kiaming_init=True, normalize_conv_out=False):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings)
        self._conv_out_size = self.conv.conv_out_size(h=h, w=w)
        self.pi_v_mlp = MlpModel(input_size=self._conv_out_size, hidden_sizes=hidden_sizes, output_size=action_size + 1)
        if kiaming_init:
            self.apply(weight_init)
        self.stop_conv_grad = stop_conv_grad
        logger.log('Model stopping gradient at CONV.' if stop_conv_grad else 'Modeul using gradients on all parameters.')
        if normalize_conv_out:
            logger.log('Model normalizing conv output across all pixels.')
            self.conv_rms = RunningMeanStdModel((1,))
            self.var_clip = 1e-06
        self.normalize_conv_out = normalize_conv_out

    def forward(self, observation, prev_action, prev_reward):
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv = self.conv(img.view(T * B, *img_shape))
        if self.stop_conv_grad:
            conv = conv.detach()
        if self.normalize_conv_out:
            conv_var = self.conv_rms.var
            conv_var = torch.clamp(conv_var, min=self.var_clip)
            conv = torch.clamp(0.29 * conv / conv_var.sqrt(), 0, 10)
        pi_v = self.pi_v_mlp(conv.view(T * B, -1))
        pi = F.softmax(pi_v[:, :-1], dim=-1)
        v = pi_v[:, -1]
        pi, v, conv = restore_leading_dims((pi, v, conv), lead_dim, T, B)
        return pi, v, conv

    def update_conv_rms(self, observation):
        if self.normalize_conv_out:
            with torch.no_grad():
                if observation.dtype == torch.uint8:
                    img = observation.type(torch.float)
                    img = img.mul_(1.0 / 255)
                else:
                    img = observation
                lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
                conv = self.conv(img.view(T * B, *img_shape))
                self.conv_rms.update(conv.view(-1, 1))

    def parameters(self):
        if not self.stop_conv_grad:
            yield from self.conv.parameters()
        yield from self.pi_v_mlp.parameters()

    def named_parameters(self):
        if not self.stop_conv_grad:
            yield from self.conv.named_parameters()
        yield from self.pi_v_mlp.named_parameters()

    @property
    def conv_out_size(self):
        return self._conv_out_size


class DmlabPgLstmModel(torch.nn.Module):

    def __init__(self, image_shape, output_size, lstm_size, skip_connections=True, hidden_sizes=None, kiaming_init=True, stop_conv_grad=False, skip_lstm=True):
        super().__init__()
        c, h, w = image_shape
        self.conv = DmlabConv2dModel(in_channels=c, use_fourth_layer=True, use_maxpool=False, skip_connections=skip_connections)
        self._conv_out_size = self.conv.output_size(h=h, w=w)
        self.fc1 = torch.nn.Linear(in_features=self._conv_out_size, out_features=lstm_size)
        self.lstm = torch.nn.LSTM(lstm_size + output_size + 1, lstm_size)
        self.pi_v_head = MlpModel(input_size=lstm_size, hidden_sizes=hidden_sizes, output_size=output_size + 1)
        if kiaming_init:
            self.apply(weight_init)
        self.stop_conv_grad = stop_conv_grad
        logger.log('Model stopping gradient at CONV.' if stop_conv_grad else 'Modeul using gradients on all parameters.')
        self._skip_lstm = skip_lstm

    def forward(self, observation, prev_action, prev_reward, init_rnn_state):
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv = self.conv(img.view(T * B, *img_shape))
        if self.stop_conv_grad:
            conv = conv.detach()
        fc1 = F.relu(self.fc1(conv.view(T * B, -1)))
        lstm_input = torch.cat([fc1.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        if self._skip_lstm:
            lstm_out = lstm_out.view(T * B, -1) + fc1
        pi_v = self.pi_v_head(lstm_out.view(T * B, -1))
        pi = F.softmax(pi_v[:, :-1], dim=-1)
        v = pi_v[:, -1]
        pi, v, conv = restore_leading_dims((pi, v, conv), lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return pi, v, next_rnn_state, conv

    def parameters(self):
        if not self.stop_conv_grad:
            yield from self.conv.parameters()
        yield from self.fc1.parameters()
        yield from self.lstm.parameters()
        yield from self.pi_v_head.parameters()

    def named_parameters(self):
        if not self.stop_conv_grad:
            yield from self.conv.named_parameters()
        yield from self.fc1.named_parameters()
        yield from self.lstm.named_parameters()
        yield from self.pi_v_head.named_parameters()

    @property
    def conv_out_size(self):
        return self._conv_out_size


class SacModel(nn.Module):
    """To keep the standard agent.model interface for shared params, etc."""

    def __init__(self, conv, pi_fc1, pi_mlp):
        super().__init__()
        self.conv = conv
        self.pi_fc1 = pi_fc1
        self.pi_mlp = pi_mlp

    def forward(self, observation, prev_action, prev_reward):
        """Just to keep the standard obs, prev_action, prev_rew interface."""
        conv = self.conv(observation)
        latent = self.pi_fc1(conv)
        mu, log_std = self.pi_mlp(latent, prev_action, prev_reward)
        return mu, log_std, latent, conv


class SacConvModel(nn.Module):

    def __init__(self, image_shape, channels=None, kernel_sizes=None, strides=None, paddings=None, final_nonlinearity=True):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels or [32, 32, 32, 32], kernel_sizes=kernel_sizes or [3, 3, 3, 3], strides=strides or [2, 1, 1, 1], paddings=paddings, final_nonlinearity=final_nonlinearity)
        self._output_shape = self.conv.conv_out_shape(h=h, w=w, c=c)
        self._output_size = self.conv.conv_out_size(h=h, w=w, c=c)

    def forward(self, observation):
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv = self.conv(img.view(T * B, *img_shape))
        conv = restore_leading_dims(conv, lead_dim, T, B)
        return conv

    @property
    def output_shape(self):
        return self._output_shape

    @property
    def output_size(self):
        return self._output_size


class SacFc1Model(nn.Module):

    def __init__(self, input_size, latent_size, layer_norm=True):
        super().__init__()
        self.linear = nn.Linear(input_size, latent_size)
        self.layer_norm = nn.LayerNorm(latent_size) if layer_norm else None
        self._output_size = latent_size

    def forward(self, conv_out):
        if conv_out.dtype == torch.uint8:
            conv_out = conv_out.type(torch.float)
            conv_out = conv_out.mul_(1.0 / 255)
        lead_dim, T, B, _ = infer_leading_dims(conv_out, 3)
        conv_out = F.relu(conv_out.view(T * B, -1))
        latent = self.linear(conv_out)
        if self.layer_norm is not None:
            latent = self.layer_norm(latent)
        latent = restore_leading_dims(latent, lead_dim, T, B)
        return latent

    @property
    def output_size(self):
        return self._output_size


class SacActorModel(nn.Module):

    def __init__(self, input_size, action_size, hidden_sizes, min_log_std=-10.0, max_log_std=2.0):
        super().__init__()
        self.mlp = MlpModel(input_size=input_size, hidden_sizes=hidden_sizes, output_size=action_size * 2)
        self.apply(weight_init)
        self.min_log_std = min_log_std
        self.max_log_std = max_log_std

    def forward(self, latent, prev_action=None, prev_reward=None):
        lead_dim, T, B, _ = infer_leading_dims(latent, 1)
        out = self.mlp(latent.view(T * B, -1))
        mu, log_std = out.chunk(chunks=2, dim=-1)
        log_std = torch.tanh(log_std)
        log_std = self.min_log_std + 0.5 * (self.max_log_std - self.min_log_std) * (1 + log_std)
        mu, log_std = restore_leading_dims((mu, log_std), lead_dim, T, B)
        return mu, log_std


class SacCriticModel(nn.Module):

    def __init__(self, input_size, action_size, hidden_sizes):
        super().__init__()
        self.mlp1 = MlpModel(input_size=input_size + action_size, hidden_sizes=hidden_sizes, output_size=1)
        self.mlp2 = MlpModel(input_size=input_size + action_size, hidden_sizes=hidden_sizes, output_size=1)
        self.apply(weight_init)

    def forward(self, latent, action, prev_action=None, prev_reward=None):
        lead_dim, T, B, _ = infer_leading_dims(latent, 1)
        q_input = torch.cat([latent.view(T * B, -1), action.view(T * B, -1)], dim=1)
        q1 = self.mlp1(q_input).squeeze(-1)
        q2 = self.mlp2(q_input).squeeze(-1)
        q1, q2 = restore_leading_dims((q1, q2), lead_dim, T, B)
        return q1, q2


class SacNoConvModel(nn.Module):
    """To keep the standard agent.model interface for shared params, etc.

    RESULT: yeah this didn't work in most envs, except a bit in walker.
    """

    def __init__(self, pi_fc1, pi_mlp):
        super().__init__()
        self.pi_fc1 = pi_fc1
        self.pi_mlp = pi_mlp

    def forward(self, observation, prev_action, prev_reward):
        """Just to keep the standard obs, prev_action, prev_rew interface."""
        conv = observation
        latent = self.pi_fc1(conv)
        mu, log_std = self.pi_mlp(latent, prev_action, prev_reward)
        return mu, log_std, latent


class UlEncoderModel(torch.nn.Module):

    def __init__(self, conv, latent_size, conv_out_size):
        super().__init__()
        self.conv = conv
        self.head = torch.nn.Linear(conv_out_size, latent_size)

    def forward(self, observation):
        lead_dim, T, B, img_shape = infer_leading_dims(observation, 3)
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        conv = self.conv(img.view(T * B, *img_shape))
        c = self.head(conv.view(T * B, -1))
        c, conv = restore_leading_dims((c, conv), lead_dim, T, B)
        return c, conv


class ContrastModel(torch.nn.Module):

    def __init__(self, latent_size, anchor_hidden_sizes):
        super().__init__()
        if anchor_hidden_sizes is not None:
            self.anchor_mlp = MlpModel(input_size=latent_size, hidden_sizes=anchor_hidden_sizes, output_size=latent_size)
        else:
            self.anchor_mlp = None
        self.W = torch.nn.Linear(latent_size, latent_size, bias=False)

    def forward(self, anchor, positive):
        lead_dim, T, B, _ = infer_leading_dims(anchor, 1)
        assert lead_dim == 1
        if self.anchor_mlp is not None:
            anchor = anchor + self.anchor_mlp(anchor)
        pred = self.W(anchor)
        logits = torch.matmul(pred, positive.T)
        logits = logits - torch.max(logits, dim=1, keepdim=True)[0]
        return logits


class EncoderModel(torch.nn.Module):

    def __init__(self, image_shape, latent_size, channels, kernel_sizes, strides, paddings=None, hidden_sizes=None, kiaming_init=True):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels, kernel_sizes=kernel_sizes, strides=strides, paddings=paddings, use_maxpool=False)
        self._output_size = self.conv.conv_out_size(h, w)
        self._output_shape = self.conv.conv_out_shape(h, w)
        self.head = MlpModel(input_size=self._output_size, hidden_sizes=hidden_sizes, output_size=latent_size)
        if kiaming_init:
            self.apply(weight_init)

    def forward(self, observation):
        lead_dim, T, B, img_shape = infer_leading_dims(observation, 3)
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        conv = self.conv(img.view(T * B, *img_shape))
        c = self.head(conv.view(T * B, -1))
        c, conv = restore_leading_dims((c, conv), lead_dim, T, B)
        return c, conv

    @property
    def output_size(self):
        return self._output_size

    @property
    def output_shape(self):
        return self._output_shape


class DmlabEncoderModel(torch.nn.Module):

    def __init__(self, image_shape, latent_size, use_fourth_layer=True, skip_connections=True, hidden_sizes=None, kiaming_init=True):
        super().__init__()
        c, h, w = image_shape
        self.conv = DmlabConv2dModel(in_channels=c, use_fourth_layer=True, skip_connections=skip_connections, use_maxpool=False)
        self._output_size = self.conv.output_size(h, w)
        self._output_shape = self.conv.output_shape(h, w)
        self.head = MlpModel(input_size=self._output_size, hidden_sizes=hidden_sizes, output_size=latent_size)
        if kiaming_init:
            self.apply(weight_init)

    def forward(self, observation):
        lead_dim, T, B, img_shape = infer_leading_dims(observation, 3)
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        conv = self.conv(img.view(T * B, *img_shape))
        c = self.head(conv.view(T * B, -1))
        c, conv = restore_leading_dims((c, conv), lead_dim, T, B)
        return c, conv

    @property
    def output_size(self):
        return self._output_size

    @property
    def output_shape(self):
        return self._output_shape


class InverseModel(torch.nn.Module):

    def __init__(self, input_size, hidden_sizes, action_size, num_actions, subtract=False, use_input='conv'):
        super().__init__()
        if use_input != 'conv':
            raise NotImplementedError
        self.mlp = MlpModel(input_size=2 * input_size, hidden_sizes=hidden_sizes, output_size=action_size * num_actions)
        self._action_size = action_size
        self._num_actions = num_actions
        self._use_input = use_input
        self._subtract = subtract

    def forward(self, conv_obs, conv_last):
        lead_dim, T, B, _ = infer_leading_dims(conv_obs, 3)
        assert lead_dim == 1
        obs = conv_obs.view(B, -1)
        last = conv_last.view(B, -1)
        if self._subtract:
            last = last - obs
        mlp_input = torch.cat([obs, last], dim=-1)
        act_logits = self.mlp(mlp_input)
        act_logits = act_logits.view(B, self._num_actions, self._action_size)
        return act_logits


class ConvTranspose2dModel(torch.nn.Module):

    def __init__(self, in_channels, channels, kernel_sizes, strides, paddings=None, output_paddings=None, nonlinearity=torch.nn.ReLU, sigmoid_output=False):
        super().__init__()
        if paddings is None:
            paddings = [(0) for _ in range(len(channels))]
        if output_paddings is None:
            output_paddings = [(0) for _ in range(len(channels))]
        assert len(channels) == len(kernel_sizes) == len(strides) == len(paddings) == len(output_paddings)
        in_channels = [in_channels] + list(channels[:-1])
        convt_layers = [torch.nn.ConvTranspose2d(in_channels=ic, out_channels=oc, kernel_size=k, stride=s, padding=p, output_padding=op) for ic, oc, k, s, p, op in zip(in_channels, channels, kernel_sizes, strides, paddings, output_paddings)]
        sequence = list()
        for convt_layer in convt_layers:
            sequence.append(convt_layer)
            sequence.append(nonlinearity())
        sequence.pop(-1)
        if sigmoid_output:
            sequence.append(torch.nn.Sigmoid())
        self.convt = torch.nn.Sequential(*sequence)

    def forward(self, input):
        """Assumes shape is already [B,C,H,W]."""
        return self.convt(input)


class PixelControlModel(torch.nn.Module):

    def __init__(self, input_shape, fc_sizes, reshape, channels, kernel_sizes, strides, paddings=None, output_paddings=None, dueling=True):
        super().__init__()
        if isinstance(input_shape, int):
            input_shape = input_shape,
        self.input_shape = input_shape
        input_size = int(np.prod(input_shape))
        if fc_sizes is None:
            self.mlp = None
            if reshape is None:
                assert len(input_shape) == 3
                in_channels = input_shape[0]
            else:
                assert input_size == int(np.prod(reshape))
        else:
            self.mlp = MlpModel(input_size=input_size, hidden_sizes=fc_sizes)
            assert self.mlp.output_size == int(np.prod(reshape))
            in_channels = reshape[0]
        self.reshape = reshape
        self.dueling = dueling
        if dueling:
            channels[-1] = channels[-1] + 1
        self.convt = ConvTranspose2dModel(in_channels=in_channels, channels=channels, kernel_sizes=kernel_sizes, strides=strides, paddings=paddings, output_paddings=output_paddings, sigmoid_output=False)

    def forward(self, input):
        lead_dim, T, B, in_shape = infer_leading_dims(input, len(self.input_shape))
        x = input.view(T * B, *in_shape)
        if self.mlp is not None:
            x = self.mlp(input.view(T * B, -1))
        if self.reshape is not None:
            x = x.view(T * B, *self.reshape)
        x = self.convt(x)
        if self.dueling:
            value = x[:, :1]
            advantage = x[:, 1:]
            x = value + (advantage - advantage.mean(dim=1, keepdim=True))
        x = restore_leading_dims(x, lead_dim, T, B)
        return x


class Conv2dStdimModel(torch.nn.Module):

    def __init__(self, in_channels, channels, kernel_sizes, strides, paddings=None, nonlinearity=torch.nn.ReLU, use_maxpool=False):
        super().__init__()
        if paddings is None:
            paddings = [(0) for _ in range(len(channels))]
        assert len(channels) == len(kernel_sizes) == len(strides) == len(paddings)
        in_channels = [in_channels] + channels[:-1]
        ones = [(1) for _ in range(len(strides))]
        if use_maxpool:
            maxp_strides = strides
            strides = ones
        else:
            maxp_strides = ones
        conv_layers = [torch.nn.Conv2d(in_channels=ic, out_channels=oc, kernel_size=k, stride=s, padding=p) for ic, oc, k, s, p in zip(in_channels, channels, kernel_sizes, strides, paddings)]
        sequence = list()
        maxp_layers = list()
        for conv_layer, maxp_stride in zip(conv_layers, maxp_strides):
            sequence.append(conv_layer)
            sequence.append(nonlinearity())
            if maxp_stride > 1:
                maxp_layer = torch.nn.MaxPool2d(maxp_stride)
                sequence.append(maxp_layer)
                maxp_layers.append(maxp_layer)
            else:
                maxp_layers.append(None)
        self.conv = torch.nn.Sequential(*sequence)
        self.conv_layers = conv_layers
        self.maxp_layers = maxp_layers

    def forward(self, input):
        conv_outs = list()
        x = input
        for conv, maxp in zip(self.conv_layers, self.maxp_layers):
            x = F.relu(conv(x))
            conv_outs.append(x)
            if maxp is not None:
                x = maxp(x)
        return x, conv_outs

    def conv_out_size(self, h, w, c=None):
        """Helper function ot return the output size for a given input shape,
        without actually performing a forward pass through the model."""
        for child in self.conv.children():
            try:
                h, w = conv2d_output_shape(h, w, child.kernel_size, child.stride, child.padding)
            except AttributeError:
                pass
            try:
                c = child.out_channels
            except AttributeError:
                pass
        return h * w * c

    def conv_layer_shapes(self, h, w, c=None):
        """Helper function ot return the output size for a given input shape,
        without actually performing a forward pass through the model."""
        shapes = list()
        for child in self.conv.children():
            try:
                h, w = conv2d_output_shape(h, w, child.kernel_size, child.stride, child.padding)
            except AttributeError:
                pass
            try:
                c = child.out_channels
                shapes.append((c, h, w))
            except AttributeError:
                pass
        return shapes

    def conv_out_shape(self, h, w, c=None):
        shapes = self.conv_layer_shapes(h=h, w=w, c=c)
        return shapes[-1]

    def conv_out_shapes(self, *args, **kwargs):
        return self.conv_layer_shapes(*args, **kwargs)


class StDimEncoderModel(torch.nn.Module):

    def __init__(self, image_shape, latent_size, channels=None, kernel_sizes=None, strides=None, paddings=None, hidden_sizes=None, kiaming_init=True):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dStdimModel(in_channels=c, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings, use_maxpool=False)
        self._output_size = self.conv.conv_out_size(h, w)
        self._output_shape = self.conv.conv_out_shape(h, w)
        self._conv_layer_shapes = self.conv.conv_layer_shapes(h, w)
        self.head = MlpModel(input_size=self._output_size, hidden_sizes=hidden_sizes, output_size=latent_size)
        if kiaming_init:
            self.apply(weight_init)

    def forward(self, observation):
        lead_dim, T, B, img_shape = infer_leading_dims(observation, 3)
        if observation.dtype == torch.uint8:
            img = observation.type(torch.float)
            img = img.mul_(1.0 / 255)
        else:
            img = observation
        conv, conv_layers = self.conv(img.view(T * B, *img_shape))
        c = self.head(conv_layers[-1].view(T * B, -1))
        c, conv = restore_leading_dims((c, conv), lead_dim, T, B)
        conv_layers = restore_leading_dims(conv_layers, lead_dim, T, B)
        return c, conv, conv_layers

    @property
    def conv_layer_shapes(self):
        return self._conv_layer_shapes

    @property
    def conv_out_shapes(self):
        return self._conv_layer_shapes

    @property
    def output_size(self):
        return self._output_size

    @property
    def output_shape(self):
        return self._output_shape


class StDimGlobalLocalContrastModel(torch.nn.Module):
    """The anchor gets contrasted over the batch for each location in the
    positive."""

    def __init__(self, latent_size, local_size, anchor_hidden_sizes):
        super().__init__()
        self.anchor_mlp = MlpModel(input_size=latent_size, hidden_sizes=anchor_hidden_sizes, output_size=latent_size)
        self.W = torch.nn.Linear(latent_size, local_size, bias=False)

    def forward(self, anchor, positive):
        anchor = anchor + self.anchor_mlp(anchor)
        b, c, h, w = positive.shape
        logits_list = list()
        pred = self.W(anchor)
        for y in range(h):
            for x in range(w):
                logits = torch.matmul(pred, positive[:, :, y, x].T)
                logits = logits - torch.max(logits, dim=1, keepdim=True)[0]
                logits_list.append(logits)
        return logits_list


class StDimLocalLocalContrastModel(torch.nn.Module):

    def __init__(self, local_size, anchor_hidden_sizes):
        super().__init__()
        self.anchor_mlp = MlpModel(input_size=local_size, hidden_sizes=anchor_hidden_sizes, output_size=local_size)
        self.W = torch.nn.Linear(local_size, local_size, bias=False)

    def forward(self, anchor, positive):
        b, c, h, w = anchor.shape
        logits_list = list()
        for y in range(h):
            for x in range(w):
                anchor_xy = anchor[:, :, y, x]
                anchor_xy = anchor_xy + self.anchor_mlp(anchor_xy)
                pred_xy = self.W(anchor_xy)
                logits = torch.matmul(pred_xy, positive[:, :, y, x].T)
                logits = logits - torch.max(logits, dim=1, keepdim=True)[0]
                logits_list.append(logits)
        return logits_list


class VaeHeadModel(torch.nn.Module):

    def __init__(self, latent_size, action_size, hidden_sizes):
        super().__init__()
        self.head = MlpModel(input_size=latent_size + action_size, hidden_sizes=hidden_sizes, output_size=latent_size * 2)
        self._latent_size = latent_size

    def forward(self, h, action=None):
        """Assume [B] leading dimension."""
        h = F.relu(h)
        x = h if action is None else torch.cat([h, action], dim=-1)
        head = self.head(x)
        mu = head[:, :-self._latent_size]
        logvar = head[:, self._latent_size:]
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = eps * std + mu
        return z, mu, logvar


class VaeDecoderModel(torch.nn.Module):

    def __init__(self, latent_size, reshape, channels=None, kernel_sizes=None, strides=None, paddings=None, output_paddings=None):
        super().__init__()
        self.linear = torch.nn.Linear(latent_size, int(np.prod(reshape)))
        self.convt = ConvTranspose2dModel(in_channels=reshape[0], channels=channels or [32, 32, 32, 9], kernel_sizes=kernel_sizes or [3, 3, 3, 3], strides=strides or [2, 2, 2, 1], paddings=paddings or [0, 0, 0, 0], output_paddings=output_paddings or [0, 1, 1, 0], sigmoid_output=True)
        self.reshape = reshape

    def forward(self, latent):
        """Assume [B] leading dimension."""
        x = self.linear(latent)
        b, h = x.shape
        x = x.reshape(b, *self.reshape)
        convt = self.convt(x)
        return convt


import torch
from torch.nn import MSELoss, ReLU
from paritybench._paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ContrastModel,
     lambda: ([], {'latent_size': 4, 'anchor_hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (Conv2dModel,
     lambda: ([], {'in_channels': 4, 'channels': [4, 4], 'kernel_sizes': [4, 4], 'strides': [4, 4]}),
     lambda: ([torch.rand([4, 4, 64, 64])], {}),
     True),
    (Conv2dStdimModel,
     lambda: ([], {'in_channels': 4, 'channels': [4, 4], 'kernel_sizes': [4, 4], 'strides': [4, 4]}),
     lambda: ([torch.rand([4, 4, 64, 64])], {}),
     False),
    (ConvTranspose2dModel,
     lambda: ([], {'in_channels': 4, 'channels': [4, 4], 'kernel_sizes': [4, 4], 'strides': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (CppoModel,
     lambda: ([], {'observation_shape': [4, 4], 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (DistributionalDuelingHeadModel,
     lambda: ([], {'input_size': 4, 'hidden_sizes': 4, 'output_size': 4, 'n_atoms': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DistributionalHeadModel,
     lambda: ([], {'input_size': 4, 'layer_sizes': 1, 'output_size': 4, 'n_atoms': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DmlabConv2dModel,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 64, 64])], {}),
     True),
    (DuelingHeadModel,
     lambda: ([], {'input_size': 4, 'hidden_sizes': 4, 'output_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MlpModel,
     lambda: ([], {'input_size': 4, 'hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MuMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4, 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (MujocoFfModel,
     lambda: ([], {'observation_shape': [4, 4], 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (PiMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4, 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (SacActorModel,
     lambda: ([], {'input_size': 4, 'action_size': 4, 'hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (SacCriticModel,
     lambda: ([], {'input_size': 4, 'action_size': 4, 'hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (StDimGlobalLocalContrastModel,
     lambda: ([], {'latent_size': 4, 'local_size': 4, 'anchor_hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (StDimLocalLocalContrastModel,
     lambda: ([], {'local_size': 4, 'anchor_hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (VMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (VaeDecoderModel,
     lambda: ([], {'latent_size': 4, 'reshape': [4, 4]}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
]

class Test_astooke_rlpyt(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

